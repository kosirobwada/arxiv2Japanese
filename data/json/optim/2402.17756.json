{
    "optim": "Robustly Learning Single-Index Models via Alignment Sharpness\nNikos Zarifis∗‖\nUW Madison\nzarifis@wisc.edu\nPuqian Wang†‖\nUW Madison\npwang333@wisc.edu\nIlias Diakonikolas‡\nUW Madison\nilias@cs.wisc.edu\nJelena Diakonikolas§\nUW Madison\njelena@cs.wisc.edu\nAbstract\nWe study the problem of learning Single-Index Models under the L2\n2 loss in the agnostic\nmodel. We give an efficient learning algorithm, achieving a constant factor approximation to the\noptimal loss, that succeeds under a range of distributions (including log-concave distributions)\nand a broad class of monotone and Lipschitz link functions. This is the first efficient constant\nfactor approximate agnostic learner, even for Gaussian data and for any nontrivial class of link\nfunctions. Prior work for the case of unknown link function either works in the realizable setting\nor does not attain constant factor approximation. The main technical ingredient enabling our\nalgorithm and analysis is a novel notion of a local error bound in optimization that we term\nalignment sharpness and that may be of broader interest.\n∗Supported in part by NSF Medium Award CCF-2107079 and NSF award 2023239.\n†Supported in part by NSF Award CCF-2007757.\n‡Supported by NSF Medium Award CCF-2107079 and a DARPA Learning with Less Labels (LwLL) grant.\n§Supported by NSF Award CCF-2007757 and by the U. S. Office of Naval Research under award number N00014-\n22-1-2348.\n‖Equal contribution.\narXiv:2402.17756v1  [cs.LG]  27 Feb 2024\n1\nIntroduction\nSingle-index models (SIMs) [Ich93, HJS01, HMS+04, DJS08, KS09, KKSK11, DH18] are a classical\nsupervised learning model extensively studied in statistics and machine learning. SIMs capture\nthe common assumption that the target function f depends on an unknown direction w, i.e.,\nf(x) = u(w · x) for some link (a.k.a. activation) function u : R 7→ R and w ∈ Rd. In most settings,\nthe link function is unknown and is assumed to satisfy certain regularity properties. Classical\nworks [KS09, KKSK11] studied the efficient learnability of SIMs for monotone and Lipschitz link\nfunctions and data distributed on the unit ball. These early algorithmic results succeed in the\nrealizable setting (i.e., with clean labels) or in the presence of zero-mean label noise.\nThe focus of this work is on learning SIMs in the challenging agnostic (or adversarial label noise)\nmodel [Hau92, KSS94], where no assumptions are made on the labels of the examples and the goal\nis to compute a hypothesis that is competitive with the best-fit function in the class. Importantly, as\nwill be formalized below, we will not assume a priori knowledge of the link function. In more detail,\nlet D be a distribution on labeled examples (x, y) ∈ Rd × R and L2(h) = E(x,y)∼D[(h(x) − y)2] be\nthe squared loss of the hypothesis h : Rd → R with respect to D. Given i.i.d. samples from D, the\ngoal of the learner is to output a hypothesis h with squared error competitive with OPT, where\nOPT = inff∈C L2(f) is the best attainable error by any function in the target class C.\nIn the context of this paper, the class C above is the class of SIMs, i.e., all functions of the form\nf(x) = u(w · x) where both the weight vector w and the link function u are unknown. For this task\nto be even information-theoretically solvable, one requires some assumptions on the vector w and the\nlink function u. We will assume, as is standard, that the ℓ2-norm of w is bounded by a parameter\nW. We will similarly assume that the link function lies in a family of well-behaved functions that\nare monotone and satisfy certain Lipschitz properties (see Definition 1.3).\nFor a weight vector w and link function u, the L2\n2 loss of the SIM hypothesis u(w · x) (defined by\nu and w) is L2(w; u) = E(x,y)∼D[(u(w · x) − y)2]. Our problem of robustly learning SIMs is defined\nas follows.\nProblem 1.1 (Robustly Learning Single-Index Models). Fix a class of distributions G on Rd and a\nclass of link functions1 F. Let D be a distribution of labeled examples (x, y) ∈ Rd × R such that its\nx-marginal Dx belongs to G. We say that an algorithm is a C-approximate proper SIM learner, for\nsome C ≥ 1, if given ϵ > 0, W > 0, and i.i.d. samples from D, the algorithm outputs a link function\nˆu ∈ F and a vector bw ∈ Rd such that with high probability it holds L2(bw; ˆu) ≤ C OPT + ϵ, where\nOPT ≜ min∥w∥2≤W,u∈F L2(w; u).\nThroughout this paper, we use u∗(w∗ · x) to denote a fixed (but arbitrary) optimal solution to\nthe above learning problem, i.e., one satisfying L2(w∗; u∗) = OPT.\nSome comments are in order. First, Problem 1.1 does not make realizability assumptions on the\ndistribution D. That is, the labels are allowed to be arbitrary and the goal is to be competitive\nagainst the best-fit function in the class C = {u(w · x) | w ∈ Rd, ∥w∥2 ≤ W, u ∈ F} . Second, our\nfocus is on obtaining efficient learners that achieve a constant factor approximation to the optimum\nloss, i.e., where C in Problem 1.1 is a universal constant — independent of the dimension d and the\nradius W of the weight space.\nIdeally, one would like an efficient learner that succeeds for all marginal distributions and achieves\noptimal error of OPT + ϵ (corresponding to C = 1). Unfortunately, known computational hardness\nresults rule out this possibility. Even for the very special case that the marginal distribution is\nGaussian and the link function is known (e.g., a ReLU), there is strong evidence that any algorithm\n1Throughout this paper, we will use the terms “link function” and “activation” interchangeably.\n1\nachieving error OPT+ϵ requires dpoly(1/ϵ) time [DKZ20, GGK20, DKPZ21, DKR23]. Moreover, even\nif we relax our goal to constant factor approximation (i.e., C = O(1)), distributional assumptions\nare required both for proper [Sím02, MR18] and improper learning [DKMR22]. As a consequence,\nalgorithmic research in this area has focused on constant factor approximate learners that succeed\nunder mild distributional assumptions.\nRecent works [DGK+20, DKTZ22, ATV23, WZDD23] gave efficient, constant factor approximate\nlearners, under natural distributional assumptions, for the special case of Problem 1.1 where the link\nfunction is known a priori (see also [FCG20]). For the general setting, the only prior algorithmic\nresult was recently obtained in [GGKS23]. Specifically, [GGKS23] gave an efficient algorithm that\nsucceeds for the class of monotone 1-Lipschitz link functions and any marginal distribution with\nsecond moment bounded by λ. Their algorithm achieves L2\n2 error\nO(W\n√\nλ\n√\nOPT) + ϵ\n(1)\nunder the assumption that the labels are bounded in [0, 1]. The error guarantee (1) is substantially\nweaker — both qualitatively and quantitatively — from the goal of this paper. Firstly, the dependence\non OPT scales with its square root, as opposed to linearly. Secondly, and arguably more importantly,\nthe multiplicative factor inside the big-O scales (linearly) with the diameter of the space W.\nInterestingly, [GGKS23] showed — via a hardness construction from [DKMR22] — that, under\ntheir distributional assumptions, a multiplicative dependence on W (in the error guarantee) is\ninherent for efficient algorithms. That is, to obtain an efficient constant factor approximation, it is\nnecessary to restrict ourselves to distributions with additional structural properties. This discussion\nraises the following question:\nCan we obtain efficient constant factor learners for Problem 1.1 under mild distributional\nassumptions?\nThe natural goal here is to match the guarantees of known algorithmic results for the special case of\nknown link function [DKTZ22, WZDD23].\nAs our main contribution, we answer this question in the affirmative. That is, we give the\nfirst efficient constant-factor approximate learner that succeeds for natural and broad families of\ndistributions (including log-concave distributions) and a broad class of link functions. We emphasize\nthat this is the first polynomial-time constant factor approximate learner even for Gaussian marginals\nand for any nontrivial class of link functions. Roughly speaking, our distributional assumptions\nrequire concentration and (anti)-anti-concentration (see Definition 1.2).\n1.1\nOverview of Results\nWe start by stating the distributional assumptions and defining the family of link functions for which\nour algorithm succeeds.\nDistributional Assumptions\nOur algorithm succeeds for the following class of structured\ndistributions.\nDefinition 1.2 (Well-Behaved Distributions). Let L, R > 0. Let V be any subspace in Rd of\ndimension at most 2. A distribution Dx on Rd is called (L, R)-well-behaved if for any projection\n(Dx)V of Dx onto subspace V , the corresponding pdf γV on R2 satisfies the following:\n• For all xV ∈ V such that ∥xV ∥∞ ≤ R, γV (xV ) ≥ L (anti-anti-concentration).\n• For all xV ∈ V , γV (xV ) ≤ (1/L)(e−L∥xV ∥2) (anti-concentration and concentration).\n2\nAs a consequence of sub-exponential concentration, we can assume without loss of generality\nthat the operator norm of Ex∼Dx[xx⊤] is bounded above by an absolute constant. For simplicity,\nwe take Ex∼Dx[xx⊤] ≼ I, which can be ensured by simple rescaling of the data.\nThe distribution class of Definition 1.2 was introduced in [DKTZ20], in the context of learning\nlinear separators with noise, and has since been used in a number of prior works — including for\nrobustly learning SIMs with known link function [DKTZ22]. The parameters L, R in Definition 1.2 are\nviewed as universal constants, i.e., L, R = O(1). Indeed, it is known that many natural distributions,\nmost importantly isotropic log-concave distributions, fall in this category; see, e.g., [DKTZ20].\nUnbounded Activations\nOur algorithm succeeds for a broad class of link functions that con-\ntains many well-studied activations, including ReLUs. This class, defined in [DKTZ22] and used\nin [WZDD23], requires the link function to be monotone, Lipschitz-continuous and strictly increasing\nin the positive region.\nDefinition 1.3 (Unbounded Activations). Let u : R 7→ R. Given a, b ∈ R such that 0 < a ≤ b, we\nsay that u(z) is (a, b)-unbounded if u(0) = 0 and u(z) is non-decreasing, b-Lipschitz-continuous, and\nu(z) − u(z′) ≥ a(z − z′) for all z ≥ z′ ≥ 0. We denote this function class by U(a,b).\nA simplified version of our main algorithmic result is as follows (see Theorem 4.2 for a more\ndetailed statement):\nTheorem 1.4 (Main Algorithmic Result, Informal). Given Problem 1.1, where G is the class of\n(L, R)-well behaved distributions with L, R = O(1) and F = U(a,b) such that (1/a), b = O(1), there\nis an algorithm that draws N = poly(W) ˜O(d/ϵ2) samples from D, runs in poly(N, d) time, and\noutputs a hypothesis ˆu(bw · x) with ˆu ∈ U(a,b), ∥bw∥2 ≤ W such that L2(bw; ˆu) = C OPT + ϵ with high\nprobability, where C > 0 is an absolute constant.\nWe reiterate that the approximation factor C in Theorem 1.4 is a universal constant, independent\nof the dimension and the diameter of the space. That is, our main result provides the first efficient\nlearning algorithm achieving a constant factor approximation, even for the most basic case of\nGaussian data and any non-trivial class of link functions.\n1.2\nTechnical Overview\nWhen it comes to learning SIMs in the agnostic model with target error COPT + ϵ, to the best of\nour knowledge, all prior work that achieves such a guarantee with C being an absolute constant\nonly applies to the special case of known link function u∗. Such results are established by proving\ngrowth conditions (local error bounds) that relate either the L2\n2 loss or a surrogate loss to (squared)\ndistance to the set of target solutions, using assumptions about the link function and the data\ndistribution, such as concentration and (anti-)anti-concentration [DGK+20, DKTZ22, WZDD23].\nAmong these, most relevant to our work is [WZDD23], which proved a “sharpness” property for the\nconvex surrogate function defined by\nLsur(w; u) =\nE\n(x,y)∼D\n\u0014 Z w·x\n0\n(u(r) − y) dr\n\u0015\n,\n(2)\nbased on certain assumptions about the link function (that are the same as ours) and distributional\nassumptions (that are somewhat weaker but comparable to ours). Their sharpness result corresponds\nto guaranteeing that for vectors w that are not already O(OPT) + ϵ accurate solutions, the following\nholds:\n∇Lsur(w; u∗) · (w − w∗) ≳ ∥w − w∗∥2\n2,\n(3)\n3\nwhere w∗ is a vector that achieves error O(OPT) + ϵ and u∗ is the (a priori known) link function.\nOne may hope that the sharpness result of [WZDD23] can be generalized to the case of unknown\nlink function and leveraged to obtain constant factor robust learners in this more general setting.\nHowever, as we discuss below, such direct generalizations are not possible and there are several\ntechnical challenges that had to be overcome in our work. To illustrate some of the intricacies,\nconsider first the following example.\nExample 1.5. Let x ∼ N(0, I) and w = (1/2)w∗, where w∗ is an arbitrary but fixed target unit\nvector. Let b > 2a. Suppose that the link function at hand is u(z) = bz and the target link function\nis u∗(z) = az. Observe that both u, u∗ ∈ U(a,b), as required by our model. Furthermore, suppose\nthere is no label noise, in which case OPT = 0. Note that the L2\n2 error of u(w · x) in this case is\nL2(w; u) =\nE\nx∼N(0,I)[(u(w · x) − u∗(w∗ · x))2]\n=\nE\nz∼N(0,1)[(b/2 − a)2z2] = (b/2 − a)2 = Θ(1).\nHowever, the gradient of the surrogate loss, ∇Lsur(w; u) = E[(u(w · x) − u∗(w∗ · x))x], is negatively\ncorrelated with w − w∗, i.e., ∇Lsur(w; u) · (w − w∗) < 0, contrary to what we would hope for if a\nsharpness property as in [WZDD23] were to hold. Thus, although w and u are both still far away\nfrom the target parameters w∗ and u∗, the gradient of the surrogate loss cannot provide useful\ninformation about the direction in which to update w.\nWhat Example 1.5 demonstrates is that we cannot hope for the surrogate loss to satisfy a local\nerror bound for an arbitrary parameter pair (u, w) that would guide the convergence of an algorithm\ntoward a target parameter pair (u∗, w∗). This seemingly insurmountable obstacle is surpassed by\nobserving that we do not, in fact, need the surrogate loss to contain a “signal” that would guide\nus toward target parameters for an arbitrary pair (u, w). Instead, we can restrict our attention\nto pairs (u, w) satisfying that u is a “reasonably good” link function for the vector w. Ideally, we\nwould like to only consider link functions u that minimize the L2\n2 loss — considering that u∗ must\nminimize the L2\n2 loss for a given, fixed w∗ — but it is unclear how to achieve that in a statistically\nand computationally efficient manner. As a natural approach, we consider link functions that\nare the best fitting functions in an empirical distribution sense. In particular, given a sample set\nS = {(x(i), y(i))}m\ni=1 and a parameter w, we select a function ˆuw that solves the following (convex)\noptimization problem:\nˆuw ∈ argmin\nu∈U(a,b)\n1\nm\nm\nX\ni=1\n(u(w · x(i)) − y(i))2.\n(P)\nFor notational simplicity, we drop the parameter wt from ˆuwt and use ˆut instead. It is worth pointing\nout here that in general the problem of finding the best function that minimizes the L2\n2 error fails\nunder the category of non-parametric regression, which unfortunately requires exponentially many\nsamples (namely, Ω(1/ϵd)). Fortunately, in our setting, we are looking for the best function that lies\nin a one-dimensional space. Therefore, instead of looking at all possible directions, we can project\nall the points of the sample set S to the direction w and find the best fitting link function efficiently.\nWe provide the full details for efficiently solving the optimization problem (P) in Appendix E.\nHaving set on the “best-fit” link functions in the sense of the problem (P), the next obstacle\none encounters when trying to prove a “sharpness-like” result is that neither the L2\n2 loss nor its\nsurrogate convey information about the scale of w and w∗. This is because models determined by\nu, w and u/c, cw for some parameter c > 0 can have the same value of both loss functions. Thus, it\nseems unlikely that a more traditional local error bound, as in (3), can be established in general,\n4\nfor either the surrogate loss or the original L2\n2 loss. Instead, we prove a weaker property that\nestablishes strong correlation between the gradient of the empirical surrogate loss ∇ bLsur(wt; ˆut) =\n(1/m) Pm\ni=1(ˆut(wt · x(i)) − y(i))x(i) and the direction wt − w∗ that holds whenever wt is not an\nO(OPT) + ϵ error solution and which is independent of the scale of wt. This constitutes our key\nstructural result, stated as Proposition 3.1 and discussed in detail in Section 3. We further discuss\nhow this result relates to classical and recent local error bounds in Appendix B.\nIn addition to this weaker version of a sharpness property, we further prove in Corollary 3.4 that\ngiven a parameter wt and a dataset of m samples from D, the activation ˆut(wt · x) generated by\noptimizing the empirical risk on the dataset as in (P) satisfies Ex∼Dx[(ˆut(wt · x) − u∗(w∗ · x))2] ≲\nb2∥wt − w∗∥2\n2 with high probability. As a result, we can guarantee that when ∥wt − w∗∥2 decreases,\nthe L2\n2 distance between ˆut and u∗ diminishes as well. This is crucial, since without such a coupling\nwe would not be able to argue about convergence over both model parameters u, w.\nLeveraging these results, we arrive at an algorithm that alternates between “gradient descent-style”\nupdates for w and best-fit updates for u. We note in passing that similar alternating updates have\nbeen used in classical work on SIM learning in the less challenging, non-agnostic setting [KKSK11].\nIn more detail, our algorithm fixes the scale β of ∥wt∥2 and alternates between taking a Riemannian\ngradient descent step on a sphere for wt with respect to the empirical surrogate loss and solving (P).\nThe unknown scale for the true parameter vector w∗ is resolved by applying this approach using\nβ chosen from a sufficiently fine grid of the interval [0, W] and employing a testing procedure at\nthe end to select the best parameter vector. Although the idea is simple, the proof of correctness is\nquite technical, as it requires ensuring that the entire process does not accumulate spurious errors\narising from the stochastic nature of the problem, adversarial labels, and approximate minimization\nof the surrogate loss, and, as a result, that it converges to the target error.\nTechnical Comparison to [GGKS23]\nThe only prior work addressing SIM learning (with\nunknown link functions) in the agnostic model is [GGKS23], thus here we provide a technical\ncomparison. While both [GGKS23] and our work make use of the surrogate loss function from\n(2), on a technical level the two works are completely disjoint. [GGKS23] uses a framework of\nomnipredictors to minimize the surrogate loss and then relates this result to the L2\n2 loss. Although\nthey handle more general distributions and activations, their learner outputs a hypothesis with error\nthat cannot be considered constant factor approximation (see (1)) and is improper. By contrast, our\nwork does not seek to minimize the surrogate loss. Instead, our main insight is that the gradient of\nthe surrogate loss at a vector w conveys information about the direction of a target vector w∗, for\na fixed link function that minimizes the L2\n2 loss. We leverage this property to construct a proper\nlearner achieving constant factor approximation.\n2\nPreliminaries\nBasic Notation\nFor n ∈ Z+, let [n] := {1, . . . , n}. We use lowercase boldface characters for\nvectors. We use x · y for the inner product of x, y ∈ Rd and θ(x, y) for the angle between x, y. For\nx ∈ Rd and k ∈ [d], xk denotes the kth coordinate of x, and ∥x∥2 denotes the ℓ2-norm of x. We use\n1A = 1{A} to denote the characteristic function of the set A. For vectors v, u ∈ Rd, we denote by\nv⊥u the projection of v onto the subspace orthogonal to u, i.e., v⊥u := v − ((v · u)u)/∥u∥2\n2. We use\nB(r) to denote the ℓ2 ball in Rd of radius r, centered at the origin.\nAsymptotic Notation\nWe use the standard O(·), Θ(·), Ω(·) asymptotic notation. We use eO(·)\nto omit polylogarithmic factors in the argument. We use Op(·) to suppress polynomial dependence\n5\non p, i.e., Op(ω) = O(poly(p)ω). Θp(·) and Ωp(·) are defined similarly. We write E ≳ F for two\nnon-negative expressions E and F to denote that there exists some positive universal constant c > 0\n(independent of the variables or parameters on which E and F depend) such that E ≥ c F. The\nnotation ≲ is defined similarly.\nProbability Notation\nWe use EX∼D[X] for the expectation of a random variable X according\nto the distribution D and Pr[E] for the probability of event E. For simplicity of notation, we omit\nthe distribution when it is clear from the context. For (x, y) distributed according to D, we use Dx\nto denote the marginal distribution of x.\nOrganization\nIn Section 3, we establish our main structural result of alignment sharpness. In\nSection 4, we describe and analyze our constant factor approximate SIM learner. We conclude the\npaper in Section 5. Some of the proofs and technical details are deferred to the Appendix.\n3\nMain Structural Result: Alignment Sharpness of Surrogate Loss\nIn this section, we establish our main structural result (Proposition 3.1), which is what crucially\nenables us to obtain the target O(OPT)+ϵ error for the studied problem. Proposition 3.1 states that\nthe empirical gradient of the surrogate loss (2) positively correlates with the direction of wt − w∗\nwhenever wt does not correspond to an O(OPT) + ϵ error solution; and, moreover, the correlation is\nproportional to the quantity ∥(w∗)⊥wt∥2\n2. This is a key property that is leveraged in our algorithmic\nresult (Theorem 4.2), both in obtaining an O(OPT) + ϵ error result, and in arguing about the\nconvergence and computational efficiency of our algorithm.\nIntuitively, what Proposition 3.1 allows us to argue is that as long as the angle between wt\nand w∗ is not close to zero, we can update wt to better align it with w∗ (in the sense that we\nreduce the angle between these two vectors).\nTo understand this statement better, note that when\n∥wt∥2 ≈ ∥w∗∥2, we also have ∥(w∗)⊥wt∥2 ≈ ∥wt − w∗∥2. Additionally, ∥wt − w∗∥2 = O(OPT + ϵ)\nimplies that the L2\n2 error of the hypothesis defined by ˆut, wt is O(OPT + ϵ) (see Claim 4.4). Thus,\nfor a sufficiently good guess of the value of ∥w∗∥2, Proposition 3.1 provides a local error bound of\nthe form ∇ bLsur(wt; ˆut) · (wt − w∗) ≳ µ∥wt − w∗∥2\n2 that holds outside of the set of O(OPT + ϵ) error\nsolutions, allowing us to contract the distance to this set.\nProposition 3.1 (Alignment Sharpness of the Convex Surrogate). Suppose that Dx is (L, R)-well-\nbehaved, U(a,b) is as in Definition 1.3, and ϵ, δ > 0. Let µ ≳ a2LR4/b. Given any wt ∈ B(W), denote\nby ˆut the optimal solution to (P) with respect to wt and the sample set S = {(x(i), y(i))}m\ni=1 drawn\ni.i.d. from D. If m satisfies\nm ≳ dW 9/2b4L−4 log4(d/(ϵδ))(1/ϵ3/2 + 1/(ϵδ)) ,\nthen, with probability at least 1 − δ,\n∇ bLsur(wt; ˆut) · (wt − w∗) ≥ µ∥(w∗)⊥wt∥2\n2 − 2(OPT + ϵ)/b − 2(\n√\nOPT + √ϵ)∥wt − w∗∥2 .\nTo prove Proposition 3.1, we rely on the following key ingredients. In Section 3.1, we prove our\nmain technical lemma (Lemma 3.2), which states that the L2\n2 distance between the hypothesis u(w·x)\nand the target u∗(w∗ · x) is bounded below by the misalignment of wt and w∗, i.e., the squared\nnorm of the component of w∗ that is orthogonal to wt, ∥(w∗)⊥wt∥2\n2. As will become apparent in\nthe proof of Proposition 3.1, the inner product ∇ bLsur(wt; ˆut) · (wt − w∗) can be bounded below as\n6\na function of the empirical L2\n2 error for wt and a different (but related) activation ˆu∗t, which can\nin turn be argued to be close to the population L2\n2 error for a sufficiently large sample size, using\nconcentration. Thus, Lemma 3.2 can be leveraged to obtain a term scaling with ∥(w∗)⊥wt∥2\n2 in the\nlower bound on ∇ bLsur(wt; ˆut) · (wt − w∗).\nIn Section 3.2, we characterize structural properties of the population-optimal link functions ut\nand u∗t (see (EP) and (EP*)), which play a crucial role in the proof of Proposition 3.1. Specifically,\nwe show that the activation ut is close to the idealized activation u∗t (the optimal activation without\nnoise, given wt) in L2\n2 distance (Lemma 3.3). Since by standard uniform convergence results we\nhave that ˆut and ˆu∗t are close to their population counterparts ut and u∗t, respectively, Lemma 3.3\ncertifies that ˆut is not far from ˆu∗t. This property enables us to replace ˆut by (the idealized) ˆu∗t in\nthe empirical surrogate gradient ∇ bLsur(wt; ˆut), which is easier to analyze, since ˆu∗t is defined with\nrespect to the “ideal” dataset (with uncorrupted labels).\nFinally, as a simple corollary of Lemma 3.3, we obtain Corollary 3.4, which gives a clear\nexplanation of why our algorithm, which alternates between updating wt and ˆut, works: we show\nthat the L2\n2 loss between the hypothesis generated by our algorithm ˆut(wt · x) and the underlying\noptimal hypothesis u∗(w∗ · x) is bounded above by the distance between wt and w∗. Since our\nstructural sharpness result (Proposition 3.1) enables us to decrease ∥wt −w∗∥2, Corollary 3.4 certifies\nthat choosing the empirically-optimal activation leads to convergence of the hypothesis ˆut(wt · x).\nEquipped with these technical lemmas, we prove our main structural result (Proposition 3.1) in\nSection 3.3.\n3.1\nL2\n2 Error and Misalignment\nOur first key result is Lemma 3.2 below, which plays a critical role in the proof of Proposition 3.1. As\ndiscussed in Section 1.2, for two different activations u and u∗ and parameters w and w∗ such that w\nand w∗ are parallel, even when the L2\n2 error is Ω(1), the gradient ∇Lsur(w; u) might not significantly\nalign with the direction of w−w∗, and thus cannot provide sufficient information about the direction\nto decrease ∥w − w∗∥2. Intuitively, the following lemma shows that this is the only thing that can\ngo wrong, and it happens when w and w∗ are parallel. In particular, Lemma 3.2 shows that for any\nsquare integrable link function f, we can relate the L2\n2 distance Ex∼Dx[(f(w · x) − u∗(w∗ · x))2] to\nthe magnitude of the component of w∗ that is orthogonal to w. Although its proof is quite technical,\nthis lemma is the main supporting result allowing us to prove Proposition 3.1, thus we provide its full\nproof below. It is however possible to follow the rest of this section by only relying on its statement.\nLemma 3.2 (Lower Bound on L2\n2 Error by Misalignment). Let u∗ ∈ U(a,b), Dx be (L, R)-well-behaved,\nand f : R 7→ R be square-integrable with respect to the measure of the distribution Dx. Then, for any\nw, w∗ ∈ Rd,\nE\nx∼Dx[(f(w · x) − u∗(w∗ · x))2] ≳ a2LR4∥(w∗)⊥w∥2\n2 .\nProof. The statement holds trivially if w is parallel to w∗, so assume this is not the case. Let\nv = (w∗)⊥w = w∗ − (w∗ · w)w/∥w∥2\n2. Suppose first that w · w∗ ≥ 0. Then w∗ = αw + v, for some\nα > 0. Let V be the subspace spanned by w, v. Then,\nE\nx∼Dx[(f(w·x)−u∗(w∗·x))2] =\nE\nx∼Dx[(f(w·xV )−u∗(w∗·xV ))2] ≥\nE\nx∼Dx[(f(w·xV )−u∗(w∗·xV ))21{xV ∈ A}] ,\nfor any A ⊆ Rd. For ease of notation, we drop the subscript V , and we assume that all x are\nprojected to the subspace V . We denote by ˜w = w/∥w∥2 (resp. ˜v = v/∥v∥2) the unit vector in the\ndirection of w (resp. v). We choose A = {x ∈ Rd : w · x ≥ 0, ˜v · x ∈ (R/16, R/8) ∪ (3R/8, R/2)}.\n7\nThe idea of the proof is to utilize the non-decreasing property of u∗ and the fact that the marginal\ndistribution Dx is anti-concentrated on the subspace V . In short, for any x such that |˜v · x| ≤ R, by\nthe non-decreasing property of u∗ we know that f(w · x) falls into one of the following four intervals:\n(−∞, u∗(αw · x + ∥v∥2R/32)] ,\n(u∗(αw · x + ∥v∥2R/32), u∗(αw · x + ∥v∥2R/4)] ,\n(u∗(αw · x + ∥v∥2R/4), u∗(αw · x + ∥v∥2R)] ,\n(u∗(αw · x + ∥v∥2R), +∞) .\nWhen f(w·x) belongs to any of the intervals above, we can show that with some constant probability,\nthe difference between w∗ · x and w · x is proportional to ∥v∥2, and hence u∗(w∗ · x) is far from\nf(w · x) (due to the well-behaved property of the marginal Dx).\nTo indicate that f(w · x) belongs to one of the intervals above, denote\nI1(x) = f(w · x) − u∗(αw · x + ∥v∥2R/32) ,\nI2(x) = f(w · x) − u∗(αw · x + ∥v∥2R/4) ,\nI3(x) = f(w · x) − u∗(αw · x + ∥v∥2R) .\nFor any x ∈ Rd, using the assumption that u∗ is non-decreasing, we have that I1(x) ≥ I2(x) ≥ I3(x);\nas a consequence, it must be that I1(x)I2(x) ≥ 0 or I2(x)I3(x) ≥ 0.\nFigure 1: Under the assumption that ˜v · x ∈ (R/16, R/8), and I1(x) ≥ 0, I2(x) ≥ 0, the distance\nbetween f(w · x) and u∗(w∗ · x) is at least |u∗(αw · x + ∥v∥2R/4) − u∗(w∗ · x)| ≥ a∥v∥2R/8.\nCase 1: f(w · x) ∈ (u∗(αw · x + ∥v∥2R/4), ∞). Then I1(x) ≥ I2(x) ≥ 0. Let\nB := {x ∈ Rd : w · x ≥ 0, ˜v · x ∈ (R/16, R/8)}\nand notice that B ⊆ A. We have that when x ∈ B,\nu∗(w∗ · x) = u∗(αw · x + ∥v∥2˜v · x) ∈ (u∗(αw · x + ∥v∥2R/16), u∗(αw · x + ∥v∥2R/8)),\nthus we can conclude that\n(f(w · x) − u∗(w∗ · x))21{x ∈ B}\n=\n\u0000{f(w · x) − u∗(αw · x + ∥v∥2R/4)} + {u∗(αw · x + ∥v∥2R/4) − u∗(w∗ · x)}\n\u000121{x ∈ B}\n≥ (u∗(αw · x + ∥v∥2R/4) − u∗(w∗ · x))21{x ∈ B} ,\nwhere in the last inequality we used that I2(x) = f(w · x) − u∗(αw · x + ∥v∥2R/4) ≥ 0 and\nu∗(αw · x + ∥v∥2R/4) − u∗(w∗ · x) ≥ 0 by the non-decreasing property of u∗, and the elementary\ninequality (a + b)2 ≥ max(a, b)2 for a, b ≥ 0. Further, using u∗(t) − u∗(t′) ≥ a(t − t′) for t ≥ t′ ≥ 0\n(which holds by assumption) and w∗ = αw + v, we have\n(u∗(αw · x + ∥v∥2R/4) − u∗(w∗ · x))21{x ∈ B} ≥ a2(∥v∥2R/4 − v · x)21{x ∈ B}\n≥ a2∥v∥2\n2(R/8)21{x ∈ B} ,\n8\nwhere in the last inequality we used that 0 ≤ ˜v · x ≤ R/8 (by the definition of the event B). A\nvisual illustration of the argument above is given in Figure 1.\nCase 2: f(w · x) ∈ (−∞, u∗(αw · x + ∥v∥2R/32)). Then 0 ≥ I1(x) ≥ I2(x). We follow a similar\nargument as in the previous case. In particular, we begin with\n(f(w · x) − u∗(w∗ · x))21{x ∈ B}\n=\n\u0000{f(w · x) − u∗(αw · x + ∥v∥2R/32)} + {u∗(αw · x + ∥v∥2R/32) − u∗(w∗ · x)}\n\u000121{x ∈ B}.\n(4)\nNote that I1(x) ≤ 0 and u∗(w∗ · x) = u∗(αw · x + ∥v∥2˜v · x) ≥ u∗(αw · x + ∥v∥2R/32) since\n˜v · x ≥ R/16 ≥ R/32 for x ∈ B; thus, the two terms in curly brackets in (4) have the same sign and\nwe further have:\n(f(w · x) − u∗(w∗ · x))21{x ∈ B} ≥ (u∗(αw · x + ∥v∥2R/32) − u∗(w∗ · x))21{x ∈ B}\n≥ a2∥v∥2\n2(R/32)21{x ∈ B} ,\nwhere in the first inequality we used the fact that (a + b)2 ≥ max{a2, b2} when both a, b ≤ 0.\nBy the analysis of Case 1 and Case 2, we can conclude that when I1(x)I2(x) ≥ 0, it must be:\n(f(w · x) − u∗(w∗ · x))21{x ∈ B} ≥ a2∥v∥2\n2R2/2101{x ∈ B} .\n(5)\nCase 3: f(w · x) ∈ (u∗(αw · x + ∥v∥2R), +∞). Then I2(x) ≥ I3(x) ≥ 0 and we choose\nB′ = {x ∈ Rd : w · x ≥ 0, ˜v · x ∈ (3R/8, R/2)} .\nFollowing the same reasoning as in the previous two cases, we have\n(f(w · x) − u∗(w∗ · x))21{x ∈ B′}\n=\n\u0000{f(w · x) − u∗(αw · x + ∥v∥2R)} + {u∗(αw · x + ∥v∥2R) − u∗(w∗ · x)}\n\u000121{x ∈ B′}\n≥ (u∗(αw · x + ∥v∥2R) − u∗(w∗ · x))21{x ∈ B′}\n≥ a2∥v∥2\n2(R/2)21{x ∈ B′} .\nCase 4: f(w · x) ∈ (−∞, u∗(αw · x + ∥v∥2R/4)). Then 0 ≥ I2(x) ≥ I3(x). It follows that\n(f(w · x) − u(w∗ · x))21{x ∈ B′}\n=\n\u0000{f(w · x) − u∗(αw · x + ∥v∥2R/4)} + {u∗(αw · x + ∥v∥2R/4) − u(w∗ · x)}\n\u000121{x ∈ B′}\n≥ a2∥v∥2\n2(R/8)21{x ∈ B′} .\nThus, from the analysis of Case 3 and Case 4, we conclude that when I2(x)I3(x) ≥ 0, we have\n(f(w · x) − u∗(w∗ · x))21{x ∈ B′} ≥ a2∥v∥2\n2(R2/64)1{x ∈ B′} .\n(6)\nRecall that for any x, at least one of the inequalities I1(x)I2(x) ≥ 0 or I2(x)I3(x) ≥ 0 happens, thus,\n1{I1(x)I2(x) ≥ 0} ≥ 1 − 1{I2(x)I3(x) ≥ 0}. Therefore, the probability mass of the region\n(B ∩ {I1(x)I2(x) ≥ 0}) ∪ (B′ ∩ {I2(x)I3(x) ≥ 0})\n9\ncan be bounded below by:\nPr\n\u0014\nx ∈ (B ∩ {I1(x)I2(x) ≥ 0}) ∪ (B′ ∩ {I2(x)I3(x) ≥ 0})\n\u0015\n=\nZ\nV\n\u0012\n1{x ∈ B}1{I1(x)I2(x) ≥ 0} + 1{x ∈ B′}1{I2(x)I3(x) ≥ 0}\n\u0013\nγ(x) dx\n≥\nZ\nV,∥x∥∞≤R\n\u0012\n1{x ∈ B}1{I1(x)I2(x) ≥ 0} + 1{x ∈ B′}1{I2(x)I3(x) ≥ 0}\n\u0013\nL dx\n≥ L\nZ\nV,∥x∥∞≤R\n\u0012\n1{x ∈ B} + (1{x ∈ B′} − 1{x ∈ B})1{I2(x)I3(x) ≥ 0}\n\u0013\ndx ,\n(7)\nwhere in the first inequality we used the assumption that Dx is (L, R)- well-behaved. As a visual\nillustration of the lower bound argument above, the reader is referred to Figure 2.\nFigure 2: On the 2-dimensional space V spanned by (xv, xw), at each point x ∈ B ∪ B′, it must\nbe that I1(x)I2(x) ≥ 0 or I2(x)I3(x) ≥ 0.\nΓ1 denotes the interval of xw = w · x such that\nf(w ·x) ≥ u∗(αw ·x+∥v∥2R), hence both I1(x)I2(x) ≥ 0, I2(x)I3(x) ≥ 0; Γ2 denotes the interval of\nxw such that f(w·x) ∈ (u∗(αw·x+∥v∥2R/32), u∗(αw·x+∥v∥2R/4)), hence I2(x)I3(x) ≥ 0; finally,\nΓ3 denotes the interval of xw such that f(w · x) ∈ (u∗(αw · x + ∥v∥2R/4), u∗(αw · x + ∥v∥2R/)),\nhence I1(x)I2(x) ≥ 0. The area of the union of the red and blue regions is the lower bound on the\nprobability in (7). As displayed in the figure, the sum of the blue and red region is lower bounded by\n1{x ∈ B} + (1{x ∈ B′} − 1{x ∈ B})1{I2(x)I3(x) ≥ 0}.\nTo finish bounding below the probability in (7), it remains to bound the integral from its\nfinal inequality, which now does not involve the probability density function anymore, as we used\nthe anti- concentration property of Dx to uniformly bound below γ(x). Recall that by definition,\nI1(x), I2(x), I3(x) are functions of w · x that do not depend on ˜v · x. Denote the projection of x on\n10\nthe standard basis of space V by x ˜w = ˜w · x and x˜v = ˜v · x. Then, we have:\nZ\nV,∥x∥∞≤R\n\u0012\n1{x ∈ B′} − 1{x ∈ B}\n\u0013\n1{I2(x)I3(x) ≥ 0} dx\n=\nZ\n|x ˜\nw|≤R\nZ\n|x˜v|≤R\n\u0012\n1\n\u001a\nx˜v ∈\n\u00123R\n8 , R\n2\n\u0013\u001b\n− 1\n\u001a\nx˜v ∈\n\u0012 R\n16, R\n8\n\u0013\u001b\u0013\ndx˜v1{x ˜w ≥ 0, I2(x)I3(x) ≥ 0} dx ˜w\n=\nZ\n|x ˜\nw|≤R\n1{x ˜w ≥ 0, I2(x)I3(x) ≥ 0} dx ˜w\nZ\n|x˜v|≤R\n\u0012\n1\n\u001a\nx˜v ∈\n\u00123R\n8 , R\n2\n\u0013\u001b\n− 1\n\u001a\nx˜v ∈\n\u0012 R\n16, R\n8\n\u0013\u001b\u0013\ndx˜v\n≥ 0 .\nPlugging the inequality above back into (7), we get:\nPr\n\u0014\nx ∈\n\u0000B ∩ {I1(x)I2(x) ≥ 0}\n\u0001\n∪\n\u0000B′ ∩ {I2(x)I3(x) ≥ 0}\n\u0001\u0015\n≥ L\nZ\nV,∥x∥∞≤R\n1{w · x ≥ 0, ˜v · x ∈ (R/16, R/8)} dx\n= L\nZZ\n(1{x ˜w ∈ (0, R)} dx ˜w)1{x˜v ∈ (R/16, R/8)} dx˜v = LR2/16 .\n(8)\nWe are now ready to provide a lower bound on the L2\n2 distance between f(w · x) and u∗(w∗ · x).\nCombining the inequalities from (5) and (6), we get\nE\nx∼Dx[(f(w · x) − u∗(w∗ · x))2]\n≥\nE\nx∼Dx[(f(w · xV ) − u∗(w∗ · xV ))21{xV ∈ A}]\n≥ a2(R2/1024)∥v∥2\n2\nE\nx∼Dx[1\n\b\n{xV ∈ B ∩ {I1(x)I2(x) ≥ 0}} ∪ {B′ ∩ {I2(x)I3(x) ≥ 0}}\n\t\n]\n≥ a2(R4/213)L∥v∥2\n2 ,\nwhere we used (8) in the last inequality.\nNow for the case where w · w∗ ≤ 0, it holds w∗ = αw + v with α ≤ 0. Considering instead\nA = {x ∈ Rd : w · x ≤ 0, ˜v · x ∈ (R/16, R/8) ∪ (3R/8, R/2)} and similarly B = {x ∈ Rd : w · x ≤\n0, ˜v · x ∈ (R/16, R/8)}, B′ = {x ∈ Rd : w · x ≤ 0, ˜v · x ∈ (R/3, R/2)}, then all the steps above\nremains valid without modification. This completes the proof of Lemma 3.2.\n3.2\nCloseness of Idealized and Attainable Activations\nIn this section, we bound the contribution of the error incurred from working with attainable link\nfunctions ˆut in the iterations of the algorithm. The error incurred is due to both the arbitrary noise\nin the labels and due to using a finite sample set. In bounding the error, for analysis purposes, we\nintroduce auxiliary population-level link functions.\nConcretely, given w ∈ B(W), a population-optimal activation is a solution to the following\nstochastic convex program:\nuw ∈ argmin\nu∈U(a,b)\nE\n(x,y)∼D[(u(w · x) − y)2].\n(EP)\nWe further introduce auxiliary “idealized, noiseless” activations, which, given noiseless labels y∗ =\nu∗(w∗ · x) and a parameter weight vector w, are defined via\nu∗\nw ∈ argmin\nu∈U(a,b)\nE\n(x,y)∼D[(u(w · x) − y∗)2].\n(EP*)\n11\nBelow we relate ut := uwt and u∗t := u∗\nwt and show that their L2\n2 error for the parameter vector\nwt is bounded by OPT. The proof of Lemma 3.3 is deferred to Appendix C.1.\nLemma 3.3 (Closeness of Population-Optimal Activations). Let wt ∈ B(W) and let u∗t, ut be\ndefined as solutions to (EP*), (EP), respectively. Then,\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤ OPT.\nAs a consequence of the lemma above, we are able to relate ˆut to the “noiseless” labels y∗ =\nu∗(w∗ · x) by showing that the L2\n2 distance between u∗(w∗ · x) and the sample-optimal activation\nˆut(wt·x) is bounded by ∥wt−w∗∥2\n2. Although Corollary 3.4 is not used in the proof of Proposition 3.1,\nwe still present it here as it justifies the mechanism of our approach alternating between updates for\nwt and ˆut. The proof of Corollary 3.4 can be found in Appendix C.2.\nCorollary 3.4 (Closeness of Idealized and Attainable Activations). Let ϵ, δ > 0. Given a parameter\nwt ∈ B(W) and m ≳ d log4(d/(ϵδ))(b2W 3/(L2ϵ))3/2 samples from D, let ˆut be the sample-optimal\nactivation on these samples given wt, as defined in (P). Then, with probability at least 1 − δ,\nE\nx∼Dx[(ˆut(wt · x) − u∗(w∗ · x))2] ≤ 3(ϵ + OPT + b2∥wt − w∗∥2\n2) .\n3.3\nProof of Proposition 3.1\nWe are now ready to prove our main structural result. We focus here on the main argument, while\nthe proofs of supporting technical claims are deferred to Appendix C.\nProof of Proposition 3.1. Given any weight parameter wt ∈ B(W) and ˆut chosen as its corresponding\nsample-optimal solution to problem (P), let ut be the population-optimal activation, as defined by\nProblem (EP). Given a sample set S = {(x(i), y(i))}m\ni=1, consider an idealized, “noise-free” set S∗\nthat assigns realizable labels to data vectors from S, i.e., S∗ = {(x(i), y∗(i))}m\ni=1, y∗(i) = u∗(w∗ · x(i)).\nFurther define idealized sample-optimal activations by\nˆu∗\nw ∈ argmin\nu∈U(a,b)\n1\nm\nm\nX\ni=1\n(u(w · x(i)) − y∗(i))2.\n(P*)\nFor a parameter wt, denote ˆu∗t := ˆu∗\nwt, for simplicity, and recall that the population version of ˆu∗t\nwas defined by (EP*). To prove Proposition 3.1, we decompose ∇ bLsur(wt; ˆut) · (wt − w∗) into three\nsummation terms:\n∇ bLsur(wt; ˆut) · (wt − w∗)\n= 1\nm\nm\nX\ni=1\n(ˆut(wt · x(i)) − y(i))(wt − w∗) · x(i)\n= 1\nm\nm\nX\ni=1\n(ˆut(wt · x(i) − ˆu∗t(wt · x(i)))(wt − w∗) · x(i)\n|\n{z\n}\nQ1\n+ 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt − w∗) · x(i)\n|\n{z\n}\nQ2\n+ 1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i))\n|\n{z\n}\nQ3\n.\n(9)\n12\nWe tackle each term Q1 to Q3 in (9) separately, using the following arguments relying on three\nauxiliary claims. Because the proofs of these claims are technical, we defer them to Appendix C.\nThe first claim states that Q1 is of the order (√ϵ +\n√\nOPT)∥wt − w∗∥2 + (OPT + ϵ)/b with high\nprobability.\nClaim 3.5. Let S = {(x(i), y(i))}m\ni=1 be i.i.d. samples from D where m is as specified in the statement\nof Proposition 3.1. Let ˆut be the solution of optimization problem (P) given wt ∈ B(W) and S.\nFurthermore, denote the idealized version of S by S∗ = {(x(i), y∗(i))}m\ni=1, where y∗(i) = u∗(w∗ · x(i)).\nLet ˆu∗t be the solution of problem (P*). Then, with probability at least 1 − δ,\nQ1 = 1\nm\nm\nX\ni=1\n((ˆut(wt·x(i))−ˆu∗t(wt·x(i)))(wt−w∗)·x(i) ≥ −(√ϵ+\n√\nOPT)∥wt−w∗∥2−(ϵ+OPT)/b .\nThe proof of Claim 3.5 is based on the following argument: first, standard concentration arguments\nensure that ˆut and ˆu∗t are close to their population counterparts, ut and u∗t, in L2\n2 distance (see\nAppendix F). Therefore, applying Chebyshev’s inequality, we are able to swap the sample-optimal\nactivations in (9) by their population-optimal counterparts with high probability and focus on\nbounding\n1\nm\nm\nX\ni=1\n(ut(wt · x(i)) − u∗t(wt · x(i)))(wt − w∗) · x(i) .\nTo bound this quantity, we leverage the result from Lemma 3.3, namely that Ex∼Dx[(ut(wt · x) −\nu∗t(wt · x))2] ≤ OPT.\nThe second claim leverages the misalignment lemma (Lemma 3.2) and shows that, up to small\nerrors, Q2 is a constant multiple of ∥(w∗)⊥wt∥2\n2.\nClaim 3.6. Let S∗ = {(x(i), y∗(i))}m\ni=1 be a sample set such that x(i)’s are i.i.d. samples from Dx\nand y∗(i) = u∗(w∗ · x(i)) for each i. Let m be the value specified in the statement of Proposition 3.1.\nThen, given a parameter wt ∈ B(W), with probability at least 1 − δ,\nQ2 = 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt − w∗) · x(i) ≥ Ca2LR4\nb\n∥(w∗)⊥wt∥2\n2 − √ϵ∥wt − w∗∥2 − ϵ/b ,\nwhere C is an absolute constant.\nThe proof of Claim 3.6 is rather technical. We first define an ‘empirical inverse’ of the activation\nu∗, and denote it by ˆf. Note that u∗(z) ∈ U(a,b) is not necessarily strictly increasing when z ≤ 0,\ntherefore (u∗)−1 is not defined everywhere on R, and the introduction of this ‘empirical inverse’\nfunction ˆf is needed. Then, adding and subtracting ˆf(ˆu∗t(wt · x(i))) in the wt · x(i) − w∗ · x(i) term,\nwe get\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))(wt − w∗) · x(i)\n= 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))(wt · x(i) − ˆf(ˆu∗t(wt · x(i))))\n+ 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))( ˆf(ˆu∗t(wt · x(i))) − w∗ · x(i)) .\n13\nAnalyzing the KKT conditions of the optimization problem (P*), we argue that the first term in\nthe equation above is always positive. Then, we argue that our definition of the empirical inverse\nˆf ensures that the second term can be bounded below by\n1\nbm\nPm\ni=1(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))2.\nUsing standard concentration arguments, the quantity above concentrates around its expectation\nEx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2], hence we complete the proof applying Lemma 3.2.\nSimilar to Claim 3.5, the last claim shows that Q3 is of the order\n√\nOPT∥w∗ − wt∥2, which is\nsmall compared to the positive term in Claim 3.6 outside the set of O(OPT) + ϵ error solutions.\nClaim 3.7. Let S = {(x(i), y(i))}m\ni=1 be i.i.d. samples from D, and denote by S∗ = {(x(i), y∗(i))}m\ni=1\nthe idealized version of S, where y∗(i) = u∗(w∗ · x(i)). Under the condition of Proposition 3.1, given\na parameter wt ∈ B(W), with probability at least 1 − δ,\nQ3 = 1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i)) ≥ −\n√\nOPT∥w∗ − wt∥2 − (OPT + ϵ)/b .\nThe proof of Claim 3.7 follows via similar arguments as the proof of Claim 3.5.\nPlugging the bounds from Claim 3.5, Claim 3.6, and Claim 3.7 back into (9) and using a union\nbound, we get that with probability at least 1 − 3δ,\n∇ bLsur(wt; ˆut) · (wt − w∗) ≥ Ca2LR4\nb\n∥(w∗)⊥wt∥2\n2 − 2(\n√\nOPT + √ϵ)∥wt − w∗∥2 − 2(OPT + ϵ)/b,\nfor some absolute constant C, completing the proof.\n4\nRobust SIM Learning via Alignment Sharpness\nAs discussed in Section 1.2, our algorithm can be viewed as employing an alternating procedure:\ntaking a Riemannian gradient descent step on a sphere with respect to the empirical surrogate,\ngiven an estimate of the activation, and optimizing the activation function on the sample set for\na given parameter weight vector. This procedure is performed using a fine grid of guesses of the\nscale of ∥w∗∥2. For this process to converge with the desired linear rate (even for a known value\nof ∥w∗∥2), the algorithm needs to be properly initialized to ensure that the initial weight vector\nhas a nontrivial alignment with the optimal vector w∗. The initialization process is handled in the\nfollowing subsection.\n4.1\nInitialization\nWe begin by showing that the Initialization subroutine stated in Algorithm 1 returns a point ¯w0\nthat has a sufficient alignment with w∗. As will become apparent later in the proof of Theorem 4.2,\nthis property of the initial point is critical for Algorithm 2 to converge at a linear rate.\nLemma 4.1 (Initialization). Let µ = Ca2LR4/b for an absolute constant C > 0 and let ϵ, δ > 0.\nChoose the step size η = µ3/(27b4) in Algorithm 1. Then, drawing m0 i.i.d. samples from D at each\niteration such that\nm0 ≳ W 9/2b10d log4(d/(ϵδ))\nL4µ6δϵ3/2\n,\nensures that within t0 ≲ b6 log(b/µ)/µ6 iterations, the initialization subroutine Algorithm 1 generates a\nlist of size t0 that contains a point ¯w0 such that ∥(w∗)⊥ ¯\nw0∥2 ≤ max{µ∥w∗∥2/(4b), 64b2/µ3(\n√\nOPT +\n√ϵ)}, with probability at least 1 − δ. The total number of samples required for Algorithm 1 is\nN0 = t0m0.\n14\nAlgorithm 1 Initialization\n1: Input: w0 = 0; ϵ, δ > 0; positive parameters a, b, L, R, W; µ ≲ a2LR4/b, step size η = µ3/(27b4),\nnumber of iterations t0 ≲ (b/µ)6 log(b/µ);\n2: for t = 0 to t0 do\n3:\nDraw m0 ≳ W 9/2b10d log4(d/(ϵδ))/(L4µ6δϵ3/2) i.i.d. samples from D\n4:\nˆut = argmin\nu∈U(a,b)\n1\nm0\nm0\nP\ni=1\n(u(wt · x(i)) − y(i))2.\n5:\n∇ bLsur(wt; ˆut) =\n1\nm0\nm0\nP\ni=1\n(ˆut(wt · x(i)) − y(i))x(i).\n6:\nwt+1 = wt − η∇ bLsur(wt; ˆut).\n7: end for\n8: Return: {w0, . . . , wt0}\nProof. Consider first the case that ∥w∗∥2 ≤ 64b2/µ3(\n√\nOPT + √ϵ). Then, for the parameter vector\nw0 = 0, we have\n∥(w∗)⊥w0∥2 = ∥w∗∥2 ≤ 64b2/µ3(\n√\nOPT + √ϵ)\nand the claimed statement holds trivially.\nThus, in the rest of the proof we assume ∥w∗∥2 ≥ 64b2/µ3(\n√\nOPT + √ϵ). Let vt denote the\ncomponent of w∗ that is orthogonal to wt; i.e., vt = w∗ − (w∗ · wt)wt/∥wt∥2\n2 = (w∗)⊥wt, where\nwt is defined in Algorithm 1. Our goal is to show that when ∥vt∥2 ≥ µ∥w∗∥2/(4b) at iteration\nt, the distance between wt+1 and w∗ contracts by a constant factor 1 − c for some c < 1, i.e.,\n∥wt+1 − w∗∥2 ≤ (1 − c)∥wt − w∗∥2. This implies that when ∥vt∥2 is greater than µ∥w∗∥2/(4b),\n∥wt+1 − wt∥2 contracts until ∥vt∥2 ≥ µ∥w∗∥2/(4b) is violated at step t0; this wt0 is exactly the\ninitial point we are seeking to initialize the optimization subroutine.\nApplying Proposition 3.1, we get that under our choice of batch size m, with probability at least\n1 − δ, at each iteration it holds\n∇ bLsur(wt; ˆut) · (wt − w∗) ≥ Ca2LR4\nb\n∥(w∗)⊥wt∥2\n2 − 2(\n√\nOPT + √ϵ)∥wt − w∗∥2 − 2(OPT + ϵ)/b .\nWe now study the distance between wt+1 and w∗, where wt+1 is updated from wt according to\nAlgorithm 1.\n∥wt+1 − w∗∥2\n2 = ∥wt − η∇ bLsur(wt; ˆut) − w∗∥2\n2\n= ∥wt − w∗∥2\n2 + η2∥∇ bLsur(wt; ˆut)∥2\n2 − 2η∇ bLsur(wt; ˆut) · (wt − w∗) .\n(10)\nApplying Lemma 4.3 to (10), and plugging in Proposition 3.1, we get that under our choice of batch\nsize m it holds that with probability at least 1 − δ,\n∥wt+1 − w∗∥2\n2 ≤ ∥wt − w∗∥2\n2 + η2(10(OPT + ϵ) + 4b2∥wt − w∗∥2\n2)\n+ 2η(2(OPT + ϵ)/b + 2(\n√\nOPT + √ϵ)∥wt − w∗∥2 − µ∥vt∥2\n2)\n≤ (1 + 4b2η2)∥wt − w∗∥2\n2 + 2η(2(\n√\nOPT + √ϵ)∥wt − w∗∥2 − µ∥vt∥2\n2)\n+ 5η(OPT + ϵ) ,\n(11)\nwhere µ = Ca2LR4/b and C is an absolute constant. Note that in the last inequality we used that\nη ≤ 1/10, hence 10η2 ≤ η, and that b ≥ 1.\n15\nWhen t = 0, v0 = w∗, hence we have ∥v0∥2 ≥ µ∥w∗∥2/(4b). Suppose that at iteration t,\n∥vt∥2 ≥ µ∥w∗∥2/(4b) is still valid. Then, (11) is transformed to:\n∥wt+1 − w∗∥2\n2 ≤ (1 + 4b2η2)∥wt − w∗∥2\n2 + 5η(OPT + ϵ)\n+ 2η((µ3/(32b2))∥wt − w∗∥2∥w∗∥2 − (µ3/(16b2))∥w∗∥2\n2) .\n(12)\nWe use an inductive argument to show that at iteration t, ∥wt−w∗∥2 ≤ ∥w∗∥2, which must eventually\nyield a contraction ∥wt+1 − w∗∥2\n2 ≤ (1 − c)∥wt − w∗∥2\n2 for some constant c < 1. This condition\n∥wt − w∗∥2 ≤ ∥w∗∥2 certainly holds for the base case t = 0 as w0 = 0, hence ∥w0 − w∗∥2 = ∥w∗∥2.\nNow, suppose ∥wt − w∗∥2 ≤ ∥w∗∥2 holds for all the iterations from 0 to t.\nThen, plugging\nη = µ3/(27b4) into (12), we get:\n∥wt+1 − w∗∥2\n2 ≤ (1 + 4b2η2)∥wt − w∗∥2\n2 + 2η((µ3/(32b2)) − (µ3/(16b2)))∥wt − w∗∥2∥w∗∥2 + 5η(OPT + ϵ)\n≤ (1 + 4η2b2 − 2ηµ3/(32b2))∥wt − w∗∥2\n2 + 5µ3/(27b4)(OPT + ϵ)\n≤ (1 − µ6/(211b6))∥wt − w∗∥2\n2 + 5µ3/(27b4)(OPT + ϵ) .\nSince we have assumed\n√\nOPT+√ϵ ≤ µ3/(64b2)∥w∗∥2, it holds ∥wt−w∗∥2 ≥ ∥vt∥2 ≥ µ∥w∗∥2/(4b) ≥\n(16b/µ2)(\n√\nOPT + √ϵ), thus, we have (noting that µ ≤ 1):\n5µ3/(27b4)(OPT + ϵ) ≤ 5µ3/(27b4)(\n√\nOPT + √ϵ)2 ≤ µ6/(212b6)∥wt − w∗∥2 .\nTherefore, combining the results above, we get:\n∥wt+1 − w∗∥2\n2 ≤ (1 − µ6/(212b6))∥wt − w∗∥2\n2 ,\nfor any iteration t such that ∥vt∥2 ≥ µ∥w∗∥2/(4b) holds. This validates the induction argument\nthat ∥wt − w∗∥2 ≤ ∥w∗∥2 for every t = 0, . . . , t0 and at the same time yields the desired contraction\nproperty of the sequence ∥wt−w∗∥2, t = 0, . . . , t0. Now, since ∥w0−w∗∥2 = ∥w∗∥2 and ∥wt−w∗∥2 ≥\n∥vt∥2, we have\n∥vt+1∥2\n2 ≤ (1 − µ6/(212b6))t∥w∗∥2\n2 ≤ exp(−tµ6/(212b6))∥w∗∥2\n2 .\nThus, after at most t0 = 212b6 log(4b/µ)/µ6 iterations, it must hold that among all those vectors\nv1, . . . , vt0, there exists a vector vt∗\n0 such that ∥vt∗\n0∥2 ≤ µ∥w∗∥2/(4b). Since there are only a constant\nnumber of candidates, we can feed each one as the initialized input to the optimization subroutine\nAlgorithm 2. This will only result in a constant factor increase in the runtime and sample complexity.\nFinally, recall that we need to draw\nm ≳ W 9/2b4 log4(d/(ϵδ))\nL4\n\u0012 1\nϵ3/2 + 1\nϵδ\n\u0013\nnew samples at each iteration for (11) to hold with probability 1 − δ, and the total number of\niterations is t0. Thus, applying a union bound, we know that the probability that (11) holds for all\nt0 is 1 − t0δ. Hence, choosing δ ← δt0, and noting that t0 ≈ b6/µ6 log(b/µ), it follows that setting\nthe batch size to be\nm0 = Θ\n\u0012W 9/2b4 log4(d/(ϵδ))\nL4\n\u0012 1\nϵ3/2 + b6 log(b/µ)\nµ6ϵδ\n\u0013\u0013\n= Θ\n\u0012W 9/2b10d log4(d/(ϵδ))\nL4µ6δϵ3/2\n\u0013\n,\nsuffices and the total number of samples required for the initialization process is t0m0.\n16\n4.2\nOptimization\nOur main optimization algorithm is summarized in Algorithm 2 (see Algorithm 4 for a more\ndetailed version). We now provide intuition for how guessing the value of ∥w∗∥2 is used in the\nconvergence analysis. Let wt = ∥w∗∥2 ¯wt/∥ ¯wt∥2 so that ∥wt∥2 = ∥w∗∥2 and let vt := (w∗)⊥wt.\nObserve that ∥vt∥2 = ∥wt − w∗∥2 cos(θ(wt, w∗)/2). Applying Proposition 3.1, it can be shown that\n∥ ¯wt+1 − w∗∥2\n2 ≤ ∥wt − w∗∥2\n2 − C∥vt∥2\n2 for some constant C. Thus, as long as the angle between wt\nand w∗ is not too large (ensured by initialization), ∥wt − w∗∥2 ≈ ∥vt∥2. Hence, we can argue that\n∥wt − w∗∥2 contracts in each iteration, by observing that ∥wt − w∗∥2\n2 ≈ ∥vt+1∥2\n2 ≤ ∥ ¯wt+1 − w∗∥2\n2.\nAlgorithm 2 Optimization\n1: Input: wini = 0; ϵ > 0; positive parameters: a, b, L, R, W, µ; step size η\n2: {wini\n0 , . . . , wini\nt0 } = Initialization[wini] (Algorithm 1)\n3: P = {(w = 0; u(z) = 0)}\n4: for k = 0 to t0 ≲ (b/µ)6 log(b/µ) do\n5:\nfor j = 1 to J = W/(η√ϵ) do\n6:\n¯w0\nj,k = wini\nk , βj = jη√ϵ\n7:\nfor t = 0 to T = O((b/µ)2 log(1/ϵ)) do\n8:\nbwt\nj,k = βj( ¯wt\nj,k/∥ ¯wt\nj,k∥2)\n9:\nDraw m = ˜ΘW,b,1/L,1/µ(d/ϵ3/2) new samples\n10:\nˆut\nj,k = argmin\nu∈U(a,b)\n1\nm\nm\nP\ni=1\n(u(bwt\nj,k · x(i)) − y(i))2\n11:\n¯wt+1\nj,k = bwt\nj,k − η∇ bLsur(bwt\nj,k; ˆut\nj,k)\n12:\nend for\n13:\nP ← P ∪ {(bwT\nj,k; ˆuT\nj,k)}\n14:\nend for\n15: end for\n16: (bw; ˆu) = Test[(w; u) ∈ P] (Algorithm 3)\n17: Return: (bw; ˆu)\nOur main result is the following theorem (see Theorem D.1 for a more detailed statement and\nproof in Appendix D.1):\nTheorem 4.2 (Main Result). Let D be a distribution in Rd × R and suppose that Dx is (L, R)-\nwell-behaved.\nLet U(a,b) be as in Definition 1.3 and let ϵ > 0.\nThen, Algorithm 2 uses N =\n˜OW,b,1/L,1/µ(d/ϵ2) samples, it runs for ˜OW,b,1/µ(1/√ϵ) iterations, and, with probability at least 2/3, re-\nturns a hypothesis (ˆu, bw), where ˆu ∈ U(a,b) and bw ∈ B(W), such that L2(bw; ˆu) = O1/L,1/R,b/a(OPT)+\nϵ .\nTo prove Theorem 4.2, we make use of two technical results stated below. First, Lemma 4.3\nprovides an upper bound on the norm of the empirical gradient of the surrogate loss. The proof of\nthe lemma relies on concentration properties of (L, R)-well behaved distributions Dx, and leverages\nthe uniform convergence of the empirically-optimal activations ˆut. A more detailed statement\n(Lemma D.5) and the proof of Lemma 4.3 is deferred to Appendix D.2.\nLemma 4.3 (Bound on Empirical Gradient Norm). Let S be a set of i.i.d. samples from D of size\nm = ˜ΘW,b,1/L(d/ϵ3/2 + d/(ϵδ)). Given any wt ∈ B(W), let ˆut ∈ U(a,b) be the solution of optimization\nproblem (P) with respect to wt and sample set S. Then, with probability at least 1 − δ,\n∥∇ bLsur(wt; ˆut)∥2\n2 ≤ 4b2∥wt − w∗∥2\n2 + 10(OPT + ϵ) .\n17\nThe following claim bounds the L2\n2 error of a hypothesis ˆuw(w · x) by the distance between w\nand w∗. We defer a more detailed statement (Claim D.6) and the proof to Appendix D.3.\nClaim 4.4. Let w ∈ B(W) be any fixed vector. Let ˆuw be defined by (P) given w and a sample set\nof size m = ˜ΘW,b,1/L(d/ϵ3/2). Then, E(x,y)∼D[(ˆuw(w · x) − y)2] ≤ 8(OPT + ϵ) + 4b2∥w − w∗∥2\n2.\nProof Sketch of Theorem 4.2. For this sketch, we consider the case ∥w∗∥2 ≳ b3/µ4(\n√\nOPT + √ϵ)\nso that the initialization subroutine generates a point wini\nk∗ ∈ {wini\nk }t0\nk=1 such that ∥(w∗)\n⊥wini\nk∗ ∥2 ≤\nµ∥w∗∥2/(4b), by Lemma 4.1. Fix this initialized parameter ¯w0\nj,k∗ = wini\nk∗ at step k∗ and drop the\nsubscript k∗ for simplicity. Since we constructed a grid with width η√ϵ, there exists an index j∗\nsuch that |βj∗ − ∥w∗∥2| ≤ η√ϵ. We consider the intermediate for-loop at this iteration j∗, and show\nthat the inner loop with normalization factor βj∗ outputs a solution with error O(OPT) + ϵ. This\nsolution can be selected using standard testing procedures. We now focus on the iteration j∗, and\ndrop the subscript j∗ for notational simplicity.\nLet wt = ∥w∗∥2( ¯wt/∥ ¯wt∥2) and denote vt := (w∗)⊥ b\nwt. Expanding ∥ ¯wt+1 − w∗∥2\n2 and applying\nProposition 3.1 and Lemma 4.3, we get\n∥ ¯wt+1 − w∗∥2\n2 = ∥bwt − η∇ bLsur(bwt; ˆut) − w∗∥2\n2\n= ∥bwt − w∗∥2\n2 + η2∥∇ bLsur(bwt; ˆut)∥2\n2 − 2η∇ bLsur(bwt; ˆut) · (bwt − w∗)\n≤ ∥bwt − w∗∥2\n2 + η2(10(OPT + ϵ) + 4b2∥bwt − w∗∥2\n2)\n+ 2η(2(\n√\nOPT + √ϵ)∥bwt − w∗∥2 − µ∥vt∥2\n2) + 4η(OPT + ϵ)/b\n≤ (1 + 4η2b2)∥wt − w∗∥2\n2 + (24η2 + 4η/b)(OPT + ϵ)\n+ 2η(2(\n√\nOPT + √ϵ)∥wt − w∗∥2 − µ∥vt∥2\n2),\n(13)\nwhere in the last inequality we used ∥bwt − wt∥2 = |βj∗ − ∥w∗∥2| ≤ η√ϵ.\nSince wt and w∗ are on the same sphere, ∥wt − w∗∥2 ≤ ∥vt∥2. In particular, letting ρt =\n∥vt∥2/∥w∗∥2, we have ∥wt − w∗∥2\n2 ≤ (1 + ρ2\nt )∥vt∥2\n2 ≤ 2∥vt∥2\n2. Recall that the algorithm is initialized\nfrom ¯w0 that satisfies ρ0 ≤ µ/(4b). If ρt ≤ µ/(4b), then ∥wt − w∗∥2\n2 ≤ (1 + (µ/(4b))2)∥vt∥2\n2.\nAssuming in addition that ∥vt∥2 ≳ (1/µ)(\n√\nOPT + √ϵ), and choosing the step-size η = µ/(4b2), (13)\nimplies that\n∥vt+1∥2\n2 ≤ ∥ ¯wt+1 − w∗∥2\n2 ≤ (1 − µ2/(32b2))∥vt∥2\n2 ,\nand thus, in addition, ρt+1 ≤ µ/(4b). Therefore, by an inductive argument, we show that as long as\nwt is still far from w∗, i.e., ∥vt∥2 ≳ (1/µ)(\n√\nOPT + √ϵ), we have\n∥vt+1∥2\n2 ≤ (1 − µ2/(32b2))∥vt∥2\n2 and ρt+1 ≤ µ/(4b) .\nHence, after T = O((b2/µ2) log(1/ϵ)) iterations, it must be ∥vT ∥2 ≲ (1/µ)(\n√\nOPT + √ϵ), which\nimplies\n∥wT − w∗∥2\n2 ≤ 2∥vT ∥2\n2 = O(OPT) + ϵ .\nFinally, by Claim 4.4, hypothesis ˆuT (bwT · x) achieves L2\n2-error O(OPT) + ϵ, which completes the\nproof.\n4.3\nTesting\nWe now briefly discuss the testing procedure, which allows our algorithm to select a hypothesis\nwith minimum empirical error while maintaining validity of the claims. This part relies on standard\narguments and is provided for completeness. Concretely, we rely on the following claim, whose proof\ncan be found in Appendix D.4.\n18\nAlgorithm 3 Testing\n1: Input:\nϵ\n> 0; positive parameters:\na, b, L, R, W; list of solutions P; let r\n≳\n1\nL log(bW/(Lϵ) log2(1/ϵ))\n2: Draw m′ ≳ (bW/L)4 log5(1/ϵ)/ϵ2 new i.i.d. samples from D.\n3: (bw; ˆu) = argmin(w;u)∈P{ 1\nm′\nPm′\ni=1(u(w · x(i)) − y(i))21{|w · x(i)| ≤ Wr} }.\n4: Return: (bw; ˆu)\nClaim 4.5. Let µ, ϵ1, δ ∈ (0, 1) be fixed. Let r = 1\nL log( Cb4W 4\nL6ϵ2\n1 log2( bW\nϵ1 )), where C is a sufficiently\nlarge absolute constant. Given a set of parameter-activation pairs P = {(wj; uj)}t0J\nj=1 such that\nwj ∈ B(W) and uj ∈ U(a,b) for j ∈ [t0J], where t0J = 4b9W/(µ8√ϵ1), we have that using\nm′ = Θ\n\u0012b4W 4 log(1/δ)\nL4ϵ2\n1\nlog5\n\u0012 bW\nLµϵ1\n\u0013\u0013\n,\ni.i.d. samples from D, for any (wj; uj) ∈ P it holds with probability at least 1 − δ,\n\f\f\f\f\n1\nm′\nm′\nX\ni=1\n(uj(wj · x(i)) − y(i))21{|wj · x(i)| ≤ Wr} −\nE\n(x,y)∼D[(uj(wj · x) − y)2]\n\f\f\f\f ≤ 2ϵ1.\nTherefore, Claim 4.5 guarantees that selecting a hypothesis using the provided testing procedure\nintroduces an error at most 2ϵ1, with high probability.\n5\nConclusion\nWe presented the first constant-factor approximate SIM learner in the agnostic model, for the class\nof (a, b)-unbounded link functions under mild distributional assumptions. Immediate questions for\nfuture research involve extending these results to other classes of link functions. More specifically, our\nresults require that b/a is bounded by a constant. It is an open question whether the constant-factor\napproximation result in the agnostic model can be extended to all b-Lipschitz functions (with a = 0).\nThis question is open in full generality, even when the link function is known to the learner.\nReferences\n[ATV23]\nP. Awasthi, A. Tang, and A. Vijayaraghavan.\nAgnostic learning of general ReLU\nactivation using gradient descent. In The Eleventh International Conference on Learning\nRepresentations, ICLR, 2023.\n[BNPS17]\nJ. Bolte, T. P. Nguyen, J. Peypouquet, and B. W. Suter.\nFrom error bounds to\nthe complexity of first-order descent methods for convex functions.\nMathematical\nProgramming, 165(2):471–507, 2017.\n[BNS16]\nS. Bhojanapalli, B. Neyshabur, and N. Srebro. Global optimality of local search for low\nrank matrix recovery. Advances in Neural Information Processing Systems, 29, 2016.\n[DGK+20] I. Diakonikolas, S. Goel, S. Karmalkar, A. R. Klivans, and M. Soltanolkotabi. Approxi-\nmation schemes for ReLU regression. In Conference on Learning Theory, COLT, volume\n125 of Proceedings of Machine Learning Research, pages 1452–1485. PMLR, 2020.\n19\n[DH18]\nR. Dudeja and D. Hsu. Learning single-index models in Gaussian space. In Conference\non Learning Theory, COLT, volume 75 of Proceedings of Machine Learning Research,\npages 1887–1930. PMLR, 2018.\n[DJS08]\nA. S. Dalalyan, A. Juditsky, and V. Spokoiny. A new algorithm for estimating the\neffective dimension-reduction subspace. The Journal of Machine Learning Research,\n9:1647–1678, 2008.\n[DKMR22] I. Diakonikolas, D. Kane, P. Manurangsi, and L. Ren. Hardness of learning a single\nneuron with adversarial label noise. In Proceedings of the 25th International Conference\non Artificial Intelligence and Statistics (AISTATS), 2022.\n[DKPZ21]\nI. Diakonikolas, D. M. Kane, T. Pittas, and N. Zarifis. The optimality of polynomial\nregression for agnostic learning under Gaussian marginals in the SQ model. In Proceedings\nof The 34th Conference on Learning Theory, COLT, 2021.\n[DKR23]\nI. Diakonikolas, D. M. Kane, and L. Ren. Near-optimal cryptographic hardness of\nagnostically learning halfspaces and ReLU regression under Gaussian marginals. In\nICML, 2023.\n[DKTZ20]\nI. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning halfspaces with\nmassart noise under structured distributions. In Conference on Learning Theory, COLT,\n2020.\n[DKTZ22]\nI. Diakonikolas, V. Kontonis, C. Tzamos, and N. Zarifis. Learning a single neuron with\nadversarial label noise via gradient descent. In Conference on Learning Theory (COLT),\npages 4313–4361, 2022.\n[DKZ20]\nI. Diakonikolas, D. M. Kane, and N. Zarifis. Near-optimal SQ lower bounds for agnosti-\ncally learning halfspaces and ReLUs under Gaussian marginals. In Advances in Neural\nInformation Processing Systems, NeurIPS, 2020.\n[FCG20]\nS. Frei, Y. Cao, and Q. Gu. Agnostic learning of a single neuron with gradient descent.\nIn Advances in Neural Information Processing Systems, NeurIPS, 2020.\n[FP03]\nF. Facchinei and J-S. Pang. Finite-dimensional variational inequalities and complemen-\ntarity problems. Springer, 2003.\n[GGK20]\nS. Goel, A. Gollakota, and A. R. Klivans. Statistical-query lower bounds via functional\ngradients. In Advances in Neural Information Processing Systems, NeurIPS, 2020.\n[GGKS23] A. Gollakota, P. Gopalan, A. R. Klivans, and K. Stavropoulos. Agnostically learning\nsingle-index models using omnipredictors. In Thirty-seventh Conference on Neural\nInformation Processing Systems, 2023.\n[Hau92]\nD. Haussler. Decision theoretic generalizations of the PAC model for neural net and\nother learning applications. Information and Computation, 100:78–150, 1992.\n[HJS01]\nM. Hristache, A. Juditsky, and V. Spokoiny. Direct estimation of the index coefficient in\na single-index model. Annals of Statistics, pages 595–623, 2001.\n[HMS+04]\nW. Härdle, M. Müller, S. Sperlich, A. Werwatz, et al. Nonparametric and semiparametric\nmodels, volume 1. Springer, 2004.\n20\n[Hof52]\nA. J. Hoffman. On approximate solutions of systems of linear inequalities. Journal of\nResearch of the National Bureau of Standards, 49:263–265, 1952.\n[Ich93]\nH. Ichimura.\nSemiparametric least squares (SLS) and weighted SLS estimation of\nsingle-index models. Journal of econometrics, 58(1-2):71–120, 1993.\n[JGN+17]\nC. Jin, R. Ge, P. Netrapalli, S. Kakade, and M. Jordan. How to escape saddle points\nefficiently. In International conference on machine learning, pages 1724–1732. PMLR,\n2017.\n[KKSK11] S. M Kakade, V. Kanade, O. Shamir, and A. Kalai. Efficient learning of generalized\nlinear and single index models with isotonic regression. Advances in Neural Information\nProcessing Systems, 24, 2011.\n[KNS16]\nH. Karimi, J. Nutini, and M. Schmidt. Linear convergence of gradient and proximal-\ngradient methods under the Polyak-łojasiewicz condition. In Joint European conference\non machine learning and knowledge discovery in databases, pages 795–811, 2016.\n[KS09]\nA. T. Kalai and R. Sastry. The isotron algorithm: High-dimensional isotonic regression.\nIn COLT, 2009.\n[KSS94]\nM. Kearns, R. Schapire, and L. Sellie. Toward efficient agnostic learning. Machine\nLearning, 17(2/3):115–141, 1994.\n[LCP22]\nJ. Liu, Y. Cui, and J-S. Pang. Solving nonsmooth and nonconvex compound stochastic\nprograms with applications to risk measure minimization. Mathematics of Operations\nResearch, 2022.\n[LH22]\nC. Lu and D. S. Hochbaum. A unified approach for a 1D generalized total variation\nproblem. Mathematical Programming, 194(1-2):415–442, 2022.\n[Łoj63]\nS. Łojasiewicz. Une propriété topologique des sous-ensembles analytiques réels. Les\néquations aux dérivées partielles, 117:87–89, 1963.\n[Łoj93]\nS. Łojasiewicz. Sur la géométrie semi-et sous-analytique. In Annales de l’institut Fourier,\nvolume 43, pages 1575–1595, 1993.\n[MR18]\nP. Manurangsi and D. Reichman. The computational complexity of training ReLU(s).\narXiv preprint arXiv:1810.04207, 2018.\n[Rd17]\nV. Roulet and A. d’Aspremont. Sharpness, restart and acceleration. Advances in Neural\nInformation Processing Systems, 30, 2017.\n[Sím02]\nJ. Síma. Training a single sigmoidal neuron is hard. Neural Computation, 14(11):2709–\n2728, 2002.\n[WZDD23] P. Wang, N. Zarifis, I. Diakonikolas, and J. Diakonikolas. Robustly learning a single\nneuron via sharpness. 40th International Conference on Machine Learning, 2023.\n[ZL16]\nQ. Zheng and J. Lafferty. Convergence analysis for rectangular matrix completion using\nBurer-Monteiro factorization and gradient descent. arXiv preprint arXiv:1605.07051,\n2016.\n[ZY13]\nH. Zhang and W. Yin. Gradient methods for convex minimization: better rates under\nweaker conditions. arXiv preprint arXiv:1303.4645, 2013.\n21\nAppendix\nOrganization\nThe appendix is organized as follows. In Appendix A, we highlight some useful\nproperties about the distribution class and the activation class. Appendix B reviews local error\nbounds and discussed their relation to our alignment sharpness structural result. In Appendix C,\nwe provide detailed proofs omitted from Section 3, and in Appendix D we complete the proofs\nomitted from Section 4. In Appendix E, we provide a detailed discussion about computing the\nsample-optimal activation. Finally, in Appendix F we state and prove standard uniform convergence\nresults that are used throughout the paper.\nA\nRemarks about the Distribution Class and the Activation Class\nIn this section, we show that without the loss of generality we can assume that the parameters L, R\nin the distributional assumptions (Definition 1.2) can be taken less than 1, while the parameters a, b\nof the activations functions (see Definition 1.3) can be taken as a, 1/b ≤ 1.\nRemark A.1 (Distribution/Activation Parameters, (Definition 1.2 & Definition 1.3)). We observe\nthat if a distribution Dx is (L, R)-well-behaved, then it is also (L′, R′)-well-behaved for any 0 < L′ ≤\nL, 0 < R′ ≤ R. Hence, it is without loss of generality to assume that L, R ∈ (0, 1]. Similarly, if an\nactivation is (a, b)-unbounded, it is also an (a, b′)-unbounded activation with b′ ≥ b. Thus, we assume\nthat b ≥ 1. We can similarly assume a ≤ 1.\nIn addition, we remark that the (L, R)-well behaved distributions are sub-exponential.\nRemark A.2 (Sub-exponential Tails of Well-Behaved Distributions, Definition 1.2). Definition 1.2\nmight seem abstract, but to put it plain it implies that the random variable x has a (1/L)-sub-\nexponential tail, and that the pdf of the projected random variable xV onto the space V is lower\nbounded by L. To see the first statement, given any unit vector p, let xp be the projection of x onto\nthe one-dimensional linear space Vp = {z ∈ Rd : z = tp, t ∈ R}, i.e., xp = p · x ∈ Vp. Then, by the\nanti-concentration and concentration property, we have\nPr[|p · x| ≥ r] = Pr[|xp| ≥ r] ≤\nZ\n|x|≥r\nγ(x) dx ≤ 2\nZ ∞\nr\n1\nL exp(−Lx) dx = 2\nL2 exp(−Lr),\nwhich implies that x possesses a sub-exponential tail.\nB\nLocal Error Bounds and Alignment Sharpness\nGiven a generic optimization problem minw f(w) and a non-negative residual function r(w) measuring\nthe approximation error of the optimization problem, we say that the problem satisfies a local error\nbound if in some neighborhood of “test” (typically optimal) solutions W∗ we have that\nr(w) ≥ (µ/ν) dist(w, W∗)ν.\n(14)\nIn other words, low value of the residual function implies that w must be close to the test set W∗.\nLocal error bounds have been studied in the optimization literature for decades, starting with the\nseminal works of [Hof52, Łoj63]; see, e.g., Chapter 6 in [FP03] for an overview of classical results and\n[BNPS17, KNS16, Rd17, LCP22] and references therein for a more cotemporary overview. While\nlocal error bounds can be shown to hold generically under fairly minimal assumptions on f and for\n22\nr(w) = f(w) − minw′ f(w′) [Łoj63, Łoj93], it is rarely the case that they can be ensured to hold\nwith a parameter µ that is not trivially small.\nOn the other hand, learning problems often possess very strong structural properties that can\nlead to stronger local error bounds. There are two main such examples we are aware of, where local\nerror bounds can be shown to hold with ν = 2 and an absolute constant µ > 0. The first example\nare low-rank matrix problems such as matrix completion and matrix sensing, which are unrelated\nto our work [BNS16, ZL16, JGN+17]. More relevant to our work is the recent result in [WZDD23],\nwhich proved a local error bound of the form\nr(w) ≥ µ\n2 dist(w, W∗)2\n(15)\nfor the more restricted problem than ours (with a known activation function) but under somewhat\nmore general distributional assumptions.\nIn [WZDD23], the residual function was defined by\nr(wt) = ∇ bLsur(wt; u∗) · (wt − w∗), where ∇ bLsur(wt; u∗) is the gradient of an empirical surrogate\nloss, and the resulting local error bound referred to as “sharpness.”2\nOur structural result can be seen as a weak notion of a local error bound, where the residual\nfunction for the empirical surrogate loss expressed as r(wt, ˆut) = ∇ bLsur(wt; ˆut)·(wt−w∗) is bounded\nbelow as a function of the magnitude of the component of w∗ that is orthogonal to wt. Compared to\nmore traditional local error bounds and the bound from [WZDD23], which bound below the residual\nerror function as a function of the distance to W∗, this is a much weaker local error bound since it\ndoes not distinguish between vectors of varying magnitudes along the direction of w∗. Since our lower\nbound is related to the “sharpness” notion studied in [WZDD23], we refer to it as the “alignment\nsharpness” to emphasize that it only relates the misalignment (as opposed to the distance) of vectors\nwt and w∗ to the residual error. To the best of our knowledge, such a form of a local error bound,\nwhich only bounds the alignment of vectors as opposed to their distance, is novel. We expect it to\nfind a more broader use in learning theory and optimization.\nC\nOmitted Proofs from Section 3\nThis section provides full technical details for results omitted from Section 3.\nC.1\nProof of Lemma 3.3\nTo prove Lemma 3.3, we first prove the following auxiliary claim, which is inspired by [KKSK11,\nLemma 9].\nClaim C.1. Let wt ∈ B(W) and let u∗t, ut be defined as solutions to (EP*), (EP), respectively.\nThen,\nE\n(x,y)∼D[(ut(wt · x) − v(wt · x))(y − ut(wt · x))] ≥ 0,\n∀ ∈ U(a,b).\nSimilarly,\nE\n(x,y)∼D[(u∗t(wt · x) − v′(wt · x))(y∗ − u∗t(wt · x))] ≥ 0,\n∀v′ ∈ U(a,b).\nProof of Claim C.1. Denote by Ft the set of functions of the form f(x) = u(wt · x), where u ∈ U(a,b)\nand wt is a fixed vector in B(W). We first argue that Ft is a convex set, using the definition of\n2A local error utilizing the same type of a residual was introduced in [ZY13] under the name “restricted secant\ninequality.”\n23\nconvexity. In particular, for any α ∈ (0, 1) and any f1, f2 ∈ Ft such that f1(x) = u1(wt · x), f2 =\nu2(wt · x), let u3(·) = αu1(·) + (1 − α)u2(·). Then:\nαf1(x) + (1 − α)f2(x) = αu1(wt · x) + (1 − α)u2(wt · x) = u3(wt · x).\nIt is immediate that u3 is also (a, b)-bounded, non-decreasing, and u3(0) = 0, hence u3 ∈ U(a,b) and\nf3(x) = u3(wt · x) ∈ Ft. Thus, Ft is convex.\nSince Ft is a convex set of functions, we can regard ut(wt · x) as the orthogonal projection of y\n(which is a function of x) onto the convex set Ft. Classic inequalities for orthogonal projections can\nthen be applied to our case. In particular, below we prove that\nE\n(x,y)∼D[(ut(wt · x) − v(wt · x))(y − ut(wt · x))] ≥ 0,\n∀v ∈ U(a,b).\n(16)\nTo prove (16), note first that fu(x) = ut(wt · x) ∈ Ft and fv(x) = v(wt · x) ∈ Ft since ut, v ∈ U(a,b).\nThus, for any α ∈ (0, 1), we have αfv(x) + (1 − α)fu(x) ∈ Ft. Furthermore, by definition of ut,\n∀f ∈ Ft we have E(x,y)∼D[(ut(wt · x) − y)2] ≤ E(x,y)∼D[(f(x) − y)2], therefore, it holds:\n0 ≤ 1\nα\nE\n(x,y)∼D[(αfv(x) + (1 − α)fu(x) − y)2] − 1\nα\nE\n(x,y)∼D[(ut(wt · x) − y)2]\n= 1\nα\nE\n(x,y)∼D[(ut(wt · x) − y + α(v(wt · x) − ut(wt · x)))2 − (ut(wt · x) − y)2]\n=\nE\n(x,y)∼D[2(ut(wt · x) − y)(v(wt · x) − ut(wt · x)) + α(v(wt · x) − ut(wt · x))2].\nLet α ↓ 0, and note that Ex∼Dx[(v(wt · x) − ut(wt · x))2] < +∞, we thus have\nE\n(x,y)∼D[(ut(wt · x) − v(wt · x))(y − ut(wt · x))] ≥ 0,\nproving the claim.\nThe second claim can be proved following the same argument and is omitted for brevity.\nWe now proceed to the proof of Lemma 3.3.\nLemma 3.3 (Closeness of Population-Optimal Activations). Let wt ∈ B(W) and let u∗t, ut be\ndefined as solutions to (EP*), (EP), respectively. Then,\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤ OPT.\nProof. Summing up the first and second statement of Claim C.1 with v = u∗t ∈ U(a,b) in (16) and\nv′ = ut ∈ U(a,b), we get:\n0 ≤\nE\n(x,y)∼D[(ut(wt · x) − u∗t(wt · x))(y − ut(wt · x)) + (u∗t(wt · x) − ut(wt · x))(y∗ − u∗t(wt · x))]\n=\nE\n(x,y)∼D[(ut(wt · x) − u∗t(wt · x))(y − y∗ + u∗t(wt · x) − ut(wt · x))]\n=\nE\n(x,y)∼D[(ut(wt · x) − u∗t(wt · x))(y − y∗)] −\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2]\nRearranging and applying the Cauchy-Schwarz inequality, we have\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤\nE\n(x,y)∼D[(ut(wt · x) − u∗t(wt · x))(y − y∗)]\n≤\nr\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2] E[(y − y∗)2].\nTo complete the proof, it remains to recall that E[(y − y∗)2] = OPT and rearrange the last\ninequality.\n24\nC.2\nProof of Corollary 3.4\nCorollary 3.4 (Closeness of Idealized and Attainable Activations). Let ϵ, δ > 0. Given a parameter\nwt ∈ B(W) and m ≳ d log4(d/(ϵδ))(b2W 3/(L2ϵ))3/2 samples from D, let ˆut be the sample-optimal\nactivation on these samples given wt, as defined in (P). Then, with probability at least 1 − δ,\nE\nx∼Dx[(ˆut(wt · x) − u∗(w∗ · x))2] ≤ 3(ϵ + OPT + b2∥wt − w∗∥2\n2) .\nProof. The corollary follows directly from the combination of Lemma F.4 and Lemma 3.3, as we\nhave:\nE\nx∼Dx[(ˆut(wt · x) − u∗(w∗ · x))2]\n=\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x) + ut(wt · x) − u∗t(wt · x) + u∗t(wt · x) − u∗(w∗ · x))2]\n≤ 3( E\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2] +\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2])\n+ 3\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2]\n≤ 3(ϵ + OPT + b2∥wt − w∗∥2\n2),\nwhere we used that because u∗t ∈ argminu∈U(a,b) Ex∼Dx[(u(wt·x)−u∗(w∗·x))2], we have Ex∼Dx[(u∗t(wt·\nx) − u∗(w∗ · x))2] ≤ Ex∼Dx[(u∗(wt · x) − u∗(w∗ · x))2] ≤ b2∥wt − w∗∥2\n2, with the last inequality\nfollowing from the fact that u∗ ∈ U(a,b).\nC.3\nProof of Claim 3.5\nIn this subsection, we prove Claim 3.5 that appeared in Section 3.3, the proof of Proposition 3.1.\nClaim 3.5. Let S = {(x(i), y(i))}m\ni=1 be i.i.d. samples from D where m is as specified in the statement\nof Proposition 3.1. Let ˆut be the solution of optimization problem (P) given wt ∈ B(W) and S.\nFurthermore, denote the idealized version of S by S∗ = {(x(i), y∗(i))}m\ni=1, where y∗(i) = u∗(w∗ · x(i)).\nLet ˆu∗t be the solution of problem (P*). Then, with probability at least 1 − δ,\nQ1 = 1\nm\nm\nX\ni=1\n((ˆut(wt·x(i))−ˆu∗t(wt·x(i)))(wt−w∗)·x(i) ≥ −(√ϵ+\n√\nOPT)∥wt−w∗∥2−(ϵ+OPT)/b .\nProof. Adding and subtracting ut(wt · x(i)) and u∗t(wt · x(i)), we have\n1\nm\nm\nX\ni=1\n((ˆut(wt · x(i)) − ˆu∗t(wt · x(i)))(wt − w∗) · x(i)\n= 1\nm\nm\nX\ni=1\n(ˆut(wt · x(i)) − ut(wt · x(i)))(wt − w∗) · x(i) + 1\nm\nm\nX\ni=1\n(u∗t(wt · x(i)) − ˆu∗t(wt · x(i)))(wt − w∗) · x(i)\n+ 1\nm\nm\nX\ni=1\n(ut(wt · x(i)) − u∗t(wt · x(i)))(wt − w∗) · x(i).\n(17)\nTo proceed, we use that both ˆu∗t(z) and ˆut(z) are close to their population counterparts ut(z) and\nu∗t(z), respectively. In particular, in Lemma F.4 and Lemma F.2, we show that using a dataset S of\nm samples such that\nm ≳ d log4(d/(ϵδ))\n\u0012b2W 3\nL2ϵ\n\u00133/2\n,\n25\nwe have that with probability at least 1 − δ, for all wt, w∗ ∈ B(W) it holds\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2] ≤ ϵ,\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] ≤ ϵ.\n(18)\nNow suppose that the inequalities in (18) hold for the given wt ∈ B(W) (which happens with\nprobability at least 1 − δ). Applying Chebyshev’s inequality to the first summation term in (17), we\nget:\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n(ˆut(wt · x(i)) − ut(wt · x(i)))(wt − w∗) · x(i) −\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))(wt − w∗) · x]\n\f\f\f\f ≥ s\n\u0015\n≤\n1\nms2\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)2],\n(19)\nsince x(i) are i.i.d. random variables. The next step is to bound the variance. Note that Dx possesses\na 1/L-sub-exponential tail, thus we have Pr[|(wt − w∗) · x| ≥ ∥wt − w∗∥2r] ≤ (2/L2) exp(−Lr).\nChoose r = 2W\nL log(2/(L2ϵ′)); then, we have Pr[|(wt − w∗) · x| ≥ r] ≤ ϵ′. Now we separate the\nvariance under the event A = {x : |(wt − w∗) · x| ≤ r} and its complement.\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)2]\n=\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)21{A}]\n+\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)2(1 − 1{A})].\n(20)\nUsing that Ex∼Dx[(ˆut(wt · x) − ut(wt · x))2] ≤ ϵ, the first term in (20) can be bounded as follows:\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)21{A}] ≤ r2\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2]\n≤ r2ϵ = 4W 2ϵ\nL2\nlog2(2/(L2ϵ′)).\n(21)\nThe second term in (20) can be bounded using that both ˆut and ut are non-decreasing b-Lipschitz\nand vanish at zero (thus |ˆut(wt ·x)| ≤ b|wt ·x| and |ut(wt ·x)| ≤ b|wt ·x|, with their signs determined\nby the sign of wt · x), and then applying Young’s inequality:\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)2(1 − 1{A})]\n≤ b2\nE\nx∼Dx[(wt · x)2(wt · x − w∗ · x)2(1 − 1{A})]\n≤ 2b2\nE\nx∼Dx[((wt · x)4 + (wt · x)2(w∗ · x)2)(1 − 1{A})] .\nSince Dx is sub-exponential, we have E[(v · x)8] ≤ c2/L8 for some absolute constant c, hence\nE\nx∼Dx[(wt · x)4(1 − 1{A})] ≤\nr\nE\nx∼Dx[W 8((wt/∥wt∥2) · x)8] Pr[|wt · x| ≥ r] ≤ cW 4√\nϵ′/L4.\nSimilarly, for E[(wt · x)2(w∗ · x)2(1 − 1{A})], we have:\nE\nx∼Dx[(wt · x)2(w∗ · x)2(1 − 1{A})] ≤ 2\nE\nx∼Dx[((wt · x)4 + (w∗ · x)4)(1 − 1{A})] ≤ 2c(W/L)4√\nϵ′.\n26\nCombining the inequalities above with (21), we get the final upper bound on the variance in (20):\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2(wt · x − w∗ · x)2] ≤ 4W 2ϵ\nL2\nlog2(2/(L2ϵ′)) + 6cb2(W/L)4√\nϵ′.\nThus, choosing s = ϵ/b in (19), ϵ′ = ϵ2, and using m ≳ W 4b4 log2(1/ϵ)/(ϵδL4) samples we get\n1\nms2\n\u00124W 2ϵ\nL2\nlog2(2/(Lϵ′)) + 12cb2W 4√\nϵ′\nL4\n\u0013\n≲\nb2L4ϵδ\nϵ2W 4b4 log2(1/ϵ)\n\u0012W 2ϵ\nL2 log2\n\u0012 1\nLϵ\n\u0013\n+ b2W 4ϵ\nL4\n\u0013\n≤ δ .\nPlugging the inequality above back into (19) and recalling that Ex∼Dx[(ˆut(wt · x) − ut(wt · x))2] ≤ ϵ\n(from (18)), we finally have with probability at least 1 − δ,\n1\nm\nm\nX\ni=1\n(ˆut(wt · x(i)) − ut(wt · x(i)))(wt − w∗) · x(i)\n≥\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))(wt − w∗) · x] − ϵ/b\n≥ −\nr\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2]\nE\nx∼Dx[(wt · x − w∗ · x)2] − ϵ/b\n≥ −√ϵ∥wt − w∗∥2 − ϵ/b,\nwhere in the second inequality we used the Cauchy-Schwarz inequality and in the last inequality we\nused the assumption that Ex∼Dx[xx⊤] ≼ I. Finally, noting that (18) holds with probability at least\n1 − δ, applying a union bound we get that with probability at least 1 − 2δ, we have\n1\nm\nm\nX\ni=1\n(ˆut(wt · x(i)) − ut(wt · x(i)))(wt − w∗) · x(i) ≥ −√ϵ∥wt − w∗∥2 − ϵ/b .\nIn summary, to guarantee that the inequality above remains valid, we need the batch size to be:\nm ≳ dW 9/2b4 log4(d/(ϵδ)\nL4\n\u0012 1\nϵ3/2 + 1\nϵδ\n\u0013\n.\n(22)\nWe finished bounding the first term in (17).\nSince the same statements hold for the relationship between ˆu∗t and u∗t as they do for ˆut and ut,\nusing the same argument we also get that with probability at least 1 − 2δ,\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗t(wt · x(i)))(wt − w∗) · x(i) ≥ −√ϵ∥wt − w∗∥2 − ϵ/b,\nwhich is the lower bound for the second term in (17).\nLastly, for the third term in (17), since in Lemma 3.3 we showed that for any wt it always holds:\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤ OPT,\nthe only change of the previous steps is at the right-hand side of (21), where instead of having the\nupper bound of r2ϵ, we have\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2(wt · x − w∗ · x)21{A}] ≤ r2OPT = 4W 2OPT\nL2\nlog2(2/(L2ϵ′)).\n27\nBy the same token, we have\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2(wt · x − w∗ · x)2(1 − 1{A})] ≤ 6cb2(W/L)4√\nϵ′.\nAs a result, Chebyshev’s inequality yields:\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n(ut(wt · x(i)) − u∗t(wt · x(i)))(wt − w∗) · x(i) −\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))(wt − w∗) · x]\n\f\f\f\f ≥ s\n\u0015\n≤\n1\nms2\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2(wt · x − w∗ · x)2]\n≤\n1\nms2\n\u00124W 2OPT\nL2\nlog2(2/(L2ϵ′)) + 6cb2W 4√\nϵ′\nL4\n\u0013\n.\nNow instead of choosing s = ϵ, we let s = (OPT + ϵ)/b and keep ϵ′ as ϵ2 to get\n1\nms2\n\u00124W 2OPT\nL2\nlog2\n\u0012 2\nL2ϵ′\n\u0013\n+ 12cb2W\n√\nϵ′\nL4\n\u0013\n≲\nb2L4ϵδ\ndW 9/2b4 log4(d/(ϵδ))(OPT + ϵ)2\n\u0012W 2OPT\nL2\nlog2\n\u0012 1\nLϵ\n\u0013\n+ b2W 4ϵ\nL4\n\u0013\n≤ δ,\nunder our choice of m as specified in (22). Thus, we have that with probability at least 1−δ, it holds\n1\nm\nm\nX\ni=1\n(ut(wt · x(i)) − u∗t(wt · x(i)))(wt − w∗) · x(i) ≥\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))(wt − w∗) · x] − (OPT + ϵ)/b\n≥ −\n√\nOPT∥wt − w∗∥2 − (OPT + ϵ)/b,\nwhere in the last inequality we used the fact that\n|\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))(wt − w∗) · x]| ≤\nr\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2]\nE\nx∼Dx[((wt − w∗) · x)2]\n≤\n√\nOPT∥wt − w∗∥2,\nsince Ex∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤ OPT by Lemma 3.3.\nTherefore, combining the upper bounds on the three terms in (17), we get that with probability\nat least 1 − 5δ, it holds:\n1\nm\nm\nX\ni=1\n((ˆut(wt·x(i))−ˆu∗t(wt·x(i)))(wt−w∗)·x(i) ≥ −(2√ϵ+\n√\nOPT)∥wt−w∗∥2−(3ϵ+OPT)/b. (23)\nSince (23) was proved using arbitrary ϵ, δ > 0, it remains to replace δ ← δ/5 and ϵ ← ϵ/4 to complete\nthe proof of Claim 3.5.\nC.4\nProof of Claim 3.6\nIn this subsection, we prove Claim 3.6 that appeared in the proof of Proposition 3.1 in Section 3.3.\n28\nClaim 3.6. Let S∗ = {(x(i), y∗(i))}m\ni=1 be a sample set such that x(i)’s are i.i.d. samples from Dx\nand y∗(i) = u∗(w∗ · x(i)) for each i. Let m be the value specified in the statement of Proposition 3.1.\nThen, given a parameter wt ∈ B(W), with probability at least 1 − δ,\nQ2 = 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt − w∗) · x(i) ≥ Ca2LR4\nb\n∥(w∗)⊥wt∥2\n2 − √ϵ∥wt − w∗∥2 − ϵ/b ,\nwhere C is an absolute constant.\nProof. Before we proceed to the proof of the claim, let us consider first the inverse of u∗. Since\nu∗(z) ∈ U(a,b) is strictly increasing when z ≥ 0, (u∗)−1(α) exists for α ≥ 0. However, when z ≤ 0,\nu∗(z) could be constant on some intervals, hence (u∗)−1(α) might not exist for every α ≤ 0. We\nconsider instead an ‘empirical’ version of (u∗)−1(α) based on S∗, which is defined on every α ∈ R.\nGiven a sample set S∗ = {(x(i), y∗(i))} where y∗(i) = u∗(w∗ · x(i)), let us sort the index i in the\nincreasing order of w∗ · x(i), i.e., w∗ · x(1) ≤ · · · ≤ w∗ · x(m). Since u∗ is a monotone function, this\nimplies y∗(i)’s are also in increasing order, i.e., we have y∗(1) ≤ · · · ≤ y∗(m). We then partition the\nset {y∗(i)}m\ni=1 into blocks\n∆s = {y∗(ks−1+1), . . . , y∗(ks)}, s.t. y∗(ks−1+1) = · · · = y∗(ks) = τs,\nfor s = 1, . . . , s′. Since {y∗(i)} is sorted in increasing order, we have τs−1 < τs for s = 2, . . . , s′. Note\nthat since u∗(z) is strictly increasing when z ≥ 0 and as u∗(0) = 0, ∆s is a singleton set whenever\nτs > 0. Furthermore, let us denote by s∗ the largest index among 1, . . . , s′ such that τs∗ ≤ 0.\nSuppose first that τs∗ < 0 and define a function ˆf : R → R in the following way:\nˆf(α) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(u∗)−1(α),\nα > 0\nw∗ · x(ks∗) + α−τs∗\nτs∗ (w∗ · x(ks∗)),\nα ∈ [τs∗, 0]\nw∗ · x(ks),\nα = τs, s = 1, . . . , s∗ − 1\nw∗ · x(ks−1) + α−τs−1\nτs−τs−1 (w∗ · x(ks−1+1) − w∗ · x(ks−1)),\nα ∈ (τs−1, τs), s = 2, . . . , s∗\nw∗ · x(1) + 1\nb(α − τ1) .\nα ∈ (−∞, τ1)\n(24)\nWhen τs∗ = 0, we define (0 − τs∗)/τs∗ = −1, and hence ˆf(0) = 0. The rest remains unchanged. A\nvisualization of ˆf with respect to the ReLU activation is presented in Figure 3.\nThe function ˆf has the following properties. First, ˆf(α) satisfies ˆf(0) = 0, (α1 − α2)/a ≥\nˆf(α1) − ˆf(α2), for all α1 ≥ α2 ≥ 0, since ˆf(α) = (u∗)−1(α) when α > 0 and u∗ ∈ U(a,b). Second,\nˆf(α1)− ˆf(α2) ≥ (α1 −α2)/b for all α1, α2 ∈ R, α1 ≥ α2. This is because each segment of ˆf has slope\nat least 1/b. Third, for any α ≥ u∗(w∗ · x(i)), it holds that ˆf(α) − w∗ · x(i) ≥ (α − u∗(w∗ · x(i)))/b.\nTo see this, suppose u∗(w∗ · x(i)) ∈ ∆s. Then for any α ≥ τs, we have\nˆf(α) − w∗ · x(i) ≥ ˆf(α) − w∗ · x(ks) = ˆf(α) − ˆf(τs) = ˆf(α) − ˆf(u∗(w∗ · x(i))) ≥ (α − u∗(w∗ · x(i)))/b,\nusing that the slope of ˆf(α) is at least 1/b. On the other hand, when α < u∗(w∗ · x(i)), we have\nw∗ · x(i) − ˆf(α) ≥ (u∗(w∗ · x(i)) − α)/b. This can be seen similarly from the construction of ˆf.\nFinally, suppose u∗(w∗ · x(i)) ∈ ∆s. Then for any α < τs, we have\nw∗·x(i)− ˆf(α) ≥ w∗·x(ks−1+1)− ˆf(α) = ˆf(τs)− ˆf(α) = ˆf(u∗(w∗·x(i)))− ˆf(α) ≥ (u∗(w∗·x(i))−α)/b.\nAgain, we used the fact that ˆf(α1) − ˆf(α2) ≥ (α1 − α2)/b for all α1, α2 ∈ R, α1 ≥ α2 in the last\ninequality.\n29\nFigure 3:\nAn illustration of\nˆf for u∗(z) = max{0, z} and a dataset S∗ = {(x(1), u∗(w∗ ·\nx(1))), . . . , (x(6), u∗(w∗ · x(6)))} where w∗ · x(1) < w∗ · x(2) < w∗ · x(3) < 0.\nNow we turn to the summation displayed in the statement of the claim. To proceed, we add and\nsubtract ˆf(ˆu∗t(wt · x)) in the second component in the inner product, which yields:\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))(wt − w∗) · x(i)\n= 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))(wt · x(i) − ˆf(ˆu∗t(wt · x(i))))\n+ 1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))( ˆf(ˆu∗t(wt · x(i))) − w∗ · x(i)).\n(25)\nTo bound below the first term in (25), we make use of the following fact, whose proof can be found\nin Appendix C.5.\nFact C.2. Let wt ∈ B(W). Given m samples S = {(x(1), y∗(1)), · · · , (x(m), y∗(m))}, let ˆu∗t be one of\nthe solutions to the optimization problem (P*) , i.e., ˆu∗t ∈ argminu∈U(a,b)(1/m) Pm\ni=1(u(wt · x(i)) −\ny∗(i))2. Then\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt · x(i) − f(ˆu∗t(wt · x(i)))) ≥ 0,\nfor any function f : R → R such that f(0) = 0, (α1 − α2)/a ≥ f(α1) − f(α2) for all α1 ≥ α2 ≥ 0,\nand f(α1) − f(α2) ≥ (α1 − α2)/b, ∀α1, α2 ∈ R, α1 ≥ α2.\nAs we have already argued, ˆf satisfies the assumptions of Fact C.2, hence\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt ·x(i))−y∗(i))(wt −w∗)·x(i) ≥ 1\nm\nm\nX\ni=1\n(ˆu∗t(wt ·x(i))−y∗(i))( ˆf(ˆu∗t(wt ·x(i)))−w∗ ·x(i)).\n(26)\nRecall that we have shown the function ˆf satisfies ˆf(α) − w∗ · x(i) ≥ (α − u∗(w∗ · x(i)))/b ≥ 0\nwhenever α ≥ u∗(w∗ · x(i)), and moreover, w∗ · x(i) − ˆf(α) ≥ (u∗(w∗ · x(i)) − α)/b ≥ 0 when\n30\nα < u∗(w∗ · x(i)). Therefore, letting α = ˆu∗t(wt · x(i)) and combining these results we get\n(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))(f(ˆu∗t(wt · x(i))) − w∗ · x(i)) ≥ 1\nb(ˆu∗t(wt · x(i)) − u∗(w∗ · x(i)))2.\nPlugging the inequality above back into (26) we then get\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt − w∗) · x(i) ≥ 1\nmb\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))2 .\n(27)\nThe goal now is to bound below the right-hand side of (27) by E[(ˆu∗t(wt · x) − y∗)2] and some small\nerror terms using Chebyshev inequality as we did in Claim 3.5. Plugging in Lemma 3.2, we can\nfurther lower bound E[(ˆu∗t(wt · x) − y∗)2] by ∥(w∗)⊥wt∥2\n2 and then we are done with the proof of\nthis claim. Note that Chebyshev’s inequality yields\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))2 −\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)2]\n\f\f\f\f ≥ s\n\u0015\n≤\n1\nms2\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)4].\n(28)\nWe now bound Ex∼Dx[(ˆu∗t(wt · x) − y∗)4]. Observe that\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)4] =\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x) + u∗t(wt · x) − y∗)2(ˆu∗t(wt · x) − y∗)2]\n≤ 4\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((ˆu∗t(wt · x))2 + (y∗)2)]\n+ 4\nE\nx∼Dx[(u∗t(wt · x) − y∗)2((ˆu∗t(wt · x))2 + (y∗)2)].\n(29)\nWe focus on the two terms in (29) separately. Again, choosing r = 2W\nL log(2/(L2ϵ′)), then by the\nL-sub-exponential tail bound of Dx, it holds Pr[|wt · x| ≥ r] ≤ ϵ′, Pr[|w∗ · x| ≥ r] ≤ ϵ′. Since\ny∗ = u∗(w∗ · x) and both u∗ and ˆu∗t are non-decreasing b-Lipschitz, it holds:\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((ˆu∗t(wt · x))2 + (y∗)2)]\n≤ b2\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((wt · x)2 + (w∗ · x)2)]\n= b2\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((wt · x)2 + (w∗ · x)2)1{|wt · x| ≤ r, |w∗ · x| ≤ r}]\n+ b2\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((wt · x)2 + (w∗ · x)2)1{|wt · x| ≥ r or |w∗ · x| ≥ r}]\n≤ 2b2r2\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2]\n+ 2b4\nE\nx∼Dx[2(wt · x)2((wt · x)2 + (w∗ · x)2)1{|wt · x| ≥ r or |w∗ · x| ≥ r}].\n(30)\nThe first term in (30) can be upper bounded using Lemma F.2, which states that when\nm ≳ d log(1/δ)(b2W 3 log2(d/ϵ)/(L2ϵ))3/2),\nwith probability at least 1 − δ it holds Ex∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] ≤ ϵ for all wt ∈ B(W).\nNow suppose this inequality is valid given wt ∈ B(W) (which happens with probability at least\n1 − δ). For the second term in (30), note that for any unit vector a it holds Ex∼Dx[(a · x)8] ≤ c2/L8\n31\nfor some absolute constant c > 0, and furthermore, the magnitude of r ensures that Pr[|wt · x| ≥\nr or |w∗ · x| ≥ r] ≤ 2ϵ′; therefore, combining these bounds, we get:\nE\nx∼Dx[2(wt · x)2((wt · x)2 + (w∗ · x)2)1{|wt · x| ≥ r or |w∗ · x| ≥ r}]\n≤ 2\nr\nE\nx∼Dx[(wt · x)8] Pr[|wt · x| ≥ r or |w∗ · x| ≥ r]\n+ 2\nr\n2( E\nx∼Dx[(wt · x)8] +\nE\nx∼Dx[(w∗ · x)8]) Pr[|wt · x| ≥ r or |w∗ · x| ≥ r]\n≤ 24c(W/L)4√\nϵ′.\nPlugging back into (30), we have\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2((ˆu∗t(wt · x))2 + (y∗)2)] ≤ 2b2r2ϵ + 48c(bW/L)4√\nϵ′,\nwhich is the upper bound on the first term of (29).\nFor the second term in (29), since by definition we have u∗t ∈ argminu∈U(a,b) Ex∼Dx[(u(wt · x) −\ny∗)2], it holds that\nE\nx∼Dx[(u∗t(wt·x)−y∗)2] ≤\nE\nx∼Dx[(u∗(wt·x)−u∗(w∗·x))2] ≤ b2\nE\nx∼Dx[((wt−w∗)·x)2] ≤ b2∥wt−w∗∥2\n2,\nnoting in addition that Ex∼Dx[xx⊤] ≼ I. Thus, using similar steps as in (30), we have\nE\nx∼Dx[(u∗t(wt · x) − y∗)2((ˆu∗t(wt · x))2 + (y∗)2)]\n≤ 2b2r2\nE\nx∼Dx[(u(wt · x) − y∗)2]\n+ 2b4\nE\nx∼Dx[2((wt · x)2 + (w∗ · x)2)21{|wt · x| ≥ r or |w∗ · x| ≥ r}]\n≤ 2b4r2∥wt − w∗∥2\n2 + 48c(bW/L)4√ϵ.\nIn summary, combining all the results and plugging them back into (29), we finally get the upper\nbound for the variance:\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)4] ≤ 32b2W 2\nL2\nlog2(2/(L2ϵ′))(b2∥wt − w∗∥2\n2 + ϵ) + 384c(bW/L)4√\nϵ′.\nLet s = b√ϵ∥wt − w∗∥2 + ϵ/b and plug the last inequality back into (28) to get:\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))2 −\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)2]\n\f\f\f\f ≥ b√ϵ∥wt − w∗∥2 + ϵ/b\n\u0015\n≤\n1\nm(ϵb2∥wt − w∗∥2\n2 + ϵ2/b2)\n\u001232b2W 2\nL2\nlog2\n\u0012 2\nL2ϵ′\n\u0013\n(b2∥wt − w∗∥2\n2 + ϵ) + 384c(bW/L)4√\nϵ′\n\u0013\n.\nChoosing ϵ′ = ϵ2/b4 and using similar arguments as in Claim 3.5, we get that the right-hand side of\nthe inequality above is bounded by δ, given our choice of m ≳ db4W 9/2 log4(d/(ϵδ))(1/ϵ3/2 + 1/(ϵδ))\nas specified in the statement of Proposition 3.1. In summary, after a union bound on the probability\nabove and the event that Ex∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] ≤ ϵ, we have with probability at least\n1 − 2δ,\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))2 ≥\nE\nx∼Dx[(ˆu∗t(wt · x) − y∗)2] − √ϵb∥wt − w∗∥2 − ϵ/b.\n32\nRecall that in Lemma 3.2 we showed that Ex∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2] ≥ Ca2LR4∥(w∗)⊥wt∥2\n2\nfor an absolute constant C; thus, our final result is that with probability at least 1 − δ,\n1\nm\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt − w∗) · x(i) ≥ 1\nmb\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))2\n≥ Ca2LR4\nb\n∥(w∗)⊥wt∥2\n2 − √ϵ∥wt − w∗∥2 − ϵ/b.\nThis completes the proof of Claim 3.6.\nC.5\nProof of Fact C.2\nWe prove a modified version of Lemma 1 [KKSK11], presented as the statement below. The statement\nconsiders a smaller activation class and a function f with different properties compared to [KKSK11],\nand the proof is based on a rigorous KKT argument.\nFact C.2. Let wt ∈ B(W). Given m samples S = {(x(1), y∗(1)), · · · , (x(m), y∗(m))}, let ˆu∗t be one of\nthe solutions to the optimization problem (P*) , i.e., ˆu∗t ∈ argminu∈U(a,b)(1/m) Pm\ni=1(u(wt · x(i)) −\ny∗(i))2. Then\nm\nX\ni=1\n(ˆu∗t(wt · x(i)) − y∗(i))(wt · x(i) − f(ˆu∗t(wt · x(i)))) ≥ 0,\nfor any function f : R → R such that f(0) = 0, (α1 − α2)/a ≥ f(α1) − f(α2) for all α1 ≥ α2 ≥ 0,\nand f(α1) − f(α2) ≥ (α1 − α2)/b, ∀α1, α2 ∈ R, α1 ≥ α2.\nProof. We transform the optimization problem (P*) to a quadratic optimization problem with linear\nconstraints. To guarantee that the solution of this quadratic problem corresponds to a function that\nis (a, b)-unbounded, we add a sample (x(k), y∗(k)) = (0, 0) to the sample set. Let zi = wt · x(i) such\nthat (after sorting the indices) z1 ≤ z2 ≤ · · · ≤ zm and zk = 0. We solve the following optimization\nproblem:\nmin\n˜y(i),i∈[m]\nm\nX\ni=1\n(˜y(i) − y∗(i))2\ns.t.\n0 ≤ ˜y(i+1) − ˜y(i),\n1 ≤ i ≤ k − 1 ,\na(zi+1 − zi) ≤ ˜y(i+1) − ˜y(i),\nk ≤ i ≤ m − 1 ,\n˜y(i+1) − ˜y(i) ≤ b(zi+1 − zi),\n1 ≤ i ≤ m − 1 ,\n˜y(k) = 0 .\n(31)\nDenote the solution of (31) as ˆy∗(i), i = 1, · · · , m. Let ˆu∗t(z) be the linear interpolation function of\n(zi, ˆy∗(i)), then ˆu∗t ∈ U(a,b) since ˆu∗t(0) = ˆu∗t(zk) = ˆy∗(k) = 0, ˆu∗t is b-Lipschitz and ˆu∗t(z)− ˆu∗t(z′) ≥\na(z − z′) for all z ≥ z′ ≥ 0. In other words, finding a solution of (P*) is equivalent to solving (31).\nNow observe that the summation Pm\ni=1(ˆy∗(i) − y∗(i))(zi − f(ˆy∗(i))) can be transformed into the\nfollowing:\nm\nX\ni=1\n(ˆy∗(i) − y∗(i))(zi − f(ˆy∗(i))) =\nm\nX\ni=1\n\u0012\ni\nX\nj=1\n(ˆy∗(j) − y∗(j))\n\u0013\n(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))), (32)\nwhere we let zm+1 = 0, ˆy∗\nm+1 = 0 (and hence f(ˆy∗\nm+1) = 0 as f(0) = 0).\n33\nTo utilize the information that ˆy∗(i) is the minimizer of the optimization problem (31), we write\ndown the KKT conditions for the optimization problem (31) described above:\nˆy∗(i) = y∗(i) + (λ′\ni − λ′\ni−1)/2 − (λi − λi−1)/2 − (νk/2)1{i = k},\ni = 1, · · · , m;\n(33)\n− λi(ˆy∗(i+1) − ˆy∗(i)) = 0,\ni = 1, · · · , k − 1;\n(34)\nλi(a(zi+1 − zi) − (ˆy∗(i+1) − ˆy∗(i))) = 0,\ni = k, · · · , m − 1;\n(35)\nλ′\ni((ˆy∗(i+1) − ˆy∗(i)) − b(zi+1 − zi)) = 0,\ni = 1, · · · , m − 1;\n(36)\nνkˆy∗(k) = 0 ,\n(37)\nwhere λi, λ′\ni ≥ 0, for i = 1, . . . , m − 1, and νk ∈ R are dual variables, and we let λ0 = λ′\n0 = 0 for the\nconvenience of presenting (33).\nSumming up (33) recursively, we immediately get that\ni\nX\nj=1\n(ˆy∗(i) − y∗(i)) = 1\n2((λ′\ni − λi) − νk1{i ≥ k}).\nPlugging the equality above back into (32), we have\nm\nX\ni=1\n(ˆy∗(i) − y∗(i))(zi − f(ˆy∗(i)))\n= 1\n2\nm\nX\ni=1\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) + 1\n2\nm\nX\ni=k\nνk(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1))))\n= 1\n2\nm\nX\ni=1\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) + νk(zk − f(ˆy∗(k)) − (zm+1 − f(ˆy∗(m+1)))).\n(38)\nSince by definition, zm+1 = f(ˆy∗(m+1)) = 0, zk = 0, and as ˆy∗(i), i ∈ [m], is a feasible solution of\n(31), it holds ˆy∗(k) = 0, we thus have\nνk(zk − f(ˆy∗(k)) − (zm+1 − f(ˆy∗(m+1)))) = 0.\nPlugging this back into (38), we get\nm\nX\ni=1\n(ˆy∗(i) − y∗(i))(zi − f(ˆy∗(i))) = 1\n2\nm\nX\ni=1\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1))))\n= 1\n2\nk−1\nX\ni=1\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1))))\n|\n{z\n}\nS1\n+ 1\n2\nm\nX\ni=k\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1))))\n|\n{z\n}\nS2\n.\n(39)\nConsider first S1. Suppose that for some i ∈ {1, . . . , k−1} we have λ′\ni, λi > 0. Then, according to\nthe complementary slackness condition (34) and (35), it holds that 0 = ˆy∗(i+1) − ˆy∗(i) = b(zi+1 − zi).\nTherefore,\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) ≥ 0.\n34\nSuppose now that for some i ∈ {1, . . . , k − 1}, it holds λ′\ni > 0, λ = 0. Then, it must be the case that\nˆy∗(i+1) − ˆy∗(i) = b(zi+1 − zi) ≥ 0,\naccording to the KKT condition (36). Since\nf(ˆy∗(i+1)) − f(ˆy∗(i)) ≥ (ˆy∗(i+1) − ˆy∗(i))/b\nby assumption on f, we thus have\n(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) ≥ 0.\nFinally, if λi > 0, λ′ = 0, then (34) indicates that 0 = ˆy∗(i+1) − ˆy∗(i). Therefore, as zi+1 ≥ zi, the ith\nsummand is also positive. In summary, S1 ≥ 0.\nNow consider S2. Observe that if for some i ∈ {k, . . . , m} it holds λi > 0 and λ′\ni > 0 at the same\ntime, then KKT conditions (35) and (36) imply that\na(zi+1 − zi) = ˆy∗(i+1) − ˆy∗(i) = b(zi+1 − zi),\nas a < b and it has to be zi+1 − zi = ˆy∗(i+1) − ˆy∗(i) = 0, which indicates that the ith summand in\nthe second term must be 0, i.e.,\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) = 0.\nNow suppose for some i ∈ {1, . . . , m}, λ′\ni > 0 and λi = 0. Then by the complementary slackness\nconditions (35) and (36), it must be that\nˆy∗(i+1) − ˆy∗(i) = b(zi+1 − zi) ≥ 0.\nAgain, since f satisfies\nf(ˆy∗(i+1)) − f(ˆy∗(i)) ≥ (ˆy∗(i+1) − ˆy∗(i))/b\nfor any ˆy∗(i+1) ≥ ˆy∗(i), we thus have\nzi − zi+1 + (f(ˆy∗(i+1)) − f(ˆy∗(i))) ≥ 0.\nThus, it holds that\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) ≥ 0.\nOn the other hand, if λ′\ni = 0 and λi > 0, then complementary slackness implies that\nˆy∗(i+1) − ˆy∗(i) = a(zi+1 − zi) ≥ 0.\nFurthermore, since ˆy∗(i) ≥ ˆy∗(k) ≥ 0 when i ≥ k, using the assumption that\n(α1 − α2)/a ≥ f(α1) − f(α2)\nwhen α1 ≥ α2 ≥ 0, we get\nzi − zi+1 + (f(ˆy∗(i+1)) − f(ˆy∗(i))) ≤ 0,\nand hence\n(λ′\ni − λi)(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) ≥ 0\nholds as well. Thus we conclude that S2 ≥ 0.\nIn summary, since each summand in (39) is non-negative, we finally get that\nm\nX\ni=1\n(ˆy∗(i) − y∗(i))(zi − f(ˆy∗(i))) =\nm\nX\ni=1\n\u0012\ni\nX\nj=1\n(ˆy∗(j) − y∗(j))\n\u0013\n(zi − f(ˆy∗(i)) − (zi+1 − f(ˆy∗(i+1)))) ≥ 0.\nThis completes the proof of Fact C.2.\n35\nC.6\nProof of Claim 3.7\nWe restate and prove Claim 3.7 that appeared in the proof of Proposition 3.1 in Section 3.3.\nClaim 3.7. Let S = {(x(i), y(i))}m\ni=1 be i.i.d. samples from D, and denote by S∗ = {(x(i), y∗(i))}m\ni=1\nthe idealized version of S, where y∗(i) = u∗(w∗ · x(i)). Under the condition of Proposition 3.1, given\na parameter wt ∈ B(W), with probability at least 1 − δ,\nQ3 = 1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i)) ≥ −\n√\nOPT∥w∗ − wt∥2 − (OPT + ϵ)/b .\nProof. By Chebyshev’s inequality, we can write\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i)) −\nE\n(x,y)∼D[(y∗ − y)(wt − w∗) · x]\n\f\f\f\f ≥ s\n\u0015\n≤ E(x,y)∼D[(y∗ − y)2(wt · x − w∗ · x)2]\nms2\n.\nLet r = 2W\nL log(2/(L2ϵ′)), then by the fact that Dx is sub-exponential, we have Pr[|(wt − w∗) · x| ≥\nr] ≤ ϵ′. Furthermore, since |y| ≤ M where M = bW\nL log(16b4W 4/ϵ2), as stated in Fact F.3, the\nvariance can be bounded as follows:\nE\n(x,y)∼D[(y∗ − y)2(wt · x − w∗ · x)2]\n≤\nE\n(x,y)∼D[(y∗ − y)2(wt · x − w∗ · x)21{|(wt − w∗) · x| ≤ r}]\n+\nE\n(x,y)∼D[(y∗ − y)2(wt · x − w∗ · x)21{|(wt − w∗) · x| ≥ r}]\n≤ r2\nE\n(x,y)∼D[(u∗(w∗ · x) − y)2]\n+\nE\n(x,y)∼D[(2(u∗(w∗ · x))2 + y2)(wt · x − w∗ · x)21{|(wt − w∗) · x| ≥ r}]\n≤ r2OPT +\nE\nx∼Dx[2(b2(wt · x)2 + M2)(wt · x − w∗ · x)21{|(wt − w∗) · x| ≥ r}].\nSince for any unit vectors a, b we have Ex∼Dx[(a · x)4] ≤ c2/L4 and Ex∼Dx[(a · x)4(b · x)4] ≤ c2/L8,\nwe have:\n2b2\nE\nx∼Dx[(wt · x)2(wt · x − w∗ · x2)21{|(wt − w∗) · x| ≥ r}]\n≤ 4b2(W/L)4r\nE\nx∼Dx[((wt/∥wt∥2) · x)4(((wt − w∗)/∥wt − w∗∥2) · x)4] Pr[|(wt − w∗) · x| ≥ r]\n≤ 4cb2(W/L)4√\nϵ′,\nand in addition,\nE\nx∼Dx[M2((wt − w∗) · x)21{|(wt − w∗) · x| ≥ r}]\n≤ 2M2W 2r\nE\nx∼Dx[((wt − w∗) · x)4] Pr[|(wt − w∗) · x| ≥ r] ≤ cM2(W/L)2√\nϵ′.\n36\nLet s = (OPT + ϵ)/b, ϵ′ = ϵ2, under our choice of m ≳ db4W 9/2 log4(d/(ϵδ))(1/ϵ3/2 + 1/(ϵδ)), it\nholds that\n1\nms2\n\u00124W 2 log2(1/(L2ϵ′))OPT\nL2\n+ (4cb2(W/L)4 + cM2(W/L)2)\n√\nϵ′\n\u0013\n≤ δ.\nThus, with probability at least 1 − δ it holds that\n1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i)) ≥\nE\n(x,y)∼D[(y − y∗)(wt − w∗) · x] − (OPT + ϵ)/b.\nSince\n\f\f\f\f\nE\n(x,y)∼D[(y − y∗)(wt − w∗) · x]\n\f\f\f\f ≤\nr\nE\n(x,y)∼D[(y − y∗)2]\nE\nx∼Dx[((wt − w∗) · x)2] ≤\n√\nOPT∥w∗ − wt∥2,\nwe finally have\n1\nm\nm\nX\ni=1\n(y∗(i) − y(i))(wt · x(i) − w∗ · x(i)) ≥ −\n√\nOPT∥w∗ − wt∥2 − (OPT + ϵ)/b,\ncompleting the proof of Claim 3.7.\nD\nOmitted Proofs from Section 4\nD.1\nProof of Theorem 4.2\nIn this subsection, we restate and prove our main theorem Theorem 4.2. The full version of the\noptimization algorithm as well as the main theorem Theorem 4.2 is displayed below:\nTheorem D.1 (Main Result). Let D be a distribution in Rd × R and suppose that Dx is (L, R)-\nwell-behaved. Furthermore, let U(a,b) be as in Definition 1.3, and ϵ > 0. Let µ = Ca2LR4/b,\nwhere C is an absolute constant. Running Algorithm 4 with the following parameters: step size\nη = µ/(4b2), batch size to be m ≳ dW 11/2b17 log5(d/ϵ)/(L4µ12ϵ3/2) and the total number of iterations\nto be T ′ = t0JT = O(Wb11/(µ10√ϵ) log(1/ϵ)), where T = O((b/µ)2 log(1/ϵ)), then with probability\nat least 2/3, Algorithm 4 returns a hypothesis (ˆu, bw) where ˆu ∈ U(a,b) and bw ∈ B(W) such that\nL2(bw; ˆu) = O\n\u0012\nb4\na4L2R8\n\u0013\nOPT + ϵ ,\nusing N = O(T ′m) = ˜O(dW 13/2b28/(L4µ22ϵ2)) samples.\nProof. As proved in Lemma 4.1, the initialization subroutine Algorithm 1 outputs a list of points\n{wini\nk }t0\nk=1 that contains a point wini\nk∗ such that\n∥(w∗)\n⊥wini\nk∗ ∥2 ≤ max{µ∥w∗∥2/(4b), 64b2/µ3(\n√\nOPT + √ϵ)}.\nSuppose first that µ∥w∗∥2/(4b) ≤ 64b2/µ3(\n√\nOPT + √ϵ).\nThen this implies that ∥w∗∥2 ≤\n256b3/µ4(\n√\nOPT + √ϵ). Therefore, applying Claim 4.4 we immediately get that the trivial hy-\npothesis (w = 0, u(z) = 0) works as a constant approximate solution, as in this case\nL2(w; u) ≤ 8(OPT + ϵ) + 4b2∥w∗∥2 = O((b/µ)8)OPT + ϵ.\n37\nAlgorithm 4 Optimization\n1: Input: wini = 0; ϵ > 0; positive parameters: a, b, L, R, W; let µ ≲ a2LR4/b; step size\nη = µ/(4b2), number of iterations T = O((b/µ)2 log(1/ϵ)).\n2: {wini\n0 , . . . , wini\nt0 } = Initialization[wini] (Algorithm 1)\n3: for k = 0 to t0 ≲ (b/µ)6 log(b/µ) do\n4:\nPk = {}\n5:\nfor j = 1 to J = W/(η√ϵ) do\n6:\n¯w0\nj,k = wini\nk .\n7:\nβj = jη√ϵ.\n▷ find an η√ϵ approximation of ∥w∗∥2\n8:\nfor t = 0 to T − 1 do\n9:\nbwt\nj,k = βj( ¯wt\nj,k/∥ ¯wt\nj,k∥2).\n▷ normalize ¯w\n10:\nDraw m ≳ W 11/2b17 log5(d/ϵ)d/(L4µ12ϵ3/2) new i.i.d. samples from D\n11:\nˆut\nj,k = argminu∈U(a,b)(1/m) Pm\ni=1(u(bwt\nj,k · x(i)) − y(i))2.\n12:\n∇ bLsur(bwt\nj,k; ˆut\nj,k) = (1/m) Pm\ni=1(ˆut\nj,k(bwt\nj,k · x(i)) − y(i))x(i).\n13:\n¯wt+1\nj,k = bwt\nj,k − η∇ bLsur(bwt\nj,k; ˆut\nj,k)\n14:\nend for\n15:\nPk ← Pk ∪ {(bwT\nj,k; ˆuT\nj,k)}.\n16:\nend for\n17:\nP = ∪t0\nk=1Pk ∪ {(w = 0; u(z) = 0)}\n18: end for\n19: (bw; ˆu) = Test[(w; u) ∈ P] (Algorithm 3)\n▷ testing\n20: Return: (bw; ˆu)\nThis hypothesis (w = 0, u(z) = 0) is contained in our solution set P (see Algorithm 4) and tested in\nAlgorithm 3.\nThus, in the rest of the proof we assume that wini\nk∗ satisfies\n∥(w∗)\n⊥wini\nk∗ ∥2 ≤ µ∥w∗∥2/(4b).\nLet us consider this initialized parameter at k∗ step in the outer loop (line 4), ¯w0\nj,k∗ = wini\nk∗ . In the\nrest of the proof we drop the subscript k∗ since the context is clear.\nSince we constructed a grid with grid width η√ϵ from 0 to W to find the (approximate) value\nof ∥w∗∥2, there must exist an index j∗ such that the value of βj∗ is η√ϵ close to ∥w∗∥2, i.e.,\n|βj∗ − ∥w∗∥2| ≤ η√ϵ. We now consider this j∗th outer loop and ignore the subscript j∗ for simplicity.\nLet wt = ∥w∗∥2( ¯wt/∥ ¯wt∥2), which is the true normalized vector of ¯wt that has no error.\nWe study the squared distance between ¯wt+1 and w∗:\n∥ ¯wt+1 − w∗∥2\n2 = ∥bwt − η∇ bLsur(bwt; ˆut) − w∗∥2\n2\n= ∥bwt − w∗∥2\n2 + η2∥∇ bLsur(bwt; ˆut)∥2\n2 − 2η∇ bLsur(bwt; ˆut) · (bwt − w∗).\n(40)\nApplying Lemma 4.3 to (40), and plugging in Proposition 3.1, we get that when drawing\nm ≳ dW 9/2b4 log4(d/(ϵδ))\nL4\n\u0012 1\nϵ3/2 + 1\nϵδ\n\u0013\n,\n(41)\nsamples from the distribution, it holds with probability at least 1 − δ that:\n∥ ¯wt+1 − w∗∥2\n2 ≤ ∥bwt − w∗∥2\n2 + η2(10(OPT + ϵ) + 4b2∥bwt − w∗∥2\n2)\n+ 2η(2(OPT + ϵ)/b + 2(\n√\nOPT + √ϵ)∥bwt − w∗∥2 − µ∥vt∥2\n2),\n(42)\n38\nwhere µ = Ca2LR4/b with C being an absolute constant, and where vt is the component of w∗ that\nis orthogonal to bwt, i.e.,\nvt = w∗ − (w∗ · bwt)bwt/∥bwt∥2\n2 = (w∗)⊥ b\nwt.\nNote that ∥vt∥2 is invariant to the rescaling of bwt, in other words, w∗ has the same orthogonal\ncomponent vt for all ¯wt, wt and bwt.\nSince ∥bwt − wt∥2 ≤ η√ϵ, we have\n∥bwt − w∗∥2\n2 = ∥bwt − wt + wt − w∗∥2\n2 ≤ ∥wt − w∗∥2\n2 + η2ϵ + 2η√ϵ∥wt − w∗∥2.\n(43)\nIn addition, by triangle inequality we have ∥bwt − w∗∥2 ≤ ∥wt − w∗∥2 + η√ϵ. Therefore, substituting\nwt with bwt in (42), we get:\n∥ ¯wt+1 − w∗∥2\n2 ≤ ∥wt − w∗∥2\n2 + η2ϵ + 2η√ϵ∥wt − w∗∥2\n+ η2(10(OPT + ϵ) + 4b2∥wt − w∗∥2\n2 + 4b2η2ϵ + 8b2η√ϵ∥wt − w∗∥2)\n+ 2η(2(OPT + ϵ)/b + 2(\n√\nOPT + √ϵ)(∥wt − w∗∥2 + η√ϵ) − µ∥vt∥2\n2)\n≤ ∥wt − w∗∥2\n2 + η2(24(OPT + ϵ) + 4b2∥wt − w∗∥2\n2)\n+ 2η(2(OPT + ϵ)/b + 4(\n√\nOPT + √ϵ)∥wt − w∗∥2 − µ∥vt∥2\n2),\n(44)\nwhere we used 4b2η2 ≤ 1, which holds because η = µ/(4b2).\nOur goal is to show that ∥vt+1∥2\n2 ≤ ∥ ¯wt+1 − w∗∥2\n2 ≤ (1 − c)∥vt∥2\n2 + ϵ, where c ∈ (0, 1) is a\nconstant and ϵ is a small error parameter. However, this linear contraction can only be obtained\nwhen ∥vt∥2 is relatively small compared to ∥w∗∥2. Specifically, as will be manifested in Claim D.2\nand the proceeding proof, the linear contraction is achieved only when ∥vt∥2 ≤ µ∥w∗∥2/(4b). Luckily,\nwe can start with a v0 such that this condition is satisfied, due to the initialization subroutine\nAlgorithm 1, as proved in Lemma 4.1. We prove the following claim.\nClaim D.2. Let η = µ/(4b2). Then, under the assumptions of Theorem 4.2, with probability at least\n1 − δ, we have\n∥ ¯wt+1 − w∗∥2\n2 ≤\n\u0012\n1 − µ2\n32b2\n\u0013\n∥vt∥2\n2,\nwhenever ∥vt∥2 ≥ (96/µ)(\n√\nOPT + √ϵ).\nProof of Claim D.2. Since the norm of wt is normalized to w∗, the quantity ∥wt −w∗∥2 is controlled\nby ∥vt∥2\n2. In particular, let w∗ = αtwt+vt. Then, since vt ⊥ wt, we have ∥w∗∥2\n2 = α2\nt ∥wt∥2\n2+∥vt∥2\n2 =\nα2\nt ∥w∗∥2\n2 + ∥vt∥2\n2, thus, α2\nt = 1 − ∥vt∥2\n2/∥w∗∥2\n2, and ∥vt∥2\n2 = (1 − α2\nt )∥w∗∥2\n2. In addition, ∥wt − w∗∥2\n2\ncan be expressed as a function of αt and w∗, as\n∥wt − w∗∥2\n2 = (1 − αt)2∥w∗∥2\n2 + ∥vt∥2\n2 = 2(1 − αt)∥w∗∥2\n2.\n(45)\nNote that since αt =\np\n1 − ∥vt∥2\n2/∥w∗∥2\n2, denoting ρt = ∥vt∥2/∥w∗∥2, we further have:\n1 − αt = 1 −\nq\n1 − ∥vt∥2\n2/∥w∗∥2\n2 = 1 −\nq\n1 − ρ2\nt ≤ 1\n2ρ2\nt + 1\n2ρ4\nt ≤ ρ2\nt , ∀ρt ∈ [0, 1].\n(46)\n39\nTherefore, plugging (45) and (46) back into (44), we get:\n∥ ¯wt+1 − w∗∥2\n2 ≤ 2(1 − αt)∥w∗∥2\n2 + 4b2η2(2(1 − αt)∥w∗∥2\n2) + 8η(\n√\nOPT + √ϵ)\np\n2(1 − αt)∥w∗∥2\n− 2ηµ∥vt∥2\n2 + 24η2(OPT + ϵ) + 4η(OPT + ϵ)/b\n≤ (ρ2\nt + ρ4\nt )∥w∗∥2\n2 + 4b2η2(ρ2\nt + ρ4\nt )∥w∗∥2\n2 + 8\n√\n2η(\n√\nOPT + √ϵ)ρt∥w∗∥2\n− 2ηµ∥vt∥2\n2 + 24η2(OPT + ϵ) + 4η(OPT + ϵ)/b\n= (1 + ρ2\nt + 4b2η2(1 + ρ2\nt ))∥vt∥2\n2 + 12η(\n√\nOPT + √ϵ)∥vt∥2 − 2ηµ∥vt∥2\n2\n+ 4(6η2 + η/b)(OPT + ϵ)\n≤ (1 + ρ2\nt + 4b2η2(1 + ρ2\nt ))∥vt∥2\n2 + 12η(\n√\nOPT + √ϵ)∥vt∥2 − 2ηµ∥vt∥2\n2 + 5η(OPT + ϵ),\n(47)\nwhere in the last inequality we observed that since η =\nµ\n4b2 , it holds that 24η ≤ 1, as µ is small and\nb ≥ 1.\nNote that we have assumed that ∥vt∥2 ≥ (96/µ)(\n√\nOPT + √ϵ), which indicates\n12η(\n√\nOPT + √ϵ)∥vt∥2 ≤ 1\n8ηµ∥vt∥2\n2,\nsince b ≥ 1 was assumed without loss of generality. Furthermore, when ∥vt∥2 ≥ (96/µ)(\n√\nOPT+√ϵ),\nit also holds that\n1\n8ηµ∥vt∥2\n2 ≥ (96)2\n8µ2 ηµ(OPT + ϵ) ≥ 5η(OPT + ϵ),\nsince we have assumed µ = Ca2LR4/b ≤ 1 without loss of generality. Finally, as we will show in\nthe rest of the proof, it holds that ∥vt+1∥2 ≤ ∥vt∥2 for t = 0, 1, . . . , T, thus as η = µ/(4b2), we have\n∥vt∥2 ≤ √ηµ∥w∗∥2/2 = µ∥w∗∥2/(4b), since ∥v0∥2 ≤ √ηµ∥w∗∥2/2. This condition guarantees that\nρ2\nt = ∥vt∥2\n2/∥w∗∥2\n2 ≤ 1\n4ηµ.\nPlugging these conditions back into (47), it is then simplified as (note that 1+ρ2\nt ≤ 1+(1/4)ηµ ≤ 9/8\nfor ηµ ≤ 1/2):\n∥ ¯wt+1 − w∗∥2\n2 ≤\n\u0012\n1 + 9\n2b2η2 − 3\n2ηµ\n\u0013\n∥vt∥2\n2.\nTherefore, when η = µ/(4b2) we have\n∥ ¯wt+1 − w∗∥2\n2 ≤\n\u0012\n1 − µ2\n32b2\n\u0013\n∥vt∥2\n2,\ncompleting the proof.\nWe proceed first under the condition that ∥vt∥2 ≥ (96/µ)(\n√\nOPT + √ϵ) holds for t = 0, . . . , T\nand show that after some certain number of iterations T this condition must be violated. Observe\nthat if ∥vt∥2 ≤ (96/µ)(\n√\nOPT + √ϵ), then it holds ∥wt − w∗∥2\n2 ≲ (1/µ2)(OPT + ϵ), implying that\nˆut(wt · x) is a hypothesis achieving constant approximation error according to Claim 4.4, hence the\nalgorithm can be terminated. However, note that T only works as an upper bound for the iteration\ncomplexity of our algorithm, and it is possible that the condition ∥vt∥2 ≥ (96/µ)(\n√\nOPT + √ϵ) is\nviolated at some step t∗ < T. However, as we show later, the value of ∥vT ∥2 cannot be larger than\nc∥vt∗∥2, where c is an absolute constant. We observe that:\nvt+1 = w∗ − (w∗ · wt+1)wt+1/∥wt+1∥2\n2 = w∗ − (w∗ · ¯wt+1) ¯wt+1/∥ ¯wt+1∥2\n2 = (w∗)⊥ ¯\nwt+1,\n40\ntherefore, ∥vt+1∥2\n2 ≤ ∥ ¯wt+1 − w∗∥2\n2, which, combined with Claim D.2, yields\n∥vt+1∥2\n2 ≤\n\u0012\n1 − µ2\n32b2\n\u0013\n∥vt∥2\n2 ≤\n\u0012\n1 − µ2\n32b2\n\u0013t\n∥v0∥2\n2 ≤ exp\n\u0012\n− µ2t\n32b2\n\u0013\n2W 2.\nThe above contraction only holds when ∥vt∥2 ≥ (96/µ)(\n√\nOPT + √ϵ). Hence, after at most\nT = O\n\u0012 b2\nµ2 log\n\u0012µW\nϵ\n\u0013\u0013\ninner iterations, the algorithm outputs a vector wt∗ with ∥vt∗∥2 ≤ 96\nµ (\n√\nOPT + √ϵ), where t∗ ∈ [T].\nNow suppose that at step t∗ < T it holds that ∥vt∗∥2 ≤ 96(\n√\nOPT + √ϵ)/µ but at the\nnext iteration ∥vt∗+1∥2 ≥ 96(\n√\nOPT + √ϵ)/µ. Recall first that in Lemma 4.3 we showed that\n∥∇ bLsur(bwt; ˆut)∥2\n2 ≤ 4b2∥bwt − w∗∥2\n2 + 10(OPT + ϵ). Therefore, revisiting the updating scheme of the\nalgorithm we have\n∥vt∗+1∥2\n2 ≤ ∥ ¯wt∗+1 − w∗∥2\n2 = ∥bwt∗ − η∇ bLsur(bwt∗; ˆut∗) − w∗∥2\n2\n≤ 2∥bwt∗ − w∗∥2\n2 + 2η2∥∇ bLsur(bwt∗; ˆut∗)∥2\n2\n≤ (2 + 8b2η2)∥bwt∗ − w∗∥2\n2 + 20η2(OPT + ϵ)\n≤ 3∥bwt∗ − w∗∥2\n2 + (OPT + ϵ),\nwhere in the last inequality we plugged in the value of η = µ/(4b2), and used the assumption that\nµ ≤ 1 and b ≥ 1, hence 20η2 ≤ 1 and 8b2η2 ≤ 1. Furthermore, recall that by the construction of the\ngrid, ∥bwt∗ − wt∥2 ≤ η√ϵ, implying that ∥bwt∗ − w∗∥2\n2 ≤ 2∥wt∗ − w∗∥2\n2 + 2η2ϵ by triangle inequality.\nTherefore, going back to the inequality of ∥vt∗+1∥2\n2 above, we get\n∥vt∗+1∥2\n2 ≤ 6∥wt∗ − w∗∥2\n2 + 6η2ϵ + OPT + ϵ ≤ 6∥wt∗ − w∗∥2\n2 + 2(OPT + ϵ).\nFinally, observe that since ∥wt∗∥2 = ∥w∗∥2, it holds ∥wt∗ − w∗∥2 ≤\n√\n2∥vt∗∥2, hence, we get\n∥vt∗+1∥2\n2 ≤ 12∥vt∗∥2\n2 + 2(OPT + ϵ).\nNow since ∥vt∗+1∥2 ≥ 96(\n√\nOPT+√ϵ)/µ, the value of ∥vt∥2\n2 will start to decrease again for t ≥ t∗+1.\nThis implies that the value of ∥vT ∥2 satisfies\n∥vT ∥2 ≤\n√\n12∥vt∗∥2 +\n√\n2(\n√\nOPT + √ϵ) ≤ 384\nµ (\n√\nOPT + √ϵ).\nCombining Claim 4.4 and Lemma F.4, as we have guaranteed that ∥vT ∥2 ≤ (384/µ)(\n√\nOPT+√ϵ),\nthe hypothesis ˆuT (bwT · x) has the L2\n2 error that can be bounded as:\nL2(bwT ; ˆuT ) ≤ 6OPT + 3b2(4∥vT ∥2\n2 + η2ϵ) + ϵ = O\n\u0012 b2\nµ2 (OPT + ϵ)\n\u0013\n.\nFor any ϵ1 > 0, setting ϵ = C′(µ2/b2)ϵ1 with C′ being some small universal absolute constant, we\nfinally get L2(bwT ; ˆuT ) ≤ O((b2/µ2)OPT) + ϵ1.\nIt still remains to determine the batch size as drawing a sample set of size m as displayed in (41)\nonly guarantees that the contraction of ∥vt∥2 at step t holds with probability 1−δ. Applying a union\nbound on all t0JT = O( Wb10\nµ9√ϵ log(1/ϵ)) = O( Wb11\nµ10√ϵ1 log(1/ϵ1)) iterations yields that the contraction\n41\nholds at every step with probability at least 1 − t0JTδ. Therefore, setting δ ← δ(t0JT) and bringing\nthe value of δ back to (41), we get that it suffices to choose the batch size as:\nm = Θ\n\u0012dW 9/2b4 log4(d/(ϵδ))\nL4\n\u0012 1\nϵ3/2 + Wb10\nµ9ϵ3/2δ\n\u0013\u0013\n= Θ\n\u0012dW 11/2b17 log5(d/(ϵ1δ))\nL4µ12δϵ3/2\n1\n\u0013\n,\nto guarantee that we get an O(OPT) + ϵ1-solution with probability at least 1 − δ. Note that we\nhave set ϵ = C′(µ2/b2)ϵ1 in the last equality above.\nThe argument above justifies the claim that among all t0J = Wb7 log(b/µ)/(ηµ7√ϵ1) hypotheses\nin P = {(bwT\nj ; ˆuT\nj )}t0J\nj=1, there exists at least one hypothesis that achieves L2\n2 error O(OPT) + ϵ1.\nTo select the correct hypothesis from the set P, one only needs to draw a new batch of m′ =\n˜Θ(b4W 4 log(1/δ)/(L4ϵ2\n1)) i.i.d. samples from D, and choose the hypothesis from P that achieves\nthe minimal empirical error defined in Algorithm 4. As discussed in Section 4.3, this procedure\nintroduces an error at most ϵ1.\nIn conclusion, it holds by a union bound that Algorithm 4 delivers a solution with O(OPT) + ϵ1\nerror with probability at least 1 − 2δ. The total sample complexity of our algorithm is\nN = t0JTm+m′ = Θ\n\u0012W 13/2b28d log5(d/(ϵ1δ))\nL4µ22δϵ2\n1\n+b4W 4 log(1/δ) log5(1/ϵ1)\nL4ϵ2\n1\n\u0013\n= Θ\n\u0012W 13/2b28d log6(d/(ϵ1δ))\nL4µ22δϵ2\n1\n\u0013\n.\nChoosing δ = 1/6 above we get that the Algorithm 4 succeeds to generate an O(OPT) + ϵ1-\nsolution for any ϵ1 > 0 with probability at least 1 − 2δ = 2/3, hence replacing ϵ1 with ϵ completes\nthe proof of Theorem D.1.\nD.2\nProof of Lemma 4.3\nThis subsection is devoted to the proof of Lemma 4.3. To this aim, we first show the following\nlemmas that bound from above the norm of the population gradient ∇Lsur(wt; ˆut) and the difference\nbetween the population gradient and the empirical gradient ∇ bLsur(wt; ˆut).\nLemma D.3. Let S be a sample set of m i.i.d. samples of size at least m ≳ d log4(d/(ϵδ))(b2W 3/L2ϵ)3/2.\nFurthermore, given wt ∈ B(W), let ˆut be defined as in (P). Then, it holds that with probability at\nleast 1 − δ,\n∥∇Lsur(wt; ˆut)∥2\n2 ≤ 8(OPT + ϵ) + 2b2∥wt − w∗∥2\n2.\nProof. By the definition of ℓ2 norms, we have:\n∥∇Lsur(wt; ˆut)∥2 = max\n∥v∥2=1 ∇Lsur(wt; ˆut) · v\n= max\n∥v∥2=1\nE\n(x,y)∼D[(ˆut(wt · x) − y)v · x]\n= max\n∥v∥2=1\n\u001a\nE\n(x,y)∼D[(ˆut(wt · x) − ut(wt · x) + ut(wt · x) − u∗t(wt · x))(v · x)]\n+\nE\n(x,y)∼D[(u∗t(wt · x) − u∗(w∗ · x) + u∗(w∗ · x) − y)(v · x)]\n\u001b\n.\n42\nBy the Cauchy-Schwarz inequality, we further have:\n∥∇Lsur(wt; ˆut)∥2\n≤ max\n∥v∥2=1\n\u001ar\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2]\nE\nx∼Dx[(v · x)2] +\nr\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2]\nE\nx∼Dx[(v · x)2]\n+\nr\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2]\nE\nx∼Dx[(v · x)2] +\nr\nE\nx∼Dx[(u∗(w∗ · x) − y)2]\nE\nx∼Dx[(v · x)2]\n\u001b\n≤\nr\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2]\n|\n{z\n}\nT1\n+\nr\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2]\n|\n{z\n}\nT2\n+\nr\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2]\n|\n{z\n}\nT3\n+\nr\nE\nx∼Dx[(u∗(w∗ · x) − y)2]\n|\n{z\n}\nT4\n,\nwhere in the last inequality we used the assumption that Ex∼Dx[xx⊤] ≼ I, hence Ex∼Dx[(v ·x)2] ≤ 1.\nIt remains to bound T1 − T4. Observe first that T1 ≤ √ϵ for every wt ∈ B(W), with probability\nat least 1 − δ, due to Lemma F.4.\nBy definition, T4 =\n√\nOPT.\nRecall that in Lemma 3.3\nwe showed the following T 2\n2 = Ex∼Dx[(ut(wt · x) − u∗t(wt · x))2] ≤ OPT.\nFor T3, note that\nu∗t ∈ argminu∈U(a,b) Ex∼Dx[(u(wt · x) − u∗(w∗ · x))2], therefore, since u∗ ∈ U(a,b), we have\nT 2\n3 =\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2] ≤\nE\nx∼Dx[(u∗(wt · x) − u∗(w∗ · x))2] ≤ b2∥wt − w∗∥2\n2,\nafter applying the assumption that u∗ is b-Lipschitz. Thus, in conclusion, we have\n∥∇Lsur(wt; ˆut)∥2 ≤ 2\n√\nOPT + √ϵ + b∥wt − w∗∥2.\nFurthermore, since (a + b)2 ≤ 2a2 + 2b2 for any a, b ∈ R, we get with probability at least 1 − δ:\n∥∇Lsur(wt; ˆut)∥2\n2 ≤ 8OPT + 8ϵ + 2b2∥wt − w∗∥2\n2,\ncompleting the proof of Lemma D.3.\nWe now prove that the distance between ∇Lsur(wt; ˆut) and ∇ bLsur(wt; ˆut) is bounded by b2∥wt −\nw∗∥2\n2 + OPT + ϵ with high probability.\nLemma D.4. Let S be a sample set of m ≳ (dW 9/2b4 log4(d/(ϵδ))/L4)(1/ϵ3/2+1/(ϵδ)) i.i.d. samples.\nGiven a vector wt ∈ B(W), it holds that with probability at least 1 − δ,\n∥∇ bLsur(wt; ˆut) − ∇Lsur(wt; ˆut)∥2 ≤\nq\nb2∥wt − w∗∥2\n2 + OPT + ϵ.\nProof. Since for any zero-mean independent random variable zj, we have E[|| P\nj zj||2\n2] = P\nj E[∥zj∥2\n2],\nby Chebyshev’s inequality:\nPr[∥∇ bLsur(wt; ˆut) − ∇Lsur(wt; ˆut)∥2 ≥ s] ≤\n1\nms2\nE\n(x,y)∼D[∥(ˆut(wt · x) − y)x∥2\n2] .\n(48)\nBy linearity of expectation, we have:\nE\n(x,y)∼D[∥(ˆut(wt · x) − y)x∥2\n2] =\nd\nX\nk=1\nE\n(x,y)∼D[(ˆut(wt · x) − y)2(xk)2],\n43\nwhere xk = ek · x and ek is the kth unit basis of Rd. Let r = O(W/L log(1/(Lϵ′))), then it holds\nPr[|xk| ≥ r] ≤ ϵ′. Then, the variance above can be decomposed into the following parts:\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk] =\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk1{|xk| ≥ r}]\n+\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk1{|xk| ≤ r}].\nSince |y| ≤ M = O(bW/L log(bW/ϵ)), and Ex∼Dx[(wt · x)4x4\nk] ≤ W 4c2/L8, Ex∼Dx[x4\nk] ≤ c2/L4 for\nDx is L-sub-exponential, we have\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk1{|xk| ≥ r}] ≤ 2\nE\n(x,y)∼D[(ˆut(wt · x))2 + y2)x2\nk1{|xk| ≥ r}]\n≤ 2\nE\n(x,y)∼D[(b(wt · x))2 + y2)x2\nk1{|xk| ≥ r}]\n≤ 2b2\nr\nE\nx∼Dx[((wt · x)4x4\nk] Pr[|xk| ≥ r]\n+ 2M2\nr\nE\nx∼Dx[x4\nk] Pr[|xk| ≥ r]\n≤ (2cb2W 2/L4)\n√\nϵ′ + (2cM2/L2)\n√\nϵ′ ≤ (4cM2/L2)\n√\nϵ′.\n(49)\nIn addition, (ˆut(wt · x) − y)2 can be decomposed as the following:\nE\n(x,y)∼D[(ˆut(wt · x) − y)2] ≤ 4\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2] + 4\nE\nx∼Dx[(ut(wt · x) − u∗t(wt · x))2]\n+ 4\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2] + 4\nE\n(x,y)∼D[(u∗(w∗ · x) − y)2].\nThe first term is bounded above by 4ϵ with probability at least 1 − δ for every wt ∈ B(W) whenever\nm ≳ d log4(d/(ϵδ))(b2W 3/L2ϵ)3/2, as proved in Lemma F.4. The second term is smaller than 4OPT,\nwhich is shown in Lemma 3.3. The third term can be bounded above using again the definition of\nu∗t = argminu∈U(a,b) Ex∼Dx[(u(wt · x) − y∗)2], as\n4\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2] ≤ 4\nE\nx∼Dx[(u∗(wt · x) − u∗(w∗ · x))2] ≤ 4b2∥wt − w∗∥2\n2,\nusing the fact that u∗ is b-Lipschitz and Ex∼Dx[xx⊤] ≼ I. Lastly, the fourth term is bounded by\n4OPT by the definition of u∗(w∗ · x). In summary, we have\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk1{|xk| ≤ r}] ≤ r2\nE\n(x,y)∼D[(ˆut(wt · x) − y)2]\n≤ 4r2(b2∥wt − w∗∥2\n2 + 2OPT + ϵ),\nwhich, combining with (49), implies that the expectation E(x,y)∼D[(ˆut(wt · x) − y)2x2\nk] is bounded\nby:\nE\n(x,y)∼D[(ˆut(wt · x) − y)2x2\nk] ≤ 4r2b2∥wt − w∗∥2\n2 + 4r2(2OPT + 2ϵ)\n≤ CW 2\nL2\nlog2\n\u0012 b\nLϵ\n\u0013\n(b2∥wt − w∗∥2\n2 + OPT + ϵ),\n44\nwhere C is a large absolute constant. Note to get the inequality above we chose ϵ′ = Cϵ2(L/b)4,\nwhich then indicates that 4c(M/L)2√\nϵ′ ≤ r2ϵ. Summing the inequality above from k = 1 to d\ndelivers the final upper bound on the variance:\nE\n(x,y)∼D[∥(ˆut(wt · x) − y)x∥2\n2] ≤ dCW 2\nL2\nlog2\n\u0012 b\nLϵ\n\u0013\n(b2∥wt − w∗∥2\n2 + OPT + ϵ).\nThus, plugging the upper bound on the variance above back to (48), as long as m ≳ (dW 2/L2) log2(b/(Lϵ))/δ,\nwe get with probability at least 1 − δ,\n∥∇ bLsur(wt; ˆut) − ∇Lsur(wt; ˆut)∥2 ≤\nq\nb2∥wt − w∗∥2\n2 + OPT + ϵ.\nNoting that m ≳ (dW 9/2b4 log4(d/(ϵδ))/L4)(1/ϵ3/2 + 1/(ϵδ)) certainly satisfies the condition on m\nabove as m ≳ (dW 2/L2) log2(b/(Lϵ))/δ, thus, we completed the proof of Lemma D.4\nWe can now proceed to the proof of Lemma 4.3 (detailed statement in Lemma D.5 below), which\ncan be derived directly from the preceding lemmas.\nLemma D.5 (Upper Bound on Empirical Gradient Norm). Let S be a set of i.i.d. samples of\nsize m ≳ (dW 9/2b4 log4(d/(ϵδ))/L4)(1/ϵ3/2 + 1/(ϵδ)). Given any wt ∈ B(W), let ˆut ∈ U(a,b) be the\nsolution of optimization problem (P) with respect to wt and sample set S. Then, with probability at\nleast 1 − δ, we have that ∥∇ bLsur(wt; ˆut)∥2\n2 ≤ 4b2∥wt − w∗∥2\n2 + 10(OPT + ϵ).\nProof. The lemma follows directly by combining Lemma D.3, Lemma D.4 and the triangle inequality.\nD.3\nProof of Claim 4.4\nWe restate (providing a more detailed statement for the sample size) and prove Claim 4.4.\nClaim D.6. Let w be any vector from B. Let ˆuw be a solution to (P) for a fixed parameter vector\nw ∈ Rd with sample size m ≳ d log4(d/(ϵδ))(b2W 3/(L2ϵ))3/2. Then\nE\n(x,y)∼D[(ˆuw(w · x) − y)2] ≤ 8(OPT + ϵ) + 4b2∥w − w∗∥2\n2.\nProof. Let u∗\nw, uw be the optimal activations for problems (EP*) and (EP) under parameter w,\nrespectively. Then, a direct calculation gives:\nE\n(x,y)∼D[(ˆuw − y)2]\n=\nE\n(x,y)∼D[(ˆuw(w · x) − uw(w · x) + uw(w · x) − u∗\nw(w · x) + u∗\nw(w · x) − u∗(w∗ · x) + u∗(w∗ · x) − y)2]\n≤ 4\nE\nx∼Dx[(ˆuw(w · x) − uw(w · x))2] + 4\nE\nx∼Dx[(uw(w · x) − u∗\nw(w · x))2]\n+ 4\nE\nx∼Dx[(u∗\nw(w · x) − u∗(w∗ · x))2] + 4OPT\n≤ 8(OPT + ϵ) + 4b2∥w − w∗∥2\n2,\n(50)\nwhere in the second inequality we used the results from Lemma 3.3, Lemma F.4 and we applied the\nobservation that:\nE\nx∼Dx[(u∗\nw(w · x) − u∗(w∗ · x))2] ≤\nE\nx∼Dx[(u∗(w · x) − u∗(w∗ · x))2] ≤ b2∥w − w∗∥2\n2,\nby the definition of u∗\nw.\n45\nD.4\nProof of Claim 4.5\nWe restate Claim 4.5 and show the number of samples needed for the testing subroutine Algorithm 3.\nClaim 4.5. Let µ, ϵ1, δ ∈ (0, 1) be fixed. Let r = 1\nL log( Cb4W 4\nL6ϵ2\n1 log2( bW\nϵ1 )), where C is a sufficiently\nlarge absolute constant. Given a set of parameter-activation pairs P = {(wj; uj)}t0J\nj=1 such that\nwj ∈ B(W) and uj ∈ U(a,b) for j ∈ [t0J], where t0J = 4b9W/(µ8√ϵ1), we have that using\nm′ = Θ\n\u0012b4W 4 log(1/δ)\nL4ϵ2\n1\nlog5\n\u0012 bW\nLµϵ1\n\u0013\u0013\n,\ni.i.d. samples from D, for any (wj; uj) ∈ P it holds with probability at least 1 − δ,\n\f\f\f\f\n1\nm′\nm′\nX\ni=1\n(uj(wj · x(i)) − y(i))21{|wj · x(i)| ≤ Wr} −\nE\n(x,y)∼D[(uj(wj · x) − y)2]\n\f\f\f\f ≤ 2ϵ1.\nProof. Fix (wj, uj) ∈ P. Since Dx is sub-exponential, we have Pr[|wj ·x| ≥ ∥wj∥2r] ≤\n1\nL2 exp(−Lr).\nConsider random variables Zi,j = (uj(wj ·x(i))−y(i))21{|w ·x(i)| ≤ r}, i = 1, · · · , m, j = 1, · · · , t0J,\nwhere (x(i), y(i)) are independent random variables drawn from D. Using Fact F.3 (Appendix F),\nwe can truncate the labels y such that |y| ≤ M, where M = C(bW/L) log(bW/ϵ1) for some large\nabsolute constant C. Hence, |Zi,j| ≤ 2(u2\nj(wj · x(i)) + (y(i))2)1{|wj · x(i)| ≤ Wr} ≤ 2(b2W 2r2 + M2),\nwhere we used the assumption that u is b-Lipschitz in the last inequality. Therefore, applying\nHoeffding’s inequality to Zi,j we get:\nPr\n\u0014\f\f\f\f\nm′\nX\ni=1\n(Zi,j − E[Zi,j])\n\f\f\f\f ≥ m′t\n\u0015\n≤ 2 exp\n\u0012\n−\nm′t2\n8(b2W 2r2 + M2)2\n\u0013\n.\nSince there are t0J = Wb7/(µ7η√ϵ1) = 4b9W/(µ8√ϵ1) elements in the set P, applying a union\nbound leads to:\nPr\n\u0014\f\f\f\f\nm′\nX\ni=1\nZi,j − E[Zi,j]\n\f\f\f\f ≥ m′t, ∀j ∈ [J]\n\u0015\n≤ 2 exp\n\u0012\n−\nm′t2\n8(b2W 2r2 + M2)2 + log(4b9W/(µ8√ϵ1))\n\u0013\n.\nTherefore, when\nm′ ≥ 8(b2W 2r2 + M2)2\nϵ2\n1\n\u0012\nlog\n\u0012 4b9W\nµ8√ϵ1\n\u0013\n+ log(2/δ)\n\u0013\n,\n(51)\nwe have that with probability at least 1 − δ:\n\f\f\f\f\n1\nm′\nm′\nX\ni=1\n(uj(wj·x(i))−y(i))21{|wj·x(i)| ≤ Wr}−\nE\n(x,y)∼D[(uj(wj·x)−y)21{|wj·x| ≤ Wr}]\n\f\f\f\f ≤ ϵ1, (52)\nfor any (wj, uj) ∈ P. In addition, as Pr[|wj · x| ≥ Wr] ≤ Pr[|wj · x| ≥ ∥wj∥2r] ≤\n2\nL2 exp(−Lr),\nletting ϵ′ =\n2\nL2 exp(−Lr), we further have:\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≥ Wr}]\n≤ 2\nE\nx∼Dx[((uj(wj · x))2 + M2)1{|wj · x| ≥ Wr}]\n≤ 2b2r\nE\nx∼Dx[(wj · x)4] Pr[|wj · x| ≥ Wr] + M2 Pr[|wj · x| ≥ Wr]\n≤ 2cb2(W/L)2√\nϵ′ + M2ϵ′ ≤ (2cb2(W/L)2 + M2)\n√\nϵ′,\n46\nwhere in the second inequality we used Cauchy-Schwarz inequality and in the last inequality we used\nthe property that for any unit vector a it holds E[(a · x)4] ≤ c2/L4 for some absolute constant c as x\npossesses a 1\nL-sub-exponential tail. Therefore, choosing r = 1\nL log( C2b4W 4\nL6ϵ2\n1\nlog2( bW\nϵ1 )) = ˜O( 1\nL log( bW\nLϵ1 ))\nfor some large absolute constant C renders\n√\nϵ′ ≤ ϵ1/(2Cb2(W/L)2 log2(bW/ϵ1)), and we have\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≥ Wr}] ≤ ϵ1.\nObserve that as E(x,y)∼D[(uj(wj · x) − y)2] is the sum of E(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≥ Wr}]\nand E(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≤ Wr}], we have\n0 ≤\nE\n(x,y)∼D[(uj(wj · x) − y)2] −\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≤ Wr}]\n≤\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≥ Wr}] ≤ ϵ1.\nPlugging the choice of r back into (51), we get that it is sufficient to choose m′ as\nm′ = C log(log(1/ϵ1))\nϵ2\n1\n\u0012\nb2\n\u0012W\nL\n\u00132\nlog2\n\u0012bW\nLϵ2\n1\n\u0013\u00132\u0012\nlog\n\u0012 4b9W\nµ8√ϵ1\n\u0013\n+log(1/δ)\n\u0013\n= ˜Θ\n\u0012b4W 4 log(1/δ)\nL4ϵ2\n1\nlog5\n\u0012 bW\nLµϵ1\n\u0013\u0013\n.\nTherefore, using m′ = ˜Ω(b4W 4/(L4ϵ2\n1)) samples, (52) indicates that with probability at least 1 − δ,\nfor any (wj, uj) ∈ P it holds\n\f\f\f\f\n1\nm′\nm′\nX\ni=1\n(uj(wj · x(i)) − y(i))21{|wj · x(i)| ≤ Wr} −\nE\n(x,y)∼D[(uj(wj · x) − y)2]\n\f\f\f\f\n≤\n\f\f\f\f\n1\nm′\nm′\nX\ni=1\n(uj(wj · x(i)) − y(i))21{|wj · x(i)| ≤ Wr} −\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≤ Wr}]\n\f\f\f\f\n+\n\f\f\f\f\nE\n(x,y)∼D[(uj(wj · x) − y)2] −\nE\n(x,y)∼D[(uj(wj · x) − y)21{|wj · x| ≤ Wr}]\n\f\f\f\f\n≤ 2ϵ1,\nthus completing the proof of Claim 4.5.\nE\nEfficiently Computing the Optimal Empirical Activation\nIn this section, we show that the optimization problem (P) can be solved efficiently, following\nthe framework from [LH22] with minor modifications. We show that, for any ϵ > 0, there is\nan efficient algorithm that runs in ˜O(m2 log(1/ϵ)) time and outputs a solution ˆvt(z) such that\n∥ˆvt(z) − ˆut(z)∥∞ ≤ ϵ. We then argue that using such approximate solutions to the optimization\nproblem (P) does not negatively impact our error guarantee, sample complexity, or runtime (up to\nconstant factors).\nProposition E.1 (Approximating the Optimal Empirical Activation). Let ϵ > 0, and Dx be (L, R)-\nwell behaved. Let ˆut ∈ U(a,b) be the optimal solution of the optimization problem (P) given a sample\nset S of size m drawn from D and a parameter wt ∈ B(W). There exists an algorithm that produces\nan activation ˆvt ∈ U(a,b) such that ∥ˆvt − ˆut∥∞ ≤ ϵ, with runtime ˜O(m2 log(bW/(Lϵ))).\nTo prove Proposition E.1, we leverage the following result:\n47\nLemma E.2 (Section 5 [LH22]). Let fi(y) and hi(y) be any convex lower semi-continuous functions\nfor i = 1, . . . , m. Consider the following convex optimization problem\n(ˆy1, . . . , ˆym) = argmin\ny1,...,ym\nm\nX\ni=1\nfi(yi) +\nm−1\nX\ni=1\nhi(yi − yi+1),\n(53)\nwhere yi ∈ [−U, U] for all i = 1, . . . , m for some positive constant U. Then, for any ϵ > 0, there\nexists an algorithm (the cc-algorithm [LH22]) that outputs an ϵ-close solution {y1, . . . , ym} such that\n|yi − ˆyi| ≤ ϵ for all i ∈ [m] with runtime O(m2 log(U/ϵ)).\nProof of Proposition E.1. We first reformulate problem (P) as a quadratic optimization problem\nwith linear constraints. To guarantee that ˆut is an element in U(a,b) that satisfies ˆut(0) = 0, we add\na zero point (x(0), y(0)) = (0, 0) to the data set S if S does not contain (0, 0) in the first place. We\ncan thus assume without loss of generality that the data set contains (0, 0). Denote zi = w · x(i)\nsuch that z1 ≤ z2 ≤ · · · ≤ zm after rearranging the order of (x(i), y(i))’s, and suppose zk = w · x0 = 0\nfor a k ∈ [m]. Then (P) is equivalent to the following optimization problem:\n(ˆy(1), · · · , ˆy(m)) = argmin\n˜y(i),i∈[m]\nm\nX\ni=1\n(˜y(i) − y(i))2\ns.t.\n0 ≤ ˜y(i+1) − ˜y(i),\n1 ≤ i ≤ k − 1,\na(zi+1 − zi) ≤ ˜y(i+1) − ˜y(i),\n1 ≤ i ≤ k − 1,\n˜y(i+1) − ˜y(i) ≤ b(zi+1 − zi),\n1 ≤ i ≤ m − 1,\n˜y(k) = 0.\nDefine hi(y) = I[−b(zi+1−zi),0](y) for i = 1 . . . , k − 1, hi(y) = I[−b(zi+1−zi),−a(zi+1−zi)](y) for\ni = k . . . , m − 1, where IY(y) is the indicator function of a convex set Y, i.e., IY(y) = 0 if y ∈ Y\nand IY(y) = +∞ otherwise. It is known that hi’s are convex and sub-differentiable on their domain\nYi. In addition, let fi(y) = 1\n2(y − y(i))2 for i ̸= k and fk(y) = I{0}(y). Then, we have the following\nformulation for problem (P):\n(ˆy(1), · · · , ˆy(m)) =\nargmin\n˜y(i),i=1,...,m\nm\nX\ni=1\nfi(˜y(i)) +\nm−1\nX\ni=1\nhi(˜y(i) − ˜y(i+1))\n(P1)\nNote that the functions fi and hi we defined above satisfy the conditions of Lemma E.2. Thus, it\nonly remains to find the bounds on the variables ˜y(i). This is easy to achieve as all ˜y(i) must satisfy\n|˜y(i)| ≤ b|zi| = b|w · x(i)| and we know that x(i) are sub-exponential random variables. Therefore,\nfollowing the same idea from the proof of Lemma F.2, we know that for U = 2W\nL log(m/(Lδ)), it holds\nthat with probability at least 1−δ, |˜y(i)| ≤ b|w·x(i)| ≤ bU for all i ∈ [m]. Hence, applying Lemma E.2\nto problem (P1), we get that it can be solved within ϵ-error in runtime ˜O(m2 log(bW/(Lϵ))).\nThe effect of approximation error in (P)\nSince the solution ˆvt is ϵ-close to ˆut, this approximated\nsolution will only result in an ϵ-additive error in the sharpness result Proposition 3.1 and the gradient\nnorm concentration Lemma 4.3. In more detail, for the result of Proposition 3.1, we have\n\f\f\f\f(∇ bLsur(wt; ˆvt) − ∇ bLsur(wt; ˆut)) · (wt − w∗)\n\f\f\f\f =\n\f\f\f\f\n1\nm\nm\nX\ni=1\n(ˆvt(wt · x(i)) − ˆut(wt · x(i)))(wt − w∗) · x(i)\n\f\f\f\f\n≤ ϵ\nm\nm\nX\ni=1\n\f\f(wt − w∗) · x(i)\f\f ≤ 2ϵU,\n48\nsince |wt · x(i)| ≤ U and |w∗ · x(i)| ≤ U with probability at least 1 − δ. Therefore, choosing ϵ′ = ϵ/U\nwe have that Proposition 3.1 holds for approximate activations ˆvt with an additional ϵ error. Observe\nthat this does not affect the approximation factor in our final O(OPT) + ϵ result, while the value\nof ϵ only needs to be rescaled by a constant factor, effectively increasing the sample size and the\nruntime by constant factors.\nLet us denote the unit ball by B. For the gradient norm concentration lemma Lemma 4.3, note\nthat at any iteration t, it always holds that\n∥∇ bLsur(wt; ˆvt) − ∇ bLsur(wt; ˆut)∥2 = max\nv∈B\n1\nm\nm\nX\ni=1\n(ˆvt(wt · x(i)) − ˆut(wt · x(i)))x(i) · v ≤ max\nv∈B\nϵ\nm\nm\nX\ni=1\n|x(i) · v|.\nSince Ex∼Dx[xx⊤] ≼ I and v ∈ B, we have Ex∼Dx[|x · v|] ≤\np\nE[(x · v)2] ≤ 1. Now since |x(i) · v|\nare independent 1/L-sub-exponential random variables, applying Bernstein’s inequality it holds that\nfor any v ∈ B and an absolute constant c,\nPr\n\u0014\f\f\f\f\n1\nm\nm\nX\ni=1\n|x(i) · v| −\nE\nx∼Dx[|x · v|]\n\f\f\f\f ≥ s\n\u0015\n≤ 2 exp\n\u0012\n− c min\n\u001a m2s2\nm/L2 , ms\n1/L\n\u001b\u0013\n= 2 exp(−cmL2s2).\nLet N(B, ϵ; ℓ2) be the ϵ-net of the unit ball B. Note that the cover number of these v ∈ B is of\norder (1/ϵ)O(d), therefore, applying a union bound on N(B, ϵ; ℓ2) and for all t0JT = O(log(1/ϵ)/√ϵ)\niterations, and setting s = 1, it holds\nPr\n\u0014\n∀v ∈ N(B, ϵ; ℓ2),\n\f\f\f\f\n1\nm\nm\nX\ni=1\n|x(i) · v| −\nE\nx∼Dx[|x · v|]\n\f\f\f\f ≥ 1\n\u0015\n≤ 2 exp(−cmL2 + c′d log(1/ϵ)) ≤ δ,\nwhere the last inequality comes from the fact that we have m ≳ W 9/2b14d log(1/δ) log4(d/ϵ)/(L4µ9δϵ3/2)\nas the batch size. Let v∗ = argmaxv∈B\nPm\ni=1 |x(i) · v|. Then there exists a v′ ∈ N(B, ϵ; ℓ2) such that\n∥v′ − v∗∥2 ≤ ϵ and hence,\n1\nm\nm\nX\ni=1\n|x(i) · v∗| ≤ 1\nm\nm\nX\ni=1\n|x(i) · (v∗ − v′)| + 1\nm\nm\nX\ni=1\n|x(i) · v′|\n= ϵ\nm\nm\nX\ni=1\n|x(i) · v∗ − v′\nϵ\n| + 1\nm\nm\nX\ni=1\n|x(i) · v′|\n≤ ϵ\nm\nm\nX\ni=1\n|x(i) · v∗| + 1\nm\nm\nX\ni=1\n|x(i) · v′|,\nwhere the last inequality comes from the observation that as (v∗ − v′)/ϵ ≤ B, it holds Pm\ni=1 |x(i) ·\n((v∗ − v′)/ϵ)| ≤ Pm\ni=1 |x(i) · v∗|, by the definition of v∗. Therefore, with probability at least 1 − δ we\nhave\n1\nm\nm\nX\ni=1\n|x(i) · v∗| ≤\n1\n1 − ϵ\n1\nm\nm\nX\ni=1\n|x(i) · v′| ≤ 2(1 +\nE\nx∼Dx[|v · x|]) ≤ 4.\nThis implies that ∥∇ bLsur(wt; ˆvt)∥2 ≤ ∥∇ bLsur(wt; ˆut)∥2 + 4ϵ for all iterations with probability at\nleast 1 − δ. Therefore, Lemma 4.3 continues to hold for the ϵ-approximate activation ˆvt.\nThus, we have that the inequalities (40) and (42) in the proof of Theorem 4.2 remain valid for\nϵ-approximate ˆvt, and hence the results in Theorem 4.2 are unchanged.\n49\nF\nUniform Convergence of Activations\nIn this section, we review and provide standard uniform convergence results showing that the\nsample-optimal activations concentrate around their population-optimal counterparts. We first\nbound the L2\n2 distance between the sample-optimal and population-optimal activations under wt.\nTo do so, we build on Lemma 8 in [KKSK11]. Note that Lemma 8 from [KKSK11] only works\nfor bounded 1-Lipschitz activations u : R 7→ [0, 1], hence it is not directly applicable to our case.\nFortunately, since Dx has a sub-exponential tail (see Definition 1.2), we are able to bound the range\nof u(w · x) for u ∈ U(a,b) and w ∈ B(W) with high probability. Concretely, we prove the following\nlemma. Note that in the lemma statement, ˆu∗t is a random variable defined w.r.t. the (random)\ndataset S∗, and thus the probabilistic statement is for this random variable.\nWe make use of the following fact from [KKSK11]:\nFact F.1 (Lemma 8 [KKSK11]). Let V be the set of non-deceasing 1-Lipschitz functions such that\nv : R → [0, 1], ∀v ∈ V. Given Sm = {(x(i), y(i))}m\ni=1, where (x(i), y(i)) are sampled i.i.d. from some\ndistribution D′, let\nˆvw ∈ argmin\nv∈V\n1\nm\nm\nX\ni=1\n(v(w · x(i)) − y(i))2.\nThen, with probability at least 1−δ over the random dataset Sm, for any w ∈ B(W) it holds uniformly\nthat\nE\n(x,y)∼D′[(ˆvw(w · x) − y)2] − inf\nv∈V\nE\n(x,y)∼D′[(v(w · x) − y)2] = O\n\u0012\nW\n\u0012d log(Wm/δ)\nm\n\u00132/3\u0013\n.\nThe first lemma states that with sufficient many of samples, the idealized sample-optimal\nactivation ˆu∗t defined as the optimal solution of (P*) is close to its population counterpart u∗t, the\noptimal solution of (EP*).\nLemma F.2 (Approximating Population-Optimal Noiseless Activation by Sample-Optimal). Let\nDx be (L, R)-well behaved and let wt ∈ B(W). Provided a dataset S∗ = {(x(i), y∗(i))}, where x(i) are\ni.i.d. samples from Dx and y∗(i) = u∗(w∗ · x(i)), let ˆu∗t be the sample-optimal activation on S∗ as\ndefined in (P*). In addition, let u∗t be the corresponding population-optimal activation, following the\ndefinition in (EP*). Then, for any ϵ, δ > 0, if the size m of the dataset S∗ is sufficiently large\nm ≳ d log4(d/(ϵδ))\n\u0012b2W 3\nL2ϵ\n\u00133/2\n,\nwe have that with probability at least 1 − δ, for any wt ∈ B(W):\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2] ≤\nE\nx∼Dx[(u∗t(wt · x) − u∗(w∗ · x))2] + ϵ ,\nand, furthermore,\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] ≤ ϵ.\nProof. Our goal is to show that with high probability, the sample-optimal activation ˆu∗t ∈ U(a,b) and\nthe population optimal activation u∗t ∈ U(a,b) can be scaled to 1-Lipschitz functions mapping R to\n[0, 1], then, Fact F.1 can be applied.\nSince x possesses a sub-exponential tail, for any w ∈ B(W) we have Pr[|w · x| ≥ ∥w∥2r] ≤\n2\nL2 exp(−Lr). Therefore, with probability at least 1 − (δ1/m)2 it holds |w · x| ≤ 2W\nL log(m/(Lδ1)).\n50\nSince we have m samples, a union bound on these m samples yields that with probability at least\n1 − δ2\n1/m it holds |w · x(i)| ≤ 2W\nL log(m/(Lδ1)), for any given w ∈ B(W). Let r = 2W\nL log(m/(Lδ1)).\nIn the remainder of the proof, we assume that wt · x(i) ≤ r holds for every x(i) in the dataset S∗,\nwhich happens with probability at least 1 − δ2\n1/m ≥ 1 − δ1.\nLet V be the set of non-decreasing 1-Lipschitz functions v : R → [0, 1] such that v(0) = 1/2, and\nv(z1) − v(z2) ≥ (a/(2br))(z1 − z2) for all z1 ≥ z2 ≥ 0. We observe that restricted on the interval\n|z| ≤ r, (ˆu∗t(z)/(2br) + 1/2)||z|≤r is 1-Lipschitz, non-decreasing and bounded in the interval [0, 1].\nThus, (ˆu∗t(z)/(2br) + 1/2)||z|≤r = ˆv∗t(z)||z|≤r, for some ˆv∗t ∈ V. Furthermore, under the condition\nthat |wt · x(i)| ≤ r, since (ˆu∗t(z)/(2br) + 1/2)||z|≤r = ˆv∗t(z)||z|≤r, we observe that v∗t(z) is the\noptimal activation in the function space V, given the dataset S∗ and parameter wt, i.e.,\nˆv∗t ∈ argmin\nv∈V\n1\nm\nm\nX\ni=1\n(v(wt · x(i)) − (u∗(w∗ · x(i))/(2br) + 1/2))2.\nIn other words, ˆu∗t(z)/(2br) + 1/2 is the sample-optimal activation in the function class V when\nrestricted to the interval |z| ≤ r. Consider x ∼ Dx. Then Pr[|wt · x| ≥ r] ≤ (δ1/m)2 and for any\nwt ∈ B(W), the expectation Ex∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2] can be decomposed into the following\nterms\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2] =\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))21{|wt · x| ≤ r}]\n+\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))21{|wt · x| > r}]\n≤\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))21{|wt · x| ≤ r}]\n+ 2\nE\nx∼Dx[(ˆu∗t(wt · x))2 + (u∗(w∗ · x))21{|wt · x| > r}] .\n(54)\nSince both ˆu∗t and u∗ are (a, b)-unbounded functions such that ˆu∗t(0) = u∗(0) = 0, we have\n(ˆu∗t(wt · x))2 ≤ b2W 2((wt/∥wt∥2) · x)2 and similarly, (u∗(w∗ · x))2 ≤ b2W 2((w∗/∥w∗∥2) · x)2.\nFurthermore, since for any unit vector a, the random variable a · x follows a (1/L)-sub-exponential\ndistribution as Dx is (L, R)-well behaved, thus, it holds that Ex∼Dx[(a·x)4] ≤ c/L4 for some absolute\nconstant c. Therefore, after applying Cauchy-Schwarz inequality to E[(ˆu∗t(wt · x))21{|wt · x| ≥ r}],\nwe get\nE[(ˆu∗t(wt · x))21{|wt · x| ≥ r}] ≤ b2W 2r\nE\nx∼Dx[((wt/∥wt∥2) · x)4] Pr[|wt · x| ≥ r]\n≤ cb2W 2δ1/(L2m),\n(55)\nand similarly, E[(u∗(w∗ · x))21{|wt · x| ≥ r}] ≤ cb2W 2δ1/(L2m). Thus, plugging these inequalities\nback into (54), we get\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2] ≤\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))21{|wt · x| ≤ r}]\n+ 2cb2W 2δ1/(L2m).\nWe are now ready to apply Fact F.1 (note that V is a smaller function class compared to the class\nof 1-Lipschitz functions described Fact F.1, hence Fact F.1 applies). Denote A = {x : |wt · x| ≤ r}.\nLet y′ = y∗/(2br) + 1/2, y∗ = u∗(w∗ · x). Since conditioning on A, ˆu∗t(z)/(2br) + 1/2 is the\n51\nsample-optimal activation, applying Fact F.1 we get that with probability at least 1 − δ2:\nE\nx∼Dx[(ˆu∗t(wt · x)/(2br) + 1/2 − (u∗(w∗ · x)/(2br) + 1/2))2|A]\n=\nE\nx∼Dx[(ˆv∗t(wt · x) − y′)2|A]\n≤ inf\nv∈V\nE\nx∼Dx[(v(wt · x) − y′)2|A] + ˜O(W(d log(m/δ2)/m)2/3).\nLet V||z|≤r and U(a,b)||z|≤r be the functions from V and U(a,b) restricted on the interval |z| ≤ r,\nrespectively. It is not hard to see that by the definition of U(a,b) and V, (U(a,b)||z|≤r)/(2br) + 1/2 ⊂\nV||z|≤r. Therefore,\ninf\nv∈V\nE\nx∼Dx[(v(wt · x) − y′)2|A] ≤ inf\nu∈U\nE\nx∼Dx[(u(wt · x)/(2br) + 1/2 − y′)2|A]\n≤\n1\n4b2r2 inf\nu∈U\nE\nx∼Dx[(u(wt · x) − y∗)2|A].\nHence, with probability at least 1 − δ2,\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))21{A}]\n= 4b2r2\nE\nx∼Dx[(ˆv∗t(wt · x) − y′)2|A] Pr[A]\n≤ 4b2r2 inf\nv∈V\nE\nx∼Dx[(v(wt · x) − y′)2|A] Pr[A] + ˜O(b2r2W(d log(m/δ2)/m)2/3) Pr[A]\n≤ inf\nu∈U\nE\nx∼Dx[(u(wt · x) − u∗(w∗ · x))21{A}] + ˜O(b2r2W(d log(m/δ2)/m)2/3)\n≤ inf\nu∈U\nE\nx∼Dx[(u(wt · x) − u∗(w∗ · x))2] + ˜O(b2r2W(d log(m/δ2)/m)2/3) .\nSetting δ1 = δ2 = δ/2 and plugging everything back into (56), we finally get that with probability at\nleast 1 − δ,\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2]\n≤ inf\nu∈U\nE\nx∼Dx[(u(wt · x) − u∗(w∗ · x))2] + O\n\u0012b2W 3\nL2\nlog2\n\u0012 m\nLδ\n\u0013\u0012d log(m/δ)\nm\n\u00132/3\u0013\n.\nTo complete the first part of the claim, it remains to choose m as the following value\nm = Θ\n\u0012\nd log4(d/(ϵδ))\n\u0012b2W 3\nL2ϵ\n\u00133/2\u0013\n.\nFor the second part of the claim, note that U(a,b) is a closed convex set of functions, and that the\ninfimum infu∈U Ex∼Dx[(u(wt · x) − u∗(w∗ · x))2] is attained by u∗t(z). Observe that we have shown\nthat with the sample size m specified above, with probability at least 1 − δ, it holds\nϵ ≥\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗(w∗ · x))2 − (u∗t(wt · x) − u∗(w∗ · x))2]\n=\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))(ˆu∗t(wt · x) + u∗t(wt · x) − 2u∗(w∗ · x))]\n=\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] + 2\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))(u∗t(wt · x) − u∗(w∗ · x))].\n52\nSince ˆu∗t(z) ∈ U(a,b), applying the second part of Claim C.1 with v′ = ˆu∗t we get\nE\nx∼Dx[(u∗t(wt · x) − ˆu∗t(wt · x))(u∗(w∗ · x) − u∗t(wt · x))] ≥ 0.\nThus, we have:\nE\nx∼Dx[(ˆu∗t(wt · x) − u∗t(wt · x))2] ≤ ϵ.\nThis completes the proof of Lemma F.2.\nTo prove a similar uniform convergence result for the attainable activations ˆut, we make use of\nthe following fact from prior literature, which shows that we can without loss of generality take the\nnoisy labels to be bounded by M = O( bW\nL log(bW/ϵ)), due to Dx being (L, R)-well behaved.\nFact F.3 (Lemma D.8 [WZDD23]). Let y′ = sign(y) min(|y|, M) for M = bW\nL log( 16b4W 4\nϵ2\n). Then:\nE\n(x,y)∼D[(u∗(w∗ · x) − y′)2] = OPT + ϵ.\nIn other words, we can assume |y| ≤ M without loss of generality by truncating labels that are\nlarger than M. Under this assumption, as stated in Lemma F.4 below, we bound the L2\n2 distance\nbetween ˆut and ut using similar arguments as in Lemma F.2.\nLemma F.4 (Approximating Population-Optimal Activation by Sample-Optimal). Let wt ∈ B(W).\nGiven a distribution D whose marginal Dx is (L, R)-well behaved, let S = {(x(i), y(i))}m\ni=1, where\n(x(i), y(i)) for i ∈ [m] are i.i.d. samples from D. Let ˆut be a sample-optimal activation for the\ndataset S and parameter vector wt, as defined in (P). In addition, let ut be the corresponding\npopulation-optimal activation, as defined in (EP). Then, for any ϵ, δ > 0, choosing a sufficiently\nlarge\nm ≳ d log4(d/(ϵδ))\n\u0012b2W 3\nL2ϵ\n\u00133/2\n,\nwe have that for any wt ∈ B(W), with probability at least 1 − δ over the dataset S:\nE\n(x,y)∼D[(ˆut(wt · x) − y)2] ≤\nE\n(x,y)∼D[(ut(wt · x) − y)2] + ϵ ,\nand, furthermore,\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2] ≤ ϵ.\nProof. As in the proof of Lemma F.2, we choose r = 2cW\nL log(m/(Lδ1)) so that |wt · x(i)| ≤ r for all\nx(i)’s from the dataset with probability at least 1 − δ2\n1/m ≥ 1 − δ1. We now condition on the event\nthat |wt · x(i)| ≤ r for all i = 1, . . . , m. Let V be the set of non-decreasing 1-Lipschitz functions\nsuch that ∀v ∈ V, v(0) = 1/2, and v(z1) − v(z2) ≥ (a/(2br))(z1 − z2) for all z1 ≥ z2 ≥ 0. Then,\nconditioned on this event, we similarly have that (ˆut(z)/(2br) + 1/2)||z|≤r = ˆvt(z) ∈ V, and ˆvt(z)\nsatisfies:\nˆvt(z) ∈ argmin\nv∈V\n1\nm\nm\nX\ni=1\n(v(wt · x(i)) − y(i))2.\nAgain, studying the L2\n2 distance between ˆut(z) and ut(z), we have:\nE\n(x,y)∼D[(ˆut(wt · x) − y)2] =\nE\n(x,y)∼D[(ˆut(wt · x) − y)21{|wt · x| ≤ r}]\n+\nE\n(x,y)∼D[(ˆut(wt · x) − y)21{|wt · x| > r}].\n53\nThe probability of |wt · x| > r is small due to the fact that Dx possesses sub-exponential tail:\nPr[|wt · x| > r] ≤ (δ1/m)2. Now note that |y| ≤ M and Ex∼Dx[((wt/∥wt∥2) · x)4] ≤ c/L4 by the\nsub-exponential property of Dx, we thus have:\nE\n(x,y)∼D[(ˆut(wt · x) − y)21{|wt · x| > r}]\n≤ 2\nE\n(x,y)∼D[((ˆut(wt · x))2 + y2)1{|wt · x| > r}]\n≤ 2\nE\nx∼Dx[b2W 2((wt/∥wt∥2) · x)21{|wt · x| > r}] + 2M2 Pr[|wt · x| > r]\n≤ 2b2W 2r\nE\nx∼Dx[((wt/∥wt∥2) · x)4] Pr[|wt · x| > r] + 2M2 Pr[|wt · x| > r]\n≤ 2cb2W 2δ1/(L2m) + 2M2(δ1/m)2,\nwhere in the second inequality we used the fact that ˆut is b-Lipschitz and wt ∈ B(W), and in the third\ninequality we applied Cauchy-Schwarz. Since M = bW\nL log( 16b4W 4\nϵ2\n), we have M2(δ1/m) ≲ cb2W 2/L2\nfor m ≳ log(bW/ϵ), thus, we get\nE\n(x,y)∼D[(ˆut(wt · x) − y)21{|wt · x| > r}] ≤ 4c(bW/L)2δ1/m,\n(56)\nfor some absolute constant c.\nThe rest remains the same as in the proof of Lemma F.2. Let A = {x : |wt · x| ≤ r}. Let\ny′ = y/(2br) + 1/2. As ˆvt(z) = ˆut(z)/(2br) + 1/2 is the sample-optimal activation in V given wt\n(conditioned on A), applying Fact F.1 we have that with probability at least 1 − δ:\nE\n(x,y)∼D[((ˆut(wt · x)/(2br) + 1/2) − y′)2|A] =\nE\n(x,y)∼D[(ˆvt(wt · x) − y′)2|A]\n≤ inf\nv∈V\nE\n(x,y)∼D[(v(wt · x) − y′)2|A] + ˜O(W(d log(m/δ2)/m)2/3).\nSince U(a,b)||z|≤r/(2br) + 1/2 ⊂ V||z|≤r, we further have\ninf\nv∈V\nE\n(x,y)∼D[(v(wt · x) − y′)2|A] ≤\ninf\nu∈U(a,b)\nE\n(x,y)∼D[(u(wt · x)/(2br) + 1/2 − y′)2|A]\n≤\n1\n4b2r2\ninf\nu∈U(a,b)\nE\n(x,y)∼D[(u(wt · x) − y)2|A].\nTherefore, E(x,y)∼D[(ˆut(wt · x) − y)21{A}] can be bounded from above by\nE\n(x,y)∼D[(ˆut(wt · x) − y)21{A}]\n= 4b2r2\nE\n(x,y)∼D[(ˆvt(wt · x) − y′)2|A] Pr[A]\n≤ 4b2r2 inf\nv∈V\nE\n(x,y)∼D[(v(wt · x) − y′)2|A] Pr[A] + ˜O(b2r2W(d log(m/δ2)/m)2/3)\n≤\ninf\nu∈U(a,b)\nE\n(x,y)∼D[(u(wt · x) − y)21{A}] + ˜O(b2r2W(d log(m/δ2)/m)2/3)\n≤\ninf\nu∈U(a,b)\nE\n(x,y)∼D[(u(wt · x) − y)2] + ˜O(b2r2W(d log(m/δ2)/m)2/3).\nThus, combining with (56), we get that with probability at least 1 − δ1 − δ2,\nE\n(x,y)∼D[(ˆut(wt · x) − y)2] ≤\nE\n(x,y)∼D[(ut(wt · x) − y)2] + ˜O\n\u0012\nWb2r2\n\u0012d log(m/δ2)\nm\n\u00132/3\u0013\n+\n\u0012bW\nL\n\u00132 δ1\nm.\n54\nChoosing the size of the sample set to be:\nm = Θ\n\u0012\nd log4(d/(ϵδ))\n\u0012b2W 3\nL2ϵ\n\u00133/2\u0013\n,\nand recalling that r = 2cW\nL log(m/(Lδ1)), we finally have\nE\n(x,y)∼D[(ˆut(wt · x) − y)2] ≤\nE\n(x,y)∼D[(ut(wt · x) − y)2] + ϵ,\nwith probability at least 1 − δ, after choosing δ1 = δ2 = δ/2.\nTo prove the final claim of the lemma, we follow the same argument as in the proof of Lemma F.2.\nSince we have just shown that with probability at least 1 − δ, it holds\nϵ ≥\nE\nx∼Dx[(ˆut(wt · x) − y)2 − (ut(wt · x) − y)2]\n=\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))2] + 2\nE\nx∼Dx[(ˆut(wt · x) − ut(wt · x))(ut(wt · x) − y)],\napplying the first statement in Claim C.1 completes the proof.\n55\n"
}