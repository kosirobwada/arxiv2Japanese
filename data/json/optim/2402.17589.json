{
    "optim": "PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning Xiaoyu Liu, Beitong Zhou, Cheng Cheng‚àó Huazhong University of Science and Technology {lxysl, zhoubt, c_cheng}@hust.edu.cn Abstract Recently, the application of Contrastive Representation Learning (CRL) in learning with noisy labels (LNL) has shown promising advancements due to its remarkable ability to learn well-distributed representations for better distin- guishing noisy labels. However, CRL is mainly used as a pre-training technique, leading to a complicated multi-stage training pipeline. We also observed that trivially combining CRL with supervised LNL methods decreases performance. Using different images from the same class as negative pairs in CRL creates optimization conflicts between CRL and the supervised loss. To address these two issues, we propose an end-to-end PLReMix framework that avoids the complicated pipeline by introducing a Pseudo-Label Relaxed (PLR) con- trastive loss to alleviate the conflicts between losses. This PLR loss constructs a reliable negative set of each sample by filtering out its inappropriate negative pairs that over- lap at the top Œ∫ indices of prediction probabilities, lead- ing to more compact semantic clusters than vanilla CRL. Furthermore, a two-dimensional Gaussian Mixture Model (GMM) is adopted to distinguish clean and noisy samples by leveraging semantic information and model outputs simul- taneously, which is expanded on the previously widely used one-dimensional form. The PLR loss and a semi-supervised loss are simultaneously applied to train on the GMM divided clean and noisy samples. Experiments on multiple bench- mark datasets demonstrate the effectiveness of the proposed method. Our proposed PLR loss is scalable, which can be easily integrated into other LNL methods and boost their performance. Codes will be available. 1. Introduction Deep neural networks (DNNs) lead to huge performance boosts in various classification tasks [14, 25, 41, 42]. How- ever, the large-scale, high-quality annotated datasets re- quired by training DNNs are extremely expensive and time- consuming. Many studies turn to collect data in less ex- Negative pair Inappropriate negative pair Prediction  probabilities ùë•! ùë•\" ùë•# x top! (‚ãÖ) Figure 1. Left: Model performance suffers from trivially combining contrastive representation learning with supervised learning. The performance of DivideMix [28] (a supervised LNL method) boosts from using SimCLR [8] pretrained weights (DivideMix‚àó), whereas suffers from being trivially combined with SimCLR contrastive loss (DivideMix w/ SimCLR). While DivideMix with our proposed PLR contrastive loss (DivideMix w/ PLR) achieves comparable results to DivideMix‚àó. Right: Our proposed PLR contrastive loss selects a set of reliable negative pairs for each sample. The negative pair should have no overlap with the sample‚Äôs prediction probabilities at topŒ∫ indices (Œ∫ = 2 in Figure). As an instance, xk is a negative pair of xi as top(i) 2 ‚à© top(k) 2 = ‚àÖ, while xj is an inappropriate negative pair of xi as top(i) 2 ‚à© top(j) 2 Ã∏= ‚àÖ. pensive but more efficient ways, such as querying search engines [6, 9, 33] , crawling images with tags [11, 35, 36] or annotating with network outputs [26]. Datasets collected in such alternative ways inevitably contain noisy labels. These corrupted noisy labels severely interfere with the model‚Äôs training process due to over-fitting, which leads to worse generalization performance [2, 59]. Existing methods dealing with the so-called learning with noisy labels (LNL) problem can be mainly classified into two categories, namely label correction and sample selection. Label correction methods leverage the model predictions to correct noisy labels [1, 40]. Sample selection techniques attempt to divide samples with clean labels (usually with small losses) from noisy datasets [13, 19, 28]. Recent studies typically filter out the potentially noisy samples first, then apply label correction to those noisy samples, and finally 1 arXiv:2402.17589v1  [cs.CV]  27 Feb 2024 train the model in a semi-supervised way [28]. Most label correction and sample selection methods take advantage of early learning phenomenon during the model training process [2]. Nevertheless, the intrinsic semantic information in data inherently resistant to label noise mem- orization has been disregarded in these methods [53]. The rapid development of self-supervised contrastive representa- tion learning (CRL) methods [8, 16] shows new possibilities for modeling data representations without the requirements of data labels, which is beneficial for the model to correct corrupted labels or filter out noisy samples in datasets. Most recent studies either utilize the CRL to acquire robust pre- trained weights [12, 44, 62] or directly add the CRL loss to the total loss accompanying the supervised loss [18, 20]. While CRL helps models to learn representations with- out labels, a decrease in accuracy has been observed when it was trivially integrated with supervised losses (Figure 1 left). To address this issue, we propose the Pseudo-Label Relaxed (PLR) contrastive representation learning to avoid the conflict between supervised learning and CRL (Figure 1 right). Inappropriate negative pairs are removed by check- ing if their top Œ∫ indices of prediction probabilities have an empty intersection. This results in a reliable negative set and will construct a more effective semantic clustering than vanilla CRL. In addition to improving model robustness to noisy labels by utilizing CRL, we also design a new technique to filter out noisy samples based on inconsistencies between labels and semantics. Considering the semantic representation cluster formed by CRL, when the similarity between one sample‚Äôs projection embedding with its label prototype is smaller than that between itself and its cluster prototype, it may be given the wrong label. We adopt cross-entropy loss to formalize this sample selection method to obtain a consistent form with small loss selection techniques. Then, we dynamically fit a two-dimensional Gaussian Mixture Model (2d GMM) on the joint distribution of these two losses to divide clean and noisy samples. Our main contributions are as follows: ‚Ä¢ We analyze the conflict between supervised learning and CRL, attributing it to objective inconsistency and gradient conflicts. A novel PLR loss is proposed to alleviate this issue, facilitating our end-to-end PLReMix framework to enhance the capacity for LNL problems. Our proposed PLR loss can be easily integrated into other LNL methods and boost their performance as well. ‚Ä¢ We propose a sample selection method based on the in- consistency between semantic representation clusters and labels. A 2d GMM is utilized to filter out noisy samples, considering both the early learning phenomenon of the model and intrinsic correlation in data. ‚Ä¢ Extensive experiments on multiple benchmark datasets with varying types and label noise ratios demonstrate the effectiveness of our proposed method. We also conduct ablation studies and other analyses to verify the robustness of the components. 2. Related Work 2.1. Learning with noisy labels Label correction. Label correction methods attempt to use the model predictions to correct the noisy labels. To this end, during training, a noise transition matrix is estimated to correct prediction by transforming noisy labels to their corresponding latent ground truth [9, 47]. Huang et al. [17] update the noisy labels with model predictions through the exponential moving average. P-correction [56] treats the latent ground truth labels as learnable parameters and utilizes back-propagation to update and correct noisy labels. Sample selection. Sample selection techniques attempt to filter the samples with clean labels from a noisy dataset. Han et al. [13] select samples with small losses from one network as clean ones to teach the other. Dividemix [28] utilizes a two-component GMM to model the loss distri- bution to separate the dataset into a clean set and a noisy set. Sukhbaatar et al. [51] also consider the samples with consistent prediction and high confidence as clean samples. Both label correction and sample selection methods bene- fit from the early learning phenomenon [2, 23, 31]. That is to say, the weights of DNNs will not stray far from the initial weights in the early stage of training [31]. Notably, several prior studies [2, 3, 23, 46] also observe the memorization effect in DNNs. This phenomenon demonstrates that DNNs tend to learn simple patterns and, over time, gradually overfit to more complex and noisy patterns. 2.2. Contrastive representation learning CRL in self-supervised learning. CRL, a representative self-supervised learning technique, holds great promise in learning label-irrelevant representations. Basically, the main idea of self-supervised learning is to design a pretext task that allows models to learn pre-trained weights. CRL learns label-irrelevant representations of the model that identify positive and negative examples corresponding to a single sample from a batch of data. In SimCLR [8], two random augmentations are applied to each sample to obtain pairs of positive samples, while the other augmented samples are considered negative examples. A momentum encoder and a memory bank are adopted in MoCo [16] to generate and retrieve negative features of samples, which alleviates the high demand for large batch sizes. CRL in learning with noisy labels. As a label-free method, CRL is resistant to noisy labels and can be beneficial for the LNL problem. Most of the recent studies follow a two or three-stage paradigm that uses CRL as a pretext task to obtain a pre-trained foundation model. Zheltonozhskii et al. 2 [62] and Ghosh et al. [12] use CRL as a warmup method and outperform the fully-supervised pre-training on ImageNet. ScanMix [44] uses the CRL following a deep clustering method to pre-train the model, then jointly train the network with semi-supervised loss and clustering loss. Zhang et al. [60] utilizes CRL to pretrain the model, then trains a robust classification head on a fixed backbone, and finally trains in a semi-supervised way with a graph-structured regularization. We compare these frameworks in Figure 2 right. A few methods have been proposed to address the com- plexity associated with pretrain fine-tuning pipeline of CRL within the overall LNL framework. Nevertheless, directly combining instance-wise CRL with supervised training ob- jectives can interfere with the class-clustering ability, unlike forming clusters in a low-dimensional hypersphere surface in an unsupervised situation. In MoPro [29] and ProtoMix [30], the prototypical CRL that enforces the sample embedding to be similar to its assigned prototypes is jointly used with standard instance-wise CRL. MOIT [39] uses mixup interpo- lated supervised contrastive learning [21] to improve model robustness to label noise. UNICON [20] divides the dataset into clean and noisy sets, then trains in a semi-supervised manner with CRL loss applied on the noisy sets. Sel-CL+ [32] selects confident pairs by measuring the agreement be- tween learned representations and labels to apply supervised contrastive learning to the LNL problem. Kim et al. [22] and Yan et al. [55] propose negative learning approaches to alleviate the impact of noisy labels by teaching the model to learn this is not something instead of this is something. Our method extends the self-supervised CRL by constructing a reliable negative set for each sample using pseudo-labels that the model predicts, thus alleviating the wrong contrast. 3. Proposed Method 3.1. Overview The noisy dataset given in LNL is denoted as D = {xi, yi}N i=1, where xi represents a sample and yi is the cor- responding label over C classes, N is the number of samples in the dataset. The presence of noisy samples causes label differences between the annotated labels {yi}N i=1 and the ground truth labels {Àúyi}N i=1. Our proposed framework it- eratively trains two identical networks, each consisting of a convolutional backbone f(¬∑; Œ∏(m)), a classification head g(¬∑; œï(m)), and a projection head h(¬∑; œà(m)). m ‚àà {0, 1} denotes the network number. Given a minibatch of b sam- ples, the output logits of input x = {xi}b i=1 are denoted as z = g(f(x; Œ∏(m)); œï(m)), the output prediction probabili- ties are denoted as t = softmax(z), the projection embed- dings are denoted as q = h(f(x; Œ∏(m)); œà(m)). We use a weak augmentation œÜ and a strong augmentation œâ. Class prototypes P = {pk}C k=1, are maintained and updated every epoch to represent the feature prototypes of the k-th class, where pk ‚àà Rd, d is the dimension of projection embedding. For the initialization of class prototypes P and initial model convergence, we warm up the two networks for a few epochs with cross-entropy loss and standard CRL loss on the whole dataset. At the end of the warmup, we initialize class prototypes by taking the average of feature embeddings with the same noisy labels. As shown in Figure 2 left, the proposed end-to-end LNL framework performs the following two steps iteratively in the training process: (1) divide the whole dataset into a clean and a noisy set considering their losses by a novel two-component 2d GMM (Section 3.2) and (2) train the other network with proposed Pseudo-Label Relaxed contrastive loss LPLR on the representations with a set of reliable negative pairs Ni for each sample (Section 3.3), and semi-supervised loss LSST on the divided clean and noisy sets (Section 3.4). Ni is denoted as N in Figure 2 left. 3.2. Joint sample selection Sample selection We compute two cross-entropy losses li,cls and li,proto to fit a two-component 2d GMM for sam- ple selection. First, following the small loss selection tech- nique [13], we compute the cross-entropy classification loss li,cls = ‚àí log(tyi i ) to measure how well the model fits the pattern of a sample. Then, we define the class prototypes as the center of projection embeddings with similar semantics. The similarity between each projection embedding qi and all the class prototypes {pk}C k=1 represents the latent class probability distributions at the semantic level. Following MoPro [29], we use softmax normalized cosine similarity to measure the similarity between qi and pk: si = {sk i }C k=1 = ( exp (qi ¬∑ pk/œÑs) PC k=1 exp (qi ¬∑ pk/œÑs) )C k=1 , (1) where sharpen temperature œÑs is 0.1 in this paper. Projection embeddings for the samples with similar semantics tend to form clusters around corresponding prototypes under the effect of CRL. In such a case, if a sample xi is mislabeled to yi, we suppose to have syi i < sÀúyi i , where Àúyi is the latent ground truth class of xi. Based on the above discussion, the distance between the distribution of si and OneHot(yi) is supposed to be smaller for clean samples than for noisy samples. Thus, we can compute another cross-entropy loss li,proto = ‚àí log(syi i ) to measure if the given label matches the semantic cluster. After obtaining the li,cls and the li,proto for all sam- ples, a two-component 2d GMM on the distribution of l = {li}N i=1 = {(li,cls, li,proto)}N i=1 can be computed. Sup- pose the two Gaussian components are G1(¬∑; ¬µ1, Œ£1) and G2(¬∑; ¬µ2, Œ£2), we obtain the probability wi of each sample to be clean from posterior probability as: wi = p (Gj | li) with j = arg min j‚àà{1,2} \r\r¬µj \r\r 2. (2) 3 ùëì(\"#$) ‚Ñé(\"#$) ùëî(\"#$) ùëû ùë∑ ùíî ùíï ùëô!\"#$# ùëô%&' ùíô ùëì($) ‚Ñé($) ùëû( ùíô ùëì($) ‚Ñé($) ùëû),( ùúî+ ùúî, ùí© ‚Ñí-./ ùëì($) ùëì($) ùëî($) ùëî($) ùíï0 ùíï( MixUp ùíô0, ùíñ0 ùíô(, ùíñ( Refine ‚Ñí112 ‚Ñí ùí≥, ùí∞ (‚Ö∞) (‚Ö±) (‚Ö≤) 2d GMM Forward Forward-Backward Selection Share Weights LNL (e.g., DivideMix) LNL (e.g., DivideMix) 1d GMM SST  ùí≥  ùí∞ 1d GMM SST  ùí≥  ùí∞ 2d GMM PLR  ùí≥  ùí∞ SST 2d GMM PLR  ùí≥  ùí∞ SST Self-supervised Pretrain Self-supervised Pretrain Clustering Pretrain (a) (b) (d) (c) Figure 2. Left: Overview of the proposed method. (i) We perform joint sample selection on the first network to divide clean and noisy samples with 2d GMM. (ii) We train the second network with proposed Pseudo-Label Relaxed contrastive loss LPLR with the constructed reliable negative set N alleviating the conflict between the supervised learning and CRL. (iii) We utilize semi-supervised training for the second network on the clean and noisy samples divided by 2d GMM separately. The final learning objective L comprises two components, namely LPLR and LSST. Right: Comparison of different LNL frameworks utilizing CRL. (a) DivideMix [28] iteratively performs sample selection (1d GMM) and semi-supervised training (SST). (b) The two-stage method [12, 60, 62] performs self-supervised pretraining before LNL. (c) The three-stage method [44] performs self-supervised pretraining and clustering pretraining before LNL. (d) By proposing PLR loss, our method unifies SST and CRL in an end-to-end PLReMix framework. Samples predicted have smaller \r\r¬µj \r\r 2 are considered as clean samples, which form the clean set denoted by X. While the remaining samples are considered as noisy and form the noisy set U. Momentum class prototypes We adopt noise correction and exponential moving average (EMA) to update the class prototypes {pk}C k=1 while training. Due to the slow conver- gence speed of CRL, its projection embeddings qi are not optimal and robust in the early epochs, thus resulting in inac- curate si in Eq. (1). To better estimate the latent ground truth label Àúyi, we generate a pseudo soft label Œ¥i = Œ±ti+(1‚àíŒ±)si for xi considering both model predictions and semantic clus- ter. Accordingly, noise correction and sample selection are performed to set a correction threshold œÑ1 and a high predic- tion confidence threshold œÑ2, i.e. ÀÜyi = Ô£± Ô£¥ Ô£≤ Ô£¥ Ô£≥ arg max c‚ààC Œ¥c i if max c‚ààC Œ¥c i > œÑ1, yi elseif max Œ¥yi i > œÑ2, unknown otherwise. (3) The selected high prediction confidence samples {xi | i Ã∏= unknown} constitute the confident set denoted as C. With the estimated labels, we update the projection embedding qi to the corresponding class prototype pÀÜyi using EMA: pÀÜyi ‚Üê Normalize \u0000Œ∑pÀÜyi + (1 ‚àí Œ∑)Normalize(qi) \u0001 , (4) for all {xi, ÀÜyi} ‚àà C, where Normalize(x) = x/ ‚à•x‚à•2 and the momentum coefficient Œ∑ is set to 0.99. 3.3. Pseudo-Label Relaxed contrastive representa- tion learning Typically, unsupervised CRL methods use two transforms of a single sample as a positive pair and combinations of transforms of other samples as negative pairs. We intend to design a loss function that will bring positive pairs together and push negative pairs apart. Specifically, the vanilla In- foNCE loss for the embedding qi,œâ of an augmented sample used in CPC [38] and SimCLR [8] are defined as: ‚àí N X i=1 log exp \u0000 qi,œâ, q+,œâ \u000b /œÑ \u0001 exp \u0000 qi,œâ, q+,œâ \u000b /œÑ \u0001 + P qj,œâ‚àà{q‚àí,œâ} exp \u0000 qi,œâ, qj,œâ \u000b /œÑ \u0001 , (5) where œÑ is the sharpening temperature. {q+,œâ} and {q‚àí,œâ} are the projection embeddings of augmented positive and negative samples, respectively, ‚ü®a, b‚ü© represents cosine sim- ilarity Normalize (a ¬∑ b). We note that there exist some negative embeddings qj,œâ ‚àà {q‚àí,œâ} whose ground truth labels are identical to the positive embedding qi,œâ. These samples will also be pushed away from qi,œâ with the use of the term exp \u0000 qi,œâ, qj,œâ \u000b /œÑ \u0001 in Eq. (5), which conflicts 4 with the design purpose of other supervised losses used in LNL problems. Worse still, this effect is exacerbated when the i-th sample and j-th sample are more similar, resulting in a more dispersed cluster. We proposed the PLR loss to address this issue, which is robust to both clean and noisy samples. A reliable negative set Ni is designed to remove the samples in {q‚àí,œâ} that share the same latent ground truth with qi,œâ. To this end, given a prediction probability ti = [ti,1, ti,2, . . . , ti,C], we first identify the indexes of the top Œ∫ digits of the prediction probability ti of the i-th sample, which is denoted by top(i) Œ∫ : top(i) Œ∫ = arg max Œ∫ {ti,Œ∫}C Œ∫=1. (6) Thus, the reliable negative set of xi can be obtained as: n j | top(i) Œ∫ ‚à© top(j) Œ∫ = ‚àÖ, ‚àÄj o . (7) During the early training stage, we additionally append the given label yi to the top(i) Œ∫ for further conflict avoidance: Ni = n j | {top(i) Œ∫ ‚à™ yi} ‚à© {top(j) Œ∫ ‚à™ yj} = ‚àÖ, ‚àÄj o . (8) Finally, the proposed PLR loss is re-formulated based on the vanilla InfoNCE loss of Eq. (5) to the following expression: LPLR = ‚àí N X i=1 log exp \u0000 qi,œâ, q+,œâ \u000b /œÑ \u0001 exp \u0000 qi,œâ, q+,œâ \u000b /œÑ \u0001 + P j‚ààNi exp \u0000 qi,œâ, qj,œâ \u000b /œÑ \u0001 . (9) We use two forms of PLR loss in this paper: the FlatNCE [7] formalized PLR loss is denoted as FlatPLR, while the InfoNCE formalized one is denoted just as PLR. We do ex- periments on gradient analysis between LPLR and LCRL in Section 4.4, whose results show that LPLR has less gradient conflicts with the supervised loss. 3.4. Semi-supervised training As described in Section 3.2, according to the proposed sample selection technique, we divide the noisy dataset D into a labeled set X and an unlabeled set U using 2d GMM. Then, we perform semi-supervised training on them follow- ing FixMatch [45] and MixMatch [4]. First, we generate pseudo targets yi for each sample using weak augmentations: Ô£± Ô£≤ Ô£≥ Sharpen \u0010 wi OneHot (yi) + 1‚àíwi card(œÜ) P œÜ ti,œÜ; T \u0011 , if xi ‚àà X, Sharpen \u0010 1 card(œÜ) card(m) P œÜ,m tm i,œÜ; T \u0011 , if xi ‚àà U, (10) and Sharpen(¬Øyi; T) = ¬Øy c 1 i T / C X c=1 ¬Øy c 1 i T , for c = 1, 2, ..., C, (11) where card(œÜ) and card(m) represent the numbers of aug- mentations and models, respectively, and the sharpening temperature T is 0.5. Then we do MixUp [61] on the strong augmented samples xi,œâ and pseudo targets ¬Øyi: x‚Ä≤ i,œâ = Œªxi,œâ + (1 ‚àí Œª)xr(i),œâ, ¬Øy‚Ä≤ i = Œª¬Øyi + (1 ‚àí Œª)¬Øyr(i), (12) where Œª ‚àº Beta(Œ≤, Œ≤), r(i) represents the random permu- tation of indices i. Finally, the semi-supervised loss LSST consists of three parts: a cross-entropy loss on the labeled set, a mean squared error on the unlabeled set, and a regular- ization term Lreg used in [49] and [1]: LSST = ‚àí 1 |X| X xi‚ààX C X c=1 ¬Øy‚Ä≤c i log(t‚Ä≤c i,œâ)+ ŒªU 1 |U| X xi‚ààU \r\r¬Øy‚Ä≤ i ‚àí t‚Ä≤ i,œâ \r\r2 2 + Lreg, (13) where ŒªU is a coefficient for tuning. The final objective objective L is to minimize the following function: L = LSST + ŒªiLPLR, (14) where Œªi is a tunable hyperparameter. 4. Experiments 4.1. Experimental settings CIFAR10/100 The CIFAR10/100 dataset [24] comprises 50K training images and 10K test images. We adopt symmet- ric and asymmetric synthetic label noise models as used in previous studies [49]. The symmetric noise setting replaces r% of labels from one class uniformly with all possible classes, while asymmetric replaces them from one class only with the most similar class. We use the PreAct ResNet-18 [15] network on CIFAR 10/100 and train it using an SGD optimizer. To avoid conflict early and gain robust representation later, we reduce Œ∫ from 3 to 2 to 1 at the epochs of 40 and 70. In this work, we use AutoAug [10] as a strong augmentation operation, which is demonstrated to be useful in the LNL problem [37]. We use FlatPLR for CIFAR experiments to gain a greater thrust between samples and their negative pairs with a relatively small batch size. For more training details, please refer to the Appendix. Tiny-ImageNet Tiny-ImageNet [27] consists of 200 classes, each containing 500 smaller-resolution images from ImageNet. We train a PreAct ResNet 18 [15] network with a batch size of 128 and a learning rate of 0.01. Clothing1M Clothing1M [54], a large-scale real-world dataset with label noise, contains 1M clothing images from 14 classes. We sample 64K images in total from each class [28]. We train a ResNet50 network with weights pre-trained on ImageNet [14] for 100 epochs. WebVision WebVision dataset [33] contains 2.4 M im- ages crawled from Flickr and Google. All of the images are 5 Table 1. Experimental and comparison results on CIFAR-10 and CIFAR-100 datasets with different noise ratios (test accuracy %). Method Dataset CIFAR-10 CIFAR-100 Mode Sym Asym Sym Asym r 20% 50% 80% 90% 40% 20% 50% 80% 90% 40% Cross-entropy Best 86.8 79.4 62.9 42.7 85.0 62.0 46.7 19.9 10.1 44.5 Last 82.7 57.9 26.1 16.8 72.3 61.8 37.3 8.8 3.5 - Co-teaching+ [58] Best 89.5 85.7 67.4 47.9 - 65.6 51.8 27.9 13.7 - Last 88.2 84.1 45.5 30.1 - 64.1 45.3 15.5 8.8 - MixUp [61] Best 95.6 87.1 71.6 52.2 - 67.8 57.3 30.8 14.6 48.1 Last 92.3 77.3 46.7 43.9 77.7 66.0 46.6 17.6 8.1 - DivideMix [28] Best 96.1 94.6 93.2 76.0 93.4 77.3 74.6 60.2 31.5 55.1 Last 95.7 94.4 92.9 75.4 92.1 76.9 74.2 59.6 31.0 - ScanMix [44] Best 96.0 94.5 93.5 91.0 93.7 77.0 75.7 66.0 58.5 - Last 95.7 93.9 92.6 90.3 93.4 76.0 75.4 65.0 58.2 - MOIT+ [39] Best 94.1 91.8 81.1 74.7 93.3 75.9 70.6 47.6 41.8 74.0 Sel-CL+ [32] Best 95.5 93.9 89.2 81.9 93.4 76.5 72.4 59.6 48.8 72.7 ELR+ [34] Best 95.8 94.8 93.3 78.7 93.3 77.6 73.6 60.8 33.4 73.2 UNICON [20] Best 96.0 95.6 93.9 90.8 94.1 78.9 77.6 63.9 44.8 74.8 (Flat) PLReMix Best 96.63 95.71 95.08 91.93 95.11 77.95 77.78 68.76 50.17 64.89 Last 96.46 95.36 94.84 91.54 94.72 77.78 77.31 68.41 49.44 62.67 Table 2. Experimental and comparison results on Tiny-ImageNet dataset (test accuracy %). Method r Sym Asym 0 20% 50% 80% 45% Cross-entropy Best 57.4 35.8 19.8 - 26.3 Last 56.7 35.6 19.6 - 26.2 Co-teaching+ [58] Best 52.4 48.2 41.8 - 26.9 Last 52.1 47.7 41.2 - 26.5 M-correction [1] Best 57.7 57.2 51.6 - 24.8 Last 57.2 56.6 51.3 - 24.1 UNICON [20] Best 63.1 59.2 52.7 - - Last 62.7 58.4 52.4 - - (Flat) PLReMix Best 62.93 60.70 54.94 37.47 33.97 Last 62.46 60.39 54.31 36.43 33.25 categorized into 1,000 classes, which is the same as Ima- geNet ILSVRC12 [43]. We use the first 50 classes of the Google image subset to train an InceptionResnet V2 [48] from scratch. Multi-crop augmentation strategy [5] is ex- ploited to obtain more robust representations in rather shorter epochs and smaller batch size so that contrastive loss can be jointly trained with the semi-supervised loss. See the Appendix for more details about multi-crop. 4.2. Experimental results The proposed end-to-end PLReMix framework is com- pared with other LNL methods on different datasets using the same network architecture. The experimental results on CIFAR10/100 are shown in Table 1. We report the best test accuracy over all epochs and the average test accuracy of the Table 3. Experimental and comparison results on Clothing1M dataset (test accuracy %). Method Test Accuracy Cross-Entropy 69.21 Joint-Optim [49] 72.16 P-correction [56] 73.49 DivideMix [28] 74.76 ELR+ [34] 74.81 UNICON [20] 74.98 C2D [62] 74.30 ScanMix [44] 74.35 PLReMix 74.85 last 10 epochs as Best and Last, respectively. Considering the intrinsic semantic features by utilizing PLR loss and 2d GMM, our proposed method outperforms state-of-the-art (SOTA) results in most experimental settings, especially on data with high noise ratios. We attribute our inferior results to SOTA on CIFAR100 with 40% asymmetric noise to worse initialization of class prototypes across too many classes, and the absence of a uniform sample selection technique proved to be useful [20]. However, our method still outperforms the baseline method [28] a lot. The experimental results on Tiny-ImageNet are shown in Table 2. Our algorithm achieves SOTA performance on most noise ratios. The results of the real-world noisy dataset Clothing1M are shown in Table 3, and the results of the We- bVision dataset are listed in Table 4. Our algorithm achieves performance comparable to SOTA. It is notable that C2D [62] and ScanMix [44] use pretrained weights in a two/three- 6 Table 4. Experimental and comparison results on WebVision and ILSVRC12 datasets (test accuracy %). Models are trained on the WebVision dataset and tested on both the WebVision and ILSVRC12 datasets. Top-1 and Top-5 test accuracy are reported. Dataset Architecture WebVision ILSVRC12 Method Top-1 Top-5 Top-1 Top-5 Co-Teaching [13] Inception V2 63.58 85.20 61.48 84.70 DivideMix [28] Inception V2 77.32 91.64 75.20 90.84 ELR+ [34] Inception V2 77.78 91.68 70.29 89.76 UNICON [20] Inception V2 77.60 93.44 75.29 93.72 Sel-CL+ [32] Inception V2 79.96 92.64 76.84 93.04 ScanMix [44] Inception V2 80.04 93.04 75.76 92.60 PLReMix Inception V2 80.19 93.51 75.82 93.16 Table 5. Experimental results of combining different CRL methods with LNL algorithms (test accuracy %). Method CRL CIFAR-10 Sym-20% Sym-50% Asym-40% Co-teaching+ [58] - 88.73 77.52 68.64 SimCLR 88.62 80.24 68.86 PLR 90.28 85.68 70.92 JoCoR [52] - 88.13 80.94 75.48 SimCLR 84.76 78.81 78.56 PLR 89.34 83.46 81.46 stage manner. As a comparison, our proposed method trains the model end-to-end with randomly initialized weights. 4.3. Scalability of PLR We do extra experiments on the CIFAR10 dataset with other different LNL algorithms to demonstrate the effective- ness and scalability of our proposed PLR loss. We integrate our proposed PLR loss into Co-teaching+ [58] and JoCoR [52] and test the model performance under different noise ratios. We use PreAct ResNet18 network, and the minor modification to the original algorithms is just training a pro- jection head using our proposed PLR loss on top of the model backbone. As a comparison, we utilize the vanilla SimCLR to train the projection head. The experimental results are listed in Table 5. As can be seen, training with our proposed PLR increases the model performance compared to the base- line methods, while training with vanilla SimCLR tends to make the model performance corrupted. These results align with our analysis of the gradient conflicts in Section 4.4. 4.4. Analysis Multi-task gradients. Given two losses L(1) and L(2) and their gradients to model parameters g(1) and g(2), we define the intensity of gradient entanglement between them as E(1) (2) = g(1)¬∑g(2)/ \r\rg(2)\r\r2 2, the ratio of gradient magnitude between them as R(1) (2) = \r\rg(1)\r\r 2/ \r\rg(2)\r\r 2. Conflicting gradients and a large difference in gradient Figure 3. Gradient conflicts of two contrastive representation learn- ing methods (vanilla SimCLR and our proposed PLR) when they are jointly trained with semi-supervised learning in the presence of noisy labels. magnitudes often lead to optimization challenges in high- dimensional neural network multi-task learning [57]. Similar issues are witnessed in our experiments. Here, LSimCLR or LPLR is treated as L(1), LSST is treated as L(2). As shown in Figure 3, we show the distribution of ESimCLR SST and EPLR SST (Left), RSimCLR SST and RPLR SST (Right) when training models on CIFAR10 dataset with 80% noise ratio at epoch 30. The entanglement intensity between SimCLR and SST shows more negative values, and their magnitude ratio has more values greater than 1. These indicate that there are more gradient conflicts between LSST and LSimCLR, which result in an optimization challenge and performance decrease shown in Figure 1 left red line. Our proposed PLR loss mit- igates the gradient conflicts with SST, which manifests as more orthogonal gradients and smaller gradient magnitude ratios. As a result, it improves the performance when inte- grated with SST to combat noisy labels, as shown in Figure 1 left green line. Ablation study. We perform thorough ablation studies on our method. The results of ablation studies are shown in Table 6. We test the effectiveness of FlatNCE by swapping it with (non-Flat) PLR, which is formalized by InfoNCE loss. Despite using InfoNCE instead of FlatNCE, our experiments still show the robustness of the proposed PLR loss. Then, we substitute the PLR loss with vanilla SimCLR [8]. Exper- iments where 2d GMM is replaced with 1d GMM confirm that semantic information helps differentiate clean and noisy samples on the distribution of l = {(li,cls)}N i=1. Supervised contrastive loss (SCL) [21] treats samples with the same labels as positive pairs, and others are negative pairs. It can be a substitute for our proposed PLR loss in our framework when it considers samples with the same predicted pseudo labels as positive pairs. We do experiments to compare SCL and PLR in our proposed framework. The results show that our proposed PLR outperforms SCL in most noise ratios except on the CIFAR100 dataset with a high noise ratio. We conduct ablation experiments on the hyperparameter Œ∫. Intuitively, larger values of Œ∫ avoid conflict between 7 Table 6. Ablation studies on CIFAR-10 and CIFAR-100 datasets with different training settings (test accuracy %). Method Dataset CIFAR-10 CIFAR-100 r 50% 80% 90% 50% 80% 90% (Flat) PLReMix Best 95.71 95.08 91.93 77.78 68.76 50.17 Last 95.36 94.84 91.54 77.31 68.41 49.44 PLReMix Best 95.71 93.58 91.71 75.64 66.98 50.78 Last 95.31 93.39 91.46 75.39 66.76 50.54 vanilla SimCLR Best 93.44 90.44 86.80 73.10 63.50 47.76 Last 93.23 90.28 86.51 72.32 63.02 46.81 (Flat) 1d GMM Best 94.51 94.82 92.36 77.11 68.54 50.25 Last 94.16 94.57 92.12 76.91 68.31 49.99 SCL Best 94.37 93.41 91.24 72.55 68.43 54.07 Last 94.08 93.11 91.00 72.18 68.03 53.72 Table 7. Ablation study results when using different Œ∫ or only given labels on CIFAR10 dataset with 80% symmetric label noise. Œ∫ = 3 Œ∫ = 2 Œ∫ = 1 only y PLReMix Best 91.83 93.93 93.74 94.74 95.08 Last 91.29 92.81 93.60 94.58 94.84 PLR and supervised loss more effectively but result in less robust representation due to insufficient negative pairs. To address this issue, we decrease Œ∫ from 3 to 2 to 1 after training a certain number of epochs. The decrease epochs are not carefully tuned and are set roughly evenly. In this ablation study, we fix the value of Œ∫ as 3, 2, or 1 during different individual training procedures. In addition, we test the performance only using the given labels (denoted by only y), which means Ni = {j | yi ‚à© yj = ‚àÖ, ‚àÄj}. The experiments are conducted on the CIFAR10 dataset with 80% symmetric noise, and the results are listed in Table 7. The experimental results verify our analysis and show the effectiveness of the proposed decrease setting. Visualization. We visualize the 2d GMM after 10 epochs of warming up and 100 epochs of training in Figure 4. We plot the losses of 512 randomly selected samples, with half from the clean set and the other half from the noisy set. As illustrated, our proposed 2d GMM effectively fits the distribution of the clean and noisy samples. Correct ratio of negative pairs. We visualize the cor- rect ratio of selected negative pairs in Figure 5 for a better understanding of the proposed PLR loss. The select ratio is the proportion of selected negative sample pairs out of all possible negative sample pairs. The correct ratio represents the proportion of correctly identified negative pairs among all selected pairs. We observed that the PLR loss tends to se- lect a limited number of negative pairs at first to ensure high precision. As Œ∫ gradually decreases, PLR loss will rapidly increase the number of negative sample pairs selected while maintaining precision. Sym 90% epoch 10 Sym 90% epoch 100 Sym 50% epoch 10 Sym 50% epoch 100 Sym 90% epoch 15 Sym 90% epoch 100 Sym 50% epoch 15 Sym 50% epoch 100 ùíç!\"# ùíç!\"#$# ùíç!\"#$# ùíç!\"# ùíç!\"# ùíç!\"# Clean Sample Noisy Sample Clean Distribution Noisy Distribution Figure 4. Visualization of the 2d GMM fitted on the normalized loss distribution {(lcls, lproto)}. The first and second rows are obtained from experiments on the CIFAR10 and CIFAR100, respectively. Figure 5. Negative sample selection when training with FlatPLR. Left: Negative pairs selected ratio. Right: Correct negative pairs selected ratio. 5. Conclusion and Future Work In this work, we proposed an end-to-end framework for solving the LNL problem by leveraging CRL. We analyzed conflicts between CRL learning objectives and supervised ones and proposed PLR loss to address this issue in a simple yet effective way. For further utilizing label-irrelevant intrinsic semantic information, we propose a joint sample selection technique, which expands previously widely used 1d GMM to 2d. Extensive experiments conducted on benchmark datasets show performance improvements of our method over existing methods. Our end-to-end framework shows significant improvement without any complicated pretrain fine-tuning pipeline. Future research should also explore other forms of PLR loss, such as MoCo-based. References [1] Eric Arazo, Diego Ortego, Paul Albert, Noel O‚ÄôConnor, and Kevin McGuinness. Unsupervised label noise modeling and loss correction. In International Conference on Machine Learning, pages 312‚Äì321. PMLR, 2019. 1, 5, 6 [2] Devansh Arpit, Stanis≈Çaw JastrzÀõebski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Ma- 8 haraj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In Inter- national Conference on Machine Learning, pages 233‚Äì242. PMLR, 2017. 1, 2 [3] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong Li, Yinian Mao, Gang Niu, and Tongliang Liu. Understand- ing and improving early stopping for learning with noisy labels. Advances in Neural Information Processing Systems, 34:24392‚Äì24403, 2021. 2 [4] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems, 32, 2019. 5 [5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi- otr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912‚Äì9924, 2020. 6, 1 [6] Youngchul Cha and Junghoo Cho. Social-network analysis using topic models. In Proceedings of the 35th international ACM SIGIR conference on Research and development in in- formation retrieval, pages 565‚Äì574, 2012. 1 [7] Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen, Shuyang Gao, Tagyoung Chung, Yi Xu, Belinda Zeng, Wen- lian Lu, et al. Simpler, faster, stronger: Breaking the log-k curse on contrastive learners with flatnce. arXiv preprint arXiv:2107.01152, 2021. 5 [8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pages 1597‚Äì1607. PMLR, 2020. 1, 2, 4, 7 [9] Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1431‚Äì 1439, 2015. 1, 2 [10] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude- van, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 113‚Äì123, 2019. 5 [11] Pengfei Deng, Jingkai Ren, Shengbo Lv, Jiadong Feng, and Hongyuan Kang. Multi-label image recognition in anime il- lustration with graph convolutional networks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020. 1 [12] Aritra Ghosh and Andrew Lan. Contrastive learning im- proves model robustness under label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2703‚Äì2708, 2021. 2, 3, 4 [13] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. Advances in Neural Information Processing Systems, 31, 2018. 1, 2, 3, 7 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770‚Äì778, 2016. 1, 5 [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Proceedings of the European Conference on Computer Vision, pages 630‚Äì 645. Springer, 2016. 5, 1 [16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual rep- resentation learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 9729‚Äì9738, 2020. 2 [17] Lang Huang, Chao Zhang, and Hongyang Zhang. Self- adaptive training: beyond empirical risk minimization. Ad- vances in Neural Information Processing Systems, 33:19365‚Äì 19376, 2020. 2 [18] Zhizhong Huang, Junping Zhang, and Hongming Shan. Twin contrastive learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11661‚Äì11670, 2023. 2 [19] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International Conference on Machine Learning, pages 2304‚Äì2313. PMLR, 2018. 1 [20] Nazmul Karim, Mamshad Nayeem Rizve, Nazanin Rah- navard, Ajmal Mian, and Mubarak Shah. Unicon: Combating label noise through uniform selection and contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9676‚Äì9686, 2022. 2, 3, 6, 7 [21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661‚Äì18673, 2020. 3, 7 [22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 101‚Äì110, 2019. 3 [23] Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard Zhou, Alexander Toshev, Tom Duerig, James Philbin, and Li Fei-Fei. The unreasonable effectiveness of noisy data for fine-grained recognition. In Proceedings of the European Conference on Computer Vision, pages 301‚Äì320. Springer, 2016. 2 [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5 [25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im- agenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84‚Äì90, 2017. 1 [26] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open im- ages dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International Jour- nal of Computer Vision, 128(7):1956‚Äì1981, 2020. 1 [27] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 5 [28] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. In 9 International Conference on Learning Representations, 2019. 1, 2, 4, 5, 6, 7, 3 [29] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly supervised learning with momentum prototypes. In Interna- tional Conference on Learning Representations, 2020. 3 [30] Junnan Li, Caiming Xiong, and Steven CH Hoi. Learning from noisy data with robust representation learning. In Pro- ceedings of the IEEE/CVF International Conference on Com- puter Vision, pages 9485‚Äì9494, 2021. 3 [31] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gra- dient descent with early stopping is provably robust to label noise for overparameterized neural networks. In International conference on artificial intelligence and statistics, pages 4313‚Äì 4324. PMLR, 2020. 2 [32] Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu. Selective-supervised contrastive learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 316‚Äì325, 2022. 3, 6, 7 [33] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and under- standing from web data. arXiv preprint arXiv:1708.02862, 2017. 1, 5 [34] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car- los Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in Neural Informa- tion Processing Systems, 33:20331‚Äì20342, 2020. 6, 7 [35] Wei Liu, Yu-Gang Jiang, Jiebo Luo, and Shih-Fu Chang. Noise resistant graph ranking for improved web image search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 849‚Äì856, 2011. 1 [36] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim- ing He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Con- ference on Computer Vision, pages 181‚Äì196, 2018. 1 [37] Kento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer. Aug- mentation strategies for learning with noisy labels. In Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8022‚Äì8031, 2021. 5 [38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre- sentation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 4 [39] Diego Ortego, Eric Arazo, Paul Albert, Noel E O‚ÄôConnor, and Kevin McGuinness. Multi-objective interpolation training for robustness to label noise. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6606‚Äì6615, 2021. 3, 6 [40] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian Szegedy, Dumitru Erhan, and Andrew Rabinovich. Train- ing deep neural networks on noisy labels with bootstrapping. arXiv preprint arXiv:1412.6596, 2014. 1 [41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in Neural Information Process- ing Systems, 28, 2015. 1 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmenta- tion. In Medical Image Computing and Computer-Assisted Intervention‚ÄìMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234‚Äì241. Springer, 2015. 1 [43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision, 115(3):211‚Äì252, 2015. 6 [44] Ragav Sachdeva, Filipe Rolim Cordeiro, Vasileios Belagian- nis, Ian Reid, and Gustavo Carneiro. Scanmix: Learning from severe label noise via semantic clustering and semi- supervised learning. Pattern Recognition, 134:109121, 2023. 2, 3, 4, 6, 7 [45] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems, 33:596‚Äì 608, 2020. 5 [46] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil Lee. How does early stopping help generalization against label noise? arXiv preprint arXiv:1911.08059, 2019. 2 [47] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir Bourdev, and Rob Fergus. Training convolutional networks with noisy labels. In International Conference on Learning Representations, 2015. 2 [48] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi. Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 2017. 6, 1 [49] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy- oharu Aizawa. Joint optimization framework for learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5552‚Äì5560, 2018. 5, 6 [50] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008. 1 [51] Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and Junbo Zhao. Promix: Combating label noise via maximizing clean sample utility. arXiv preprint arXiv:2207.10276, 2022. 2 [52] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Com- bating noisy labels by agreement: A joint training method with co-regularization. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 13726‚Äì13735, 2020. 7 [53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3733‚Äì 3742, 2018. 2 [54] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning from massive noisy labeled data for image classification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2691‚Äì2699, 2015. 5 10 [55] Jiexi Yan, Lei Luo, Chenghao Xu, Cheng Deng, and Heng Huang. Noise is also useful: Negative correlation-steered latent contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31‚Äì40, 2022. 3 [56] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise cor- rection for learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7017‚Äì7025, 2019. 2, 6 [57] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi- task learning. Advances in Neural Information Processing Systems, 33:5824‚Äì5836, 2020. 7 [58] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help general- ization against label corruption? In International Conference on Machine Learning, pages 7164‚Äì7173. PMLR, 2019. 6, 7 [59] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) re- quires rethinking generalization. Communications of the ACM, 64(3):107‚Äì115, 2021. 1 [60] Hui Zhang and Quanming Yao. Decoupling representa- tion and classifier for noisy label learning. arXiv preprint arXiv:2011.08145, 2020. 3, 4 [61] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations, 2018. 5, 6 [62] Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson, Alex M Bronstein, and Or Litany. Contrast to divide: Self- supervised pre-training for learning with noisy labels. In Proceedings of the IEEE/CVF Winter Conference on Appli- cations of Computer Vision, pages 1657‚Äì1667, 2022. 2, 3, 4, 6 11 PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive Representation Learning Supplementary Material A. Pseudocode of Pseudo-Label Relaxed loss Algorithm 1 provides the pseudocode of our proposed PLR loss. For a mini-batch of samples x, we first calculate their model prediction probabilities using the first network. Based on the predicted probabilities, we then determine if there are overlapping topŒ∫ predictions denoted as conflicts and use it to identify feasible negative samples. The reliable negative set Ni for each sample is built in the form of a con- trastive mask, indicating which pairs are negative, positive, or neglected when conducting CRL. Finally, we transform the samples x into two strong augmented views, which are then utilized to train a contrastive loss (using the mask to identify the positive and reliable negative pairs) on the other network. B. Training Details CIFAR10/100 We use the PreAct ResNet-18 [15] net- work on CIFAR 10/100 and train it using an SGD optimizer with a weight decay of 5e-4, a momentum of 0.9, and a batch size of 64. We choose ŒªU from {0, 25, 50, 150} following the previous work [28], although experiments show that our method is not sensitive to this parameter. We set the initial learning rate to 0.02 and reduce it by a factor of 10 after 200 epochs. The warmup period is 10 for CIFAR10 and 15 for CIFAR100. We use the Flat version of the proposed PLR loss on these two datasets. To avoid conflict early and gain robust representation later, we reduce Œ∫ from 3 to 2 to 1 at the epochs of 40 and 70. We empirically set œÑ1 = 0.5 for X, especially when the label noise is asymmetric; otherwise, set œÑ1 = 1/C. Tiny-ImageNet We train a PreAct ResNet 18 [15] net- work with an SGD optimizer and a batch size of 128. We set the initial learning rate to 0.01 and reduce it by a factor of 10 after 200 epochs. ŒªU is chosen from {0, 25, 50, 150} and Œ≤ is 0.5. We use the Flat version of the proposed PLR loss on this dataset, and Œ∫ is reduced at the epochs of 40 and 70. Clothing1M We train a ResNet50 network with weights pre-trained on ImageNet [14] for 80 epochs with a learning rate of 5e-3, a weight decay of 1e-3, and a batch size of 64. We use ŒªU = 0 and Œ≤ = 0.5 on the Clothing1M dataset. The network is trained for 100 epochs with a warmup period of 1 epoch. We reduce Œ∫ at the epochs of 15 and 30. We fix the backbone parameters and use strong augmented samples to warmup the classification and projection heads. The learning rate is reduced by a factor of 10 after 40 epochs. WebVision We train an InceptionResnet V2 [48] from scratch with a learning rate of 0.015, a weight decay of 5e-4, a batch size of 96, and a warmup of 2 epochs. We set ŒªU = 0 and Œ≤ = 0.5, and reduce Œ∫ at the epochs of 15 and 30. Multi-crop Strategy Multi-crop strategy [5] is an effi- cient augmentation method used in CRL. Images are cropped into multiple smaller views of varying sizes, which can be viewed as positive pairs in CRL. This strategy increases the diversity of positive pairs without incurring significant ad- ditional computational costs. In the WebVision dataset, we resize the images to a size of 320, then randomly crop and resize them into two large views with a size of 224 and six small views with a size of 128. We list hyperparameters used in our method for various datasets in Table 8 and Table 9. To maintain simplicity, we keep most of the hyperparameters consistent across all datasets. Additionally, to facilitate comparison with Di- videMix [28], we also maintain consistency with most of the hyperparameters used in their approach. It is noteworthy that our PLR loss is insensitive to the hyperparameter Œ∫, as is further discussed in the paper. We didn‚Äôt carefully tune it and empirically set it to dynamically decrease for full utilization of negative pairs. C. Visualization In Figure 6, we use t-SNE [50] to visualize the features of training images for different noise modes and ratios. We randomly select 5% of samples from each class. Circles rep- resent the features q and stars represent the class prototypes P . It can be seen that the feature embeddings form distinct clusters based on their latent ground truth labels rather than the given noisy labels, which demonstrates the robustness of the proposed method. In Figure 7, we visualize the images that have the top-5 largest cosine similarity features to the prototypes of each class on the CIFAR10 dataset with 90% symmetric noise. Corresponding ground truth labels and given noisy labels are listed above each image. If the noisy label differs from the latent ground truth, it is colored in red; otherwise in green. We use a ‚úì to indicate that an image has been correctly assigned to its corresponding cluster and √ó if not. As can be seen, most images have been assigned to its ground truth cluster, which shows the effectiveness of our method. 1 Algorithm 1 Pseudocode of computing PLR loss in a PyTorch-like style. # net0, net1: two identical networks with backbone f, classification head g, and projection head h # PLRLoss: contrastive loss, Eqn.(9), which takes a mask as parameter # k: hyperparameter kappa in Eqn.(6) def build_mask(x, y, net): z = net.g.forward(net.f.forward(x)) # model outputs indices_k = torch.topk(z, k, dim=1)[1] # top k indices of model outputs # tops: assign 1 to the top k indices, 0 to the rest tops = torch.zeros(len(x), C) tops = torch.scatter(tops, 1, indices_k, 1) tops = torch.scatter(tops, 1, y.unsqueeze(1), 1) # append labels to top k # intersection, Eqn.(8), where ‚Äòconflicts‚Äô equals 0 are feasible negative pairs conflicts = torch.matmul(tops, tops.t()) # contrastive mask, where negative pairs are -1, positive pairs are 1, neglect pairs are 0 mask = torch.where(conflicts == 0, -1, 0) mask = torch.where(eye(len(x)) == 1, 1, mask) return mask for (x, y) in loader: # load a minibatch of samples and labels mask = build_mask(x, y, net0) # get mask from one network x1 = aug(x) # random strong augmentation x2 = aug(x) # another strong augmentation # train the other network f1 = net1.h.forward(net1.f.forward(x1)) f2 = net1.h.forward(net1.f.forward(x2)) f = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1) plr_loss = InfoNCELoss()(f, mask=mask) # if other losses exist, sum all losses up, then backward and update plr_loss.backward() update(net1.params) # train the other network Table 8. Hyperparameter settings of our proposed method on different datasets. Hyperparameters CIFAR-10 CIFAR-100 Tiny-ImageNet Clothing1M WebVision Initial Learning Rate 0.02 0.02 0.01 0.004 0.015 Momentum 0.9 Weight Decay 0.0005 0.0005 0.0005 0.001 0.0005 Batch Size 128 128 256 64 96 Epochs 400 400 400 80 100 warmup epochs 10 15 10 1 2 Œ≤ 4 4 0.5 0.5 0.5 Œªi 1 T 0.5 Œ± 0.5 œÑ 1 œÑs 0.1 œÑ1 0.5 or 1/C œÑ2 0.8 2 Table 9. The value of hyperparameter ŒªU on different datasets, following previous work [28]. Hyperparameter Dataset Noise Ratio r Sym Asym 0 20% 50% 80% 90% 40% / 45% ŒªU CIFAR10 - 0 25 25 50 0 CIFAR100 - 25 150 150 150 0 Tiny-ImageNet 0 30 200 300 - 0 Clothing1M 0 WebVision 0 (a) Sym 20% (b) Sym 50% (c) Sym 80% (d) Sym 90% Figure 6. T-SNE visualizations of features and prototypes. The subplots show class distributions after training networks for 400 epochs on the CIFAR10 dataset with various noise ratios: (a) 20% symmetric, (b) 50% symmetric, (c) 80% symmetric, (d) 90% symmetric. Our proposed method effectively learns robust representations and class prototypes even with high noise ratios. 3 airplane‚úî automobile clean: noisy: airplane‚úî horse airplane‚úî cat airplane‚úî deer airplane‚úî frog dog‚úî bird clean: noisy: dog‚úî dog dog‚úî dog dog‚úî frog dog‚úî frog automobile‚úî    bird clean: noisy: automobile‚úî   ship automobile‚úî bird automobile‚úî automobile   automobile‚úî horse   bird‚úî truck clean: noisy: bird‚úî bird bird‚úî bird bird‚úî ship bird‚úî airplane cat‚úî cat clean: noisy: cat‚úî ship cat‚úî cat cat‚úî cat cat‚úî cat deer‚úî ship clean: noisy: deer‚úî dog bird‚ùå bird deer‚úî deer deer‚úî truck frog‚úî deer clean: noisy: frog‚úî frog frog‚úî dog frog‚úî horse frog‚úî ship horse‚úî dog clean: noisy: horse‚úî automobile horse‚úî ship horse‚úî deer horse‚úî bird ship‚úî bird clean: noisy: ship‚úî horse ship‚úî cat ship‚úî horse ship‚úî cat truck‚úî truck clean: noisy: truck‚úî frog truck‚úî automobile truck‚úî dog truck‚úî truck airplane automobile bird cat deer dog frog horse ship truck Figure 7. Visualization of images and labels in CIFAR10 dataset with 90% label noise. For each class, we select and show those images with top-5 largest cosine similarity features to the prototypes. Most of the image features are correctly assigned to the correct class, which demonstrates the effectiveness of our proposed PLR loss. 4 "
}