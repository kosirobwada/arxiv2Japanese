{
    "optim": "PLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive\nRepresentation Learning\nXiaoyu Liu, Beitong Zhou, Cheng Cheng‚àó\nHuazhong University of Science and Technology\n{lxysl, zhoubt, c_cheng}@hust.edu.cn\nAbstract\nRecently, the application of Contrastive Representation\nLearning (CRL) in learning with noisy labels (LNL) has\nshown promising advancements due to its remarkable ability\nto learn well-distributed representations for better distin-\nguishing noisy labels. However, CRL is mainly used as a\npre-training technique, leading to a complicated multi-stage\ntraining pipeline. We also observed that trivially combining\nCRL with supervised LNL methods decreases performance.\nUsing different images from the same class as negative pairs\nin CRL creates optimization conflicts between CRL and the\nsupervised loss. To address these two issues, we propose an\nend-to-end PLReMix framework that avoids the complicated\npipeline by introducing a Pseudo-Label Relaxed (PLR) con-\ntrastive loss to alleviate the conflicts between losses. This\nPLR loss constructs a reliable negative set of each sample\nby filtering out its inappropriate negative pairs that over-\nlap at the top Œ∫ indices of prediction probabilities, lead-\ning to more compact semantic clusters than vanilla CRL.\nFurthermore, a two-dimensional Gaussian Mixture Model\n(GMM) is adopted to distinguish clean and noisy samples by\nleveraging semantic information and model outputs simul-\ntaneously, which is expanded on the previously widely used\none-dimensional form. The PLR loss and a semi-supervised\nloss are simultaneously applied to train on the GMM divided\nclean and noisy samples. Experiments on multiple bench-\nmark datasets demonstrate the effectiveness of the proposed\nmethod. Our proposed PLR loss is scalable, which can be\neasily integrated into other LNL methods and boost their\nperformance. Codes will be available.\n1. Introduction\nDeep neural networks (DNNs) lead to huge performance\nboosts in various classification tasks [14, 25, 41, 42]. How-\never, the large-scale, high-quality annotated datasets re-\nquired by training DNNs are extremely expensive and time-\nconsuming. Many studies turn to collect data in less ex-\nNegative\npair\nInappropriate\nnegative\npair\nPrediction \nprobabilities\nùë•!\nùë•\"\nùë•#\nx\ntop!\n(‚ãÖ)\nFigure 1. Left: Model performance suffers from trivially combining\ncontrastive representation learning with supervised learning. The\nperformance of DivideMix [28] (a supervised LNL method) boosts\nfrom using SimCLR [8] pretrained weights (DivideMix‚àó), whereas\nsuffers from being trivially combined with SimCLR contrastive loss\n(DivideMix w/ SimCLR). While DivideMix with our proposed\nPLR contrastive loss (DivideMix w/ PLR) achieves comparable\nresults to DivideMix‚àó. Right: Our proposed PLR contrastive\nloss selects a set of reliable negative pairs for each sample. The\nnegative pair should have no overlap with the sample‚Äôs prediction\nprobabilities at topŒ∫ indices (Œ∫ = 2 in Figure). As an instance,\nxk is a negative pair of xi as top(i)\n2\n‚à© top(k)\n2\n= ‚àÖ, while xj is an\ninappropriate negative pair of xi as top(i)\n2\n‚à© top(j)\n2\nÃ∏= ‚àÖ.\npensive but more efficient ways, such as querying search\nengines [6, 9, 33] , crawling images with tags [11, 35, 36] or\nannotating with network outputs [26]. Datasets collected in\nsuch alternative ways inevitably contain noisy labels. These\ncorrupted noisy labels severely interfere with the model‚Äôs\ntraining process due to over-fitting, which leads to worse\ngeneralization performance [2, 59].\nExisting methods dealing with the so-called learning with\nnoisy labels (LNL) problem can be mainly classified into\ntwo categories, namely label correction and sample selection.\nLabel correction methods leverage the model predictions to\ncorrect noisy labels [1, 40]. Sample selection techniques\nattempt to divide samples with clean labels (usually with\nsmall losses) from noisy datasets [13, 19, 28]. Recent studies\ntypically filter out the potentially noisy samples first, then\napply label correction to those noisy samples, and finally\n1\narXiv:2402.17589v1  [cs.CV]  27 Feb 2024\ntrain the model in a semi-supervised way [28].\nMost label correction and sample selection methods take\nadvantage of early learning phenomenon during the model\ntraining process [2]. Nevertheless, the intrinsic semantic\ninformation in data inherently resistant to label noise mem-\norization has been disregarded in these methods [53]. The\nrapid development of self-supervised contrastive representa-\ntion learning (CRL) methods [8, 16] shows new possibilities\nfor modeling data representations without the requirements\nof data labels, which is beneficial for the model to correct\ncorrupted labels or filter out noisy samples in datasets. Most\nrecent studies either utilize the CRL to acquire robust pre-\ntrained weights [12, 44, 62] or directly add the CRL loss to\nthe total loss accompanying the supervised loss [18, 20].\nWhile CRL helps models to learn representations with-\nout labels, a decrease in accuracy has been observed when\nit was trivially integrated with supervised losses (Figure 1\nleft). To address this issue, we propose the Pseudo-Label\nRelaxed (PLR) contrastive representation learning to avoid\nthe conflict between supervised learning and CRL (Figure 1\nright). Inappropriate negative pairs are removed by check-\ning if their top Œ∫ indices of prediction probabilities have an\nempty intersection. This results in a reliable negative set\nand will construct a more effective semantic clustering than\nvanilla CRL.\nIn addition to improving model robustness to noisy labels\nby utilizing CRL, we also design a new technique to filter out\nnoisy samples based on inconsistencies between labels and\nsemantics. Considering the semantic representation cluster\nformed by CRL, when the similarity between one sample‚Äôs\nprojection embedding with its label prototype is smaller than\nthat between itself and its cluster prototype, it may be given\nthe wrong label. We adopt cross-entropy loss to formalize\nthis sample selection method to obtain a consistent form\nwith small loss selection techniques. Then, we dynamically\nfit a two-dimensional Gaussian Mixture Model (2d GMM)\non the joint distribution of these two losses to divide clean\nand noisy samples.\nOur main contributions are as follows:\n‚Ä¢ We analyze the conflict between supervised learning and\nCRL, attributing it to objective inconsistency and gradient\nconflicts. A novel PLR loss is proposed to alleviate this\nissue, facilitating our end-to-end PLReMix framework to\nenhance the capacity for LNL problems. Our proposed\nPLR loss can be easily integrated into other LNL methods\nand boost their performance as well.\n‚Ä¢ We propose a sample selection method based on the in-\nconsistency between semantic representation clusters and\nlabels. A 2d GMM is utilized to filter out noisy samples,\nconsidering both the early learning phenomenon of the\nmodel and intrinsic correlation in data.\n‚Ä¢ Extensive experiments on multiple benchmark datasets\nwith varying types and label noise ratios demonstrate the\neffectiveness of our proposed method. We also conduct\nablation studies and other analyses to verify the robustness\nof the components.\n2. Related Work\n2.1. Learning with noisy labels\nLabel correction. Label correction methods attempt to\nuse the model predictions to correct the noisy labels. To this\nend, during training, a noise transition matrix is estimated\nto correct prediction by transforming noisy labels to their\ncorresponding latent ground truth [9, 47]. Huang et al. [17]\nupdate the noisy labels with model predictions through the\nexponential moving average. P-correction [56] treats the\nlatent ground truth labels as learnable parameters and utilizes\nback-propagation to update and correct noisy labels.\nSample selection. Sample selection techniques attempt\nto filter the samples with clean labels from a noisy dataset.\nHan et al. [13] select samples with small losses from one\nnetwork as clean ones to teach the other. Dividemix [28]\nutilizes a two-component GMM to model the loss distri-\nbution to separate the dataset into a clean set and a noisy\nset. Sukhbaatar et al. [51] also consider the samples with\nconsistent prediction and high confidence as clean samples.\nBoth label correction and sample selection methods bene-\nfit from the early learning phenomenon [2, 23, 31]. That is\nto say, the weights of DNNs will not stray far from the initial\nweights in the early stage of training [31]. Notably, several\nprior studies [2, 3, 23, 46] also observe the memorization\neffect in DNNs. This phenomenon demonstrates that DNNs\ntend to learn simple patterns and, over time, gradually overfit\nto more complex and noisy patterns.\n2.2. Contrastive representation learning\nCRL in self-supervised learning. CRL, a representative\nself-supervised learning technique, holds great promise in\nlearning label-irrelevant representations. Basically, the main\nidea of self-supervised learning is to design a pretext task\nthat allows models to learn pre-trained weights. CRL learns\nlabel-irrelevant representations of the model that identify\npositive and negative examples corresponding to a single\nsample from a batch of data. In SimCLR [8], two random\naugmentations are applied to each sample to obtain pairs\nof positive samples, while the other augmented samples are\nconsidered negative examples. A momentum encoder and\na memory bank are adopted in MoCo [16] to generate and\nretrieve negative features of samples, which alleviates the\nhigh demand for large batch sizes.\nCRL in learning with noisy labels. As a label-free\nmethod, CRL is resistant to noisy labels and can be beneficial\nfor the LNL problem. Most of the recent studies follow a two\nor three-stage paradigm that uses CRL as a pretext task to\nobtain a pre-trained foundation model. Zheltonozhskii et al.\n2\n[62] and Ghosh et al. [12] use CRL as a warmup method and\noutperform the fully-supervised pre-training on ImageNet.\nScanMix [44] uses the CRL following a deep clustering\nmethod to pre-train the model, then jointly train the network\nwith semi-supervised loss and clustering loss. Zhang et al.\n[60] utilizes CRL to pretrain the model, then trains a robust\nclassification head on a fixed backbone, and finally trains in a\nsemi-supervised way with a graph-structured regularization.\nWe compare these frameworks in Figure 2 right.\nA few methods have been proposed to address the com-\nplexity associated with pretrain fine-tuning pipeline of CRL\nwithin the overall LNL framework. Nevertheless, directly\ncombining instance-wise CRL with supervised training ob-\njectives can interfere with the class-clustering ability, unlike\nforming clusters in a low-dimensional hypersphere surface in\nan unsupervised situation. In MoPro [29] and ProtoMix [30],\nthe prototypical CRL that enforces the sample embedding\nto be similar to its assigned prototypes is jointly used with\nstandard instance-wise CRL. MOIT [39] uses mixup interpo-\nlated supervised contrastive learning [21] to improve model\nrobustness to label noise. UNICON [20] divides the dataset\ninto clean and noisy sets, then trains in a semi-supervised\nmanner with CRL loss applied on the noisy sets. Sel-CL+\n[32] selects confident pairs by measuring the agreement be-\ntween learned representations and labels to apply supervised\ncontrastive learning to the LNL problem. Kim et al. [22]\nand Yan et al. [55] propose negative learning approaches to\nalleviate the impact of noisy labels by teaching the model to\nlearn this is not something instead of this is something. Our\nmethod extends the self-supervised CRL by constructing a\nreliable negative set for each sample using pseudo-labels that\nthe model predicts, thus alleviating the wrong contrast.\n3. Proposed Method\n3.1. Overview\nThe noisy dataset given in LNL is denoted as D =\n{xi, yi}N\ni=1, where xi represents a sample and yi is the cor-\nresponding label over C classes, N is the number of samples\nin the dataset. The presence of noisy samples causes label\ndifferences between the annotated labels {yi}N\ni=1 and the\nground truth labels {Àúyi}N\ni=1. Our proposed framework it-\neratively trains two identical networks, each consisting of\na convolutional backbone f(¬∑; Œ∏(m)), a classification head\ng(¬∑; œï(m)), and a projection head h(¬∑; œà(m)). m ‚àà {0, 1}\ndenotes the network number. Given a minibatch of b sam-\nples, the output logits of input x = {xi}b\ni=1 are denoted as\nz = g(f(x; Œ∏(m)); œï(m)), the output prediction probabili-\nties are denoted as t = softmax(z), the projection embed-\ndings are denoted as q = h(f(x; Œ∏(m)); œà(m)). We use a\nweak augmentation œÜ and a strong augmentation œâ. Class\nprototypes P = {pk}C\nk=1, are maintained and updated every\nepoch to represent the feature prototypes of the k-th class,\nwhere pk ‚àà Rd, d is the dimension of projection embedding.\nFor the initialization of class prototypes P and initial\nmodel convergence, we warm up the two networks for a few\nepochs with cross-entropy loss and standard CRL loss on\nthe whole dataset. At the end of the warmup, we initialize\nclass prototypes by taking the average of feature embeddings\nwith the same noisy labels. As shown in Figure 2 left, the\nproposed end-to-end LNL framework performs the following\ntwo steps iteratively in the training process: (1) divide the\nwhole dataset into a clean and a noisy set considering their\nlosses by a novel two-component 2d GMM (Section 3.2)\nand (2) train the other network with proposed Pseudo-Label\nRelaxed contrastive loss LPLR on the representations with\na set of reliable negative pairs Ni for each sample (Section\n3.3), and semi-supervised loss LSST on the divided clean\nand noisy sets (Section 3.4). Ni is denoted as N in Figure 2\nleft.\n3.2. Joint sample selection\nSample selection We compute two cross-entropy losses\nli,cls and li,proto to fit a two-component 2d GMM for sam-\nple selection. First, following the small loss selection tech-\nnique [13], we compute the cross-entropy classification loss\nli,cls = ‚àí log(tyi\ni ) to measure how well the model fits the\npattern of a sample. Then, we define the class prototypes as\nthe center of projection embeddings with similar semantics.\nThe similarity between each projection embedding qi and\nall the class prototypes {pk}C\nk=1 represents the latent class\nprobability distributions at the semantic level. Following\nMoPro [29], we use softmax normalized cosine similarity to\nmeasure the similarity between qi and pk:\nsi = {sk\ni }C\nk=1 =\n(\nexp (qi ¬∑ pk/œÑs)\nPC\nk=1 exp (qi ¬∑ pk/œÑs)\n)C\nk=1\n,\n(1)\nwhere sharpen temperature œÑs is 0.1 in this paper. Projection\nembeddings for the samples with similar semantics tend to\nform clusters around corresponding prototypes under the\neffect of CRL. In such a case, if a sample xi is mislabeled\nto yi, we suppose to have syi\ni\n< sÀúyi\ni , where Àúyi is the latent\nground truth class of xi. Based on the above discussion,\nthe distance between the distribution of si and OneHot(yi)\nis supposed to be smaller for clean samples than for noisy\nsamples. Thus, we can compute another cross-entropy loss\nli,proto = ‚àí log(syi\ni ) to measure if the given label matches\nthe semantic cluster.\nAfter obtaining the li,cls and the li,proto for all sam-\nples, a two-component 2d GMM on the distribution of\nl = {li}N\ni=1 = {(li,cls, li,proto)}N\ni=1 can be computed. Sup-\npose the two Gaussian components are G1(¬∑; ¬µ1, Œ£1) and\nG2(¬∑; ¬µ2, Œ£2), we obtain the probability wi of each sample\nto be clean from posterior probability as:\nwi = p (Gj | li)\nwith\nj = arg min\nj‚àà{1,2}\n\r\r¬µj\n\r\r\n2.\n(2)\n3\nùëì(\"#$)\n‚Ñé(\"#$)\nùëî(\"#$)\nùëû\nùë∑\nùíî\nùíï\nùëô!\"#$#\nùëô%&'\nùíô\nùëì($)\n‚Ñé($)\nùëû(\nùíô\nùëì($)\n‚Ñé($)\nùëû),(\nùúî+\nùúî,\nùí©\n‚Ñí-./\nùëì($)\nùëì($)\nùëî($)\nùëî($)\nùíï0\nùíï(\nMixUp\nùíô0, ùíñ0\nùíô(, ùíñ(\nRefine\n‚Ñí112\n‚Ñí\nùí≥, ùí∞\n(‚Ö∞)\n(‚Ö±)\n(‚Ö≤)\n2d GMM\nForward\nForward-Backward\nSelection\nShare Weights\nLNL (e.g., DivideMix)\nLNL (e.g., DivideMix)\n1d GMM\nSST\n ùí≥\n ùí∞\n1d GMM\nSST\n ùí≥\n ùí∞\n2d GMM\nPLR\n ùí≥\n ùí∞\nSST\n2d GMM\nPLR\n ùí≥\n ùí∞\nSST\nSelf-supervised\nPretrain\nSelf-supervised\nPretrain\nClustering\nPretrain\n(a)\n(b)\n(d)\n(c)\nFigure 2. Left: Overview of the proposed method. (i) We perform joint sample selection on the first network to divide clean and noisy\nsamples with 2d GMM. (ii) We train the second network with proposed Pseudo-Label Relaxed contrastive loss LPLR with the constructed\nreliable negative set N alleviating the conflict between the supervised learning and CRL. (iii) We utilize semi-supervised training for the\nsecond network on the clean and noisy samples divided by 2d GMM separately. The final learning objective L comprises two components,\nnamely LPLR and LSST. Right: Comparison of different LNL frameworks utilizing CRL. (a) DivideMix [28] iteratively performs sample\nselection (1d GMM) and semi-supervised training (SST). (b) The two-stage method [12, 60, 62] performs self-supervised pretraining before\nLNL. (c) The three-stage method [44] performs self-supervised pretraining and clustering pretraining before LNL. (d) By proposing PLR\nloss, our method unifies SST and CRL in an end-to-end PLReMix framework.\nSamples predicted have smaller\n\r\r¬µj\n\r\r\n2 are considered\nas clean samples, which form the clean set denoted by X.\nWhile the remaining samples are considered as noisy and\nform the noisy set U.\nMomentum class prototypes We adopt noise correction\nand exponential moving average (EMA) to update the class\nprototypes {pk}C\nk=1 while training. Due to the slow conver-\ngence speed of CRL, its projection embeddings qi are not\noptimal and robust in the early epochs, thus resulting in inac-\ncurate si in Eq. (1). To better estimate the latent ground truth\nlabel Àúyi, we generate a pseudo soft label Œ¥i = Œ±ti+(1‚àíŒ±)si\nfor xi considering both model predictions and semantic clus-\nter. Accordingly, noise correction and sample selection are\nperformed to set a correction threshold œÑ1 and a high predic-\ntion confidence threshold œÑ2, i.e.\nÀÜyi =\nÔ£±\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£≥\narg max\nc‚ààC\nŒ¥c\ni\nif max\nc‚ààC Œ¥c\ni > œÑ1,\nyi\nelseif max Œ¥yi\ni > œÑ2,\nunknown\notherwise.\n(3)\nThe selected high prediction confidence samples {xi | i Ã∏=\nunknown} constitute the confident set denoted as C. With\nthe estimated labels, we update the projection embedding qi\nto the corresponding class prototype pÀÜyi using EMA:\npÀÜyi ‚Üê Normalize\n\u0000Œ∑pÀÜyi + (1 ‚àí Œ∑)Normalize(qi)\n\u0001\n,\n(4)\nfor all {xi, ÀÜyi} ‚àà C, where Normalize(x) = x/ ‚à•x‚à•2 and\nthe momentum coefficient Œ∑ is set to 0.99.\n3.3. Pseudo-Label Relaxed contrastive representa-\ntion learning\nTypically, unsupervised CRL methods use two transforms\nof a single sample as a positive pair and combinations of\ntransforms of other samples as negative pairs. We intend to\ndesign a loss function that will bring positive pairs together\nand push negative pairs apart. Specifically, the vanilla In-\nfoNCE loss for the embedding qi,œâ of an augmented sample\nused in CPC [38] and SimCLR [8] are defined as:\n‚àí\nN\nX\ni=1\nlog\nexp\n\u0000\nqi,œâ, q+,œâ\n\u000b\n/œÑ\n\u0001\nexp\n\u0000\nqi,œâ, q+,œâ\n\u000b\n/œÑ\n\u0001\n+\nP\nqj,œâ‚àà{q‚àí,œâ} exp\n\u0000\nqi,œâ, qj,œâ\n\u000b\n/œÑ\n\u0001\n,\n(5)\nwhere œÑ is the sharpening temperature. {q+,œâ} and {q‚àí,œâ}\nare the projection embeddings of augmented positive and\nnegative samples, respectively, ‚ü®a, b‚ü© represents cosine sim-\nilarity Normalize (a ¬∑ b). We note that there exist some\nnegative embeddings qj,œâ ‚àà {q‚àí,œâ} whose ground truth\nlabels are identical to the positive embedding qi,œâ. These\nsamples will also be pushed away from qi,œâ with the use\nof the term exp\n\u0000\nqi,œâ, qj,œâ\n\u000b\n/œÑ\n\u0001\nin Eq. (5), which conflicts\n4\nwith the design purpose of other supervised losses used in\nLNL problems. Worse still, this effect is exacerbated when\nthe i-th sample and j-th sample are more similar, resulting\nin a more dispersed cluster.\nWe proposed the PLR loss to address this issue, which is\nrobust to both clean and noisy samples. A reliable negative\nset Ni is designed to remove the samples in {q‚àí,œâ} that\nshare the same latent ground truth with qi,œâ. To this end,\ngiven a prediction probability ti = [ti,1, ti,2, . . . , ti,C], we\nfirst identify the indexes of the top Œ∫ digits of the prediction\nprobability ti of the i-th sample, which is denoted by top(i)\nŒ∫ :\ntop(i)\nŒ∫ = arg max\nŒ∫\n{ti,Œ∫}C\nŒ∫=1.\n(6)\nThus, the reliable negative set of xi can be obtained as:\nn\nj | top(i)\nŒ∫ ‚à© top(j)\nŒ∫\n= ‚àÖ, ‚àÄj\no\n.\n(7)\nDuring the early training stage, we additionally append the\ngiven label yi to the top(i)\nŒ∫ for further conflict avoidance:\nNi =\nn\nj | {top(i)\nŒ∫ ‚à™ yi} ‚à© {top(j)\nŒ∫\n‚à™ yj} = ‚àÖ, ‚àÄj\no\n.\n(8)\nFinally, the proposed PLR loss is re-formulated based on the\nvanilla InfoNCE loss of Eq. (5) to the following expression:\nLPLR = ‚àí\nN\nX\ni=1\nlog\nexp\n\u0000\nqi,œâ, q+,œâ\n\u000b\n/œÑ\n\u0001\nexp\n\u0000\nqi,œâ, q+,œâ\n\u000b\n/œÑ\n\u0001\n+\nP\nj‚ààNi exp\n\u0000\nqi,œâ, qj,œâ\n\u000b\n/œÑ\n\u0001\n.\n(9)\nWe use two forms of PLR loss in this paper: the FlatNCE\n[7] formalized PLR loss is denoted as FlatPLR, while the\nInfoNCE formalized one is denoted just as PLR. We do ex-\nperiments on gradient analysis between LPLR and LCRL in\nSection 4.4, whose results show that LPLR has less gradient\nconflicts with the supervised loss.\n3.4. Semi-supervised training\nAs described in Section 3.2, according to the proposed\nsample selection technique, we divide the noisy dataset D\ninto a labeled set X and an unlabeled set U using 2d GMM.\nThen, we perform semi-supervised training on them follow-\ning FixMatch [45] and MixMatch [4].\nFirst, we generate pseudo targets yi for each sample using\nweak augmentations:\nÔ£±\nÔ£≤\nÔ£≥\nSharpen\n\u0010\nwi OneHot (yi) +\n1‚àíwi\ncard(œÜ)\nP\nœÜ ti,œÜ; T\n\u0011\n, if xi ‚àà X,\nSharpen\n\u0010\n1\ncard(œÜ) card(m)\nP\nœÜ,m tm\ni,œÜ; T\n\u0011\n, if xi ‚àà U,\n(10)\nand\nSharpen(¬Øyi; T) = ¬Øy\nc 1\ni T\n/\nC\nX\nc=1\n¬Øy\nc 1\ni T\n, for c = 1, 2, ..., C, (11)\nwhere card(œÜ) and card(m) represent the numbers of aug-\nmentations and models, respectively, and the sharpening\ntemperature T is 0.5. Then we do MixUp [61] on the strong\naugmented samples xi,œâ and pseudo targets ¬Øyi:\nx‚Ä≤\ni,œâ = Œªxi,œâ + (1 ‚àí Œª)xr(i),œâ,\n¬Øy‚Ä≤\ni = Œª¬Øyi + (1 ‚àí Œª)¬Øyr(i),\n(12)\nwhere Œª ‚àº Beta(Œ≤, Œ≤), r(i) represents the random permu-\ntation of indices i. Finally, the semi-supervised loss LSST\nconsists of three parts: a cross-entropy loss on the labeled\nset, a mean squared error on the unlabeled set, and a regular-\nization term Lreg used in [49] and [1]:\nLSST = ‚àí 1\n|X|\nX\nxi‚ààX\nC\nX\nc=1\n¬Øy‚Ä≤c\ni log(t‚Ä≤c\ni,œâ)+\nŒªU\n1\n|U|\nX\nxi‚ààU\n\r\r¬Øy‚Ä≤\ni ‚àí t‚Ä≤\ni,œâ\n\r\r2\n2 + Lreg,\n(13)\nwhere ŒªU is a coefficient for tuning. The final objective\nobjective L is to minimize the following function:\nL = LSST + ŒªiLPLR,\n(14)\nwhere Œªi is a tunable hyperparameter.\n4. Experiments\n4.1. Experimental settings\nCIFAR10/100 The CIFAR10/100 dataset [24] comprises\n50K training images and 10K test images. We adopt symmet-\nric and asymmetric synthetic label noise models as used in\nprevious studies [49]. The symmetric noise setting replaces\nr% of labels from one class uniformly with all possible\nclasses, while asymmetric replaces them from one class only\nwith the most similar class.\nWe use the PreAct ResNet-18 [15] network on CIFAR\n10/100 and train it using an SGD optimizer. To avoid conflict\nearly and gain robust representation later, we reduce Œ∫ from\n3 to 2 to 1 at the epochs of 40 and 70. In this work, we use\nAutoAug [10] as a strong augmentation operation, which\nis demonstrated to be useful in the LNL problem [37]. We\nuse FlatPLR for CIFAR experiments to gain a greater thrust\nbetween samples and their negative pairs with a relatively\nsmall batch size. For more training details, please refer to\nthe Appendix.\nTiny-ImageNet Tiny-ImageNet [27] consists of 200\nclasses, each containing 500 smaller-resolution images from\nImageNet. We train a PreAct ResNet 18 [15] network with a\nbatch size of 128 and a learning rate of 0.01.\nClothing1M Clothing1M [54], a large-scale real-world\ndataset with label noise, contains 1M clothing images from\n14 classes. We sample 64K images in total from each class\n[28]. We train a ResNet50 network with weights pre-trained\non ImageNet [14] for 100 epochs.\nWebVision WebVision dataset [33] contains 2.4 M im-\nages crawled from Flickr and Google. All of the images are\n5\nTable 1. Experimental and comparison results on CIFAR-10 and CIFAR-100 datasets with different noise ratios (test accuracy %).\nMethod\nDataset\nCIFAR-10\nCIFAR-100\nMode\nSym\nAsym\nSym\nAsym\nr\n20%\n50%\n80%\n90%\n40%\n20%\n50%\n80%\n90%\n40%\nCross-entropy\nBest\n86.8\n79.4\n62.9\n42.7\n85.0\n62.0\n46.7\n19.9\n10.1\n44.5\nLast\n82.7\n57.9\n26.1\n16.8\n72.3\n61.8\n37.3\n8.8\n3.5\n-\nCo-teaching+ [58]\nBest\n89.5\n85.7\n67.4\n47.9\n-\n65.6\n51.8\n27.9\n13.7\n-\nLast\n88.2\n84.1\n45.5\n30.1\n-\n64.1\n45.3\n15.5\n8.8\n-\nMixUp [61]\nBest\n95.6\n87.1\n71.6\n52.2\n-\n67.8\n57.3\n30.8\n14.6\n48.1\nLast\n92.3\n77.3\n46.7\n43.9\n77.7\n66.0\n46.6\n17.6\n8.1\n-\nDivideMix [28]\nBest\n96.1\n94.6\n93.2\n76.0\n93.4\n77.3\n74.6\n60.2\n31.5\n55.1\nLast\n95.7\n94.4\n92.9\n75.4\n92.1\n76.9\n74.2\n59.6\n31.0\n-\nScanMix [44]\nBest\n96.0\n94.5\n93.5\n91.0\n93.7\n77.0\n75.7\n66.0\n58.5\n-\nLast\n95.7\n93.9\n92.6\n90.3\n93.4\n76.0\n75.4\n65.0\n58.2\n-\nMOIT+ [39]\nBest\n94.1\n91.8\n81.1\n74.7\n93.3\n75.9\n70.6\n47.6\n41.8\n74.0\nSel-CL+ [32]\nBest\n95.5\n93.9\n89.2\n81.9\n93.4\n76.5\n72.4\n59.6\n48.8\n72.7\nELR+ [34]\nBest\n95.8\n94.8\n93.3\n78.7\n93.3\n77.6\n73.6\n60.8\n33.4\n73.2\nUNICON [20]\nBest\n96.0\n95.6\n93.9\n90.8\n94.1\n78.9\n77.6\n63.9\n44.8\n74.8\n(Flat)\nPLReMix\nBest\n96.63\n95.71\n95.08\n91.93\n95.11\n77.95\n77.78\n68.76\n50.17\n64.89\nLast\n96.46\n95.36\n94.84\n91.54\n94.72\n77.78\n77.31\n68.41\n49.44\n62.67\nTable 2. Experimental and comparison results on Tiny-ImageNet\ndataset (test accuracy %).\nMethod\nr\nSym\nAsym\n0\n20%\n50%\n80%\n45%\nCross-entropy\nBest\n57.4\n35.8\n19.8\n-\n26.3\nLast\n56.7\n35.6\n19.6\n-\n26.2\nCo-teaching+ [58]\nBest\n52.4\n48.2\n41.8\n-\n26.9\nLast\n52.1\n47.7\n41.2\n-\n26.5\nM-correction [1]\nBest\n57.7\n57.2\n51.6\n-\n24.8\nLast\n57.2\n56.6\n51.3\n-\n24.1\nUNICON [20]\nBest\n63.1\n59.2\n52.7\n-\n-\nLast\n62.7\n58.4\n52.4\n-\n-\n(Flat)\nPLReMix\nBest\n62.93\n60.70\n54.94\n37.47\n33.97\nLast\n62.46\n60.39\n54.31\n36.43\n33.25\ncategorized into 1,000 classes, which is the same as Ima-\ngeNet ILSVRC12 [43]. We use the first 50 classes of the\nGoogle image subset to train an InceptionResnet V2 [48]\nfrom scratch. Multi-crop augmentation strategy [5] is ex-\nploited to obtain more robust representations in rather shorter\nepochs and smaller batch size so that contrastive loss can\nbe jointly trained with the semi-supervised loss. See the\nAppendix for more details about multi-crop.\n4.2. Experimental results\nThe proposed end-to-end PLReMix framework is com-\npared with other LNL methods on different datasets using\nthe same network architecture. The experimental results on\nCIFAR10/100 are shown in Table 1. We report the best test\naccuracy over all epochs and the average test accuracy of the\nTable 3. Experimental and comparison results on Clothing1M\ndataset (test accuracy %).\nMethod\nTest Accuracy\nCross-Entropy\n69.21\nJoint-Optim [49]\n72.16\nP-correction [56]\n73.49\nDivideMix [28]\n74.76\nELR+ [34]\n74.81\nUNICON [20]\n74.98\nC2D [62]\n74.30\nScanMix [44]\n74.35\nPLReMix\n74.85\nlast 10 epochs as Best and Last, respectively. Considering\nthe intrinsic semantic features by utilizing PLR loss and 2d\nGMM, our proposed method outperforms state-of-the-art\n(SOTA) results in most experimental settings, especially on\ndata with high noise ratios. We attribute our inferior results\nto SOTA on CIFAR100 with 40% asymmetric noise to worse\ninitialization of class prototypes across too many classes, and\nthe absence of a uniform sample selection technique proved\nto be useful [20]. However, our method still outperforms the\nbaseline method [28] a lot.\nThe experimental results on Tiny-ImageNet are shown\nin Table 2. Our algorithm achieves SOTA performance on\nmost noise ratios. The results of the real-world noisy dataset\nClothing1M are shown in Table 3, and the results of the We-\nbVision dataset are listed in Table 4. Our algorithm achieves\nperformance comparable to SOTA. It is notable that C2D\n[62] and ScanMix [44] use pretrained weights in a two/three-\n6\nTable 4.\nExperimental and comparison results on WebVision\nand ILSVRC12 datasets (test accuracy %). Models are trained\non the WebVision dataset and tested on both the WebVision and\nILSVRC12 datasets. Top-1 and Top-5 test accuracy are reported.\nDataset\nArchitecture\nWebVision\nILSVRC12\nMethod\nTop-1\nTop-5\nTop-1\nTop-5\nCo-Teaching [13]\nInception V2\n63.58\n85.20\n61.48\n84.70\nDivideMix [28]\nInception V2\n77.32\n91.64\n75.20\n90.84\nELR+ [34]\nInception V2\n77.78\n91.68\n70.29\n89.76\nUNICON [20]\nInception V2\n77.60\n93.44\n75.29\n93.72\nSel-CL+ [32]\nInception V2\n79.96\n92.64\n76.84\n93.04\nScanMix [44]\nInception V2\n80.04\n93.04\n75.76\n92.60\nPLReMix\nInception V2\n80.19\n93.51\n75.82\n93.16\nTable 5. Experimental results of combining different CRL methods\nwith LNL algorithms (test accuracy %).\nMethod\nCRL\nCIFAR-10\nSym-20%\nSym-50%\nAsym-40%\nCo-teaching+ [58]\n-\n88.73\n77.52\n68.64\nSimCLR\n88.62\n80.24\n68.86\nPLR\n90.28\n85.68\n70.92\nJoCoR [52]\n-\n88.13\n80.94\n75.48\nSimCLR\n84.76\n78.81\n78.56\nPLR\n89.34\n83.46\n81.46\nstage manner. As a comparison, our proposed method trains\nthe model end-to-end with randomly initialized weights.\n4.3. Scalability of PLR\nWe do extra experiments on the CIFAR10 dataset with\nother different LNL algorithms to demonstrate the effective-\nness and scalability of our proposed PLR loss. We integrate\nour proposed PLR loss into Co-teaching+ [58] and JoCoR\n[52] and test the model performance under different noise\nratios. We use PreAct ResNet18 network, and the minor\nmodification to the original algorithms is just training a pro-\njection head using our proposed PLR loss on top of the model\nbackbone. As a comparison, we utilize the vanilla SimCLR\nto train the projection head. The experimental results are\nlisted in Table 5. As can be seen, training with our proposed\nPLR increases the model performance compared to the base-\nline methods, while training with vanilla SimCLR tends to\nmake the model performance corrupted. These results align\nwith our analysis of the gradient conflicts in Section 4.4.\n4.4. Analysis\nMulti-task gradients. Given two losses L(1) and L(2)\nand their gradients to model parameters g(1) and g(2), we\ndefine the intensity of gradient entanglement between them\nas E(1)\n(2) = g(1)¬∑g(2)/\n\r\rg(2)\r\r2\n2, the ratio of gradient magnitude\nbetween them as R(1)\n(2) =\n\r\rg(1)\r\r\n2/\n\r\rg(2)\r\r\n2.\nConflicting gradients and a large difference in gradient\nFigure 3. Gradient conflicts of two contrastive representation learn-\ning methods (vanilla SimCLR and our proposed PLR) when they\nare jointly trained with semi-supervised learning in the presence of\nnoisy labels.\nmagnitudes often lead to optimization challenges in high-\ndimensional neural network multi-task learning [57]. Similar\nissues are witnessed in our experiments. Here, LSimCLR or\nLPLR is treated as L(1), LSST is treated as L(2). As shown\nin Figure 3, we show the distribution of ESimCLR\nSST\nand EPLR\nSST\n(Left), RSimCLR\nSST\nand RPLR\nSST (Right) when training models\non CIFAR10 dataset with 80% noise ratio at epoch 30.\nThe entanglement intensity between SimCLR and SST\nshows more negative values, and their magnitude ratio has\nmore values greater than 1. These indicate that there are\nmore gradient conflicts between LSST and LSimCLR, which\nresult in an optimization challenge and performance decrease\nshown in Figure 1 left red line. Our proposed PLR loss mit-\nigates the gradient conflicts with SST, which manifests as\nmore orthogonal gradients and smaller gradient magnitude\nratios. As a result, it improves the performance when inte-\ngrated with SST to combat noisy labels, as shown in Figure\n1 left green line.\nAblation study. We perform thorough ablation studies\non our method. The results of ablation studies are shown in\nTable 6. We test the effectiveness of FlatNCE by swapping it\nwith (non-Flat) PLR, which is formalized by InfoNCE loss.\nDespite using InfoNCE instead of FlatNCE, our experiments\nstill show the robustness of the proposed PLR loss. Then,\nwe substitute the PLR loss with vanilla SimCLR [8]. Exper-\niments where 2d GMM is replaced with 1d GMM confirm\nthat semantic information helps differentiate clean and noisy\nsamples on the distribution of l = {(li,cls)}N\ni=1. Supervised\ncontrastive loss (SCL) [21] treats samples with the same\nlabels as positive pairs, and others are negative pairs. It can\nbe a substitute for our proposed PLR loss in our framework\nwhen it considers samples with the same predicted pseudo\nlabels as positive pairs. We do experiments to compare SCL\nand PLR in our proposed framework. The results show that\nour proposed PLR outperforms SCL in most noise ratios\nexcept on the CIFAR100 dataset with a high noise ratio.\nWe conduct ablation experiments on the hyperparameter\nŒ∫. Intuitively, larger values of Œ∫ avoid conflict between\n7\nTable 6. Ablation studies on CIFAR-10 and CIFAR-100 datasets\nwith different training settings (test accuracy %).\nMethod\nDataset\nCIFAR-10\nCIFAR-100\nr\n50%\n80%\n90%\n50%\n80%\n90%\n(Flat)\nPLReMix\nBest\n95.71\n95.08\n91.93\n77.78\n68.76\n50.17\nLast\n95.36\n94.84\n91.54\n77.31\n68.41\n49.44\nPLReMix\nBest\n95.71\n93.58\n91.71\n75.64\n66.98\n50.78\nLast\n95.31\n93.39\n91.46\n75.39\n66.76\n50.54\nvanilla\nSimCLR\nBest\n93.44\n90.44\n86.80\n73.10\n63.50\n47.76\nLast\n93.23\n90.28\n86.51\n72.32\n63.02\n46.81\n(Flat)\n1d GMM\nBest\n94.51\n94.82\n92.36\n77.11\n68.54\n50.25\nLast\n94.16\n94.57\n92.12\n76.91\n68.31\n49.99\nSCL\nBest\n94.37\n93.41\n91.24\n72.55\n68.43\n54.07\nLast\n94.08\n93.11\n91.00\n72.18\n68.03\n53.72\nTable 7. Ablation study results when using different Œ∫ or only given\nlabels on CIFAR10 dataset with 80% symmetric label noise.\nŒ∫ = 3\nŒ∫ = 2\nŒ∫ = 1\nonly y\nPLReMix\nBest\n91.83\n93.93\n93.74\n94.74\n95.08\nLast\n91.29\n92.81\n93.60\n94.58\n94.84\nPLR and supervised loss more effectively but result in less\nrobust representation due to insufficient negative pairs. To\naddress this issue, we decrease Œ∫ from 3 to 2 to 1 after\ntraining a certain number of epochs. The decrease epochs\nare not carefully tuned and are set roughly evenly. In this\nablation study, we fix the value of Œ∫ as 3, 2, or 1 during\ndifferent individual training procedures. In addition, we\ntest the performance only using the given labels (denoted\nby only y), which means Ni = {j | yi ‚à© yj = ‚àÖ, ‚àÄj}. The\nexperiments are conducted on the CIFAR10 dataset with\n80% symmetric noise, and the results are listed in Table 7.\nThe experimental results verify our analysis and show the\neffectiveness of the proposed decrease setting.\nVisualization. We visualize the 2d GMM after 10 epochs\nof warming up and 100 epochs of training in Figure 4. We\nplot the losses of 512 randomly selected samples, with half\nfrom the clean set and the other half from the noisy set.\nAs illustrated, our proposed 2d GMM effectively fits the\ndistribution of the clean and noisy samples.\nCorrect ratio of negative pairs. We visualize the cor-\nrect ratio of selected negative pairs in Figure 5 for a better\nunderstanding of the proposed PLR loss. The select ratio is\nthe proportion of selected negative sample pairs out of all\npossible negative sample pairs. The correct ratio represents\nthe proportion of correctly identified negative pairs among\nall selected pairs. We observed that the PLR loss tends to se-\nlect a limited number of negative pairs at first to ensure high\nprecision. As Œ∫ gradually decreases, PLR loss will rapidly\nincrease the number of negative sample pairs selected while\nmaintaining precision.\nSym 90%\nepoch 10\nSym 90%\nepoch 100\nSym 50%\nepoch 10\nSym 50%\nepoch 100\nSym 90%\nepoch 15\nSym 90%\nepoch 100\nSym 50%\nepoch 15\nSym 50%\nepoch 100\nùíç!\"#\nùíç!\"#$#\nùíç!\"#$#\nùíç!\"#\nùíç!\"#\nùíç!\"#\nClean Sample\nNoisy Sample\nClean Distribution\nNoisy Distribution\nFigure 4. Visualization of the 2d GMM fitted on the normalized loss\ndistribution {(lcls, lproto)}. The first and second rows are obtained\nfrom experiments on the CIFAR10 and CIFAR100, respectively.\nFigure 5. Negative sample selection when training with FlatPLR.\nLeft: Negative pairs selected ratio. Right: Correct negative pairs\nselected ratio.\n5. Conclusion and Future Work\nIn this work, we proposed an end-to-end framework for\nsolving the LNL problem by leveraging CRL. We analyzed\nconflicts between CRL learning objectives and supervised\nones and proposed PLR loss to address this issue in a simple\nyet effective way.\nFor further utilizing label-irrelevant\nintrinsic semantic information, we propose a joint sample\nselection technique, which expands previously widely\nused 1d GMM to 2d. Extensive experiments conducted on\nbenchmark datasets show performance improvements of our\nmethod over existing methods. Our end-to-end framework\nshows significant improvement without any complicated\npretrain fine-tuning pipeline. Future research should also\nexplore other forms of PLR loss, such as MoCo-based.\nReferences\n[1] Eric Arazo, Diego Ortego, Paul Albert, Noel O‚ÄôConnor, and\nKevin McGuinness. Unsupervised label noise modeling and\nloss correction. In International Conference on Machine\nLearning, pages 312‚Äì321. PMLR, 2019. 1, 5, 6\n[2] Devansh Arpit, Stanis≈Çaw JastrzÀõebski, Nicolas Ballas, David\nKrueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Ma-\n8\nharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al.\nA closer look at memorization in deep networks. In Inter-\nnational Conference on Machine Learning, pages 233‚Äì242.\nPMLR, 2017. 1, 2\n[3] Yingbin Bai, Erkun Yang, Bo Han, Yanhua Yang, Jiatong\nLi, Yinian Mao, Gang Niu, and Tongliang Liu. Understand-\ning and improving early stopping for learning with noisy\nlabels. Advances in Neural Information Processing Systems,\n34:24392‚Äì24403, 2021. 2\n[4] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas\nPapernot, Avital Oliver, and Colin A Raffel. Mixmatch: A\nholistic approach to semi-supervised learning. Advances in\nNeural Information Processing Systems, 32, 2019. 5\n[5] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Pi-\notr Bojanowski, and Armand Joulin. Unsupervised learning of\nvisual features by contrasting cluster assignments. Advances\nin Neural Information Processing Systems, 33:9912‚Äì9924,\n2020. 6, 1\n[6] Youngchul Cha and Junghoo Cho. Social-network analysis\nusing topic models. In Proceedings of the 35th international\nACM SIGIR conference on Research and development in in-\nformation retrieval, pages 565‚Äì574, 2012. 1\n[7] Junya Chen, Zhe Gan, Xuan Li, Qing Guo, Liqun Chen,\nShuyang Gao, Tagyoung Chung, Yi Xu, Belinda Zeng, Wen-\nlian Lu, et al. Simpler, faster, stronger: Breaking the log-k\ncurse on contrastive learners with flatnce. arXiv preprint\narXiv:2107.01152, 2021. 5\n[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International Conference on\nMachine Learning, pages 1597‚Äì1607. PMLR, 2020. 1, 2, 4, 7\n[9] Xinlei Chen and Abhinav Gupta. Webly supervised learning\nof convolutional networks. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 1431‚Äì\n1439, 2015. 1, 2\n[10] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasude-\nvan, and Quoc V Le. Autoaugment: Learning augmentation\nstrategies from data. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n113‚Äì123, 2019. 5\n[11] Pengfei Deng, Jingkai Ren, Shengbo Lv, Jiadong Feng, and\nHongyuan Kang. Multi-label image recognition in anime il-\nlustration with graph convolutional networks. In Proceedings\nof the AAAI Conference on Artificial Intelligence, 2020. 1\n[12] Aritra Ghosh and Andrew Lan.\nContrastive learning im-\nproves model robustness under label noise. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 2703‚Äì2708, 2021. 2, 3, 4\n[13] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu,\nWeihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching:\nRobust training of deep neural networks with extremely noisy\nlabels. Advances in Neural Information Processing Systems,\n31, 2018. 1, 2, 3, 7\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 770‚Äì778, 2016. 1, 5\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. In Proceedings\nof the European Conference on Computer Vision, pages 630‚Äì\n645. Springer, 2016. 5, 1\n[16] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\nGirshick. Momentum contrast for unsupervised visual rep-\nresentation learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n9729‚Äì9738, 2020. 2\n[17] Lang Huang, Chao Zhang, and Hongyang Zhang.\nSelf-\nadaptive training: beyond empirical risk minimization. Ad-\nvances in Neural Information Processing Systems, 33:19365‚Äì\n19376, 2020. 2\n[18] Zhizhong Huang, Junping Zhang, and Hongming Shan. Twin\ncontrastive learning with noisy labels. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 11661‚Äì11670, 2023. 2\n[19] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li\nFei-Fei. Mentornet: Learning data-driven curriculum for very\ndeep neural networks on corrupted labels. In International\nConference on Machine Learning, pages 2304‚Äì2313. PMLR,\n2018. 1\n[20] Nazmul Karim, Mamshad Nayeem Rizve, Nazanin Rah-\nnavard, Ajmal Mian, and Mubarak Shah. Unicon: Combating\nlabel noise through uniform selection and contrastive learning.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9676‚Äì9686, 2022. 2,\n3, 6, 7\n[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna,\nYonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and\nDilip Krishnan. Supervised contrastive learning. Advances\nin Neural Information Processing Systems, 33:18661‚Äì18673,\n2020. 3, 7\n[22] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim.\nNlnl: Negative learning for noisy labels. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision,\npages 101‚Äì110, 2019. 3\n[23] Jonathan Krause, Benjamin Sapp, Andrew Howard, Howard\nZhou, Alexander Toshev, Tom Duerig, James Philbin, and\nLi Fei-Fei. The unreasonable effectiveness of noisy data for\nfine-grained recognition. In Proceedings of the European\nConference on Computer Vision, pages 301‚Äì320. Springer,\n2016. 2\n[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 5\n[25] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classification with deep convolutional neural networks.\nCommunications of the ACM, 60(6):84‚Äì90, 2017. 1\n[26] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings,\nIvan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov,\nMatteo Malloci, Alexander Kolesnikov, et al. The open im-\nages dataset v4: Unified image classification, object detection,\nand visual relationship detection at scale. International Jour-\nnal of Computer Vision, 128(7):1956‚Äì1981, 2020. 1\n[27] Ya Le and Xuan Yang. Tiny imagenet visual recognition\nchallenge. CS 231N, 7(7):3, 2015. 5\n[28] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix:\nLearning with noisy labels as semi-supervised learning. In\n9\nInternational Conference on Learning Representations, 2019.\n1, 2, 4, 5, 6, 7, 3\n[29] Junnan Li, Caiming Xiong, and Steven Hoi. Mopro: Webly\nsupervised learning with momentum prototypes. In Interna-\ntional Conference on Learning Representations, 2020. 3\n[30] Junnan Li, Caiming Xiong, and Steven CH Hoi. Learning\nfrom noisy data with robust representation learning. In Pro-\nceedings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 9485‚Äì9494, 2021. 3\n[31] Mingchen Li, Mahdi Soltanolkotabi, and Samet Oymak. Gra-\ndient descent with early stopping is provably robust to label\nnoise for overparameterized neural networks. In International\nconference on artificial intelligence and statistics, pages 4313‚Äì\n4324. PMLR, 2020. 2\n[32] Shikun Li, Xiaobo Xia, Shiming Ge, and Tongliang Liu.\nSelective-supervised contrastive learning with noisy labels.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 316‚Äì325, 2022. 3, 6, 7\n[33] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc\nVan Gool. Webvision database: Visual learning and under-\nstanding from web data. arXiv preprint arXiv:1708.02862,\n2017. 1, 5\n[34] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Car-\nlos Fernandez-Granda. Early-learning regularization prevents\nmemorization of noisy labels. Advances in Neural Informa-\ntion Processing Systems, 33:20331‚Äì20342, 2020. 6, 7\n[35] Wei Liu, Yu-Gang Jiang, Jiebo Luo, and Shih-Fu Chang.\nNoise resistant graph ranking for improved web image search.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 849‚Äì856, 2011. 1\n[36] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaim-\ning He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and\nLaurens Van Der Maaten. Exploring the limits of weakly\nsupervised pretraining. In Proceedings of the European Con-\nference on Computer Vision, pages 181‚Äì196, 2018. 1\n[37] Kento Nishi, Yi Ding, Alex Rich, and Tobias Hollerer. Aug-\nmentation strategies for learning with noisy labels. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 8022‚Äì8031, 2021. 5\n[38] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Repre-\nsentation learning with contrastive predictive coding. arXiv\npreprint arXiv:1807.03748, 2018. 4\n[39] Diego Ortego, Eric Arazo, Paul Albert, Noel E O‚ÄôConnor, and\nKevin McGuinness. Multi-objective interpolation training for\nrobustness to label noise. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 6606‚Äì6615, 2021. 3, 6\n[40] Scott Reed, Honglak Lee, Dragomir Anguelov, Christian\nSzegedy, Dumitru Erhan, and Andrew Rabinovich. Train-\ning deep neural networks on noisy labels with bootstrapping.\narXiv preprint arXiv:1412.6596, 2014. 1\n[41] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. Advances in Neural Information Process-\ning Systems, 28, 2015. 1\n[42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmenta-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention‚ÄìMICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234‚Äì241. Springer, 2015. 1\n[43] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision, 115(3):211‚Äì252,\n2015. 6\n[44] Ragav Sachdeva, Filipe Rolim Cordeiro, Vasileios Belagian-\nnis, Ian Reid, and Gustavo Carneiro. Scanmix: Learning\nfrom severe label noise via semantic clustering and semi-\nsupervised learning. Pattern Recognition, 134:109121, 2023.\n2, 3, 4, 6, 7\n[45] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao\nZhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,\nAlexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying\nsemi-supervised learning with consistency and confidence.\nAdvances in Neural Information Processing Systems, 33:596‚Äì\n608, 2020. 5\n[46] Hwanjun Song, Minseok Kim, Dongmin Park, and Jae-Gil\nLee. How does early stopping help generalization against\nlabel noise? arXiv preprint arXiv:1911.08059, 2019. 2\n[47] Sainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir\nBourdev, and Rob Fergus. Training convolutional networks\nwith noisy labels. In International Conference on Learning\nRepresentations, 2015. 2\n[48] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander Alemi. Inception-v4, inception-resnet and the\nimpact of residual connections on learning. In Proceedings\nof the AAAI Conference on Artificial Intelligence, 2017. 6, 1\n[49] Daiki Tanaka, Daiki Ikami, Toshihiko Yamasaki, and Kiy-\noharu Aizawa. Joint optimization framework for learning with\nnoisy labels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 5552‚Äì5560,\n2018. 5, 6\n[50] Laurens Van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-sne. Journal of machine learning research, 9(11),\n2008. 1\n[51] Haobo Wang, Ruixuan Xiao, Yiwen Dong, Lei Feng, and\nJunbo Zhao. Promix: Combating label noise via maximizing\nclean sample utility. arXiv preprint arXiv:2207.10276, 2022.\n2\n[52] Hongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Com-\nbating noisy labels by agreement: A joint training method\nwith co-regularization. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n13726‚Äì13735, 2020. 7\n[53] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.\nUnsupervised feature learning via non-parametric instance\ndiscrimination. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 3733‚Äì\n3742, 2018. 2\n[54] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang\nWang. Learning from massive noisy labeled data for image\nclassification. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 2691‚Äì2699,\n2015. 5\n10\n[55] Jiexi Yan, Lei Luo, Chenghao Xu, Cheng Deng, and Heng\nHuang. Noise is also useful: Negative correlation-steered\nlatent contrastive learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 31‚Äì40, 2022. 3\n[56] Kun Yi and Jianxin Wu. Probabilistic end-to-end noise cor-\nrection for learning with noisy labels. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 7017‚Äì7025, 2019. 2, 6\n[57] Tianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine,\nKarol Hausman, and Chelsea Finn. Gradient surgery for multi-\ntask learning. Advances in Neural Information Processing\nSystems, 33:5824‚Äì5836, 2020. 7\n[58] Xingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang,\nand Masashi Sugiyama. How does disagreement help general-\nization against label corruption? In International Conference\non Machine Learning, pages 7164‚Äì7173. PMLR, 2019. 6, 7\n[59] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,\nand Oriol Vinyals. Understanding deep learning (still) re-\nquires rethinking generalization.\nCommunications of the\nACM, 64(3):107‚Äì115, 2021. 1\n[60] Hui Zhang and Quanming Yao.\nDecoupling representa-\ntion and classifier for noisy label learning. arXiv preprint\narXiv:2011.08145, 2020. 3, 4\n[61] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David\nLopez-Paz. mixup: Beyond empirical risk minimization. In\nInternational Conference on Learning Representations, 2018.\n5, 6\n[62] Evgenii Zheltonozhskii, Chaim Baskin, Avi Mendelson,\nAlex M Bronstein, and Or Litany. Contrast to divide: Self-\nsupervised pre-training for learning with noisy labels. In\nProceedings of the IEEE/CVF Winter Conference on Appli-\ncations of Computer Vision, pages 1657‚Äì1667, 2022. 2, 3, 4,\n6\n11\nPLReMix: Combating Noisy Labels with Pseudo-Label Relaxed Contrastive\nRepresentation Learning\nSupplementary Material\nA. Pseudocode of Pseudo-Label Relaxed loss\nAlgorithm 1 provides the pseudocode of our proposed\nPLR loss. For a mini-batch of samples x, we first calculate\ntheir model prediction probabilities using the first network.\nBased on the predicted probabilities, we then determine if\nthere are overlapping topŒ∫ predictions denoted as conflicts\nand use it to identify feasible negative samples. The reliable\nnegative set Ni for each sample is built in the form of a con-\ntrastive mask, indicating which pairs are negative, positive,\nor neglected when conducting CRL. Finally, we transform\nthe samples x into two strong augmented views, which are\nthen utilized to train a contrastive loss (using the mask to\nidentify the positive and reliable negative pairs) on the other\nnetwork.\nB. Training Details\nCIFAR10/100 We use the PreAct ResNet-18 [15] net-\nwork on CIFAR 10/100 and train it using an SGD optimizer\nwith a weight decay of 5e-4, a momentum of 0.9, and a batch\nsize of 64. We choose ŒªU from {0, 25, 50, 150} following\nthe previous work [28], although experiments show that our\nmethod is not sensitive to this parameter. We set the initial\nlearning rate to 0.02 and reduce it by a factor of 10 after 200\nepochs. The warmup period is 10 for CIFAR10 and 15 for\nCIFAR100. We use the Flat version of the proposed PLR\nloss on these two datasets. To avoid conflict early and gain\nrobust representation later, we reduce Œ∫ from 3 to 2 to 1 at\nthe epochs of 40 and 70. We empirically set œÑ1 = 0.5 for\nX, especially when the label noise is asymmetric; otherwise,\nset œÑ1 = 1/C.\nTiny-ImageNet We train a PreAct ResNet 18 [15] net-\nwork with an SGD optimizer and a batch size of 128. We set\nthe initial learning rate to 0.01 and reduce it by a factor of\n10 after 200 epochs. ŒªU is chosen from {0, 25, 50, 150} and\nŒ≤ is 0.5. We use the Flat version of the proposed PLR loss\non this dataset, and Œ∫ is reduced at the epochs of 40 and 70.\nClothing1M We train a ResNet50 network with weights\npre-trained on ImageNet [14] for 80 epochs with a learning\nrate of 5e-3, a weight decay of 1e-3, and a batch size of 64.\nWe use ŒªU = 0 and Œ≤ = 0.5 on the Clothing1M dataset. The\nnetwork is trained for 100 epochs with a warmup period of 1\nepoch. We reduce Œ∫ at the epochs of 15 and 30. We fix the\nbackbone parameters and use strong augmented samples to\nwarmup the classification and projection heads. The learning\nrate is reduced by a factor of 10 after 40 epochs.\nWebVision We train an InceptionResnet V2 [48] from\nscratch with a learning rate of 0.015, a weight decay of 5e-4,\na batch size of 96, and a warmup of 2 epochs. We set ŒªU = 0\nand Œ≤ = 0.5, and reduce Œ∫ at the epochs of 15 and 30.\nMulti-crop Strategy Multi-crop strategy [5] is an effi-\ncient augmentation method used in CRL. Images are cropped\ninto multiple smaller views of varying sizes, which can be\nviewed as positive pairs in CRL. This strategy increases the\ndiversity of positive pairs without incurring significant ad-\nditional computational costs. In the WebVision dataset, we\nresize the images to a size of 320, then randomly crop and\nresize them into two large views with a size of 224 and six\nsmall views with a size of 128.\nWe list hyperparameters used in our method for various\ndatasets in Table 8 and Table 9. To maintain simplicity,\nwe keep most of the hyperparameters consistent across all\ndatasets. Additionally, to facilitate comparison with Di-\nvideMix [28], we also maintain consistency with most of the\nhyperparameters used in their approach. It is noteworthy that\nour PLR loss is insensitive to the hyperparameter Œ∫, as is\nfurther discussed in the paper. We didn‚Äôt carefully tune it and\nempirically set it to dynamically decrease for full utilization\nof negative pairs.\nC. Visualization\nIn Figure 6, we use t-SNE [50] to visualize the features\nof training images for different noise modes and ratios. We\nrandomly select 5% of samples from each class. Circles rep-\nresent the features q and stars represent the class prototypes\nP . It can be seen that the feature embeddings form distinct\nclusters based on their latent ground truth labels rather than\nthe given noisy labels, which demonstrates the robustness of\nthe proposed method.\nIn Figure 7, we visualize the images that have the top-5\nlargest cosine similarity features to the prototypes of each\nclass on the CIFAR10 dataset with 90% symmetric noise.\nCorresponding ground truth labels and given noisy labels are\nlisted above each image. If the noisy label differs from the\nlatent ground truth, it is colored in red; otherwise in green.\nWe use a ‚úì to indicate that an image has been correctly\nassigned to its corresponding cluster and √ó if not. As can\nbe seen, most images have been assigned to its ground truth\ncluster, which shows the effectiveness of our method.\n1\nAlgorithm 1 Pseudocode of computing PLR loss in a PyTorch-like style.\n# net0, net1: two identical networks with backbone f, classification head g, and projection head h\n# PLRLoss: contrastive loss, Eqn.(9), which takes a mask as parameter\n# k: hyperparameter kappa in Eqn.(6)\ndef build_mask(x, y, net):\nz = net.g.forward(net.f.forward(x)) # model outputs\nindices_k = torch.topk(z, k, dim=1)[1] # top k indices of model outputs\n# tops: assign 1 to the top k indices, 0 to the rest\ntops = torch.zeros(len(x), C)\ntops = torch.scatter(tops, 1, indices_k, 1)\ntops = torch.scatter(tops, 1, y.unsqueeze(1), 1) # append labels to top k\n# intersection, Eqn.(8), where ‚Äòconflicts‚Äô equals 0 are feasible negative pairs\nconflicts = torch.matmul(tops, tops.t())\n# contrastive mask, where negative pairs are -1, positive pairs are 1, neglect pairs are 0\nmask = torch.where(conflicts == 0, -1, 0)\nmask = torch.where(eye(len(x)) == 1, 1, mask)\nreturn mask\nfor (x, y) in loader: # load a minibatch of samples and labels\nmask = build_mask(x, y, net0) # get mask from one network\nx1 = aug(x) # random strong augmentation\nx2 = aug(x) # another strong augmentation\n# train the other network\nf1 = net1.h.forward(net1.f.forward(x1))\nf2 = net1.h.forward(net1.f.forward(x2))\nf = torch.cat([f1.unsqueeze(1), f2.unsqueeze(1)], dim=1)\nplr_loss = InfoNCELoss()(f, mask=mask)\n# if other losses exist, sum all losses up, then backward and update\nplr_loss.backward()\nupdate(net1.params) # train the other network\nTable 8. Hyperparameter settings of our proposed method on different datasets.\nHyperparameters\nCIFAR-10\nCIFAR-100\nTiny-ImageNet\nClothing1M\nWebVision\nInitial Learning Rate\n0.02\n0.02\n0.01\n0.004\n0.015\nMomentum\n0.9\nWeight Decay\n0.0005\n0.0005\n0.0005\n0.001\n0.0005\nBatch Size\n128\n128\n256\n64\n96\nEpochs\n400\n400\n400\n80\n100\nwarmup epochs\n10\n15\n10\n1\n2\nŒ≤\n4\n4\n0.5\n0.5\n0.5\nŒªi\n1\nT\n0.5\nŒ±\n0.5\nœÑ\n1\nœÑs\n0.1\nœÑ1\n0.5 or 1/C\nœÑ2\n0.8\n2\nTable 9. The value of hyperparameter ŒªU on different datasets, following previous work [28].\nHyperparameter\nDataset\nNoise Ratio r\nSym\nAsym\n0\n20%\n50%\n80%\n90%\n40% / 45%\nŒªU\nCIFAR10\n-\n0\n25\n25\n50\n0\nCIFAR100\n-\n25\n150\n150\n150\n0\nTiny-ImageNet\n0\n30\n200\n300\n-\n0\nClothing1M\n0\nWebVision\n0\n(a) Sym 20%\n(b) Sym 50%\n(c) Sym 80%\n(d) Sym 90%\nFigure 6. T-SNE visualizations of features and prototypes. The subplots show class distributions after training networks for 400 epochs\non the CIFAR10 dataset with various noise ratios: (a) 20% symmetric, (b) 50% symmetric, (c) 80% symmetric, (d) 90% symmetric. Our\nproposed method effectively learns robust representations and class prototypes even with high noise ratios.\n3\nairplane‚úî\nautomobile\nclean:\nnoisy:\nairplane‚úî\nhorse\nairplane‚úî\ncat\nairplane‚úî\ndeer\nairplane‚úî\nfrog\ndog‚úî\nbird\nclean:\nnoisy:\ndog‚úî\ndog\ndog‚úî\ndog\ndog‚úî\nfrog\ndog‚úî\nfrog\nautomobile‚úî\n   bird\nclean:\nnoisy:\nautomobile‚úî\n  ship\nautomobile‚úî\nbird\nautomobile‚úî\nautomobile  \nautomobile‚úî\nhorse  \nbird‚úî\ntruck\nclean:\nnoisy:\nbird‚úî\nbird\nbird‚úî\nbird\nbird‚úî\nship\nbird‚úî\nairplane\ncat‚úî\ncat\nclean:\nnoisy:\ncat‚úî\nship\ncat‚úî\ncat\ncat‚úî\ncat\ncat‚úî\ncat\ndeer‚úî\nship\nclean:\nnoisy:\ndeer‚úî\ndog\nbird‚ùå\nbird\ndeer‚úî\ndeer\ndeer‚úî\ntruck\nfrog‚úî\ndeer\nclean:\nnoisy:\nfrog‚úî\nfrog\nfrog‚úî\ndog\nfrog‚úî\nhorse\nfrog‚úî\nship\nhorse‚úî\ndog\nclean:\nnoisy:\nhorse‚úî\nautomobile\nhorse‚úî\nship\nhorse‚úî\ndeer\nhorse‚úî\nbird\nship‚úî\nbird\nclean:\nnoisy:\nship‚úî\nhorse\nship‚úî\ncat\nship‚úî\nhorse\nship‚úî\ncat\ntruck‚úî\ntruck\nclean:\nnoisy:\ntruck‚úî\nfrog\ntruck‚úî\nautomobile\ntruck‚úî\ndog\ntruck‚úî\ntruck\nairplane\nautomobile\nbird\ncat\ndeer\ndog\nfrog\nhorse\nship\ntruck\nFigure 7. Visualization of images and labels in CIFAR10 dataset with 90% label noise. For each class, we select and show those images\nwith top-5 largest cosine similarity features to the prototypes. Most of the image features are correctly assigned to the correct class, which\ndemonstrates the effectiveness of our proposed PLR loss.\n4\n"
}