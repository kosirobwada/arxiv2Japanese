{
    "optim": "MedContext: Learning Contextual Cues for Efficient Volumetric Medical Segmentation Hanan Gani1 Muzammal Naseer1 Fahad Khan1,2 Salman Khan1,3 1Mohamed bin Zayed University of Artificial Intelligence 2Link¨oping University 3Australian National University Abstract Volumetric medical segmentation is a critical component of 3D medical image analysis that delineates different se- mantic regions. Deep neural networks have significantly improved volumetric medical segmentation, but they gen- erally require large-scale annotated data to achieve better performance, which can be expensive and prohibitive to ob- tain. To address this limitation, existing works typically perform transfer learning or design dedicated pretraining- finetuning stages to learn representative features. However, the mismatch between the source and target domain can make it challenging to learn optimal representation for vol- umetric data, while the multi-stage training demands higher compute as well as careful selection of stage-specific de- sign choices. In contrast, we propose a universal training framework called MedContext that is architecture-agnostic and can be incorporated into any existing training frame- work for 3D medical segmentation. Our approach effec- tively learns self-supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data or dedicated pretraining-finetuning stages. The proposed approach in- duces contextual knowledge in the network by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. The effectiveness of MedCon- text is validated across multiple 3D medical datasets and four state-of-the-art model architectures. Our approach demonstrates consistent gains in segmentation performance across datasets and different architectures even in few-shot data scenarios. Our code and pretrained models are avail- able at https://github.com/hananshafi/MedContext. 1. Introduction Medical imaging analysis uses a variety of techniques to analyze scans obtained from procedures such as X-rays, CT scans, and MRIs [28]. The volumetric medical segmenta- tion of a scan is a critical component of this analysis, which Figure 1. Comparison, in term of Dice scores (%), when integrat- ing our approach into UNETR [19], SwinUNETR [17] and nn- Former [44] for medical segmentation on Synapse dataset (Sec. 4) using conventional setting (Left) and few-shot setting (5 samples only, Right). Without any modification to the model architecture or its training pipeline, our proposed universal approach comple- ments the supervised voxel-wise segmentation and enhances the performance of state-of-the-art architectures. identifies specific structural features by separating them into different segments or regions. A clinician may use this voxel-wise segmentation to diagnose medical conditions for treatment or further testing. Deep neural networks have improved volumetric medi- cal segmentation. The convolutional encoder-decoder net- works, U-NET [9, 36], as well as the development of vision transformers [13], has led to hybrid architectures [3, 24] with complementary strengths of self-attention and convo- lution for medical segmentation. Despite the architectural advances, deep neural networks generally require large- scale annotated data to achieve better performance. How- ever, collecting and annotating medical images at a large scale can be expensive and prohibitive due to privacy con- cerns. To deal with the data scarcity, weights learned on Ima- geNet [11] can be used to initialize the encoder, however, pre-training on 2D natural images may not capture the con- textual information essential to understanding 3D medical images. Recent studies [17, 20, 40] explore self-supervised pre-training on extra auxiliary medical data, but this ap- proach has two limitations: a) the training process con- sists of a two-stage pre-training process on the auxiliary data, followed by fine-tuning on the target data which can 1 arXiv:2402.17725v1  [eess.IV]  27 Feb 2024 Spleen R-Kid L-Kid Gal Eso Liv Sto Aor ICV PSV Pan Rad. Lad. RV cavity Myocaridum LV cavity Figure 2. Qualitative Comparison between the baseline nnFormer [44] and our proposed MedContext integrated with nnFormer. The examples display different abdominal organs (Synapse) (Left) and regions of the heart (ACDC) (Right), with their corresponding labels in the legend below. The baseline nnFormer struggles to accurately segment the organs and heart regions. In certain cases, it gives false segmentation results highlighted in red boxes. Best viewed zoomed in. Refer to supplementary material for additional qualitative comparisons. be computationally expensive, and b) the success of fine- tuning depends on how well the auxiliary data distribution matches the target data. Moreover, there may not be a direct relationship between the self-supervised objectives (e.g., in- painting, solving jigsaws, or predicting rotation) and voxel- wise segmentation. Therefore jointly optimizing such self- supervised losses with 3D segmentation is non-trivial. To address these limitations, we propose a generic train- ing framework dubbed MedContext to learn self-supervised contextual cues jointly with supervised voxel segmentation without requiring large-scale annotated volumetric medical data. Specifically, we propose to reconstruct the masked or- gans or parts of an organ of an input image in the output segmentation space. Since our voxel-wise segmentation re- construction is well aligned with the voxel-wise prediction task, both tasks can be optimized together. To further re- duce the disparity between voxel segmentation reconstruc- tion and prediction tasks, we deploy a student-teacher distil- lation strategy to guide reconstruction from a slow-moving online teacher model which also helps avoid representa- tion collapse. MedContext encourages contextual learning within the model and allows it to learn local-global rela- tionships between different input components. This leads to better segmentation of organ boundaries (see Fig. 2). Our proposed approach is architecture-agnostic and can be incorporated into any training framework, making it uni- versally applicable. We integrate our approach into three recent state-of-the-art medical 3D transformer based archi- tectures: UNETR [19], SwinUNETR [17] and nnFormer [44]; and one CNN based 3D architecture PCRLv2 [45]. Using these architectures, we validate our approach across three medical imaging datasets: Multi-organ Synapse [25], ACDC [2] and BraTS [1, 30]. We observe consistent perfor- mance gains on all the datasets. Specifically, on the Synapse dataset with nnFormer and UNETR architectures (Fig. 1), we observe a performance improvement of ∼1% and 2% respectively in terms of Dice score. Similarly, on ACDC dataset with UNETR architecture, we observe a ∼4% gain in the Dice score compared to the baseline. We further as- sess the effectiveness of our framework in comparison to various pretraining-finetuning methods including [7], [41], and [21]. Our evaluation reveals consistent performance im- provements across all compared methods. In summary, our contributions are three-fold: 1. We propose a universal training framework to jointly op- timize supervised segmentation and self-supervised seg- mentation reconstruction via student-teacher knowledge distillation for the volumetric medical data. 2. Our approach induces contextual knowledge in the net- work by learning to reconstruct the missing organ or parts of an organ in the output segmentation space. 3. We validate the effectiveness of our approach across multiple 3D medical datasets and state-of-the-art model architectures. Our approach complements existing meth- ods and improves segmentation performance in conven- tional as well as few-shot data scenarios. 2. Related Work 3D Medical Segmentation: Several U-Net [36] based encoder-decoder architectures have been proposed to solve the problem of 3D medical segmentation. [10] modifies the basic U-Net architecture by replacing 2D operations with 3D operations to capture the 3D context. [23] proposed to learn multi-scale feature representations from varying resolutions for multi-organ segmentation. Other works 2 have suggested integrating the contextual information with CNN-based frameworks using image pyramids [43], large kernels [32], dilated convolution [6], and deformable con- volution [26]. Few recent works [3, 24] have explored the use of transformer architectures for 3D volumetric segmen- tation by dividing the volumetric images into 3D patches which are then flattened to construct a 1D embedding and passed to transformer module. More recently, hybrid archi- tectures [5, 19, 27, 39, 42, 44] combining the strengths of both CNNs and transformers have been proposed to encode both local and global contexts. We build our approach on these hybrid architectures by proposing a complementary training mechanism that induces contextual information in these architectures. Masked Image Modeling (MIM): MIM has emerged as an appealing self-supervised representation learning method, fueled by the success of ViTs. Recent MIM meth- ods [21, 41] are trained to predict the pixel values to recon- struct the corresponding missing tokens in the input. Re- cent works on videos [14, 21, 38] use temporal masking to learn self-supervised representations for videos. In case of medical imaging, [8] explores MIM on 3D medical data. [45] proposes pixel restoration task on 3D medical images. However, these approaches are based on a two-stage pro- cess of self-supervised pretraining on large public datasets [16, 37] and then finetuning. Different from existing self- supervised learning methods which require an additional stage of supervised finetuning on labeled data, we syner- gically incorporate MIM with the voxel-wise segmentation objective in a single training framework without any exter- nal data. Knowledge Distillation: Introduced by [22], knowl- edge distillation aims to transfer knowledge from a complex model (teacher) to a simpler model (student). Recent works [4, 35] have explored the use of knowledge distillation with an online student-teacher strategy for self-supervised learning. [35] employs a student-teacher based knowl- edge distillation framework for self-supervised representa- tion learning using a contrastive objective. [4] uses a dy- namic student-teacher framework to learn local-global cor- respondences by distilling the knowledge from the teacher which is built dynamically. [34] extends the knowledge dis- tillation approach to medical segmentation where a strong teacher model is used to distill knowledge within a small compressed model. All these approaches are applicable in the 2D data scenario. In contrast, our proposed knowledge distillation framework works on 3D volumetric inputs and serves as a means to reconstruct the missing voxels in the segmentation space in a single end-to-end training stage. 3. Methodology The existing 3D medical segmentation methods focus on optimizing voxel-wise segmentation, typically by minimiz- ing the dice loss between predicted and ground truth seg- mentation masks. However, this approach may not capture the underlying 3D structure of the input or the contextual relationships between different objects, especially in data- scarce scenarios such as in the medical domain. To address this limitation, we propose a novel approach that learns contextual relationships between different organs or organ parts in the output segmentation space. Specifically, we for- mulate the problem as reconstructing missing organ parts from a masked input volume in the segmentation space us- ing student-teacher knowledge distillation. Our proposed approach complements the voxel-wise segmentation task by inducing contextual consistency through joint optimization of two objectives: a) voxel-wise segmentation reconstruction from the masked input, and b) supervised voxel-wise segmentation prediction. The learn- ing objectives enable the model to reconstruct and segment the missing organs or organ parts, thereby encoding con- textual information. This allows the model to learn local- global relationships between different input components for a data-efficient and accurate 3D semantic segmentation. In summary, our approach provides a solution to the limita- tions of current 3D medical semantic segmentation methods by incorporating contextual learning via joint optimization of multi-task objectives. Next, we explain the network ar- chitecture, objective functions and optimization scheme. 3.1. Architecture Our approach is complementary and can be applied to the existing encoder-decoder architectures designed for 3D medical image segmentation. Specifically, we show the benefit of our approach using three recently introduced models: UNETR [19], SwinUNETR [17], and nnFormer [44]. In these architectures, the encoder processes a 3D in- put volume and produces latent feature representation. The decoder then maps this representation to the corresponding segmentation map through upsampling blocks. Addition- ally, skip connections are used to exchange the multi-level features across different encoder-decoder layers. We illus- trate how our approach can be integrated into these hybrid architectures, which combine both convolution and trans- former components, for 3D medical image segmentation. As shown in Fig. 3 our design includes a student Fs and a teacher Ft network that operate on the input volume X ∈ RH×W ×D and its masked version XM ∈ RH×W ×D generated using the masking function g(.). Here, H, W, and D represent the height, width, and depth of the 3D input volume, respectively. During the training phase, the input views are fed to the student-teacher framework as 3D patches, which generates voxel-wise semantic logits for each input view. The student network is provided with both the masked (XM) and unmasked (X) inputs, and the corre- sponding output voxel-wise semantic logits are denoted as 3 Figure 3. Overview of our MedContext approach: The original 3D volume is masked and fed to the student model (top-row) along with the original input. The teacher model (bottom-row) is only fed with the original volume. The difference between the semantic voxelwise predictions for the masked and original inputs corresponding to the student and teacher networks respectively is minimized to guide the reconstruction of masked regions in the output segmentation space. Our approach induces contextual consistency by enabling the model to reconstruct and segment the missing organs or organ parts and therefore yields more precise and accurate segmentation results. Fs and F M s , respectively. On the other hand, the teacher network is provided with the original unmasked input X which outputs voxel-wise semantic logits denoted as Ft. The feature map produced at intermediate layers of the 3D architecture has a shape of H P1 × W P2 × D P3 × C, where (P1, P2, P3) is the resolution of each patch and C is the feature dimension. For each output prediction from the student , a super- vised loss is computed using the ground truth label Y , as shown in the figure. Additionally, a self-supervised objec- tive is minimized between the masked student logits F M s and the teacher logits Ft. Finally, both the supervised and self-supervised objectives are jointly optimized during our single-stage training process. 3.2. Volumetric Masking Strategy To model contextual relationships, we employ a masking technique on the original input X to reconstruct missing parts in the segmentation space. As illustrated in Fig. 3, we ensure mask consistency across the depth by apply- ing the same mask to all subsequent slices in the volume, thereby ensuring that any masked organ or region in the first slice remains masked in all subsequent slices. Our encoder- decoder architectures convert the input volume into patch tokens before processing. To generate a masked view XM, we randomly mask a certain fraction δ of the patch tokens. Following [12], the masked tokens are replaced with learn- able tokens Hξ, such that, XM = g(X, δ) = X ◦ (1 − Iδ) + Hξ ◦ Iδ, (1) where Iδ is a binary mask generated according to a Bernoulli distribution using g(.), i.e., Iδ ∼ Bernoulli(δ) and ◦ denotes the element-wise product. This masking tech- nique prevents information leakage from neighboring cubes by enforcing the same masking map for all cubes along the depth dimension. Our approach encourages the model to learn contextual semantic relationships by recovering the segmentation map for such masked inputs. 3.3. Voxel-wise Segmentation Reconstruction Using masked input, we reconstruct the segmentation map to facilitate learning contextual semantic relationships. To accomplish this task, a naive approach would be to feed both the original and masked views into a single model and optimize the outputs of both views using supervised loss with ground truth labels. However, this approach has cer- tain limitations. When a single model is used to reconstruct the masked volume, the model’s weight updates are solely dependent on information from the current views. A num- ber of self-supervised learning techniques, such as DINO [4] and BYOL [35], have shown that leveraging the knowl- edge acquired by the model during previous weights up- dates can lead to more effective guidance and avoids repre- sentation collapse. This can be achieved by implementing a student-teacher strategy where teacher weights are updated by a moving average of the student weights. In this way, the collective knowledge learned during previous weight up- dates assists in the reconstruction of the masked views and thereby induces enriched contextual cues. Additionally, the teacher network provides soft semantic targets that guide the training of the student network. The soft targets contain information about the relationships between the views and aids the student network in generalizing more effectively. We initialize the student model Fs and teacher model Ft with the same randomly initialized weight parame- ters. The student network receives both the original and masked inputs, while the teacher network receives only the original non-masked input. The networks generate voxel-wise semantic logits, represented by {F M s , Fs} and 4 Ft respectively, which are processed to obtain semantic voxel-wise segmentation map predictions. Following this, we reconstruct semantic voxel-wise logits of the masked input from the student model guided by two supervised signals: supervision through knowledge distillation and ground truth labels, as explained below. Reconstruction through Knowledge Distillation: A self- supervised distillation loss (Eq. 2) is used to guide the train- ing of the student network to encourage modeling the con- textual consistency. It minimizes the difference between the voxel-wise logits generated by the teacher network given the original input Ft and the voxel-wise logits produced by the student network using the masked input F M s . The ob- jective function, referred to as Consistency Loss (CL), is denoted as Lc(F M s , Ft) and is expressed as, Lc(F M s , Ft) = ∥ F M s − Ft∥ 2 2 ∥ Ft∥ 2 2 . (2) Reconstruction through Ground truth Labels: The voxel-wise semantic logits output by the student network for a given masked input F M s are further reconstructed us- ing the ground truth labels. This is accomplished by mini- mizing the soft dice loss [31] using the ground truth labels Y . The general expression for Dice-CE Loss for some ar- bitrary output prediction F is given as, LDice−CE(Y , F ) = 1 − C X c=1   2 ∗ PV v=1 Yv,c · Fv,c PV v=1 Y 2 v,c + PV v=1 F 2v,c + V X v=1 Yv,c log Fv,c ! , (3) where, C denotes the number of classes; V denotes the number of voxels; Yv,i and Fv,i denote the ground truths and output probabilities for class i at voxel v, respectively. In our case, the supervised objective function for recon- struction is calculated using above Dice-CE loss between the ground truth label Y and voxel-wise semantic logits output F M s and is denoted as LDice−CE(Y , F M s ) and re- ferred to as Masked Student Loss (MSL). Our reconstruction objective on the masked input vol- umes induces contextual consistency to enhance segmen- tation performance. The reconstruction of the missing re- gions enables the network to capture intricate relationships between various organs in the input volume and learn the broader context of the input volume going beyond the local features to segment organs. This promotes the preservation of the global structure of the input volume, yielding more precise and accurate segmentation results. Moreover, the objective of predicting missing regions in the masked in- put further encourages the model to learn correspondence between the local and global structure in the input vol- ume. Masked input volumes represent local views of in- puts that are matched with original global input volumes. This matching objective allows the model to capture the re- lationships between neighboring regions which is essential for learning class-specific semantic features to better cap- ture the object boundaries and shapes. 3.4. Supervised Voxel-wise Segmentation Our primary task of supervised voxel-wise segmentation prediction takes place in conjunction with the voxel-wise segmentation reconstruction as discussed above. For the supervised voxel-wise segmentation, we optimize the pre- dictions of the original input X from the student network Fs through the supervision of the ground truth labels Y using Soft Dice Loss (Eq. 3) denoted by the objective LDice−CE(Y , Fs). 3.5. Overall Multi-task Objective Our framework leverages a combination of supervised and self-supervised losses to optimize the learning process. The multi-task objectives synergistically reinforce each other and provide complementary advantages. The overall loss objective L is given as, L =LDice−CE(Y , Fs) + LDice−CE(Y , F M s ) + βLc(F M s , Ft), (4) where the hyperparameter β controls the contribution of self-supervised consistency loss during optimization. 3.6. Optimization strategy Following a typical student-teacher optimization strategy as ultilized by [4, 15], the gradient of the total loss is back- propagated through the student network and parameters are updated as follows, Θ ← Θ − α · ∇Θ(L), (5) where Θ represents the joint parameters of student network (θs) and learnable mask embeddings (ξ) i.e. Θ = {θs; ξ}. The teacher network is updated via exponential moving av- erage (EMA) of the weights of the student network using, θt ← λθt + (1 − λ)θs, (6) where θt denote the parameters of teacher and, λ follows the cosine schedule from 0.996 to 1 during training. The gradi- ent step through the student network comprises of the con- tributions from both the supervised and self-supervised ob- jectives, thereby aiding in the reconstruction of the masked input by updating the differentiable volumetric embeddings associated with the masked regions in the input volume. Avoiding Mode collapse: The student-teacher frameworks 5 in general are often prone to mode collapse where the stu- dent model fails to learn the entire range of outputs that the teacher model can generate. Our framework avoids the mode collapse by introducing additional supervised loss from the student corresponding to the output from the masked volume, which in addition to assisting in the masked reconstruction, also acts as a discriminative objec- tive to encourage the student model to learn a more diverse set of outputs that better match the full range of outputs gen- erated by the teacher. Additionally, it further encourages teacher model to generate high-quality targets for distilla- tion. We show the effect of our different losses in Sec. 4.4. 4. Experiments Datasets: We evaluate on three volumetric medical scan datasets. Synapse BTCV Dataset: The BTCV dataset [25], known as Synapse for Multi-organ CT Segmentation, in- cludes abdominal CT scans of 30 subjects encompassing 8 organs. Following previous methods, we adopt the same dataset split as used in [5] with 18 train samples and test on the remaining 12 cases. We evaluate the performance on eight abdominal organs (i.e. spleen, right kidney, left kidney, gallbladder, liver, stomach, aorta, and pancreas) us- ing Dice Similarity Coefficient (DSC) and 95% Hausdorff Distance (HD95). In all the cases, the intensities of input volumes are normalized from the range of [-1000, 1000] to [0,1] Hounsfield Units (HU). ACDC Dataset: The ACDC dataset [2] is a collection of cardiac MRI images and associ- ated segmentation annotations for the right ventricle (RV), left ventricle (LV), and myocardium (MYO) of 100 patients, obtained from actual clinical exams. we split the dataset into 80 training and 20 testing samples following [44] and report the results on all three classes using Dice similarity coefficient (DSC). BraTS Dataset: We use two versions of BraTS dataset: BraTS17 [30] and BraTS21 [1]. For UN- ETR, SwinUNTER and PCRLv2 we report results on the BraTS21 dataset to be consistent with the baseline settings. The BraTS21 dataset [1] is from the BraTS challenge and includes 1251 subjects. The annotations have been com- bined into three nested sub-regions: Whole Tumor (WT), Tumor Core (TC), and Enhancing Tumor (ET). Following the data split used by [18], we train on 1000 subjects and test on 251 subjects. For nnFormer, we use BraTS17 dataset [30]. The task comprises of 484 MRI images. Following the dataset split of [44], we train on 387 training samples and test on 73 cases. Further details about the datasets and pre-processing are provided in Appendix B. Evaluation Metrics: To evaluate the models’ perfor- mance, we utilize two metrics: the Dice Similarity Score (DSC) and the 95% Hausdorff Distance (HD95). The DSC metric measures the degree of overlap between the volumet- ric segmentation predictions and the voxels of the ground truths as follows, DSC(Y, F) = 2 ∗ |Y ∩ F| |Y | ∪ |F| = 2 ∗ Y · F Y 2 + F 2 (7) where, Y and F denote the ground truths and output logits for all the voxels, respectively. The HD95 metric is frequently employed as a boundary- based measure for determining the 95th percentile of dis- tances between the boundaries of the volumetric segmen- tation predictions and the voxels of the ground truths. Its definition is as follows: HD95(Y, F) = max{dY F , dF Y } (8) Here, dY F represents the maximum distance at the 95th per- centile between the predicted voxels and the ground truth, while dY represents the maximum distance at the 95th per- centile between the ground truth and the predicted voxels. Training and Implementation details: Our approach utilizes Pytorch version 1.10.1 in conjunction with MONAI libraries [33] for implementation. To ensure fairness, we follow the respective training frameworks of the baseline architectures. Specifically, we use an input size of 128 x 128 x 64 for all datasets when training with nnFormer, and 96 x 96 x 96 for UNETR and SwinUNTER. All models are trained using a single A100 40GB GPU. For nnFormer, we train on all datasets for 1000 epochs, using AdamW opti- mizer [29] with a learning rate of 0.01 and weight decay of 3e−5, and for UNETR and SwinUNETR, we train for 5000 epochs on BTCV synapse, 1000 epochs on ACDC, and 300 epochs on BRaTs dataset, consistent with baseline settings. The learning rate is kept default as per the given framework. During training, we apply the same data augmentations as used in UNETR, Swin UNETR, and nnFormer. Further de- tails are available in the supplementary materials, and the code and models will be made publicly available. 4.1. Comparison with state-of-the-art Baselines We show the effectiveness of our approach using three state-of-the-art 3D transformer based segmentation mod- els: UNETR, SwinUNETR, nnFormer, and one CNN-based model: PCRLlv2, across three datasets: Synapse Multi- Organ, ACDC, and BraTS (2017 and 2021). Synapse Multi-Organ Dataset: Table 1 shows the results on the synapse multi-organ dataset. We calculate perfor- mance metrics using Dice similarity score and HD95 score, maintaining consistent training settings as discussed above. UNETR with our approach achieves 2.5% higher Dice Score (81.13%) than the baseline (78.76%), and over 1% reduction in HD95 score. With hierarchical SwinUNETR, Dice Score increases by >1% (80.66% to 82.00%) and HD95 improves. For the complex nnFormer architecture, our approach enhances Dice score from 86.57% to 87.35%, 6 Models MedContext Spleen Right Kidney Left Kidney Gallbladder Liver Stomach Aorta Pancreas Average HD95 ↓ DSC ↑ UNETR ✗ 89.64 83.02 84.86 63.06 95.58 73.06 87.47 53.40 11.04 78.76 ✓ 90.73 83.36 86.03 67.94 95.59 78.62 87.30 59.51 9.44 81.13 Swin-UNETR ✗ 86.33 80.63 84.07 67.24 94.98 74.97 90.53 66.49 20.32 80.66 ✓ 91.45 80.80 84.85 67.70 94.60 76.20 90.88 67.74 14.45 82.00 nnFormer ✗ 90.51 86.25 86.57 70.17 96.84 86.83 92.04 83.35 10.63 86.57 ✓ 95.97 87.05 87.63 72.87 96.43 84.57 91.85 82.40 8.29 87.35 Table 1. Abdominal multi-organ Synapse: Our MedContext consistently improves the segmentation performance of all organs across different models. We observe significant improvements in HD95 along with the dice score (DSC). The best results are highlighted in bold. Models MedContext RV Myo LV Average UNETR ✗ 77.81 72.74 79.46 76.67 ✓ 84.77 75.82 81.21 80.60 SwinUNETR ✗ 83.47 75.54 83.09 80.70 ✓ 84.79 79.17 86.15 83.38 nnFormer ✗ 91.18 86.24 94.07 90.50 ✓ 92.14 86.52 93.52 90.73 Table 2. ACDC: We report the performance on the right ventricle (RV), left ventricle (LV), and myocardium (MYO). Models MedContext WT ET TC Average UNETR ✗ 87.35 90.88 84.29 87.50 ✓ 87.43 91.45 85.23 88.04 SwinUNETR ✗ 90.36 91.72 86.24 89.44 ✓ 90.57 92.30 86.64 89.83 nnFormer ✗ 80.80 58.86 77.42 72.36 ✓ 81.00 59.87 77.45 72.78 Table 3. BraTS: We report the performance on three brain tumour types, demonstrating effec- tiveness of our approach for all the three cases. Models MedContext Synapse ACDC UNETR ✗ 53.83 18.53 ✓ 56.25 28.63 SwinUNETR ✗ 54.13 32.62 ✓ 61.15 35.80 nnFormer ✗ 67.90 52.23 ✓ 70.96 58.05 Table 4. Few-shot: Performance of our MedContext in 5-shot scenario (5 sam- ples only). with over 2% HD95 reduction. ACDC Dataset: Table 2 shows the results on the relatively bigger ACDC dataset We observe that UNETR architecture complemented with our approach outperforms the baseline by a good margin of roughly 4% in terms of Dice score (80.60% vs 76.67%). We further observe that the Dice score per organ with our approach improves as compared to the baseline in each case. SwinUNETR shows over 2.5% increment in Dice score over baseline, along with higher per organ dice scores. nnFormer follows a similar trend, achieving 90.73% Dice score compared to the baseline’s 90.50%. BraTS Dataset: We use BraTS17 for UNETR and SwinUNETR architec- tures, and BraTS21 for nnFormer architecture. As shown in Table 3, with UNETR architecture, we report a gain of 0.54% in the overall dice score as compared to the base- line. With SwinUNTER, we report a Dice score of 89.83% which is greater than baseline dice score of 89.44%. Finally, with nnFormer, we observe a gain in the overall dice score (72.78%) as compared to the baseline dice score (72.36%). Overall we show that our approach achieves gains even on larger datasets such as BraTS, but has more pronounced im- provement for the low-data setups. 4.2. Few-shot performance We also validate the effectiveness of our approach in the few-shot scenario. Table 4 shows the performance compar- ison of our approach with the baselines in a 5-shot setting. We conduct few-shot experiments on Synapse BTCV and ACDC datasets using all three model architectures. Specif- ically, on Synapse BTCV, our approach results in a 3-10% Method Pretrain Spleen RKid LKid Gall Eso Liv Sto Aorta IVC Veins Pan RAG LAG Avg Baseline ✗ 89.0 89.2 87.7 47.6 48.9 94.4 74.7 82.0 77.3 61.7 64.4 56.6 46.9 70.8 SimCLR ✓ 91.1 91.3 89.7 48.7 50.0 96.6 76.5 83.9 79.1 63.2 65.9 57.9 48.1 72.4 MAE ✓ 94.8 95.0 93.4 50.6 52.1 98.6 79.7 87.4 82.4 65.9 68.6 60.5 50.1 75.3 SimMIM ✓ 95.2 95.4 93.7 51.9 52.3 98.7 79.9 87.7 82.6 66.0 68.9 60.7 51.2 75.7 MedContext ✗ 93.8 93.7 93.6 54.9 72.6 96.6 80.3 89.9 83.3 72.9 73.9 64.4 65.3 79.6 Table 5. MedContext vs. pretraining-finetuning [8]. DSC (%) on Synapse dataset with UNETR architecture. Method Pretrain Brats21 ACDC Synapse PCRLv2 ✓ 79.90 78.53 64.00 PCRLv2 + MedContext ✗ 82.03 82.57 72.30 Table 6. Improving PCRLv2 with our proposed MedContext across 3 datasets. We report Avg Dice scores. increase in Dice score across all cases, indicating a signif- icant improvement in segmentation accuracy. Similarly, on the relatively larger ACDC dataset, we observe a similar trend of higher Dice scores when our approach is integrated with the baseline models. This finding highlights the poten- tial of our approach in improving the segmentation accuracy of medical images in situations where annotated data is lim- ited, making it suitable for data-efficient training. 4.3. Pretraining-Finetuning Baselines We demonstrate the effectiveness of MedContext by com- paring its performance with existing pretraining-finetuning methods in Table 5. The baseline [8] method utilizes bet- ter weight initialization based on pretraining with large dataset [16] across state-of-the-art self-supervised meth- 7 ods [7, 21, 41] and then fine-tuned on the target dataset. In contrast, our MedContext directly learns the contextual cues from the target small dataset without the pre-training stage and outperforms the methods employing pretraining- finetuning paradigm. We further integrate our MedContext into the official implementation of PCRLv2 [45], a 3D CNN based state-of-the-art architecture, which is first pretrained in a self-supervised fashion on [37] and then finetuned on the specific target dataset. As seen in Table 6, our MedCon- text shows a complementary effect and improves the per- formance (Avg. Dice Score) of PCRLv2 architecture con- sistently without requiring pretraining. This also affirms the versatility of our framework, as it can be applied to CNN ar- chitectures, showcasing its universality. 4.4. Ablation Studies Effect of Student-Teacher framework: As discussed in Sec. 3.3, we hypothesize that the approach of training a sin- gle model on both original and masked input views using supervised loss with ground truth may not be sufficient for learning contextual relationships, as this method does not take into the account the knowledge acquired by the model during previous weights updates, which can lead to more ef- fective guidance. This prompts us to use a student-teacher framework that leverages the information captured in the previous weight updates. We provide empirical evidence to support our claim in Fig. 4 (left), where we report results on the Synapse multi-organ dataset across all three model ar- chitectures. In all cases, we observe a drop in performance in the absence of student-teacher framework. Effect of Student vs Teacher weights: Our proposed MedContext jointly optimizes the multi-task objectives in a student-teacher framework. As discussed in Sec. 3.6, the parameters of student and teacher networks are learned through different update rules. At inference, we can either choose the student or the teacher weights for predictions. We study the performance of student and teacher weights during inference in Fig. 4 (right). We observe that the stu- dent weights perform better than the teacher counterpart on synapse multi-organ dataset across two architectures. Effect of Masking Ratio: We propose to learn the con- textual knowledge by allowing the model to reconstruct the missing regions of the masked input in the segmentation space. However, the number of missing patches to recover may influence the performance of the model. We conduct an ablation study on a held-out validation set to determine the optimal masking ratio. Table 7 shows the Dice scores for a range of masking ratios across Synapse dataset us- ing UNETR and SwinUNETR architectures complemented with our approach. Although our method shows improve- ment on all masking ratios, however, we observe that a 40% masking ratio works best in our case. Effect of different losses: Our proposed approach utilizes Figure 4. DSC (%) on Synapse across different models. Left: Distillation from Teacher. We demonstrate the importance of knowledge distillation through teacher for effectively leveraging contextual cues. Right: Student vs Teacher. We show that utiliz- ing student weights during inference benefits overall performance. Masking ratio Average Dice Score UNETR SwinUNETR 30% 79.54 80.92 40% 80.47 82.00 50% 80.00 81.03 60% 80.20 81.70 80% 79.90 81.27 Table 7. Understanding the ef- fect of masking ratio on our MedContext. We report the Dice score on the Synapse across UNETR & SwinUNETR architectures. MSL CL Average Dice Score UNETR SwinUNETR ✓ ✗ 78.69 81.03 ✗ ✓ 79.46 81.25 ✓ ✓ 80.32 81.70 Table 8. We show the effect of each loss component on fi- nal objective (Eq. 4). We re- port dice score (%) on Synapse dataset. multiple supervised and self-supervised losses in the train- ing stage. As discussed in Secs. 3.3 and 3.4, each loss com- ponent holds a specific significance towards the final objec- tive. We conduct an ablative analysis on BTCV Synapse dataset to study the importance of the 2 loss components: Masked student loss (MSL) and Consistency Loss (CL), across two architectures: UNETR and SwinUNETR, in Ta- ble 8. We observe that removing any one of the loss com- ponents results in a drop in the Dice score. We observe this trend with both the architectures, providing empirical evidence that mutual synergy between supervised and self- supervised losses helps induce contextual cues for effective 3D medical segmentation. 5. Conclusion In this paper, we propose a universal training frame- work called MedContext which effectively learns self- supervised contextual cues jointly with the supervised voxel segmentation task without requiring large-scale annotated volumetric medical data. Our proposed ap- proach employs a student-teacher distillation strategy to reconstruct missing organs or parts of organs in the output segmentation space. Through extensive experi- mentation, our approach demonstrates complementary benefits to existing state-of-the-art 3D medical segmen- tation architectures in both conventional and few-shot settings without pretraining on large-scale datasets. Moreover, the plug-and-play design of our approach allows for its easy integration into any architectural design. 8 References [1] Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel Bilello, Evan Calabrese, Errol Colak, Keyvan Farahani, Jayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak Pati, et al. The rsna-asnr-miccai brats 2021 benchmark on brain tumor segmentation and radiogenomic classification. arXiv preprint arXiv:2107.02314, 2021. 2, 6, 13 [2] Olivier Bernard, Alain Lalande, Clement Zotti, Freder- ick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin, Karim Lekadir, Oscar Camara, Miguel Angel Gonza- lez Ballester, Gerard Sanroma, Sandy Napel, Steffen Pe- tersen, Georgios Tziritas, Elias Grinias, Mahendra Khened, Varghese Alex Kollerathu, Ganapathy Krishnamurthi, Marc- Michel Roh´e, Xavier Pennec, Maxime Sermesant, Fabian Isensee, Paul J¨ager, Klaus H. Maier-Hein, Peter M. Full, Ivo Wolf, Sandy Engelhardt, Christian F. Baumgartner, Lisa M. Koch, Jelmer M. Wolterink, Ivana Iˇsgum, Yeonggul Jang, Yoonmi Hong, Jay Patravali, Shubham Jain, Olivier Hum- bert, and Pierre-Marc Jodoin. Deep learning techniques for automatic mri cardiac multi-structures segmentation and di- agnosis: Is the problem solved? IEEE Transactions on Med- ical Imaging, 37(11):2514–2525, 2018. 2, 6, 13 [3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi- aopeng Zhang, Qi Tian, and Manning Wang. Swin-unet: Unet-like pure transformer for medical image segmentation. In European Conference on Computer Vision Workshops, 2022. 1, 3 [4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg- ing properties in self-supervised vision transformers. In Pro- ceedings of the IEEE/CVF international conference on com- puter vision, pages 9650–9660, 2021. 3, 4, 5 [5] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medi- cal image segmentation. arXiv preprint arXiv:2102.04306, 2021. 3, 6, 12 [6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801–818, 2018. 3 [7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge- offrey Hinton. A simple framework for contrastive learning of visual representations. In International conference on ma- chine learning, pages 1597–1607. PMLR, 2020. 2, 8 [8] Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem Safta, Mariann Micsinai Balan, and Kevin Brown. Masked image modeling advances 3d medical image analysis. In Pro- ceedings of the IEEE/CVF Winter Conference on Applica- tions of Computer Vision (WACV), pages 1970–1980, 2023. 3, 7 [9] ¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learn- ing dense volumetric segmentation from sparse annota- tion. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pages 424–432. Springer, 2016. 1 [10] ¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net: learn- ing dense volumetric segmentation from sparse annota- tion. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2016: 19th International Conference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 19, pages 424–432. Springer, 2016. 2 [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255, 2009. 1 [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 4 [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1 [14] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim- ing He. Masked autoencoders as spatiotemporal learners. arXiv preprint arXiv:2205.09113, 2022. 3 [15] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh- laghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271–21284, 2020. 5 [16] Stephanie A. Harmon, Thomas Sanford, Sheng Xu, Evrim B Turkbey, Holger R. Roth, Ziyue Xu, Dong Yang, An- driy Myronenko, Victoria L. Anderson, Amel Amalou, Maxime Blain, Michael T Kassin, Dilara Long, Nicole Var- ble, Stephanie M. Walker, Ulas Bagci, Anna Maria Ier- ardi, Elvira Stellato, Guido Giovanni Plensich, Giuseppe Franceschelli, Cristiano Girlando, Giovanni Irmici, Dominic Labella, Dima A. Hammoud, Ashkan A. Malayeri, Eliz- abeth C. Jones, Ronald M. Summers, Peter L. Choyke, Daguang Xu, Mona G. Flores, Kaku Tamura, Hirofumi Obi- nata, Hitoshi Mori, F. Patella, Maurizio Cariati, Gianpaolo Carrafiello, Peng An, Bradford J. Wood, and Baris I Turkbey. Artificial intelligence for the detection of covid-19 pneumo- nia on chest ct using multinational datasets. Nature Commu- nications, 11, 2020. 3, 7 [17] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In International MICCAI Brainlesion Workshop, 2022. 1, 2, 3, 12, 13 [18] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong Yang, Holger R Roth, and Daguang Xu. Swin unetr: Swin transformers for semantic segmentation of brain tumors in mri images. In Brainlesion: Glioma, Multiple Sclero- sis, Stroke and Traumatic Brain Injuries: 7th International 9 Workshop, BrainLes 2021, Held in Conjunction with MIC- CAI 2021, Virtual Event, September 27, 2021, Revised Se- lected Papers, Part I, pages 272–284. Springer, 2022. 6 [19] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong Yang, Andriy Myronenko, Bennett Landman, Holger R Roth, and Daguang Xu. Unetr: Transformers for 3d med- ical image segmentation. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 574–584, 2022. 1, 2, 3, 12 [20] Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger Roth, and Daguang Xu. Unetformer: A unified vision trans- former model and pre-training framework for 3d medical im- age segmentation. arXiv preprint arXiv:2204.00631, 2022. 1 [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll´ar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000– 16009, 2022. 2, 3, 8 [22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distill- ing the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. 3 [23] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Pe- tersen, and Klaus H Maier-Hein. nnu-net: a self-configuring method for deep learning-based biomedical image segmen- tation. Nature methods, 18(2):203–211, 2021. 2 [24] Davood Karimi, Serge Didenko Vasylechko, and Ali Gholipour. Convolution-free medical image segmentation using transformers. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th Inter- national Conference, Strasbourg, France, September 27– October 1, 2021, Proceedings, Part I 24, pages 78–88. Springer, 2021. 1, 3 [25] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner, T Langerak, and Arno Klein. Miccai multi-atlas labeling beyond the cranial vault–workshop and challenge. In MIC- CAI Multi-Atlas Labeling Beyond Cranial Vault—Workshop Challenge, 2015. 2, 6, 12 [26] Ziqiang Li, Hong Pan, Yaping Zhu, and A Kai Qin. Pgd- unet: a position-guided deformable network for simultane- ous segmentation of organs and tumors. In 2020 Interna- tional Joint Conference on Neural Networks (IJCNN), pages 1–8. IEEE, 2020. 3 [27] Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, Guang- ming Lu, and David Zhang. Ds-transunet: Dual swin trans- former u-net for medical image segmentation. IEEE Trans- actions on Instrumentation and Measurement, 2022. 3 [28] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Ar- naud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen Ghafoorian, Jeroen A.W.M. van der Laak, Bram van Gin- neken, and Clara I. S´anchez. A survey on deep learning in medical image analysis. Medical Image Analysis, 42:60–88, 2017. 1 [29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [30] Bjoern H. Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, and et. al. The multimodal brain tumor image segmentation benchmark (brats). IEEE Transactions on Medical Imaging, 34(10):1993–2024, 2015. 2, 6, 13 [31] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In Fourth International Con- ference on 3D Vision (3DV), 2016. 5 [32] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and Jian Sun. Large kernel matters–improve semantic segmen- tation by global convolutional network. In Proceedings of the IEEE conference on computer vision and pattern recog- nition, pages 4353–4361, 2017. 3 [33] Project-MONAI. Medical open network for ai. https: //github.com/Project-MONAI/MONAI, 2020. 6 [34] Dian Qin, Jia-Jun Bu, Zhe Liu, Xin Shen, Sheng Zhou, Jing- Jun Gu, Zhi-Hua Wang, Lei Wu, and Hui-Fen Dai. Effi- cient medical image segmentation based on knowledge dis- tillation. IEEE Transactions on Medical Imaging, 40(12): 3820–3831, 2021. 3 [35] Pierre H Richemond, Jean-Bastien Grill, Florent Altch´e, Corentin Tallec, Florian Strub, Andrew Brock, Samuel Smith, Soham De, Razvan Pascanu, Bilal Piot, et al. Byol works even without batch statistics. arXiv preprint arXiv:2010.10241, 2020. 3, 4 [36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. 1, 2 [37] Arnaud Arindra Adiyoso Setio, Alberto Traverso, and et. al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed to- mography images: The luna16 challenge. Medical Image Analysis, 42:1–13, 2017. 3, 8 [38] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learn- ers for self-supervised video pre-training. arXiv preprint arXiv:2203.12602, 2022. 3 [39] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu, and Vishal M Patel. Medical transformer: Gated axial- attention for medical image segmentation. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part I 24, pages 36–46. Springer, 2021. 3 [40] Yutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unified 2d and 3d pre-training for medical image classification and segmentation. arXiv preprint arXiv:2112.09356, 2021. 1 [41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653–9663, 2022. 2, 3, 8 [42] Yundong Zhang, Huiye Liu, and Qiang Hu. Transfuse: Fusing transformers and cnns for medical image segmenta- tion. In Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, 10 Strasbourg, France, September 27–October 1, 2021, Pro- ceedings, Part I 24, pages 14–24. Springer, 2021. 3 [43] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2881–2890, 2017. 3 [44] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201, 2021. 1, 2, 3, 6, 12, 13 [45] Hong-Yu Zhou, Chixiang Lu, Chaoqi Chen, Sibei Yang, and Yizhou Yu. Pcrlv2: A unified visual information preserva- tion framework for self-supervised pre-training in medical image analysis. arXiv preprint arXiv:2301.00772, 2023. 2, 3, 8 11 Supplementary Material In this section we discuss the Psuedocode of our Med- Context Algorithm in Appendix A, provide additional de- tails about the datasets in Appendix B, finally, in Appendix C, we provide some qualitative visualizations to show the superiority of our MedContext for effective 3D medical seg- mentation. A. MedContext Algorithm In section 3.1 of the main paper, we describe our MedCon- text algorithm which comprises a student network (Fs) and a teacher network (Ft). The student-teacher framework pro- cesses the input volume X ∈ RH×W ×D and its masked version XM ∈ RH×W ×D created using the masking func- tion g(.). During training, the student-teacher framework generates voxel-wise semantic logits for each input view. The student network receives both the masked (XM) and unmasked (X) inputs, and outputs voxel-wise semantic log- its represented by Fs and F M s . The teacher network re- ceives only the original unmasked input X and generates voxel-wise semantic logits denoted by Ft. We calculate a supervised loss using the ground truth label Y for each out- put prediction from student. Additionally, we minimize a self-supervised objective between the masked student logits F M s and the teacher logits Ft. The hyperparameter β con- trols the contribution of self-supervised loss to the overall objective function. Our single-stage training process jointly optimizes both the supervised and self-supervised objec- tives. At the inference stage, we use the weights of the stu- dent network to output the corresponding voxel-wise pre- dictions. We provide the Pseudo-code of our MedContext approach in Algorithms 1 and 2. B. Additional Dataset details We use three medical segmnetation datasets in our paper. The additional details of each dataset along with the pre- processing settings are given below: Synapse BTCV Multi-organ Dataset [25]: The BTCV dataset, known as Synapse for Multi-organ CT Segmenta- tion, is derived from the MICCAI Multi-Atlas Labeling Be- yond the Cranial Vault challenge. It includes abdominal CT scans of 30 subjects encompassing 8 organs. The dataset is expertly annotated under the supervision of clinical ra- diologists at Vanderbilt University Medical Center. Each scan is captured using contrast enhancement in the portal venous phase and contains between 80 to 225 slices with 512×512 pixels. The thickness of each slice varies from 1 to 6 mm. Following previous methods, we adopt the same dataset split as used in [5] with 18 train samples and test on the remaining 12 cases. We evaluate the performance on eight abdominal organs (i.e., spleen, right kidney, left Algorithm 1 MedContext: Training Input: Dataset D, student Fθs, teacher Fθt, masking function g(.) with masking ratio α, Step = 0. Require: Initialize teacher weights θt with the student weights θs. Network momentum rate m follows a cosine schedule from 0.996 to 1. Soft Dice loss LDice−CE. repeat Step ← Step + 1 // Sample data. sample {X, Y } ⊆ D, XM ← g(X, α) // student and teacher outputs. Fs , F M s = Fθs(X) , Fθs(XM) Ft = Fθt(X) // supervised objective L(Y, Fs) ← LDice−CE(Y, Fs) // masked reconstruction objectives. L(Y, F M s ) ← LDice−CE(Y, F M s ) Lc(Ft, F M s ) ← ∥ F M s −Ft∥ 2 2 ∥ Ft∥ 2 2 // Combined Loss. L ← L(Y, Fs) + L(Y, F M s ) + βLc(Ft, F M s ) // update student with combined loss. θs ← θs − δ∇θs(L) // update teacher weights by EMA. θt ← mθt − (1 − m)θs until converge Algorithm 2 Inference Input: Test Dataset D′, student network Fθs initialized with learned paramaters θs, Step = 0. Require: Evaluation metric E. repeat Step ← Step + 1 // Sample data. sample {X, Y } ⊆ D′ // model prediction. F = Fθs(X) // calculate evaluation metric. Evaluate ← E(Y, F) until go through all test data kidney, gallbladder, liver, stomach, aorta, and pancreas) by measuring the Dice Similarity Coefficient (DSC) and 95% Hausdorff Distance (HD95). In all the cases, the intensities of input volumes are normalized from the range of [-1000, 1000] to [0,1] Hounsfield Units (HU). For UNETR [19] and SwinUNETR [17], we follow their respective data process- ing pipeline where each volume is pre-processed indepen- dently and resampled to have an isotropic voxel spacing of [1.5, 1.5, 2.0]. Input is sampled at a crop size of 96 x 96 x 96. For nnFormer [44], we use their data preprocessing pipeline in which each CT scan is independently processed by applying patch cropping to sample the input at a resolu- tion 128 × 128 × 64 with a spacing of [0.76, 0.76, 3]. 12 ACDC Dataset [2]: The ACDC dataset is a collection of cardiac MRI images and associated segmentation an- notations for the right ventricle (RV), left ventricle (LV), and myocardium (MYO) of 100 patients, obtained from ac- tual clinical exams. The dataset includes patients with var- ious heart conditions, including normal patients, patients with a myocardial infarction, dilated cardiomyopathy, hy- pertrophic cardiomyopathy, and abnormal right ventricle. For UNETR and SwinUNETR,[17], we split the dataset into 80 training and 20 testing samples following [44]. The input is sampled at a resolution 96 x 96 x 96 with a voxel spacing of [1.5, 1.5, 2.0]. For nnFormer [44], we follow their re- spective data processing pipeline and sample the input at a resolution 128 x 128 x 64 and report the results on all three classes using Dice similarity coefficient (DSC). BraTS Dataset [1, 30]: We use two versions of BraTS dataset: BraTS17 [30] and BraTS21 [1]. For UNETR and SwinUNTER we report results on the BraTS21 dataset to be consistent with the baseline settings. The BraTS21 dataset [1] is from the BraTS challenge which provides a large dataset of 3D MRI scans, with voxel-wise ground truth la- bels annotated by clinicians. The dataset includes 1251 sub- jects, each with four 3D MRI modalities: native T1, post- contrast T1-weighted (T1Gd), T2-weighted (T2), and T2 Fluid-attenuated Inversion Recovery (T2-FLAIR). The im- ages have been rigidly aligned, resampled to a 1 x 1 x 1 mm isotropic resolution, and skull-stripped, resulting in an input image size of 240 x 240 x 155. The dataset includes anno- tations for three tumor sub-regions: the enhancing tumor, the peritumoral edema, and the necrotic and non-enhancing tumor core. The annotations have been combined into three nested sub-regions: Whole Tumor (WT), Tumor Core (TC), and Enhancing Tumor (ET). Following the data split used by [17], we train on 1000 subjects and test on 251 subjects. The input is cropped to a size 96 x 96 x 96 for training and Dice Similarity score (DSC) is used as an evaluation met- ric. For nnFormer, we use BraTS17 dataset [30]. The task comprises of 484 MRI images, each having four channels - FLAIR, T1w, T1gd and T2w. These images are obtained from 19 different institutions and represent a subset of the data used in the 2016 and 2017 Brain Tumor Segmentation (BraTS) challenges. The objective was to identify the three tumor sub-regions: edema (ED), enhancing tumor (ET), and non-enhancing tumor (NET). C. Qualitative Comparisons In Figure 5, we present a comparison between our MedCon- text approach and the baseline method on the synapse multi- organ dataset using the UNETR architecture. The results indicate that our approach produces more accurate segmen- tations than the baseline. As for instance, in the first row of Figure 5, the baseline (first row, third column) wrongly segments the right kidney (dark green) as left kidney (dark blue). Our approach on the other hand segments both the kidneys correctly (first row, fourth column). In the sec- ond row of the figure, for example, the baseline struggles to segment the pancreas, but when combined with our Med- Context approach, the model correctly segments the organ. Similarly, in the third row, our approach correctly segments the right adrenal gland, which the baseline model almost wrongly segments as spleen. Similarly, Figure 6 illustrates the outcomes of integrat- ing our MedContext technique with UNETR on the ACDC dataset. Our method produces segmentation boundaries that are more precise and well-defined than the baseline UNETR method. The baseline’s segmentation boundary of the My- ocardium (shown in green) in the first row’s third column is not accurate when compared to the ground truth segmen- tation boundary. This trend is also observed in the second row’s third column where the baseline fails to segment the Myocardium efficiently. Additionally, in the third row, the baseline fails to identify the right ventricular cavity. In con- trast, our MedContext outperforms the baseline in all the cases and efficiently segments all the organs. 13 Spleen R-Kid L-Kid Gal Eso Liv Sto Aor ICV PSV Pan Rad. Lad. Figure 5. Qualitative comparison on multi-organ synapse dataset: We showcase the benefit of our MedContext framework implemented on the UNETR architecture. The examples display various abdominal organs, with their corresponding labels in the legend below. The existing baseline method struggles to accurately segment the organs as can be seen from the red boxes. Best viewed in zoom. RV cavity Myocaridum LV cavity Figure 6. Qualitative comparison on ACDC dataset using UNETR: We showcase the benefit of our MedContext framework integrated with UNETR architecture on ACDC dataset. The examples display three heart regions with their corresponding labels in the legend below. The baseline UNETR struggles to accurately segment the organs as can be seen from the red boxes. Our approach on the other hand produces correct and sharp segmentation boundaries. Best viewed in zoom. 14 "
}