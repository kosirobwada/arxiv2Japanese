{
    "optim": "MedContext: Learning Contextual Cues for Efficient\nVolumetric Medical Segmentation\nHanan Gani1\nMuzammal Naseer1\nFahad Khan1,2\nSalman Khan1,3\n1Mohamed bin Zayed University of Artificial Intelligence\n2Link¨oping University\n3Australian National University\nAbstract\nVolumetric medical segmentation is a critical component\nof 3D medical image analysis that delineates different se-\nmantic regions. Deep neural networks have significantly\nimproved volumetric medical segmentation, but they gen-\nerally require large-scale annotated data to achieve better\nperformance, which can be expensive and prohibitive to ob-\ntain.\nTo address this limitation, existing works typically\nperform transfer learning or design dedicated pretraining-\nfinetuning stages to learn representative features. However,\nthe mismatch between the source and target domain can\nmake it challenging to learn optimal representation for vol-\numetric data, while the multi-stage training demands higher\ncompute as well as careful selection of stage-specific de-\nsign choices. In contrast, we propose a universal training\nframework called MedContext that is architecture-agnostic\nand can be incorporated into any existing training frame-\nwork for 3D medical segmentation. Our approach effec-\ntively learns self-supervised contextual cues jointly with\nthe supervised voxel segmentation task without requiring\nlarge-scale annotated volumetric medical data or dedicated\npretraining-finetuning stages. The proposed approach in-\nduces contextual knowledge in the network by learning to\nreconstruct the missing organ or parts of an organ in the\noutput segmentation space. The effectiveness of MedCon-\ntext is validated across multiple 3D medical datasets and\nfour state-of-the-art model architectures.\nOur approach\ndemonstrates consistent gains in segmentation performance\nacross datasets and different architectures even in few-shot\ndata scenarios. Our code and pretrained models are avail-\nable at https://github.com/hananshafi/MedContext.\n1. Introduction\nMedical imaging analysis uses a variety of techniques to\nanalyze scans obtained from procedures such as X-rays, CT\nscans, and MRIs [28]. The volumetric medical segmenta-\ntion of a scan is a critical component of this analysis, which\nFigure 1. Comparison, in term of Dice scores (%), when integrat-\ning our approach into UNETR [19], SwinUNETR [17] and nn-\nFormer [44] for medical segmentation on Synapse dataset (Sec. 4)\nusing conventional setting (Left) and few-shot setting (5 samples\nonly, Right). Without any modification to the model architecture\nor its training pipeline, our proposed universal approach comple-\nments the supervised voxel-wise segmentation and enhances the\nperformance of state-of-the-art architectures.\nidentifies specific structural features by separating them into\ndifferent segments or regions.\nA clinician may use this\nvoxel-wise segmentation to diagnose medical conditions for\ntreatment or further testing.\nDeep neural networks have improved volumetric medi-\ncal segmentation. The convolutional encoder-decoder net-\nworks, U-NET [9, 36], as well as the development of vision\ntransformers [13], has led to hybrid architectures [3, 24]\nwith complementary strengths of self-attention and convo-\nlution for medical segmentation. Despite the architectural\nadvances, deep neural networks generally require large-\nscale annotated data to achieve better performance. How-\never, collecting and annotating medical images at a large\nscale can be expensive and prohibitive due to privacy con-\ncerns.\nTo deal with the data scarcity, weights learned on Ima-\ngeNet [11] can be used to initialize the encoder, however,\npre-training on 2D natural images may not capture the con-\ntextual information essential to understanding 3D medical\nimages. Recent studies [17, 20, 40] explore self-supervised\npre-training on extra auxiliary medical data, but this ap-\nproach has two limitations: a) the training process con-\nsists of a two-stage pre-training process on the auxiliary\ndata, followed by fine-tuning on the target data which can\n1\narXiv:2402.17725v1  [eess.IV]  27 Feb 2024\nSpleen\nR-Kid\nL-Kid\nGal\nEso\nLiv\nSto\nAor\nICV\nPSV\nPan\nRad.\nLad.\nRV cavity\nMyocaridum\nLV cavity\nFigure 2. Qualitative Comparison between the baseline nnFormer [44] and our proposed MedContext integrated with nnFormer. The\nexamples display different abdominal organs (Synapse) (Left) and regions of the heart (ACDC) (Right), with their corresponding labels\nin the legend below. The baseline nnFormer struggles to accurately segment the organs and heart regions. In certain cases, it gives\nfalse segmentation results highlighted in red boxes. Best viewed zoomed in. Refer to supplementary material for additional qualitative\ncomparisons.\nbe computationally expensive, and b) the success of fine-\ntuning depends on how well the auxiliary data distribution\nmatches the target data. Moreover, there may not be a direct\nrelationship between the self-supervised objectives (e.g., in-\npainting, solving jigsaws, or predicting rotation) and voxel-\nwise segmentation. Therefore jointly optimizing such self-\nsupervised losses with 3D segmentation is non-trivial.\nTo address these limitations, we propose a generic train-\ning framework dubbed MedContext to learn self-supervised\ncontextual cues jointly with supervised voxel segmentation\nwithout requiring large-scale annotated volumetric medical\ndata. Specifically, we propose to reconstruct the masked or-\ngans or parts of an organ of an input image in the output\nsegmentation space. Since our voxel-wise segmentation re-\nconstruction is well aligned with the voxel-wise prediction\ntask, both tasks can be optimized together. To further re-\nduce the disparity between voxel segmentation reconstruc-\ntion and prediction tasks, we deploy a student-teacher distil-\nlation strategy to guide reconstruction from a slow-moving\nonline teacher model which also helps avoid representa-\ntion collapse. MedContext encourages contextual learning\nwithin the model and allows it to learn local-global rela-\ntionships between different input components. This leads\nto better segmentation of organ boundaries (see Fig. 2).\nOur proposed approach is architecture-agnostic and can\nbe incorporated into any training framework, making it uni-\nversally applicable. We integrate our approach into three\nrecent state-of-the-art medical 3D transformer based archi-\ntectures: UNETR [19], SwinUNETR [17] and nnFormer\n[44]; and one CNN based 3D architecture PCRLv2 [45].\nUsing these architectures, we validate our approach across\nthree medical imaging datasets: Multi-organ Synapse [25],\nACDC [2] and BraTS [1, 30]. We observe consistent perfor-\nmance gains on all the datasets. Specifically, on the Synapse\ndataset with nnFormer and UNETR architectures (Fig. 1),\nwe observe a performance improvement of ∼1% and 2%\nrespectively in terms of Dice score. Similarly, on ACDC\ndataset with UNETR architecture, we observe a ∼4% gain\nin the Dice score compared to the baseline. We further as-\nsess the effectiveness of our framework in comparison to\nvarious pretraining-finetuning methods including [7], [41],\nand [21]. Our evaluation reveals consistent performance im-\nprovements across all compared methods.\nIn summary, our contributions are three-fold:\n1. We propose a universal training framework to jointly op-\ntimize supervised segmentation and self-supervised seg-\nmentation reconstruction via student-teacher knowledge\ndistillation for the volumetric medical data.\n2. Our approach induces contextual knowledge in the net-\nwork by learning to reconstruct the missing organ or\nparts of an organ in the output segmentation space.\n3. We validate the effectiveness of our approach across\nmultiple 3D medical datasets and state-of-the-art model\narchitectures. Our approach complements existing meth-\nods and improves segmentation performance in conven-\ntional as well as few-shot data scenarios.\n2. Related Work\n3D Medical Segmentation:\nSeveral U-Net [36] based\nencoder-decoder architectures have been proposed to solve\nthe problem of 3D medical segmentation. [10] modifies the\nbasic U-Net architecture by replacing 2D operations with\n3D operations to capture the 3D context. [23] proposed\nto learn multi-scale feature representations from varying\nresolutions for multi-organ segmentation.\nOther works\n2\nhave suggested integrating the contextual information with\nCNN-based frameworks using image pyramids [43], large\nkernels [32], dilated convolution [6], and deformable con-\nvolution [26]. Few recent works [3, 24] have explored the\nuse of transformer architectures for 3D volumetric segmen-\ntation by dividing the volumetric images into 3D patches\nwhich are then flattened to construct a 1D embedding and\npassed to transformer module. More recently, hybrid archi-\ntectures [5, 19, 27, 39, 42, 44] combining the strengths of\nboth CNNs and transformers have been proposed to encode\nboth local and global contexts. We build our approach on\nthese hybrid architectures by proposing a complementary\ntraining mechanism that induces contextual information in\nthese architectures.\nMasked Image Modeling (MIM): MIM has emerged\nas an appealing self-supervised representation learning\nmethod, fueled by the success of ViTs. Recent MIM meth-\nods [21, 41] are trained to predict the pixel values to recon-\nstruct the corresponding missing tokens in the input. Re-\ncent works on videos [14, 21, 38] use temporal masking to\nlearn self-supervised representations for videos. In case of\nmedical imaging, [8] explores MIM on 3D medical data.\n[45] proposes pixel restoration task on 3D medical images.\nHowever, these approaches are based on a two-stage pro-\ncess of self-supervised pretraining on large public datasets\n[16, 37] and then finetuning. Different from existing self-\nsupervised learning methods which require an additional\nstage of supervised finetuning on labeled data, we syner-\ngically incorporate MIM with the voxel-wise segmentation\nobjective in a single training framework without any exter-\nnal data.\nKnowledge Distillation: Introduced by [22], knowl-\nedge distillation aims to transfer knowledge from a complex\nmodel (teacher) to a simpler model (student). Recent works\n[4, 35] have explored the use of knowledge distillation\nwith an online student-teacher strategy for self-supervised\nlearning.\n[35] employs a student-teacher based knowl-\nedge distillation framework for self-supervised representa-\ntion learning using a contrastive objective. [4] uses a dy-\nnamic student-teacher framework to learn local-global cor-\nrespondences by distilling the knowledge from the teacher\nwhich is built dynamically. [34] extends the knowledge dis-\ntillation approach to medical segmentation where a strong\nteacher model is used to distill knowledge within a small\ncompressed model. All these approaches are applicable in\nthe 2D data scenario. In contrast, our proposed knowledge\ndistillation framework works on 3D volumetric inputs and\nserves as a means to reconstruct the missing voxels in the\nsegmentation space in a single end-to-end training stage.\n3. Methodology\nThe existing 3D medical segmentation methods focus on\noptimizing voxel-wise segmentation, typically by minimiz-\ning the dice loss between predicted and ground truth seg-\nmentation masks. However, this approach may not capture\nthe underlying 3D structure of the input or the contextual\nrelationships between different objects, especially in data-\nscarce scenarios such as in the medical domain. To address\nthis limitation, we propose a novel approach that learns\ncontextual relationships between different organs or organ\nparts in the output segmentation space. Specifically, we for-\nmulate the problem as reconstructing missing organ parts\nfrom a masked input volume in the segmentation space us-\ning student-teacher knowledge distillation.\nOur proposed approach complements the voxel-wise\nsegmentation task by inducing contextual consistency\nthrough joint optimization of two objectives: a) voxel-wise\nsegmentation reconstruction from the masked input, and b)\nsupervised voxel-wise segmentation prediction. The learn-\ning objectives enable the model to reconstruct and segment\nthe missing organs or organ parts, thereby encoding con-\ntextual information. This allows the model to learn local-\nglobal relationships between different input components for\na data-efficient and accurate 3D semantic segmentation. In\nsummary, our approach provides a solution to the limita-\ntions of current 3D medical semantic segmentation methods\nby incorporating contextual learning via joint optimization\nof multi-task objectives. Next, we explain the network ar-\nchitecture, objective functions and optimization scheme.\n3.1. Architecture\nOur approach is complementary and can be applied to\nthe existing encoder-decoder architectures designed for 3D\nmedical image segmentation.\nSpecifically, we show the\nbenefit of our approach using three recently introduced\nmodels: UNETR [19], SwinUNETR [17], and nnFormer\n[44]. In these architectures, the encoder processes a 3D in-\nput volume and produces latent feature representation. The\ndecoder then maps this representation to the corresponding\nsegmentation map through upsampling blocks. Addition-\nally, skip connections are used to exchange the multi-level\nfeatures across different encoder-decoder layers. We illus-\ntrate how our approach can be integrated into these hybrid\narchitectures, which combine both convolution and trans-\nformer components, for 3D medical image segmentation.\nAs shown in Fig. 3 our design includes a student Fs\nand a teacher Ft network that operate on the input volume\nX ∈ RH×W ×D and its masked version XM ∈ RH×W ×D\ngenerated using the masking function g(.). Here, H, W,\nand D represent the height, width, and depth of the 3D\ninput volume, respectively. During the training phase, the\ninput views are fed to the student-teacher framework as\n3D patches, which generates voxel-wise semantic logits for\neach input view. The student network is provided with both\nthe masked (XM) and unmasked (X) inputs, and the corre-\nsponding output voxel-wise semantic logits are denoted as\n3\nFigure 3. Overview of our MedContext approach: The original 3D volume is masked and fed to the student model (top-row) along with\nthe original input. The teacher model (bottom-row) is only fed with the original volume. The difference between the semantic voxelwise\npredictions for the masked and original inputs corresponding to the student and teacher networks respectively is minimized to guide the\nreconstruction of masked regions in the output segmentation space. Our approach induces contextual consistency by enabling the model to\nreconstruct and segment the missing organs or organ parts and therefore yields more precise and accurate segmentation results.\nFs and F M\ns , respectively. On the other hand, the teacher\nnetwork is provided with the original unmasked input X\nwhich outputs voxel-wise semantic logits denoted as Ft.\nThe feature map produced at intermediate layers of the 3D\narchitecture has a shape of\nH\nP1 × W\nP2 ×\nD\nP3 × C, where\n(P1, P2, P3) is the resolution of each patch and C is the\nfeature dimension.\nFor each output prediction from the student , a super-\nvised loss is computed using the ground truth label Y , as\nshown in the figure. Additionally, a self-supervised objec-\ntive is minimized between the masked student logits F M\ns\nand the teacher logits Ft. Finally, both the supervised and\nself-supervised objectives are jointly optimized during our\nsingle-stage training process.\n3.2. Volumetric Masking Strategy\nTo model contextual relationships, we employ a masking\ntechnique on the original input X to reconstruct missing\nparts in the segmentation space. As illustrated in Fig. 3,\nwe ensure mask consistency across the depth by apply-\ning the same mask to all subsequent slices in the volume,\nthereby ensuring that any masked organ or region in the first\nslice remains masked in all subsequent slices. Our encoder-\ndecoder architectures convert the input volume into patch\ntokens before processing. To generate a masked view XM,\nwe randomly mask a certain fraction δ of the patch tokens.\nFollowing [12], the masked tokens are replaced with learn-\nable tokens Hξ, such that,\nXM = g(X, δ) = X ◦ (1 − Iδ) + Hξ ◦ Iδ,\n(1)\nwhere Iδ is a binary mask generated according to a\nBernoulli distribution using g(.), i.e., Iδ ∼ Bernoulli(δ) and\n◦ denotes the element-wise product. This masking tech-\nnique prevents information leakage from neighboring cubes\nby enforcing the same masking map for all cubes along the\ndepth dimension. Our approach encourages the model to\nlearn contextual semantic relationships by recovering the\nsegmentation map for such masked inputs.\n3.3. Voxel-wise Segmentation Reconstruction\nUsing masked input, we reconstruct the segmentation map\nto facilitate learning contextual semantic relationships. To\naccomplish this task, a naive approach would be to feed\nboth the original and masked views into a single model and\noptimize the outputs of both views using supervised loss\nwith ground truth labels. However, this approach has cer-\ntain limitations. When a single model is used to reconstruct\nthe masked volume, the model’s weight updates are solely\ndependent on information from the current views. A num-\nber of self-supervised learning techniques, such as DINO\n[4] and BYOL [35], have shown that leveraging the knowl-\nedge acquired by the model during previous weights up-\ndates can lead to more effective guidance and avoids repre-\nsentation collapse. This can be achieved by implementing a\nstudent-teacher strategy where teacher weights are updated\nby a moving average of the student weights. In this way, the\ncollective knowledge learned during previous weight up-\ndates assists in the reconstruction of the masked views and\nthereby induces enriched contextual cues. Additionally, the\nteacher network provides soft semantic targets that guide\nthe training of the student network. The soft targets contain\ninformation about the relationships between the views and\naids the student network in generalizing more effectively.\nWe initialize the student model Fs and teacher model\nFt with the same randomly initialized weight parame-\nters. The student network receives both the original and\nmasked inputs, while the teacher network receives only\nthe original non-masked input.\nThe networks generate\nvoxel-wise semantic logits, represented by {F M\ns , Fs} and\n4\nFt respectively, which are processed to obtain semantic\nvoxel-wise segmentation map predictions. Following this,\nwe reconstruct semantic voxel-wise logits of the masked\ninput from the student model guided by two supervised\nsignals:\nsupervision through knowledge distillation and\nground truth labels, as explained below.\nReconstruction through Knowledge Distillation: A self-\nsupervised distillation loss (Eq. 2) is used to guide the train-\ning of the student network to encourage modeling the con-\ntextual consistency. It minimizes the difference between the\nvoxel-wise logits generated by the teacher network given\nthe original input Ft and the voxel-wise logits produced by\nthe student network using the masked input F M\ns . The ob-\njective function, referred to as Consistency Loss (CL), is\ndenoted as Lc(F M\ns , Ft) and is expressed as,\nLc(F M\ns , Ft) = ∥ F M\ns\n− Ft∥ 2\n2\n∥ Ft∥ 2\n2\n.\n(2)\nReconstruction through Ground truth Labels:\nThe\nvoxel-wise semantic logits output by the student network\nfor a given masked input F M\ns\nare further reconstructed us-\ning the ground truth labels. This is accomplished by mini-\nmizing the soft dice loss [31] using the ground truth labels\nY . The general expression for Dice-CE Loss for some ar-\nbitrary output prediction F is given as,\nLDice−CE(Y , F ) = 1 −\nC\nX\nc=1\n \n2 ∗ PV\nv=1 Yv,c · Fv,c\nPV\nv=1 Y 2\nv,c + PV\nv=1 F 2v,c\n+\nV\nX\nv=1\nYv,c log Fv,c\n!\n, (3)\nwhere, C denotes the number of classes; V denotes the\nnumber of voxels; Yv,i and Fv,i denote the ground truths\nand output probabilities for class i at voxel v, respectively.\nIn our case, the supervised objective function for recon-\nstruction is calculated using above Dice-CE loss between\nthe ground truth label Y and voxel-wise semantic logits\noutput F M\ns\nand is denoted as LDice−CE(Y , F M\ns ) and re-\nferred to as Masked Student Loss (MSL).\nOur reconstruction objective on the masked input vol-\numes induces contextual consistency to enhance segmen-\ntation performance. The reconstruction of the missing re-\ngions enables the network to capture intricate relationships\nbetween various organs in the input volume and learn the\nbroader context of the input volume going beyond the local\nfeatures to segment organs. This promotes the preservation\nof the global structure of the input volume, yielding more\nprecise and accurate segmentation results. Moreover, the\nobjective of predicting missing regions in the masked in-\nput further encourages the model to learn correspondence\nbetween the local and global structure in the input vol-\nume. Masked input volumes represent local views of in-\nputs that are matched with original global input volumes.\nThis matching objective allows the model to capture the re-\nlationships between neighboring regions which is essential\nfor learning class-specific semantic features to better cap-\nture the object boundaries and shapes.\n3.4. Supervised Voxel-wise Segmentation\nOur primary task of supervised voxel-wise segmentation\nprediction takes place in conjunction with the voxel-wise\nsegmentation reconstruction as discussed above. For the\nsupervised voxel-wise segmentation, we optimize the pre-\ndictions of the original input X from the student network\nFs through the supervision of the ground truth labels Y\nusing Soft Dice Loss (Eq.\n3) denoted by the objective\nLDice−CE(Y , Fs).\n3.5. Overall Multi-task Objective\nOur framework leverages a combination of supervised and\nself-supervised losses to optimize the learning process. The\nmulti-task objectives synergistically reinforce each other\nand provide complementary advantages. The overall loss\nobjective L is given as,\nL =LDice−CE(Y , Fs) + LDice−CE(Y , F M\ns )\n+ βLc(F M\ns , Ft),\n(4)\nwhere the hyperparameter β controls the contribution of\nself-supervised consistency loss during optimization.\n3.6. Optimization strategy\nFollowing a typical student-teacher optimization strategy as\nultilized by [4, 15], the gradient of the total loss is back-\npropagated through the student network and parameters are\nupdated as follows,\nΘ ← Θ − α · ∇Θ(L),\n(5)\nwhere Θ represents the joint parameters of student network\n(θs) and learnable mask embeddings (ξ) i.e. Θ = {θs; ξ}.\nThe teacher network is updated via exponential moving av-\nerage (EMA) of the weights of the student network using,\nθt ← λθt + (1 − λ)θs,\n(6)\nwhere θt denote the parameters of teacher and, λ follows the\ncosine schedule from 0.996 to 1 during training. The gradi-\nent step through the student network comprises of the con-\ntributions from both the supervised and self-supervised ob-\njectives, thereby aiding in the reconstruction of the masked\ninput by updating the differentiable volumetric embeddings\nassociated with the masked regions in the input volume.\nAvoiding Mode collapse: The student-teacher frameworks\n5\nin general are often prone to mode collapse where the stu-\ndent model fails to learn the entire range of outputs that\nthe teacher model can generate.\nOur framework avoids\nthe mode collapse by introducing additional supervised\nloss from the student corresponding to the output from\nthe masked volume, which in addition to assisting in the\nmasked reconstruction, also acts as a discriminative objec-\ntive to encourage the student model to learn a more diverse\nset of outputs that better match the full range of outputs gen-\nerated by the teacher. Additionally, it further encourages\nteacher model to generate high-quality targets for distilla-\ntion. We show the effect of our different losses in Sec. 4.4.\n4. Experiments\nDatasets: We evaluate on three volumetric medical scan\ndatasets. Synapse BTCV Dataset: The BTCV dataset [25],\nknown as Synapse for Multi-organ CT Segmentation, in-\ncludes abdominal CT scans of 30 subjects encompassing 8\norgans. Following previous methods, we adopt the same\ndataset split as used in [5] with 18 train samples and test\non the remaining 12 cases. We evaluate the performance\non eight abdominal organs (i.e. spleen, right kidney, left\nkidney, gallbladder, liver, stomach, aorta, and pancreas) us-\ning Dice Similarity Coefficient (DSC) and 95% Hausdorff\nDistance (HD95). In all the cases, the intensities of input\nvolumes are normalized from the range of [-1000, 1000] to\n[0,1] Hounsfield Units (HU). ACDC Dataset: The ACDC\ndataset [2] is a collection of cardiac MRI images and associ-\nated segmentation annotations for the right ventricle (RV),\nleft ventricle (LV), and myocardium (MYO) of 100 patients,\nobtained from actual clinical exams. we split the dataset\ninto 80 training and 20 testing samples following [44] and\nreport the results on all three classes using Dice similarity\ncoefficient (DSC). BraTS Dataset: We use two versions of\nBraTS dataset: BraTS17 [30] and BraTS21 [1]. For UN-\nETR, SwinUNTER and PCRLv2 we report results on the\nBraTS21 dataset to be consistent with the baseline settings.\nThe BraTS21 dataset [1] is from the BraTS challenge and\nincludes 1251 subjects. The annotations have been com-\nbined into three nested sub-regions: Whole Tumor (WT),\nTumor Core (TC), and Enhancing Tumor (ET). Following\nthe data split used by [18], we train on 1000 subjects and\ntest on 251 subjects. For nnFormer, we use BraTS17 dataset\n[30]. The task comprises of 484 MRI images. Following\nthe dataset split of [44], we train on 387 training samples\nand test on 73 cases. Further details about the datasets and\npre-processing are provided in Appendix B.\nEvaluation Metrics: To evaluate the models’ perfor-\nmance, we utilize two metrics: the Dice Similarity Score\n(DSC) and the 95% Hausdorff Distance (HD95). The DSC\nmetric measures the degree of overlap between the volumet-\nric segmentation predictions and the voxels of the ground\ntruths as follows,\nDSC(Y, F) = 2 ∗ |Y ∩ F|\n|Y | ∪ |F| = 2 ∗\nY · F\nY 2 + F 2\n(7)\nwhere, Y and F denote the ground truths and output logits\nfor all the voxels, respectively.\nThe HD95 metric is frequently employed as a boundary-\nbased measure for determining the 95th percentile of dis-\ntances between the boundaries of the volumetric segmen-\ntation predictions and the voxels of the ground truths. Its\ndefinition is as follows:\nHD95(Y, F) = max{dY F , dF Y }\n(8)\nHere, dY F represents the maximum distance at the 95th per-\ncentile between the predicted voxels and the ground truth,\nwhile dY represents the maximum distance at the 95th per-\ncentile between the ground truth and the predicted voxels.\nTraining and Implementation details: Our approach\nutilizes Pytorch version 1.10.1 in conjunction with MONAI\nlibraries [33] for implementation. To ensure fairness, we\nfollow the respective training frameworks of the baseline\narchitectures. Specifically, we use an input size of 128 x\n128 x 64 for all datasets when training with nnFormer, and\n96 x 96 x 96 for UNETR and SwinUNTER. All models are\ntrained using a single A100 40GB GPU. For nnFormer, we\ntrain on all datasets for 1000 epochs, using AdamW opti-\nmizer [29] with a learning rate of 0.01 and weight decay of\n3e−5, and for UNETR and SwinUNETR, we train for 5000\nepochs on BTCV synapse, 1000 epochs on ACDC, and 300\nepochs on BRaTs dataset, consistent with baseline settings.\nThe learning rate is kept default as per the given framework.\nDuring training, we apply the same data augmentations as\nused in UNETR, Swin UNETR, and nnFormer. Further de-\ntails are available in the supplementary materials, and the\ncode and models will be made publicly available.\n4.1. Comparison with state-of-the-art Baselines\nWe show the effectiveness of our approach using three\nstate-of-the-art 3D transformer based segmentation mod-\nels: UNETR, SwinUNETR, nnFormer, and one CNN-based\nmodel: PCRLlv2, across three datasets: Synapse Multi-\nOrgan, ACDC, and BraTS (2017 and 2021).\nSynapse Multi-Organ Dataset: Table 1 shows the results\non the synapse multi-organ dataset. We calculate perfor-\nmance metrics using Dice similarity score and HD95 score,\nmaintaining consistent training settings as discussed above.\nUNETR with our approach achieves\n2.5% higher Dice\nScore (81.13%) than the baseline (78.76%), and over 1%\nreduction in HD95 score. With hierarchical SwinUNETR,\nDice Score increases by >1% (80.66% to 82.00%) and\nHD95 improves. For the complex nnFormer architecture,\nour approach enhances Dice score from 86.57% to 87.35%,\n6\nModels\nMedContext\nSpleen\nRight Kidney\nLeft Kidney\nGallbladder\nLiver\nStomach\nAorta\nPancreas\nAverage\nHD95 ↓\nDSC ↑\nUNETR\n✗\n89.64\n83.02\n84.86\n63.06\n95.58\n73.06\n87.47\n53.40\n11.04\n78.76\n✓\n90.73\n83.36\n86.03\n67.94\n95.59\n78.62\n87.30\n59.51\n9.44\n81.13\nSwin-UNETR\n✗\n86.33\n80.63\n84.07\n67.24\n94.98\n74.97\n90.53\n66.49\n20.32\n80.66\n✓\n91.45\n80.80\n84.85\n67.70\n94.60\n76.20\n90.88\n67.74\n14.45\n82.00\nnnFormer\n✗\n90.51\n86.25\n86.57\n70.17\n96.84\n86.83\n92.04\n83.35\n10.63\n86.57\n✓\n95.97\n87.05\n87.63\n72.87\n96.43\n84.57\n91.85\n82.40\n8.29\n87.35\nTable 1. Abdominal multi-organ Synapse: Our MedContext consistently improves the segmentation performance of all organs across\ndifferent models. We observe significant improvements in HD95 along with the dice score (DSC). The best results are highlighted in bold.\nModels\nMedContext\nRV\nMyo\nLV\nAverage\nUNETR\n✗\n77.81 72.74 79.46\n76.67\n✓\n84.77 75.82 81.21\n80.60\nSwinUNETR\n✗\n83.47 75.54 83.09\n80.70\n✓\n84.79 79.17 86.15\n83.38\nnnFormer\n✗\n91.18 86.24 94.07\n90.50\n✓\n92.14 86.52 93.52\n90.73\nTable 2. ACDC: We report the performance on\nthe right ventricle (RV), left ventricle (LV), and\nmyocardium (MYO).\nModels\nMedContext\nWT\nET\nTC\nAverage\nUNETR\n✗\n87.35 90.88 84.29\n87.50\n✓\n87.43 91.45 85.23\n88.04\nSwinUNETR\n✗\n90.36 91.72 86.24\n89.44\n✓\n90.57 92.30 86.64\n89.83\nnnFormer\n✗\n80.80 58.86 77.42\n72.36\n✓\n81.00 59.87 77.45\n72.78\nTable 3. BraTS: We report the performance on\nthree brain tumour types, demonstrating effec-\ntiveness of our approach for all the three cases.\nModels\nMedContext Synapse ACDC\nUNETR\n✗\n53.83\n18.53\n✓\n56.25\n28.63\nSwinUNETR\n✗\n54.13\n32.62\n✓\n61.15\n35.80\nnnFormer\n✗\n67.90\n52.23\n✓\n70.96\n58.05\nTable 4. Few-shot: Performance of our\nMedContext in 5-shot scenario (5 sam-\nples only).\nwith over 2% HD95 reduction. ACDC Dataset: Table 2\nshows the results on the relatively bigger ACDC dataset\nWe observe that UNETR architecture complemented with\nour approach outperforms the baseline by a good margin\nof roughly 4% in terms of Dice score (80.60% vs 76.67%).\nWe further observe that the Dice score per organ with our\napproach improves as compared to the baseline in each\ncase.\nSwinUNETR shows over 2.5% increment in Dice\nscore over baseline, along with higher per organ dice scores.\nnnFormer follows a similar trend, achieving 90.73% Dice\nscore compared to the baseline’s 90.50%. BraTS Dataset:\nWe use BraTS17 for UNETR and SwinUNETR architec-\ntures, and BraTS21 for nnFormer architecture. As shown\nin Table 3, with UNETR architecture, we report a gain of\n0.54% in the overall dice score as compared to the base-\nline. With SwinUNTER, we report a Dice score of 89.83%\nwhich is greater than baseline dice score of 89.44%. Finally,\nwith nnFormer, we observe a gain in the overall dice score\n(72.78%) as compared to the baseline dice score (72.36%).\nOverall we show that our approach achieves gains even on\nlarger datasets such as BraTS, but has more pronounced im-\nprovement for the low-data setups.\n4.2. Few-shot performance\nWe also validate the effectiveness of our approach in the\nfew-shot scenario. Table 4 shows the performance compar-\nison of our approach with the baselines in a 5-shot setting.\nWe conduct few-shot experiments on Synapse BTCV and\nACDC datasets using all three model architectures. Specif-\nically, on Synapse BTCV, our approach results in a 3-10%\nMethod\nPretrain\nSpleen\nRKid\nLKid\nGall\nEso\nLiv\nSto\nAorta\nIVC\nVeins\nPan\nRAG\nLAG\nAvg\nBaseline\n✗\n89.0\n89.2\n87.7\n47.6\n48.9\n94.4\n74.7\n82.0\n77.3\n61.7\n64.4\n56.6\n46.9\n70.8\nSimCLR\n✓\n91.1\n91.3\n89.7\n48.7\n50.0\n96.6\n76.5\n83.9\n79.1\n63.2\n65.9\n57.9\n48.1\n72.4\nMAE\n✓\n94.8\n95.0\n93.4\n50.6\n52.1\n98.6\n79.7\n87.4\n82.4\n65.9\n68.6\n60.5\n50.1\n75.3\nSimMIM\n✓\n95.2\n95.4\n93.7\n51.9\n52.3\n98.7\n79.9\n87.7\n82.6\n66.0\n68.9\n60.7\n51.2\n75.7\nMedContext\n✗\n93.8\n93.7\n93.6\n54.9\n72.6\n96.6\n80.3\n89.9\n83.3\n72.9\n73.9\n64.4\n65.3\n79.6\nTable 5. MedContext vs. pretraining-finetuning [8]. DSC (%) on\nSynapse dataset with UNETR architecture.\nMethod\nPretrain\nBrats21\nACDC\nSynapse\nPCRLv2\n✓\n79.90\n78.53\n64.00\nPCRLv2 + MedContext\n✗\n82.03\n82.57\n72.30\nTable 6. Improving PCRLv2 with our proposed MedContext\nacross 3 datasets. We report Avg Dice scores.\nincrease in Dice score across all cases, indicating a signif-\nicant improvement in segmentation accuracy. Similarly, on\nthe relatively larger ACDC dataset, we observe a similar\ntrend of higher Dice scores when our approach is integrated\nwith the baseline models. This finding highlights the poten-\ntial of our approach in improving the segmentation accuracy\nof medical images in situations where annotated data is lim-\nited, making it suitable for data-efficient training.\n4.3. Pretraining-Finetuning Baselines\nWe demonstrate the effectiveness of MedContext by com-\nparing its performance with existing pretraining-finetuning\nmethods in Table 5. The baseline [8] method utilizes bet-\nter weight initialization based on pretraining with large\ndataset [16] across state-of-the-art self-supervised meth-\n7\nods [7, 21, 41] and then fine-tuned on the target dataset.\nIn contrast, our MedContext directly learns the contextual\ncues from the target small dataset without the pre-training\nstage and outperforms the methods employing pretraining-\nfinetuning paradigm. We further integrate our MedContext\ninto the official implementation of PCRLv2 [45], a 3D CNN\nbased state-of-the-art architecture, which is first pretrained\nin a self-supervised fashion on [37] and then finetuned on\nthe specific target dataset. As seen in Table 6, our MedCon-\ntext shows a complementary effect and improves the per-\nformance (Avg. Dice Score) of PCRLv2 architecture con-\nsistently without requiring pretraining. This also affirms the\nversatility of our framework, as it can be applied to CNN ar-\nchitectures, showcasing its universality.\n4.4. Ablation Studies\nEffect of Student-Teacher framework:\nAs discussed in\nSec. 3.3, we hypothesize that the approach of training a sin-\ngle model on both original and masked input views using\nsupervised loss with ground truth may not be sufficient for\nlearning contextual relationships, as this method does not\ntake into the account the knowledge acquired by the model\nduring previous weights updates, which can lead to more ef-\nfective guidance. This prompts us to use a student-teacher\nframework that leverages the information captured in the\nprevious weight updates. We provide empirical evidence to\nsupport our claim in Fig. 4 (left), where we report results on\nthe Synapse multi-organ dataset across all three model ar-\nchitectures. In all cases, we observe a drop in performance\nin the absence of student-teacher framework.\nEffect of Student vs Teacher weights:\nOur proposed\nMedContext jointly optimizes the multi-task objectives in\na student-teacher framework.\nAs discussed in Sec. 3.6,\nthe parameters of student and teacher networks are learned\nthrough different update rules. At inference, we can either\nchoose the student or the teacher weights for predictions.\nWe study the performance of student and teacher weights\nduring inference in Fig. 4 (right). We observe that the stu-\ndent weights perform better than the teacher counterpart on\nsynapse multi-organ dataset across two architectures.\nEffect of Masking Ratio: We propose to learn the con-\ntextual knowledge by allowing the model to reconstruct the\nmissing regions of the masked input in the segmentation\nspace. However, the number of missing patches to recover\nmay influence the performance of the model. We conduct\nan ablation study on a held-out validation set to determine\nthe optimal masking ratio. Table 7 shows the Dice scores\nfor a range of masking ratios across Synapse dataset us-\ning UNETR and SwinUNETR architectures complemented\nwith our approach. Although our method shows improve-\nment on all masking ratios, however, we observe that a 40%\nmasking ratio works best in our case.\nEffect of different losses: Our proposed approach utilizes\nFigure 4. DSC (%) on Synapse across different models. Left:\nDistillation from Teacher. We demonstrate the importance of\nknowledge distillation through teacher for effectively leveraging\ncontextual cues. Right: Student vs Teacher. We show that utiliz-\ning student weights during inference benefits overall performance.\nMasking ratio\nAverage Dice Score\nUNETR\nSwinUNETR\n30%\n79.54\n80.92\n40%\n80.47\n82.00\n50%\n80.00\n81.03\n60%\n80.20\n81.70\n80%\n79.90\n81.27\nTable 7. Understanding the ef-\nfect of masking ratio on our\nMedContext.\nWe report the\nDice score on the Synapse\nacross UNETR & SwinUNETR\narchitectures.\nMSL CL\nAverage Dice Score\nUNETR SwinUNETR\n✓\n✗\n78.69\n81.03\n✗\n✓\n79.46\n81.25\n✓\n✓\n80.32\n81.70\nTable 8.\nWe show the effect\nof each loss component on fi-\nnal objective (Eq. 4).\nWe re-\nport dice score (%) on Synapse\ndataset.\nmultiple supervised and self-supervised losses in the train-\ning stage. As discussed in Secs. 3.3 and 3.4, each loss com-\nponent holds a specific significance towards the final objec-\ntive. We conduct an ablative analysis on BTCV Synapse\ndataset to study the importance of the 2 loss components:\nMasked student loss (MSL) and Consistency Loss (CL),\nacross two architectures: UNETR and SwinUNETR, in Ta-\nble 8. We observe that removing any one of the loss com-\nponents results in a drop in the Dice score. We observe\nthis trend with both the architectures, providing empirical\nevidence that mutual synergy between supervised and self-\nsupervised losses helps induce contextual cues for effective\n3D medical segmentation.\n5. Conclusion\nIn this paper, we propose a universal training frame-\nwork called MedContext which effectively learns self-\nsupervised contextual cues jointly with the supervised\nvoxel segmentation task without requiring large-scale\nannotated volumetric medical data.\nOur proposed ap-\nproach employs a student-teacher distillation strategy\nto reconstruct missing organs or parts of organs in the\noutput segmentation space.\nThrough extensive experi-\nmentation,\nour approach demonstrates complementary\nbenefits to existing state-of-the-art 3D medical segmen-\ntation architectures in both conventional and few-shot\nsettings\nwithout\npretraining\non\nlarge-scale\ndatasets.\nMoreover,\nthe plug-and-play design of our approach\nallows for its easy integration into any architectural design.\n8\nReferences\n[1] Ujjwal Baid, Satyam Ghodasara, Suyash Mohan, Michel\nBilello, Evan Calabrese, Errol Colak, Keyvan Farahani,\nJayashree Kalpathy-Cramer, Felipe C Kitamura, Sarthak\nPati, et al. The rsna-asnr-miccai brats 2021 benchmark on\nbrain tumor segmentation and radiogenomic classification.\narXiv preprint arXiv:2107.02314, 2021. 2, 6, 13\n[2] Olivier Bernard, Alain Lalande, Clement Zotti, Freder-\nick Cervenansky, Xin Yang, Pheng-Ann Heng, Irem Cetin,\nKarim Lekadir, Oscar Camara, Miguel Angel Gonza-\nlez Ballester, Gerard Sanroma, Sandy Napel, Steffen Pe-\ntersen, Georgios Tziritas, Elias Grinias, Mahendra Khened,\nVarghese Alex Kollerathu, Ganapathy Krishnamurthi, Marc-\nMichel Roh´e, Xavier Pennec, Maxime Sermesant, Fabian\nIsensee, Paul J¨ager, Klaus H. Maier-Hein, Peter M. Full, Ivo\nWolf, Sandy Engelhardt, Christian F. Baumgartner, Lisa M.\nKoch, Jelmer M. Wolterink, Ivana Iˇsgum, Yeonggul Jang,\nYoonmi Hong, Jay Patravali, Shubham Jain, Olivier Hum-\nbert, and Pierre-Marc Jodoin. Deep learning techniques for\nautomatic mri cardiac multi-structures segmentation and di-\nagnosis: Is the problem solved? IEEE Transactions on Med-\nical Imaging, 37(11):2514–2525, 2018. 2, 6, 13\n[3] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\naopeng Zhang, Qi Tian, and Manning Wang.\nSwin-unet:\nUnet-like pure transformer for medical image segmentation.\nIn European Conference on Computer Vision Workshops,\n2022. 1, 3\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers. In Pro-\nceedings of the IEEE/CVF international conference on com-\nputer vision, pages 9650–9660, 2021. 3, 4, 5\n[5] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan\nAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.\nTransunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306,\n2021. 3, 6, 12\n[6] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian\nSchroff, and Hartwig Adam. Encoder-decoder with atrous\nseparable convolution for semantic image segmentation. In\nProceedings of the European conference on computer vision\n(ECCV), pages 801–818, 2018. 3\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learning\nof visual representations. In International conference on ma-\nchine learning, pages 1597–1607. PMLR, 2020. 2, 8\n[8] Zekai Chen, Devansh Agarwal, Kshitij Aggarwal, Wiem\nSafta, Mariann Micsinai Balan, and Kevin Brown. Masked\nimage modeling advances 3d medical image analysis. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision (WACV), pages 1970–1980, 2023.\n3, 7\n[9] ¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,\nThomas Brox, and Olaf Ronneberger.\n3d u-net: learn-\ning dense volumetric segmentation from sparse annota-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2016: 19th International Conference,\nAthens, Greece, October 17-21, 2016, Proceedings, Part II\n19, pages 424–432. Springer, 2016. 1\n[10] ¨Ozg¨un C¸ ic¸ek, Ahmed Abdulkadir, Soeren S Lienkamp,\nThomas Brox, and Olaf Ronneberger.\n3d u-net: learn-\ning dense volumetric segmentation from sparse annota-\ntion. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2016: 19th International Conference,\nAthens, Greece, October 17-21, 2016, Proceedings, Part II\n19, pages 424–432. Springer, 2016. 2\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE Conference on Computer Vision and\nPattern Recognition, pages 248–255, 2009. 1\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova.\nBert:\nPre-training of deep bidirectional\ntransformers for language understanding.\narXiv preprint\narXiv:1810.04805, 2018. 4\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 1\n[14] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim-\ning He.\nMasked autoencoders as spatiotemporal learners.\narXiv preprint arXiv:2205.09113, 2022. 3\n[15] Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin\nTallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch,\nBernardo Avila Pires, Zhaohan Guo, Mohammad Ghesh-\nlaghi Azar, et al. Bootstrap your own latent-a new approach\nto self-supervised learning. Advances in neural information\nprocessing systems, 33:21271–21284, 2020. 5\n[16] Stephanie A. Harmon, Thomas Sanford, Sheng Xu, Evrim B\nTurkbey, Holger R. Roth, Ziyue Xu, Dong Yang, An-\ndriy Myronenko, Victoria L. Anderson, Amel Amalou,\nMaxime Blain, Michael T Kassin, Dilara Long, Nicole Var-\nble, Stephanie M. Walker, Ulas Bagci, Anna Maria Ier-\nardi, Elvira Stellato, Guido Giovanni Plensich, Giuseppe\nFranceschelli, Cristiano Girlando, Giovanni Irmici, Dominic\nLabella, Dima A. Hammoud, Ashkan A. Malayeri, Eliz-\nabeth C. Jones, Ronald M. Summers, Peter L. Choyke,\nDaguang Xu, Mona G. Flores, Kaku Tamura, Hirofumi Obi-\nnata, Hitoshi Mori, F. Patella, Maurizio Cariati, Gianpaolo\nCarrafiello, Peng An, Bradford J. Wood, and Baris I Turkbey.\nArtificial intelligence for the detection of covid-19 pneumo-\nnia on chest ct using multinational datasets. Nature Commu-\nnications, 11, 2020. 3, 7\n[17] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong\nYang, Holger R Roth, and Daguang Xu. Swin unetr: Swin\ntransformers for semantic segmentation of brain tumors in\nmri images. In International MICCAI Brainlesion Workshop,\n2022. 1, 2, 3, 12, 13\n[18] Ali Hatamizadeh, Vishwesh Nath, Yucheng Tang, Dong\nYang, Holger R Roth, and Daguang Xu. Swin unetr: Swin\ntransformers for semantic segmentation of brain tumors in\nmri images.\nIn Brainlesion:\nGlioma, Multiple Sclero-\nsis, Stroke and Traumatic Brain Injuries: 7th International\n9\nWorkshop, BrainLes 2021, Held in Conjunction with MIC-\nCAI 2021, Virtual Event, September 27, 2021, Revised Se-\nlected Papers, Part I, pages 272–284. Springer, 2022. 6\n[19] Ali Hatamizadeh, Yucheng Tang, Vishwesh Nath, Dong\nYang, Andriy Myronenko, Bennett Landman, Holger R\nRoth, and Daguang Xu. Unetr: Transformers for 3d med-\nical image segmentation. In Proceedings of the IEEE/CVF\nwinter conference on applications of computer vision, pages\n574–584, 2022. 1, 2, 3, 12\n[20] Ali Hatamizadeh, Ziyue Xu, Dong Yang, Wenqi Li, Holger\nRoth, and Daguang Xu. Unetformer: A unified vision trans-\nformer model and pre-training framework for 3d medical im-\nage segmentation. arXiv preprint arXiv:2204.00631, 2022.\n1\n[21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr\nDoll´ar, and Ross Girshick. Masked autoencoders are scalable\nvision learners. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16000–\n16009, 2022. 2, 3, 8\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.\nDistill-\ning the knowledge in a neural network.\narXiv preprint\narXiv:1503.02531, 2015. 3\n[23] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Pe-\ntersen, and Klaus H Maier-Hein. nnu-net: a self-configuring\nmethod for deep learning-based biomedical image segmen-\ntation. Nature methods, 18(2):203–211, 2021. 2\n[24] Davood Karimi,\nSerge Didenko Vasylechko,\nand Ali\nGholipour.\nConvolution-free medical image segmentation\nusing transformers.\nIn Medical Image Computing and\nComputer Assisted Intervention–MICCAI 2021: 24th Inter-\nnational Conference, Strasbourg, France, September 27–\nOctober 1, 2021, Proceedings, Part I 24, pages 78–88.\nSpringer, 2021. 1, 3\n[25] Bennett Landman, Zhoubing Xu, J Igelsias, Martin Styner,\nT Langerak, and Arno Klein.\nMiccai multi-atlas labeling\nbeyond the cranial vault–workshop and challenge. In MIC-\nCAI Multi-Atlas Labeling Beyond Cranial Vault—Workshop\nChallenge, 2015. 2, 6, 12\n[26] Ziqiang Li, Hong Pan, Yaping Zhu, and A Kai Qin. Pgd-\nunet: a position-guided deformable network for simultane-\nous segmentation of organs and tumors. In 2020 Interna-\ntional Joint Conference on Neural Networks (IJCNN), pages\n1–8. IEEE, 2020. 3\n[27] Ailiang Lin, Bingzhi Chen, Jiayu Xu, Zheng Zhang, Guang-\nming Lu, and David Zhang. Ds-transunet: Dual swin trans-\nformer u-net for medical image segmentation. IEEE Trans-\nactions on Instrumentation and Measurement, 2022. 3\n[28] Geert Litjens, Thijs Kooi, Babak Ehteshami Bejnordi, Ar-\nnaud Arindra Adiyoso Setio, Francesco Ciompi, Mohsen\nGhafoorian, Jeroen A.W.M. van der Laak, Bram van Gin-\nneken, and Clara I. S´anchez. A survey on deep learning in\nmedical image analysis. Medical Image Analysis, 42:60–88,\n2017. 1\n[29] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 6\n[30] Bjoern H. Menze, Andras Jakab, Stefan Bauer, Jayashree\nKalpathy-Cramer, and et. al. The multimodal brain tumor\nimage segmentation benchmark (brats). IEEE Transactions\non Medical Imaging, 34(10):1993–2024, 2015. 2, 6, 13\n[31] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi.\nV-net: Fully convolutional neural networks for volumetric\nmedical image segmentation. In Fourth International Con-\nference on 3D Vision (3DV), 2016. 5\n[32] Chao Peng, Xiangyu Zhang, Gang Yu, Guiming Luo, and\nJian Sun. Large kernel matters–improve semantic segmen-\ntation by global convolutional network. In Proceedings of\nthe IEEE conference on computer vision and pattern recog-\nnition, pages 4353–4361, 2017. 3\n[33] Project-MONAI. Medical open network for ai. https:\n//github.com/Project-MONAI/MONAI, 2020. 6\n[34] Dian Qin, Jia-Jun Bu, Zhe Liu, Xin Shen, Sheng Zhou, Jing-\nJun Gu, Zhi-Hua Wang, Lei Wu, and Hui-Fen Dai.\nEffi-\ncient medical image segmentation based on knowledge dis-\ntillation. IEEE Transactions on Medical Imaging, 40(12):\n3820–3831, 2021. 3\n[35] Pierre H Richemond, Jean-Bastien Grill, Florent Altch´e,\nCorentin Tallec, Florian Strub, Andrew Brock, Samuel\nSmith, Soham De, Razvan Pascanu, Bilal Piot, et al.\nByol works even without batch statistics.\narXiv preprint\narXiv:2010.10241, 2020. 3, 4\n[36] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234–241. Springer, 2015. 1, 2\n[37] Arnaud Arindra Adiyoso Setio, Alberto Traverso, and et. al.\nValidation, comparison, and combination of algorithms for\nautomatic detection of pulmonary nodules in computed to-\nmography images: The luna16 challenge. Medical Image\nAnalysis, 42:1–13, 2017. 3, 8\n[38] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.\nVideomae: Masked autoencoders are data-efficient learn-\ners for self-supervised video pre-training.\narXiv preprint\narXiv:2203.12602, 2022. 3\n[39] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu,\nand Vishal M Patel.\nMedical transformer: Gated axial-\nattention for medical image segmentation. In Medical Image\nComputing and Computer Assisted Intervention–MICCAI\n2021: 24th International Conference, Strasbourg, France,\nSeptember 27–October 1, 2021, Proceedings, Part I 24,\npages 36–46. Springer, 2021. 3\n[40] Yutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unified\n2d and 3d pre-training for medical image classification and\nsegmentation. arXiv preprint arXiv:2112.09356, 2021. 1\n[41] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin\nBao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple\nframework for masked image modeling. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9653–9663, 2022. 2, 3, 8\n[42] Yundong Zhang, Huiye Liu, and Qiang Hu.\nTransfuse:\nFusing transformers and cnns for medical image segmenta-\ntion. In Medical Image Computing and Computer Assisted\nIntervention–MICCAI 2021: 24th International Conference,\n10\nStrasbourg, France, September 27–October 1, 2021, Pro-\nceedings, Part I 24, pages 14–24. Springer, 2021. 3\n[43] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang\nWang, and Jiaya Jia. Pyramid scene parsing network. In\nProceedings of the IEEE conference on computer vision and\npattern recognition, pages 2881–2890, 2017. 3\n[44] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Lequan Yu,\nLiansheng Wang, and Yizhou Yu.\nnnformer: Interleaved\ntransformer for volumetric segmentation.\narXiv preprint\narXiv:2109.03201, 2021. 1, 2, 3, 6, 12, 13\n[45] Hong-Yu Zhou, Chixiang Lu, Chaoqi Chen, Sibei Yang, and\nYizhou Yu. Pcrlv2: A unified visual information preserva-\ntion framework for self-supervised pre-training in medical\nimage analysis. arXiv preprint arXiv:2301.00772, 2023. 2,\n3, 8\n11\nSupplementary Material\nIn this section we discuss the Psuedocode of our Med-\nContext Algorithm in Appendix A, provide additional de-\ntails about the datasets in Appendix B, finally, in Appendix\nC, we provide some qualitative visualizations to show the\nsuperiority of our MedContext for effective 3D medical seg-\nmentation.\nA. MedContext Algorithm\nIn section 3.1 of the main paper, we describe our MedCon-\ntext algorithm which comprises a student network (Fs) and\na teacher network (Ft). The student-teacher framework pro-\ncesses the input volume X ∈ RH×W ×D and its masked\nversion XM ∈ RH×W ×D created using the masking func-\ntion g(.). During training, the student-teacher framework\ngenerates voxel-wise semantic logits for each input view.\nThe student network receives both the masked (XM) and\nunmasked (X) inputs, and outputs voxel-wise semantic log-\nits represented by Fs and F M\ns . The teacher network re-\nceives only the original unmasked input X and generates\nvoxel-wise semantic logits denoted by Ft. We calculate a\nsupervised loss using the ground truth label Y for each out-\nput prediction from student. Additionally, we minimize a\nself-supervised objective between the masked student logits\nF M\ns\nand the teacher logits Ft. The hyperparameter β con-\ntrols the contribution of self-supervised loss to the overall\nobjective function. Our single-stage training process jointly\noptimizes both the supervised and self-supervised objec-\ntives. At the inference stage, we use the weights of the stu-\ndent network to output the corresponding voxel-wise pre-\ndictions. We provide the Pseudo-code of our MedContext\napproach in Algorithms 1 and 2.\nB. Additional Dataset details\nWe use three medical segmnetation datasets in our paper.\nThe additional details of each dataset along with the pre-\nprocessing settings are given below:\nSynapse BTCV Multi-organ Dataset [25]: The BTCV\ndataset, known as Synapse for Multi-organ CT Segmenta-\ntion, is derived from the MICCAI Multi-Atlas Labeling Be-\nyond the Cranial Vault challenge. It includes abdominal CT\nscans of 30 subjects encompassing 8 organs. The dataset\nis expertly annotated under the supervision of clinical ra-\ndiologists at Vanderbilt University Medical Center. Each\nscan is captured using contrast enhancement in the portal\nvenous phase and contains between 80 to 225 slices with\n512×512 pixels. The thickness of each slice varies from 1\nto 6 mm. Following previous methods, we adopt the same\ndataset split as used in [5] with 18 train samples and test\non the remaining 12 cases. We evaluate the performance\non eight abdominal organs (i.e., spleen, right kidney, left\nAlgorithm 1 MedContext: Training\nInput: Dataset D, student Fθs, teacher Fθt, masking function\ng(.) with masking ratio α, Step = 0.\nRequire: Initialize teacher weights θt with the student weights\nθs. Network momentum rate m follows a cosine schedule from\n0.996 to 1. Soft Dice loss LDice−CE.\nrepeat\nStep ← Step + 1\n// Sample data.\nsample {X, Y } ⊆ D, XM ← g(X, α)\n// student and teacher outputs.\nFs , F M\ns\n= Fθs(X) , Fθs(XM)\nFt = Fθt(X)\n// supervised objective\nL(Y, Fs) ← LDice−CE(Y, Fs)\n// masked reconstruction objectives.\nL(Y, F M\ns ) ← LDice−CE(Y, F M\ns )\nLc(Ft, F M\ns ) ←\n∥ F M\ns\n−Ft∥ 2\n2\n∥ Ft∥ 2\n2\n// Combined Loss.\nL ← L(Y, Fs) + L(Y, F M\ns ) + βLc(Ft, F M\ns )\n// update student with combined loss.\nθs ← θs − δ∇θs(L)\n// update teacher weights by EMA.\nθt ← mθt − (1 − m)θs\nuntil converge\nAlgorithm 2 Inference\nInput: Test Dataset D′, student network Fθs initialized with\nlearned paramaters θs, Step = 0.\nRequire: Evaluation metric E.\nrepeat\nStep ← Step + 1\n// Sample data.\nsample {X, Y } ⊆ D′\n// model prediction.\nF = Fθs(X)\n// calculate evaluation metric.\nEvaluate ← E(Y, F)\nuntil go through all test data\nkidney, gallbladder, liver, stomach, aorta, and pancreas) by\nmeasuring the Dice Similarity Coefficient (DSC) and 95%\nHausdorff Distance (HD95). In all the cases, the intensities\nof input volumes are normalized from the range of [-1000,\n1000] to [0,1] Hounsfield Units (HU). For UNETR [19] and\nSwinUNETR [17], we follow their respective data process-\ning pipeline where each volume is pre-processed indepen-\ndently and resampled to have an isotropic voxel spacing of\n[1.5, 1.5, 2.0]. Input is sampled at a crop size of 96 x 96\nx 96. For nnFormer [44], we use their data preprocessing\npipeline in which each CT scan is independently processed\nby applying patch cropping to sample the input at a resolu-\ntion 128 × 128 × 64 with a spacing of [0.76, 0.76, 3].\n12\nACDC Dataset [2]: The ACDC dataset is a collection\nof cardiac MRI images and associated segmentation an-\nnotations for the right ventricle (RV), left ventricle (LV),\nand myocardium (MYO) of 100 patients, obtained from ac-\ntual clinical exams. The dataset includes patients with var-\nious heart conditions, including normal patients, patients\nwith a myocardial infarction, dilated cardiomyopathy, hy-\npertrophic cardiomyopathy, and abnormal right ventricle.\nFor UNETR and SwinUNETR,[17], we split the dataset into\n80 training and 20 testing samples following [44]. The input\nis sampled at a resolution 96 x 96 x 96 with a voxel spacing\nof [1.5, 1.5, 2.0]. For nnFormer [44], we follow their re-\nspective data processing pipeline and sample the input at a\nresolution 128 x 128 x 64 and report the results on all three\nclasses using Dice similarity coefficient (DSC).\nBraTS Dataset [1, 30]: We use two versions of BraTS\ndataset: BraTS17 [30] and BraTS21 [1]. For UNETR and\nSwinUNTER we report results on the BraTS21 dataset to be\nconsistent with the baseline settings. The BraTS21 dataset\n[1] is from the BraTS challenge which provides a large\ndataset of 3D MRI scans, with voxel-wise ground truth la-\nbels annotated by clinicians. The dataset includes 1251 sub-\njects, each with four 3D MRI modalities: native T1, post-\ncontrast T1-weighted (T1Gd), T2-weighted (T2), and T2\nFluid-attenuated Inversion Recovery (T2-FLAIR). The im-\nages have been rigidly aligned, resampled to a 1 x 1 x 1 mm\nisotropic resolution, and skull-stripped, resulting in an input\nimage size of 240 x 240 x 155. The dataset includes anno-\ntations for three tumor sub-regions: the enhancing tumor,\nthe peritumoral edema, and the necrotic and non-enhancing\ntumor core. The annotations have been combined into three\nnested sub-regions: Whole Tumor (WT), Tumor Core (TC),\nand Enhancing Tumor (ET). Following the data split used\nby [17], we train on 1000 subjects and test on 251 subjects.\nThe input is cropped to a size 96 x 96 x 96 for training and\nDice Similarity score (DSC) is used as an evaluation met-\nric. For nnFormer, we use BraTS17 dataset [30]. The task\ncomprises of 484 MRI images, each having four channels\n- FLAIR, T1w, T1gd and T2w. These images are obtained\nfrom 19 different institutions and represent a subset of the\ndata used in the 2016 and 2017 Brain Tumor Segmentation\n(BraTS) challenges. The objective was to identify the three\ntumor sub-regions: edema (ED), enhancing tumor (ET), and\nnon-enhancing tumor (NET).\nC. Qualitative Comparisons\nIn Figure 5, we present a comparison between our MedCon-\ntext approach and the baseline method on the synapse multi-\norgan dataset using the UNETR architecture. The results\nindicate that our approach produces more accurate segmen-\ntations than the baseline. As for instance, in the first row\nof Figure 5, the baseline (first row, third column) wrongly\nsegments the right kidney (dark green) as left kidney (dark\nblue). Our approach on the other hand segments both the\nkidneys correctly (first row, fourth column).\nIn the sec-\nond row of the figure, for example, the baseline struggles\nto segment the pancreas, but when combined with our Med-\nContext approach, the model correctly segments the organ.\nSimilarly, in the third row, our approach correctly segments\nthe right adrenal gland, which the baseline model almost\nwrongly segments as spleen.\nSimilarly, Figure 6 illustrates the outcomes of integrat-\ning our MedContext technique with UNETR on the ACDC\ndataset. Our method produces segmentation boundaries that\nare more precise and well-defined than the baseline UNETR\nmethod. The baseline’s segmentation boundary of the My-\nocardium (shown in green) in the first row’s third column\nis not accurate when compared to the ground truth segmen-\ntation boundary. This trend is also observed in the second\nrow’s third column where the baseline fails to segment the\nMyocardium efficiently. Additionally, in the third row, the\nbaseline fails to identify the right ventricular cavity. In con-\ntrast, our MedContext outperforms the baseline in all the\ncases and efficiently segments all the organs.\n13\nSpleen\nR-Kid\nL-Kid\nGal\nEso\nLiv\nSto\nAor\nICV\nPSV\nPan\nRad.\nLad.\nFigure 5. Qualitative comparison on multi-organ synapse dataset: We showcase the benefit of our MedContext framework implemented\non the UNETR architecture. The examples display various abdominal organs, with their corresponding labels in the legend below. The\nexisting baseline method struggles to accurately segment the organs as can be seen from the red boxes. Best viewed in zoom.\nRV cavity\nMyocaridum\nLV cavity\nFigure 6. Qualitative comparison on ACDC dataset using UNETR: We showcase the benefit of our MedContext framework integrated with\nUNETR architecture on ACDC dataset. The examples display three heart regions with their corresponding labels in the legend below. The\nbaseline UNETR struggles to accurately segment the organs as can be seen from the red boxes. Our approach on the other hand produces\ncorrect and sharp segmentation boundaries. Best viewed in zoom.\n14\n"
}