{
    "optim": "Rose: Efficient and Extensible Autodiff on the Web SAM ESTEP, Carnegie Mellon University, USA RAVEN ROTHKOPF, Barnard College, Columbia University, USA WODE NI, Carnegie Mellon University, USA JOSHUA SUNSHINE, Carnegie Mellon University, USA Automatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains such as computer graphics and machine learning over the past decade. However, existing AD systems face limitations, either lacking support for in-browser development or failing to harness more recent, compiler- based approaches to achieve both expressiveness and size-preserving differentiation. This work introduces Rose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions with custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two differentiable simulations and a diagram authoring tool to demonstrate the utility of Rose’s design. Finally, we show that Rose is 173× as fast as TensorFlow.js in compiling and running a benchmark suite of optimized diagrams. CCS Concepts: • Software and its engineering → Domain specific languages; Compilers. Additional Key Words and Phrases: automatic differentiation, compilers, web 1 INTRODUCTION Automatic differentiation (autodiff or AD) is a widely used technique for efficiently computing derivatives of any program that computes a numerical value. The computed derivatives can be used to determine how small perturbations in a program’s inputs can effect its outputs. Gradient-based optimization uses these derivatives to determine which direction to move to optimize an objective function. Derivatives of  functions  JavaScript  functions Fig. 1. We introduce Rose, an automatic differentia- tion engine on the web. Rose takes derivatives of func- tions defined using JavaScript, and compiles to We- bAssembly which the browser can efficiently execute. Significant advancements in AD algorithms have resulted in systems like PyTorch [Paszke et al. 2017], TensorFlow [Abadi et al. 2015], and JAX [Bradbury et al. 2018; Frostig et al. 2018]. These frameworks provide high-level APIs for computing gradients of functions, making it easier for users to leverage AD in their work. The computation model of each of these pop- ular machine learning frameworks is centered around the tensor: you must structure your pro- gram as a composition of operations on ten- sors. However, many programs are expressed using mathematical operations on scalars; we call these pointful programs. In addition, none of these tools can run in a web browser. Web appli- cations require no installation or central main- tenance and can reduce security risks. When teaching students with a simulator, for instance, it is advantageous to deploy instantly to every Authors’ addresses: Sam Estep, estep@cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania, USA, 15213; Raven Rothkopf, rgr2124@barnard.edu, Barnard College, Columbia University, 3009 Broadway, New York, New York, USA, 10027; Wode Ni, nimo@cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania, USA, 15213; Joshua Sunshine, sunshine@cs.cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania, USA, 15213. arXiv:2402.17743v1  [cs.PL]  27 Feb 2024 2 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine student’s web browser and to avoid exposing servers to potentially insecure code the students write. In this paper we present Rose,1 an extensible efficient pointful autodiff framework for the web. Rose supports an important class of applications, like educational simulators and customizable opti- mization engines that are not well supported by popular AD frameworks. This class of applications leads to the following design goals for Rose: D1 Programs written with Rose run in a web browser. D2 AD functions can be partially composed of opaque functions with custom derivatives. D3 AD functions can be written manually in a pointful style. D4 AD functions can be constructed dynamically, for example based on user input. D5 Rose programs should compile quickly. D6 Rose programs should run efficiently. The primary contribution of this paper is the design of the Rose system. Rose combines advanced research in a novel way to allow a neglected class of applications to flourish. Rose builds on the theoretical foundations [Radul et al. 2023] provided by JAX and Dex [Paszke et al. 2021]. Rose is written in Rust and TypeScript and available for anyone to use through npm. Rose uses JavaScript as a metaprogramming environment so programmers can write custom AD functions and generate them at runtime. Rose compiles to WebAssembly [Haas et al. 2017] for performance. We holistically evaluate Rose against its design goals. We ported two DiffTaichi [Hu et al. 2019] simulations to Rose and run them in the browser. We make use of custom opaque functions and write AD functions in a pointful style. We also integrated Rose with Penrose [Ye et al. 2020], a web-based diagramming tool. Penrose allows users to specify a diagram by constructing a custom numerical optimization problem, then runs a numerical solver to rearrange the shapes in the diagram until it finds a local minimum. Penrose previously used TensorFlow.js to compute derivatives. The median speedup of Penrose with Rose is 173× compared to Penrose with TensorFlow.js. The rest of this paper is structured as follows. In Section 2 we give general background information about autodiff. In Section 3 we walk through a detailed example Rose program illustrating many different features. In Section 4 we describe the Rose IR, over which we define our autodiff program transformations. In Section 5 we discuss practical challenges that arise on top of the theoretical ones, along with design decisions that place Rose more concretely in the space of possible solutions. In Section 6 we lay out the results of experiments we have conducted to show that we achieved the design goals for Rose as we described above. In Section 7 we discuss related work. Finally we discuss limitations and future work in Section 8 before concluding in Section 9. 2 BACKGROUND Autodiff comes in two forms: forward-mode and reverse-mode. Reverse-mode is used to compute gradients which inform optimization. Forward-mode is easier to specify and compute. Rose trans- poses automatically from forward-mode to reverse-mode. These correspond to two different kinds of derivatives, the Jacobian-vector product (JVP) and vector-Jacobian product (VJP), respectively. In this section we define these two concepts. We largely follow the presentation from Radul et al. [2023, section 2.1], with a couple tweaks such as introducing the dual numbers which Rose uses to allow definitions of custom derivatives. 2.1 Jacobian-vector product The Jacobian-vector product allows one to ask, how the output to 𝑓 would change if the input 𝑥 (the primal value) was perturbed in the direction 𝑣 (the tangent)? 1Not to be confused with the ROSE (all caps) compiler infrastructure: http://rosecompiler.org/ Rose: Efficient and Extensible Autodiff on the Web 3 First, suppose we have a function 𝑓 : R𝑛 → R𝑚. We define the Jacobian of 𝑓 at 𝑥 ∈ R𝑛 to be the linear map 𝜕𝑓 (𝑥) : R𝑛 → R𝑚 such that for 𝑣 ∈ R𝑛, 𝑓 (𝑥 + 𝑣) = 𝑓 (𝑥) + 𝜕𝑓 (𝑥)(𝑣) + 𝑜(∥𝑣∥). The JVP for 𝑓 is the mapping (𝑥, 𝑣) ↦→ (𝑓 (𝑥), 𝜕𝑓 (𝑥)(𝑣)). Note that this definition is compositional, so the JVP of 𝑓 ◦ 𝑔 is the same as the JVP of 𝑓 composed with the JVP of 𝑔. In Rose, we use a slightly different definition based on the dual numbers [Clifford 1871], that is, the commutative algebra D = {𝑎 + 𝑏𝜀 | 𝑎,𝑏 ∈ R} where𝜀2 = 0. Usefully, arithmetic operations on D correspond directly to JVPs of their corresponding operations on R. For instance, the JVP of (𝑥,𝑦) ↦→ 𝑥 + 𝑦 for 𝑥,𝑦 ∈ R is ((𝑥,𝑦), (𝑢, 𝑣)) ↦→ (𝑥 + 𝑦,𝑢 + 𝑣), but also the sum of two dual numbers is (𝑥 + 𝑢𝜀) + (𝑦 + 𝑣𝜀) = (𝑥 + 𝑦) + (𝑢 + 𝑣)𝜀. Next observe that the JVP (𝑥,𝑦) ↦→ 𝑥𝑦 is just the product rule ((𝑥,𝑦), (𝑢, 𝑣)) ↦→ (𝑥𝑦,𝑥𝑣 + 𝑦𝑢), and so is the product of two dual numbers (𝑥 + 𝑢𝜀)(𝑦 + 𝑣𝜀) = 𝑥𝑦 + (𝑥𝑣 + 𝑦𝑢)𝜀 because 𝑢𝑣𝜀2 = 0. There is a similar correspondence between the quotient rule and the division operation on dual numbers. Thus, instead of defining the type of the JVP of 𝑓 to be R𝑛 × R𝑛 → R𝑚 × R𝑚, we define its type to be D𝑛 → D𝑚, like this: 𝑥 + 𝑣𝜀 ↦→ 𝑓 (𝑥) + 𝜕𝑓 (𝑥)(𝑣) 𝜀 We use dual numbers to represent the primal and tangent together, as we’ll discuss in Section 3. 2.2 Vector-Jacobian product While the JVP deals with vectors (perturbations to the input or output), the VJP deals with covectors, that is, linear maps from R𝑛 or R𝑚 to R. Any covector on a finite-dimensional real vector space can be uniquely characterized as the inner product with some fixed vector. Intuitively, the vector points in the direction that maximizes the output when the covector is applied. The VJP of 𝑓 at 𝑥 ∈ R𝑛 takes the dual of 𝑢 ∈ R𝑚 and returns the dual of 𝑤 ∈ R𝑛 such that for all 𝑣 ∈ R𝑛, 𝑢 · 𝜕𝑓 (𝑥) = 𝑤 · 𝑢. This allows one to ask, given a covector on small perturbations to the output, what is the corre- sponding covector on small perturbations to the input? The mapping from 𝑢 to 𝑤 is linear, letting us define a new linear map 𝜕𝑓 (𝑥)𝑇 : R𝑚 → R𝑛 by 𝑢 · 𝜕𝑓 (𝑥)(𝑣) = 𝜕𝑓 (𝑥)𝑇 (𝑢) · 𝑣. So, more specifically, the VJP for 𝑓 is the mapping 𝑥 ↦→ (𝑓 (𝑥),𝑢 ↦→ 𝜕𝑓 (𝑥)𝑇 (𝑢)). 4 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine Similar to the JVP, the VJP is also compositional, although not quite as directly: for 𝑓 ◦ 𝑔, we need to compose the linear functions returned by the VJP in reverse-order as 𝜕𝑔(𝑥)𝑇 ◦ 𝜕𝑓 (𝑔(𝑥))𝑇 , which is why this is called “reverse-mode” autodiff. The VJP is useful for optimization because in that setting we usually need the gradient of 𝑓 : R𝑛 → R, and usefully, this is simply ∇𝑓 (𝑥) = 𝜕𝑓 (𝑥)𝑇 (1) ∈ R𝑛. As we will see, autodiff shows us that the time to compute the VJP of 𝑓 is linear with respect to the time to compute 𝑓 itself, so we can compute gradients efficiently. In contrast, even though the time to compute the JVP of 𝑓 is clearly linear with respect to the time to compute 𝑓 , we would need to evaluate the JVP 𝑛 times to compute the gradient, ruining the asymptotic complexity; this is what often makes reverse-mode autodiff more useful than forward-mode in practice. Note that we cannot use the dual numbers for the VJP like we did for the JVP, because the input and output spaces are different: there is no way to “pair up” components of R𝑛 with components of R𝑚. 2.3 Hessian We can treat a JVP or VJP itself as a function between two real vector spaces, allowing us to take higher derivatives. The most common is the Hessian of a function 𝑓 : R𝑛 → R, which is a matrix in R𝑛×𝑛 of all the second-order partial derivatives of 𝑓 at a point 𝑥 ∈ R𝑛. If we use the VJP to define 𝑔 : R𝑛 → R𝑛 by 𝑔(𝑥) = ∇𝑓 (𝑥) = 𝜕𝑓 (𝑥)𝑇 (1), then we can construct the 𝑖th row of the Hessian H𝑓 : R𝑛 → R𝑛×𝑛 by taking either the JVP or the VJP of 𝑔: (H𝑓 (𝑥))𝑖 = 𝜕𝑔(𝑥)(e𝑖) = 𝜕𝑔(𝑥)𝑇 (e𝑖) 3 EXAMPLE Fig. 2. An interactive demonstra- tion of local quadratic approxi- mation, built with Rose and run- ning on an iPhone in Safari. Figure 2 shows an interactive widget on the Rose project web- site2 displaying the local quadratic approximation to the function (𝑥,𝑦) ↦→ 𝑥𝑦, allowing a user to drag the point around to see how the shape of the local quadratic approximation shifts. The page also allows the user to modify the mathematical expression defining the function, causing Rose to immediately re-derive the gradient and Hessian, and compile the new function to WebAssembly. For brevity we omit the code to generate the user interface, and instead focus on how one would use Rose to calculate the first and second derivatives used to visualize the quadratic approximation. 3.1 Using Rose Listing 1 shows a Rose program calculating the value, gradient, and Hessian of the power function at the point (2, 3). This is all just JavaScript code: from the user perspective, Rose is simply an embedded domain-specific language (eDSL) inside of JavaScript/- TypeScript. Line 1 uses a standard JavaScript import statement to pull in definitions of types and higher-order functions from Rose. Line 2 imports the power function itself, which is already defined in a library of basic math functions. For completeness, we will discuss the full definition of pow in Section 3.2. Lines 4 and 5 define type aliases for R2 and R2×2, respectively. Rose types are simply JavaScript values, so type aliases are defined using const in the same way as any other JavaScript value. 2See https://rosejs.dev/. Rose: Efficient and Extensible Autodiff on the Web 5 1 import { Real, Vec, compile, fn, vjp } from \"rose\"; 2 import { pow } from \"./pow.js\"; 3 4 const Vec2 = Vec(2, Real); 5 const Mat2 = Vec(2, Vec2); 6 7 const f = fn([Vec2], Real, ([x, y]) => pow(x, y)); 8 const g = fn([Vec2], Vec2, (v) => vjp(f)(v).grad(1)); 9 const h = fn([Vec2], Mat2, (v) => { 10 const { grad } = vjp(g)(v); 11 return [grad([1, 0]), grad([0, 1])]; 12 }); 13 14 const all = fn( 15 [Real, Real], 16 { val: Real, grad: Vec2, hess: Mat2 }, 17 (x, y) => { 18 const v = [x, y]; 19 return { val: f(v), grad: g(v), hess: h(v) }; 20 }, 21 ); 22 23 const compiled = await compile(all); 24 console.log(compiled(2, 3)); Listing 1. An example Rose program. Remember that the vector-Jacobian product (VJP) introduced in Section 2 swaps the domain and codomain from the original function. In addition, JavaScript only allows functions to return one argument. Therefore a Rose function of which you take the VJP must only have one parameter. So, line 7 wraps the pow function to take a single vector argument rather than two scalar arguments, allowing it to be passed to Rose’s vjp function. Just as we discussed in Section 2.2, we compute the gradient by passing in a value of 1. Lines 9 to 12 then use the gradient g of f to compute its Hessian by differentiating once more. Line 10 runs the forward pass for the Hessian just once and saves all necessary intermediate values, after which line 11 runs the backward pass twice with the two basis vectors to compute the full Hessian matrix. Lines 14 to 21 wrap these three functions into a single function that calls all three and returns the results in a structured form. Finally, line 23 compiles that function to WebAssembly, and line 24 calls it at the point (2, 3). 3.2 Custom functions Rose allows users to define custom derivatives. Listing 2 defines the pow and log functions to demonstrate this feature. In practice, these basic functions would be provided by a library, but many real world examples require custom derivatives. Line 3 defines the natural logarithm as an opaque function that calls JavaScript’s builtin Math.log function. Because Rose cannot see the definition of this function, it must be given a definition for its derivative. Lines 4 to 6 define the logarithm’s 6 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine 1 import { Dual, Real, add, div, mul, fn, opaque } from \"rose\"; 2 3 const log = opaque([Real], Real, Math.log); 4 log.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => { 5 return { re: log(x), du: div(dx, x) }; 6 }); 7 8 export const pow = opaque([Real, Real], Real, Math.pow); 9 pow.jvp = fn( 10 [Dual, Dual], 11 Dual, 12 ({ re: x, du: dx }, { re: y, du: dy }) => { 13 const z = pow(x, y); 14 const dw = add(mul(dx, div(y, x)), mul(dy, log(x))); 15 return { re: z, du: mul(dw, z) }; 16 }, 17 ); Listing 2. The contents of pow.js defining a differentiable power function. 𝑚,𝑛 ∈ Z≥0 𝑐 ∈ R 𝜅 ::= Type | Value | Index 𝜏 ::= 𝑡 | Bool | Real | 𝑛 | &𝜏 | [𝜏]𝜏 | (𝜏) ⊖ ::= ¬ | − | abs | sgn | ceil | floor | trunc | sqrt ⊕ ::= ∧ | ∨ | iff | xor | ≠ | < | ≤ | = | > | ≥ | + | − | × | ÷ 𝑒 ::= () | true | false | 𝑐 | 𝑛 | [𝑥] | (𝑥, 𝑥) | ⊖𝑥 | 𝑥 ⊕ 𝑥 | 𝑥 ? 𝑥 : 𝑥 | 𝑥 += 𝑥 | 𝑥[𝑥] | fst 𝑥 | snd 𝑥 | &𝑥[𝑥] | &fst 𝑥 | &snd 𝑥 | 𝑓 <𝜏>(𝑥) | [for 𝑥: 𝜏, 𝑏] | accum 𝑥 from 𝑥 in 𝑏 𝑏 ::= 𝑥 | let 𝑥: 𝜏 = 𝑒 in 𝑏 𝑑 ::= def 𝑓 <𝑡: 𝜅>(𝑥: 𝜏): 𝜏 = 𝑏 Fig. 3. Abstract syntax for Rose. Jacobian-vector product. Specifically, the signature of this function takes the original log function and maps every instance of the Real numbers to become the Dual numbers we introduced in Section 2.1. In this case, the returned tangent is given by the familiar rule d d𝑥 ln𝑥 = 1 𝑥 from calculus. Similarly, lines 8 to 17 define the power function along with its derivative. Note that, while these two functions use opaque to define their bodies, they define their derivatives via fn, the same as the Rose functions we discussed in Section 3.1. This means that only the first forward derivative needs to be provided. Since the body of this first derivative is transparent to Rose, the reverse derivative and any higher derivatives can be computed automatically. 4 ROSE INTERMEDIATE REPRESENTATION (IR) We’ve seen what it looks like to use Rose; let’s now describe how it works. When a user calls Rose functions from their JavaScript program, we construct values of a data structure which we call Rose: Efficient and Extensible Autodiff on the Web 7 the Rose intermediate representation (IR). This IR is what Rose uses to compute forward-mode derivatives and transpose from forward-mode to reverse-mode. Figure 3 shows the Rose IR’s abstract syntax.3 In this section, we walk through the IR’s semantics by way of examples. Consider this function that computes the sum of all elements from an array: 1 def sum <n: Index >(v: [n]Real): Real = 2 let z: Real = 0.0 in 3 let t: (Real , [n]()) = 4 accum a from z in 5 [for i: n, 6 let x: Real = v[i] in 7 let u: () = a += x in 8 u 9 ] 10 in 11 let y: Real = fst t in 12 y Line 1 says that the function is generic over the size of the array, where the size is represented as a type n with the Index constraint. Line 4 uses the accum keyword to introduce the variable a of type &Real, in a new scope that lasts through its body ending on line 9. The ampersand here means that the type &Real denotes a mutable reference to a value of type Real, the same type as whatever was written after the from keyword (z, in this case). Then, the accum body is an array constructor with index type n, as shown on line 5. The value type of this array constructor is the unit type () so its resulting array type [n]() holds no data; its sole importance comes from the side effect it performs on line 7. Regardless, the accum construct always returns both the final value contained in its reference (a, in this case) and the value of its body, together, as a pair. Line 3 binds this pair to the variable t, after which lines 11 and 12 extract and return the desired value. That function definition was quite verbose, as it strictly adhered to the syntax from Fig. 3 for clarity. In the remainder of this section, we will allow ourselves syntactic sugar to write expressions in places where the strict syntax requires variable names, with the understanding that these could be desugared by introducing intermediate let bindings: def sum <n: Index >(v: [n]Real): Real = fst (accum a from 0.0 in [for i: n, a += v[i]]) 4.1 Autodiff To make it easier for users to specify custom derivatives (see Section 5), we build on theoretical work from Radul et al. [2023] decomposing reverse-mode autodiff into three transformations: forward-mode autodiff, unzipping, and transposition. We combine unzipping and transposition together, so we have only the two transformations. We will use the same running example from the aforementioned paper: 3Note that this is not what the user sees, because actual Rose programs are constructed using an embedded DSL inside JavaScript as a metaprogramming environment, as described in Sections 3 and 5. To make the distinction more clear, we use black-and-white formatting for Rose IR, in contrast to the colored syntax highlighting we use for JavaScript code. 8 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine def f(u: Real): Real = let v: Real = sin(u) in let w: Real = -v in w To implement forward-mode autodiff, we first introduce a new type4 Tan which is identical to Real except for how it will later be treated by transposition in Section 4.2. Then we define a Dual number type to be an alias for the type (Real, Tan). We’ll notate type aliases with this syntax: type Tan = Real type Dual = (Real , Tan) We assume that we are already given a JVP for sin. For instance: def jvp_sin ((x, dx): Dual): Dual = (sin(x), dx * cos(x)) Then, the forward-mode derivative of f is: def jvp_f(u: Dual): Dual = let v: Dual = jvp_sin(u) in let v_re: Real = fst v in let v_du: Tan = snd v in let w_re: Real = -v_re in let w_du: Tan = -v_du in let w: Dual = (w_re , w_du) in w To construct jvp_f, we first replaced all instance of the Real type with Dual. Then we walked through f’s instructions in order and mechanically replaced each with one or more instructions according to a small set of builtin templates for the basic arithmetic operations. For instance, the template for a function call is to replace the function with its JVP, and the template for negation is to extract the fst and snd components of the dual number, negate each, and repackage them as a new Dual number. 4.2 Transposition Transposition is more complicated. First we must transpose jvp_sin: type Tape_sin = (Real) def fwd_sin(x: Real): (Real , Tape_sin) = (sin(x), (cos(x))) def bwd_sin(dx: &Real , dy: Real , (z): Tape_sin ): () = dx += dy * z As shown here, transposition splits a JVP into two functions (a “nonlinear” forward pass and a “linear” backward pass) where the backward pass is effectively reversed (hence the name). Recall that the fst component of each Dual number is of type Real; we call this the “nonlinear” part. The snd component, of type Tan, is called the “linear” part. For the forward pass, we first replace all instances of the Dual type with the Real type, but we also create a new type for the tape; that 4We don’t list this type in our abstract syntax because it can be erased once autodiff is done; it is only needed at the boundary between forward-mode autodiff and transposition. Rose: Efficient and Extensible Autodiff on the Web 9 is, a collection of nonlinear intermediate values we compute along the way, which we will need later when we compute the backward pass. Then we augment the return type to include both the original return type as well as the tape; in this case, the only value we need to store for the tape is cos(x). For the backward pass, we first must again map Dual to Real, and then we must “flip” the type of every parameter: value types become reference types, and vice versa. The jvp_sin function had only one parameter, which here translates to one parameter dx: &Real. Then we add two more parameters: one corresponding to the derivative on the output which we will propagate backward, and one corresponding to the tape. We use the tape to retrieve any intermediate values we saved in the forward pass, such as z here which is the cosine of the original nonlinear argument. Then we walk through the function body in reverse, flipping the directionality of every linear expression we see. Here we only had one linear expression dx * cos(x) which gets translated to dx += dy * z. Then, we can use the same process to transpose jvp_f: type Tape_f = (Tape_sin) def fwd_f(u: Real): (Real , Tape_f) = let (v, t): (Real , Tape_sin) = fwd_sin(u) in let w: Real = -v in (w, (t)) def bwd_f(du: &Real , dw: Real , (t): Tape_f ): () = let dv: Real = -dw in bwd_sin(du, dv, t) Here the body of jvp_f called jvp_sin, so when we transpose, we split that call into two separate calls to fwd_sin and bwd_sin. This example also shows how the backward pass runs “in reverse”: in the forward pass, we call fwd_sin before doing negation, then in the backward pass, we do negation first and then call bwd_sin. In the next section we talk about our implementation, which exposes operations to construct programs in this IR, as well as the program transformations for differentiation and transposition that we discussed here. 5 IMPLEMENTATION The Rose implementation consists of a core written in Rust (which defines data structures for the IR, as well as program transformations for autodiff, and a backend to emit WebAssembly) and a binding layer in JavaScript and TypeScript (which is used by all the JavaScript code examples in this paper). Rose maintains a garbage-collected DAG of functions, where an edge in the DAG means that one function’s body contains a call to another function. To construct a Rose function, a user uses fn from the Rose JavaScript bindings, which introduces a new context with an empty function body of Rose IR. As the user calls Rose arithmetic operations like add or sub, Rose appends those instructions to the body of the function in the current context. Then, once the user exits the scope defining their function, Rose finalizes the body and adds the new function to the graph. We compile the Rose core to WebAssembly [Haas et al. 2017], and use wasm-bindgen to expose to JavaScript in the npm package rose.5 As of rose version 0.4.5 (the latest at time of writing), our Wasm binary size [Ayers et al. 2022] is 164.09 kB (63.31 kB after gzip), and our JavaScript wrapper 5https://www.npmjs.com/package/rose 10 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine import { Real, add, compile, fn, jvp } from \"rose\"; let f = fn([Real], Real, (x) => x); for (let i = 0; i < 20; ++i) { f = fn([Real], Real, (x) => add(f(x), f(x))); } const g = await compile(jvp(f)); console.log(g({ re: 2, du: 3 })); Listing 3. A tower of functions where inlining would explode program size exponentially. layer is 29.66 kB after minification (8.31 kB after gzip). For comparison, @tensorflow/tfjs-core version 4.13.0 (again, latest at time of writing) is 478.00 kB after minification (85.32 kB after gzip). In a web setting, having a relatively small binary like this is crucial; for example, there are projects that package heavy compiler infrastructure like LLVM to WebAssembly [Soedirgo 2023], but those produce binaries on the order of a hundred megabytes, causing unacceptable load times for end users. We faced several important design questions while implementing Rose, which go beyond the theoretical concerns described in Section 4: • How should users define Rose programs (Section 5.1)? • How should we implement the data structures and core functionality for autodiff and our compiler backend (Section 5.2)? • How should users extend Rose when its builtin set of functionality is insufficiently expressive (Section 5.3)? In the next three subsections, we discuss in turn the decisions we made to answer each of those questions, and retrospectively evaluate those decisions in light of the system as a whole. In the spirit of transparency and community learning, we also highlight implementation design decisions we believe were mistakes. 5.1 Metaprogramming Instead of parsing a textual syntax like conventional programming languages and some autodiff systems like Dex [Paszke et al. 2021], or transforming an existing AST format like DiffTaichi [Hu et al. 2019], we follow an approach more similar to TensorFlow [Abadi et al. 2015], PyTorch [Paszke et al. 2017, 2019], and JAX [Bradbury et al. 2018; Frostig et al. 2018] in which users define Rose programs via metaprogramming in the host language, which in this case is JavaScript or TypeScript. However, Rose differs from prior work in that it allows the user to define functions via metaprogramming. For instance, consider the code in Listing 3. Here, the user dynamically defines a tower of twenty-one functions which (very inefficiently) compute the function 𝑥 ↦→ 220𝑥, then compiles its forward-mode derivative and calls that with a primal input of two and an input tangent of three. If you remove the calls to fn which make each layer of the tower its own function, this example instead generates a line of IR code for each of the 220 individual terms. The result is that the browser simply refuses to compile the resulting WebAssembly code, because it has too many local variables; see also Section 6.4. This may seem like a silly example, but this ability to explicitly demarcate reused chunks of code is crucial to keeping down compile times when Rose is used inside of real applications, including several examples in Section 6. This is one of Rose’s central ideas. Rose: Efficient and Extensible Autodiff on the Web 11 import { Real, div, mul } from \"rose\"; export const powi = (x: Real, n: number): Real => { if (n < 0) return powi(div(1, x), -n); else if (n == 0) return 1; else if (n == 1) return x; else if (n % 2 == 0) return powi(mul(x, x), n / 2); else return mul(x, powi(mul(x, x), (n - 1) / 2)); }; Listing 4. Exponentiation by squaring. import { Real, add, mul, sub } from \"rose\"; import { powi } from \"./powi.js\"; const myPolynomial = fn([Real, Real], Real, (x, y) => { let f = mul(2, powi(x, 3)); f = add(f, mul(4, mul(powi(x, 2), y))); f = add(f, mul(x, powi(y, 5))); f = add(f, powi(y, 2)); f = sub(f, 7); return f; }); Listing 5. A polynomial. But as we described in Section 4, the Rose IR is limited: for instance, you currently cannot define a recursive function in Rose IR. In many cases, though, it is useful to be able to perform recursion on static data, even if no recursion is necessary after compilation. Rose places no restrictions whatsoever on use of recursion in metaprogramming, so a user can easily define a function for exponentiation-by-squaring like in Listing 4. Then they can use this powi function wherever they please, such as to compute the polynomial (𝑥,𝑦) ↦→ 2𝑥3 + 4𝑥2𝑦 + 𝑥𝑦5 + 𝑦2 − 7 in Listing 5. This generates efficient code by avoiding a more expensive call to a fully general power function that allows a floating-point exponent, like the one we used in Section 3. By letting the user decide when to create a function abstraction and when to simply have their code act as a macro to generate Rose IR, we get the best of both worlds, simultaneously allowing flexible definition of a computation graph while also maintaining compilation efficiency by allowing construction of reusable functions. 5.2 Rust core As mentioned above, we implemented the core of Rose in Rust. This has a couple advantages over implementing directly in JavaScript or TypeScript. First, in theory it could mean that the compiler itself is faster. Second, it means that our Rust types defining our core IR, as well as our functionality for differentiation, transposition, interpretation, and compilation are all available as Rust libraries (called “crates”) which can then be used in a variety of contexts. However, this approach also has a major downside: it greatly limits extensibility on the JavaScript side. We package together all these Rose crates as dependencies of a single crate called rose-web, 12 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine import { Dual, Real, fn, mul, neg, opaque } from \"rose\"; const sin = opaque([Real], Real, Math.sin); const cos = opaque([Real], Real, Math.cos); sin.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => { return { re: sin(x), du: mul(dx, cos(x)) }; }); cos.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => { return { re: cos(x), du: mul(dx, neg(sin(x))) }; }); Listing 6. Definitions of sine and cosine functions with custom derivatives. import { Dual, Real, fn, opaque } from \"rose\"; const print = opaque([Real], Real, (x) => { console.log(x); return x; }); print.jvp = fn([Dual], Dual, (z) => z); Listing 7. A custom Rose function for print debugging. which is then compiled as a single WebAssembly binary. The JavaScript wrapper ships this We- bAssembly binary directly, rather than any of the Rust source code. This works great if the user only needs Rose’s builtin functionality for defining, differentiating, and compiling functions. But a more advanced user might want to go beyond this: for instance, they may want to define an optimization pass over Rose IR which gets called before code generation, or perhaps even before differentiation. Currently this is impossible without forking the rose-web crate and recompiling it manually into a new WebAssembly binary, which is then completely incompatible with any code that was previously using the npm rose package. Because of this, we retrospectively believe that this design decision was incorrect, and that it would be preferable for Rose itself to be written in TypeScript instead. The performance advantage of WebAssembly is unclear [Yan et al. 2021]; while we show in Section 6.5 that it provides a significant speedup for generated code, it is harder to make a case that compilation time deserves such prioritization of raw speed. 5.3 Custom derivatives There are two situations in which a user might want to define a custom derivative for a function. Section 3 showed an example of the first kind: when using an opaque function that calls out to JavaScript, Rose cannot peer into the body of that function, so it must be explicitly told the function’s derivative. As another example, Rose even lets you define custom derivatives for functions that depend on each other, as in Listing 6. The user can also define their own functions to use with opaque; for instance, one might want to define a print function for debugging purposes as in Listing 7, and tell Rose that the derivative of this function does nothing because, other than its side effect, it acts like the identity function: Rose: Efficient and Extensible Autodiff on the Web 13 import * as rose from \"rose\"; import { Dual, Real, div, fn, gt, mul, select } from \"rose\"; const max = (x: Real, y: Real) => select(gt(x, y), Real, x, y); const sqrt = fn([Real], Real, (x) => rose.sqrt(x)); sqrt.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => { const y = sqrt(x); const dy = mul(dx, div(1 / 2, max(1e-5, y))); return { re: y, du: dy }; }); Listing 8. A custom derivative of the square root function to avoid exploding gradients. The other situation is when Rose has automatically constructed a derivative for a function, but that derivative is unstable or otherwise exhibits some undesirable property. Rose allows the user to set a custom derivative for any function, not just opaque ones. For instance, by default the derivative of the square root function tends to infinity as the argument approaches zero, which causes problems if it is ever called with a zero argument. To prevent this exploding-gradient problem, we sometimes use a square root with a clamped derivative, as in Listing 8. In all of these examples, notice that the user only needs to specify the JVP, and not the VJP; this is true even if they later decide to use any of these functions in a VJP context, because Rose uses transposition (described in Section 4.2) to automatically construct a VJP from the JVP. A large part of the value of autodiff is that it ensures that the derivative remains in sync with the primal function by construction, so if we can also assist in keeping the forward-mode and reverse-mode derivatives in sync when one of them must be manually specified, this is a significant benefit for ergonomics and maintainability on the user side. 6 EVALUATION In this section we evaluate the design goals we laid out in Section 1. We discuss each design goal in a different subsection. Each subsection describes the method used to evaluate its goal and the results obtained. Some of our methods involved gathering timing data from running real programs; all such numbers we report were measured in the V8 JavaScript engine (used in both Chrome and Node.js) on a 2020 MacBook Pro with M1 chip. 6.1 In-browser test cases We integrated Rose into Penrose [Ye et al. 2020], a web-based diagramming tool. Penrose allows users to specify a diagram by constructing a custom numerical optimization problem, then runs a numerical solver to rearrange the shapes in the diagram until it finds a local minimum. Penrose previously used TensorFlow.js to compute derivatives; we contributed to Penrose by replacing its autodiff engine with one written in Rose. We also used Rose to implement and augment two differentiable physics simulations from DiffTaichi [Hu et al. 2019]: billiards and robot (shown in Fig. 4). The billiards example is a differentiable simulation of pool combination shots. The program simulates rigid body collisions between a cue ball and object balls. Leveraging the differentiability of the simulation, a gradient descent optimizer solves for the initial position and velocity of the cue ball to send a designated 14 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine E F A B C D G Fig. 4. Differentiable simulations from DiffTaichi [Hu et al. 2019] made interactive using Rose, where interac- tive features are denoted by hand icons. Left: billiards simulator that optimizes D for cue ball angle and speed such that the object ball A reaches the target B . Right: mass-spring robot controlled by a neural net trained G with a designated goal D . Both simulations can be replayed by dragging the sliders at any point D and G . object ball to a target position. The robot example simulates a robot made of a mass-spring system, where springs are actuated to move the robot towards a goal position. A neural network controller is trained on simulator gradients to update the spring actuation magnitude over time. All three examples were implemented using Rose’s TypeScript binding, and transpiled to JavaScript. Supplementary materials include the source code for both the original and Rose versions of all three examples. They all run in major browsers such as Safari and Chrome, showing that we achieved D1. To showcase the benefits of running in the web browser (D1), we added interactive features to the DiffTaichi applications (Fig. 4). For instance, the DiffTaichi version of billiards is a command-line application that outputs a series of static images based on hard-coded parameters for the choice of the object ball and goal position. The Rose version allows the user to interactively explore the simulator by selecting the object ball (Fig. 4 A ), moving the goal position (Fig. 4 B ), optimizing the cue ball position (Fig. 4 D ), and re-playing the simulation (Fig. 4 C ). 6.2 Function dynamism Metaprogramming using JavaScript enables the user to dynamically generate complex compu- tation graphs that are impossible to specify with the Rose IR alone. For instance, the bboxGroup function in Listing 9 computes the bounding box of a Group in Penrose, a recursive collection of shapes. For non-collection shape types such as Circle, we ported the TensorFlow.js implementa- tion to Rose easily, e.g. bboxCircle. However, bboxGroup needs to recurse over the Group data structure to find out the bounding boxes of individual shapes before aggregating them into the final bounding box. This requires conditional dispatch of (1) Rose functions based on a discrete tag (shape.kind) and (2) recursive calls to bboxGroup to handle nested groups. Figure 5 shows an example of calling bboxGroup on nested groups of shapes. The diagram in Fig. 5 (left) has 1 group containing the whole diagram, and 3 subgroups of molecules that contain Rose: Efficient and Extensible Autodiff on the Web 15 1 const bboxGroup = (shapes) => { 2 const bboxes = shapes.map(bbox); 3 const left = bboxes.map((b) => b.left).reduce(min); 4 const right = bboxes.map((b) => b.right).reduce(max); 5 const bottom = bboxes.map((b) => b.bottom).reduce(min); 6 const top = bboxes.map((b) => b.top).reduce(max); 7 return { left, right, bottom, top }; 8 }; 9 10 const bboxCircle = fn([Circle], Rectangle, 11 ({ center: [x, y], radius: r }) => { 12 const left = sub(x, r); 13 const right = add(x, r); 14 const bottom = sub(y, r); 15 const top = add(y, r); 16 return { left, right, bottom, top }; 17 }, 18 ); 19 20 const bbox = (shape) => { 21 switch (shape.kind) { 22 case \"Rectangle\": return shape.value; 23 case \"Circle\": return bboxCircle(shape.value); 24 case \"Group\": return bboxGroup(shape.value); 25 } 26 }; Listing 9. Examples of JavaScript metaprogramming to construct Rose functions for recursive data structures. switch switch bboxGroup( ) bboxGroup( ) bboxGroup( ) bboxGroup( ) bboxCircle( ) bboxText( ) bboxLine( ) Fig. 5. In Penrose, we used JavaScript to programmatically generate Rose functions. Left: a figure comprised of a top-level group containing all molecules and sub-groups for each molecule. Right: the bboxGroup function conditionally generates Rose functions or recursively calls itself based on the shape type. 16 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine shapes such as Text and Circle. Figure 5 shows how bboxGroup uses JavaScript language features to compose Rose functions into a computation graph, denoting JavaScript constructs in gray and Rose functions in red. First, for each member shape, we switch on shape.kind to determine whether to call (a) individual Rose bounding box functions like bboxCircle or (b) recurse to call bboxGroup. Then, after all the child bounding boxes are computed, we use JavaScript map and reduce to aggregate the results via Rose min and max functions. 6.3 Custom derivatives Both the Penrose and DiffTaichi examples require custom functions (Section 3.2) beyond the arithmetic operations built into the Rose IR. For instance, in the robot example, the neural network controller uses the hypobolic tangent activation function in its hidden layer. Similar to the examples in previous sections, we implemented a differentiable tanh function using Rose’s opaque and jvp. Using the same mechanism, we also implemented a larger suite of custom differentiable functions in Penrose, including cbrt, atanh, expm1, and many others. This demonstrates the effectiveness of Rose’s facilities for custom derivatives (D2) and shows that custom derivatives are useful for real applications. 6.4 Writing pointful programs For D3, we compare pointful programs rewritten in Rose from other state-of-the-art tools. The original versions of Penrose, billiards, and robot are naturally written as pointful programs. In Penrose, bboxCircle (line 10 of Listing 9) computes the bounding box by performing arithmetic on scalar values for the center and radius of a circle. In DiffTaichi, both billiards and robot involve hand-crafted pointful programs for differentiable simulations. For instance, apply_spring_force (Fig. 6) loops through individual springs in the robot, computing the force on the spring based on scalar-valued parameters, and scatter forces to end points of springs. Because Rose is designed for writing pointful programs (D3), translating both Penrose and DiffTaichi source programs to Rose is straightforward and largely preserves the structures of the programs. For instance, when translating the Python programs from DiffTaichi into TypeScript and Rose, as shown in Fig. 6, DiffTaichi kernels can be translated one-to-one to Rose functions. Note that in the case of DiffTaichi, the Rose abstraction of fn is not only useful for one-to- one translation from DiffTaichi, but also necessary for running the simulator in browsers. Major WebAssembly engines have limits on WebAssembly binary size and on the number of local variables in each function. While it is possible to encapsulate much of the simulation code of billiards and robot in bigger JavaScript functions, the compiled size and local counts of these functions would quickly exceed these limits and would not run in the browser (D1). Therefore, segmenting the source into functional units of fns effectively reduces the size of emitted WebAssembly functions and modules, avoiding these errors and reducing compile times (D5). 6.5 Performance We ran both a TensorFlow.js version of Penrose and a Rose version on a set of 173 “registry” diagrams, and measured the amount of time it took for each autodiff engine to perform any necessary compilation, plus the time taken by the Penrose L-BFGS [Liu and Nocedal 1989] optimization engine to converge on each diagram. Optimizing the layout of these diagrams involves a wide range of mathematical operations on scalars. These include simple operations like finding the distance between points. The diagrams also use sophisticated mathematics like Minkowski addition, KL divergence, and silhouette points. The Penrose performance benchmark therefore contains a wide range of real-world AD functions. Rose: Efficient and Extensible Autodiff on the Web 17 @ti.kernel def apply_spring_force(t: ti.i32): for i in range(n_springs): a = spring_anchor_a[i] b = spring_anchor_b[i] pos_a = x[t, a] pos_b = x[t, b] dist = pos_a - pos_b length = dist.norm() + 1e-4 target_length = spring_length[i] * (1.0 + spring_actuation[i] * act[t, i]) impulse = dt * (length - target_length) * spring_stiffness[i] / length * dist ti.atomic_add(v_inc[t + 1, a], -impulse) ti.atomic_add(v_inc[t + 1, b], impulse) const apply_spring_force = fn( [Objects, Act], Objects, (x, act) => { const v_inc = []; for (let i = 0; i < n_objects; i++) v_inc.push([0, 0]); for (let i = 0; i < n_springs; i++) { const spring = robot.springs[i]; const a = spring.object1; const b = spring.object2; const pos_a = x[a]; const pos_b = x[b]; const dist = vsub2(pos_a, pos_b); const length = add(norm(dist), 1e-4); const target_length = mul(spring.length, add(1, mul(act[i], spring.actuation)) ); const impulse = vmul(div(mul(dt * spring.stiffness, sub(length, target_length)), length), dist); v_inc[a] = vsub2(v_inc[a], impulse); v_inc[b] = vadd2(v_inc[b], impulse); } return v_inc; }); Fig. 6. A function that applies spring actuation on the mass-spring robot model in the robot example, written in DiffTaichi (Left) and Rose (Right). The translation from DiffTaichi to Rose is straightforward. 10 2 100 102 104 TensorFlow.js (seconds) 10 3 10 2 10 1 100 Rose (seconds) 100 102 104 106 TensorFlow.js / Rose 0.0 0.1 0.2 0.3 0.4 probability density Fig. 7. Left: Log-log scatterplot of Penrose diagram optimization time with TensorFlow.js versus Rose. Right: Log-scale kernel density estimate (KDE) plot of the optimization time of TensorFlow.js to Rose. Note that we specifically include the time it takes for Rose to do autodiff, transposition, and Wasm compilation, despite the fact that TensorFlow.js does not have an analogous compilation step. On the surface this puts Rose at a disadvantage, but per D5 we believe that fast compilation time is essential when constructing Rose functions dynamically in a user-facing web application, as Penrose does. 18 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine Figure 7 shows the results. Supplementary materials include side-by-side comparisons of the generated SVG diagrams, to show that both versions produced similar results. We omitted 10 of the 173 diagrams from our data analysis: • 9 NaN failures: Penrose aborts if it detects a “not-a-number” (NaN) value in the gradient as it is optimizing. This occurred in the TensorFlow.js version of Penrose for nine diagrams. The Rose version of Penrose did not encounter NaNs for these programs. • 1 timeout: For one diagram, we stopped the TensorFlow.js version of Penrose after it had run for over 24 hours. The Rose version of Penrose took 42 milliseconds to compile and optimize this diagram. We used the \"cpu\" backend for TensorFlow.js because we found that, for pointful programs, it was faster than their GPU backend. To double-check this, we took the 88 diagrams (over half) that were quickest to run with TensorFlow.js, and also ran them with @tensorflow/tfjs-node and @tensorflow/tfjs-node-gpu, which they claim are faster than the \"cpu\" backend. We found that the Node backend is 79% slower (median ratio) than the \"cpu\" backend, and the Node GPU backend is 75% slower (median ratio) than the \"cpu\" backend. Also, those backends are unable to run in a browser, unlike the \"cpu\" backend, so they would be inappropriate for a direct comparison to Rose. The quartiles for the ratio of TensorFlow.js optimization time to Rose optimization time were 37×, 173×, and 598×. These results show that Rose provides an enormous advantage over Ten- sorFlow.js (the state-of-the-art for autodiff on the web) for pointful programs like those found in Penrose diagrams. Because these numbers include both compile time and optimization time, this demonstrates the performance of Rose according to our goals D5 and D6. As discussed earlier in Section 6.4, Rose’s ability to define separate functions in a graph (rather than just a single graph of scalar or tensor values) is crucial to producing small enough WebAssem- bly binaries to feed to the browser. To investigate whether WebAssembly brought significant performance gains in the first place to be worth facing that challenge, we compared against a modified version of Rose which emits JavaScript code instead of WebAssembly. This experiment gave quartile slowdowns of 10%, 49%, and 100% for optimization of Penrose diagrams, showing that WebAssembly provides a significant advantage over JavaScript as a compilation target for Rose. 7 RELATED WORK There is a fair amount of existing work on automatic differentiation theory and practice, and also some work on embedding rich DSLs inside a general-purpose programming language. One of the first to explore reverse-mode autodiff from a compiler perspective was Speelpenning [1980], also showing the value of reverse-mode autodiff over forward-mode in many situations. Since then, many presentations of autodiff as a program transformation have appeared, such as Innes [2018] introducing Zygote, which handles a range of language constructs far broader than what we support in this paper. The reason for our higher level of conservatism is that we follow the more pure functional approach from Paszke et al. [2021] to leave open room to target the GPU (see Section 8.2) and automatically parallelize programs and their derivatives. Specifically, we build on theoretical work from Radul et al. [2023] to split reverse-mode autodiff into forward-mode autodiff followed by transposition. Our implementation builds on important theoretical groundwork. For instance, Bernstein et al. [2020] show cost preservation on a pure tensor language and achieve sparsity via a theoretical “Iverson bracket” construct that hides nontrivial constant- or logarithmic-factor costs in practice. On the other hand, Wang et al. [2019] handle control flow of a functional language, but do not support array parallelism. Importantly, none these prior approaches use our transposition approach Rose: Efficient and Extensible Autodiff on the Web 19 to decompose forward-mode autodiff as a part of reverse-mode autodiff, which is vital for our implementation to ease definition of custom derivatives. Rose supports higher-order derivatives because its core IR is closed under differentiation and transposition. A more sophisticated approach we don’t explore here would be derivative tow- ers [Karczmarczuk 1998; Pearlmutter and Siskind 2007], sometimes called “Taylor towers” because they use Taylor expansions instead of the chain rule. We would be interested to see how derivative towers can be combined with our approach in future work. Venturing more into the systems landscape, we’ve already discussed the Python libraries Tensor- Flow [Abadi et al. 2015], PyTorch [Paszke et al. 2017, 2019], JAX [Bradbury et al. 2018; Frostig et al. 2018], and Taichi [Hu et al. 2019], as well as the web library TensorFlow.js [Smilkov et al. 2019]. C++ autodiff systems include the popular Tapenade [Hascoet and Pascual 2013] for reverse-mode, and the more recent TinyAD [Schmidt et al. 2022] for forward-mode. Zygote [Innes et al. 2019] provides autodiff for Julia. Both of the latter two languages are often compiled via LLVM [Lattner and Adve 2004], for which Enzyme [Moses and Churavy 2020; Moses et al. 2021, 2022] can produce derivatives by operating only over IR rather than source code. For graphics programming, A𝛿 [Yang et al. 2022] and Dr.Jit [Jakob et al. 2022], can be used to differentiate shaders. And now, Rose starts to bring modern autodiff research to the web. Rose makes the explicit decision to embed itself inside JavaScript/TypeScript as a library, some- times called a domain-specific embedded language (DSEL) [Hudak 1996], to take advantage of all the facilities of the host language for metaprogramming, as described in Section 5.1. We believe that there is room for future work on what makes different host languages more or less suitable for creating these sorts of embedded DSLs. For instance, Rose makes clever use of JavaScript’s Symbol and Proxy types to make it so that code dealing with arrays and structs looks like normal JavaScript code, while actually intercepting property accesses to generate Rose IR. On the other hand, JavaScript does not support general operator overloading, so Rose users must write add and sub instead of + and -. Further, it would be difficult for Rose to record source location information for debugging purposes, because JavaScript does not provide anything analogous to Python’s inspect module. One method to do this could be to have every Rose function throw an Error and then immediately catch it, and use string processing to extract the caller’s source location from the error backtrace, but this would be slow and non-portable. 8 DISCUSSION AND FUTURE WORK Here we discuss a few topics that we did not have a chance to explore in this paper, but would like to note as potential directions for future work. 8.1 Linear types Radul et al. [2023] provide a cost model for a linearly-typed language and prove that their three-pass autodiff scheme preserves both cost and code size. We would like to extend their results to Rose IR, which includes array size polymorphism. It is not immediately obvious what should be the cost semantics here; consider this function: def copy <n: Index , T: Value >(x: T): [n]T = [for i: n, x] Let $T denote the cost of cloning a value of type T, and let |n| denote the number of elements in the type n. Depending on whether the implementation allows structural sharing, the cost of this copy function might be either 𝑂(|n|) or 𝑂(|n| × $T). Rose IR shares many similarities with Paszke et al. [2021]; from direct correspondence with the authors of that paper, we know that Dex performs cloning here and so their cost is the latter. The current implementation of Rose does allow 20 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine structural sharing, so our cost is the former. But then it is unclear how to translate this cost to the derivative; for instance, here is the reverse-mode derivative of copy: def fwd_copy <n: Index , T: Value >(x: T): ([n]T, ()) = ([for i: n, x], ()) def bwd_copy <n: Index , T: Value >(dx: &T, dv: [n]T, (): ()) = let _ = [for i: n, dx += dv[i]] in () The cost of bwd_copy is 𝑂(|n| × $T), which appears unavoidable because we don’t control the structure of dv. We believe that to address this, it may be valuable to use a type system that is linear for primal values, unlike the type system from Radul et al. [2023] which only enforces linearity for tangents. 8.2 WebGPU Rose currently targets WebAssembly, which runs on the CPU. As we showed in Section 6.5, this already provides an enormous performance advantage for pointful programs when compared to the state-of-the-art for autodiff on the web. However, we would also like to pursue further performance gains by implementing a backend that targets WebGPU [Kenwright 2022], a modern GPU interface for the web. Many elements of Rose IR are inspired by Paszke et al. [2021] to be friendly to automatic parallelization, such as the for construct and accumulate-only reference types; regardless, some engineering effort would still be required to compile for the GPU. 8.3 Tree shaking Rose’s mechanism for constructing functions has a subtle interaction with popular JavaScript bundling techniques, which are used to shrink the size of a website’s source code to actually ship to the client’s browser; this is sometimes referred to as “tree shaking.” Specifically, defining a Rose function using fn immediately executes its body once to emit Rose IR. Even if this is conceptually a “pure” operation, JavaScript build tools count it as a side effect, so they do not eliminate unused Rose function definitions from a bundle. This concern would be more important if a user imports some Rose functions from a large library. We would like to investigate mechanisms to manage bundle sizes in such cases. 9 CONCLUSION This paper introduces Rose, an embedded domain-specific language for automatic differentiation of web programs. Rose is targeted at applications where users can define custom functions involving scalar math that need to be differentiated. These applications are not well suited to the most popular autodiff libraries since they focus on statically-defined block operations on tensors. It is our hope that Rose will enable a flourishing ecosystem of simulations, custom optimization engines, and differentiable graphics applications on the web. You may say I’m a derivative, but I’m not the only one. I hope someday you’ll join us, and the world will live as one. You must be the derivative you wish to see in the world. You have derivatives in your head. Rose: Efficient and Extensible Autodiff on the Web 21 You have derivatives in your shoes. You can steer yourself any direction you choose. ACKNOWLEDGMENTS Thanks to Adam Paszke for corresponding about JAX and Dex. The Rose icons were created by Aaron Weiss; we use them via the CC BY 4.0 license. REFERENCES Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. https: //www.tensorflow.org/ Software available from tensorflow.org. Hudson Ayers, Evan Laufer, Paul Mure, Jaehyeon Park, Eduardo Rodelo, Thea Rossman, Andrey Pronin, Philip Levis, and Johnathan Van Why. 2022. Tighten Rust’s Belt: Shrinking Embedded Rust Binaries. In Proceedings of the 23rd ACM SIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems (San Diego, CA, USA) (LCTES 2022). Association for Computing Machinery, New York, NY, USA, 121–132. https://doi.org/10.1145/ 3519941.3535075 Gilbert Bernstein, Michael Mara, Tzu-Mao Li, Dougal Maclaurin, and Jonathan Ragan-Kelley. 2020. Differentiating a Tensor Language. arXiv:2008.11256 [cs.PL] James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, et al. 2018. JAX: composable transformations of Python+ NumPy programs. (2018). Clifford. 1871. Preliminary sketch of biquaternions. Proceedings of the London Mathematical Society 1, 1 (1871), 381–395. Roy Frostig, Matthew James Johnson, and Chris Leary. 2018. Compiling machine learning programs via high-level tracing. Systems for Machine Learning 4, 9 (2018). Andreas Haas, Andreas Rossberg, Derek L. Schuff, Ben L. Titzer, Michael Holman, Dan Gohman, Luke Wagner, Alon Zakai, and JF Bastien. 2017. Bringing the Web up to Speed with WebAssembly. SIGPLAN Not. 52, 6 (jun 2017), 185–200. https://doi.org/10.1145/3140587.3062363 Laurent Hascoet and Valérie Pascual. 2013. The Tapenade Automatic Differentiation Tool: Principles, Model, and Specification. ACM Trans. Math. Softw. 39, 3, Article 20 (may 2013), 43 pages. https://doi.org/10.1145/2450153.2450158 Yuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and Frédo Durand. 2019. DiffTaichi: Differentiable Programming for Physical Simulation. CoRR abs/1910.00935 (2019). arXiv:1910.00935 http://arxiv.org/abs/ 1910.00935 Paul Hudak. 1996. Building domain-specific embedded languages. Acm computing surveys (csur) 28, 4es (1996), 196–es. Michael Innes. 2018. Don’t Unroll Adjoint: Differentiating SSA-Form Programs. https://doi.org/10.48550/ARXIV.1810.07951 Mike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. 2019. A Differentiable Programming System to Bridge Machine Learning and Scientific Computing. arXiv:1907.07587 [cs.PL] Wenzel Jakob, Sébastien Speierer, Nicolas Roussel, and Delio Vicini. 2022. DR.JIT: A Just-in-Time Compiler for Differentiable Rendering. ACM Trans. Graph. 41, 4, Article 124 (jul 2022), 19 pages. https://doi.org/10.1145/3528223.3530099 Jerzy Karczmarczuk. 1998. Functional Differentiation of Computer Programs. In Proceedings of the Third ACM SIGPLAN International Conference on Functional Programming (Baltimore, Maryland, USA) (ICFP ’98). Association for Computing Machinery, New York, NY, USA, 195–203. https://doi.org/10.1145/289423.289442 Benjamin Kenwright. 2022. Introduction to the WebGPU API. In ACM SIGGRAPH 2022 Courses (Vancouver, British Columbia, Canada) (SIGGRAPH ’22). Association for Computing Machinery, New York, NY, USA, Article 10, 184 pages. https://doi.org/10.1145/3532720.3535625 C. Lattner and V. Adve. 2004. LLVM: a compilation framework for lifelong program analysis & transformation. In International Symposium on Code Generation and Optimization, 2004. CGO 2004. 75–86. https://doi.org/10.1109/CGO.2004.1281665 Dong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical programming 45, 1-3 (1989), 503–528. William Moses and Valentin Churavy. 2020. Instead of Rewriting Foreign Code for Machine Learning, Automatically Synthesize Fast Gradients. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 12472–12485. https://proceedings.neurips.cc/paper/2020/ 22 Sam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine file/9332c513ef44b682e9347822c2e457ac-Paper.pdf William S. Moses, Valentin Churavy, Ludger Paehler, Jan Hückelheim, Sri Hari Krishna Narayanan, Michel Schanen, and Johannes Doerfert. 2021. Reverse-Mode Automatic Differentiation and Optimization of GPU Kernels via Enzyme. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (St. Louis, Missouri) (SC ’21). Association for Computing Machinery, New York, NY, USA, Article 61, 16 pages. https: //doi.org/10.1145/3458817.3476165 William S. Moses, Sri Hari Krishna Narayanan, Ludger Paehler, Valentin Churavy, Michel Schanen, Jan Hückelheim, Johannes Doerfert, and Paul Hovland. 2022. Scalable Automatic Differentiation of Multiple Parallel Paradigms through Compiler Augmentation. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis (Dallas, Texas) (SC ’22). IEEE Press, Article 60, 18 pages. Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. 31st Conference on Neural Information Processing Systems (Oct. 2017). https://openreview.net/forum?id=BJJsrmfCZ Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An Imperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf Adam Paszke, Daniel D. Johnson, David Duvenaud, Dimitrios Vytiniotis, Alexey Radul, Matthew J. Johnson, Jonathan Ragan- Kelley, and Dougal Maclaurin. 2021. Getting to the Point: Index Sets and Parallelism-Preserving Autodiff for Pointful Array Programming. Proc. ACM Program. Lang. 5, ICFP, Article 88 (aug 2021), 29 pages. https://doi.org/10.1145/3473593 Barak A. Pearlmutter and Jeffrey Mark Siskind. 2007. Lazy Multivariate Higher-Order Forward-Mode AD. In Proceedings of the 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Nice, France) (POPL ’07). Association for Computing Machinery, New York, NY, USA, 155–160. https://doi.org/10.1145/1190216.1190242 Alexey Radul, Adam Paszke, Roy Frostig, Matthew J. Johnson, and Dougal Maclaurin. 2023. You Only Linearize Once: Tangents Transpose to Gradients. Proc. ACM Program. Lang. 7, POPL, Article 43 (jan 2023), 29 pages. https://doi.org/10. 1145/3571236 P. Schmidt, J. Born, D. Bommes, M. Campen, and L. Kobbelt. 2022. TinyAD: Automatic Differentiation in Geome- try Processing Made Simple. Computer Graphics Forum 41, 5 (2022), 113–124. https://doi.org/10.1111/cgf.14607 arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14607 Daniel Smilkov, Nikhil Thorat, Yannick Assogba, Charles Nicholson, Nick Kreeger, Ping Yu, Shanqing Cai, Eric Nielsen, David Soegel, Stan Bileschi, Michael Terry, Ann Yuan, Kangyi Zhang, Sandeep Gupta, Sarah Sirajuddin, D Sculley, Rajat Monga, Greg Corrado, Fernanda Viegas, and Martin M Wattenberg. 2019. TensorFlow.js: Machine Learning For The Web and Beyond. In Proceedings of Machine Learning and Systems, A. Talwalkar, V. Smith, and M. Zaharia (Eds.), Vol. 1. 309–321. Bobbie Soedirgo. 2023. Compile and run LLVM IR in the browser. https://github.com/soedirgo/llvm-wasm original-date: 2021-02-24T14:29:16Z. Bert Speelpenning. 1980. Compiling fast partial derivatives of functions given by algorithms. University of Illinois at Urbana-Champaign. Fei Wang, Daniel Zheng, James Decker, Xilun Wu, Grégory M. Essertel, and Tiark Rompf. 2019. Demystifying Differentiable Programming: Shift/Reset the Penultimate Backpropagator. Proc. ACM Program. Lang. 3, ICFP, Article 96 (jul 2019), 31 pages. https://doi.org/10.1145/3341700 Yutian Yan, Tengfei Tu, Lijian Zhao, Yuchen Zhou, and Weihang Wang. 2021. Understanding the Performance of Webassembly Applications. In Proceedings of the 21st ACM Internet Measurement Conference (Virtual Event) (IMC ’21). Association for Computing Machinery, New York, NY, USA, 533–549. https://doi.org/10.1145/3487552.3487827 Yuting Yang, Connelly Barnes, Andrew Adams, and Adam Finkelstein. 2022. A𝛿: Autodiff for Discontinuous Programs – Applied to Shaders. In ACM SIGGRAPH, to appear. Katherine Ye, Wode Ni, Max Krieger, Dor Ma’ayan, Jenna Wise, Jonathan Aldrich, Joshua Sunshine, and Keenan Crane. 2020. Penrose: From Mathematical Notation to Beautiful Diagrams. ACM Trans. Graph. 39, 4, Article 144 (aug 2020), 16 pages. https://doi.org/10.1145/3386569.3392375 "
}