{
    "optim": "Rose: Efficient and Extensible Autodiff on the Web\nSAM ESTEP, Carnegie Mellon University, USA\nRAVEN ROTHKOPF, Barnard College, Columbia University, USA\nWODE NI, Carnegie Mellon University, USA\nJOSHUA SUNSHINE, Carnegie Mellon University, USA\nAutomatic differentiation (AD) has become the backbone for a new wave of optimization-driven domains\nsuch as computer graphics and machine learning over the past decade. However, existing AD systems face\nlimitations, either lacking support for in-browser development or failing to harness more recent, compiler-\nbased approaches to achieve both expressiveness and size-preserving differentiation. This work introduces\nRose, a portable, extensible AD library that runs on the web. Rose allows users to write opaque functions\nwith custom derivatives and supports dynamic construction of AD functions. We integrated Rose into two\ndifferentiable simulations and a diagram authoring tool to demonstrate the utility of Roseâ€™s design. Finally, we\nshow that Rose is 173Ã— as fast as TensorFlow.js in compiling and running a benchmark suite of optimized\ndiagrams.\nCCS Concepts: â€¢ Software and its engineering â†’ Domain specific languages; Compilers.\nAdditional Key Words and Phrases: automatic differentiation, compilers, web\n1\nINTRODUCTION\nAutomatic differentiation (autodiff or AD) is a widely used technique for efficiently computing\nderivatives of any program that computes a numerical value. The computed derivatives can be used\nto determine how small perturbations in a programâ€™s inputs can effect its outputs. Gradient-based\noptimization uses these derivatives to determine which direction to move to optimize an objective\nfunction.\nDerivatives of \nfunctions \nJavaScript \nfunctions\nFig. 1. We introduce Rose, an automatic differentia-\ntion engine on the web. Rose takes derivatives of func-\ntions defined using JavaScript, and compiles to We-\nbAssembly which the browser can efficiently execute.\nSignificant advancements in AD algorithms\nhave resulted in systems like PyTorch [Paszke\net al. 2017], TensorFlow [Abadi et al. 2015], and\nJAX [Bradbury et al. 2018; Frostig et al. 2018].\nThese frameworks provide high-level APIs for\ncomputing gradients of functions, making it\neasier for users to leverage AD in their work.\nThe computation model of each of these pop-\nular machine learning frameworks is centered\naround the tensor: you must structure your pro-\ngram as a composition of operations on ten-\nsors. However, many programs are expressed\nusing mathematical operations on scalars; we\ncall these pointful programs. In addition, none of\nthese tools can run in a web browser. Web appli-\ncations require no installation or central main-\ntenance and can reduce security risks. When\nteaching students with a simulator, for instance,\nit is advantageous to deploy instantly to every\nAuthorsâ€™ addresses: Sam Estep, estep@cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania,\nUSA, 15213; Raven Rothkopf, rgr2124@barnard.edu, Barnard College, Columbia University, 3009 Broadway, New York, New\nYork, USA, 10027; Wode Ni, nimo@cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania, USA,\n15213; Joshua Sunshine, sunshine@cs.cmu.edu, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, Pennsylvania,\nUSA, 15213.\narXiv:2402.17743v1  [cs.PL]  27 Feb 2024\n2\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nstudentâ€™s web browser and to avoid exposing servers to potentially insecure code the students\nwrite.\nIn this paper we present Rose,1 an extensible efficient pointful autodiff framework for the web.\nRose supports an important class of applications, like educational simulators and customizable opti-\nmization engines that are not well supported by popular AD frameworks. This class of applications\nleads to the following design goals for Rose:\nD1 Programs written with Rose run in a web browser.\nD2 AD functions can be partially composed of opaque functions with custom derivatives.\nD3 AD functions can be written manually in a pointful style.\nD4 AD functions can be constructed dynamically, for example based on user input.\nD5 Rose programs should compile quickly.\nD6 Rose programs should run efficiently.\nThe primary contribution of this paper is the design of the Rose system. Rose combines advanced\nresearch in a novel way to allow a neglected class of applications to flourish. Rose builds on the\ntheoretical foundations [Radul et al. 2023] provided by JAX and Dex [Paszke et al. 2021]. Rose is\nwritten in Rust and TypeScript and available for anyone to use through npm. Rose uses JavaScript\nas a metaprogramming environment so programmers can write custom AD functions and generate\nthem at runtime. Rose compiles to WebAssembly [Haas et al. 2017] for performance.\nWe holistically evaluate Rose against its design goals. We ported two DiffTaichi [Hu et al. 2019]\nsimulations to Rose and run them in the browser. We make use of custom opaque functions and write\nAD functions in a pointful style. We also integrated Rose with Penrose [Ye et al. 2020], a web-based\ndiagramming tool. Penrose allows users to specify a diagram by constructing a custom numerical\noptimization problem, then runs a numerical solver to rearrange the shapes in the diagram until it\nfinds a local minimum. Penrose previously used TensorFlow.js to compute derivatives. The median\nspeedup of Penrose with Rose is 173Ã— compared to Penrose with TensorFlow.js.\nThe rest of this paper is structured as follows. In Section 2 we give general background information\nabout autodiff. In Section 3 we walk through a detailed example Rose program illustrating many\ndifferent features. In Section 4 we describe the Rose IR, over which we define our autodiff program\ntransformations. In Section 5 we discuss practical challenges that arise on top of the theoretical\nones, along with design decisions that place Rose more concretely in the space of possible solutions.\nIn Section 6 we lay out the results of experiments we have conducted to show that we achieved\nthe design goals for Rose as we described above. In Section 7 we discuss related work. Finally we\ndiscuss limitations and future work in Section 8 before concluding in Section 9.\n2\nBACKGROUND\nAutodiff comes in two forms: forward-mode and reverse-mode. Reverse-mode is used to compute\ngradients which inform optimization. Forward-mode is easier to specify and compute. Rose trans-\nposes automatically from forward-mode to reverse-mode. These correspond to two different kinds\nof derivatives, the Jacobian-vector product (JVP) and vector-Jacobian product (VJP), respectively.\nIn this section we define these two concepts. We largely follow the presentation from Radul et al.\n[2023, section 2.1], with a couple tweaks such as introducing the dual numbers which Rose uses to\nallow definitions of custom derivatives.\n2.1\nJacobian-vector product\nThe Jacobian-vector product allows one to ask, how the output to ğ‘“ would change if the input ğ‘¥\n(the primal value) was perturbed in the direction ğ‘£ (the tangent)?\n1Not to be confused with the ROSE (all caps) compiler infrastructure: http://rosecompiler.org/\nRose: Efficient and Extensible Autodiff on the Web\n3\nFirst, suppose we have a function ğ‘“ : Rğ‘› â†’ Rğ‘š. We define the Jacobian of ğ‘“ at ğ‘¥ âˆˆ Rğ‘› to be the\nlinear map ğœ•ğ‘“ (ğ‘¥) : Rğ‘› â†’ Rğ‘š such that for ğ‘£ âˆˆ Rğ‘›,\nğ‘“ (ğ‘¥ + ğ‘£) = ğ‘“ (ğ‘¥) + ğœ•ğ‘“ (ğ‘¥)(ğ‘£) + ğ‘œ(âˆ¥ğ‘£âˆ¥).\nThe JVP for ğ‘“ is the mapping\n(ğ‘¥, ğ‘£) â†¦â†’ (ğ‘“ (ğ‘¥), ğœ•ğ‘“ (ğ‘¥)(ğ‘£)).\nNote that this definition is compositional, so the JVP of ğ‘“ â—¦ ğ‘” is the same as the JVP of ğ‘“ composed\nwith the JVP of ğ‘”.\nIn Rose, we use a slightly different definition based on the dual numbers [Clifford 1871], that is,\nthe commutative algebra\nD = {ğ‘ + ğ‘ğœ€ | ğ‘,ğ‘ âˆˆ R}\nwhereğœ€2 = 0. Usefully, arithmetic operations on D correspond directly to JVPs of their corresponding\noperations on R. For instance, the JVP of (ğ‘¥,ğ‘¦) â†¦â†’ ğ‘¥ + ğ‘¦ for ğ‘¥,ğ‘¦ âˆˆ R is\n((ğ‘¥,ğ‘¦), (ğ‘¢, ğ‘£)) â†¦â†’ (ğ‘¥ + ğ‘¦,ğ‘¢ + ğ‘£),\nbut also the sum of two dual numbers is\n(ğ‘¥ + ğ‘¢ğœ€) + (ğ‘¦ + ğ‘£ğœ€) = (ğ‘¥ + ğ‘¦) + (ğ‘¢ + ğ‘£)ğœ€.\nNext observe that the JVP (ğ‘¥,ğ‘¦) â†¦â†’ ğ‘¥ğ‘¦ is just the product rule\n((ğ‘¥,ğ‘¦), (ğ‘¢, ğ‘£)) â†¦â†’ (ğ‘¥ğ‘¦,ğ‘¥ğ‘£ + ğ‘¦ğ‘¢),\nand so is the product of two dual numbers\n(ğ‘¥ + ğ‘¢ğœ€)(ğ‘¦ + ğ‘£ğœ€) = ğ‘¥ğ‘¦ + (ğ‘¥ğ‘£ + ğ‘¦ğ‘¢)ğœ€\nbecause ğ‘¢ğ‘£ğœ€2 = 0. There is a similar correspondence between the quotient rule and the division\noperation on dual numbers.\nThus, instead of defining the type of the JVP of ğ‘“ to be Rğ‘› Ã— Rğ‘› â†’ Rğ‘š Ã— Rğ‘š, we define its type\nto be Dğ‘› â†’ Dğ‘š, like this:\nğ‘¥ + ğ‘£ğœ€ â†¦â†’ ğ‘“ (ğ‘¥) + ğœ•ğ‘“ (ğ‘¥)(ğ‘£) ğœ€\nWe use dual numbers to represent the primal and tangent together, as weâ€™ll discuss in Section 3.\n2.2\nVector-Jacobian product\nWhile the JVP deals with vectors (perturbations to the input or output), the VJP deals with covectors,\nthat is, linear maps from Rğ‘› or Rğ‘š to R. Any covector on a finite-dimensional real vector space\ncan be uniquely characterized as the inner product with some fixed vector. Intuitively, the vector\npoints in the direction that maximizes the output when the covector is applied. The VJP of ğ‘“ at\nğ‘¥ âˆˆ Rğ‘› takes the dual of ğ‘¢ âˆˆ Rğ‘š and returns the dual of ğ‘¤ âˆˆ Rğ‘› such that for all ğ‘£ âˆˆ Rğ‘›,\nğ‘¢ Â· ğœ•ğ‘“ (ğ‘¥) = ğ‘¤ Â· ğ‘¢.\nThis allows one to ask, given a covector on small perturbations to the output, what is the corre-\nsponding covector on small perturbations to the input? The mapping from ğ‘¢ to ğ‘¤ is linear, letting\nus define a new linear map ğœ•ğ‘“ (ğ‘¥)ğ‘‡ : Rğ‘š â†’ Rğ‘› by\nğ‘¢ Â· ğœ•ğ‘“ (ğ‘¥)(ğ‘£) = ğœ•ğ‘“ (ğ‘¥)ğ‘‡ (ğ‘¢) Â· ğ‘£.\nSo, more specifically, the VJP for ğ‘“ is the mapping\nğ‘¥ â†¦â†’ (ğ‘“ (ğ‘¥),ğ‘¢ â†¦â†’ ğœ•ğ‘“ (ğ‘¥)ğ‘‡ (ğ‘¢)).\n4\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nSimilar to the JVP, the VJP is also compositional, although not quite as directly: for ğ‘“ â—¦ ğ‘”, we need\nto compose the linear functions returned by the VJP in reverse-order as ğœ•ğ‘”(ğ‘¥)ğ‘‡ â—¦ ğœ•ğ‘“ (ğ‘”(ğ‘¥))ğ‘‡ , which\nis why this is called â€œreverse-modeâ€ autodiff.\nThe VJP is useful for optimization because in that setting we usually need the gradient of\nğ‘“ : Rğ‘› â†’ R, and usefully, this is simply âˆ‡ğ‘“ (ğ‘¥) = ğœ•ğ‘“ (ğ‘¥)ğ‘‡ (1) âˆˆ Rğ‘›. As we will see, autodiff shows\nus that the time to compute the VJP of ğ‘“ is linear with respect to the time to compute ğ‘“ itself, so\nwe can compute gradients efficiently. In contrast, even though the time to compute the JVP of ğ‘“ is\nclearly linear with respect to the time to compute ğ‘“ , we would need to evaluate the JVP ğ‘› times to\ncompute the gradient, ruining the asymptotic complexity; this is what often makes reverse-mode\nautodiff more useful than forward-mode in practice.\nNote that we cannot use the dual numbers for the VJP like we did for the JVP, because the input\nand output spaces are different: there is no way to â€œpair upâ€ components of Rğ‘› with components of\nRğ‘š.\n2.3\nHessian\nWe can treat a JVP or VJP itself as a function between two real vector spaces, allowing us to take\nhigher derivatives. The most common is the Hessian of a function ğ‘“ : Rğ‘› â†’ R, which is a matrix in\nRğ‘›Ã—ğ‘› of all the second-order partial derivatives of ğ‘“ at a point ğ‘¥ âˆˆ Rğ‘›. If we use the VJP to define\nğ‘” : Rğ‘› â†’ Rğ‘› by ğ‘”(ğ‘¥) = âˆ‡ğ‘“ (ğ‘¥) = ğœ•ğ‘“ (ğ‘¥)ğ‘‡ (1), then we can construct the ğ‘–th row of the Hessian\nHğ‘“ : Rğ‘› â†’ Rğ‘›Ã—ğ‘› by taking either the JVP or the VJP of ğ‘”:\n(Hğ‘“ (ğ‘¥))ğ‘– = ğœ•ğ‘”(ğ‘¥)(eğ‘–) = ğœ•ğ‘”(ğ‘¥)ğ‘‡ (eğ‘–)\n3\nEXAMPLE\nFig. 2. An interactive demonstra-\ntion of local quadratic approxi-\nmation, built with Rose and run-\nning on an iPhone in Safari.\nFigure 2 shows an interactive widget on the Rose project web-\nsite2 displaying the local quadratic approximation to the function\n(ğ‘¥,ğ‘¦) â†¦â†’ ğ‘¥ğ‘¦, allowing a user to drag the point around to see how\nthe shape of the local quadratic approximation shifts. The page also\nallows the user to modify the mathematical expression defining\nthe function, causing Rose to immediately re-derive the gradient\nand Hessian, and compile the new function to WebAssembly. For\nbrevity we omit the code to generate the user interface, and instead\nfocus on how one would use Rose to calculate the first and second\nderivatives used to visualize the quadratic approximation.\n3.1\nUsing Rose\nListing 1 shows a Rose program calculating the value, gradient,\nand Hessian of the power function at the point (2, 3). This is all\njust JavaScript code: from the user perspective, Rose is simply an\nembedded domain-specific language (eDSL) inside of JavaScript/-\nTypeScript. Line 1 uses a standard JavaScript import statement to\npull in definitions of types and higher-order functions from Rose.\nLine 2 imports the power function itself, which is already defined in\na library of basic math functions. For completeness, we will discuss\nthe full definition of pow in Section 3.2.\nLines 4 and 5 define type aliases for R2 and R2Ã—2, respectively. Rose types are simply JavaScript\nvalues, so type aliases are defined using const in the same way as any other JavaScript value.\n2See https://rosejs.dev/.\nRose: Efficient and Extensible Autodiff on the Web\n5\n1\nimport { Real, Vec, compile, fn, vjp } from \"rose\";\n2\nimport { pow } from \"./pow.js\";\n3\n4\nconst Vec2 = Vec(2, Real);\n5\nconst Mat2 = Vec(2, Vec2);\n6\n7\nconst f = fn([Vec2], Real, ([x, y]) => pow(x, y));\n8\nconst g = fn([Vec2], Vec2, (v) => vjp(f)(v).grad(1));\n9\nconst h = fn([Vec2], Mat2, (v) => {\n10\nconst { grad } = vjp(g)(v);\n11\nreturn [grad([1, 0]), grad([0, 1])];\n12\n});\n13\n14\nconst all = fn(\n15\n[Real, Real],\n16\n{ val: Real, grad: Vec2, hess: Mat2 },\n17\n(x, y) => {\n18\nconst v = [x, y];\n19\nreturn { val: f(v), grad: g(v), hess: h(v) };\n20\n},\n21\n);\n22\n23\nconst compiled = await compile(all);\n24\nconsole.log(compiled(2, 3));\nListing 1. An example Rose program.\nRemember that the vector-Jacobian product (VJP) introduced in Section 2 swaps the domain and\ncodomain from the original function. In addition, JavaScript only allows functions to return one\nargument. Therefore a Rose function of which you take the VJP must only have one parameter. So,\nline 7 wraps the pow function to take a single vector argument rather than two scalar arguments,\nallowing it to be passed to Roseâ€™s vjp function. Just as we discussed in Section 2.2, we compute the\ngradient by passing in a value of 1.\nLines 9 to 12 then use the gradient g of f to compute its Hessian by differentiating once more.\nLine 10 runs the forward pass for the Hessian just once and saves all necessary intermediate values,\nafter which line 11 runs the backward pass twice with the two basis vectors to compute the full\nHessian matrix.\nLines 14 to 21 wrap these three functions into a single function that calls all three and returns\nthe results in a structured form. Finally, line 23 compiles that function to WebAssembly, and line 24\ncalls it at the point (2, 3).\n3.2\nCustom functions\nRose allows users to define custom derivatives. Listing 2 defines the pow and log functions to\ndemonstrate this feature. In practice, these basic functions would be provided by a library, but many\nreal world examples require custom derivatives. Line 3 defines the natural logarithm as an opaque\nfunction that calls JavaScriptâ€™s builtin Math.log function. Because Rose cannot see the definition\nof this function, it must be given a definition for its derivative. Lines 4 to 6 define the logarithmâ€™s\n6\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\n1\nimport { Dual, Real, add, div, mul, fn, opaque } from \"rose\";\n2\n3\nconst log = opaque([Real], Real, Math.log);\n4\nlog.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => {\n5\nreturn { re: log(x), du: div(dx, x) };\n6\n});\n7\n8\nexport const pow = opaque([Real, Real], Real, Math.pow);\n9\npow.jvp = fn(\n10\n[Dual, Dual],\n11\nDual,\n12\n({ re: x, du: dx }, { re: y, du: dy }) => {\n13\nconst z = pow(x, y);\n14\nconst dw = add(mul(dx, div(y, x)), mul(dy, log(x)));\n15\nreturn { re: z, du: mul(dw, z) };\n16\n},\n17\n);\nListing 2. The contents of pow.js defining a differentiable power function.\nğ‘š,ğ‘› âˆˆ Zâ‰¥0\nğ‘\nâˆˆ R\nğœ…\n::= Type | Value | Index\nğœ\n::= ğ‘¡ | Bool | Real | ğ‘› | &ğœ | [ğœ]ğœ | (ğœ)\nâŠ–\n::= Â¬ | âˆ’ | abs | sgn | ceil | floor | trunc | sqrt\nâŠ•\n::= âˆ§ | âˆ¨ | iff | xor | â‰  | < | â‰¤ | = | > | â‰¥ | + | âˆ’ | Ã— | Ã·\nğ‘’\n::= () | true | false | ğ‘ | ğ‘› | [ğ‘¥] | (ğ‘¥, ğ‘¥) | âŠ–ğ‘¥ | ğ‘¥ âŠ• ğ‘¥ | ğ‘¥ ? ğ‘¥ : ğ‘¥ | ğ‘¥ += ğ‘¥\n| ğ‘¥[ğ‘¥] | fst ğ‘¥ | snd ğ‘¥ | &ğ‘¥[ğ‘¥] | &fst ğ‘¥ | &snd ğ‘¥ | ğ‘“ <ğœ>(ğ‘¥) | [for ğ‘¥: ğœ, ğ‘]\n| accum ğ‘¥ from ğ‘¥ in ğ‘\nğ‘\n::= ğ‘¥ | let ğ‘¥: ğœ = ğ‘’ in ğ‘\nğ‘‘\n::= def ğ‘“ <ğ‘¡: ğœ…>(ğ‘¥: ğœ): ğœ = ğ‘\nFig. 3. Abstract syntax for Rose.\nJacobian-vector product. Specifically, the signature of this function takes the original log function\nand maps every instance of the Real numbers to become the Dual numbers we introduced in\nSection 2.1. In this case, the returned tangent is given by the familiar rule\nd\ndğ‘¥ lnğ‘¥ = 1\nğ‘¥ from calculus.\nSimilarly, lines 8 to 17 define the power function along with its derivative. Note that, while these\ntwo functions use opaque to define their bodies, they define their derivatives via fn, the same as the\nRose functions we discussed in Section 3.1. This means that only the first forward derivative needs\nto be provided. Since the body of this first derivative is transparent to Rose, the reverse derivative\nand any higher derivatives can be computed automatically.\n4\nROSE INTERMEDIATE REPRESENTATION (IR)\nWeâ€™ve seen what it looks like to use Rose; letâ€™s now describe how it works. When a user calls Rose\nfunctions from their JavaScript program, we construct values of a data structure which we call\nRose: Efficient and Extensible Autodiff on the Web\n7\nthe Rose intermediate representation (IR). This IR is what Rose uses to compute forward-mode\nderivatives and transpose from forward-mode to reverse-mode. Figure 3 shows the Rose IRâ€™s abstract\nsyntax.3 In this section, we walk through the IRâ€™s semantics by way of examples. Consider this\nfunction that computes the sum of all elements from an array:\n1\ndef sum <n: Index >(v: [n]Real): Real =\n2\nlet z: Real = 0.0 in\n3\nlet t: (Real , [n]()) =\n4\naccum a from z in\n5\n[for i: n,\n6\nlet x: Real = v[i] in\n7\nlet u: () = a += x in\n8\nu\n9\n]\n10\nin\n11\nlet y: Real = fst t in\n12\ny\nLine 1 says that the function is generic over the size of the array, where the size is represented\nas a type n with the Index constraint. Line 4 uses the accum keyword to introduce the variable a\nof type &Real, in a new scope that lasts through its body ending on line 9. The ampersand here\nmeans that the type &Real denotes a mutable reference to a value of type Real, the same type as\nwhatever was written after the from keyword (z, in this case).\nThen, the accum body is an array constructor with index type n, as shown on line 5. The value\ntype of this array constructor is the unit type () so its resulting array type [n]() holds no data; its\nsole importance comes from the side effect it performs on line 7. Regardless, the accum construct\nalways returns both the final value contained in its reference (a, in this case) and the value of its\nbody, together, as a pair. Line 3 binds this pair to the variable t, after which lines 11 and 12 extract\nand return the desired value.\nThat function definition was quite verbose, as it strictly adhered to the syntax from Fig. 3 for\nclarity. In the remainder of this section, we will allow ourselves syntactic sugar to write expressions\nin places where the strict syntax requires variable names, with the understanding that these could\nbe desugared by introducing intermediate let bindings:\ndef sum <n: Index >(v: [n]Real): Real =\nfst (accum a from 0.0 in [for i: n, a += v[i]])\n4.1\nAutodiff\nTo make it easier for users to specify custom derivatives (see Section 5), we build on theoretical\nwork from Radul et al. [2023] decomposing reverse-mode autodiff into three transformations:\nforward-mode autodiff, unzipping, and transposition. We combine unzipping and transposition\ntogether, so we have only the two transformations. We will use the same running example from\nthe aforementioned paper:\n3Note that this is not what the user sees, because actual Rose programs are constructed using an embedded DSL inside\nJavaScript as a metaprogramming environment, as described in Sections 3 and 5. To make the distinction more clear, we use\nblack-and-white formatting for Rose IR, in contrast to the colored syntax highlighting we use for JavaScript code.\n8\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\ndef f(u: Real): Real =\nlet v: Real = sin(u) in\nlet w: Real = -v in\nw\nTo implement forward-mode autodiff, we first introduce a new type4 Tan which is identical to\nReal except for how it will later be treated by transposition in Section 4.2. Then we define a Dual\nnumber type to be an alias for the type (Real, Tan). Weâ€™ll notate type aliases with this syntax:\ntype Tan = Real\ntype Dual = (Real , Tan)\nWe assume that we are already given a JVP for sin. For instance:\ndef jvp_sin ((x, dx): Dual): Dual = (sin(x), dx * cos(x))\nThen, the forward-mode derivative of f is:\ndef jvp_f(u: Dual): Dual =\nlet v: Dual = jvp_sin(u) in\nlet v_re: Real = fst v in\nlet v_du: Tan = snd v in\nlet w_re: Real = -v_re in\nlet w_du: Tan = -v_du in\nlet w: Dual = (w_re , w_du) in\nw\nTo construct jvp_f, we first replaced all instance of the Real type with Dual. Then we walked\nthrough fâ€™s instructions in order and mechanically replaced each with one or more instructions\naccording to a small set of builtin templates for the basic arithmetic operations. For instance, the\ntemplate for a function call is to replace the function with its JVP, and the template for negation is\nto extract the fst and snd components of the dual number, negate each, and repackage them as a\nnew Dual number.\n4.2\nTransposition\nTransposition is more complicated. First we must transpose jvp_sin:\ntype Tape_sin = (Real)\ndef fwd_sin(x: Real): (Real , Tape_sin) =\n(sin(x), (cos(x)))\ndef bwd_sin(dx: &Real , dy: Real , (z): Tape_sin ): () =\ndx += dy * z\nAs shown here, transposition splits a JVP into two functions (a â€œnonlinearâ€ forward pass and a\nâ€œlinearâ€ backward pass) where the backward pass is effectively reversed (hence the name). Recall\nthat the fst component of each Dual number is of type Real; we call this the â€œnonlinearâ€ part.\nThe snd component, of type Tan, is called the â€œlinearâ€ part. For the forward pass, we first replace\nall instances of the Dual type with the Real type, but we also create a new type for the tape; that\n4We donâ€™t list this type in our abstract syntax because it can be erased once autodiff is done; it is only needed at the boundary\nbetween forward-mode autodiff and transposition.\nRose: Efficient and Extensible Autodiff on the Web\n9\nis, a collection of nonlinear intermediate values we compute along the way, which we will need\nlater when we compute the backward pass. Then we augment the return type to include both the\noriginal return type as well as the tape; in this case, the only value we need to store for the tape is\ncos(x).\nFor the backward pass, we first must again map Dual to Real, and then we must â€œflipâ€ the type\nof every parameter: value types become reference types, and vice versa. The jvp_sin function had\nonly one parameter, which here translates to one parameter dx: &Real. Then we add two more\nparameters: one corresponding to the derivative on the output which we will propagate backward,\nand one corresponding to the tape. We use the tape to retrieve any intermediate values we saved in\nthe forward pass, such as z here which is the cosine of the original nonlinear argument. Then we\nwalk through the function body in reverse, flipping the directionality of every linear expression we\nsee. Here we only had one linear expression dx * cos(x) which gets translated to dx += dy * z.\nThen, we can use the same process to transpose jvp_f:\ntype Tape_f = (Tape_sin)\ndef fwd_f(u: Real): (Real , Tape_f) =\nlet (v, t): (Real , Tape_sin) = fwd_sin(u) in\nlet w: Real = -v in\n(w, (t))\ndef bwd_f(du: &Real , dw: Real , (t): Tape_f ): () =\nlet dv: Real = -dw in\nbwd_sin(du, dv, t)\nHere the body of jvp_f called jvp_sin, so when we transpose, we split that call into two separate\ncalls to fwd_sin and bwd_sin. This example also shows how the backward pass runs â€œin reverseâ€:\nin the forward pass, we call fwd_sin before doing negation, then in the backward pass, we do\nnegation first and then call bwd_sin.\nIn the next section we talk about our implementation, which exposes operations to construct\nprograms in this IR, as well as the program transformations for differentiation and transposition\nthat we discussed here.\n5\nIMPLEMENTATION\nThe Rose implementation consists of a core written in Rust (which defines data structures for the\nIR, as well as program transformations for autodiff, and a backend to emit WebAssembly) and a\nbinding layer in JavaScript and TypeScript (which is used by all the JavaScript code examples in this\npaper). Rose maintains a garbage-collected DAG of functions, where an edge in the DAG means\nthat one functionâ€™s body contains a call to another function.\nTo construct a Rose function, a user uses fn from the Rose JavaScript bindings, which introduces\na new context with an empty function body of Rose IR. As the user calls Rose arithmetic operations\nlike add or sub, Rose appends those instructions to the body of the function in the current context.\nThen, once the user exits the scope defining their function, Rose finalizes the body and adds the\nnew function to the graph.\nWe compile the Rose core to WebAssembly [Haas et al. 2017], and use wasm-bindgen to expose\nto JavaScript in the npm package rose.5 As of rose version 0.4.5 (the latest at time of writing), our\nWasm binary size [Ayers et al. 2022] is 164.09 kB (63.31 kB after gzip), and our JavaScript wrapper\n5https://www.npmjs.com/package/rose\n10\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nimport { Real, add, compile, fn, jvp } from \"rose\";\nlet f = fn([Real], Real, (x) => x);\nfor (let i = 0; i < 20; ++i) {\nf = fn([Real], Real, (x) => add(f(x), f(x)));\n}\nconst g = await compile(jvp(f));\nconsole.log(g({ re: 2, du: 3 }));\nListing 3. A tower of functions where inlining would explode program size exponentially.\nlayer is 29.66 kB after minification (8.31 kB after gzip). For comparison, @tensorflow/tfjs-core\nversion 4.13.0 (again, latest at time of writing) is 478.00 kB after minification (85.32 kB after gzip).\nIn a web setting, having a relatively small binary like this is crucial; for example, there are projects\nthat package heavy compiler infrastructure like LLVM to WebAssembly [Soedirgo 2023], but those\nproduce binaries on the order of a hundred megabytes, causing unacceptable load times for end\nusers.\nWe faced several important design questions while implementing Rose, which go beyond the\ntheoretical concerns described in Section 4:\nâ€¢ How should users define Rose programs (Section 5.1)?\nâ€¢ How should we implement the data structures and core functionality for autodiff and our\ncompiler backend (Section 5.2)?\nâ€¢ How should users extend Rose when its builtin set of functionality is insufficiently expressive\n(Section 5.3)?\nIn the next three subsections, we discuss in turn the decisions we made to answer each of those\nquestions, and retrospectively evaluate those decisions in light of the system as a whole. In the\nspirit of transparency and community learning, we also highlight implementation design decisions\nwe believe were mistakes.\n5.1\nMetaprogramming\nInstead of parsing a textual syntax like conventional programming languages and some autodiff\nsystems like Dex [Paszke et al. 2021], or transforming an existing AST format like DiffTaichi [Hu et al.\n2019], we follow an approach more similar to TensorFlow [Abadi et al. 2015], PyTorch [Paszke et al.\n2017, 2019], and JAX [Bradbury et al. 2018; Frostig et al. 2018] in which users define Rose programs\nvia metaprogramming in the host language, which in this case is JavaScript or TypeScript. However,\nRose differs from prior work in that it allows the user to define functions via metaprogramming.\nFor instance, consider the code in Listing 3.\nHere, the user dynamically defines a tower of twenty-one functions which (very inefficiently)\ncompute the function ğ‘¥ â†¦â†’ 220ğ‘¥, then compiles its forward-mode derivative and calls that with\na primal input of two and an input tangent of three. If you remove the calls to fn which make\neach layer of the tower its own function, this example instead generates a line of IR code for each\nof the 220 individual terms. The result is that the browser simply refuses to compile the resulting\nWebAssembly code, because it has too many local variables; see also Section 6.4.\nThis may seem like a silly example, but this ability to explicitly demarcate reused chunks of code\nis crucial to keeping down compile times when Rose is used inside of real applications, including\nseveral examples in Section 6. This is one of Roseâ€™s central ideas.\nRose: Efficient and Extensible Autodiff on the Web\n11\nimport { Real, div, mul } from \"rose\";\nexport const powi = (x: Real, n: number): Real => {\nif (n < 0) return powi(div(1, x), -n);\nelse if (n == 0) return 1;\nelse if (n == 1) return x;\nelse if (n % 2 == 0) return powi(mul(x, x), n / 2);\nelse return mul(x, powi(mul(x, x), (n - 1) / 2));\n};\nListing 4. Exponentiation by squaring.\nimport { Real, add, mul, sub } from \"rose\";\nimport { powi } from \"./powi.js\";\nconst myPolynomial = fn([Real, Real], Real, (x, y) => {\nlet f = mul(2, powi(x, 3));\nf = add(f, mul(4, mul(powi(x, 2), y)));\nf = add(f, mul(x, powi(y, 5)));\nf = add(f, powi(y, 2));\nf = sub(f, 7);\nreturn f;\n});\nListing 5. A polynomial.\nBut as we described in Section 4, the Rose IR is limited: for instance, you currently cannot define\na recursive function in Rose IR. In many cases, though, it is useful to be able to perform recursion\non static data, even if no recursion is necessary after compilation. Rose places no restrictions\nwhatsoever on use of recursion in metaprogramming, so a user can easily define a function for\nexponentiation-by-squaring like in Listing 4. Then they can use this powi function wherever they\nplease, such as to compute the polynomial (ğ‘¥,ğ‘¦) â†¦â†’ 2ğ‘¥3 + 4ğ‘¥2ğ‘¦ + ğ‘¥ğ‘¦5 + ğ‘¦2 âˆ’ 7 in Listing 5. This\ngenerates efficient code by avoiding a more expensive call to a fully general power function that\nallows a floating-point exponent, like the one we used in Section 3.\nBy letting the user decide when to create a function abstraction and when to simply have their\ncode act as a macro to generate Rose IR, we get the best of both worlds, simultaneously allowing\nflexible definition of a computation graph while also maintaining compilation efficiency by allowing\nconstruction of reusable functions.\n5.2\nRust core\nAs mentioned above, we implemented the core of Rose in Rust. This has a couple advantages over\nimplementing directly in JavaScript or TypeScript. First, in theory it could mean that the compiler\nitself is faster. Second, it means that our Rust types defining our core IR, as well as our functionality\nfor differentiation, transposition, interpretation, and compilation are all available as Rust libraries\n(called â€œcratesâ€) which can then be used in a variety of contexts.\nHowever, this approach also has a major downside: it greatly limits extensibility on the JavaScript\nside. We package together all these Rose crates as dependencies of a single crate called rose-web,\n12\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nimport { Dual, Real, fn, mul, neg, opaque } from \"rose\";\nconst sin = opaque([Real], Real, Math.sin);\nconst cos = opaque([Real], Real, Math.cos);\nsin.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => {\nreturn { re: sin(x), du: mul(dx, cos(x)) };\n});\ncos.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => {\nreturn { re: cos(x), du: mul(dx, neg(sin(x))) };\n});\nListing 6. Definitions of sine and cosine functions with custom derivatives.\nimport { Dual, Real, fn, opaque } from \"rose\";\nconst print = opaque([Real], Real, (x) => {\nconsole.log(x);\nreturn x;\n});\nprint.jvp = fn([Dual], Dual, (z) => z);\nListing 7. A custom Rose function for print debugging.\nwhich is then compiled as a single WebAssembly binary. The JavaScript wrapper ships this We-\nbAssembly binary directly, rather than any of the Rust source code. This works great if the user\nonly needs Roseâ€™s builtin functionality for defining, differentiating, and compiling functions. But\na more advanced user might want to go beyond this: for instance, they may want to define an\noptimization pass over Rose IR which gets called before code generation, or perhaps even before\ndifferentiation. Currently this is impossible without forking the rose-web crate and recompiling it\nmanually into a new WebAssembly binary, which is then completely incompatible with any code\nthat was previously using the npm rose package.\nBecause of this, we retrospectively believe that this design decision was incorrect, and that it\nwould be preferable for Rose itself to be written in TypeScript instead. The performance advantage\nof WebAssembly is unclear [Yan et al. 2021]; while we show in Section 6.5 that it provides a\nsignificant speedup for generated code, it is harder to make a case that compilation time deserves\nsuch prioritization of raw speed.\n5.3\nCustom derivatives\nThere are two situations in which a user might want to define a custom derivative for a function.\nSection 3 showed an example of the first kind: when using an opaque function that calls out to\nJavaScript, Rose cannot peer into the body of that function, so it must be explicitly told the functionâ€™s\nderivative. As another example, Rose even lets you define custom derivatives for functions that\ndepend on each other, as in Listing 6.\nThe user can also define their own functions to use with opaque; for instance, one might want\nto define a print function for debugging purposes as in Listing 7, and tell Rose that the derivative\nof this function does nothing because, other than its side effect, it acts like the identity function:\nRose: Efficient and Extensible Autodiff on the Web\n13\nimport * as rose from \"rose\";\nimport { Dual, Real, div, fn, gt, mul, select } from \"rose\";\nconst max = (x: Real, y: Real) =>\nselect(gt(x, y), Real, x, y);\nconst sqrt = fn([Real], Real, (x) => rose.sqrt(x));\nsqrt.jvp = fn([Dual], Dual, ({ re: x, du: dx }) => {\nconst y = sqrt(x);\nconst dy = mul(dx, div(1 / 2, max(1e-5, y)));\nreturn { re: y, du: dy };\n});\nListing 8. A custom derivative of the square root function to avoid exploding gradients.\nThe other situation is when Rose has automatically constructed a derivative for a function, but\nthat derivative is unstable or otherwise exhibits some undesirable property. Rose allows the user\nto set a custom derivative for any function, not just opaque ones. For instance, by default the\nderivative of the square root function tends to infinity as the argument approaches zero, which\ncauses problems if it is ever called with a zero argument. To prevent this exploding-gradient\nproblem, we sometimes use a square root with a clamped derivative, as in Listing 8.\nIn all of these examples, notice that the user only needs to specify the JVP, and not the VJP;\nthis is true even if they later decide to use any of these functions in a VJP context, because Rose\nuses transposition (described in Section 4.2) to automatically construct a VJP from the JVP. A large\npart of the value of autodiff is that it ensures that the derivative remains in sync with the primal\nfunction by construction, so if we can also assist in keeping the forward-mode and reverse-mode\nderivatives in sync when one of them must be manually specified, this is a significant benefit for\nergonomics and maintainability on the user side.\n6\nEVALUATION\nIn this section we evaluate the design goals we laid out in Section 1. We discuss each design goal\nin a different subsection. Each subsection describes the method used to evaluate its goal and the\nresults obtained. Some of our methods involved gathering timing data from running real programs;\nall such numbers we report were measured in the V8 JavaScript engine (used in both Chrome and\nNode.js) on a 2020 MacBook Pro with M1 chip.\n6.1\nIn-browser test cases\nWe integrated Rose into Penrose [Ye et al. 2020], a web-based diagramming tool. Penrose allows\nusers to specify a diagram by constructing a custom numerical optimization problem, then runs a\nnumerical solver to rearrange the shapes in the diagram until it finds a local minimum. Penrose\npreviously used TensorFlow.js to compute derivatives; we contributed to Penrose by replacing its\nautodiff engine with one written in Rose.\nWe also used Rose to implement and augment two differentiable physics simulations from\nDiffTaichi [Hu et al. 2019]: billiards and robot (shown in Fig. 4). The billiards example is a\ndifferentiable simulation of pool combination shots. The program simulates rigid body collisions\nbetween a cue ball and object balls. Leveraging the differentiability of the simulation, a gradient\ndescent optimizer solves for the initial position and velocity of the cue ball to send a designated\n14\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nE\nF\nA\nB\nC\nD\nG\nFig. 4. Differentiable simulations from DiffTaichi [Hu et al. 2019] made interactive using Rose, where interac-\ntive features are denoted by hand icons. Left: billiards simulator that optimizes D for cue ball angle and\nspeed such that the object ball A reaches the target B . Right: mass-spring robot controlled by a neural\nnet trained G with a designated goal D . Both simulations can be replayed by dragging the sliders at any\npoint D and G .\nobject ball to a target position. The robot example simulates a robot made of a mass-spring system,\nwhere springs are actuated to move the robot towards a goal position. A neural network controller\nis trained on simulator gradients to update the spring actuation magnitude over time.\nAll three examples were implemented using Roseâ€™s TypeScript binding, and transpiled to\nJavaScript. Supplementary materials include the source code for both the original and Rose versions\nof all three examples. They all run in major browsers such as Safari and Chrome, showing that we\nachieved D1. To showcase the benefits of running in the web browser (D1), we added interactive\nfeatures to the DiffTaichi applications (Fig. 4). For instance, the DiffTaichi version of billiards is\na command-line application that outputs a series of static images based on hard-coded parameters\nfor the choice of the object ball and goal position. The Rose version allows the user to interactively\nexplore the simulator by selecting the object ball (Fig. 4 A ), moving the goal position (Fig. 4 B ),\noptimizing the cue ball position (Fig. 4 D ), and re-playing the simulation (Fig. 4 C ).\n6.2\nFunction dynamism\nMetaprogramming using JavaScript enables the user to dynamically generate complex compu-\ntation graphs that are impossible to specify with the Rose IR alone. For instance, the bboxGroup\nfunction in Listing 9 computes the bounding box of a Group in Penrose, a recursive collection of\nshapes. For non-collection shape types such as Circle, we ported the TensorFlow.js implementa-\ntion to Rose easily, e.g. bboxCircle. However, bboxGroup needs to recurse over the Group data\nstructure to find out the bounding boxes of individual shapes before aggregating them into the\nfinal bounding box. This requires conditional dispatch of (1) Rose functions based on a discrete tag\n(shape.kind) and (2) recursive calls to bboxGroup to handle nested groups.\nFigure 5 shows an example of calling bboxGroup on nested groups of shapes. The diagram in\nFig. 5 (left) has 1 group containing the whole diagram, and 3 subgroups of molecules that contain\nRose: Efficient and Extensible Autodiff on the Web\n15\n1\nconst bboxGroup = (shapes) => {\n2\nconst bboxes = shapes.map(bbox);\n3\nconst left = bboxes.map((b) => b.left).reduce(min);\n4\nconst right = bboxes.map((b) => b.right).reduce(max);\n5\nconst bottom = bboxes.map((b) => b.bottom).reduce(min);\n6\nconst top = bboxes.map((b) => b.top).reduce(max);\n7\nreturn { left, right, bottom, top };\n8\n};\n9\n10\nconst bboxCircle = fn([Circle], Rectangle,\n11\n({ center: [x, y], radius: r }) => {\n12\nconst left = sub(x, r);\n13\nconst right = add(x, r);\n14\nconst bottom = sub(y, r);\n15\nconst top = add(y, r);\n16\nreturn { left, right, bottom, top };\n17\n},\n18\n);\n19\n20\nconst bbox = (shape) => {\n21\nswitch (shape.kind) {\n22\ncase \"Rectangle\": return shape.value;\n23\ncase \"Circle\": return bboxCircle(shape.value);\n24\ncase \"Group\": return bboxGroup(shape.value);\n25\n}\n26\n};\nListing 9. Examples of JavaScript metaprogramming to construct Rose functions for recursive data structures.\nswitch\nswitch\nbboxGroup(\n)\nbboxGroup(\n)\nbboxGroup(\n)\nbboxGroup(\n)\nbboxCircle(\n)\nbboxText(\n)\nbboxLine(\n)\nFig. 5. In Penrose, we used JavaScript to programmatically generate Rose functions. Left: a figure comprised\nof a top-level group containing all molecules and sub-groups for each molecule. Right: the bboxGroup function\nconditionally generates Rose functions or recursively calls itself based on the shape type.\n16\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nshapes such as Text and Circle. Figure 5 shows how bboxGroup uses JavaScript language features\nto compose Rose functions into a computation graph, denoting JavaScript constructs in gray and\nRose functions in red. First, for each member shape, we switch on shape.kind to determine\nwhether to call (a) individual Rose bounding box functions like bboxCircle or (b) recurse to call\nbboxGroup. Then, after all the child bounding boxes are computed, we use JavaScript map and\nreduce to aggregate the results via Rose min and max functions.\n6.3\nCustom derivatives\nBoth the Penrose and DiffTaichi examples require custom functions (Section 3.2) beyond the\narithmetic operations built into the Rose IR. For instance, in the robot example, the neural network\ncontroller uses the hypobolic tangent activation function in its hidden layer. Similar to the examples\nin previous sections, we implemented a differentiable tanh function using Roseâ€™s opaque and jvp.\nUsing the same mechanism, we also implemented a larger suite of custom differentiable functions\nin Penrose, including cbrt, atanh, expm1, and many others. This demonstrates the effectiveness of\nRoseâ€™s facilities for custom derivatives (D2) and shows that custom derivatives are useful for real\napplications.\n6.4\nWriting pointful programs\nFor D3, we compare pointful programs rewritten in Rose from other state-of-the-art tools. The\noriginal versions of Penrose, billiards, and robot are naturally written as pointful programs. In\nPenrose, bboxCircle (line 10 of Listing 9) computes the bounding box by performing arithmetic on\nscalar values for the center and radius of a circle. In DiffTaichi, both billiards and robot involve\nhand-crafted pointful programs for differentiable simulations. For instance, apply_spring_force\n(Fig. 6) loops through individual springs in the robot, computing the force on the spring based on\nscalar-valued parameters, and scatter forces to end points of springs.\nBecause Rose is designed for writing pointful programs (D3), translating both Penrose and\nDiffTaichi source programs to Rose is straightforward and largely preserves the structures of the\nprograms. For instance, when translating the Python programs from DiffTaichi into TypeScript\nand Rose, as shown in Fig. 6, DiffTaichi kernels can be translated one-to-one to Rose functions.\nNote that in the case of DiffTaichi, the Rose abstraction of fn is not only useful for one-to-\none translation from DiffTaichi, but also necessary for running the simulator in browsers. Major\nWebAssembly engines have limits on WebAssembly binary size and on the number of local variables\nin each function. While it is possible to encapsulate much of the simulation code of billiards and\nrobot in bigger JavaScript functions, the compiled size and local counts of these functions would\nquickly exceed these limits and would not run in the browser (D1). Therefore, segmenting the\nsource into functional units of fns effectively reduces the size of emitted WebAssembly functions\nand modules, avoiding these errors and reducing compile times (D5).\n6.5\nPerformance\nWe ran both a TensorFlow.js version of Penrose and a Rose version on a set of 173 â€œregistryâ€\ndiagrams, and measured the amount of time it took for each autodiff engine to perform any necessary\ncompilation, plus the time taken by the Penrose L-BFGS [Liu and Nocedal 1989] optimization engine\nto converge on each diagram.\nOptimizing the layout of these diagrams involves a wide range of mathematical operations on\nscalars. These include simple operations like finding the distance between points. The diagrams\nalso use sophisticated mathematics like Minkowski addition, KL divergence, and silhouette points.\nThe Penrose performance benchmark therefore contains a wide range of real-world AD functions.\nRose: Efficient and Extensible Autodiff on the Web\n17\n@ti.kernel\ndef apply_spring_force(t: ti.i32):\nfor i in range(n_springs):\na = spring_anchor_a[i]\nb = spring_anchor_b[i]\npos_a = x[t, a]\npos_b = x[t, b]\ndist = pos_a - pos_b\nlength = dist.norm() + 1e-4\ntarget_length = spring_length[i] *\n(1.0 + spring_actuation[i] * act[t, i])\nimpulse = dt * (length - target_length) *\nspring_stiffness[i] / length * dist\nti.atomic_add(v_inc[t + 1, a], -impulse)\nti.atomic_add(v_inc[t + 1, b], impulse)\nconst apply_spring_force = fn(\n[Objects, Act], Objects, (x, act) => {\nconst v_inc = [];\nfor (let i = 0; i < n_objects; i++)\nv_inc.push([0, 0]);\nfor (let i = 0; i < n_springs; i++) {\nconst spring = robot.springs[i];\nconst a = spring.object1;\nconst b = spring.object2;\nconst pos_a = x[a];\nconst pos_b = x[b];\nconst dist = vsub2(pos_a, pos_b);\nconst length = add(norm(dist), 1e-4);\nconst target_length = mul(spring.length,\nadd(1, mul(act[i], spring.actuation))\n);\nconst impulse =\nvmul(div(mul(dt * spring.stiffness,\nsub(length, target_length)),\nlength),\ndist);\nv_inc[a] = vsub2(v_inc[a], impulse);\nv_inc[b] = vadd2(v_inc[b], impulse);\n}\nreturn v_inc;\n});\nFig. 6. A function that applies spring actuation on the mass-spring robot model in the robot example, written\nin DiffTaichi (Left) and Rose (Right). The translation from DiffTaichi to Rose is straightforward.\n10 2\n100\n102\n104\nTensorFlow.js (seconds)\n10 3\n10 2\n10 1\n100\nRose (seconds)\n100\n102\n104\n106\nTensorFlow.js / Rose\n0.0\n0.1\n0.2\n0.3\n0.4\nprobability density\nFig. 7. Left: Log-log scatterplot of Penrose diagram optimization time with TensorFlow.js versus Rose.\nRight: Log-scale kernel density estimate (KDE) plot of the optimization time of TensorFlow.js to Rose.\nNote that we specifically include the time it takes for Rose to do autodiff, transposition, and\nWasm compilation, despite the fact that TensorFlow.js does not have an analogous compilation\nstep. On the surface this puts Rose at a disadvantage, but per D5 we believe that fast compilation\ntime is essential when constructing Rose functions dynamically in a user-facing web application,\nas Penrose does.\n18\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nFigure 7 shows the results. Supplementary materials include side-by-side comparisons of the\ngenerated SVG diagrams, to show that both versions produced similar results. We omitted 10 of the\n173 diagrams from our data analysis:\nâ€¢ 9 NaN failures: Penrose aborts if it detects a â€œnot-a-numberâ€ (NaN) value in the gradient\nas it is optimizing. This occurred in the TensorFlow.js version of Penrose for nine diagrams.\nThe Rose version of Penrose did not encounter NaNs for these programs.\nâ€¢ 1 timeout: For one diagram, we stopped the TensorFlow.js version of Penrose after it had\nrun for over 24 hours. The Rose version of Penrose took 42 milliseconds to compile and optimize\nthis diagram.\nWe used the \"cpu\" backend for TensorFlow.js because we found that, for pointful programs,\nit was faster than their GPU backend. To double-check this, we took the 88 diagrams (over half)\nthat were quickest to run with TensorFlow.js, and also ran them with @tensorflow/tfjs-node\nand @tensorflow/tfjs-node-gpu, which they claim are faster than the \"cpu\" backend. We found\nthat the Node backend is 79% slower (median ratio) than the \"cpu\" backend, and the Node GPU\nbackend is 75% slower (median ratio) than the \"cpu\" backend. Also, those backends are unable to\nrun in a browser, unlike the \"cpu\" backend, so they would be inappropriate for a direct comparison\nto Rose.\nThe quartiles for the ratio of TensorFlow.js optimization time to Rose optimization time were\n37Ã—, 173Ã—, and 598Ã—. These results show that Rose provides an enormous advantage over Ten-\nsorFlow.js (the state-of-the-art for autodiff on the web) for pointful programs like those found in\nPenrose diagrams. Because these numbers include both compile time and optimization time, this\ndemonstrates the performance of Rose according to our goals D5 and D6.\nAs discussed earlier in Section 6.4, Roseâ€™s ability to define separate functions in a graph (rather\nthan just a single graph of scalar or tensor values) is crucial to producing small enough WebAssem-\nbly binaries to feed to the browser. To investigate whether WebAssembly brought significant\nperformance gains in the first place to be worth facing that challenge, we compared against a\nmodified version of Rose which emits JavaScript code instead of WebAssembly. This experiment\ngave quartile slowdowns of 10%, 49%, and 100% for optimization of Penrose diagrams, showing that\nWebAssembly provides a significant advantage over JavaScript as a compilation target for Rose.\n7\nRELATED WORK\nThere is a fair amount of existing work on automatic differentiation theory and practice, and also\nsome work on embedding rich DSLs inside a general-purpose programming language.\nOne of the first to explore reverse-mode autodiff from a compiler perspective was Speelpenning\n[1980], also showing the value of reverse-mode autodiff over forward-mode in many situations.\nSince then, many presentations of autodiff as a program transformation have appeared, such as\nInnes [2018] introducing Zygote, which handles a range of language constructs far broader than\nwhat we support in this paper. The reason for our higher level of conservatism is that we follow the\nmore pure functional approach from Paszke et al. [2021] to leave open room to target the GPU (see\nSection 8.2) and automatically parallelize programs and their derivatives. Specifically, we build on\ntheoretical work from Radul et al. [2023] to split reverse-mode autodiff into forward-mode autodiff\nfollowed by transposition.\nOur implementation builds on important theoretical groundwork. For instance, Bernstein et al.\n[2020] show cost preservation on a pure tensor language and achieve sparsity via a theoretical\nâ€œIverson bracketâ€ construct that hides nontrivial constant- or logarithmic-factor costs in practice.\nOn the other hand, Wang et al. [2019] handle control flow of a functional language, but do not\nsupport array parallelism. Importantly, none these prior approaches use our transposition approach\nRose: Efficient and Extensible Autodiff on the Web\n19\nto decompose forward-mode autodiff as a part of reverse-mode autodiff, which is vital for our\nimplementation to ease definition of custom derivatives.\nRose supports higher-order derivatives because its core IR is closed under differentiation and\ntransposition. A more sophisticated approach we donâ€™t explore here would be derivative tow-\ners [Karczmarczuk 1998; Pearlmutter and Siskind 2007], sometimes called â€œTaylor towersâ€ because\nthey use Taylor expansions instead of the chain rule. We would be interested to see how derivative\ntowers can be combined with our approach in future work.\nVenturing more into the systems landscape, weâ€™ve already discussed the Python libraries Tensor-\nFlow [Abadi et al. 2015], PyTorch [Paszke et al. 2017, 2019], JAX [Bradbury et al. 2018; Frostig et al.\n2018], and Taichi [Hu et al. 2019], as well as the web library TensorFlow.js [Smilkov et al. 2019].\nC++ autodiff systems include the popular Tapenade [Hascoet and Pascual 2013] for reverse-mode,\nand the more recent TinyAD [Schmidt et al. 2022] for forward-mode. Zygote [Innes et al. 2019]\nprovides autodiff for Julia. Both of the latter two languages are often compiled via LLVM [Lattner\nand Adve 2004], for which Enzyme [Moses and Churavy 2020; Moses et al. 2021, 2022] can produce\nderivatives by operating only over IR rather than source code. For graphics programming, Ağ›¿ [Yang\net al. 2022] and Dr.Jit [Jakob et al. 2022], can be used to differentiate shaders. And now, Rose starts\nto bring modern autodiff research to the web.\nRose makes the explicit decision to embed itself inside JavaScript/TypeScript as a library, some-\ntimes called a domain-specific embedded language (DSEL) [Hudak 1996], to take advantage of all\nthe facilities of the host language for metaprogramming, as described in Section 5.1. We believe\nthat there is room for future work on what makes different host languages more or less suitable\nfor creating these sorts of embedded DSLs. For instance, Rose makes clever use of JavaScriptâ€™s\nSymbol and Proxy types to make it so that code dealing with arrays and structs looks like normal\nJavaScript code, while actually intercepting property accesses to generate Rose IR. On the other\nhand, JavaScript does not support general operator overloading, so Rose users must write add and\nsub instead of + and -. Further, it would be difficult for Rose to record source location information\nfor debugging purposes, because JavaScript does not provide anything analogous to Pythonâ€™s\ninspect module. One method to do this could be to have every Rose function throw an Error and\nthen immediately catch it, and use string processing to extract the callerâ€™s source location from the\nerror backtrace, but this would be slow and non-portable.\n8\nDISCUSSION AND FUTURE WORK\nHere we discuss a few topics that we did not have a chance to explore in this paper, but would like\nto note as potential directions for future work.\n8.1\nLinear types\nRadul et al. [2023] provide a cost model for a linearly-typed language and prove that their three-pass\nautodiff scheme preserves both cost and code size. We would like to extend their results to Rose IR,\nwhich includes array size polymorphism. It is not immediately obvious what should be the cost\nsemantics here; consider this function:\ndef copy <n: Index , T: Value >(x: T): [n]T = [for i: n, x]\nLet $T denote the cost of cloning a value of type T, and let |n| denote the number of elements\nin the type n. Depending on whether the implementation allows structural sharing, the cost of\nthis copy function might be either ğ‘‚(|n|) or ğ‘‚(|n| Ã— $T). Rose IR shares many similarities with\nPaszke et al. [2021]; from direct correspondence with the authors of that paper, we know that Dex\nperforms cloning here and so their cost is the latter. The current implementation of Rose does allow\n20\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nstructural sharing, so our cost is the former. But then it is unclear how to translate this cost to the\nderivative; for instance, here is the reverse-mode derivative of copy:\ndef fwd_copy <n: Index , T: Value >(x: T): ([n]T, ()) =\n([for i: n, x], ())\ndef bwd_copy <n: Index , T: Value >(dx: &T, dv: [n]T, (): ()) =\nlet _ = [for i: n, dx += dv[i]] in\n()\nThe cost of bwd_copy is ğ‘‚(|n| Ã— $T), which appears unavoidable because we donâ€™t control the\nstructure of dv. We believe that to address this, it may be valuable to use a type system that is linear\nfor primal values, unlike the type system from Radul et al. [2023] which only enforces linearity for\ntangents.\n8.2\nWebGPU\nRose currently targets WebAssembly, which runs on the CPU. As we showed in Section 6.5, this\nalready provides an enormous performance advantage for pointful programs when compared to the\nstate-of-the-art for autodiff on the web. However, we would also like to pursue further performance\ngains by implementing a backend that targets WebGPU [Kenwright 2022], a modern GPU interface\nfor the web. Many elements of Rose IR are inspired by Paszke et al. [2021] to be friendly to automatic\nparallelization, such as the for construct and accumulate-only reference types; regardless, some\nengineering effort would still be required to compile for the GPU.\n8.3\nTree shaking\nRoseâ€™s mechanism for constructing functions has a subtle interaction with popular JavaScript\nbundling techniques, which are used to shrink the size of a websiteâ€™s source code to actually ship\nto the clientâ€™s browser; this is sometimes referred to as â€œtree shaking.â€ Specifically, defining a Rose\nfunction using fn immediately executes its body once to emit Rose IR. Even if this is conceptually\na â€œpureâ€ operation, JavaScript build tools count it as a side effect, so they do not eliminate unused\nRose function definitions from a bundle. This concern would be more important if a user imports\nsome Rose functions from a large library. We would like to investigate mechanisms to manage\nbundle sizes in such cases.\n9\nCONCLUSION\nThis paper introduces Rose, an embedded domain-specific language for automatic differentiation of\nweb programs. Rose is targeted at applications where users can define custom functions involving\nscalar math that need to be differentiated. These applications are not well suited to the most popular\nautodiff libraries since they focus on statically-defined block operations on tensors. It is our hope\nthat Rose will enable a flourishing ecosystem of simulations, custom optimization engines, and\ndifferentiable graphics applications on the web.\nYou may say Iâ€™m a derivative,\nbut Iâ€™m not the only one.\nI hope someday youâ€™ll join us,\nand the world will live as one.\nYou must be the derivative you wish to see in the world.\nYou have derivatives in your head.\nRose: Efficient and Extensible Autodiff on the Web\n21\nYou have derivatives in your shoes.\nYou can steer yourself any direction you choose.\nACKNOWLEDGMENTS\nThanks to Adam Paszke for corresponding about JAX and Dex. The Rose icons were created by\nAaron Weiss; we use them via the CC BY 4.0 license.\nREFERENCES\nMartÃ­n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\nJeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Yangqing\nJia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dandelion ManÃ©, Rajat Monga, Sherry Moore,\nDerek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\nVincent Vanhoucke, Vijay Vasudevan, Fernanda ViÃ©gas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\nYuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. https:\n//www.tensorflow.org/ Software available from tensorflow.org.\nHudson Ayers, Evan Laufer, Paul Mure, Jaehyeon Park, Eduardo Rodelo, Thea Rossman, Andrey Pronin, Philip Levis, and\nJohnathan Van Why. 2022. Tighten Rustâ€™s Belt: Shrinking Embedded Rust Binaries. In Proceedings of the 23rd ACM\nSIGPLAN/SIGBED International Conference on Languages, Compilers, and Tools for Embedded Systems (San Diego, CA,\nUSA) (LCTES 2022). Association for Computing Machinery, New York, NY, USA, 121â€“132.\nhttps://doi.org/10.1145/\n3519941.3535075\nGilbert Bernstein, Michael Mara, Tzu-Mao Li, Dougal Maclaurin, and Jonathan Ragan-Kelley. 2020. Differentiating a Tensor\nLanguage. arXiv:2008.11256 [cs.PL]\nJames Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin, George Necula, Adam\nPaszke, Jake VanderPlas, Skye Wanderman-Milne, et al. 2018. JAX: composable transformations of Python+ NumPy\nprograms. (2018).\nClifford. 1871. Preliminary sketch of biquaternions. Proceedings of the London Mathematical Society 1, 1 (1871), 381â€“395.\nRoy Frostig, Matthew James Johnson, and Chris Leary. 2018. Compiling machine learning programs via high-level tracing.\nSystems for Machine Learning 4, 9 (2018).\nAndreas Haas, Andreas Rossberg, Derek L. Schuff, Ben L. Titzer, Michael Holman, Dan Gohman, Luke Wagner, Alon\nZakai, and JF Bastien. 2017. Bringing the Web up to Speed with WebAssembly. SIGPLAN Not. 52, 6 (jun 2017), 185â€“200.\nhttps://doi.org/10.1145/3140587.3062363\nLaurent Hascoet and ValÃ©rie Pascual. 2013. The Tapenade Automatic Differentiation Tool: Principles, Model, and Specification.\nACM Trans. Math. Softw. 39, 3, Article 20 (may 2013), 43 pages. https://doi.org/10.1145/2450153.2450158\nYuanming Hu, Luke Anderson, Tzu-Mao Li, Qi Sun, Nathan Carr, Jonathan Ragan-Kelley, and FrÃ©do Durand. 2019. DiffTaichi:\nDifferentiable Programming for Physical Simulation. CoRR abs/1910.00935 (2019). arXiv:1910.00935 http://arxiv.org/abs/\n1910.00935\nPaul Hudak. 1996. Building domain-specific embedded languages. Acm computing surveys (csur) 28, 4es (1996), 196â€“es.\nMichael Innes. 2018. Donâ€™t Unroll Adjoint: Differentiating SSA-Form Programs. https://doi.org/10.48550/ARXIV.1810.07951\nMike Innes, Alan Edelman, Keno Fischer, Chris Rackauckas, Elliot Saba, Viral B Shah, and Will Tebbutt. 2019. A Differentiable\nProgramming System to Bridge Machine Learning and Scientific Computing. arXiv:1907.07587 [cs.PL]\nWenzel Jakob, SÃ©bastien Speierer, Nicolas Roussel, and Delio Vicini. 2022. DR.JIT: A Just-in-Time Compiler for Differentiable\nRendering. ACM Trans. Graph. 41, 4, Article 124 (jul 2022), 19 pages. https://doi.org/10.1145/3528223.3530099\nJerzy Karczmarczuk. 1998. Functional Differentiation of Computer Programs. In Proceedings of the Third ACM SIGPLAN\nInternational Conference on Functional Programming (Baltimore, Maryland, USA) (ICFP â€™98). Association for Computing\nMachinery, New York, NY, USA, 195â€“203. https://doi.org/10.1145/289423.289442\nBenjamin Kenwright. 2022. Introduction to the WebGPU API. In ACM SIGGRAPH 2022 Courses (Vancouver, British\nColumbia, Canada) (SIGGRAPH â€™22). Association for Computing Machinery, New York, NY, USA, Article 10, 184 pages.\nhttps://doi.org/10.1145/3532720.3535625\nC. Lattner and V. Adve. 2004. LLVM: a compilation framework for lifelong program analysis & transformation. In International\nSymposium on Code Generation and Optimization, 2004. CGO 2004. 75â€“86. https://doi.org/10.1109/CGO.2004.1281665\nDong C Liu and Jorge Nocedal. 1989. On the limited memory BFGS method for large scale optimization. Mathematical\nprogramming 45, 1-3 (1989), 503â€“528.\nWilliam Moses and Valentin Churavy. 2020. Instead of Rewriting Foreign Code for Machine Learning, Automatically\nSynthesize Fast Gradients. In Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell,\nM. F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 12472â€“12485. https://proceedings.neurips.cc/paper/2020/\n22\nSam Estep, Raven Rothkopf, Wode Ni, and Joshua Sunshine\nfile/9332c513ef44b682e9347822c2e457ac-Paper.pdf\nWilliam S. Moses, Valentin Churavy, Ludger Paehler, Jan HÃ¼ckelheim, Sri Hari Krishna Narayanan, Michel Schanen, and\nJohannes Doerfert. 2021. Reverse-Mode Automatic Differentiation and Optimization of GPU Kernels via Enzyme.\nIn Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (St.\nLouis, Missouri) (SC â€™21). Association for Computing Machinery, New York, NY, USA, Article 61, 16 pages.\nhttps:\n//doi.org/10.1145/3458817.3476165\nWilliam S. Moses, Sri Hari Krishna Narayanan, Ludger Paehler, Valentin Churavy, Michel Schanen, Jan HÃ¼ckelheim, Johannes\nDoerfert, and Paul Hovland. 2022. Scalable Automatic Differentiation of Multiple Parallel Paradigms through Compiler\nAugmentation. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and\nAnalysis (Dallas, Texas) (SC â€™22). IEEE Press, Article 60, 18 pages.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Alban Desmaison,\nLuca Antiga, and Adam Lerer. 2017. Automatic differentiation in PyTorch. 31st Conference on Neural Information\nProcessing Systems (Oct. 2017). https://openreview.net/forum?id=BJJsrmfCZ\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin,\nNatalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison,\nAlykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. PyTorch: An\nImperative Style, High-Performance Deep Learning Library. In Advances in Neural Information Processing Systems,\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'AlchÃ©-Buc, E. Fox, and R. Garnett (Eds.), Vol. 32. Curran Associates, Inc.\nhttps://proceedings.neurips.cc/paper_files/paper/2019/file/bdbca288fee7f92f2bfa9f7012727740-Paper.pdf\nAdam Paszke, Daniel D. Johnson, David Duvenaud, Dimitrios Vytiniotis, Alexey Radul, Matthew J. Johnson, Jonathan Ragan-\nKelley, and Dougal Maclaurin. 2021. Getting to the Point: Index Sets and Parallelism-Preserving Autodiff for Pointful\nArray Programming. Proc. ACM Program. Lang. 5, ICFP, Article 88 (aug 2021), 29 pages. https://doi.org/10.1145/3473593\nBarak A. Pearlmutter and Jeffrey Mark Siskind. 2007. Lazy Multivariate Higher-Order Forward-Mode AD. In Proceedings of\nthe 34th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Nice, France) (POPL â€™07).\nAssociation for Computing Machinery, New York, NY, USA, 155â€“160. https://doi.org/10.1145/1190216.1190242\nAlexey Radul, Adam Paszke, Roy Frostig, Matthew J. Johnson, and Dougal Maclaurin. 2023. You Only Linearize Once:\nTangents Transpose to Gradients. Proc. ACM Program. Lang. 7, POPL, Article 43 (jan 2023), 29 pages. https://doi.org/10.\n1145/3571236\nP. Schmidt, J. Born, D. Bommes, M. Campen, and L. Kobbelt. 2022.\nTinyAD: Automatic Differentiation in Geome-\ntry Processing Made Simple.\nComputer Graphics Forum 41, 5 (2022), 113â€“124.\nhttps://doi.org/10.1111/cgf.14607\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14607\nDaniel Smilkov, Nikhil Thorat, Yannick Assogba, Charles Nicholson, Nick Kreeger, Ping Yu, Shanqing Cai, Eric Nielsen,\nDavid Soegel, Stan Bileschi, Michael Terry, Ann Yuan, Kangyi Zhang, Sandeep Gupta, Sarah Sirajuddin, D Sculley, Rajat\nMonga, Greg Corrado, Fernanda Viegas, and Martin M Wattenberg. 2019. TensorFlow.js: Machine Learning For The\nWeb and Beyond. In Proceedings of Machine Learning and Systems, A. Talwalkar, V. Smith, and M. Zaharia (Eds.), Vol. 1.\n309â€“321.\nBobbie Soedirgo. 2023. Compile and run LLVM IR in the browser. https://github.com/soedirgo/llvm-wasm original-date:\n2021-02-24T14:29:16Z.\nBert Speelpenning. 1980. Compiling fast partial derivatives of functions given by algorithms. University of Illinois at\nUrbana-Champaign.\nFei Wang, Daniel Zheng, James Decker, Xilun Wu, GrÃ©gory M. Essertel, and Tiark Rompf. 2019. Demystifying Differentiable\nProgramming: Shift/Reset the Penultimate Backpropagator. Proc. ACM Program. Lang. 3, ICFP, Article 96 (jul 2019),\n31 pages. https://doi.org/10.1145/3341700\nYutian Yan, Tengfei Tu, Lijian Zhao, Yuchen Zhou, and Weihang Wang. 2021. Understanding the Performance of Webassembly\nApplications. In Proceedings of the 21st ACM Internet Measurement Conference (Virtual Event) (IMC â€™21). Association for\nComputing Machinery, New York, NY, USA, 533â€“549. https://doi.org/10.1145/3487552.3487827\nYuting Yang, Connelly Barnes, Andrew Adams, and Adam Finkelstein. 2022. Ağ›¿: Autodiff for Discontinuous Programs â€“\nApplied to Shaders. In ACM SIGGRAPH, to appear.\nKatherine Ye, Wode Ni, Max Krieger, Dor Maâ€™ayan, Jenna Wise, Jonathan Aldrich, Joshua Sunshine, and Keenan Crane.\n2020. Penrose: From Mathematical Notation to Beautiful Diagrams. ACM Trans. Graph. 39, 4, Article 144 (aug 2020),\n16 pages. https://doi.org/10.1145/3386569.3392375\n"
}