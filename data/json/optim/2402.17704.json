{
    "optim": "Transfer Learning Bayesian Optimization to Design Competitor DNA\nMolecules for Use in Diagnostic Assays\nRuby Sedgwick1,2, John P. Goertz1, Molly M. Stevens1,3, Ruth Misener2, and Mark van der\nWilk2\n1Department of Materials, Department of Bioengineering and Institute of Biomedical\nEngineering, Imperial College London, London\n2Department of Computing, Imperial College London, London\n3Department of Physiology, Anatomy and Genetics, Department of Engineering Science, and\nKavli Institute for Nanoscience Discovery, University of Oxford, Oxford\nFebruary 28, 2024\nAbstract\nWith the rise in engineered biomolecular devices, there is an increased need for tailor-made\nbiological sequences. Often, many similar biological sequences need to be made for a specific\napplication meaning numerous, sometimes prohibitively expensive, lab experiments are nec-\nessary for their optimization. This paper presents a transfer learning design of experiments\nworkflow to make this development feasible. By combining a transfer learning surrogate\nmodel with Bayesian optimization, we show how the total number of experiments can be\nreduced by sharing information between optimization tasks. We demonstrate the reduction\nin the number of experiments using data from the development of DNA competitors for use\nin an amplification-based diagnostic assay. We use cross-validation to compare the predic-\ntive accuracy of different transfer learning models, and then compare the performance of the\nmodels for both single objective and penalized optimization tasks.\n1\nIntroduction\nTailoring biological sequences, such as oligonucleotides or proteins, for specific applications is a common\nchallenge in bioengineering. These engineered molecules have a variety of uses including in biosensors\n(Hua et al., 2022; Deng et al., 2023; Goertz et al., 2023), medical therapeutics (Badeau et al., 2018;\nBlakney et al., 2019; Ebrahimi and Samanta, 2023) and bio-computing (Siuti et al., 2013; Qian et al.,\n2011; Lv et al., 2021). However, development often requires expensive or time consuming experiments,\nmeaning good experimental design is necessary to optimize the biological sequences within the experimen-\ntal budget (Cox and Reid, 2000). This also leads to better analysis, especially when there are interaction\neffects between input factors, which is common in biological experiments (Kreutz and Timmer, 2009;\nPolitis et al., 2017; Papaneophytou, 2019; Fellermann et al., 2019; Narayanan et al., 2020; Gilman et al.,\n2021).\nIterative experimental designs have the advantage of using information from previous experiments to\ninform future ones. Bayesian optimization is an iterative global black-box optimization strategy (Snoek\net al., 2012; Shahriari et al., 2016) which has proven effective for the design of biomolecular experiments\nincluding antibody development (Khan et al., 2023), extracellular vesicle production (Bader et al., 2023),\n1\narXiv:2402.17704v1  [q-bio.QM]  27 Feb 2024\ndesign and manufacturing of proteins and tissues (Romero et al., 2013; Mehrian et al., 2018; Narayanan\net al., 2021; Gamble et al., 2021), validation of molecular networks (Sedgwick et al., 2020) and vaccine\nproduction (Rosa et al., 2022). In Bayesian optimization, a surrogate model, usually a Gaussian process,\nof the system is built using data and an acquisition function decides which data point to collect next.\nGaussian processes are a powerful tool for designing biological experiments in low data regimes due to\ntheir uncertainty estimates (Hie et al., 2020).\nWhen many similar biological sequences need to be designed, it can be even harder to optimize\nall the sequences within the experimental budget.\nOptimizing each sequence from scratch discards\nuseful information from previous tasks, meaning more experiments are required. An alternative is to\nuse transfer learning — a technique that improves the learning of new sequences by sharing information\nbetween optimization tasks (Zhuang et al., 2021).\nAs we require our surrogate model to be data efficient and have uncertainty quantification, we consider\nfour transfer learning Gaussian process models: an average Gaussian process (AvgGP), the multi-output\nGaussian process (MOGP), the linear model of coregionalisation (LMC) and the latent variable multi-\noutput Gaussian process (LVMOGP). The key difference between these Gaussian process models lies in\ntheir handling of correlations between outputs: from no correlation in the MOGP to non-linear correlation\nin the LVMOGP.\nWe apply these surrogate model in conjunction with Bayesian optimization for efficient optimization\nof bio-molecules, as shown in Figure 1. We focus specifically on the development of a new modular\ndiagnostic assay, based on competitive polymerase chain reaction (PCR), for measuring expression of\nmultiple genes simultaneously, giving a single end point readout (Goertz et al., 2023). This diagnostic\nrequires many competitor DNA sequences to be optimized to have the correct amplification properties\nin PCR reactions, and we believe the relationship between the responses of the competitors may be\nnon-linear. For optimal results, these competitors should have a predefined amplification curve rate; and\na nuisance drift factor should ideally be below a certain threshold to allow for a more stable readout.\nWe use synthetic data experiments to compare transfer learning Gaussian process models in different\nsettings. We then use cross-validation to verify the benefit of the LVMOGP for modeling the response of\nthe competitors, using data from DNA amplification experiments. We confirm that a LVMOGP surrogate\nmodel in conjunction with the design of experiments workflow speeds up optimization of the competitors\nfor both a single objective case, where only rate is optimized, and an optimization case with a penalty\non drift over a given threshold.\n2\nMaterials and Methods\n2.1\nGaussian Process Regression\nA Gaussian process is a stochastic process representing an infinite collection of random variables, the joint\ndistribution of any subset of which is a multi-dimensional Gaussian distribution (Rasmussen and Williams,\n2006). A Gaussian process is fully defined by its mean m : RD 7→ R and covariance k : RD × RD 7→ R\nfunctions:\nf (x) ∼ GP\n`\nm(x); k(x; x′)\n´\n;\n(1)\nwhere x ∈ RD. For a full nomenclature see Appendix 8. The output data y(x) ∈ R is assumed to be\nnoisy evaluations of f (x) ∈ R:\ny(x) = f (x) + ›;\n(2)\nwhere › ∼ N(0; ff2\nn) and ff2\nn is the noise variance.\n2\nLVMOGP latent space\nBayesian Optimization\nFit Transfer Learning Model\nCalculate Rate and Drift\nCollect Data\ncontinuous variables\ncategorical variable\n% guanine-cytosine\nnumber of base pairs\npoints indicate unique primer and reporter combinations\nRFU = relative fluorescence units\ndistance indicates\n similarity\nprimer and reporter\n combination\nA\nRate\nDrift\nB\nD\nC\nFigure 1: Design of experiments workflow for optimizing the competitor DNA molecules. (A) Data is\ncollected in the lab using a DNA amplification reaction assay. (B) The rate and drift are then calculated\nby fitting them to the amplification curves. (C) Transfer learning surrogate models use the data to\npredict the rate and drift for each of the given competitors. (D) A Bayesian optimization algorithm\nselects the experiment to run for each competitor. This process is repeated until all optimal competitor\nsequences are found or the experimental budget is exhausted.\nPrior beliefs about the data can be expressed in the selection of the mean and covariance functions.\nOften this implies setting the mean function to zero, which is what we do here. A common kernel function\nis the squared exponential, which is a stationary kernel that assumes the data-generating function is\nsmooth:\nk(x; x′) = ff2\nkexp\n \n−\nD\nX\nd=1\n(xd − x′\nd)2\n2‘2\nd\n!\n;\n(3)\nwhere ff2\nk is the kernel variance and ‘d is the lengthscale of dimension d. Given a set of N training data\nD = {(xi; yi)|i = 1; :::; N}, the training inputs {xn}N\ni=1 can be aggregated into the matrix X ∈ RN×D\nand the training observations {yn}N\ni=1 aggregated into the vector y ∈ RN. It is then possible to write\na joint distribution of the training observations y and predicted function value f∗ at prediction locations\nX∗. Thus, the mean and covariance of the Gaussian process at the prediction points can be calculated\nrespectively:\n—(X∗) = E[ ¯f∗|X; y; X∗] = K(X∗; X)[K(X; X) + ff2\nnI]−1y\n(4)\n3\nff(X∗) = K(X∗; X∗) − K(X∗; X)[K(X; X) + ff2\nnI]−1K(X; X∗):\n(5)\nThe hyperparameters „ = {ff2\nn; ff2\nk; ‘d} are optimized by maximizing the marginal likelihood p(y|X; „),\nwhich is calculated in closed form (Rasmussen and Williams, 2006).\n2.2\nTransfer Learning Gaussian Processes\n2.2.1\nIndependent Gaussian Processes with Shared Kernel\nA simple way of transferring information is through the kernel hyperparameters. In the multi-output\nGaussian process (MOGP), the outputs are assumed to be multi-dimensional such that y ∈ RN×P\n(Álvarez et al., 2012). All outputs have the same kernel function and hyperparameters but function\nvalues on different outputs are uncorrelated. This means the kernel of the MOGP is a block diagonal\nwith k(Xp; X′\np) = k(Xp; X′\np) if p = p′ and k(Xp; X′\np) = 0 if p ̸= p′ where p is the output index. The\njoint distribution for two outputs f1 and f2 evaluated at points X1 and X2 is given by:\n»f1\nf2\n–\n∼ N\n„\n0;\n»K(X1; X1)\n0\n0\nK(X2; X2)\n–«\n:\n(6)\n2.2.2\nLinear Model of Coregionalization\nThe linear model of coregionalization (LMC) extends the MOGP to model linear correlations between\noutput surfaces by assuming they are linear combinations of Gaussian process latent functions:\nfp(x) = Wpg(x) + »pflp(x):\n(7)\nwhere W ∈ RP×Q is a vector of weights g(x) = {gq(x)}Q\nq=1 are shared latent functions, flp(x) is a\nlatent function that shares the kernel of g(x) and allows for some independent behavior and »p is a\nlearned constant (Álvarez et al., 2012; Bonilla et al., 2007).\nThis leads to a kernel structured in such a way that the joint distribution between two functions f1\nand f2 is given by:\n»f1\nf2\n–\n∼ N\n \n0;\n\"PQ\nq=1 b11kq(X1; X1)\nPQ\nq=1 b12kq(X2; X2)\nPQ\nq=1 b21kq(X1; X1)\nPQ\nq=1 b22kq(X2; X2)\n#!\n;\n(8)\nwhere bpp′ is an element of B = WW T +diag(»), a P ×P matrix determining the similarity between\nfunctions and there are Q different covariance functions kq(x; x′). If Q = 1, this is known as the intrinsic\ncoregionalization model (Álvarez et al., 2012).\nCoregionalization methods have successfully been used for Bayesian optimization (Cao et al., 2010;\nSwersky et al., 2013; Tighineanu et al., 2022) and applied to the optimization of synthetic genes (González\net al., 2015) and chemical reactions (Taylor et al., 2023). However, coregionalization methods assume\nthe response surfaces are linear combinations of a small number of latent functions, so they can fail to\nfit and predict well on data with non-linear similarity between surfaces.\n2.2.3\nLatent Variable Multi-output Gaussian Process\nThe latent variable multi-output Gaussian process (LVMOGP) introduced by Dai et al. (2017) can model\nnon-linear similarities. It does so by augmenting the input domain of a Gaussian process with a QH\ndimensional latent space H. Each output function has a latent variable, such that the latent variables\nare denoted by H = [h1; :::; hP ]T ∈ RP×QH. The LVMOGP assumes output yp is generated by:\nyp(x) = f (x; hp) + ›;\n(9)\n4\nwhere › ∼ N(0; ff2\nn). The latent space allows the LVMOGP to automatically transfer learn between\noutput functions as it will cluster similar output functions together and place widely different ones far\napart on the latent space.\nThe distance on the latent space and the latent space lengthscale then\ndetermine the amount of correlation between different output functions. To account for uncertainty in\nthe placement of the latent variables, they are treated as distributions rather than point estimates, such\nthat hp ∼ N(—hp; Σhp). For more details on the implementation of the LVMOGP see Appendix 8.1.\nSimilar latent variable models have been used for Bayesian optimization of material development\n(Zhang et al., 2020) and for transfer learning across cell lines (Hutter et al., 2021). However, these\nmethods treat the latent variables as point estimates rather than distributions as in the LVMOGP, which\ncan cause poor uncertainty estimates, especially at low data regimes.\n2.2.4\nComparison of Gaussian Process Models\nIn our comparisons, we include a fourth model called the average Gaussian process (AvgGP), which\ntreats all the data as if it has come from the same response surface. Figure 2 shows predictions on\na toy data set of the four Gaussian process models we consider. As the AvgGP doesn’t differentiate\nbetween surfaces, it doesn’t fit any response surface well. The MOGP only shares hyperparameters but\nno information about function values between response surfaces, meaning it makes worse predictions and\nhas more uncertainty on new response surfaces. The LMC has a better mean prediction than the MOGP\nas it shares information between response surfaces. The LVMOGP similarly has better mean prediction\nthan the MOGP as it shares information across response surfaces through the latent space. If Q = 1\nand B is the identity matrix, then the LMC recovers the MOGP. If a linear kernel is applied to the latent\ndimensions of the LVMOGP, the LMC is recovered, and by making the distance between latent variables\nlarge relative to the lengthscale, the MOGP can be recovered too. The fact there are hyperparameter\nsettings for the LMC and LVMOGP that recover the MOGP is promising for preventing negative transfer,\nas in the case where there is no correlation between response surfaces they can just revert to the MOGP.\nHowever, this is only true for large data sets — in low data regimes, we may expect some negative\ntransfer in the no correlation case, due to uncertainty in the hyperparameter values and, in the case of\nthe LVMOGP, a prior on the existence of correlations.\n2.2.5\nGaussian Process Implementation\nAll coding was done in Python using version 3.9. The Gaussian process models were implemented using\nGPFlow 2.3.0 (Matthews et al., 2017). GPFlow has implementations of the standard Gaussian process,\nMOGP and the LMC. Our LVMOGP was implemented as a new GPflow model class, which can be\naccessed via the Github links in Appendix 8.2. Other packages used include PyMC3 3.11.4 (Salvatier\net al., 2016) for Bayesian parameter estimation, Numpy 1.21.4 (Harris et al., 2020), Scipy 1.7.1 (Virtanen\net al., 2020) and Pandas 1.3.4 (The pandas development team, 2023) for data processing and Matplotlib\n3.4.3 (Droettboom et al., 2015) for visualization.\n2.3\nBayesian Optimization\nBayesian optimization is a sequential experimental design strategy for finding the global minimum (or\nmaximum) of an objective function (Shahriari et al., 2016; Snoek et al., 2012). As the objective function\nis unknown, a surrogate model is used to represent the posterior belief of the objective function and\nupdated every time a new data point is observed. An acquisition function is then used to select the\nnext data point to collect. A common acquisition function is the expected improvement which trades\noff exploration of regions with little data and exploitation of regions which are expected to be optimal\n5\nfunction 1\nfunction 2\nfunction 3\nmethod of transfer\nMOGP\nshared \nhyperparameters\nlatent variables\nLVMOGP\nKronecker kernel\nLMC\nall data on same \nsurface\nAvgGP\nFigure 2: Predictions of the four Gaussian process models fitted to a toy dataset. MOGP: multioutput\nGaussian process, AvgGP: average Gaussian process, LMC: linear model of coregionalization, LVMOGP:\nlatent variable multi-output Gaussian process. The dots are the data, the dashed line is the true function,\nthe solid line is the Gaussian process mean prediction and the shaded region is 2 times the predicted\nstandard deviation, meaning around 95% of the data points should lie within the shaded region. The\nbottom row explains how data is transferred between the surfaces by each model.\n(Jones et al., 1998; Garnett, 2023). This process is repeated until the optimum has been found or the\nexperimental budget exhausted.\n2.3.1\nAcquisition Function\nRather than maximizing or minimizing the rate, as is usual in Bayesian optimization, we wish to minimize\nthe difference between the rate, frate, and the target rate, Trate:\narg min\nBP;GC\nq\n(frate − Trate)2\n(10)\nTherefore, we use the target vector optimization acquisition function, that extends the expected im-\nprovement acquisition function to minimize the Euclidean distance between a target vector and a vector\nof the current predicted values (Uhrenholt and Jensen, 2019). As we are only optimizing the rate, we use\ntheir formulation with scalars instead of vectors. In this formulation, a stochastic variable is defined as\n‹|x = ∥y(x) − yt∥2\n2 where y(x) is the output value at input x and yt is our target value. The distribu-\ntion of p(‹|x) is modeled with the aim of minimizing ‹. If the response surfaces are Gaussian processes,\nthen p(‹|x) can be approximated using a non-central ffl2 distribution. The expected improvement for\nthis non-central ffl2 distribution is expressed as:\n¸EI = ‹minG–(‹min=‚2) − ‚2E[t|t < ‹min=‚2]G–(‹min=‚2);\n(11)\n6\nwhere ‹min is the minimum ‹ observed so far, ‚ is root mean of the variances of each output evaluated at\nthe training points, t = ‹‚−2, and G– is an approximate cumulative ffl2 distribution with non-centrality\nparameter – defined in the paper (Uhrenholt and Jensen, 2019).\n2.3.2\nBayesian Optimization with Drift Penalty\nTo ensure the drift value remains below, or close to the threshold, we use the probability of feasibility to\nencourage the algorithm to select points that have a high chance of being below the threshold (Schonlau\net al., 1998):\nPF(x) = p(fdrif t(x) ≤ Tdrif t);\n(12)\nwhere fdrif t(x) is the value of drift function at x, and Tdrif t is the drift threshold.\nWe then multiply the expected improvement by the probability of feasibility to get our final acquisition\nfunction:\n¸p = PF(x)¸EI(x):\n(13)\nThe probability of feasibility has been used for optimization applications including analog circuits (Lyu\net al., 2018) and materials design (Sharpe et al., 2018).\n2.3.3\nPerformance Metrics\nFor both the synthetic experiments and the cross-validation experiments we assessed the fit of Gaussian\nprocess models with two performance metrics: root mean squared error (RMSE):\nRMSE =\nsPN∗\ni=0(—(x∗\ni ) − y∗\ni )2\nN∗\n;\n(14)\nand negative log predictive density (NLPD):\nNLPD = 1\nN∗\nN∗\nX\ni=0\nlog p(y∗\ni |x∗\ni ; X; y; „) = − 1\n2N∗\nN∗\nX\ni=0\n \n− log(2ıff(x∗\ni )2) − (y∗\ni − —(x∗\ni ))2\nff(x∗\ni )2\n!\n:\n(15)\nThese are both calculated on a test set of input locations X∗ of length N∗. The RMSE is useful\nfor comparing the mean predictions of the Gaussian processes, while the NLPD also indicates how good\nthe uncertainty estimate is, both of which are important for effective exploration and exploitation. For\nassessing the Bayesian optimization algorithm, we use cumulative regret:\nregret = min\nxi∈X\n„q\n—(x∗i) − ybest)2 + max(0; fdrift(xi) − Tdrift)\n«\n;\n(16)\nwhere ybest is the data point closest to the target out of both training and candidate sets for that\nsurface. max(0; fdrift(xi) − Tdrift) is a penalty for exceeding the drift threshold.\n2.4\nData Collection\nEach competitor has predefined primers and fluorescent probes and a design region where the sequence\ncan be altered. Rather than tackling the difficult combinatorial problem of optimizing the sequence\ndirectly, we reduce the problem to two key input variables: the number of base pairs (BP) and guanine-\ncytosine content (GC) as in Figure 3. This converts the design space into a more manageable continuous\nform and reduces the input dimensions, which is beneficial when data is limited.\nFor each BP-GC\n7\nprimer\nprimer\nfluorescent probe\ndesign region\nrate \ndrift\nFigure 3: Schematic of the competitor design space. For a given competitor DNA molecule, the primers\nand fluorescent probe regions are fixed. We can edit the design region to ensure the sequence has a given\nnumber of base pairs and guanine-cytosine content. Changing the number of base pairs and guanine-\ncytosine-content affects the rate and drift of the competitor, allowing us to fine-tune to the rate and\ndrift required for the diagnostic assay.\ncombination, chosen by an expert researcher, a polymerase chain reaction (PCR) assay generates an\namplification curve, from which rate and drift are calculated. In total, we have data on 34 different\ncompetitors and wish to optimize 16 of these. Across the 34 competitors, we have 592 data points at\n327 unique input locations, with 1 to 6 repeats at each location. See Appendix 8.3 for a summary of the\ndata.\nThe rate and drift for each amplification curve were calculated using the following equations:\nFT =\n‌\n1 + (‌−F0)\nF0\n· e−r·fi ;\n(17)\nsignal = FT ·\n„\n1 + FT\n‌ · m · (ln(F0)=r)\n«\n;\n(18)\nwhere FT and F0 are the end point and starting fluorescence, ‌ is carrying capacity, r is the rate, m is\nthe drift and fi is cycle number.\n2.4.1\nPolymerase Chain Reactions\nTo perform the PCR reactions, we used an Applied Biosystems QuantStudio 6 Flex using Applied Biosys-\ntems MicroAmp EnduraPlate Optical 384-well plates (Thermo Fisher Scientific, Waltham, MA, USA).\nThe theromcycling stages consisted of a melt step at 95°C for 3 seconds and an annealing step at 60°C.\nAll reactions were performed at 10 µL and used Applied Biosystems TaqMan Fast Advanced Master Mix.\nEither fluorescent probes or EvaGreen dye (Biotium, Fremont, CA, USA) were used as reporters.\n8\n2.4.2\nDNA Sequences\nFor each BP-GC combination for a given competitor, NUPACK (Zadeh et al., 2011) was used to create a\nDNA sequence with the correct number of base pairs and guanine-cytosine content, as well as the correct\nsequences for the primer and probes. These sequences, alongside synthetic natural target analogs, were\npurchased from Twist Biosciences (San Francisco, CA) or as eBlock Gene Fragments from Integrated\nDNA Technologies (“IDT”, Coralville, IA, USA). Primers and probes were also purchased from IDT.\n3\nResults\n3.1\nSynthetic Data Experiments\nTo explore the performance of the MOGP, AvgGP, LMC and LVMOGP, we ran experiments on synthetic\ndata sets representing three test cases: uncorrelated, linearly correlated and horizontally offset response\nsurfaces. All synthetic experiments had two response surfaces each with 30 points observed and 10 new\nresponse surfaces with no points observed initially. We added one random point to each new response\nsurface every iteration and recorded the RMSE and NLPD for the Gaussian process models’ predictions.\nFigure 4 shows the RMSEs and NLPDs of the Gaussian process models for these test settings.\nFor the uncorrelated test case, response surfaces were generated as independent samples of a Gaussian\nprocess prior with a ‘ = 0:3 and ff2\nk = 2. This test case was to check for negative transfer, where the\nsharing of information hinders rather than aids the learning process. In Figure 4, the MOGP outperforms\nthe other Gaussian process models for RMSE and NLPD until approximately 10 data points. We expect\nthe LMC and LVMOGP to have some negative transfer at very low data regimes as they have a prior\nexpectation of correlations between response surfaces. However, with enough data, they should perform\nthe same at the MOGP, which is corroborated by the results in Figure 4.\nThe response surfaces for the linearly-correlated test case were created as linear combinations of two\nlatent functions, both generated as independent samples of a Gaussian process with ‘ = 0:3 and ff2\nk = 2.\nThe LMC outperforms the other two Gaussian process models except at very low data regimes, which is\nlikely due to overconfidence of the LMC when it has little data. The LMC and LVMOGP outperform the\nMOGP even at high data regimes, showing the advantage of transfer learning.\nThe horizontally offset test case was chosen as a simple example where the LMC struggles to fit the\ndata. The response surfaces were generated by offsetting a sigmoid function horizontally by a random\nconstant. In this case, the LVMOGP outperforms the other Gaussian process models for both RMSE and\nNLPD. This is because the LVMOGP can learn new surfaces with very few data points, as all it needs\nto do is to correctly predict where the sloped region is. The LMC performs worse than the LVMOGP\nbecause the offset cannot be represented by a linear combination of its latent functions, meaning it\nrequires more data to perform as well.\nAcross all the test cases, the LMC has poor NLPD at low data regimes. This is likely because it\ncannot express uncertainty in the deterministic B matrix.\n3.2\nPrediction of DNA Amplification Experiments\nThe performance of the proposed design of experiments workflow was validated using data from competi-\ntor DNA amplification experiments. This was done in three parts: first cross-validation was performed to\ncompare the predictive accuracy of the Gaussian process models; then a Bayesian optimization procedure\nwas used to optimize only the rate; finally the Bayesian optimization with drift penalty procedure was\napplied.\n9\nPerformance Metrics\nRMSE\nNLPD\nRMSE\nNLPD\nRMSE\nNLPD\nExample Functions\nuncorrelated\noffset\nlinearly\ncorrelated\nFigure 4: Results of experiments with synthetically-generated data. The plots on the left show example\ndata-generating functions used for the synthetic experiments. The plots on the right show the RMSE\nand NLPD for the three different test response surface types for each of the Gaussian process models.\nNew points are added randomly, and each line is the mean of 5 different randomly generated data sets,\nall generated from the same test functions.\nIn cross-validation, the training set consisted of all the data from the two competitors that had the\nmost observations as well as a random subset of the remaining data, but ensuring all competitors had at\nleast one data point. This was repeated 150 times for each percentage of data in the training set. Figure\n5 shows the RMSE and NLPD of the Gaussian process models’ predictions. The LVMOGP outperforms\nthe other Gaussian process models for both RMSE and NLPD for both rate and drift. The LMC has poor\nNLPD in comparison to the other Gaussian process models, suggesting it has poor uncertainty estimates.\nThe AvgGP model shows little improvement with increased amounts of training data. This shows the\nlimitations of averaging the surfaces and justifies modeling each response surface separately.\n3.3\nOptimization of DNA Amplification Experiments\nIdeally, for the Bayesian optimization experiments we would integrate the algorithm into the experimental\nloop, collecting new data with each new recommendation of each Gaussian process model. However, due\nto the cost of experiments, this was infeasible. Instead, we performed retrospective Bayesian optimization\nusing the existing competitive DNA amplification dataset. The data was split into training and candidate\n10\nFigure 5: Results of cross-validation on the DNA amplification data for both rate and drift. For each\ncross-validation run, the training set consisted of all the data from two competitors and a random subset\nof the data on the remaining competitors, ensuring all competitors had at least one data point. This is\nrepeated for different percentages of data in the training set, and for each percentage, it is repeated 200\ntimes.\nsets, with the design of experiments algorithm only allowed to choose the next point out of the candidate\nset. Bayesian optimization was run iteratively until all points had been selected or up to a maximum\nnumber of iterations, whichever happened first.\nTwo learning scenarios were tested: the \"Learning Many\" scenario where all data from two competi-\ntors were fully observed to begin with and then 16 competitors optimized in parallel; and the \"One at a\nTime\" where each of the 16 competitors was optimized individually, with the 33 remaining competitors\nincluded in the training set. These scenarios replicate likely wet lab experimentation scenarios — the first\nfor when many competitors need to be optimized at once, and the second for when many competitors are\nalready optimized and we want to add an extra one. The maximum number of iterations was 15 for the\nrate-only optimization and 20 or 10 for the penalized optimization, depending on the learning scenario.\nWe also considered two methods for choosing the first experiment for a new competitor with no\npreviously observed data.\nChoosing the most central data point (\"Center\" in Figure 6) offers both\nmaximum reduction in variance across the response surface and ensures all competitor response surfaces\nhave a comparable point, which may help the transfer learning methods determine their similarities. It is\nalso a reasonable approximation to what a human experimenter might do if they had no prior knowledge\nof the response surface. The second method is to let the Gaussian process model choose the first point\n(\"Model’s Choice\" in Figure 6) for a new competitor. For the AvgGP and the LVMOGP, this is possible\nas they can make posterior predictions on new response surface. For the LVMOGP, the latent variable of\nthe new surface is determined as a weighted average of the latent variables of the response surfaces with\ndata that have the same probe and at least one matching primer. If there are no surfaces with matching\nprimers, we use a weighted average of the surfaces with the same probe. For the LMC and MOGP we\nhave no posterior, so the first point is selected randomly.\n11\n3.3.1\nSingle Objective Bayesian Optimization\nno. iterations\nno. iterations\nSingle Objective\nWith Drift Penalty\nCumulative Regret\nLearning Many\nOne at a Time\nLearning Many\nOne at a Time\nCenter\nModel's Choice\ncumulative regret\nCentre\nModel's Choice\nFigure 6: Cumulative regret of each of the Gaussian process models for single objective (left) and\npenalized (right) Bayesian optimization. Each line indicates the mean across 24 random seeds and all\ncompetitors, while the shaded regions indicate the upper and lower 5% quantiles by random seed. The\ntop row is when the first point on each new surface is selected as being the center point, and the bottom\nis when the model is allowed to choose the first point. The \"Learning Many\" scenario is when many\ncompetitors are being optimized at the same time, and the \"One at a Time\" scenario is when one\ncompetitor is being optimized, with all others being in the training set.\nThe left panel of Figure 6 shows the results of optimizing rate without considering the drift penalty.\nThe variance in the results comes from three sources. The first is the random selection of the next point\nwhen two points have the same expected improvement — this causes unavoidable variation. The second\nis due to the Gaussian process models optimizing to different hyperparameter values due to different\ninitializations. The different values arise because the optimization of the non-convex hyperparameter loss\nsurfaces is difficult. The final source of variation is the random starting point for the MOGP and LMC.\nIn all cases, the LVMOGP has much lower cumulative regret than the other models. The \"Center\"\nstart point allows us to compare the performance of the Gaussian process models without being skewed\nby the first point. In this case the LMC and LVMOGP have the lowest cumulative regret. The ordering\nchanges between the \"Center\" and \"Model’s choice\" scenario, as in the latter the AvgGP and LVMOGP\nare able to predict on new surfaces, giving them an advantage over the LMC and MOGP when choosing\nthe first point. See Appendix 8.4.3 for a table of the mean regrets of the first points for a quantification\nof this improvement.\nAs the \"One at a Time\" scenario includes the data from all other competitors, the Gaussian process\nmodels start with far more data than the \"Learning Many\" scenario. This means the AvgGP, LMC and\nLVMOGP all have less regret in the \"One at a Time\" scenario, as they are able to transfer information\nabout the function values of competitors to improve prediction of the target competitor behavior. The\nMOGP does not transfer information about function values, so performs relatively worse than the others\nfor the \"One at a Time\" scenario.\n12\n3.3.2\nBayesian Optimization with Drift Penalty\nLMC\nLVMOGP\nmean\nuncertainty\nRate\nDrift\nmean\nuncertainty\nexpected\nimprovement\nMOGP\nAvgGP\nFigure 7: Predictions for the rate and drift for each of the Gaussian process models. The BP and GC\naxes are in log and logit scales respectively. These plots show the mean of the Gaussian process model\npredictions and the uncertainty which here is 2 × standard deviation. The expected improvement with\nprobability of feasibility is then plotted in the final column. This is for the case where we are optimizing\ncompetitor FP005-FP004-EvaGreen and have observed one data point so far, with the models able to\nchoose the first point. The black contour lines on the mean plots indicate the target rate and threshold\ndrift values.\nThe right-hand panel of Figure 6 shows the cumulative regret for optimization of the rate with a\npenalty on the drift. In all cases, the LVMOGP has the lowest cumulative regret at the end. In the\n\"Learning Many\" scenario the AvgGP again benefits from selecting the first point for the \"Model’s\nChoice\" starting point, but the LVMOGP actually performs slightly worse than it did for the \"Center\"\nstart point. This may be due to negative transfer in the drift predictions at very low data regimes making\nthe selection of the first point sub-optimal.\nThe ordering of the Gaussian process models is different for the \"Learning Many\" and \"One at a\nTime\", probably because the increased amount of data allows the LMC to predict comparatively better in\nthe \"One at a Time\" scenario than the \"Learning Many\". The LVMOGP outperforms the other Gaussian\nprocess models the most in the \"One at a Time\" \"Model’s Choice\" experiment, which is likely due to\nthe large amounts of data on all competitors, except the target, and effective transfer of information\nbetween them.\nFigure 7 shows the rate and drift predictions and expected improvement for one iteration. Most\nnotably, the MOGP has no transfer learning, so has almost equal expected improvement for most of the\ncandidate points. The other three models transfer information across the competitors, meaning even\nwith one data point, they have much more complex predictions than the MOGP. We can also see how\nthe AvgGP, MOGP and LMC fit the drift poorly. This is because the drift is of a different order of\n13\nDrift\nRate\nFigure 8: Latent space of the LVMOGP for the rate and drift. The crosses indicate competitors with probe\nprimers and the dots indicate those with EvaGreen primers. The shaded circles indicate the uncertainty\nin the latent positions.\nmagnitude depending on the fluorescent probe used. Most of the Gaussian process models are unable\nto detect this, meaning they end up with a poor fit to the data. The LVMOGP, however, does identify\nthis —Figure 8 shows how it clusters the two probe types at different sides of the latent space. This\nindicates it has recognized there are two regimes for drift, despite not being explicitly told which probe\na competitor uses.\nSee Appendix 8.4 for further Bayesian optimization results for both the single objective and penalized\noptimizations. These results show the LVMOGP reaches the best point on the surfaces faster and with\nless cumulative regret, more often than the other models for most test cases.\n4\nDiscussion\nExpensive and time consuming experiments require an intelligent design of experiments strategy. This\nstudy demonstrates how a transfer learning surrogate model can be used in conjunction with Bayesian op-\ntimization to optimize biological sequences. For the specific case of designing competitor DNA molecules\nfor a new diagnostic, reducing the number and therefore cost of experiments can help it reach the af-\nfordability criteria for point of care settings (Land et al., 2019).\nIn Bayesian optimization, we need a surrogate function with reliable mean and uncertainty estimates\nto ensure a balance between exploration and exploitation when selecting new points. Our cross-validation\nresults in Section. 3.2 show the LVMOGP has better predictive accuracy than the other Gaussian process\nmodels for both rate and drift. These results also demonstrate one of the limitations of the LMC: the\nLMC has very high NLPD at low data regimes. This implies the LMC has poor uncertainty estimates\nand is overfitting, a result which has been previously observed (Dai et al., 2017).\nTo replicate a real-life iterative design of experiments regime, we performed Bayesian optimization\non DNA amplification experimental data, but only allowing the models to select new points from existing\ndata. For the single objective optimization case, the LVMOGP has lower cumulative regret than the\nother Gaussian process models for all test cases and starting points. This shows the LVMOGP transfer\nlearning approach is useful both when optimizing multiple competitors at a time, and when using the\ndata from all previous competitors to optimize a new one. The superior performance of the LMC and\nLVMOGP for the \"Center\" starting point shows transfer learning speeds up the learning process. These\nresults also demonstrate the advantage of a surrogate model that can predict unseen surfaces — both\nthe LVMOGP and the AvgGP see a large improvement in regret when they are allowed to select the first\n14\npoint, both outperforming the MOGP and LMC where the first point is chosen at random.\nWhen optimizing new biological sequences, there are often factors we wish to keep within a certain\nrange such as purity (Degerman et al., 2006) or biophysical properties (Khan et al., 2023). While these\ncan be treated as constraints, sometimes we may be willing to violate them slightly if it leads to a large\nimprovement in the objective function. In these scenarios, we can add a penalty. To apply a penalty on\nthe nuisance drift factor, we used probability of feasibility to penalize any point predicted to be above the\nthreshold drift value. In the penalized optimization, the LVMOGP had less cumulative regret than the\nother models but the difference in performance was smaller than that of the single objective optimization.\nThis could be due to the added challenge of dealing with the penalized on drift.\nThere was variation in the performance of the Gaussian process models’ across random seeds due to\nthe hyperparameter initialization. The LVMOGP has more variation due to its training being a harder\noptimization problem. While smart initialization and random restarts helped with this issue, future work\ncould simplify the optimization procedures. The optimization of the Gaussian process models is discussed\nin Appendix 8.5.\nWhile the workflow outlined here will be useful for the optimization of new competitor DNA molecules,\nit is not specific to this application and could be used for other applications where it is necessary to\noptimize many similar tasks, such as engineering DNA probes (Lopez et al., 2018; Wadle et al., 2016),\nexploring protein fitness landscapes (Hu et al., 2023), optimizing conditions for different cell lines (Hutter\net al., 2021), or inferring psuedotime for cellular processes (Campbell and Yau, 2015). With the rise in\nlab automation, this workflow can be integrated into a design build test pipeline similar to Carbonell\net al. (2018) and HamediRad et al. (2019) which can greatly reduce the time required to optimize\nnew biomolecular components, speeding up the creation of new devices. This method could also be\nincorporated into hybrid models in bio-processing and chemical engineering, for decision making for\nsystems with many similar components (Narayanan et al., 2023; Mowbray et al., 2021; Schweidtmann\net al., 2021).\nThis workflow could also be extended to multi-output optimization problems by using a multi-output\nacquisition function or by finding Pareto optimal solutions (Belanger et al., 2019; Selega and Campbell,\n2022; Jablonka et al., 2021; Schweidtmann et al., 2018). Similarly, the surrogate functions needed for\nmulti-fidelity learning, where we have multiple sources of information about an optimization task with\nsome sources being cheaper but less informative than others, are similar to those for transfer learning,\nmaking it an easy extension (Folch et al., 2023; Sun et al., 2022).\n5\nConclusion\nWe have shown how a transfer learning design of experiments workflow can be used to optimize many\ncompetitor DNA molecules for an amplification-based diagnostics device. We used cross-validation to\ndemonstrate that the latent variable multi-output Gaussian process has the best predictive accuracy and\nhave shown it has the least regret when Bayesian optimization is performed on the DNA amplification\ndata. Future improvements to the optimization of the model hyperparameters would lead to faster and\nmore consistent performance of the algorithm. Despite this, we believe this workflow is applicable to\nmany other biotechnology applications and should be used to reduce the experimental load when there\nare many similar tasks to be optimized but their similarity is a priori unknown.\n6\nFunding Information\nThis work was supported by the UKRI CDT in AI for Healthcare Grant No. EP/S023283/1, UK Re-\nsearch and Innovation Grant No. EP/P016871/1, the BASF / RAEng Research Chair in Data-Driven\n15\nOptimization, the US NIH Grant No.\n5F32GM131594, the EPSRC IRC Next Steps Plus grant No.\nEP/R018707/1 and the RAEng Chair in Emerging Technologies award No. CiET2021/ 94. For the\npurpose of open access, the authors have applied a Creative Commons Attribution (CC BY) licence to\nany Author Accepted Manuscript version arising.\n7\nConflict of Interest\nJPG and MMS are co-founders at Signatur Biosciences, Inc., a company which seeks to commercialize\nthe medical diagnostic technology this paper focuses on as a use-case, and they are co-inventors in a\npatent describing a method for amplification-based quantification of nucleic acids. The remaining authors\ndeclare no conflict of interest.\n16\nReferences\nBadeau, B. A., Comerford, M. P., Arakawa, C. K., Shadish, J. A. and DeForest, C. A. (2018) Engineered\nmodular biomaterial logic gates for environmentally triggered therapeutic delivery. Nature chemistry,\n10, 251–258. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5822735/.\nBader, J., Narayanan, H., Arosio, P. and Leroux, J.-C. (2023) Improving extracellular vesicles production\nthrough a Bayesian optimization-based experimental design. European Journal of Pharmaceutics and\nBiopharmaceutics, 182, 103–114.\nURL: https://www.sciencedirect.com/science/article/\npii/S0939641122002983.\nBelanger, D., Vora, S., Mariet, Z., Deshpande, R., Dohan, D., Angermueller, C., Murphy, K., Chapelle,\nO. and Colwell, L. (2019) Biological Sequences Design using Batched Bayesian Optimization.\nBlakney, A. K., McKay, P. F., Ibarzo Yus, B., Hunter, J. E., Dex, E. A. and Shattock, R. J. (2019)\nThe Skin You Are In: Design-of-Experiments Optimization of Lipid Nanoparticle Self-Amplifying RNA\nFormulations in Human Skin Explants. ACS Nano, 13, 5920–5930. URL: https://pubs.acs.org/\ndoi/10.1021/acsnano.9b01774.\nBonilla, E. V., Chai, K. and Williams, C. (2007) Multi-task Gaussian Process Prediction. Advances\nin Neural Information Processing Systems, 20. URL: https://proceedings.neurips.cc/paper/\n2007/hash/66368270ffd51418ec58bd793f2d9b1b-Abstract.html.\nCampbell, K. and Yau, C. (2015) Bayesian Gaussian Process Latent Variable Models for pseudotime\ninference in single-cell RNA-seq data.\nCao, B., Pan, S. J., Zhang, Y., Yeung, D.-Y. and Yang, Q. (2010) Adaptive Transfer Learning. Proceed-\nings of the AAAI Conference on Artificial Intelligence, 24. URL: https://ojs.aaai.org/index.\nphp/AAAI/article/view/7682. Number: 1.\nCarbonell, P., Jervis, A. J., Robinson, C. J., Yan, C., Dunstan, M., Swainston, N., Vinaixa, M., Hollywood,\nK. A., Currin, A., Rattray, N. J. W., Taylor, S., Spiess, R., Sung, R., Williams, A. R., Fellows,\nD., Stanford, N. J., Mulherin, P., Le Feuvre, R., Barran, P., Goodacre, R., Turner, N. J., Goble,\nC., Chen, G. G., Kell, D. B., Micklefield, J., Breitling, R., Takano, E., Faulon, J.-L. and Scrutton,\nN. S. (2018) An automated Design-Build-Test-Learn pipeline for enhanced microbial production of fine\nchemicals. Communications Biology, 1, 1–10. URL: https://www.nature.com/articles/s42003-\n018-0076-9. Number: 1 Publisher: Nature Publishing Group.\nCox, D. R. and Reid, N. (2000) The Theory of the Design of Experiments. CRC Press.\nDai, Z., Álvarez, M. and Lawrence, N. (2017) Efficient Modeling of Latent Information in Super-\nvised Learning using Gaussian Processes.\nIn Advances in Neural Information Processing Systems,\nvol. 30. Curran Associates, Inc.\nURL: https://proceedings.neurips.cc/paper/2017/file/\n1680e9fa7b4dd5d62ece800239bb53bd-Paper.pdf.\nDegerman, M., Jakobsson, N. and Nilsson, B. (2006) Constrained optimization of a preparative ion-\nexchange step for antibody purification. Journal of Chromatography A, 1113, 92–100. URL: https:\n//www.sciencedirect.com/science/article/pii/S0021967306003013.\nDeng, F., Pan, J., Liu, Z., Zeng, L. and Chen, J. (2023) Programmable DNA biocomputing circuits for\nrapid and intelligent screening of SARS-CoV-2 variants. Biosensors and Bioelectronics, 223, 115025.\nURL: https://www.sciencedirect.com/science/article/pii/S095656632201065X.\n17\nDroettboom, M., Hunter, J., Firing, E., Caswell, T. A., Elson, P., Dale, D., Lee, J.-J., McDougall,\nD., Root, B., Straw, A., Seppänen, J. K., Nielsen, J. H., May, R., Varoquaux, Yu, T. S., Moad,\nC., Gohlke, C., Würtz, P., Hisch, T., Silvester, S., Ivanov, P., Whitaker, J., Cimarron, Hobson, P.,\nGiuca, M., Thomas, I., mmetz bn, Evans, J., dhyams and NNemec (2015) matplotlib: v1.4.3. URL:\nhttps://zenodo.org/record/15423.\nEbrahimi, S. B. and Samanta, D. (2023) Engineering protein-based therapeutics through structural and\nchemical design. Nature Communications, 14, 2411. URL: https://www.nature.com/articles/\ns41467-023-38039-x. Number: 1 Publisher: Nature Publishing Group.\nFellermann, H., Shirt-Ediss, B., Kozyra, J., Linsley, M., Lendrem, D., Isaacs, J. and Howard, T. (2019)\nDesign of experiments and the virtual PCR simulator: An online game for pharmaceutical scientists\nand biotechnologists. Pharmaceutical Statistics, 18, 402–406. URL: https://www.ncbi.nlm.nih.\ngov/pmc/articles/PMC6767770/.\nFolch, J. P., Lee, R. M., Shafei, B., Walz, D., Tsay, C., van der Wilk, M. and Misener, R. (2023)\nCombining multi-fidelity modelling and asynchronous batch Bayesian Optimization.\nComputers &\nChemical Engineering, 172, 108194. Publisher: Elsevier.\nGamble, C., Bryant, D., Carrieri, D., Bixby, E., Dang, J., Marshall, J., Doughty, D., Colwell, L., Berndl,\nM., Roberts, J. and Frumkin, M. (2021) Machine Learning Optimization of Photosynthetic Microbe\nCultivation and Recombinant Protein Production. preprint, Bioengineering. URL: http://biorxiv.\norg/lookup/doi/10.1101/2021.08.06.453272.\nGarnett, R. (2023) Bayesian Optimization. 127–129. Cambridge University Press.\nGilman, J., Walls, L., Bandiera, L. and Menolascina, F. (2021) Statistical Design of Experiments for\nSynthetic Biology. ACS Synthetic Biology, 10, 1–18. URL: https://doi.org/10.1021/acssynbio.\n0c00385. Publisher: American Chemical Society.\nGoertz, J. P., Sedgwick, R., Smith, F., Kaforou, M., Wright, V. J., Herberg, J. A., Kote-Jarai, Z., Eeles,\nR., Levin, M., Misener, R., Wilk, M. v. d. and Stevens, M. M. (2023) Competitive Amplification Net-\nworks enable molecular pattern recognition with PCR. URL: https://www.biorxiv.org/content/\n10.1101/2023.06.29.546934v1.\nGonzález, J., Longworth, J., James, D. C. and Lawrence, N. D. (2015) Bayesian Optimization for\nSynthetic Gene Design. arXiv:1505.01627 [stat]. URL: http://arxiv.org/abs/1505.01627. ArXiv:\n1505.01627.\nHamediRad, M., Chao, R., Weisberg, S., Lian, J., Sinha, S. and Zhao, H. (2019) Towards a fully au-\ntomated algorithm driven platform for biosystems design. Nature Communications, 10, 5150. URL:\nhttps://www.nature.com/articles/s41467-019-13189-z. Number: 1 Publisher: Nature Pub-\nlishing Group.\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser,\nE., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M.,\nHaldane, A., del Río, J. F., Wiebe, M., Peterson, P., Gérard-Marchant, P., Sheppard, K., Reddy, T.,\nWeckesser, W., Abbasi, H., Gohlke, C. and Oliphant, T. E. (2020) Array programming with NumPy.\nNature, 585, 357–362. URL: https://www.nature.com/articles/s41586-020-2649-2. Number:\n7825 Publisher: Nature Publishing Group.\n18\nHie, B., Bryson, B. D. and Berger, B. (2020) Leveraging Uncertainty in Machine Learning Accelerates\nBiological Discovery and Design. Cell Systems, 11, 461–477.e9. URL: https://www.cell.com/\ncell-systems/abstract/S2405-4712(20)30364-1. Publisher: Elsevier.\nHu, R., Fu, L., Chen, Y., Chen, J., Qiao, Y. and Si, T. (2023) Protein engineering via Bayesian\noptimization-guided evolutionary algorithm and robotic experiments. Briefings in Bioinformatics, 24,\nbbac570. URL: https://doi.org/10.1093/bib/bbac570.\nHua, Y., Ma, J., Li, D. and Wang, R. (2022) DNA-Based Biosensors for the Biochemical Analysis: A Re-\nview. Biosensors, 12, 183. URL: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8945906/.\nHutter, C., von Stosch, M., Cruz Bournazou, M. N. and Butté, A. (2021) Knowledge transfer across\ncell lines using hybrid Gaussian process models with entity embedding vectors. Biotechnology and\nBioengineering, 118, 4389–4401. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/\nbit.27907.\nJablonka, K. M., Jothiappan, G. M., Wang, S., Smit, B. and Yoo, B. (2021) Bias free multiobjective\nactive learning for materials design and discovery. Nature Communications, 12, 2312. URL: https:\n//www.nature.com/articles/s41467-021-22437-0.\nNumber: 1 Publisher: Nature Publishing\nGroup.\nJones, D. R., Schonlau, M. and Welch, W. J. (1998) Efficient Global Optimization of Expensive Black-\nBox Functions. Journal of Global Optimization, 13, 455–492. URL: https://doi.org/10.1023/A:\n1008306431147.\nKhan, A., Cowen-Rivers, A. I., Grosnit, A., Deik, D.-G.-X., Robert, P. A., Greiff, V., Smorodina, E.,\nRawat, P., Akbar, R., Dreczkowski, K., Tutunov, R., Bou-Ammar, D., Wang, J., Storkey, A. and\nBou-Ammar, H. (2023) Toward real-world automated antibody design with combinatorial Bayesian\noptimization. Cell Reports Methods, 3, 100374.\nKreutz, C. and Timmer, J. (2009) Systems biology: experimental design.\nThe FEBS journal, 276,\n923–942.\nLand, K. J., Boeras, D. I., Chen, X.-S., Ramsay, A. R. and Peeling, R. W. (2019) REASSURED diag-\nnostics to inform disease control strategies, strengthen health systems and improve patient outcomes.\nNature Microbiology, 4, 46–54. URL: https://www.nature.com/articles/s41564-018-0295-3.\nNumber: 1 Publisher: Nature Publishing Group.\nLopez, R., Wang, R. and Seelig, G. (2018) A molecular multi-gene classifier for disease diagnostics.\nNature Chemistry, 10, 746–754. URL: https://www.nature.com/articles/s41557-018-0056-1.\nNumber: 7 Publisher: Nature Publishing Group.\nLv, H., Li, Q., Shi, J., Fan, C. and Wang, F. (2021) Biocomputing Based on DNA Strand Displacement\nReactions. ChemPhysChem, 22, 1151–1166. URL: https://onlinelibrary.wiley.com/doi/abs/\n10.1002/cphc.202100140.\nLyu, W., Xue, P., Yang, F., Yan, C., Hong, Z., Zeng, X. and Zhou, D. (2018) An Efficient Bayesian\nOptimization Approach for Automated Optimization of Analog Circuits. IEEE Transactions on Circuits\nand Systems I: Regular Papers, 65, 1954–1967. Conference Name: IEEE Transactions on Circuits and\nSystems I: Regular Papers.\n19\nMatthews, A. G., Van Der Wilk, M., Nickson, T., Fujii, K., Boukouvalas, A., León-Villagrá, P., Ghahra-\nmani, Z. and Hensman, J. (2017) GPflow: a Gaussian process library using tensorflow. The Journal\nof Machine Learning Research, 18, 1299–1304.\nMehrian, M., Guyot, Y., Papantoniou, I., Olofsson, S., Sonnaert, M., Misener, R. and Geris, L. (2018)\nMaximizing neotissue growth kinetics in a perfusion bioreactor: An in silico strategy using model\nreduction and Bayesian optimization. Biotechnology and Bioengineering, 115, 617–629. URL: https:\n//onlinelibrary.wiley.com/doi/abs/10.1002/bit.26500.\nMowbray, M., Savage, T., Wu, C., Song, Z., Cho, B. A., Del Rio-Chanona, E. A. and Zhang, D.\n(2021) Machine learning for biochemical engineering: A review. Biochemical Engineering Journal, 172,\n108054. URL: https://www.sciencedirect.com/science/article/pii/S1369703X21001303.\nNarayanan, H., Dingfelder, F., Condado Morales, I., Patel, B., Heding, K. E., Bjelke, J. R., Egebjerg, T.,\nButté, A., Sokolov, M., Lorenzen, N. and Arosio, P. (2021) Design of Biopharmaceutical Formulations\nAccelerated by Machine Learning. Molecular Pharmaceutics, 18, 3843–3853. URL: https://pubs.\nacs.org/doi/10.1021/acs.molpharmaceut.1c00469.\nNarayanan, H., Luna, M. F., von Stosch, M., Cruz Bournazou, M. N., Polotti, G., Morbidelli, M.,\nButté, A. and Sokolov, M. (2020) Bioprocessing in the Digital Age: The Role of Process Models.\nBiotechnology Journal, 15, 1900172.\nURL: https://onlinelibrary.wiley.com/doi/abs/10.\n1002/biot.201900172.\nNarayanan, H., von Stosch, M., Feidl, F., Sokolov, M., Morbidelli, M. and Butté, A. (2023) Hybrid\nmodeling for biopharmaceutical processes: advantages, opportunities, and implementation. Frontiers\nin Chemical Engineering, 5. URL: https://www.frontiersin.org/articles/10.3389/fceng.\n2023.1157889.\nPapaneophytou, C. (2019) Design of Experiments As a Tool for Optimization in Recombinant Protein\nBiotechnology: From Constructs to Crystals. Molecular Biotechnology, 61, 873–891. URL: https:\n//doi.org/10.1007/s12033-019-00218-x.\nPolitis, S. N., Colombo, P., Colombo, G. and Rekkas, D. M. (2017) Design of experiments (DoE)\nin pharmaceutical development. Drug Development and Industrial Pharmacy, 43, 889–901. URL:\nhttps://doi.org/10.1080/03639045.2017.1291672.\nQian, L., Winfree, E. and Bruck, J. (2011) Neural network computation with DNA strand displacement\ncascades. Nature, 475, 368–372. URL: https://www.nature.com/articles/nature10262.\nRasmussen, C. E. and Williams, C. K. I. (2006) Gaussian processes for machine learning.\nAdaptive\ncomputation and machine learning. Cambridge, Mass: MIT Press.\nRomero, P. A., Krause, A. and Arnold, F. H. (2013) Navigating the protein fitness landscape with\nGaussian processes.\nProceedings of the National Academy of Sciences, 110, E193–E201.\nURL:\nhttps://www.pnas.org/content/110/3/E193. Publisher: National Academy of Sciences Section:\nPNAS Plus.\nRosa, S. S., Nunes, D., Antunes, L., Prazeres, D. M. F., Marques, M. P. C. and Azevedo, A. M. (2022)\nMaximizing mRNA vaccine production with Bayesian optimization. Biotechnology and Bioengineering,\n119, 3127–3139. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/bit.28216.\n20\nSalvatier, J., Wiecki, T. V. and Fonnesbeck, C. (2016) Probabilistic programming in Python using\nPyMC3. PeerJ Computer Science, 2, e55. URL: https://peerj.com/articles/cs-55. Publisher:\nPeerJ Inc.\nSchonlau, M., Welch, W. J. and Jones, D. R. (1998) Global versus local search in constrained optimiza-\ntion of computer models. In New developments and applications in experimental design, vol. 34, 11–26.\nInstitute of Mathematical Statistics.\nURL: https://projecteuclid.org/ebooks/institute-\nof-mathematical-statistics-lecture-notes-monograph-series/New-developments-\nand-applications-in-experimental-design/chapter/Global-versus-local-search-in-\nconstrained-optimization-of-computer-models/10.1214/lnms/1215456182.\nSchweidtmann, A. M., Clayton, A. D., Holmes, N., Bradford, E., Bourne, R. A. and Lapkin, A. A.\n(2018) Machine learning meets continuous flow chemistry: Automated optimization towards the Pareto\nfront of multiple objectives.\nChemical Engineering Journal, 352, 277–282.\nURL: https://www.\nsciencedirect.com/science/article/pii/S1385894718312634.\nSchweidtmann, A. M., Esche, E., Fischer, A., Kloft, M., Repke, J.-U., Sager, S. and Mitsos, A. (2021)\nMachine Learning in Chemical Engineering: A Perspective. Chemie Ingenieur Technik, 93, 2029–2039.\nURL: https://onlinelibrary.wiley.com/doi/abs/10.1002/cite.202100083.\nSedgwick, R., Goertz, J., Stevens, M., Misener, R. and van der Wilk, M. (2020) Design of Experiments\nfor Verifying Biomolecular Networks. arXiv:2011.10575 [cs, q-bio, stat]. URL: http://arxiv.org/\nabs/2011.10575. ArXiv: 2011.10575.\nSelega, A. and Campbell, K. R. (2022) Multi-objective Bayesian Optimization with Heuristic Objectives\nfor Biomedical and Molecular Data Analysis Workflows. Transactions on Machine Learning Research.\nURL: https://openreview.net/forum?id=QspAcsAyis.\nShahriari, B., Swersky, K., Wang, Z., Adams, R. P. and de Freitas, N. (2016) Taking the Human Out of\nthe Loop: A Review of Bayesian Optimization. Proceedings of the IEEE, 104, 148–175.\nSharpe, C., Seepersad, C. C., Watts, S. and Tortorelli, D. (2018) Design of Mechanical Metama-\nterials via Constrained Bayesian Optimization.\nIn Volume 2A: 44th Design Automation Confer-\nence, V02AT03A029. Quebec City, Quebec, Canada: American Society of Mechanical Engineers.\nURL: https://asmedigitalcollection.asme.org/IDETC-CIE/proceedings/IDETC-CIE2018/\n51753/Quebec%20City,%20Quebec,%20Canada/273625.\nSiuti, P., Yazbek, J. and Lu, T. K. (2013) Synthetic circuits integrating logic and memory in living cells.\nNature Biotechnology, 31, 448–452. URL: https://www.nature.com/articles/nbt.2510.\nSnoek,\nJ.,\nLarochelle,\nH.\nand\nAdams,\nR.\nP.\n(2012)\nPractical\nBayesian\nOptimization\nof\nMachine\nLearning\nAlgorithms.\nIn\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems,\nvol. 25. Curran Associates, Inc.\nURL: https://proceedings.neurips.cc/paper/2012/hash/\n05311655a15b75fab86956663e1819cd-Abstract.html.\nSun, Y., Nathan-Roberts, W., Pham, T. D., Otte, E. and Aickelin, U. (2022) Multi-fidelity Gaussian\nProcess for Biomanufacturing Process Modeling with Small Data. URL: http://arxiv.org/abs/\n2211.14493. ArXiv:2211.14493 [cs].\nSwersky, K., Snoek, J. and Adams, R. P. (2013) Multi-Task Bayesian Optimization. In Advances in Neural\nInformation Processing Systems 26 (eds. C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani\nand K. Q. Weinberger), 2004–2012. URL: http://papers.nips.cc/paper/5086-multi-task-\nbayesian-optimization.pdf.\n21\nTaylor, C. J., Felton, K. C., Wigh, D., Jeraal, M. I., Grainger, R., Chessari, G., Johnson, C. N. and\nLapkin, A. A. (2023) Accelerated Chemical Reaction Optimization Using Multi-Task Learning. ACS\nCentral Science, 9, 957–968. URL: https://pubs.acs.org/doi/10.1021/acscentsci.3c00050.\nThe pandas development team, T. p. d. (2023) pandas-dev/pandas: Pandas. URL: https://zenodo.\norg/record/7979740.\nTighineanu, P., Skubch, K., Baireuther, P., Reiss, A., Berkenkamp, F. and Vinogradska, J. (2022)\nTransfer Learning with Gaussian Processes for Bayesian Optimization. In Proceedings of The 25th\nInternational Conference on Artificial Intelligence and Statistics, 6152–6181. PMLR. URL: https:\n//proceedings.mlr.press/v151/tighineanu22a.html. ISSN: 2640-3498.\nTitsias, M. (2009) Variational Learning of Inducing Variables in Sparse Gaussian Processes. In Proceedings\nof the Twelth International Conference on Artificial Intelligence and Statistics, 567–574. PMLR. URL:\nhttps://proceedings.mlr.press/v5/titsias09a.html. ISSN: 1938-7228.\nTitsias, M. and Lawrence, N. D. (2010) Bayesian Gaussian Process Latent Variable Model. In Proceedings\nof the Thirteenth International Conference on Artificial Intelligence and Statistics, 844–851. JMLR\nWorkshop and Conference Proceedings. URL: https://proceedings.mlr.press/v9/titsias10a.\nhtml. ISSN: 1938-7228.\nUhrenholt, A. K. and Jensen, B. S. (2019) Efficient Bayesian Optimization for Target Vector Estimation.\nIn Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics\n(eds. K. Chaudhuri and M. Sugiyama), vol. 89 of Proceedings of Machine Learning Research, 2661–\n2670. PMLR. URL: https://proceedings.mlr.press/v89/uhrenholt19a.html.\nVirtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski,\nE., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman,\nK. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, I., Feng,\nY., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero,\nE. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F. and van Mulbregt, P. (2020)\nSciPy 1.0: fundamental algorithms for scientific computing in Python. Nature Methods, 17, 261–272.\nURL: https://www.nature.com/articles/s41592-019-0686-2. Number: 3 Publisher: Nature\nPublishing Group.\nWadle, S., Lehnert, M., Rubenwolf, S., Zengerle, R. and von Stetten, F. (2016) Real-time PCR probe\noptimization using design of experiments approach. Biomolecular Detection and Quantification, 7,\n1–8. URL: https://www.sciencedirect.com/science/article/pii/S2214753515300139.\nZadeh, J. N., Steenberg, C. D., Bois, J. S., Wolfe, B. R., Pierce, M. B., Khan, A. R., Dirks, R. M.\nand Pierce, N. A. (2011) NUPACK: Analysis and design of nucleic acid systems. Journal of Compu-\ntational Chemistry, 32, 170–173. URL: https://onlinelibrary.wiley.com/doi/abs/10.1002/\njcc.21596.\nZhang, Y., Tao, S., Chen, W. and Apley, D. W. (2020) A Latent Variable Approach to Gaussian Process\nModeling with Qualitative and Quantitative Factors. Technometrics, 62, 291–302. URL: https:\n//www.tandfonline.com/doi/full/10.1080/00401706.2019.1638834.\nZhuang, F., Qi, Z., Duan, K., Xi, D., Zhu, Y., Zhu, H., Xiong, H. and He, Q. (2021) A Comprehensive\nSurvey on Transfer Learning. Proceedings of the IEEE, 109, 43–76. Conference Name: Proceedings\nof the IEEE.\n22\nÁlvarez, M. A., Rosasco, L. and Lawrence, N. D. (2012) Kernels for Vector-Valued Functions: A Review.\nFoundations and Trends® in Machine Learning, 4, 195–266. URL: https://www.nowpublishers.\ncom/article/Details/MAL-036. Publisher: Now Publishers, Inc.\n23\n8\nAppendix\nNomenclature\nAcronyms\nAvgGP Average Gaussian Process\nBP\nNumber of Base Pairs\nDNA\nDeoxyribonucleic Acid\nELBO Evidence lower bound to marginal likelihood for LVMOGP\nGC\nPercentage Guanine-Cytosine Content\nLMC\nLinear Model of Coregionalization\nLVMOGP Latent Variable Multi-output Gaussian Process\nMOGP Multi-output Gaussian Process\nNLPD Negative Log Predictive Density\nPCR\nPolymerase Chain Reaction\nRMSE Root Mean Squared Error\nFunctions\n¸EI(·) Expected improvement acquisition function\n¸p(·)\nAcquisition function including probability of feasibility\nGP\nGaussian Process\nf (·)\nFunction of x\nfrate(·) Rate function in competitor amplification\nfdrif t(·) Drift function\ng(·)\nLatent Gaussian processes in the linear model of coregionalisation\nk(·; ·) Gaussian Process covariance function\nKii(·; ·) Covariance function of the data\nKiu(·; ·) Cross covariance function between the data and inducing points\nKuu(·; ·) Covariance function of the inducing points\nm(·)\nGaussian Process mean function\nPF(·) Probability of feasibility\nParameters and Variables\n24\n‹\nstochastic variable defined as the squared difference between observed outputs and the target\nvalue\nf∗\nPredictions at locations X∗\nhp\nLatent variable of the pth output function\nI\nIdentity matrix\nu\nInducing variables\nW\nVector of weights of the latent functions in the linear model of coregionalisation\nx\nInput location such that x ∈ RD\n‘d\nLengthscale of dimension d\n›\nNoise added to y where › ∼ N(0; ff2\nnI))\n–\nNon-centrality parameter of target vector optimization expected improvement\n—(X∗)\nPredicted mean at locations X∗\n—hp\nMean of the pth latent variable\n‌\nCarrying capacity\nff(X∗) Predicted covariance at locations X∗\nff2\nk\nKernel variance\nff2\nn\nNoise variance of Gaussian process\nΣhp\nVariance of the pth latent variable\nfi\nCycle number\n„\nGaussian Process hyperparameters\nB\nCoregionalization matrix in the LMC\nD\nDimensions of x\nF\nFluorescence in DNA amplification reaction\nF0\nFluorescence at the beginning of the DNA amplification reaction\nFT\nFluorescence at the end of the DNA amplification reaction\nH\nLatent variables such that H = [h1; :::; hp]T ∈ RQH×P\nM\nMean of the variational distribution on Z\nP\nNumber of output functions in multi-output Gaussian Process\nq(·)\nVariational distribution\nQ\nNumber of covariance matrices in the LMC\n25\nS\nVariance of the variational distribution on Z\nt\n= ‹‚−2\nTrate\nTarget rate\nTdrif t Drift threshold\nX\nTraining inputs of Gaussian Process X = {x1; :::; xN} ∈ RN×D\nX∗\nLocations to be evaluated\ny\nNoisy evaluations of x\nybest\nData point which is closest to the target out of the train and test datasets for a given surface\nZ\nInducing points\nMiscellaneous\nH\nThe latent space in the LVMOGP\nG\nAn approximation to the cumulative non-central ffl2 distribution function\n26\n8.1\nLatent Variable Multi-output Gaussian Process Implementation\nGaussian processes are normally trained by maximizing the log marginal likelihood. However, the pres-\nence of the latent variable distributions in the LVMOGP means the log marginal likelihood is no longer\ntractable. Instead, Dai et al. (2017) used variational inference to approximate a lower bound to this log\nmarginal likelihood, following the method proposed by Titsias (2009) and Titsias and Lawrence (2010).\nIn variational inference, the aim is to minimize the Kullback-Leibler divergence between an approximate\nposterior and a true posterior.\nOur implementation of the LVMOGP takes a concatenation of the input data and their corresponding\nlatent variables ˜X = [X; H:] ∈ RN×(D+QH) where H: to denotes the vector of latent inputs for each\nobserved data point. All inputs Xp for the same output dimension will have the same latent variable, hp.\nFor the LVMOGP this variational lower bound is given as:\nELBO = −1\n2log(2ıff2\nn) +\nN\nX\ni=1\n»\n− 1\n2ff2n\nyT\n:i y:i + 1\nff2n\ny:i⟨Kiu⟩q(H:i)K−1\nuu M\n− 1\n2ff2n\nTr(K−1\nuu ⟨KT\niuKiu⟩q(H:i)K−1\nuu (MMT + S))\n− 1\n2ff2n\n(Tr(⟨Kii⟩q(H:i)) − Tr(K−1\nuu ⟨KT\niuKiu⟩q(H:i))\n–\n− KL[q(u)||p(u)] −\nN\nX\ni=1\nKL[q(H:i)||p(H:i)]\n(19)\nwhere ⟨K⟩q(hi) denotes a kernel expectation over the variational distribution of the latent variable of\ndata point i. Kii and Kuu are the covariance functions of the data and the inducing points Z respectively,\nwhile Kiu is the cross covariance function between the two. Tr is the trace of a matrix. M and S are the\nmean and covariance of the variational distribution over inducing points q(Z) ∼ N(M; S). The second\nterm in this expression can be viewed as a data fit term, while the last term can be seen as a complexity\npenalty.\nTwo types of prediction are relevant using the LVMOGP. The first is when we have new input points\nX∗ and new position on the latent space h∗. In this case, the posterior prediction can be calculated in\nclosed form. The second, and more likely, prediction case is when we want to predict a new point X∗\nat a point on the latent space where we already have data with latent variable hp. This integration is\nintractable, but following Titsias and Lawrence (2010), the first and second moments can be computed\nin closed form if using a squared exponential kernel.\n8.2\nData Availability\nRaw data is available on request from rdm-enquiries@imperial.ac.uk. Code for the synthetic experiments\ncan be found at the following link: https://github.com/RSedgwick/TLGPs Code for the DNA am-\nplification experiments Bayesian optimization can be found here: https://github.com/RSedgwick/\nTL_DOE_4_DNA.\n8.3\nData Summary\nEach competitor is defined by its primer-reporter combination. For each of these primer-pair combinations\nwe then have data at different guanine-cytosine content and no. of base pairs combinations. Table 1\ngives a summary of the number of unique locations on each of the competitors.\n27\nNot To Be Optimized\nTo Be Optimized\nPrimer Reporter Combination\nNo. Unique\nLocations\nPrimer Reporter Combination\nNo. Unique\nLocations\nFP004-RP004-EvaGreen\n28\nFP004-RP004-Probe\n53\nFP002-RP002x-Probe\n12\nFP001-RP001x-EvaGreen\n24\nFP004-RP004x-Probe\n12\nFP001-RP001x-Probe\n20\nFP001-RP001-Probe\n9\nRP001x-FP002-Probe\n19\nFP001-RP005-Probe\n8\nFP002-RP002x-EvaGreen\n15\nFP004-RP004x-EvaGreen\n8\nFP005-FP001-EvaGreen\n14\nFP003-RP008-Probe\n5\nFP004-FP005-Probe\n8\nFP006-RP006-Probe\n5\nFP005-FP001-Probe\n8\nFP005-RP005-Probe\n5\nFP005-FP004-EvaGreen\n8\nFP002-RP002-EvaGreen\n4\nRP002x-FP005-Probe\n8\nFP002-RP006-Probe\n4\nRP008x-FP001-EvaGreen\n8\nFP057.1.0-RP003x-Probe\n3\nRP008x-FP005-Probe\n8\nFP003-RP008x-EvaGreen\n3\nFP001-RP004-EvaGreen\n7\nFP003-RP008-EvaGreen\n3\nRP002x-FP004-EvaGreen\n6\nFP002-RP002-Probe\n3\nFP002-RP004-EvaGreen\n3\nFP001-RP001-EvaGreen\n2\nRP002x-FP002-EvaGreen\n2\nFP003-RP003-Probe\n1\nFP057.1.0-RP003x-EvaGreen\n1\nTable 1: Summary of the amount of data we have for each competitor design surface. Each unique\nlocation refers to a unique GC-BP combination.\n8.4\nExtra Bayesian Optimization Results\nThe following tables contain extra results for the Bayesian optimization experiments. The first table in\neach section, Tables 2 and 5, shows counts of the first model to get to the best point on a surface for\nall competitors and seeds. If two models get to the best point on the same iteration, they are both\ncounted as \"winners\". The second table, Tables 3 and 6 shows counts of the models with the lowest\ncumulative regret for each competitor and seed. The same thing applies if two models have the same\ncumulative regret. For the single objective optimization, Table 4 shows the average number of iterations\nfor each model to get within tolerance of the target rate (+/- 0.05). For the penalized optimization\nTable 7 shows the average number of iterations for each model to get either within tolerance of the rate\ntarget with no drift penalty, or to the best point (which may have a drift penalty). For some of the runs\nwith the drift penalty, some of the models failed to get to the best point for some surfaces within the\nexperimental budget. In these cases, those surfaces were discarded and the average was taken for the\nsurfaces where all the models had managed to get to the best point within the experimental budget.\n8.4.1\nSingle Objective Optimization\nExtra results for the single objective Bayesian optimization. These results demonstrate that the LVMOGP\ngets to the best point more often (Table 2) and has has the lowest cumulative regret (Table 3) more\noften than the other models. The LVMOGP also reaches the best point in the lowest number of iterations\nfor all the learning scenarios (Table 4).\n28\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n124\n121\n144\n255\nmodel’s choice\n107\n119\n97\n147\none at a time\ncenter\n140\n140\n156\n215\nmodel’s choice\n86\n118\n87\n191\nTable 2: Table showing counts of the first Gaussian process model to reach the best point on a surface for\nthe single objective Bayesian optimization experiments. The counts are the number of times a Gaussian\nprocess model did the best on a competitor for each seed. If two Gaussian process models performed the\nsame for a given instance, they are both counted. This is for 16 competitors and 25 random seeds.\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n182\n80\n140\n197\nmodel’s choice\n85\n94\n83\n117\none at a time\ncenter\n129\n140\n131\n206\nmodel’s choice\n99\n106\n87\n159\nTable 3: Table showing counts of the first Gaussian process model had the lowest cumulative regret\non a surface for the single objective Bayesian optimization experiments. The counts are the number of\ntimes a Gaussian process model did the best on a competitor for each seed. If two Gaussian process\nmodels performed the same for a given instance, they are both counted. This is for 16 competitors and\n25 random seeds.\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n3.13\n3.25\n3.11\n2.58\nmodel’s choice\n3.08\n2.63\n3.09\n2.44\none at a time\ncenter\n2.94\n3.06\n2.85\n2.15\nmodel’s choice\n2.94\n2.63\n2.63\n1.81\nTable 4: Table showing the mean number of iterations need for the models to get within tolerance of the\ntarget rate (+/- 0.05) for the single objective optimization. This is for 16 competitors and 25 random\nseeds.\n8.4.2\nBayesian Optimization with Drift Penalty\nExtra results for the Bayesian optimization with a penalty on drift. These results demonstrate that the\nLVMOGP gets to the best point more often (Table 5) and has has the lowest cumulative regret (Table 6)\nmore often than the other models for most of the learning scenarios. The LVMOGP also reaches the\nbest point in the lowest number of iterations for all the learning scenarios (Table 7).\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n142\n157\n123\n165\nmodel’s choice\n89\n122\n101\n111\none at a time\ncenter\n141\n137\n153\n217\nmodel’s choice\n75\n102\n79\n164\nTable 5: Table showing counts of the first Gaussian process model to reach the best point on a surface\nfor the penalized Bayesian optimization experiments. The counts are the number of times a Gaussian\nprocess model did the best on a competitor for each seed. If two Gaussian process models performed the\nsame for a given instance, they are both counted. This is for 16 competitors and 24 random seeds.\n29\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n180\n118\n100\n163\nmodel’s choice\n85\n103\n84\n111\none at a time\ncenter\n173\n118\n139\n204\nmodel’s choice\n83\n70\n65\n156\nTable 6: Table showing counts of the first Gaussian process model had the lowest cumulative regret on\na surface for the penalized Bayesian optimization experiments. The counts are the number of times a\nGaussian process model did the best on a competitor for each seed. If two Gaussian process models\nperformed the same for a given instance, they are both counted. This is for 16 competitors and 24\nrandom seeds.\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n2.47\n3.26\n2.95\n2.38\nmodel’s choice\n3.39\n3.13\n3.23\n2.13\none at a time\ncenter\n3.00\n3.20\n2.82\n2.47\nmodel’s choice\n2.70\n2.69\n2.44\n1.41\nTable 7: Table showing the mean number of iterations need for the models to either get within tolerance\nof the target rate (+/- 0.05) without drift penalty or reach the best point (which may have a penalty)\nfor the penalized optimization. For some runs, one or more of the models would not achieve this within\nthe experimental budget. In these cases, the affected competitors were removed and the mean taken of\nthe remaining. This is for 16 competitors and 24 random seeds.\n8.4.3\nComparison of Choice of First Point\nTable 8 shows the average regret of the first data point chosen by each of the models for each of the\nlearning scenarios. From this table, it is clear to see the AvgGP and the LVMOGP improve on the regret\nof the central point, and outperform the random selection of the MOGP and LMC. This demonstrates\nthat having a principled method of selecting the first point is useful for reducing regret.\nlearning scenario\nstarting point\nMOGP\nAvg GP\nLMC\nLVMOGP\nlearning many\ncenter\n0.588\n0.588\n0.588\n0.588\nmodel’s choice\n0.651\n0.499\n0.703\n0.464\none at a time\ncenter\n0.588\n0.588\n0.588\n0.588\nmodel’s choice\n0.675\n0.308\n0.623\n0.309\nTable 8: Table of the mean regret of the first data point for each of the learning scenarios for each of\nthe models.\n8.5\nOptimization of Gaussian Process Models\nWe used gradient descent to optimize the Gaussian process hyperparameters. The optimization of the\nhyperparameters of the Gaussian process models are non-convex problems, meaning gradient descent\nalgorithms will only find local optima. To improve the hyperparameter optimization procedure, we used\nprincipled methods of initialization along side random restarts to fit the same Gaussian process model\nmultiple times, and then select the hyperparameter configuration with the best log marginal likelihood.\nThese regimes differ slightly for the different models.\n30\nFor all model, unless otherwise states, we initialize the lengthscale randomly as ‘ ∼ Uniform(0; 1),\nnoise variance randomly as ffn ∼ Uniform(0; 0:1) and kernel variance ffk = 1. For the MOGP and AvgGP\nwe did nine random restarts with these settings.\nFor the LMC we used three different methods for initializing W and », with three random restarts\nfor each:\n• Both W and » random.\nIn this initialization, we initialize W ∼ Uniform(0:1; 1) and » ∼\nUniform(0:1; 1).\n• W random and » = 0. In this initialization W ∼ Uniform(0:1; 1) and » = 10−6. This initialization\nwas chosen as we thought it would favor solutions with small » so it would better fit the linear\ncorrelation case, where the test functions are generated as linear combinations of some linear\nfunctions.\n• W random and » = 1. In this initialization W ∼ Uniform(0:1; 1) and » = 1. We chose this\ninitialization to favor large », which is useful for the uncorrelated test case, as it would encourage\nthe output functions to behave independently of each other.\nThe random initialisations for W helped the initialisations for two reasons: firstly, in the GPflow\nimplementation if W is not initialized it defaults to a rank of 1, and secondly by initializing to random\nvalues rather than all one value we avoid saddle points on the optimization surface.\nFor the LVMOGP we used three different initialization procedures, again with three random restarts\nfor each:\n• Random. In this initialization all hyperparameters and variational parameters were initialized ran-\ndomly. the means of the latent variables were initialized as —H ∼ Uniform(−1; 1).\n• GPy. This is the method used in the GPy implementation of the LVMOGP (Dai et al., 2017), that\nhas following three steps:\n1. A sparse MOGP is fitted to the data using a set of inducing points Z which are common\nto all outputs. The mean predictions —(Z) ∈ RNU×P of the output function values at these\ninducing inputs is then calculated:\n—(Z) = K(Z; Z)[K(Z; Z) + ff2\nnI]−1Y:\n(20)\nThe sparse MOGP is used is ensure all output functions are observed at the same input\nlocations for the functional PCA, which is necessary when data is observed at different loca-\ntions on different surfaces. It also serves the purpose of smoothing the data plus the trained\nlengthscales are used to initialise the lengthscales of the observed dimensions of the LVMOGP.\n2. The mean predictions —(Z) ∈ RNU×P are then used as inputs to functional PCA. The first\nQH eigenvectors V ∈ RNU×QH and eigenvalues {–q}QH\nq=1 of —(Z)T —(Z) are calculated and\nused to project —(Z) into latent space\nH = —(Z)T V;\n(21)\nwhere H ∈ RP×QH. The relative contributions of each of the eigenvalues is also calculated\nas:\n&q =\n˜–q\nmax{˜–i}QH\ni=1\n˜–q =\n–q\nPQH\ni=1 –i\n(22)\n31\n3. The latent variables H from the functional PCA are used to initialize the latent variables of a\nBayesian Gaussian process latent variable model. The lengthscales of the Bayesian Gaussian\nprocess latent variable model are initialized to { 1\n&q }QH\nq=1. Once the Bayesian Gaussian process\nlatent variable model is trained, the latent variables and hyperparameters of the Bayesian\nGaussian process latent variable model are used to initialize those of the LVMOGP.\n• PCA. In this initialization, the first two steps of the GPy initialization are followed. This means\nfitting a sparse MOGP to the data and performing principle component analysis (PCA) on the\nposterior predictions at inducing point locations. The MOGP hyperparameters were then used to\ninitialize the LVMOGP observed lengthscale, kernel variance and noise variance. The output of the\nPCA was used to initialize the latent variable means and the lengthscale of the latent dimensions.\nThis initialization was chosen as a simplified version of the GPy initialization.\nSee the github repositories in Appendix 8.2 for more details.\nIn the synthetic experiments, we found the method of initializing the hyperparamters affected the end\nlog marginal likelihood, with no initialization outperforming all others for each model. Therefore, we\ndecided to continue with all initializations for the PCR data experiments. For the PCR data experiments\nwe did 10 random restarts for each initialization, due to the randomness of some of the initializations.\n32\n"
}