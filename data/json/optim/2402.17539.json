{
    "optim": "arXiv:2402.17539v1  [math.OC]  27 Feb 2024 The optimizing mode classiﬁcation stabilization of sampled stochastic jump systems via an improved hill-climbing algorithm based on Q-learning Guoliang Wang Abstract—This paper addresses the stabilization problem of stochastic jump systems (SJSs) closed by a generally sampled controller. Because of the controller’s switching and state both sampled, it is challenging to study its stabilization. A new stabilizing method deeply depending on the mode classiﬁcations is proposed to deal with the above sampling situation, whose quantity is equal to a Stirling number of the second kind. For the sake of ﬁnding the best stabilization effect among all the classiﬁcations, a convex optimization problem is developed, whose globally solution is proved to be existent and can be computed by an augmented Lagrangian function. More importantly, in order to further reduce the computation complexity but retaining a better performance as much as possible, a novelly improved hill- climbing algorithm is established by applying the Q-learning technique to provide an optimal attenuation coefﬁcient. A nu- merical example is offered so as to verify the effectiveness and superiority of the methods proposed in this study. Index Terms—Stochastic jump systems; sampled control; mode classiﬁcation and optimization; Lagrangian function; hill- climbing algorithm; Q-learning. I. INTRODUCTION S IGNIFICANTLY different from deterministic systems, stochastic jump system (SJS) can represent physical sys- tems experiencing random structure changes. Due to this sys- tem with multiple structures or modes, it makes its stabilization problems quite distinctive whose controller’s quantity is not unique. According to the designed controller depending on mode or not, the existing stabilization results are mainly classiﬁed into two categories. The ﬁrst kind needs the mode of controller to keep pace with others and are usually called to be mode-dependent control method such as [1]–[6]. Due to all the modes synchronized with each other at the same time, the effect of stabilization effect will be the best and have the least conservatism. However, this advantage is also its disadvantage which will be limited in practice, since a lot of effort is needed to keep this synchronization all the time. In theoretical research, an ideal assumption for all the mode information available in real time is commonly acquiescent. The second kind is mentioned as mode-independent control approach [7]– [10], in which the mode information is totally removed even if This work was supported by the Open Project of Key Field Alliance of Liaoning Province under Grant No. 2022-KF-11-03, the Educational Depart- ment Foundation of Liaoning Province under Grant No. JYTMS20231435, the National Natural Science Foundation of China under Grant No. 62073158. G. Wang is with School of Information and Control Engineering, Liaon- ing Petrochemical University, Fushun 113001, Liaoning, China (e-mail: glwang@lnpu.edu.cn). it is available sometimes. Because the mode information is to- tally removed in controller, it can stabilize a system regardless of its mode accessible or not and naturally bring more con- servatism. In contrast to mode-dependent control being very ideal, mode-independent control is excessively absolute due to mode information neglected completely. For the purpose of bridging the above methods and balancing their advantages and disadvantages, some improved controllers were developed, such as partially mode-dependent controller [11] and partial information controller [12]. However, the drawbacks of mode- dependent and -independent control methods have not been partly solved, and there are still some problems to be further studied. For example, the quantity of controllers denoted as M in the above references is no less than number of modes referred to be N. Consequently, an interesting problem about SJSs can be proposed such that whether one can stabilize an SJS using fewer controllers but more than one. In this situation, some results were found, see, e.g., disordered or unmatched controller [13], [14], and scheduling controller [15], [16]. Though the aim of fewer controllers stabilizing an SJS with more subsystems was realized, it can be found from these references that for each subsystem or mode, at most total M controllers were added once, while the equipment cost and conservatism of stabilization realizing were both increased. The main reason is that the developed controller in these ref- erences was designed on the whole modes without considering them separately. When the controller is designed according to the mode classiﬁcations, a quantity limited controller method was in proposed in [17]. However, the mode classiﬁcation method for designing controllers is not fully studied, and many interesting and signiﬁcant topics need to be further researched. For example, whether there is an optimization classiﬁcation or not and how to ﬁnd the best mode classiﬁcation will be the ﬁrst problems, and more extensions based on optimizing mode classiﬁcation method will also be meaningful. On the other hand, it is well known that networked control system (NCS) is a control system whose components are con- nected through a communication network. Though NCS has such advantages, there is a precondition that all the transmitted data should be ﬁrst sampled. Importantly, the appearance of sampled data complicates system analysis and synthesis and causes many unpredictable problems. It can further bring some negative effects such as communication delays, data packet dropouts and/or packet disordering, network attack, some of which are not easy. Thus, it is very important to research the sampling phenomenon scientiﬁcally. Particularly, when an SJS is connected by a network, some novel but hard issues will encounter such that not only state but also switching signal are sampled. Due to switching being stochastic, its sampling is speciﬁc and signiﬁcantly different from state sampled. Ac- cordingly, some interesting but challenging problems emerge. Up to now, very few results are found to study the stabilization of stochastic systems via a sampled-switching controller. On the one hand, the author in [18] ﬁrst considered its stabi- lization problem by applying an auxiliary system approach. More extensions [19], [20] were further obtained base in this method. Since the stabilization problem of original sampling system was transformed to study its auxiliary system without any sampling, it decided that the sampling bound should be very small. On the other hand, based on an augmented system method, the stabilization of Markovian jump systems (MJSs) closed by a sampled switching and state controller was studied in [21], whose sampling effect was modeled to be a time- varying exponential matrix. Unfortunately, the convergency guaranteed by the reference was only asymptotically mean stable and was worse than the common stability concepts such as almost surely exponentially stable, globally asymptotically stable and so on. To summarize, it is necessary to further in- vestigate the stabilization problem of sampled stochastic jump systems. Moreover, new approaches are expected to provide larger sampling bounds and better convergency properties. Meanwhile, many difﬁculties will encounter in the research, some of which are challenging. For example, how to establish a quantized correlation between the original switching and its sampled value will be the ﬁrst difﬁculty to encounter, since no more suitable models are available. Particularly, it will be challenging to achieve the above expected objectives, because both switching and state are sampled simultaneously. Especially so many possible combinations of switching and its sampled values will emerge and bring large difﬁculties in system analysis and synthesis. The main contributions of this paper are as follows: 1) For the aim of overcoming the control difﬁculties of sampling in both state and switching signals, a new stabilizing controller is established to be very closed to the mode classiﬁcations. On the one hand, the controller’s quantity is smaller than mode-dependent controllers, and its switching is not necessary synchronous to the original one. On the other hand, the conservatism is smaller than mode-independent ones [7]–[10]; 2) So as to further improve the stabilization performance, a convex optimization problem is presented to determine the best classiﬁcation of sampled stabilization, which is better than [17] without mode optimization. It can be shown that the optimal solution can exist and be obtained by computing some equations coming from an augmented Lagrangian function; 3) Due to the classiﬁcation quantity being a Stirling number of the second kind and large, a novel method having less com- plexity but better control performance is proposed based on the hill-climbing algorithm by using the Q-learning technique to ensure an optimal attenuation coefﬁcient. Particularly, not only the monotonicity but also the convergency of method in this paper is guaranteed; 4) Compared with the existing auxiliary system approach [18] and augmented system method [21], the developed method is less conservative and has larger sampling bounds. Moreover, the key idea in this paper can popularize in many situations as long as the system switching experiences a sampling or mismatching phenomenon. Notation R, R>0, R≥0 and N>0 represent the sets of real numbers, positive real numbers, non-negative real numbers and positive integers respectively. Rn denotes the n-dimensional Euclidean space, and RN ⪰0 is a set of vector whose each element is non-negative. P (·) and E (·) are the probability and expectation operators respectively. ∥·∥ refers to the Euclidean vector norm or spectral matrix norm. λmin(M) and λmax(M) denote the smallest and largest eigenvalues of a square matrix M, χM ≜ λmax \u0010 M+MT 2 \u0011 and (M)⋆ ≜ M + M ⊤. II. PROBLEM FORMULATION Consider a stochastic jump system described as ˙x(t) = Aη(t)x(t) + Bη(t)u(t) (1) where x(t) ∈ Rn, Aη(t) ∈ Rn×n and Bη(t) ∈ Rn×m and u(t) ∈ Rm. The stochastic switching process {η(t), t ≥ 0} is a piecewise constant function and right-continuous. In detail, it is actual the semi-Markvoian switching and takes values from a set N ≜ {1, 2, . . . , N} such as η(t) = i ∈ N , ∀t ∈ [Tn, Tn+1), n ∈ N, where switching instant Tn satisﬁes 0 = T0 < T1 < · · · < Tn < · · · . For simplicity, η(Tn) = i at the nth switching instant is simply denoted to be ηn = i. In contrast to the usual controllers designed for stochastic systems including (1), a general controller with sampling phenomenon is described to be u(t) = Kη(tk)x(tk), ∀t ∈ [tk, tk+1) , ∀k ∈ N (2) where Kη(tk) ∈ Rm×n is the control gain, tk is the sampling instant such as 0 = t0 < t1 < · · · < tk < · · · , and sk ≜ tk −tk−1 is denoted as the sampling interval. Obviously, the simultaneous existence of two instant sequences {Tn}n∈N and {tk}k∈N makes the system analysis and synthesis not easy. Especially, due to η(t) being sampled, it will bring large difﬁculties and also complicates the closed-loop system. The ﬁrst reason is that so many combinations about original signal η(t) and its sampled signal η(tk) encounter and inevitably result in great complexity and large conservatism. The second reason, but not the last, is that the sampled state x(tk) existing simultaneously also leads to negative effects such that the correlation between the original and sampled states on each subinterval is hard to be done. In order to solve the above mentioned problems, the controller is developed as u(t) = N X ℓ=1 α[ν(η(tk))] ℓ ¯Kℓx(tk), ∀t ∈ [tk, tk+1) , ∀k ∈ N (3) where α[ν(η(tk))] ℓ ∈ R and ¯Kℓ ∈ Rm×n are to be designed. Particularly, the sampling instant tk of controller (3) is differ- ent from (2). In detail, the sampling instant of (3) is event- triggered such as tk+1 = min t≥tk {t : η(tk) ∈ Nh, η(t) /∈ Nh} (4) Accordingly, function ν (η(t)) on interval ∀t ∈ [tk, tk+1) with property (4) is deﬁned as ν (η(t)) ≡ ν (η(tk)) = h, when η(t) ∈ Nh. Meanwhile, set Nh is a newly constructed subset based on N and deﬁned as N D = {N1, N2, . . . , NM} (5) where Nh ⊆ N , h ∈ M ≜ {1, 2, . . ., M}, and constant M is the quantity of elements of set N D and denoted as num(N D) = M ∈ N>0. Moreover, all the elements of set N D are mutually exclusive such as Nh \\ Nℓ = ∅, ∀h ̸= ℓ ∈ M (6) Then, it can be concluded that N = M [ h=1 Nh and M X h=1 nh = N (7) where nh ≜ num (Nh) ∈ N>0. Moreover, subset Nh can be further expressed as Nh = n i[Nh] 1 , i[Nh] 2 , . . . , i[Nh] nh o (8) where i[Nh] ℓ ∈ N , ℓ = 1, 2, . . ., nh. When the above men- tioned classiﬁcation and event-trigger scheme achieve, con- troller (3) will be superior to (2), which can be realized easily and of larger signiﬁcance in theory and practice. Moreover, a mapping between M controllers and N subsystems or modes should be introduced. In the next, it will be seen that this mapping can not only be used to design a new controller for stochastic systems but also bridge the traditionally mode- dependent and -independent controllers very well. REMARK 1: It is noted that controller (3) includes some existing controllers without any sampling signals as special situations in which neither x(t) or η(t) is sampled. First of all, when M = N and Nh = {h}, ∀h ∈ N , in addition to PN ℓ=1 α[h] ℓ ≡ 1 with α[h] ℓ ∈ {0, 1} but α[h] h ≡ 1, controller (3) without sampled state x(tk) but with x(t) will be simpliﬁed to the traditionally mode-dependent controller such as u(t) = ¯Kη(t)x(t). Second, under a deterministic classiﬁcation same to the ﬁrst situation, and if PN ℓ=1 α[h] ℓ ≡ 1 with α[h] ℓ ∈ (0, 1), one could get a partial information controller similar to [12]. Moreover, if α[h] ℓ takes discrete values such as α[h] ℓ ∈ {0, 1}, a disordered controller similar to [13] will be obtained. Thirdly, when there is only one element in N D such as N1 = N , there will be only one element in M such as M = {1}. Then, controller (3) with x(tk) replaced by x(t) will reduce to be the traditionally mode-independent controller u(t) = K1x(t) = PN ℓ=1 αℓ ¯Kℓ. When αℓ is equal to the stationary distribution, the mode-independent controller u(t) = PN ℓ=1 αℓ ¯Kℓ will be an optimal estimation of ¯Kℓ similar to [17], [22]. Though some existing controllers are included as special situations of sampled controller (3), their methods cannot be used to deal with (3). Obviously, controller (3) deeply depends on the mode classiﬁcation. Meanwhile, so many possible combinations of the division about set N are involved whose division value is a positive natural number. Particularly, it can be known from [23] that its value is equal to S2(N, M) which is a Stirling number of the second kind and described to be S2(N, M) = S2(N − 1, M − 1) + MS2(N − 1, M) Its detailed expansion [24] is given to be S2(N, M) = 1 M! M X v=0 (−1)v \u0012 M v \u0013 (M − v)N (9) When the classiﬁcation of (3) is not given in advance, con- troller (3) should deeply depend on the Stirling number of the second kind and be rewritten as u(t) = N X ℓ=1 α[ν(η(tk))] ℓ (c) ¯Kℓx(tk), ∀t ∈ [tk, tk+1) , ∀k ∈ N (10) where c is selected to be only one value from c ∈ C ≜ {1, 2, . . . , S2(N, M)}. Accordingly, the related symbols in deﬁnitions (5)-(8) will change to be N D(c) = {N1(c), N2(c), . . . , NM(c)} Nh(c) = n i[Nh(c)] 1 , i[Nh(c)] 2 , . . . , i[Nh(c)] nh(c) o where nh(c) ≜ num (Nh(c)) ∈ N>0, whose properties (6) and (7) satisfy too. Then, two necessary but not easy problems are proposed as follows: Problem 1: Whether does the best classiﬁcation of set N exist or not? Problem 2: How to ﬁnd the best classiﬁcation to design a sampled stabilizing controller such as (10)? First of all, a cost function is introduced to be J(α(c)) = M X h=1 \r\r\r\r\r N X ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r\r\r 2 (11) where matrix α(c) ≜ \u0010 α[h] ℓ (c) \u0011 ∈ RM×N is composed of α[h] ℓ (c) but depends on classiﬁcation parameter c. Then, the related optimization problem is to ﬁnd the optimal classiﬁca- tion and its related optimal classiﬁcation parameter α[h] ℓ (c) of sampled controller (10) satisfying J∗ = min c∈C min α[h] ℓ (c)∈R M X h=1 \r\r\r\r\r N X ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r\r\r 2 = min c∈C M X h=1 J[h] \u0010\u0010 α[h](c) \u0011∗\u0011 = min c∈C J (α∗ (c)) (12) where the optimal solution α∗ (c) satisﬁes J (α∗ (c)) ≜ minα[h] ℓ (c)∈R PM h=1 \r\r\rPN ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r 2. Similarly, one can deﬁne \u0000α[h](c) \u0001∗ such as J[h] \u0010\u0000α[h](c) \u0001∗\u0011 ≜ minα[h] ℓ (c)∈R \r\r\rPN ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r 2, where J[h] \u0000\u0000α[h](c) \u0001\u0001 ≜ \r\r\rPN ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r 2 > 0 means there is always a controller added to each subsystem. It can be seen from (12) that a normal method getting the solution to Problem 2 is to compute cost function (11) one by one whose process will repeat S2(N, M) times. After doing that, one can select the solution minimizing the value of (11) as the globally optimal solution. Obviously, when N and M become large, the complexity of computation will be very large. Thus, it is necessary to develop a new way which provides a suboptimal solution nearing the globally optimal solution as much as possible but has smaller complexity without computing (11) one by one. In this paper, an improved hill- climbing algorithm (HCA) based on the Q-learning technique will be developed, which can not only avoid traversing all the classiﬁcations but also guarantee the proposed algorithm some good properties such as convergence and monotonicity. As a result, the computation complexity will be largely reduced, while the cost function will also have an optimal value. Some fundamentally dealing processes should be established ﬁrstly. First of all, some constants about classiﬁcation set N D(c) are needed and important in designing the optimization method to be presented. In detail, each element Nh(c) of N D(c) is ar- ranged by h such as N D(c) = {N1(c), N2(c), . . . , NM(c)}. Meanwhile, all the elements of Nh(c) are rewritten as an ascending sequence such as i[Nh(c)] 1 (c) < i[Nh(c)] 2 (c) < · · · < i[Nh(c)] nh(c) (c), since they are natural numbers and distinct from each other. Then, two kind parameters referred to be ξ and ζ about subset Nh(c) can be generated. On the one hand, parameter ξ is generated by arranging every nh(c) with h selecting 1, 2, . . . , M simultaneously and sequently such as ξ = n1(c)n2(c) · · · nM(c). On the other hand, similar to ξ, parameter ζ is constructed by arranging all the elements of every subset following an ascending sequence such as ζ = i[N1(c)] 1 i[N1(c)] 2 · · · i[N1(c)] n1(c) · · · i[Nℓ(c)] nℓ(c) · · · i[NM(c)] nM(c) Based on these parameters, an one-to-one correlation or mapping between sets D ≜ \b N D(1), N D(2), . . . , N D(S2(N, M)) \t and C can be obtained by arranging the value of c orderly along with the values of ξ and ζ increasing successively. To illustrate the above process clear and vividly, an example about N = {1, 2, 3, 4, 5} and M = {1, 2, 3} is given in Table I. There, all the subset classiﬁcations about N = 5 and M = 3 with S2(5, 3) = 25 are listed followed by the above proposed process, while parameters ξ and ζ are computed too. Without loss of generality, only two situations are discussed in detail. The ﬁrst situation is N1(c) = {1}, N2(c) = {2} and N3(c) = {3, 4, 5}. Obviously, one knows n1(c) = 1, n2(c) = 1 and n3(c) = 3, while ξ = 113 and ζ = 12345. Because of ζ = 12345 being the smallest among ξ = 113, and based on the proposed mapping principle, one should select c = 1. Similarly, c = 2 means the second-smallest value of ζ among ξ = 113 such as ζ = 13245. The second situation is N1(c) = {1}, N2(c) = {2, 3} and N3(c) = {4, 5}. Then, one knows n1(c) = 1, n2(c) = 2 and n3(c) = 2, such that ξ = 122 and ζ = 12345. Though the values of ζ about the two situations are the same, the values of ξ are different. Due to 113 < 122, the values of parameter c are different and selected to be c = 1 and c = 11 respectively followed by the above mapping. The other situations are all given in Table I, whose detailed computation processes are omitted. TABLE I THE CLASSIFICATION OF COMBINATIONS AND THEIR NUMBERS N1(c) N2(c) N3(c) ξ ζ c {1} {2} {3, 4, 5} 113 12345 1 {1} {3} {2, 4, 5} 113 13245 2 {1} {4} {2, 3, 5} 113 14235 3 {1} {5} {2, 3, 4} 113 15234 4 {2} {3} {1, 4, 5} 113 23145 5 {2} {4} {1, 3, 5} 113 24135 6 {2} {5} {1, 3, 4} 113 25134 7 {3} {4} {1, 2, 5} 113 34125 8 {3} {5} {1, 2, 4} 113 35124 9 {4} {5} {1, 2, 3} 113 45123 10 {1} {2, 3} {4, 5} 122 12345 11 {1} {2, 4} {3, 5} 122 12435 12 {1} {2, 5} {3, 4} 122 12534 13 {2} {1, 3} {4, 5} 122 21345 14 {2} {1, 4} {3, 5} 122 21435 15 {2} {1, 5} {3, 4} 122 21534 16 {3} {1, 2} {4, 5} 122 31245 17 {3} {1, 4} {2, 5} 122 31425 18 {3} {1, 5} {2, 4} 122 31524 19 {4} {1, 2} {3, 5} 122 41235 20 {4} {1, 3} {2, 5} 122 41325 21 {4} {1, 5} {2, 3} 122 41523 22 {5} {1, 2} {3, 4} 122 51234 23 {5} {1, 3} {2, 4} 122 51324 24 {5} {1, 4} {2, 3} 122 51423 25 Secondly, after introducing the mapping between sets D and C , an optimization algorithm based on a local search can be proposed and remain some advantages. In detail, on the one hand, a trajectory only using the selected partial nodes instead of exploiting all the division nodes is generated. In other words, it can lead to a locally even globally optimal solution by an iterating method. On the other hand, a favorable con- vergence and monotonicity of the proposed algorithm should be satisﬁed. Particularly, based on the proposed mapping, the normal HCA can be used to realize the former requirement of locally optimal solution. However, the other properties on the convergence and monotonicity described in the latter are not guaranteed. In order to make a brief introduction to the HCA, some deﬁnitions are needed to be clariﬁed, while the other details can be found in the existing similar references such as [25]. For convenience, in the following, we will mention the detailed classiﬁcation set N D(c) by using scalar c equiva- lently. Then, the scalar parameter c in (11) will have a speciﬁc meaning, which actually corresponds a detailed classiﬁcation N D(c) and makes it possible to get the optimal solution by applying the HCA. Unfortunately, at least two shortcomings are found in directly applying set C , when the HCA is to be used. For one thing, the element of C has a clustering phenomenon. In other words, the clustering phenomenon will likely lead to a worse locally optimal solution, when the search interval radius of the traditionally HCA is not large enough. For another, due to the element of C being a natural number, a big range between two elements exists and complicates the computation of (11). In order to solve the problems coming from C , another set Ω referred to be “the nominal solution set” is deﬁned as Ω ≜ {ω(1), ω(2), . . . , ω (S2(N, M))}, where ω(c), ∀c ∈ C , is an any real number in the interval (0, ǫ] with a given real ǫ ∈ R>0. Meanwhile, it is further required that ω(c), ∀c ∈ C , is distinct such as ω(c) ̸= ω(j), ∀c, j ∈ C and c ̸= j. More importantly, a corresponding correlation between sets C and Ω is denoted such that the value of ω(c) corresponds to the classiﬁcation set N D(c), when the scalar c belongs to set C . After doing that, another discrete mapping between sets J ≜ {J (α∗ (1)) , J (α∗ (2)) , . . . , J (α∗ (S2(N, M)))} and Ω is introduced to be f : Ω → J such as f(ω(c)) = J (α∗ (c)) , ∀c ∈ C (13) When some elements’ values of set J are equal, one just needs to remain one and delete the other same values. Based on the characteristic of mapping (13), it can be seen that the deletion of the same values of set J has nothing to do with the considered problem in this paper. Without loss of generality, all the elements’ values of J are assumed to be distinct in the next. Particularly, due to f(ω(c)) = J (α∗ (c)), ω(c) is said to be “the nominally optimal solution” to (11) with c ∈ D given in advance, whose optimization effect is the same as α∗ (c). After doing this, the problems mentioned can be done by applying set C directly. Finally, in order to exploit the HCA successfully, another variable ˆω(k) ∈ Ω, k = 1, 2, . . . , kmax, is introduced, where k is the a number of k-th iteration and kmax is the maximum number of iteration and obviously satisﬁes kmax ≤ S2(N, M). Because of ˆω(k) ∈ Ω, it is true that for any iteration number k, there is always only one ω(c) satisfying ˆω(k) = ω(c), ∃c ∈ C . Meanwhile, when the HCA is mentioned, variable ˆω(k) is normally simply said to be “a solution” and selected as the k-th iteration value. For the sake of distinction and simplicity, it will be named as “nominal solution” in the next. Then, a neighbour set Θ (ˆω(k)) about nominal solution ˆω(k) ∈ Ω is deﬁned as Θ (ˆω(k)) ≜ n ˆω′(k) ∈ Ω \f\f\f|ˆω′(k) − ˆω(k)| < L, ˆω′(k) ̸= ˆω(k) o (14) where L > 0 is the radius of search interval. The neighborhood probability mass function gˆω(k)(ˆω′(k)) is described as gˆω(k)(ˆω′(k)) ≜ P (ˆω′(k) ∈ Θ (ˆω(k))) = 1 num (Θ (ˆω(k))) (15) Meanwhile, the update law of ˆω(k) is given by ˆω(k + 1) = ˆω′(k), if f(ˆω′(k)) − f(ˆω(k)) < 0 (16) It is noted that the above algorithm only ensures the mono- tonicity of function such as f(ˆω(k+1)) < f (ˆω(k)) but not the convergence of nominal solution ˆω(k) such as |eˆω(k + 1)| < |eˆω(k)| where eˆω(k) ≜ ˆω(k+1)−ˆω(k). Thus, the convergence speed of nominal solution cannot be guaranteed. To illustrate the utility of the above proposed algorithm, an example simulation about 100 distinct random points will be given. In detail, their horizontal and vertical co- ordinates referred to be ω(c) and f (ω(c)) respectively are randomly generated on interval (0, 20], which are fur- ther denoted as Ω = {ω(1), ω(2), . . . , ω(100)} and J = {f (ω(1)) , f (ω(2)) , . . . , f (ω(100))}. First of all, some sim- ulations are shown in Fig. 1, in which different search radius L such as L = 20, 10.71, 5.68 and 2.84 are given. On 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 a1: L = 20 a2: L = 10.71 a3: L = 5.68 a4: L = 2.84 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process Subtitle 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 b1: L = 20 b2: L = 10.71 b3: L = 5.68 b4: L = 2.84 Fig. 1. The iteration effects of (14) under different constants L with different initial points. the one hand, the red point is the initial iteration point, and the gray points are “useless” ones which are not used at all. Without loss of generality, two situations about differ- ent initial iteration points such as Point 16 and Point 83 are considered. Their horizontal and vertical coordinates are selected to be (ˆω(1), f (ˆω(1))) = (ω(16), f (ω(16))) and (ˆω(1), f (ˆω(1))) = (ω(83), f (ω(83))) respectively. On the other hand, the blue points are the failed ones during the iter- ation, while the green ones are the successful ones satisfying the update law (16). By investigating these simulations, it can be found that the bigger the value L takes, the smaller the value f (ˆω(k)) arrives. However, the computation complexity will be larger as L takes bigger values. To the contrary, when the radius is selected to be small enough, it can be concluded from Fig. 1 that the computation complexity will be great reduced. However, the probability of the obtained optimal solution being a locally optimal solution will be higher and have a worse quality. Thus, how to select a suitable radius is also one topic studied in this paper. The search radius L of (14) is selected to be Lk+1 ≜ δ · |ˆω(k + 1) − ˆω(k)| , k = 1, 2, . . . (17) where δ ∈ (0, 1] is a attenuation coefﬁcient. Particularly, the initial search radius L1 is also deﬁned as L1 ≜ δ · |ˆω(1) − ˆω(0)|, where ˆω(0) is given to be ˆω(0) = ( 0, if ˆω(1) > ǫ 2 ǫ, otherwise Then, one can get a simulation Fig. 2 similar to Fig. 1, where the values of δ about the initial radius are selected to be δ = 1, 0.7, 0.4 and 0.2 respectively. Accordingly, the values of initial radius L1 about initial value ˆω(1) = ω(16) based on (17) are computed to be L1 = 15.3, 10.71, 6.12 and 3.06 respectively. Meanwhile, the values of initial radius L1 about initial value ˆω(1) = ω(82) are computed to be L1 = 14.2, 9.94, 5.68 and 2.84 respectively. On the one hand, by the simulations of Fig. 2 presented in rows respectively, it can be concluded that the computation complexity is usually reduced along with δ decreasing under the same initial condition. On the other hand, in order to make some comparisons between Fig. 1 based on (14) and Fig. 2 based on (17), it had better to compare them under the same conditions such as the same ˆω(1) and L1. Thus, only a part but not all the subgraphs are necessary to make comparisons such as the comparisons on subgraphs ai, i = 1, 2, and bi, i = 1, 3, 4 between Fig. 1 and Fig. 2 respectively. In detail, not only the fewer iteration times but also the fewer failed points are presented in Fig. 2. More importantly, the convergence of nominal solution ˆω(k) is guaranteed in Fig. 2, which is not satisﬁed in Fig. 1. However, there is also a negative effect that the gaps among the locally optimal solutions obtained in Fig. 2 are bigger, some of which are farther away from the globally optimal solution to subgraphs ai and bi of Fig. 1. Thus, it is necessary to further study the effect of variable radius in order to give an optimal variation of radius Lk. 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 a1 a2 a3 a4 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 b1 b2 b3 b4 Fig. 2. The iteration effects of (17) under different constants L with different initial points. For another, based on (17), it can be known that δ plays an important role in generating variable Lk. In order to realize an optimization to radius Lk, expression (17) is revised to be Lk+1 ≜ δk+1 · |ˆω(k + 1) − ˆω(k)| , k = 1, 2, . . . (18) where δk ∈ (0, 1]. Similar to the deﬁnition L1 in (17), it is deﬁned as L1 ≜ δ1 · |ˆω(1) − ˆω(0)|. Then, similar to Fig. 2, similar simulations under (18) are given in Fig. 3. Particularly, the initial values of parameters δk and Lk of subgraphs in Fig. 3 are given such as a1 : δ1 = 1, L1 = 15.3; a2 : δ1 = 0.7, L1 = 10.71; a3 : δ1 = 0.4, L1 = 6.12; a4 : δ1 = 0.2, L1 = 3.06; b1 : δ1 = 1, L1 = 14.2; b2 : δ1 = 0.7, L1 = 9.94; b3 : δ1 = 0.4, L1 = 5.68 and b4 : δ1 = 0.2, L1 = 2.84. By making the comparisons of all the subgraphs between Figs. 2 and 3, it can be found that less computation complexity of (18) is presented in Fig. 3, whose iteration time is fewer too. Moreover, the minimum value of f (ˆω(k)) obtained in Fig. 2 based on (17) is larger than one in Fig. 3 by applying (18). In other words, it is necessary and important to study how to select the detailed value during every iteration to gain a better performance such as smaller value of (12), less computation complexity, but better monotonicity and convergence. All these problems will be considered and ﬁnally solved in this paper. 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 a1 a2 a3 a4 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 14 16 18 20 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 b1 b2 b3 b4 Fig. 3. The iteration effects of (18) under different constants L with different initial points.. DEFINITION 1: A control update sequence {tk, k ∈ N} is said to have the ﬁnite sampling rate property if there exists a scalar s such that sk ≥ s > 0, k ∈ N>0. Assumption 1: It is assumed that the event-triggered sam- pling interval sk, k ∈ N>0, has an upper bound such as sk ≤ min {¯τ1, ¯τ2, . . . , ¯τM}, where ¯τh = P ℓ∈Nh ˆτℓ, ∀h ∈ M , and ˆτℓ ≜ E [τℓ(n)], ∀ℓ ∈ Nh, ∀n ∈ N>0. DEFINITION 2: System (1) closed by controller (3) is globally asymptotically stable almost surely (GAS a.s.), if the following conditions hold simultaneously. (C1) For any ǫ ∈ (0, 1), there is a δ = δ(ǫ) > 0 such that P \u0000supt≥0 ∥x(t)∥ < ǫ \u0001 > 1 − ǫ, when ∥x0∥ < δ. (C2) For any γ > 0 and ξ > 0, there is a positive random variable ϕ(γ, ξ) such that P \u0010 supt≥ϕ(γ,ξ) ∥x(t)∥ < ξ \u0011 = 1, when ∥x0∥ < γ. III. MAIN RESULTS THEOREM 1: Consider the system composed of (1) and (3), where the detailed classiﬁcation of controller (3) is assumed to be given in advance such that N D is given beforehand, so do the α[h] ℓ and ¯Kℓ, ∀h ∈ M , ∀ℓ ∈ N . Then, the related closed-loop system is GAS a.s., if given parameters λi ∈ R, µi > 1 and σh ∈ R>0, there exists a matrix Pi > 0 satisfying \u0010 Pi ¯A[h] i \u0011⋆ +2σh∥PiBi N X ℓ=1 α[h] ℓ ¯Kℓ∥I ≤ λiPi, ∀h ∈ M , ∀i ∈ Nh (19) Pi ≤ µiPj, ∀i, j ∈ N (20) N X i=1 πi \u0012 λi + ln µi ˆτi \u0013 < 0 (21) where ¯A[h] i = Ai + Bi PN ℓ=1 α[h] ℓ ¯Kℓ, πi = ¯πiˆτi PN j=1 ¯πj ˆτj , and ¯πi, ∀i ∈ N , is the stationary distribution of the embedded chain of semi-Markov process. Meanwhile, the upper sampling bound ¯τh is computed by ¯τh =          1 ∥ ˆAh∥ σh 1 + σh , if ˆχh ≤ 0 1 ˆχh log   ˆ σhχh ∥ ˆAh∥(1 + σh) + 1 ! , if ˆχh > 0 , ∀h ∈ M (22) where ∥ ˆAh∥ = maxℓ∈Nh n ∥ ¯A[h] ℓ ∥ o and ˆχh = maxℓ∈Nh {χAℓ}. Proof The proof is given in Appendix A. □ Next, Problems 1 and 2 will be considered. Before dis- cussing them, a convex optimization problem with some constraints should be proposed ﬁrst. On the one hand, from Ex- ample 3.11 of [27] on Page 82, it is clear that J[h] \u0000\u0000α[h](c) \u0001\u0001 , ∀h ∈ M but given c ∈ C , is convex. Moreover, function J(α(c)) = PM h=1 J[h] \u0000\u0000α[h](c) \u0001\u0001 is also convex. On the other hand, even if the set classiﬁcation of controllers is ﬁxed, the given conditions of Theorem 1 to be as constraints will be hard to be solved in the optimization problems, since parameters ¯Kℓ and Pi are vector and matrix variables respectively. As a result, different from Theorem 1, not only ¯Kℓ satisfying Kh ≜ PN ℓ=1 α[h] ℓ ¯Kℓ but also Pi here is given beforehand. Without loss of generality, they will be calculated by following LMIs   (Ai + BiYi)⋆ − λiXi BiYi Xi ∗ −2Xi + I 0 ∗ ∗ −σ−2I   ≤ 0, ∀i ∈ N (23) \u0014 −µiXj Xj ∗ −Xi \u0015 < 0, ∀i, j ∈ N (24) and (21), where λi ∈ R, σ ∈ R>0 and µi > 1 are given scalars, and Xi ≜ P −1 i and Yi ≜ ¯KiXi are variables to be computed by solving the above LMIs. Due to page limitation, the detailed calculation process is omitted here. REMARK 2: It is worth mentioning that the computation way for ¯Kℓ and Pi described in (21), (23) and (24) is not unique. In other words, they can be computed in advance by other ways, as long as the corresponding conditions described by (19)-(21) with given ¯Kℓ and Pi satisfy. Naturally, there exist inevitable differences among the different techniques providing different parameters’ values. However, such differ- ences will not lead to essential distinctions to the optimization problems in this paper and will not discuss in detail. Since the parameters of Theorem 1 such as λi ∈ R, µi > 1, σh ∈ R>0, ¯Kℓ and Pi are given beforehand, conditions (20) and (21) are known and always satisﬁed. In detail, parameters λi ∈ R, µi > 1, ¯Kℓ and Pi are deﬁned by (21), (23) and (24), while σh ∈ R>0 is a new given parameter and only deﬁned in (19). Then, the constraints described by conditions (19)-(21) are equivalently simpliﬁed to be s.t. Hi (α(c)) ≤ 0, ∀i ∈ Nh(c), c ∈ C (25) where Hi (α(c)) ≜ M X h=1 (\" Pi   Ai + Bi N X ℓ=1 α[h] ℓ (c) ¯Kℓ !#⋆ − λiPi + 2σh \r\r\r\r\rPiBi N X ℓ=1 α[h] ℓ (c) ¯Kℓ \r\r\r\r\r I ) According to Example 2.10 on Page 38 and Example 3.11 on Page 82 of [27], it is clear that constraint (25) is a convex set. Then, a convex optimization problem described by cost function (11) with constraint (25) is successfully proposed. According to Example 3.10 of [27] on Page 82, it can be concluded that function λmax Hi(α(c)) is also convex. Thus, the feasible domain of (25) is equivalently rewritten to be s.t. λmax Hi(α(c)) ≤ 0, ∀i ∈ Nh, c ∈ C (26) Since (26) being inequalities, a slack variable such as ςi(c) is introduced to change (26) be an equation constraint such as s.t. λmax Hi(α(c)) + (ςi(c))2 = 0, ∀i ∈ Nh, c ∈ C (27) Based on the given results of [28] on Pages 98 and 99, it can be known that such slack variables do not bring any negative effects to (26) but make the equivalent constraint (27) solved easily. Moreover, it can be concluded that constraint (27) is convex. Thus, the original convex optimization problem is transformed into convex cost function (11) with convex constraint (27). In order to solve this convex optimization problem, an augmented Lagrangian function is constructed as L (α(c), γ(c), ς(c), φ) ≜ J ((α(c))) + L1 (α(c), γ(c), ς(c)) + L2(α(c), ς(c), φ) (28) where L1(α(c), γ(c), ς(c)) ≜ N X i=1 γi(c)Ei(α(c)), ςi(c)) L2(α(c), ς(c), φ) ≜ φ 2 N X i=1 E2 i (α(c), ςi(c)) Ei(α(c), ςi(c)) ≜ λmax Hi(α(c)) + (ςi(c))2 γ(c) ≜ \u0002 γ1(c) γ2(c) · · · γN(c) \u0003⊤ ∈ RN ⪰0 ς(c) ≜ \u0002 ς1(c) ς2(c) · · · ςN(c) \u0003⊤ ∈ RN ⪰0 and penalty factor φ ∈ R>0. Here, ς(c) is the slack variable and γ(c) is the multiplier. Because (11) and (27) are both convex, it is obvious that function (28) is convex too. Meanwhile, it is obvious that the above computation process of the globally optimal value J∗ should be repeated by selecting every c ∈ C . Based on the above illustrations and simulations about mode classiﬁcation, the repetition number of computation will equal to the Stirling number of the second kind. Obviously, the computation complexity will be very large especially N and M increase big. In order to deal with this problem and motivated by the phenomena discovered in Figs. 1-3, an optimization algorithm based on an improved HCA will be developed, which can generate an iteration sequence of solution {ˆω(k)}kmax k=1 and so does the corresponding sequence of objective function {f(ˆω(k))}kmax k=1 . Moreover, the locally optimal solution ˆω∗ and its value f ∗ ≜ f(ˆω∗) can be reached such as ˆω∗ = ˆω(kmax). Moreover, one can conclude from Figs. 1-3 that strategy (18) will be the best if its dynamically attenuation coefﬁcient δk is suitably adjusted. In detail, it not only ensures all the advantages of strategy (17) but also improves the iteration speed and the quality of optimal solution ˆω∗. How to dynamically adjust the attenuation coefﬁcient δk to make (18) work optimally is an open problem. Here, we will use the Q-learning algorithm to dynamically adjust δk, so as to dynamically change the radius of search interval Lk and lead to an improvement in performance. Meanwhile, it is very necessary and important to analyze its mathematical model, which includes the state, action, and reward function. State: The state Sk is a feedback of the agent at the kth step and denotes the impact of its taking action on the environment. Here, Sk described as the produced solution ˆω(k) ∈ Ω at step k such as Sk ≜ ˆω(k). Meanwhile, a termination state Send should be given beforehand but does not belong to the solution set Ω. Its aim is to end the current training round when the state Sk arrives at the termination state Send. Finally, the state space S is composed of the solution set Ω of HCA and the termination state Send such as S = Ω S {Send}. Action: The action Ak is a description of the agent’s behavior at the kth step. Here, it is the speciﬁc value of δk of HCA at the kth iteration step such as Ak ≜ δk, which can change the neighborhood set Θ (ˆω(k)) and affect the generation of the next solution. As a result, all values of δk or Ak constitute the action space A. Reward: The reward Rk+1 is a feedback of the environ- ment in which the agent has taken the action Ak in state Sk. Here, it is described by Rk+1 =    f(ˆω(k)) − f(ˆω′(k)) |ˆω′(k) − ˆω(k)| , if num (Θ (ˆω(k))) ̸= 0 − f(ˆω(k)), otherwise (29) where function f(·) has been deﬁned in (13). In detail, two situations will be found if condition num (Θ (ˆω(k))) ̸= 0 is satisﬁed. On the one hand, the agent will be penalized and receive a negative reward, if the agent takes an action Ak in state Sk such that the solution ˆω′(k) satisﬁes f(ˆω′(k)) ≥ f(ˆω(k)). On the other hand, the agent will be encouraged and receive a positive reward, if the solution ˆω′(k) satisﬁes f(ˆω′(k)) < f(ˆω(k)). In other words, if the agent wants to obtain a larger positive reward, the solution ˆω′(k) should satisfy not only f(ˆω(k)) is larger than f(ˆω′(k)) as much as possible, but also the distance between ˆω′(k) and ˆω(k) is as small as possible. To the contrary, num (Θ (ˆω(k))) = 0 means the neighbor set Θ (ˆω(k)) of ˆω(k) is the empty set. It also means that no solution within Θ (ˆω(k)) whose objective function value is smaller than f(ˆω(k)). In this situation, the agent is still penalized but receives a negative reward −f(ˆω(k)). Then, according to [29] on Page 70, it is well known that the Q-learning effect of taking action Ak in state Sk can be depicted by the Q-value, i.e. it can make an evaluation about the variation of δk. Accordingly, a speciﬁc deﬁnition of Q-value similar to (3.11) in [29] is given as Q̟ (s, a) ≜ E̟  ∞ X q=0 γqRk+q+1 \f\f\fSk = s, Ak = a ! (30) where Q̟(s, a) is the Q-value and quantizes the effect of taking action Ak = a followed by the policy ̟ in state Sk = s, and γ ∈ [0, 1] is a given discount factor. Particularly, policy ̟ is a mapping from each state s ∈ S and action a ∈ A to the probability P (a|s) of taking action a when in state s. As a result, a different taking action Ak = a can be generated by a different strategy ̟ in state Sk = s and will lead to a different Q̟(s, a). In detail, when the Q-value is larger, the taking action will be better. To the contrary, the action selection will be worse. THEOREM 2: Consider a convex optimization problem described by (11) and (25), where the parameters ¯Kℓ and Pi are computed by solving conditions (21), (23) and (24), and parameters λi ∈ R, σ ∈ R>0, σh ∈ R>0 and µi > 1 are also given in advance. The optimization problem (11) constrained on (25) will have a globally optimal solution, if the following conditions are satisﬁed ∇α(c)L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (31) ∇ς(c)L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (32) ∇γ(c)L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (33) Then, the globally optimal value of (11) is obtained by J∗ ≜ J (α∗) = min c∈C J (α∗ (c)) = min c∈C L (α∗(c), γ∗(c), ς∗(c), φ) (34) Meanwhile, the globally optimal solution is computed by α∗ ≜ arg min c∈C J (α∗ (c)) = arg min c∈C L (α∗(c), γ∗(c), ς∗(c), φ) (35) Particularly, in order to get an optimal solution to convex problem (11) with (25) without computing c ∈ C one by one, an improved HCA based on Q-learning (Q-HCA) is proposed, whose searching radius iteration rule is given in (18). Meanwhile, δk is obtained by the Q-learning with state Sk = s, action Ak = a and reward Rk+1 presented in (29). Then, an iteration rule for estimating Q-value (30) is given as Qk+1(Sk, Ak) =              Qk (Sk, Ak) − β [Qk(Sk, Ak) − (Rk+1 + γ max ˆa∈A(Sk+1) Qk(Sk+1, ˆa) \u0013\u0015 , if (Sk, Ak) = (s, a); Qk (Sk, Ak) , otherwise (36) where β ∈ [0, 1] is the given learning rate, and A(Sk+1) is the set of taking actions in state Sk+1. As a result, an optimal attenuation coefﬁcient δ∗ k is obtained. Moreover, under any initial condition ∀ˆω(1) ∈ Ω with ˆω(0) similarly deﬁned in (17), a preferably nominal solution sequence {ˆωpre(k)}kmax k=1 with ˆωpre(1) = ˆω(1) can be developed by following rules (36) and (18), where kmax is a maximum value of iteration. Thus, a locally optimal solution to convex problem (11) with (25) satisfying a part of C is computed by ˆωpre(k) → ˆω∗ ≜ ˆωpre(kmax) (37) Meanwhile, the corresponding value of (11) is also locally optimal such as J∗ ≤ f ∗ and described to be f ∗ = f (ˆω∗) = f (ˆωpre(kmax)) < · · · < f (ˆωpre(1)) (38) More importantly, both convergence and monotonicity proper- ties of Q-HCA are ensured. Finally, the upper sampling bound ¯τh of the related optimal controllers is also computed by (22). Proof The proof is given in Appendix B. □ Specially, when controller (3) has no sampling phenomenon, it will reduce to be u(t) = N X ℓ=1 α[ν(η(t))] ℓ ¯Kℓx(t), ∀t ∈ [Tn, Tn+1) , ∀n ∈ N (39) where α[ν(η(tk))] ℓ ∈ R and ¯Kℓ ∈ Rm×n are similar to (3), and function ν (η(t)) also has the similar deﬁnition but on interval ∀t ∈ [Tn, Tn+1). Then, one has the following corollaries. COROLLARY 1: Consider the system composed of (1) and (39) with mode classiﬁcation N D and α[h] ℓ given beforehand. Then, the related closed-loop system is GAS a.s., if given parameters λi ∈ R and µi > 1, there exist Xi > 0, ¯Yℓ and W satisfying LMIs (21), (24) and \u0014 Γ[h] i1 Γ[h] i2 ∗ (−W)⋆ \u0015 < 0, ∀h ∈ M , ∀i ∈ Nh (40) where Γ[h] i1 = \u0010 AiW + Bi PN ℓ=1 α[h] ℓ ¯Yℓ \u0011⋆ − λiXi and Γ[h] i2 = AiW + Bi PN ℓ=1 α[h] ℓ ¯Yℓ + Xi − W ⊤. Then, the control gains can be computed by ¯Kℓ = ¯YℓW −1 (41) Proof The proof is established by Theorem 1 and omitted. □ REMARK 3: When parameter α[h] ℓ of controller (39) is selected to be special values such as α[h] ℓ = πℓ P j∈Nh πj , if ℓ ∈ Nh; α[h] ℓ = 0, otherwise. Accordingly, controller (39) can be simply rewritten as u(t) = P ℓ∈Nh πℓ P j∈Nh πj ¯Kℓx(t) and similar to [17] which is related to the stationary distribution probability. In this situation, similar results can be obtained and omitted for page limitation. Particularly, if all the values of PN ℓ=1 α[h] ℓ ¯Yℓ are equal such as ¯Y = PN ℓ=1 α[h] ℓ ¯Yℓ, ∀h ∈ N and M = N , a mode-independent controller u(t) = Kx(t) with ¯K = ¯Y W −1 will be obtained and similar to [17], [22]. In other words, Corollary 1 contains some existing results as special situations, which is more general and less conservative. Accordingly, when the mode classiﬁcation is not given in advance and similar to (10), controller (39) is described as u(t) = N X ℓ=1 α[ν(η(t))] ℓ (c) ¯Kℓ(c)x(t), ∀t ∈ [Tn, Tn+1) (42) After giving parameters λi ∈ R and µi > 1, one can compute ¯Kℓ and Pi by solving conditions (21), (24) and (Ai + BiYi)⋆ − λiXi ≤ 0, ∀i ∈ N (43) Then, similar to constraint (25), the constraint of the above conditions is equivalently simpliﬁed to be s.t. ¯Hi (α(c)) ≤ 0, ∀i ∈ Nh(c), c ∈ C (44) where ¯Hi (α(c)) ≜ M X h=1 (\" Pi   Ai + Bi N X ℓ=1 α[h] ℓ (c) ¯Kℓ !#⋆ − λiPi ) A similar augmented Lagrangian function is constructed as ¯ L (α(c), γ(c), ς(c), φ) ≜ J ((α(c))) + T1 (α(c), γ(c), ς(c)) + T2(α(c), ς(c), φ) (45) where T1(α(c), γ(c), ς(c)) ≜ N X i=1 γi(c) ¯Ei(α(c)), ςi(c)) T2(α(c), ς(c), φ) ≜ φ 2 N X i=1 ¯E2 i (α(c), ςi(c)) ¯Ei(α(c), ςi(c)) ≜ λmax ¯ Hi(α(c)) + (ςi(c))2 and parameers γ(c) and ς(c) are deﬁned in (28). Then, one can obtain the following corollary. COROLLARY 2: Consider a convex optimization problem described by (11) and (44), where the parameters ¯Kℓ and Pi are computed by solving conditions (21), (24) and (43) under giving parameters λi ∈ R and µi > 1. The optimization problem (11) constrained on (44) will have a globally optimal solution, if the following conditions are satisﬁed ∇α(c) ¯ L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (46) ∇ς(c) ¯ L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (47) ∇γ(c) ¯ L (α(c), γ(c), ς(c), φ) = 0, ∀c ∈ C (48) Then, the globally optimal value of (11) is obtained by J∗ = min c∈C ¯ L (α∗(c), γ∗(c), ς∗(c), φ) (49) Meanwhile, the globally optimal solution is computed by α∗ = arg min c∈C ¯ L (α∗(c), γ∗(c), ς∗(c), φ) (50) Moreover, based on the developed Q-HCA algorithm, one also gets a locally optimal solution and its locally optimal value such as (37) and (38) without computing c ∈ C one by one. Proof Its proof is similar to Theorem 2 and omitted. □ REMARK 4: It can be seen that some parameters such as ¯Kℓ and Pi in main results need to be given in advance. Particularly, in order to give them easily and directly, they are solved by some LMIs such as (23) and (24). It is worth noted that these convenient techniques will lead to some conservatism in terms of some of constraints such as (25) unsatisﬁed. In this situation, some possible ways may be used to deal with them, and only two possible ways are mentioned here. On the one hand, one can select different values of the scalars given in the presented LMIs time and time again, until the dissatisﬁed constraints hold. On the other hand, other different conditions providing ¯Kℓ and Pi in advance can be developed so as to make the dissatisﬁed constraints satisfying. IV. NUMERICAL EXAMPLE Example 1: Considering a system of form (1) with N = {1, 2, 3, 4, 5} whose matrices are described as A1 = \u0014 −2 2 0.1 0.14 \u0015 , B1 = \u0014 0.32 −1.7 \u0015 ; A2 = \u0014 −3.6 0.2 1 0.2 \u0015 B2 = \u0014 0.32 −2 \u0015 ; A3 = \u0014 −0.7 2 0 0.32 \u0015 , B3 = \u0014 0.25 −0.45 \u0015 ; A4 = \u0014 −3.3 0.1 0.8 0.03 \u0015 , B4 = \u0014 0.22 −0.61 \u0015 ; A5 = \u0014 −2.5 0 0.1 0.95 \u0015 , B5 = \u0014 0.12 −0.53 \u0015 The transition probabilities of the embodied chain is given as P =   0 0.1 0.25 0.25 0.4 0.1 0 0.2 0.4 0.3 0.2 0.35 0 0.3 0.15 0.15 0.3 0.2 0 0.35 0.3 0.15 0.25 0.3 0   It has a unique stationary distribution computed as ¯π = [0.21 0.19 0.2 0.15 0.25]. In addition, the expectations of sojourn time for modes are given to be ˆτ1 = 0.03, ˆτ2 = 0.05, ˆτ3 = 0.04, ˆτ4 = 0.09 and ˆτ5 = 0.07. Firstly, based on conditions (23) and (24), after letting λ1 = −0.8, λ2 = −0.9, λ3 = −0.5, λ4 = −0.6, λ5 = 0.4, µ1 = 1.005, µ2 = 1.001, µ3 = 1.009, µ4 = 1.002, µ5 = 1.001 and σ = 0.069, one can obtain that PN i=1 πi \u0010 λi + ln µi ˆτi \u0011 = −0.2549 < 0, and the control gains are computed such as ¯K1 = \u0002 0.1921 0.1857 \u0003 , ¯K2 = \u0002 0.2087 0.9061 \u0003 , ¯K3 = \u0002 1.0049 4.9363 \u0003 , ¯K4 = \u0002 0.6863 3.1342 \u0003 and ¯K5 = \u0002 0.2835 3.7523 \u0003 , while matrix Pi is computed as P1 = \u0014 0.0047 0.0034 0.0034 0.0236 \u0015 , P2 = \u0014 0.0047 0.0033 0.0033 0.0235 \u0015 , P3 = \u0014 0.0047 0.0034 0.0034 0.0236 \u0015 P4 = \u0014 0.0047 0.0033 0.0033 0.0235 \u0015 , P5 = \u0014 0.0047 0.0033 0.0033 0.0235 \u0015 As we know, based on the commonly mode-dependent methods, see, e.g., [1]–[4], [6], there will be 5 controllers since the mode quantity of this example is 5. More importantly, an ideal assumption that the switching signals between controller and other system matrices should be synchronous is also needed. To the contrary, the method in this paper can provide fewer effective controllers whose modes are not necessary synchronous to system matrices’, which has larger application scope. Without loss of generality, set N is separated into 3 classes such as M = {1, 2, 3}. Consequently, it can be computed that S2(5, 3) = 25 with N = 5 and M = 3, and the classiﬁcation parameter c is the same to Table I. Then, after applying the above designed controllers in addition to some related parameters, the optimal value J (α∗ (c)) with its optimal solution α∗ (c) of each classiﬁcation is given in Table II, where ¯τ = minh∈M {¯τh} is denoted as the ultimate upper sampling bound. It can be seen from this table that there is no optimal solution to some classiﬁcations. The reason is that the given parameters ¯Kℓ and Pi computed by (23) and (24) dissatisfy constraint (25), whose possibly solvable algorithms are presented in Remark 4. Moreover, some comparisons between this paper and some similar ex- isting references can be done in the next. In references [18]– [20], a method based on an auxiliary system approach was found and can be used to discuss stochastic systems having a sampling phenomenon in either state or switching or both of them. Since their methods about sampling are the same, without loss of generality, only comparisons between this paper and [18] are enough. Under the above given parameters and by using the method in [18], the maximum sampling upper bound can be computed as ¯τ = 0.000047. It is far less than ¯τ = 0.0174 which results from the globally optimal solution such as α∗ 1 = 0.4581, α∗ 2 = 0.4668, α∗ 3 = −0.0713, α∗ 4 = 1.6570, α∗ 5 = −0.6935 and J∗ = 3.3820 and also corresponds to the situation of Table II with c = 1. It can be further seen from Table II, all the upper sampling bounds are larger than 0.000047, so long as there exists a suitable solution. Based on these comparisons, it can be concluded that the developed mode classiﬁcation and optimization method is superior to the above auxiliary system approach for providing a larger sampling interval and improving the efﬁciency of communication network. Moreover, under the initial condition x0 = \u0002 2 −2 \u0003⊤ and selecting any effective classiﬁcation such as N1 = {1}, N2 = {2, 3}, N3 = {4, 5}, corresponding to c = 11 in Table II, one can compute its controller such as KN1 = \u0002 0.0880 0.5432 \u0003 , KN2 = \u0002 0.8654 2.2401 \u0003 and KN3 = \u0002 −0.2397 1.5940 \u0003 . Then, the state curves of the closed-loop system are shown in Fig. 4 (a), and the schematic diagram of mode signals is displayed in Fig. 4 (b). Although both state and mode are sampled, the presented controller is still useful in stabilizing a stochastic system. 0 0.2 0.4 0.6 0.8 1 (b) Time(s) 2 4 6 Mode (t) (tk) 0 5 10 15 (a) Time(s) -2 -1 0 1 2 x(t) x1(t) x2(t) Fig. 4. (a) The state responds of the closed-loop system by controller (3). (b) The simulations of signals η(t) and η(tk). Furthermore, based on the developed Q-HCA in Theorem 2, a suboptimal solution or even an optimal solution can be obtained. In detailed, the iteration processes under different initial points are shown in Fig. 5 with ǫ = 10, where TABLE II THE OPTIMIZATION RESULTS OF THEOREM 2 FOR DIFFERENT MODE CLASSIFICATIONS. c α∗ 1(c) α∗ 2(c) α∗ 3(c) α∗ 4(c) α∗ 5(c) J(α∗(c)) ¯τ 1 0.46 0.47 -0.07 1.66 -0.69 3.3820 0.0174 2 – – – – – – – 3 0.46 2.70 0.05 0.24 -0.12 3.5891 0.0146 4 0.46 1.50 -0.18 0.57 0.45 4.5422 0.0142 5 – – – – – – – 6 0 11.83 -1.70 0.24 0.45 4.8947 0.0136 7 0.68 0.47 -0.09 1.23 0.45 6.4733 0.0123 8 – – – – – – – 9 – – – – – – – 10 0 11.83 -1.70 0.24 0.45 4.8947 0.0137 11 0.46 16.92 -2.65 -0.80 1.09 4.5637 0.0146 12 0.46 -4.97 1.08 1.68 -0.82 3.7408 0.0172 13 0.46 -0.51 0.10 0.75 0.57 5.1091 0.0153 14 -1.88 0.47 0.99 -0.80 1.09 9.2897 0.0123 15 0 0.47 0.67 0.24 -0.18 3.9675 0.0174 16 0 0.47 -0.08 1.01 0.45 4.9755 0.0161 17 – – – – – – – 18 – – – – – – – 19 – – – – – – – 20 0.40 0.08 0.67 0.24 -0.18 4.0838 0.0173 21 -1.88 -4.01 0.99 0.24 1.39 9.4961 0.0123 22 0 16.92 -2.65 0.24 0.45 4.8516 0.0146 23 0 0.59 -0.10 1.06 0.45 5.0880 0.0174 24 -1.88 -1.43 0.99 0.65 0.45 9.6922 0.0091 25 0 16.92 -2.65 0.26 0.45 4.8516 0.0146 the points in the horizontal axis denote as infeasible points. Particularly, the infeasible points in blue means that they have been selected to participate in calculation, while the black ones are only infeasible points but without being selected during the iteration. Moreover, from this simulation, it can be seen that when the initial points are Points 24 and 7 respectively, the ﬁnal point is Point 12 corresponding to c = 12 in Table II, whose cost function is computed as f ∗ = 3.7408. Similarly, one can further know that when the initial point is Point 14, the ﬁnal point is Point 3 with f ∗ = 3.5891. And when the initial point is Point 7, the ﬁnal point is Point 1. Especially, in this situation, its cost function is computed as f ∗ = 3.3820 and equals to the globally optimal value J∗ = 3.3820. Obviously, by comparing Table II and Fig. 5, one can obtain that the computation complexity and time of Fig. 5 by the Q-HCA method is both largely reduced and without computing mode classiﬁcation one by one. Moreover, it also can be seen from Fig. 5 that the convergence of the Q-HCA method is also ensured, which is asymptotically convergent. In a word, all the statements about the effectiveness and superiority of the developed method in this paper have been shown by the simulations and comparisons. 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Initial : 24 Initial : 14 Initial : 21 Initial : 7 Fig. 5. The effects of Q-HCA in Theorem 2 under different initial points. Next, some simulations and comparison about Corollary 2 will be done, where set N is also separated into 3 classes such as M = {1, 2, 3}. First, letting λ1 = −0.8, λ2 = −0.9, λ3 = −0.5, λ4 = −0.6, λ5 = 0.4, µ1 = 1.005, µ2 = 1.001, µ3 = 1.009, µ4 = 1.002 and µ5 = 1.001, it can be known from Corollary 2 that PN i=1 πi \u0010 λi + ln µi ˆτi \u0011 = −0.2549 < 0. Meanwhile, the control gains computed by conditions (24) and (43) are listed as follows: ¯K1 = \u0002 1.9651 1.6991 \u0003 , ¯K2 = \u0002 1.8758 1.1729 \u0003 , ¯K3 = \u0002 4.6357 4.8861 \u0003 , ¯K4 = \u0002 6.0120 4.4209 \u0003 and ¯K5 = \u0002 4.7489 5.3510 \u0003 . Now, more comparisons will be done. In reference [17], a quantity-limited controller was proposed based on mode sep- aration. However, the optimization problem about such mode separation was not considered in the reference. Moreover, it has been shown there that the method in [17] is less conser- vative than mode-independent ones [7]–[10]. In this situation, only comparisons between Corollary 2 and [17] will be done. Based on [17], it is assumed that modes {1} and {2} are known, and the rest modes are unknown. In this situation, the above assumption is the same as the following mode classiﬁca- tion such as N1 = {1}, N2 = {2} and N3 = {3, 4, 5}. Then, by using the method in [17], one can obtain the control gains as K1 = \u0002 1.7897 2.0866 \u0003 , K2 = \u0002 2.1638 4.7343 \u0003 and K = \u0002 −0.1417 −9.2833 \u0003 , and the same cost function (11) about the above designed controllers of [17] is equal to J = ∥K1∥ + ∥K2∥ + ∥K∥ = 17.2387. Investigating all the optimal function value J∗ computed similar to Table II and omitted due to page limiation, it can be seen that the global cost function value after optimization is much smaller and less conservative. In order to further illustrate the superiority, more comparisons are done such that a positive perturbation ∆ is only added in block (1,1) of matrix A1 such as −2 + ∆, and the others remain unchanged. Then, the respective maximum allowable bound about Corollary 2 and [17] under the same λ1 is given in Table III, which is denoted as ∆max. It is obvious that Corollary 2 by optimizing the mode classiﬁcations is less conservative than [17]. Similar to Fig. 5, some simulations about Q-HCA of Corollary 2 under different initial points are given in Fig. 6 with ǫ = 10. It can be found that when the initial points are Points 21, 22 and 23 respectively, the ﬁnal point is Point 2 corresponding to c = 2 similar to Table II, whose cost function is computed as f ∗ = 6.7184 and equals to the globally optimal value J∗ = 6.7184. Also, when the initial point is Point 10, the ﬁnal point is Point 1 with f ∗ = 7.1112 which is a locally optimal value. As a result, similar conclusions about the given Q-HCA in this paper can be obtained and omitted here. 0 1 2 3 4 5 6 7 8 9 10 6.5 7 7.5 8 8.5 9 9.5 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 6.5 7 7.5 8 8.5 9 9.5 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 6.5 7 7.5 8 8.5 9 9.5 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 0 1 2 3 4 5 6 7 8 9 10 6.5 7 7.5 8 8.5 9 9.5 Selection Process 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 Initial : 21 Initial : 22 Initial : 10 Initial : 23 Fig. 6. The effects of Q-HCA in Corollary 2 under different initial points. Finally, more comparisons will be done to further demon- TABLE III THE MAXIMUM ALLOWABLE BOUND ∆max UNDER λ1 TAKING DIFFERENT VALUES. λ1 -0.1 -0.2 -0.3 -0.4 -0.5 Corollary 2 5.595 5.562 5.529 5.496 5.463 [17] 4.341 4.302 4.263 4.224 4.185 -0.6 -0.7 -0.8 -0.9 -1 5.430 5.397 5.364 5.331 5.298 4.146 4.108 4.069 4.030 3.991 strate the advantages of the method proposed here. As we know, a sampled system can be discussed by transforming the sampled term into a time-varying delay term. Similarly, when both state and switching are sampled, a input delay approach was used in [33]. Unfortunately, its method has some limits such that it cannot be used to stabilize an MJS having non-negative eigenvalues. Meanwhile, an augmented system approach was presented in [21] and can be used to deal with MJSs with sampled state and switching simultaneously. Due to it considering about MJSs instead of semi-Markovian jump systems (SMJSs) and in order to make some comparisons, it is assumed that the related system in this example is reduced to be an MJS. In detail, all the system parameters are the same as the above SMJS, except the transition rate matrix of MJSs is given as Λ =   −3 1 0.5 0.2 1.3 1 −3 0.5 1.3 0.2 1 0.5 −2 0.25 0.25 1.5 0.2 0.9 −4 1.4 0.4 0.3 0.2 0.1 −1   Then, based on [21] with given control gains such as K1 = \u0002 0.1710 1.1218 \u0003 , K2 = \u0002 0.1853 0.8848 \u0003 , K3 = \u0002 0.9524 4.8487 \u0003 , K4 = \u0002 0.6209 2.8994 \u0003 and K5 = \u0002 0.2302 3.7272 \u0003 , one can get the upper sampling bound is ¯τ = 0.0028. On the other hand, it can be known that similar results about MJSs can be easily obtained from the results about SMJSs. Thus, under the same control gains given above, and selecting α1 = 0.335, α2 = 29.5068, α3 = −4.8934, α4 = −0.6157 and α5 = 0.9425 with a speciﬁc mode classiﬁcation such as N1 = {1}, N2 = {2, 3} and N3 = {4, 5}, the upper sampling bound based on Theorem 1 without any optimization is equal to ¯τ = 0.01324. It showns that the method in this paper can provide a larger sampling bound and is less conservative than [21]. V. CONCLUSIONS In this study, the sampled stabilization for stochastic jump systems has been investigated by a mode classiﬁcation opti- mization method. A new mode-classiﬁcation-based controller has been proposed to handle the general sampling situation that both controller’s state and switching are sampled. So as to ﬁnd the best classiﬁcation for stabilization, an optimization problem has been proposed to provide a globally optimal solution among such classiﬁcations. An Q-HCA algorithm has been established to provide an optimal attenuation coefﬁcient, whose monotonicity and convergency have been both satisﬁed. Finally, the utility and advantage of this paper have been obviously illustrated and shown by a numerical example. REFERENCES [1] H. Zhang, J. Wang, Z. Wang and H. Liang, “Mode-dependent stochastic synchronization for Markovian coupled neural networks with time- varying mode-delays,” IEEE Transactions on Neural Networks and Learning Systems, vol. 26, no. 11, pp. 2621-2634, 2015. [2] G. Zhuang, Q. Ma, B. Zhang, S. Xu and J. Xia, “Admissibility and stabilization of stochastic singular Markovian jump systems with time delays,” Systems and Control Letters, vol. 114, pp.1-10, 2018. [3] B. Cai, L. Zhang and Y. Shi, “Observed-mode-dependent state estimation of hidden semi-Markov jump linear systems,” IEEE Transactions on Automatic Control, vol. 65, no. 1, pp. 442-449, 2019. [4] K. Ding and Q. Zhu, “Extended dissipative anti-disturbance control for delayed switched singular semi-Markovian jump systems with multi- disturbance via disturbance observer,” Automatica, vol. 128, pp. 109556, 2021. [5] W. Qi, Y. Zhou, L. Zhang, J. Cao and J. Cheng, “Non-fragile H∞ SMC for Markovian jump systems in a ﬁnite-time,” Journal of the Franklin Institute, vol. 358, no. 9, pp. 4721-4740, 2021. [6] R. Zhang, H. Wang, J. H. Park, K. Shi and P. He, “Mode- dependent adaptive event-triggered control for stabilization of Marko- vian memristor-based reaction-diffusion neural networks,” IEEE Trans- actions on Neural Networks and Learning Systems, vol. 34, no. 8, pp. 3939-3951, 2023. [7] C. E. de. Souza, A. Troﬁno and K. A. Barbosa, “Mode-independent H∞ ﬁlters for Markovian jump linear systems,” IEEE Transactions on Automatic Control, vol. 51, no. 11, pp. 1837-1841, 2006. [8] H. N. Wu, and K. Y. Cai, “Mode-independent robust stabilization for uncertain Markovian jump nonlinear systems via fuzzy control,” IEEE Transactions on Systems Man and Cybernetics Part B-Cybernetics, vol. 36, no. 3, pp. 509-519, 2005. [9] P. H. Liu, D. W. C. Ho and F. C. Sun, “Design of H∞ ﬁlter for Markov jumping linear systems with non-accessible mode information,” Automatica, vol. 44, no, 10, pp. 2655-2660, 2008. [10] X. Li, W. Zhang and D. Lu, “Robust asynchronous output-feedback controller design for Markovian jump systems with output quantization,” IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 52, no. 2, pp. 1214-1223, 2022. [11] G. L. Wang, B. Y. Li, Q. L. Zhang and C. Y. Yang, “A partially delay- dependent and disordered controller design for discrete-time delayed systems,” Interational Journal of Robust Nonlinear Control, vol. 27, no. 16, pp. 2646-2668, 2017. [12] O. L. V. Costa, M. D. Fragoso and M. G. Todorov, “A detector-based approach for the H2 control of Markov jump linear systems with partial information,” IEEE Transactions on Automatic Control, vol. 60, no. 5, pp. 1219-1234, 2014. [13] G. L. Wang, Q. L. Zhang and C. Y. Yang, “Fault-tolerant control of Markovian jump systems via a partially mode-available but unmatched controller,” Journal of the Franklin Institute, vol. 354, no. 17, pp. 7717- 7731, 2017. [14] G. L. Wang and L. Xu, “Almost sure stability and stabilization of Markovian jump systems with stochastic switching,” IEEE Transactions on Automatic Control, vol. 67, no. 3, pp. 1529-1536, 2022. [15] G. L. Wang and Y. Y. Sun, “Almost sure stabilization of continuous- time jump linear systems via a stochastic scheduled controller,” IEEE Transactions on Cybernetics, vol. 52, no. 5, pp. 2712-2724, 2022. [16] G. L. Wang, Y. S. Ren and Z. Q. Li, “Almost Sure Stabilization of Continuous-Time Semi-Markov Jump Systems via an Earliest Deadline First Scheduling Controller,” IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol. 54, no. 1, pp. 656-667, 2024. [17] G. L. Wang, “Stabilization of semi-Markovian jump systems via a quantity limited controller,” Nonlinear Analysis: Hybrid Systems, vol. 42, Artical: 101085, 2021. [18] X. Mao, “Stabilization of continuous-time hybrid stochastic differential equations by discrete-time feedback control,” Automatica, vol. 49, no. 12, pp. 3677-3681, 2013. [19] X. Mao, “Almost sure exponential stabilization by discrete-time stochas- tic feedback control,” IEEE Transactions on Automatic Control, vol. 61, pp. 1619-1624, 2015. [20] G. F. Song, Z. Y. Lu, B. C. Zheng and et al, “Almost sure stabilization of hybrid systems by feedback control based on discrete-time observations of mode and state,” Science China Information Sciences, vol. 61, pp. 70213, 2018. [21] G. L. Wang, Y. S. Ren and C. Huang, “Stabilizing control of Markovian jump systems with sampled switching and state signals and applica- tions,” Interational Journal of Robust Nonlinear Control, vol. 33, no. 10, pp. 5198-5228, 2023. [22] G. L. Wang, “Mode-independent control of singular Markovian jump systems: A stochastic optimization viewpoint,” Applied Mathematics and Computation, vol. 286, pp. 155-170, 2016. [23] B. C. Rennie and A. J. Dobson, “On stirling numbers of the second kind,” Journal of Combinatorial Theory, vol. 7, no. 2, pp. 116-121, 1969. [24] K. N. Boyadzhiev, “Close encounters with the Stirling numbers of the second kind,” Mathematics Magazine, vol. 85, no. 4, pp. 252-266, 2012. [25] S. H. Jacobson and E. Y¨ucesan, “Analyzing the performance of gen- eralized hill climbing algorithms,” Journal of Heuristics, vol. 10, pp. 387-405, 2004. [26] X. T. Wu, Y. Tang, J. D. Cao and X. R. Mao, “Stability analysis for continuous-time switched systems with stochastic switching signals,” IEEE Transactions on Automatic Control, vol. 63, no. 9, pp. 3083-3090, 2017. [27] S. P. Boyd and L. Vandenberghe, Convex optimization. Cambridge university press, 2004. [28] F. S. Hillier, Introduction to operations research. McGrawHill, 2001. [29] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction. MIT press, 2018. [30] Y. J. Wang and Z. A. Liang, Optimization of the basic theory and methods. Fudan University Press, 2011. [31] D. P. Bertsekas, Constrained optimization and Lagrange multiplier methods. Academic press, 2014. [32] C. J. C. H. Watkins and P. Dayan, “Q-learning,” Machine learning, vol. 8, pp. 279-292, 1992. [33] G. L. Wang and Y. S. Ren, “Stability analysis of delayed Markovian jump systems with delay switching and state signals and applications,” Interational Journal of Robust Nonlinear Control, vol. 32, no. 9, pp. 5141-5163, 2022. VI. APPENDIX A. Proof of Theorem 1 First of all, Nη(t0, t), ∀t ≥ 0, is deﬁned to be the switching quantity of η(t) on interval [t0, t]. For any Nη(t0, t) = n, ∀t ≥ 0 and n ∈ N, one concludes that [t0, t] = Sn−1 i=0 [Ti, Ti+1) S [Tn, t] and t ∈ [Tn, Tn+1). Consequently, one gets [t0, t] = Sk−1 j=0 [tj, tj+1) S [tk, t] and t ∈ [tk, tk+1), k ∈ N, from event-triggered mechanism (4) under As- sumption 1. Moreover, one can further obtain k ≤ n such that every subinterval of Sn−1 i=0 [Ti, Ti+1) S [Tn, t] such as [Ti, Ti+1) or [Tn, t] certainly belongs to one subinterval of Sk−1 j=0 [tj, tj+1) S [tk, t] such as [tj, tj+1) or [tk, t] uniquely. Alternatively, {tk}k∈N established based on principle (4) is a subsequence of {Tn}n∈N. As a result, and without loss of generality, it is assumed that [tj, tj+1) = Nη(tj,tj+1)−1 [ q=0 \u0002 TNη(t0,tj)+q, TNη(t0,tj)+q+1 \u0001 (51) where tj = TNη(t0,tj) and tj+1 = TNη(t0,tj+1). Particularly, [tk, t] is denoted as [tk, t] = Nη(tk,t)−1 [ q=0 \u0002 TNη(t0,tk)+q, TNη(t0,tk)+q+1 \u0001 [ \u0002 TNη(t0,t), t \u0003 (52) In addition, for ∀t ∈ [tk, tk+1) , ∀k ∈ N, one can conclude that η(t) ∈ Nη(tk), if η(tk) ∈ M . Then, after applying (3), the system (1) closed by an event-triggered controller described by (3)-(8) is equal to      ˙x(t) = Aη(t)x(t) + Bη(t) N X ℓ=1 α[ν(η(tk))] ℓ ¯Kℓx(tk) x(tk) = x(t− k ) (53) Accordingly, choose the Lyapunov function as V (x(t), η(t)) = xT (t)Pη(t)x(t) and based on (51)-(53) and condition (20), for any t ∈ \u0002 TNη(t0,t), TNη(t0,t)+1 \u0001 , one gets that V \u0000x(TNη(t0,tk)+1), ηNη(t0,tk) \u0001 × e −ληNη(t0,tk) \u0010 TNη(t0,tk)+1−TNη(t0,tk) \u0011 = V \u0000x \u0000TNη(t0,tk) \u0001 , ηNη(t0,tk) \u0001 + Z TNη(t0,tk)+1 TNη(t0,tk) d \u0000V \u0000x(s), ηNη(t0,tk) \u0001 ×e −ληNη(t0,tk) \u0010 s−TNη(t0,tk) \u0011\u0013 ≤ V \u0000x \u0000TNη(t0,tk) \u0001 , ηNη(t0,tk) \u0001 ≤ µηNη(t0,tk)−1V \u0000x \u0000TNη(t0,tk) \u0001 , ηNη(t0,tk)−1 \u0001 (54) ... V \u0000x(TNη(t0,t)), ηNη(t0,t)−1 \u0001 × e −ληNη(t0,t)−1(TNη(t0,t)−TNη(t0,t)−1) = V \u0000x \u0000TNη(t0,t)−1 \u0001 , ηNη(t0,t)−1 \u0001 + Z TNη(t0,t) TNη(t0,t)−1 d \u0000V \u0000x(s), ηNη(t0,t)−1 \u0001 ×e −ληNη(t0,t)−1(s−TNη(t0,t)−1) \u0013 ≤ V \u0000x \u0000TNη(t0,t)−1 \u0001 , ηNη(t0,t)−1 \u0001 ≤ µηNη(t0,t)−2V \u0000x \u0000TNη(t0,t)−1 \u0001 , ηNη(t0,t)−2 \u0001 (55) V \u0000x(t), ηNη(t0,t) \u0001 e −ληNη(t0,t)(t−TNη(t0,t)) = V \u0000x \u0000TNη(t0,t) \u0001 , ηNη(t0,t) \u0001 + Z t TNη(t0,t) d \u0012 V \u0000x(s), ηNη(t0,t) \u0001 e −ληNη(t0,t)(s−TNη(t0,t)) \u0013 ≤ V \u0000x \u0000TNη(t0,t) \u0001 , ηNη(t0,t) \u0001 ≤ µηNη(t0,t)−1V \u0000x \u0000TNη(t0,t) \u0001 , ηNη(t0,t)−1 \u0001 (56) Particularly, the above inequalities holding also need the following condition \" x⊤(t)Pη(t)   Aη(t)x(t) + Bη(t) N X ℓ=1 α[h] ℓ ¯Kℓx(tk) !#⋆ ≤ λη(t)x⊤(t)Pη(t)x(t) (57) where ∀t ∈ [tk, tk+1), η(tk) = h ∈ M and η(t) ∈ Nh. On the other hand, let δ(t) = x(t) − x(tk), ∀t ∈ [tk, tk+1), it can be concluded from (53) that ( ˙δ(t) = Aη(t)δ(t) + ¯A[h] η(t)x(tk) δ(tk) = 0 , ∀t ∈ [tk, tk+1) , ∀k ∈ N (58) Its solution is computed as δ(t) = e R t tk Aη(s)dsδ(tk) + Z t tk e R t θ Aη(s)ds ¯A[h] η(θ)x(tk)dθ (59) which implies ∥δ(t)∥ ≤ Z t tk ∥e R t θ Aη(s)ds∥∥ ¯A[h] η(θ)∥∥x(tk)∥dθ ≤ max ℓ∈Nh n ∥ ¯A[h] ℓ ∥ o ∥x(tk)∥ Z t tk emaxℓ∈Nh{χAℓ}(t−θ)dθ = ∥ ˆAh∥ Z t tk eˆχh(t−θ)dθ∥x(tk)∥ (60) It can be known that function ζh(t − tk) ≜ R t tk eˆχh(t−θ)dθ, ∀t ∈ [tk, tk+1) , ∀k ∈ N, is monotonically increasing with t and ζ(0) = 0. Based on Assumption 1, one has ζh(t − tk) ≤ ζh(sk+1) ≤ ζh(¯τh) (61) Then, one obtains that ∥δ(t)∥ ≤ ∥ ˆAh∥ζh(¯τh) (∥δ(t)∥ + ∥x(t)∥) (62) When function ζh(¯τh) satisﬁes ζh(¯τh) ≤ 1 ∥ ˆAh∥ σh 1 + σh (63) where σh > 0 is a suitable design parameter, inequality (62) becomes to be ∥δ(t)∥ ≤ σh∥x(t)∥, ∀t ∈ [tk, tk+1) , ∀k ∈ N, η(tk) = h ∈ M (64) Under condition x(tk) = x(t) − δ(t), inequality (57) is equivalent to xT (t) h\u0010 Pη(t) ¯A[h] η(t) \u0011⋆ − λη(t)Pη(t) i x(t) − 2x⊤(t)Pη(t)Bη(t) N X ℓ=1 α[ν(η(tk))] ℓ ¯Kℓδ(t) ≤ 0 (65) Based on condition (19) and (64), inequality (65) is guaran- teed. Thus, conditions (54)-(56) are satisﬁed. They ﬁnally yield that for ∀t ≥ 0, V (x(t), η(t)) ≤ µηNη(t0,t)−1V \u0000x \u0000TNη(t0,t) \u0001 , ηNη(t0,t)−1 \u0001 × e ληNη(t0,t)(t−TNη(t0,t)) ≤ µηNη(t0,t)−2µηNη(t0,t)−1 × V \u0000x \u0000TNη(t0,t)−1 \u0001 , ηNη(t0,t)−2 \u0001 × e ληNη(t0,t)(t−TNη(t0,t)) × e ληNη(t0,t)−1(TNη(t0,t)−TNη(t0,t)−1) ... ≤ V (x0, η0) Nη(t0,t) Y k=1 µηke R t 0 λη(s)ds = V (x0, η0) N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds (66) Then, one could obtain that ∥x(t)∥ ≤ maxi∈N {λmax (Pi)} mini∈N {λmin (Pi)} ∥x0∥ N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds (67) It is obvious from (21) that there is always a scalar ǫ1 > 0 such that N X i=1 πiλi + N X i=1 \u0012πi ˆτi + ǫ1 \u0013 ln µi < 0 (68) Then, similar to the proof of Theorem 1 in [26], one knows that if ∀i ∈ N and ǫ1 deﬁned in (68), there exists a positive constant H (ǫ1) such that if t ≥ H (ǫ1), then Ni(t0, t) ≤ \u0012πi ˆτi + ǫ1 \u0013 t, a.s. (69) Accordingly, for t ≥ H (ǫ1), one has N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds = e PN i=1 Ni(t0,t) ln µi+R t 0 λη(s)ds ≤ e PN i=1 \u0010 πi ˆτi +ǫ1 \u0011 (ln µi)t+ R t 0 λη(s)ds, a.s. (70) Based on the strong law of large numbers, one obtains that lim t→∞ 1 t Z t 0 λη(s)ds = N X i=1 λiπi, a.s. (71) Then, it can be concluded from (21) that lim t→∞ N X i=1 \u0012πi ˆτi + ǫ1 \u0013 (ln µi) t + Z t 0 λη(s)ds = −∞, a.s. (72) Consequently, one gets that N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds = 0, a.s. (73) Based on the above equation, it can be known that for any given scalars γ > 0 and ξ > 0, there exits a random variable ϕ (γ, ξ) such that P   sup t≥ϕ(γ,ξ)  N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds ! < mini∈N {λmin (Pi)} maxi∈N {λmax (Pi)} ξ γ \u0013 = 1 (74) Then, for any ∥x0∥ < γ and combing conditions (67) and (74), one can conclude condition C2. Moreover, it can be concluded from (74) that sup t≥0  N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds ! < ∞, a.s. (75) Obviously, for any ǫ ∈ (0, 1), there is always ρ > 0 such that P   sup t≥0  N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds ! < ρ ! > 1 − ǫ. (76) By selecting δ = mini∈N {λmin(Pi)} maxi∈N {λmax(Pi)} ǫ ρ and letting ∥x0∥ < δ, and considering inequality (67) under condition (76), one has P \u0012 sup t≥0 ∥x(t)∥ ≤ maxi∈N {λmax (Pi)} mini∈N {λmin (Pi)} ∥x0∥ × sup t≥0  N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds ! < ǫ ρ sup t≥0  N Y i=1 µNi(t0,t) i e R t 0 λη(s)ds ! < ǫ ! > 1 − ǫ (77) which implies condition C1. Due to conditions C1 and C2 both satisfying, the resulting closed-loop system is ﬁnally GAS a.s.. As for condition (63) with η(tk) = h ∈ M , ∀t ∈ [tk, tk+1), it is implied by Z t tk eˆχh(t−θ)dθ ≤ 1 ∥ ˆAh∥ σh 1 + σh (78) When ˆχh ≤ 0, it could be guaranteed by sk ≤ ¯τh ≤ 1 ∥ ˆAh∥ σh 1 + σh (79) which is actually the former condition of (22). To the contrary, condition (63) under ˆχh > 0 is ensured by 1 ˆχh eˆχh(t−s)|tk t ≤ 1 ˆχh \u0000eˆχh¯τh − 1 \u0001 ≤ 1 ∥ ˆAh∥ σh 1 + σh (80) It is equivalent to the latter condition of (22). This completes the proof. □ B. Proof of Theorem 2 On the one hand, ﬁrst of all, it is obvious that the augmented Lagrangian function (28) is convex. Then, based on the result given in [30] on Page 108, it can be concluded that there exists a globally optimal solution (α∗(c), γ∗(c), ς∗(c)) to (28) which can be computed by solving conditions (31)-(33) and deﬁned as L (α∗(c), γ∗(c), ς∗(c), φ) ≜ minα(c),γ(c),ς(c) L (α(c), γ(c), ς(c), φ). Particularly, it can be seen that (28) is strictly convex about α(c) such as ∇2 α(c)L (α(c), γ(c), ς(c), φ) > 0. In other words, there is only one globally optimal solution α∗(c) satisfying (31)-(33). Meanwhile, according to reference [31] on Page 97, it can be known that the optimization problem (11) constrained by (27) has the same optimal value as the optimization problem (28). That is J (α∗(c)) = L (α∗(c), γ∗(c), ς∗(c), φ) (81) Finally, one can easily obtain the globally optimal value of (11) with constraint (27) described by (34), whose corresponding optimal solution is obviously computed by (35). On the other hand, it has been shown in [32] that Qk(s, a) generated by (36) can theoretically converge to Q∗(s, a) with probability 1 when k → ∞. As a result, an optimal taking action A∗ k, k = 1, 2, . . ., maximizing Q-value (30) can be es- tablished, which is actually an optimal attenuation coefﬁcient δ∗ k. Then, a preferable searching radius Lpre k , k = 1, 2, . . ., can be gotten based on the iterative update rule (18), while the initial condition ˆω(1) is given to be ∀ˆω(1) ∈ Ω but ˆω(0) should be given such as one deﬁned in (17). Based on the obtained Lpre k , a preferably nominal solution referred as ˆωpre(k), k = 2, 3, . . ., can be gotten by using HCA. Finally, by repeating the above process along with iteration number k having a maximum value kmax, two optimal sequences {A∗ k}kmax k=1 and {δ∗ k}kmax k=1 can be established based on (36), while two preferable sequence {L∗ k}kmax k=1 and {ˆωpre(k)}kmax k=1 with ˆωpre(1) = ˆω(1) can also be developed by (18). The detailed repeating process is given as follows: ˆω(1) → δ∗ 1 ˆω(1) −−−→ ˆω(0) L1 → ˆω(2) → · · · → ˆω(kmax) → δ∗ kmax ˆω(kmax) −−−−−−−→ ˆω(kmax−1) Lkmax Consequently, based on the characteristics of Q-HCA, it can be obviously known that sequence {ˆωpre(k)}kmax k=1 will have a good convergence characteristic such as (37). Meanwhile, the monotonicity of (38) can be guaranteed. Moreover, since all the elements of {ˆωpre(k)}kmax k=1 belong to Ω but is only a part of Ω, it is obviously concluded that the obtained optimal value f ∗ will be no less than J∗ obtained by (34). In this situation, it can be said that the above obtained ˆω∗ is only a locally optimal solution. Finally, the iteration process of the developed Q-HCA is given below: Algorithm 1: Improved hill climbing algorithm based on Q-learning Initial Q-values with Q-HCA, parameter ǫ, iterative index k = 1, Select a starting state S1 = ˆω(1), calculate L1 by the parameter ǫ while (k ≤ maximum number of iterations) while Sk is not a termination state Send Choose Ak according to the max Q(Sk) Take action Ak Return reward Rk+1 by the equation (29) Update Q-value by the equation (36) Update Lk by the equation (19) Update Θ(ˆω(k)) of ˆω(k) corresponding to Sk by the equation (14) if Θ(ˆω(k)) is not empty set Generate a new state by gˆω(k)(ˆω′(k)) of ˆω(k) corresponding to Sk Move to new state else if Θ(ˆω(k)) is empty set Move to the termination state Send end if end while end while This completes the proof. □ "
}