{
    "optim": "Towards Fairness-Aware Adversarial Learning\nYanghao Zhang\nTianle Zhang\nRonghui Mu\nXiaowei Huang\nWenjie Ruan*\nUniversity of Liverpool, UK\n{yanghao.zhang, tianle.zhang, ronghui.mu, xiaowei.huang}@liverpool.ac.uk, w.ruan@trustai.uk\nAbstract\nAlthough adversarial training (AT) has proven effective\nin enhancing the model’s robustness, the recently revealed\nissue of fairness in robustness has not been well addressed,\ni.e. the robust accuracy varies significantly among different\ncategories. In this paper, instead of uniformly evaluating\nthe model’s average class performance, we delve into the\nissue of robust fairness, by considering the worst-case dis-\ntribution across various classes. We propose a novel learn-\ning paradigm, named Fairness-Aware Adversarial Learn-\ning (FAAL). As a generalization of conventional AT, we re-\ndefine the problem of adversarial training as a min-max-\nmax framework, to ensure both robustness and fairness of\nthe trained model. Specifically, by taking advantage of dis-\ntributional robust optimization, our method aims to find the\nworst distribution among different categories, and the solu-\ntion is guaranteed to obtain the upper bound performance\nwith high probability. In particular, FAAL can fine-tune an\nunfair robust model to be fair within only two epochs, with-\nout compromising the overall clean and robust accuracies.\nExtensive experiments on various image datasets validate\nthe superior performance and efficiency of the proposed\nFAAL compared to other state-of-the-art methods.\n1. Introduction\nDeep learning models have undoubtedly achieved remark-\nable success across various domains, such as computer vi-\nsion [30] and natural language processing [40]. However,\nthey still remain susceptible to deliberate adversarial manip-\nulations of input data [14, 18, 19, 45]. Robust learning tech-\nniques [21, 22, 28, 36] have emerged as a potential solution,\naiming to enhance a model’s resilience against such vulner-\nabilities. These techniques have demonstrated a promising\nability to enhance a model’s overall robustness, yet the in-\ntricate connection between robustness and fairness, as re-\nvealed by [42], demonstrates that the robust accuracy of the\nmodels can vary considerably across different categories or\n*corresponding author\nFigure 1. Class-wise accuracy of the Wide-ResNet34-10 model\non CIFAR-10 dataset, where AA accuracy represents the robust\naccuracy against AutoAttack.\nclasses. Consider a scenario where an autonomous driv-\ning system attains commendable average robust accuracy in\nrecognizing road objects. Despite this success, the system\nmight demonstrate robustness against categories like inani-\nmate objects (with high accuracy) while displaying vulner-\nability to crucial categories such as “human” (with low ac-\ncuracy). This disparity or unfair robustness could poten-\ntially endanger drivers and pedestrians. Hence, it is vital to\nensure consistent, equitable model performance against ad-\nversarial attacks by assessing worst-case robustness beyond\naverage levels. This provides a more accurate evaluation\nthan the average performance, recognizing the model’s lim-\nitations while ensuring reliability across diverse categories\nin real-world applications.\nFigure 1 provides an example, displaying the class-\nwise clean accuracy and robust accuracy (against AutoAt-\ntack [11]) using the Wide-ResNet34-10 [43] models on\nCIFAR-10 dataset, where both models are trained via stan-\ndard training and adversarial training, respectively. It comes\nas no surprise that the model with standard training is vul-\nnerable to adversarial attacks, yet it manages to attain com-\nparable performance across various classes. In contrast, the\nadversarially trained model exhibits a noticeable bias, con-\narXiv:2402.17729v1  [cs.CV]  27 Feb 2024\nfidently classifying “automobile” but hesitating with “cat”.\nwhere robust accuracy diverges significantly across classes.\nFurthermore, it is noteworthy that even though the class\n“cat” attains the lowest clean and robust accuracy, the\nmost significant disparity between clean and robust accu-\nracy arises in the case of the class “deer”. This finding\nhighlights an inconsistency between the clean and adver-\nsarial performances, particularly in terms of fairness where\nrobust accuracy diverges significantly across classes. It is\nclear that the disparity between clean and robust accuracy is\nmore pronounced in terms of fairness issues, where the ro-\nbust accuracy on different classes illustrates severe diverges\nin the model.\nThis phenomenon is called the “robust fairness” issue,\nwhich is first revealed in [42] referring to the gap between\naverage robustness and worst-class robustness. Recently,\nsome pioneering solutions have been proposed to address\nthe robust fairness issue [25, 34, 39]. These efforts either\ntackle the robust fairness from the adversarial example gen-\neration or adjust the class weights empirically. However,\nessentially, most of these methods can be regarded as in-\nstances of reweighting, albeit diverse heuristic strategies are\nadopted. It is worth noting that such issues are absent in\nmodels trained without adversarial training, thus it is ob-\nvious that the underlying issue stems from the adversar-\nial perturbations during the adversarial training. Inspired\nby but beyond these studies, we are prompted to consider\nwhy reweighting strategies are effective in mitigating the\nrobust fairness issue. Intuitively, a model trained adversar-\nially without reweighting fails to achieve high worst-class\nrobust accuracy because it treats all classes equally, yet ne-\nglects the fact that the replaced adversarial examples may\nintroduce bias to the final model. Conversely, reweighting\ncan be perceived as a means of inducing a form of group\ndistribution shift. This shift disrupts the uniform optimiza-\ntion of different classes, compelling the model to acquire\nresistance against these distribution shifts. This, in turn,\nleads to an enhancement in robust fairness. In line with\nthis, we adopt an alternative approach to address this issue\nin the adversarial learning paradigm, where the following\nassumption is made:\nThe robust fairness issue in the conventional AT\nis due to the unknown group (class) distribution\nshift induced by adversarial perturbations, which\nresults in the overfitting problem.\nAs opposed to the heuristic assignment for the reweight-\ning item in the prior works, we believe that leveraging some\noptimization techniques for solving this overfitting prob-\nlem can bring better results. Hence Distributional Robust\nOptimization (DRO) [3, 12] naturally emerges as a viable\nchoice. Rather than assuming a fixed uniform data distribu-\ntion, DRO acknowledges the inherent distributional uncer-\ntainty in real-world data, offering a more resilient and adapt-\nable model structure. Therefore, this paper delves into the\nexploration and adaption of DRO, as a sensible solution for\nthe robust fairness challenge. Specifically, after finding the\nadversarial example in adversarial training, instead of man-\nually or empirically adjusting the weights for each class, we\nresort to learning the class-wise distributionally adversarial\nweights with the pre-defined constraints via DRO. By learn-\ning with these weights, the model will be guided to acquire\nthe capacity to resist unknown group distribution shifts. The\ncontributions of this paper are summarized as follows:\n• We investigate the robust fairness issue from the per-\nspective of group/class distributional shift, by taking the\nrecent advances of Distributional Robust Optimization\n(DRO), which ultimately falls into a reweighting prob-\nlem. To the best of our knowledge, this work is the first at-\ntempt to address the challenge of robust fairness through\ndistributional robust optimization.\n• We introduce a novel learning paradigm, named FAAL\n(Fairness-Aware Adversarial Learning). This innovative\napproach extends the conventional min-max adversarial\ntraining framework into a min-max-max formulation. The\nintermediate maximization is dedicated to dealing with\nthe robust fairness issue, by learning with the class-wise\ndistributionally adversarial weights.\n• Comprehensive experiments are conducted on CIFAR-10\nand CIFAR-100 datasets across different models. We em-\npirically validate that the proposed method is able to fine-\ntune a robust model with intensive bias into a model with\nboth fairness and robustness within only two epochs.\n2. Related Work\n2.1. Robust Fairness\nIt is noted that in traditional machine learning, the defini-\ntion of fairness [1, 15] might be different from the robust\nfairness we want to tackle, where the focus of this paper is\non mitigating the fairness issue under the scenario against\nadversarial attacks.\nSeveral works [25, 27, 34, 39, 42] are explored to allevi-\nate the fairness issue in the robustness. Xu et al. [42] firstly\nrevealed that the issue of robust fairness occurs in conven-\ntional adversarial training, which can introduce severe dis-\nparity of accuracy and robustness between different groups\nof data when boosts the average robustness. To mitigate\nthis problem, they proposed a Fair-Robust-Learning (FRL)\nframework, by employing reweight and remargin strategies\nto finetune the pre-trained model, it is able to reduce the sig-\nnificant boundary error in a certain margin. Ma et al. [27]\nempirically discovered that the trade-off between robust-\nness and robustness fairness exists and AT with a larger\nperturbation radius will result in a larger variance. To miti-\ngate the trade-off between robustness and fairness, they add\na variance regularization term into the objective function,\nnamed FAT, which relieves the trade-off between average\nrobustness and robust fairness. Sun et al. [34] proposed a\nmethod called Balance Adversarial Training (BAT), which\nadjusts the attack strengths and difficulties of each class to\ngenerate samples near the decision boundary for easier and\nfairer model learning. Wei et al. [39] presented a frame-\nwork named CFA, which customizes specific training con-\nfigurations for each class automatically according to which\ncustomizes specific training configurations, such that im-\nproving the worst-class robustness while maintaining the\naverage performance. More recently, Li and Liu [25] con-\nsidered the worst-class robust risk, where they proposed a\nframework named WAT (worst-class adversarial training)\nand leverage no-regret dynamics to solve this problem.\n2.2. Distributional Robust Optimization\nThe origins of Distributional Robust Optimization (DRO)\nare found in the early studies on robust optimization [2, 4,\n7], which eventually led to the development of DRO as a\ntool for handling distributional uncertainties. The applica-\ntion of DRO to machine learning problems has garnered\nsignificant attention like domain generalization [31], data\ndistribution shift [26, 32], adversarial robustness [5, 8, 33]\nand traditional fairness in machine learning [13, 15, 23, 35].\nWhile the intersection of DRO with robustness and fairness\nhas begun to receive attention, there remain gaps in the lit-\nerature, particularly in understanding how DRO can address\nrobust fairness.\n3. Methodology\n3.1. Preliminaries\nGiven the training data drawn from a distribution P, when\nit comes to predicting labels y ∈ Y based on input features\nx ∈ X, within a model family denoted as Θ, and utilizing\na loss function ℓ := Θ × (X × Y) → R, the conventional\ntraining approach for achieving this objective is precisely\nwhat’s known as empirical risk minimization (ERM):\nmin\nθ\nE(x,y)∼P ℓ(fθ(x), y)\n(1)\nIn traditional adversarial training, the focus is on identifying\nthe worst-case perturbation for each input. This is formu-\nlated as a min-max problem, as defined in [28]. Mathemat-\nically, it can be expressed as follows:\nmin\nθ\nE(x,y)∼P max\nδ∈Bϵ ℓ(fθ(x + δ), y)\n(2)\nWhen accounting for fairness, specifically the performance\nacross different classes, Eq. (2) can be rewritten as:\nmin\nθ\n1\nC\nC\nX\nc=1\nE(x,y)∼Pc max\nδ∈Bϵ ℓ(fθ(x + δ), y)\n(3)\nIt is noted that when a batch contains an equal number of\ndata points for each class, Eq. (3) is technically identical\nto Eq. (2). According to the definition of distributional ro-\nbust optimization [3, 12], we now consider minimizing the\nexpected loss in the worst-case scenario over a set of uncer-\ntain distributions. This can be mathematically expressed as\nfollows:\nmin\nθ\nsup\nQ∈Q\nE(x,y)∼Q max\nδ∈Bϵ ℓ(fθ(x + δ), y)\n(4)\nwhere Q represents the uncertainty set, encompassing the\nrange of potential test distributions for which we seek the\nmodel to exhibit commendable performance aligned with\nthe data distribution P.\nTo establish a connection between robust fairness and\nDRO, we can naturally delineate the classes as distinct\ngroups within the training data. Subsequently, the uncer-\ntainty set Q can be defined with respect to these groups.\nSpecifically, the setting of group DRO are borrowed [17,\n29, 31], where the training distribution P is assumed to\nbe a mixture of C groups (classes) PC indexed by c =\n{1, 2, ..., C}. Thus the uncertainty set Q is defined as any\nmixture of these classes, i.e. Q := {PC\nc=1 qcPc : q ∈ ∆C},\nwhere ∆C is the probability simplex. Hence, the worst-\ncase risk can be reformulated as the most detrimental com-\nbination across different groups, taking into account the ex-\npected loss for each individual class:\nmin\nθ\nsup\nq∈∆C\nC\nX\nc=1\nqc · E(x,y)∼Pc max\nδ∈Bϵ ℓ(fθ(x + δ), y)\n(5)\nHowever, as proven in [17], applying DRO directly to ro-\nbust learning training is overly pessimistic, which often\nyields results that do not surpass those achieved by a clas-\nsifier adversarially trained using ERM. This outcome can\nbe attributed to the specific classification loss function and\nthe distributions that DRO seeks to encompass for the pur-\npose of robustness are notably extensive. A similar pat-\ntern of failure is also encountered in the context of group\nDRO [31], and they advocate that sufficient regularization\nis required for over-parameterized neural networks to en-\nhance worst-group generalization.\n3.2. Problem definition\nBy disentangling the Eq. (5), it becomes evident that it\ncan be interpreted as a reweighting objective of the ERM\nframework within the context of adversarial settings, in-\ncorporating the weighted factor q.\nEmpirical evidence\nhas demonstrated the efficacy of several reweighting meth-\nods [6, 39, 42] in improving robust fairness. This aligns\nwith the assumption stated in the introduction section, while\nwe believe that leveraging optimization techniques, as op-\nposed to heuristic assignment, will yield more effective ad-\nvancements in promoting fairness.\nAs previously mentioned, the extensive range of distri-\nbutions encompassed by the uncertainty set Q could present\ndifficulties for DRO in sustaining its robustness. Neverthe-\nless, the pivotal factor in addressing the group DRO lies in\nconfiguring the uncertainty set. To tackle this issue, we ad-\nvocate an alternative approach to resolution: instead of rely-\ning solely on substantial regularization [31], we propose to\nuse a straightforward yet effective ambiguity set with extra\nconstraint. This is achieved by defining the ambiguity set as\nQ′ := {PC\nc=1 qcPc : d(U, q) ≤ τ, q ∈ ∆C}, where d(·, ·)\nrepresents some distance metrics measuring the difference\nbetween two distributions, τ is the constraint parameter and\nU is the uniform distribution. This choice of Q′ shrinks the\nwidth of the original Q and allows us to learn models that\nare robust to some group shifts, rather than identically uni-\nform distribution among different classes. Therefore, our\nfinal objective in addressing the challenge posed by group\ndistribution shifts within an adversarial setting can be ex-\npressed as follows:\nmin\nθ\nmax\nd(U,q)≤τ,q∈∆C\nC\nX\nc=1\nqc · E(x,y)∼Pc max\nδ∈Bϵ ℓ(fθ(x + δ), y)\n(6)\nSince the robust fairness issue occurs in general adversar-\nial training, we also do not know the real class distribution\nshift may occur at test time, especially under adversarial\ntraining. The uncertainty set Q′ encodes the possible test\ndistributions that we want our model to perform well on.\nTherefore, a suitable divergence ball around the class dis-\ntribution confers robustness to a set of distributional shifts.\nIn our settings, we use KL divergence as d in our following\nexperiments, as we will see the KL-DRO [5] has its unique\nproperty which provides the optimal solution for handling\nthe fairness issue. In other words, such an overall objec-\ntive will optimize the worse distribution of the neighbor-\nhood around the uniform distribution for different classes\nby learning those adversarial examples.\n3.3. Fairness-Aware Adversarial Learning\nBased on the above objective, we propose a novel adversar-\nial learning paradigm, named Fairness-Aware Adversarial\nLearning (FAAL)1, to improve the robust fairness via dis-\ntributional robust optimization. Specifically, within the in-\ntermediary stage of the conventional adversarial training\n(between inner maximization and outer minimization), we\nintroduce a class-wise distributionally adversarial weight\nfor orientating the learning directions among different cate-\ngories, which can be optimally solved by leveraging distri-\nbutional robust optimization. By incorporating this weight\nin the outer minimization process to update the model’s\nparameters, the class (group) distribution shift can be pro-\ntected, so the robust fairness issue will be alleviated.\n1Our code is available at https://github.com/TrustAI/FAAL\nAlgorithm 1 Fairness-Aware Adversarial Learning\nInput:\nTraining set {X, Y }, total epochs T, adversar-\nial radius ϵ, step size α, the number of adversarial itera-\ntion K, model f parameterized by θ, the number of mini-\nbatches M, batch size B, distribution shift constraint τ\nOutput: A robust and fair model\n1: for t = 1 . . . T do\n2:\nfor i = 1 . . . M do\n3:\n# Phase 1: Inner maximization\nδ = 0\n4:\nfor j = 1 . . . K do\n5:\nδ = δ + α · sign(∇δℓCE(fθ(xi + δ), yi))\n6:\nδ = max(min(δ, ϵ), −ϵ)\n7:\nend for\n8:\nxadv\ni\n= clip(xi + δ, 0, 1)\n9:\n# Phase 2: Intermediate maximization\nℓi = ℓCE(fθ(xadv\ni\n), yi, reduction =‘none’)\n# Calculate the cross-entropy loss for each in-\nstance\n10:\nfor c = 1 . . . C do\n11:\nℓ′\nc = ℓCW(fθ(xadv\ni\n, yi)[yi = c]\n# Calculate the average margin for each class c\n12:\nend for\n13:\nwadv = solve kl dro(ℓ′, τ)\n# Solve the optimal weights for the current batch\nunder the worst distribution via DRO\n14:\nLFAAL = 1\nB\nPB\ni=1 wadv[y] · ℓi · C\n15:\n# Phase 3: Outer Minimization\nθ = θ − ∇θLFAAL\n16:\nend for\n17: end for\n18: return Robust model fθ with high fairness\nTo provide a clearer illustration of the comprehensive\nlearning problem, we break down the entire learning task\ninto three distinct stages:\n• Phase 1: Inner maximization for finding adversarial ex-\namples;\n• Phase 2: Intermediate maximization for finding the dis-\ntributionally adversarial weight (worst-case distribution\naround the uniform distribution);\n• Phase 3: Outer minimization for updating model’s pa-\nrameters.\nPhases 1 and 3 are the classic processes of conventional ad-\nversarial training, and Phase 2 is the core element of our\nproposed learning paradigm, as we assume that tackling the\nunknown class distributional shift can contribute to enhanc-\ning robust fairness. The pseudo-code of the whole learning\nframework can be seen in Algorithm 1. In the following\ncontent, we replace the notion of q in Eq. (6) with wadv\nfor convenience and define it as Class-wise Distributionally\nAdversarial Weight.\nDefinition 1 (CDAW: Class-wise Distributionally Adver-\nsarial Weight). Given a class-wise objective loss ℓ′\nc ∈ R on\nthe adversarial examples, for all classes c ∈ C, the Class-\nwise Distributionally Adversarial Weight vector wadv aims\nto maximize the overall loss:\nLFAAL := max\nC\nX\nc=1\nwadv\nc\nℓ′\nc\ns.t.\nd(U, wadv) ≤ τ, wadv ∈ ∆C\n(7)\nIn the case of a reweighting strategy employed to ad-\ndress robust fairness, it dictates the learning trajectory for\neach individual class. By optimizing the model using this\noptimized weight, the model is exposed to learning from\nthe worst-case distribution under the pre-defined constraint\nτ, such that fairness among different classes will be encour-\naged. When wadv is identical to U, i.e. τ = 0, it reduces\nthe regular mean calculation for the overall loss, making the\nentire learning paradigm regress to conventional adversarial\ntraining that contains Phases 1 and 3 only. We use KL di-\nvergence as d in the intermediate maximization, such that it\ncan be solved via the conic convex optimization, and another\nelegant property of it on the generalization can be obtained,\nas demonstrated in Theorem 1 below.\nTheorem 1. Given the loss LFAAL in Definition 1 on the\nobserved distribution, and suppose the regular loss L =:\n1\nC\nPC\nc=1 ℓ′\nc on the test distribution with unknown group dis-\ntribution shift, then the following holds for all wadv ∈ W:\nPr(LFAAL>L) ≥ 1 − e−τn+O(n)\n(8)\nWhere KL(U, wadv) ≤ τ, U is the uniform distribution.\nTheorem 1 tells that the Ladv is guaranteed to be the up-\nper bound of L with high probability. In line with this, it en-\njoys a strong generalization where the performance on the\ntest distribution with some unknown group distribution shift\nis at least as good as the estimated performance with high\nprobability. So the solution of the distributionally adver-\nsarial weight solving by the convex optimization is optimal\nand will provide protection on the unknown class distribu-\ntion shift. As cross-entropy loss cannot well-represents how\ngood the discrepancy between classes [9], we instead use\nthe CW margin loss [10] as L for calculating the class-wise\ndistributionally adversarial weight:\nℓ′\nc := E(x,y)∼Pc(max\nj!=y zj − zy)\n(9)\nwhere zj is the probability of the class j, i.e. the softmax\noutput of the network. It is noted that the objective func-\ntions in Phases 1-3 of our learning paradigm are not nec-\nessarily consistent, so the proposed learning mechanism is\nflexible and can be combined with any min-max adversarial\napproaches. In the following experiments, we will mainly\nsolve the distributional robust optimization on the bounded\nmargin loss among classes, which provides better perfor-\nmance than using cross-entropy loss in the intermediate\nmaximization. More details can be found in the Appendix.\n4. Experimental Results\nGiven our method’s focus on the robust fairness challenge,\nit is reasonable to assume that the model already possesses\na certain degree of average robustness. Otherwise, consid-\nering the robust fairness issue might not yield meaningful\nresults. Hence, the question arises: Is it imperative to initi-\nate the training of a model from the beginning for achieving\nfairness with a certain robustness level? In the next section,\nwe first test our approach through adversarial fine-tuning,\nand then explore if training from scratch with our method\noffers any additional benefits.\n4.1. Fine-tuning for Enhancing Robust Fairness\nBaselines and experiment settings: We first conducted ex-\nperiments on CIFAR-10 dataset [24], which is popularly\nused for adversarial training evaluation. We use the average\n& worst-class accuracy under different adversarial attacks\n(Clean / PGD [28] / CW [10] / AutoAttack [11]) as the eval-\nuation metrics. The perturbation budget is set to ϵ = 8/255\non CIFAR-10 dataset. FRL [42] is the only existing state-of-\nthe-art technique from the recent literature which performs\nfine-tuning to a pre-trained model for improving robust fair-\nness. FRL proposed two strategies based on TRADES [44]\nfor enhancing robust fairness, including reweight (RW) and\nremargin (RM). Hence we apply the best versions of FRL\nfrom their paper: FRL-RWRM with τ1 = τ2 = 0.05 and\nFRL-RWRM with τ1 = τ2 = 0.07, where τ1 and τ2 are the\nfairness constraint parameters for reweight and remargin of\nFRL, we name them FRL-RWRM0.05 and FRL-RWRM0.07\nfor short. The results of FRL are reproduced using their\npublic code, where the target models are fine-tuned for\n80 epochs and the best results are presented. In terms of\nthe proposed method, although we utilize the PGD-AT ad-\nversary method by default (named FAALAT for short), it\nis completely compatible with other AT approaches. We\nfound that 2 epochs of fine-tuning are enough to improve\nthe robust fairness greatly, without sacrificing too much av-\nerage clean/robust accuracy. We set the value of τ in our\nmethod as 0.5, and the learning rate is configured from 0.01\nin the first epoch and drops to 0.001 in the second epoch.\nTable 1 demonstrates our main results of finetuning\nWide-Resnet34-10 (WRN-34-10) models [43] on CIFAR-\n10 dataset, where different state-of-the-art adversarial de-\nfensed methods are adopted, including PGD-AT [28],\nTRADES [44], MART [38] and AWP [41]. We can see\nthat FAALAT outperforms the two FRL methods with re-\nTable 1. Evaluation of different fine-tuning methods on CIFAR-10 dataset using Wide-ResNet34-10 model. The best performance is\nhighlighted in Bold.\nAdversarially Trained WRN-34-10 Model\nFine-Tuning\nEpochs\nAverage Accuracy (Worst-class Accuracy) (%)\nClean\nPGD-20\nCW-20\nAutoAttack\nPGD-AT\n-\n86.07 (69.70)\n55.90 (29.90)\n54.29 (28.30)\n52.46 (24.40)\n+ Fine-tune with FRL-RWRM0.05\n80\n83.25 (74.80)\n50.37 (38.10)\n49.77 (36.60)\n46.97 (33.10)\n+ Fine-tune with FRL-RWRM0.07\n80\n85.12 (71.60)\n52.56 (37.10)\n51.92 (35.50)\n49.60 (31.70)\n+ Fine-tune with FAALAT\n2\n86.23 (69.70)\n54.00 (37.60)\n53.11 (36.90)\n50.81 (35.70)\n+ Fine-tune with FAALAT−AWP\n2\n85.47 (69.40)\n56.46 (39.20)\n54.50 (38.10)\n52.47 (36.90)\nTRADES\n-\n84.92 (67.00)\n55.32 (27.10)\n53.92 (24.80)\n52.51 (23.20)\n+ Fine-tune with FRL-RWRM0.05\n80\n82.90 (72.70)\n53.16 (40.60)\n51.39 (36.30)\n49.97 (35.40)\n+ Fine-tune with FRL-RWRM0.07\n80\n85.19 (70.90)\n53.76 (39.20)\n52.92 (36.80)\n51.30 (34.60)\n+ Fine-tune with FAALAT\n2\n85.96 (75.00)\n53.46 (39.80)\n52.72 (38.20)\n50.91 (35.30)\n+ Fine-tune with FAALAT−AWP\n2\n85.39 (72.90)\n56.07 (43.30)\n54.16 (38.60)\n52.45 (35.40)\nMART\n-\n83.62 (67.90)\n56.22 (32.50)\n52.79 (25.70)\n50.95 (22.00)\n+ Fine-tune with FRL-RWRM0.05\n80\n83.72 (71.80)\n52.16 (37.50)\n50.73 (35.00)\n49.19 (31.70)\n+ Fine-tune with FRL-RWRM0.07\n80\n82.09 (71.80)\n50.86 (36.00)\n49.78 (33.00)\n47.78 (30.30)\n+ Fine-tune with FAALAT\n2\n83.49 (68.00)\n51.65 (37.80)\n50.36 (37.10)\n48.63 (34.00)\n+ Fine-tune with FAALAT−AWP\n2\n82.17 (64.00)\n54.31 (39.50)\n51.72 (37.70)\n50.31 (36.40)\nTRADES-AWP\n-\n85.35 (67.90)\n59.20 (28.80)\n57.14 (26.50)\n56.18 (25.80)\n+ Fine-tune with FRL-RWRM0.05\n80\n82.31 (65.90)\n49.90 (31.70)\n49.68 (34.00)\n46.50 (27.70)\n+ Fine-tune with FRL-RWRM0.07\n80\n84.24 (65.70)\n48.63 (30.90)\n49.77 (31.50)\n46.53 (28.60)\n+ Fine-tune with FAALAT\n2\n87.02 (76.30)\n52.54 (35.00)\n51.70 (34.40)\n49.87 (30.60)\n+ Fine-tune with FAALAT−AWP\n2\n86.75 (74.80)\n57.14(43.40)\n55.34 (40.10)\n53.93 (37.00)\nspect to both average and worst-class robustness. Notably,\nin the majority of cases, FAALAT achieves this without\nsignificant compromises in clean accuracy, unlike the FRL\nmethods which tend to trade off the clean accuracy for im-\nproving robustness. FAALAT promotes the worst-class Au-\ntoAttack (AA) accuracy by approximately 2.6% than FRL\nfor fine-tuning PGD-AT, MART, and AWP. Except for fine-\ntuning TRADES, both methods yield comparable perfor-\nmance, this is due to that FRL is a TRADES-based method\nand it takes advantage of knowing the source method. Most\nimportantly, FRL requires many epochs (80 epochs) to ob-\ntain the best results, while our method, is able to achieve\nbetter results within only 2 epochs. As adversarially train-\ning a large model with high robustness is already time-\nconsuming, to circumvent the need for retraining the model\nfrom the beginning, FAAL offers a solution for saving time\nand computational resources. It demonstrates the capability\nto quickly fine-tune a robust model that initially lacks fair-\nness, resulting in a model that is both robust and fair. Due to\nthe space limit, similar improvements on the Preact-Resnet\nmodel can be found in the Appendix.\nStrong adversarial attacks can help? The remargin\nof FRL [42] claims that increasing the perturbation margin\ncan help for obtaining better robust fairness, while this may\nhurt the average clean accuracy/robustness, as indicated in\nTab. 1. Certainly, there is a trade-off existing between the\naverage robustness and worst-class robustness, but is it nec-\nessary to increase the perturbation margin ϵ for improving\nthe class-wise robustness? We question whether this is a\nmandatory requirement for improvements, and we assume\nthe benefits come from the stronger strength of adversar-\nial perturbation. Hence, instead of enlarging the perturba-\ntion margin, we capitalize on the flexibility of our learning\nframework and integrate our method with AWP [41], a well-\nregarded model weight perturbation technique, to strength\nthe attacks. As shown in Tab. 1, when combining with AWP,\nFAALAT−AWP further enhances the worst-class robust ac-\ncuracy on WRN34-10 models, especially for the original\none is adversarial trained with AWP. FAALAT−AWP is al-\nmost unharmed on the improvement to the original unfair\nmodels most of the time.\nTherefore, it is not compul-\nsory to enlarge the perturbation margin to gain better re-\nsults, where applying a stronger adversary indeed benefits\nrobust fairness without enlarging the perturbation margin.\nFigure 2 visualizes the results of class-wise AA accuracy\nfor the comparison of the proposed method FAALAT and\nFAALAT−AWP, and two FRL baselines, respectively. It can\nbe seen that FAAL clearly boost the worst-class robust ac-\ncuracy, presenting outstanding capacity to improve robust\nfairness with high effectiveness and efficiency, respectively,\nwhere it outperforms FRL not only for the average/worst-\nclass robustness but also for the very rare fine-tuning steps.\nFigure 2. Class-wise robust accuracy against AutoAttack after fine-tuning the PGD adversarially trained WRN model\nFigure 3. Class-wise robust accuracy against AutoAttack after adversarially trained PRN-18 model from scratch\nTable 2. Training from scratch with different methods on CIFAR-\n10 dataset using Preact-ResNet18 model.\nAdversarially Trained PRN-18 Model\nAverage Acc (Worst-class Acc) (%)\nClean\nAutoAttack\nPGD-AT\n82.72 (55.80)\n47.38 (12.90)\nTRADES\n82.54 (66.10)\n49.05 (20.70)\nCFAAT\n80.82 (64.60)\n50.10 (24.40)\nCFATRADES\n80.36 (66.20)\n50.10 (26.50)\nWATTRADES\n80.37 (66.00)\n46.16 (30.70)\nFAALAT\n82.20 (62.90)\n49.10 (33.70)\nFAALTRADES\n81.62 (68.90)\n48.48 (33.60)\n4.2. Training from Scratch for Enhancing Fairness\nPrevious sections demonstrate the effectiveness and effi-\nciency of the proposed approaches. Here we also investigate\nthe advancements by training the model from the ground\nup using our method. We compare our methods with two\ncommon adversarial training methods (PGD-AT [28] and\nTRADES [44]) and two recent state-of-the-art techniques:\nCFA [39] and WAT [25], which have been proposed to mit-\nigate the robust fairness issues recently. We adversarially\ntrained Preact-ResNet-18 models [16] for 200 epochs with a\nlearning rate of 0.1, which will be decayed by a factor of 0.1\nat 100 and 150 epochs, successively. We start to facilitate\nthe proposed intermediate maximization (see Algorithm 1\nlines 9-14) after the 100-th epoch with the only hyper-\nparameter τ from 0.25 and enlarge it to 0.5 after the 150-th\nepoch. In addition, similar to CFA using weight average,\nwe also applied EMA [20, 37], to gain a more stable perfor-\nmance, however, we only applied it after the 100-th epoch,\nwhere we start to apply the intermediate maximization. We\nreport the best results under AutoAttack on the average ac-\ncuracy and worst-class accuracy in Tab. 2. Besides, Fig. 3\nvisualizes the results of different training approaches in-\ncluding the proposed FAALTRADES with other 3 TRADES-\nbased models: TRADES, CFATRADES and WATTRADES\nrespectively. We can observe that FAAL outperforms other\napproaches on the worst-class clean/robust accuracy, with\nless sacrifice on the average robustness.\n4.3. Additional Experiments on CIFAR-100 dataset\nThe experiments above mainly focused on CIFAR-10\ndataset, which only has 10 classes in the dataset. In this sec-\ntion, we explore the proposed FAAL into a more challeng-\ning dataset, i.e. CIFAR-100 with 100 categories. Similarly,\nwe reported the results of the average/worst clean accuracy\nand AutoAttack accuracy. The value of τ in our method\nTable 3. Result comparison of different methods on CIFAR-100\ndataset using ResNet18 model.\nAdversarially Trained RN-18 Model\nAverage Acc (Worst-class Acc) (%)\nClean\nAutoAttack\nTRADES\n54.57 (19.00)\n23.57 (1.00)\n+ Fine-tune with FRL-RWRM0.05\n52.55 (22.00)\n21.11 (2.00)\n+ Fine-tune with FAALAT\n58.50 (21.00)\n21.91 (2.00)\n+ Fine-tune with FAALAT−AWP\n58.41 (19.00)\n23.44 (2.00)\n+ Fine-tune with FAALTRADES\n54.96 (18.00)\n22.71 (2.00)\n+ Fine-tune with FAALTRADES−AWP\n54.90 (18.00)\n23.25 (2.00)\nCFATRADES\n55.57 (23.00)\n24.56 (2.00)\nWATTRADES\n53.99 (19.00)\n22.89 (3.00)\nFAALAT\n56.84 (16.00)\n21.85 (3.00)\nFAALTRADES\n55.87 (21.00)\n23.57 (3.00)\nTable 4. Comparison among different SOTA methods, all mod-\nels are trained with the same number of samples under a single\nNVIDIA 3090Ti GPU in the same conda environment.\nMethods\nTraining time per epoch (min)\nReweighting\nlevel\nAdversary\nfree\nValidation\nset\nCIFAR-10\n(PRN-18)\nCIFAR-100\n(RN-18)\nTRADES\n2.63\n2.68\nfixed\n×\n×\nFRL-RWRM\n2.73\n2.80\nepoch\n×\n✓\nWAT\n2.88\n3.00\nepoch\n×\n✓\nCFA\n2.75\n2.78\nepoch\n✓\n✓\nFAAL\n2.69\n2.73\nbatch\n✓\n×\nis set to 0.05. For fine-tuning, we compare our proposed\nmethod FAAL with FRL-RWRM0.05 [42], it can be seen\nthat FAAL is able to achieve comparable to FRL-RWRM\nwhile reducing the amount of learning epoch up to 40 times\n(2 epochs vs. 80 epochs). For full adversarial training, fol-\nlowing the experimental settings in WAT [25], we train the\nResNet-18 models for 100 epochs via different adversar-\nial training approaches, where the learning rate is decayed\nfrom 0.1 to 0.01 and 0.001 at the 75-th epoch and the 90-th\nepoch, respectively. We compare the results of FAAL com-\npared to three baselines, i.e. TRADES, CFA and WAT. It\ncan be seen that FAATTRADES achieves the highest worst-\nclass robust accuracy (same as WAT), meanwhile, it remains\ncomparable results on the average robustness without sacri-\nficing the average/worst-class clean accuracy. More details\nof the training settings can be found in the Appendix.\n5. Essential Differences to SOTAs\nIn this section, we highlight the essential differences\nof FAAL with existing state-of-the-art works, including\nFRL [42], WAT [25] and CFA [39]. Both FRL and WAT\nare TRADES-based approaches, which require a separate\nvalidation set for performing the reweight strategies. For\nexample, FRL updates the lagrangian multiplier according\nto the performance of the validation set to meet the fairness\nconstraints, so it requires many epochs for fine-tuning since\nit needs to search the whole space to achieve the optimal\nequilibrium without fairness constraint violation. Also, the\nremargin strategy in FRL essentially sacrifices some aver-\nage clean accuracy. We argue that it is not necessary to en-\nlarge the margin for improvement, which can be achieved\nby combining stronger perturbations instead. As another\nTRADES-based variant, WAT leverages no-regret dynam-\nics and also relies on the validation set to tune the class\nweights for the current epoch training, Similarly, CFA pro-\nposed to apply the weight averaging only if the performance\non the extra validation set meets a certain threshold, and re-\nlies on empirically adjusting the class margins and class reg-\nularization based on the performance of the previous epoch.\nUnlike those methods that rely on historical performance\nor an extra validation set for manual or heuristic weight ad-\njustment per class in each epoch, our method bypasses these\nrequirements. FAAL introduces an additional conic convex\noptimization problem after the adversarial example gener-\nation, based solely on the current batch’s objective loss,\nthe bringing solving cost is negligible. The comparison of\ntraining computation time and other key properties is illus-\ntrated in Tab. 4. As model training can be unpredictable\ndue to random mini-batch sampling, causing quick shifts\nin class distribution and bias that may differ from previous\nepochs or validations More importantly, FAAL can gener-\nalize to any adversarial training methods, as our interme-\ndiate maximization is a completely dependent component\nof the popular min-max framework, so it is not limited to\nany adversaries, unlike some methods FRL and WAT that\nare restricted to TRADES. Our data-driven component en-\nhances flexibility in managing the balance between average\nrobustness and robust fairness during adversarial training,\nand demonstrates its potential in handling various distribu-\ntion shifts for the current batch.\n6. Conclusion\nIn conclusion, we establish a connection between robust\nfairness and group distribution shift overfitting and present\na new fairness-aware adversarial learning paradigm to ad-\ndress robust fairness. Compared to state-of-the-art meth-\nods, extensive experiments on CIFAR-10 and CIFAR-100\ndatasets demonstrate the effectiveness and superior effi-\nciency of the proposed approach.\nNotably, by just two\nepochs of fine-tuning, our training strategy can transform\na biased robust model into one with high fairness with lit-\ntle cost for average accuracy. We believe our research pro-\nvides a meaningful contribution to the discourse on robust-\nness and fairness in machine learning, deepening our insight\ninto the model’s behaviors under adversarial settings.\nAcknowledgement\nThe research is supported by the UK EPSRC under project\nEnnCORE [EP/T026995/1].\nReferences\n[1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John\nLangford, and Hanna Wallach.\nA reductions approach to\nfair classification. In International Conference on Machine\nLearning, pages 60–69. PMLR, 2018. 2\n[2] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Ne-\nmirovski. Robust optimization. Princeton university press,\n2009. 3\n[3] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere,\nBertrand Melenberg, and Gijs Rennen.\nRobust solutions\nof optimization problems affected by uncertain probabilities.\nManagement Science, 59(2):341–357, 2013. 2, 3\n[4] Amine Bennouna and Bart Van Parys. Holistic robust data-\ndriven decisions. arXiv preprint arXiv:2207.09560, 2022. 3\n[5] Amine Bennouna, Ryan Lucas, and Bart Van Parys. Certi-\nfied robust neural networks: Generalization and corruption\nresistance. arXiv preprint arXiv:2303.02251, 2023. 3, 4\n[6] Philipp Benz, Chaoning Zhang, Adil Karjauv, and In So\nKweon. Robustness may be at odds with fairness: An em-\npirical study on class-wise accuracy. In NeurIPS 2020 Work-\nshop on Pre-registration in Machine Learning, pages 325–\n342. PMLR, 2021. 3\n[7] Dimitris Bertsimas, David B Brown, and Constantine Cara-\nmanis. Theory and applications of robust optimization. SIAM\nreview, 53(3):464–501, 2011. 3\n[8] Tuan Anh Bui, Trung Le, Quan Tran, He Zhao, and\nDinh Phung.\nA unified wasserstein distributional robust-\nness framework for adversarial training.\narXiv preprint\narXiv:2202.13437, 2022. 3\n[9] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga,\nand Tengyu Ma. Learning imbalanced datasets with label-\ndistribution-aware margin loss. Advances in neural informa-\ntion processing systems, 32, 2019. 5\n[10] Nicholas Carlini and David Wagner. Towards evaluating the\nrobustness of neural networks. In 2017 ieee symposium on\nsecurity and privacy (sp), pages 39–57. Ieee, 2017. 5\n[11] Francesco Croce and Matthias Hein.\nReliable evalua-\ntion of adversarial robustness with an ensemble of diverse\nparameter-free attacks. In International conference on ma-\nchine learning, pages 2206–2216. PMLR, 2020. 1, 5\n[12] John C Duchi, Peter W Glynn, and Hongseok Namkoong.\nStatistics of robust optimization: A generalized empirical\nlikelihood approach. Mathematics of Operations Research,\n46(3):946–969, 2021. 2, 3\n[13] Julien Ferry, Ulrich Aivodji, S´ebastien Gambs, Marie-Jos´e\nHuguet, and Mohamed Siala. Improving fairness generaliza-\ntion through a sample-robust optimization method. Machine\nLearning, pages 1–62, 2022. 3\n[14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.\nExplaining and harnessing adversarial examples.\narXiv\npreprint arXiv:1412.6572, 2014. 1\n[15] Tatsunori\nHashimoto,\nMegha\nSrivastava,\nHongseok\nNamkoong, and Percy Liang.\nFairness without demo-\ngraphics in repeated loss minimization.\nIn International\nConference on Machine Learning,\npages 1929–1938.\nPMLR, 2018. 2, 3\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 7\n[17] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama.\nDoes distributionally robust supervised learning give robust\nclassifiers? In International Conference on Machine Learn-\ning, pages 2029–2037. PMLR, 2018. 3\n[18] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James\nSharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping\nYi. A survey of safety and trustworthiness of deep neural net-\nworks: Verification, testing, adversarial attack and defence,\nand interpretability. Computer Science Review, 37:100270,\n2020. 1\n[19] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi\nDong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi\nQi, Xingyu Zhao, et al. A survey of safety and trustworthi-\nness of large language models through the lens of verification\nand validation. arXiv preprint arXiv:2305.11391, 2023. 1\n[20] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry\nVetrov, and Andrew Gordon Wilson.\nAveraging weights\nleads to wider optima and better generalization.\narXiv\npreprint arXiv:1803.05407, 2018. 7\n[21] Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xi-\naowei Huang. Enhancing adversarial training with second-\norder statistics of weights. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 15273–15283, 2022. 1\n[22] Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xi-\naowei Huang.\nRandomized adversarial training via taylor\nexpansion.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 16447–\n16457, 2023. 1\n[23] Sangwon Jung, Taeeon Park, Sanghyuk Chun, and Taesup\nMoon. Re-weighting based group fairness regularization via\nclasswise robust optimization. In The Eleventh International\nConference on Learning Representations, 2022. 3\n[24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 5\n[25] Boqi Li and Weiwei Liu. Wat: improve the worst-class ro-\nbustness in adversarial training. In Proceedings of the AAAI\nConference on Artificial Intelligence, pages 14982–14990,\n2023. 2, 3, 7, 8\n[26] Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun\nKuang, and Bo Li. Distributionally robust learning with sta-\nble adversarial training. IEEE Transactions on Knowledge\nand Data Engineering, 2022. 3\n[27] Xinsong Ma, Zekai Wang, and Weiwei Liu. On the trade-\noff between robustness and fairness. In Advances in Neural\nInformation Processing Systems, 2022. 2\n[28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,\nDimitris Tsipras, and Adrian Vladu. Towards deep learn-\ning models resistant to adversarial attacks. arXiv preprint\narXiv:1706.06083, 2017. 1, 3, 5, 7\n[29] Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and\nPercy Liang.\nDistributionally robust language modeling.\narXiv preprint arXiv:1909.02060, 2019. 3\n[30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115:211–252, 2015. 1\n[31] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and\nPercy Liang.\nDistributionally robust neural networks for\ngroup shifts: On the importance of regularization for worst-\ncase generalization. arXiv preprint arXiv:1911.08731, 2019.\n3, 4\n[32] Aman Sinha, Hongseok Namkoong, and John Duchi. Certi-\nfying some distributional robustness with principled adver-\nsarial training.\nIn International Conference on Learning\nRepresentations, 2018. 3\n[33] Matthew Staib and Stefanie Jegelka. Distributionally robust\ndeep learning as a generalization of adversarial training. In\nNIPS workshop on Machine Learning and Computer Secu-\nrity, page 4, 2017. 3\n[34] Chunyu Sun, Chenye Xu, Chengyuan Yao, Siyuan Liang,\nYichao Wu, Ding Liang, XiangLong Liu, and Aishan Liu.\nImproving robust fairness via balance adversarial training.\narXiv preprint arXiv:2209.07534, 2022. 2, 3\n[35] Hieu Vu, Toan Tran, Man-Chung Yue, and Viet Anh\nNguyen. Distributionally robust fair principal components\nvia geodesic descents.\nIn International Conference on\nLearning Representations, 2021. 3\n[36] Fu Wang, Yanghao Zhang, Yanbin Zheng, and Wenjie Ruan.\nDynamic efficient adversarial training guided by gradient\nmagnitude. In Progress and Challenges in Building Trust-\nworthy Embodied AI, 2022. 1\n[37] Hongjun Wang and Yisen Wang.\nSelf-ensemble adver-\nsarial training for improved robustness.\narXiv preprint\narXiv:2203.09678, 2022. 7\n[38] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun\nMa, and Quanquan Gu. Improving adversarial robustness\nrequires revisiting misclassified examples. In International\nconference on learning representations, 2019. 5\n[39] Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. Cfa:\nClass-wise calibrated fair adversarial training. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 8193–8201, 2023. 2, 3, 7, 8\n[40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau-\nmond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, R´emi Louf, Morgan Funtowicz, et al. Transformers:\nState-of-the-art natural language processing.\nIn Proceed-\nings of the 2020 conference on empirical methods in natural\nlanguage processing: system demonstrations, pages 38–45,\n2020. 1\n[41] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial\nweight perturbation helps robust generalization. Advances\nin Neural Information Processing Systems, 33:2958–2969,\n2020. 5, 6\n[42] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang.\nTo be robust or to be fair: Towards fairness in adversarial\ntraining. In International Conference on Machine Learning,\npages 11492–11501. PMLR, 2021. 1, 2, 3, 5, 6, 8\n[43] Sergey Zagoruyko and Nikos Komodakis. Wide residual net-\nworks. arXiv preprint arXiv:1605.07146, 2016. 1, 5\n[44] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Lau-\nrent El Ghaoui, and Michael Jordan. Theoretically principled\ntrade-off between robustness and accuracy. In International\nconference on machine learning, pages 7472–7482. PMLR,\n2019. 5, 7\n[45] Yanghao Zhang, Wenjie Ruan, Fu Wang, and Xiaowei\nHuang. Generalizing universal adversarial perturbations for\ndeep neural networks.\nMachine Learning, 112(5):1597–\n1626, 2023. 1\n"
}