{
    "optim": "Towards Fairness-Aware Adversarial Learning Yanghao Zhang Tianle Zhang Ronghui Mu Xiaowei Huang Wenjie Ruan* University of Liverpool, UK {yanghao.zhang, tianle.zhang, ronghui.mu, xiaowei.huang}@liverpool.ac.uk, w.ruan@trustai.uk Abstract Although adversarial training (AT) has proven effective in enhancing the model’s robustness, the recently revealed issue of fairness in robustness has not been well addressed, i.e. the robust accuracy varies significantly among different categories. In this paper, instead of uniformly evaluating the model’s average class performance, we delve into the issue of robust fairness, by considering the worst-case dis- tribution across various classes. We propose a novel learn- ing paradigm, named Fairness-Aware Adversarial Learn- ing (FAAL). As a generalization of conventional AT, we re- define the problem of adversarial training as a min-max- max framework, to ensure both robustness and fairness of the trained model. Specifically, by taking advantage of dis- tributional robust optimization, our method aims to find the worst distribution among different categories, and the solu- tion is guaranteed to obtain the upper bound performance with high probability. In particular, FAAL can fine-tune an unfair robust model to be fair within only two epochs, with- out compromising the overall clean and robust accuracies. Extensive experiments on various image datasets validate the superior performance and efficiency of the proposed FAAL compared to other state-of-the-art methods. 1. Introduction Deep learning models have undoubtedly achieved remark- able success across various domains, such as computer vi- sion [30] and natural language processing [40]. However, they still remain susceptible to deliberate adversarial manip- ulations of input data [14, 18, 19, 45]. Robust learning tech- niques [21, 22, 28, 36] have emerged as a potential solution, aiming to enhance a model’s resilience against such vulner- abilities. These techniques have demonstrated a promising ability to enhance a model’s overall robustness, yet the in- tricate connection between robustness and fairness, as re- vealed by [42], demonstrates that the robust accuracy of the models can vary considerably across different categories or *corresponding author Figure 1. Class-wise accuracy of the Wide-ResNet34-10 model on CIFAR-10 dataset, where AA accuracy represents the robust accuracy against AutoAttack. classes. Consider a scenario where an autonomous driv- ing system attains commendable average robust accuracy in recognizing road objects. Despite this success, the system might demonstrate robustness against categories like inani- mate objects (with high accuracy) while displaying vulner- ability to crucial categories such as “human” (with low ac- curacy). This disparity or unfair robustness could poten- tially endanger drivers and pedestrians. Hence, it is vital to ensure consistent, equitable model performance against ad- versarial attacks by assessing worst-case robustness beyond average levels. This provides a more accurate evaluation than the average performance, recognizing the model’s lim- itations while ensuring reliability across diverse categories in real-world applications. Figure 1 provides an example, displaying the class- wise clean accuracy and robust accuracy (against AutoAt- tack [11]) using the Wide-ResNet34-10 [43] models on CIFAR-10 dataset, where both models are trained via stan- dard training and adversarial training, respectively. It comes as no surprise that the model with standard training is vul- nerable to adversarial attacks, yet it manages to attain com- parable performance across various classes. In contrast, the adversarially trained model exhibits a noticeable bias, con- arXiv:2402.17729v1  [cs.CV]  27 Feb 2024 fidently classifying “automobile” but hesitating with “cat”. where robust accuracy diverges significantly across classes. Furthermore, it is noteworthy that even though the class “cat” attains the lowest clean and robust accuracy, the most significant disparity between clean and robust accu- racy arises in the case of the class “deer”. This finding highlights an inconsistency between the clean and adver- sarial performances, particularly in terms of fairness where robust accuracy diverges significantly across classes. It is clear that the disparity between clean and robust accuracy is more pronounced in terms of fairness issues, where the ro- bust accuracy on different classes illustrates severe diverges in the model. This phenomenon is called the “robust fairness” issue, which is first revealed in [42] referring to the gap between average robustness and worst-class robustness. Recently, some pioneering solutions have been proposed to address the robust fairness issue [25, 34, 39]. These efforts either tackle the robust fairness from the adversarial example gen- eration or adjust the class weights empirically. However, essentially, most of these methods can be regarded as in- stances of reweighting, albeit diverse heuristic strategies are adopted. It is worth noting that such issues are absent in models trained without adversarial training, thus it is ob- vious that the underlying issue stems from the adversar- ial perturbations during the adversarial training. Inspired by but beyond these studies, we are prompted to consider why reweighting strategies are effective in mitigating the robust fairness issue. Intuitively, a model trained adversar- ially without reweighting fails to achieve high worst-class robust accuracy because it treats all classes equally, yet ne- glects the fact that the replaced adversarial examples may introduce bias to the final model. Conversely, reweighting can be perceived as a means of inducing a form of group distribution shift. This shift disrupts the uniform optimiza- tion of different classes, compelling the model to acquire resistance against these distribution shifts. This, in turn, leads to an enhancement in robust fairness. In line with this, we adopt an alternative approach to address this issue in the adversarial learning paradigm, where the following assumption is made: The robust fairness issue in the conventional AT is due to the unknown group (class) distribution shift induced by adversarial perturbations, which results in the overfitting problem. As opposed to the heuristic assignment for the reweight- ing item in the prior works, we believe that leveraging some optimization techniques for solving this overfitting prob- lem can bring better results. Hence Distributional Robust Optimization (DRO) [3, 12] naturally emerges as a viable choice. Rather than assuming a fixed uniform data distribu- tion, DRO acknowledges the inherent distributional uncer- tainty in real-world data, offering a more resilient and adapt- able model structure. Therefore, this paper delves into the exploration and adaption of DRO, as a sensible solution for the robust fairness challenge. Specifically, after finding the adversarial example in adversarial training, instead of man- ually or empirically adjusting the weights for each class, we resort to learning the class-wise distributionally adversarial weights with the pre-defined constraints via DRO. By learn- ing with these weights, the model will be guided to acquire the capacity to resist unknown group distribution shifts. The contributions of this paper are summarized as follows: • We investigate the robust fairness issue from the per- spective of group/class distributional shift, by taking the recent advances of Distributional Robust Optimization (DRO), which ultimately falls into a reweighting prob- lem. To the best of our knowledge, this work is the first at- tempt to address the challenge of robust fairness through distributional robust optimization. • We introduce a novel learning paradigm, named FAAL (Fairness-Aware Adversarial Learning). This innovative approach extends the conventional min-max adversarial training framework into a min-max-max formulation. The intermediate maximization is dedicated to dealing with the robust fairness issue, by learning with the class-wise distributionally adversarial weights. • Comprehensive experiments are conducted on CIFAR-10 and CIFAR-100 datasets across different models. We em- pirically validate that the proposed method is able to fine- tune a robust model with intensive bias into a model with both fairness and robustness within only two epochs. 2. Related Work 2.1. Robust Fairness It is noted that in traditional machine learning, the defini- tion of fairness [1, 15] might be different from the robust fairness we want to tackle, where the focus of this paper is on mitigating the fairness issue under the scenario against adversarial attacks. Several works [25, 27, 34, 39, 42] are explored to allevi- ate the fairness issue in the robustness. Xu et al. [42] firstly revealed that the issue of robust fairness occurs in conven- tional adversarial training, which can introduce severe dis- parity of accuracy and robustness between different groups of data when boosts the average robustness. To mitigate this problem, they proposed a Fair-Robust-Learning (FRL) framework, by employing reweight and remargin strategies to finetune the pre-trained model, it is able to reduce the sig- nificant boundary error in a certain margin. Ma et al. [27] empirically discovered that the trade-off between robust- ness and robustness fairness exists and AT with a larger perturbation radius will result in a larger variance. To miti- gate the trade-off between robustness and fairness, they add a variance regularization term into the objective function, named FAT, which relieves the trade-off between average robustness and robust fairness. Sun et al. [34] proposed a method called Balance Adversarial Training (BAT), which adjusts the attack strengths and difficulties of each class to generate samples near the decision boundary for easier and fairer model learning. Wei et al. [39] presented a frame- work named CFA, which customizes specific training con- figurations for each class automatically according to which customizes specific training configurations, such that im- proving the worst-class robustness while maintaining the average performance. More recently, Li and Liu [25] con- sidered the worst-class robust risk, where they proposed a framework named WAT (worst-class adversarial training) and leverage no-regret dynamics to solve this problem. 2.2. Distributional Robust Optimization The origins of Distributional Robust Optimization (DRO) are found in the early studies on robust optimization [2, 4, 7], which eventually led to the development of DRO as a tool for handling distributional uncertainties. The applica- tion of DRO to machine learning problems has garnered significant attention like domain generalization [31], data distribution shift [26, 32], adversarial robustness [5, 8, 33] and traditional fairness in machine learning [13, 15, 23, 35]. While the intersection of DRO with robustness and fairness has begun to receive attention, there remain gaps in the lit- erature, particularly in understanding how DRO can address robust fairness. 3. Methodology 3.1. Preliminaries Given the training data drawn from a distribution P, when it comes to predicting labels y ∈ Y based on input features x ∈ X, within a model family denoted as Θ, and utilizing a loss function ℓ := Θ × (X × Y) → R, the conventional training approach for achieving this objective is precisely what’s known as empirical risk minimization (ERM): min θ E(x,y)∼P ℓ(fθ(x), y) (1) In traditional adversarial training, the focus is on identifying the worst-case perturbation for each input. This is formu- lated as a min-max problem, as defined in [28]. Mathemat- ically, it can be expressed as follows: min θ E(x,y)∼P max δ∈Bϵ ℓ(fθ(x + δ), y) (2) When accounting for fairness, specifically the performance across different classes, Eq. (2) can be rewritten as: min θ 1 C C X c=1 E(x,y)∼Pc max δ∈Bϵ ℓ(fθ(x + δ), y) (3) It is noted that when a batch contains an equal number of data points for each class, Eq. (3) is technically identical to Eq. (2). According to the definition of distributional ro- bust optimization [3, 12], we now consider minimizing the expected loss in the worst-case scenario over a set of uncer- tain distributions. This can be mathematically expressed as follows: min θ sup Q∈Q E(x,y)∼Q max δ∈Bϵ ℓ(fθ(x + δ), y) (4) where Q represents the uncertainty set, encompassing the range of potential test distributions for which we seek the model to exhibit commendable performance aligned with the data distribution P. To establish a connection between robust fairness and DRO, we can naturally delineate the classes as distinct groups within the training data. Subsequently, the uncer- tainty set Q can be defined with respect to these groups. Specifically, the setting of group DRO are borrowed [17, 29, 31], where the training distribution P is assumed to be a mixture of C groups (classes) PC indexed by c = {1, 2, ..., C}. Thus the uncertainty set Q is defined as any mixture of these classes, i.e. Q := {PC c=1 qcPc : q ∈ ∆C}, where ∆C is the probability simplex. Hence, the worst- case risk can be reformulated as the most detrimental com- bination across different groups, taking into account the ex- pected loss for each individual class: min θ sup q∈∆C C X c=1 qc · E(x,y)∼Pc max δ∈Bϵ ℓ(fθ(x + δ), y) (5) However, as proven in [17], applying DRO directly to ro- bust learning training is overly pessimistic, which often yields results that do not surpass those achieved by a clas- sifier adversarially trained using ERM. This outcome can be attributed to the specific classification loss function and the distributions that DRO seeks to encompass for the pur- pose of robustness are notably extensive. A similar pat- tern of failure is also encountered in the context of group DRO [31], and they advocate that sufficient regularization is required for over-parameterized neural networks to en- hance worst-group generalization. 3.2. Problem definition By disentangling the Eq. (5), it becomes evident that it can be interpreted as a reweighting objective of the ERM framework within the context of adversarial settings, in- corporating the weighted factor q. Empirical evidence has demonstrated the efficacy of several reweighting meth- ods [6, 39, 42] in improving robust fairness. This aligns with the assumption stated in the introduction section, while we believe that leveraging optimization techniques, as op- posed to heuristic assignment, will yield more effective ad- vancements in promoting fairness. As previously mentioned, the extensive range of distri- butions encompassed by the uncertainty set Q could present difficulties for DRO in sustaining its robustness. Neverthe- less, the pivotal factor in addressing the group DRO lies in configuring the uncertainty set. To tackle this issue, we ad- vocate an alternative approach to resolution: instead of rely- ing solely on substantial regularization [31], we propose to use a straightforward yet effective ambiguity set with extra constraint. This is achieved by defining the ambiguity set as Q′ := {PC c=1 qcPc : d(U, q) ≤ τ, q ∈ ∆C}, where d(·, ·) represents some distance metrics measuring the difference between two distributions, τ is the constraint parameter and U is the uniform distribution. This choice of Q′ shrinks the width of the original Q and allows us to learn models that are robust to some group shifts, rather than identically uni- form distribution among different classes. Therefore, our final objective in addressing the challenge posed by group distribution shifts within an adversarial setting can be ex- pressed as follows: min θ max d(U,q)≤τ,q∈∆C C X c=1 qc · E(x,y)∼Pc max δ∈Bϵ ℓ(fθ(x + δ), y) (6) Since the robust fairness issue occurs in general adversar- ial training, we also do not know the real class distribution shift may occur at test time, especially under adversarial training. The uncertainty set Q′ encodes the possible test distributions that we want our model to perform well on. Therefore, a suitable divergence ball around the class dis- tribution confers robustness to a set of distributional shifts. In our settings, we use KL divergence as d in our following experiments, as we will see the KL-DRO [5] has its unique property which provides the optimal solution for handling the fairness issue. In other words, such an overall objec- tive will optimize the worse distribution of the neighbor- hood around the uniform distribution for different classes by learning those adversarial examples. 3.3. Fairness-Aware Adversarial Learning Based on the above objective, we propose a novel adversar- ial learning paradigm, named Fairness-Aware Adversarial Learning (FAAL)1, to improve the robust fairness via dis- tributional robust optimization. Specifically, within the in- termediary stage of the conventional adversarial training (between inner maximization and outer minimization), we introduce a class-wise distributionally adversarial weight for orientating the learning directions among different cate- gories, which can be optimally solved by leveraging distri- butional robust optimization. By incorporating this weight in the outer minimization process to update the model’s parameters, the class (group) distribution shift can be pro- tected, so the robust fairness issue will be alleviated. 1Our code is available at https://github.com/TrustAI/FAAL Algorithm 1 Fairness-Aware Adversarial Learning Input: Training set {X, Y }, total epochs T, adversar- ial radius ϵ, step size α, the number of adversarial itera- tion K, model f parameterized by θ, the number of mini- batches M, batch size B, distribution shift constraint τ Output: A robust and fair model 1: for t = 1 . . . T do 2: for i = 1 . . . M do 3: # Phase 1: Inner maximization δ = 0 4: for j = 1 . . . K do 5: δ = δ + α · sign(∇δℓCE(fθ(xi + δ), yi)) 6: δ = max(min(δ, ϵ), −ϵ) 7: end for 8: xadv i = clip(xi + δ, 0, 1) 9: # Phase 2: Intermediate maximization ℓi = ℓCE(fθ(xadv i ), yi, reduction =‘none’) # Calculate the cross-entropy loss for each in- stance 10: for c = 1 . . . C do 11: ℓ′ c = ℓCW(fθ(xadv i , yi)[yi = c] # Calculate the average margin for each class c 12: end for 13: wadv = solve kl dro(ℓ′, τ) # Solve the optimal weights for the current batch under the worst distribution via DRO 14: LFAAL = 1 B PB i=1 wadv[y] · ℓi · C 15: # Phase 3: Outer Minimization θ = θ − ∇θLFAAL 16: end for 17: end for 18: return Robust model fθ with high fairness To provide a clearer illustration of the comprehensive learning problem, we break down the entire learning task into three distinct stages: • Phase 1: Inner maximization for finding adversarial ex- amples; • Phase 2: Intermediate maximization for finding the dis- tributionally adversarial weight (worst-case distribution around the uniform distribution); • Phase 3: Outer minimization for updating model’s pa- rameters. Phases 1 and 3 are the classic processes of conventional ad- versarial training, and Phase 2 is the core element of our proposed learning paradigm, as we assume that tackling the unknown class distributional shift can contribute to enhanc- ing robust fairness. The pseudo-code of the whole learning framework can be seen in Algorithm 1. In the following content, we replace the notion of q in Eq. (6) with wadv for convenience and define it as Class-wise Distributionally Adversarial Weight. Definition 1 (CDAW: Class-wise Distributionally Adver- sarial Weight). Given a class-wise objective loss ℓ′ c ∈ R on the adversarial examples, for all classes c ∈ C, the Class- wise Distributionally Adversarial Weight vector wadv aims to maximize the overall loss: LFAAL := max C X c=1 wadv c ℓ′ c s.t. d(U, wadv) ≤ τ, wadv ∈ ∆C (7) In the case of a reweighting strategy employed to ad- dress robust fairness, it dictates the learning trajectory for each individual class. By optimizing the model using this optimized weight, the model is exposed to learning from the worst-case distribution under the pre-defined constraint τ, such that fairness among different classes will be encour- aged. When wadv is identical to U, i.e. τ = 0, it reduces the regular mean calculation for the overall loss, making the entire learning paradigm regress to conventional adversarial training that contains Phases 1 and 3 only. We use KL di- vergence as d in the intermediate maximization, such that it can be solved via the conic convex optimization, and another elegant property of it on the generalization can be obtained, as demonstrated in Theorem 1 below. Theorem 1. Given the loss LFAAL in Definition 1 on the observed distribution, and suppose the regular loss L =: 1 C PC c=1 ℓ′ c on the test distribution with unknown group dis- tribution shift, then the following holds for all wadv ∈ W: Pr(LFAAL>L) ≥ 1 − e−τn+O(n) (8) Where KL(U, wadv) ≤ τ, U is the uniform distribution. Theorem 1 tells that the Ladv is guaranteed to be the up- per bound of L with high probability. In line with this, it en- joys a strong generalization where the performance on the test distribution with some unknown group distribution shift is at least as good as the estimated performance with high probability. So the solution of the distributionally adver- sarial weight solving by the convex optimization is optimal and will provide protection on the unknown class distribu- tion shift. As cross-entropy loss cannot well-represents how good the discrepancy between classes [9], we instead use the CW margin loss [10] as L for calculating the class-wise distributionally adversarial weight: ℓ′ c := E(x,y)∼Pc(max j!=y zj − zy) (9) where zj is the probability of the class j, i.e. the softmax output of the network. It is noted that the objective func- tions in Phases 1-3 of our learning paradigm are not nec- essarily consistent, so the proposed learning mechanism is flexible and can be combined with any min-max adversarial approaches. In the following experiments, we will mainly solve the distributional robust optimization on the bounded margin loss among classes, which provides better perfor- mance than using cross-entropy loss in the intermediate maximization. More details can be found in the Appendix. 4. Experimental Results Given our method’s focus on the robust fairness challenge, it is reasonable to assume that the model already possesses a certain degree of average robustness. Otherwise, consid- ering the robust fairness issue might not yield meaningful results. Hence, the question arises: Is it imperative to initi- ate the training of a model from the beginning for achieving fairness with a certain robustness level? In the next section, we first test our approach through adversarial fine-tuning, and then explore if training from scratch with our method offers any additional benefits. 4.1. Fine-tuning for Enhancing Robust Fairness Baselines and experiment settings: We first conducted ex- periments on CIFAR-10 dataset [24], which is popularly used for adversarial training evaluation. We use the average & worst-class accuracy under different adversarial attacks (Clean / PGD [28] / CW [10] / AutoAttack [11]) as the eval- uation metrics. The perturbation budget is set to ϵ = 8/255 on CIFAR-10 dataset. FRL [42] is the only existing state-of- the-art technique from the recent literature which performs fine-tuning to a pre-trained model for improving robust fair- ness. FRL proposed two strategies based on TRADES [44] for enhancing robust fairness, including reweight (RW) and remargin (RM). Hence we apply the best versions of FRL from their paper: FRL-RWRM with τ1 = τ2 = 0.05 and FRL-RWRM with τ1 = τ2 = 0.07, where τ1 and τ2 are the fairness constraint parameters for reweight and remargin of FRL, we name them FRL-RWRM0.05 and FRL-RWRM0.07 for short. The results of FRL are reproduced using their public code, where the target models are fine-tuned for 80 epochs and the best results are presented. In terms of the proposed method, although we utilize the PGD-AT ad- versary method by default (named FAALAT for short), it is completely compatible with other AT approaches. We found that 2 epochs of fine-tuning are enough to improve the robust fairness greatly, without sacrificing too much av- erage clean/robust accuracy. We set the value of τ in our method as 0.5, and the learning rate is configured from 0.01 in the first epoch and drops to 0.001 in the second epoch. Table 1 demonstrates our main results of finetuning Wide-Resnet34-10 (WRN-34-10) models [43] on CIFAR- 10 dataset, where different state-of-the-art adversarial de- fensed methods are adopted, including PGD-AT [28], TRADES [44], MART [38] and AWP [41]. We can see that FAALAT outperforms the two FRL methods with re- Table 1. Evaluation of different fine-tuning methods on CIFAR-10 dataset using Wide-ResNet34-10 model. The best performance is highlighted in Bold. Adversarially Trained WRN-34-10 Model Fine-Tuning Epochs Average Accuracy (Worst-class Accuracy) (%) Clean PGD-20 CW-20 AutoAttack PGD-AT - 86.07 (69.70) 55.90 (29.90) 54.29 (28.30) 52.46 (24.40) + Fine-tune with FRL-RWRM0.05 80 83.25 (74.80) 50.37 (38.10) 49.77 (36.60) 46.97 (33.10) + Fine-tune with FRL-RWRM0.07 80 85.12 (71.60) 52.56 (37.10) 51.92 (35.50) 49.60 (31.70) + Fine-tune with FAALAT 2 86.23 (69.70) 54.00 (37.60) 53.11 (36.90) 50.81 (35.70) + Fine-tune with FAALAT−AWP 2 85.47 (69.40) 56.46 (39.20) 54.50 (38.10) 52.47 (36.90) TRADES - 84.92 (67.00) 55.32 (27.10) 53.92 (24.80) 52.51 (23.20) + Fine-tune with FRL-RWRM0.05 80 82.90 (72.70) 53.16 (40.60) 51.39 (36.30) 49.97 (35.40) + Fine-tune with FRL-RWRM0.07 80 85.19 (70.90) 53.76 (39.20) 52.92 (36.80) 51.30 (34.60) + Fine-tune with FAALAT 2 85.96 (75.00) 53.46 (39.80) 52.72 (38.20) 50.91 (35.30) + Fine-tune with FAALAT−AWP 2 85.39 (72.90) 56.07 (43.30) 54.16 (38.60) 52.45 (35.40) MART - 83.62 (67.90) 56.22 (32.50) 52.79 (25.70) 50.95 (22.00) + Fine-tune with FRL-RWRM0.05 80 83.72 (71.80) 52.16 (37.50) 50.73 (35.00) 49.19 (31.70) + Fine-tune with FRL-RWRM0.07 80 82.09 (71.80) 50.86 (36.00) 49.78 (33.00) 47.78 (30.30) + Fine-tune with FAALAT 2 83.49 (68.00) 51.65 (37.80) 50.36 (37.10) 48.63 (34.00) + Fine-tune with FAALAT−AWP 2 82.17 (64.00) 54.31 (39.50) 51.72 (37.70) 50.31 (36.40) TRADES-AWP - 85.35 (67.90) 59.20 (28.80) 57.14 (26.50) 56.18 (25.80) + Fine-tune with FRL-RWRM0.05 80 82.31 (65.90) 49.90 (31.70) 49.68 (34.00) 46.50 (27.70) + Fine-tune with FRL-RWRM0.07 80 84.24 (65.70) 48.63 (30.90) 49.77 (31.50) 46.53 (28.60) + Fine-tune with FAALAT 2 87.02 (76.30) 52.54 (35.00) 51.70 (34.40) 49.87 (30.60) + Fine-tune with FAALAT−AWP 2 86.75 (74.80) 57.14(43.40) 55.34 (40.10) 53.93 (37.00) spect to both average and worst-class robustness. Notably, in the majority of cases, FAALAT achieves this without significant compromises in clean accuracy, unlike the FRL methods which tend to trade off the clean accuracy for im- proving robustness. FAALAT promotes the worst-class Au- toAttack (AA) accuracy by approximately 2.6% than FRL for fine-tuning PGD-AT, MART, and AWP. Except for fine- tuning TRADES, both methods yield comparable perfor- mance, this is due to that FRL is a TRADES-based method and it takes advantage of knowing the source method. Most importantly, FRL requires many epochs (80 epochs) to ob- tain the best results, while our method, is able to achieve better results within only 2 epochs. As adversarially train- ing a large model with high robustness is already time- consuming, to circumvent the need for retraining the model from the beginning, FAAL offers a solution for saving time and computational resources. It demonstrates the capability to quickly fine-tune a robust model that initially lacks fair- ness, resulting in a model that is both robust and fair. Due to the space limit, similar improvements on the Preact-Resnet model can be found in the Appendix. Strong adversarial attacks can help? The remargin of FRL [42] claims that increasing the perturbation margin can help for obtaining better robust fairness, while this may hurt the average clean accuracy/robustness, as indicated in Tab. 1. Certainly, there is a trade-off existing between the average robustness and worst-class robustness, but is it nec- essary to increase the perturbation margin ϵ for improving the class-wise robustness? We question whether this is a mandatory requirement for improvements, and we assume the benefits come from the stronger strength of adversar- ial perturbation. Hence, instead of enlarging the perturba- tion margin, we capitalize on the flexibility of our learning framework and integrate our method with AWP [41], a well- regarded model weight perturbation technique, to strength the attacks. As shown in Tab. 1, when combining with AWP, FAALAT−AWP further enhances the worst-class robust ac- curacy on WRN34-10 models, especially for the original one is adversarial trained with AWP. FAALAT−AWP is al- most unharmed on the improvement to the original unfair models most of the time. Therefore, it is not compul- sory to enlarge the perturbation margin to gain better re- sults, where applying a stronger adversary indeed benefits robust fairness without enlarging the perturbation margin. Figure 2 visualizes the results of class-wise AA accuracy for the comparison of the proposed method FAALAT and FAALAT−AWP, and two FRL baselines, respectively. It can be seen that FAAL clearly boost the worst-class robust ac- curacy, presenting outstanding capacity to improve robust fairness with high effectiveness and efficiency, respectively, where it outperforms FRL not only for the average/worst- class robustness but also for the very rare fine-tuning steps. Figure 2. Class-wise robust accuracy against AutoAttack after fine-tuning the PGD adversarially trained WRN model Figure 3. Class-wise robust accuracy against AutoAttack after adversarially trained PRN-18 model from scratch Table 2. Training from scratch with different methods on CIFAR- 10 dataset using Preact-ResNet18 model. Adversarially Trained PRN-18 Model Average Acc (Worst-class Acc) (%) Clean AutoAttack PGD-AT 82.72 (55.80) 47.38 (12.90) TRADES 82.54 (66.10) 49.05 (20.70) CFAAT 80.82 (64.60) 50.10 (24.40) CFATRADES 80.36 (66.20) 50.10 (26.50) WATTRADES 80.37 (66.00) 46.16 (30.70) FAALAT 82.20 (62.90) 49.10 (33.70) FAALTRADES 81.62 (68.90) 48.48 (33.60) 4.2. Training from Scratch for Enhancing Fairness Previous sections demonstrate the effectiveness and effi- ciency of the proposed approaches. Here we also investigate the advancements by training the model from the ground up using our method. We compare our methods with two common adversarial training methods (PGD-AT [28] and TRADES [44]) and two recent state-of-the-art techniques: CFA [39] and WAT [25], which have been proposed to mit- igate the robust fairness issues recently. We adversarially trained Preact-ResNet-18 models [16] for 200 epochs with a learning rate of 0.1, which will be decayed by a factor of 0.1 at 100 and 150 epochs, successively. We start to facilitate the proposed intermediate maximization (see Algorithm 1 lines 9-14) after the 100-th epoch with the only hyper- parameter τ from 0.25 and enlarge it to 0.5 after the 150-th epoch. In addition, similar to CFA using weight average, we also applied EMA [20, 37], to gain a more stable perfor- mance, however, we only applied it after the 100-th epoch, where we start to apply the intermediate maximization. We report the best results under AutoAttack on the average ac- curacy and worst-class accuracy in Tab. 2. Besides, Fig. 3 visualizes the results of different training approaches in- cluding the proposed FAALTRADES with other 3 TRADES- based models: TRADES, CFATRADES and WATTRADES respectively. We can observe that FAAL outperforms other approaches on the worst-class clean/robust accuracy, with less sacrifice on the average robustness. 4.3. Additional Experiments on CIFAR-100 dataset The experiments above mainly focused on CIFAR-10 dataset, which only has 10 classes in the dataset. In this sec- tion, we explore the proposed FAAL into a more challeng- ing dataset, i.e. CIFAR-100 with 100 categories. Similarly, we reported the results of the average/worst clean accuracy and AutoAttack accuracy. The value of τ in our method Table 3. Result comparison of different methods on CIFAR-100 dataset using ResNet18 model. Adversarially Trained RN-18 Model Average Acc (Worst-class Acc) (%) Clean AutoAttack TRADES 54.57 (19.00) 23.57 (1.00) + Fine-tune with FRL-RWRM0.05 52.55 (22.00) 21.11 (2.00) + Fine-tune with FAALAT 58.50 (21.00) 21.91 (2.00) + Fine-tune with FAALAT−AWP 58.41 (19.00) 23.44 (2.00) + Fine-tune with FAALTRADES 54.96 (18.00) 22.71 (2.00) + Fine-tune with FAALTRADES−AWP 54.90 (18.00) 23.25 (2.00) CFATRADES 55.57 (23.00) 24.56 (2.00) WATTRADES 53.99 (19.00) 22.89 (3.00) FAALAT 56.84 (16.00) 21.85 (3.00) FAALTRADES 55.87 (21.00) 23.57 (3.00) Table 4. Comparison among different SOTA methods, all mod- els are trained with the same number of samples under a single NVIDIA 3090Ti GPU in the same conda environment. Methods Training time per epoch (min) Reweighting level Adversary free Validation set CIFAR-10 (PRN-18) CIFAR-100 (RN-18) TRADES 2.63 2.68 fixed × × FRL-RWRM 2.73 2.80 epoch × ✓ WAT 2.88 3.00 epoch × ✓ CFA 2.75 2.78 epoch ✓ ✓ FAAL 2.69 2.73 batch ✓ × is set to 0.05. For fine-tuning, we compare our proposed method FAAL with FRL-RWRM0.05 [42], it can be seen that FAAL is able to achieve comparable to FRL-RWRM while reducing the amount of learning epoch up to 40 times (2 epochs vs. 80 epochs). For full adversarial training, fol- lowing the experimental settings in WAT [25], we train the ResNet-18 models for 100 epochs via different adversar- ial training approaches, where the learning rate is decayed from 0.1 to 0.01 and 0.001 at the 75-th epoch and the 90-th epoch, respectively. We compare the results of FAAL com- pared to three baselines, i.e. TRADES, CFA and WAT. It can be seen that FAATTRADES achieves the highest worst- class robust accuracy (same as WAT), meanwhile, it remains comparable results on the average robustness without sacri- ficing the average/worst-class clean accuracy. More details of the training settings can be found in the Appendix. 5. Essential Differences to SOTAs In this section, we highlight the essential differences of FAAL with existing state-of-the-art works, including FRL [42], WAT [25] and CFA [39]. Both FRL and WAT are TRADES-based approaches, which require a separate validation set for performing the reweight strategies. For example, FRL updates the lagrangian multiplier according to the performance of the validation set to meet the fairness constraints, so it requires many epochs for fine-tuning since it needs to search the whole space to achieve the optimal equilibrium without fairness constraint violation. Also, the remargin strategy in FRL essentially sacrifices some aver- age clean accuracy. We argue that it is not necessary to en- large the margin for improvement, which can be achieved by combining stronger perturbations instead. As another TRADES-based variant, WAT leverages no-regret dynam- ics and also relies on the validation set to tune the class weights for the current epoch training, Similarly, CFA pro- posed to apply the weight averaging only if the performance on the extra validation set meets a certain threshold, and re- lies on empirically adjusting the class margins and class reg- ularization based on the performance of the previous epoch. Unlike those methods that rely on historical performance or an extra validation set for manual or heuristic weight ad- justment per class in each epoch, our method bypasses these requirements. FAAL introduces an additional conic convex optimization problem after the adversarial example gener- ation, based solely on the current batch’s objective loss, the bringing solving cost is negligible. The comparison of training computation time and other key properties is illus- trated in Tab. 4. As model training can be unpredictable due to random mini-batch sampling, causing quick shifts in class distribution and bias that may differ from previous epochs or validations More importantly, FAAL can gener- alize to any adversarial training methods, as our interme- diate maximization is a completely dependent component of the popular min-max framework, so it is not limited to any adversaries, unlike some methods FRL and WAT that are restricted to TRADES. Our data-driven component en- hances flexibility in managing the balance between average robustness and robust fairness during adversarial training, and demonstrates its potential in handling various distribu- tion shifts for the current batch. 6. Conclusion In conclusion, we establish a connection between robust fairness and group distribution shift overfitting and present a new fairness-aware adversarial learning paradigm to ad- dress robust fairness. Compared to state-of-the-art meth- ods, extensive experiments on CIFAR-10 and CIFAR-100 datasets demonstrate the effectiveness and superior effi- ciency of the proposed approach. Notably, by just two epochs of fine-tuning, our training strategy can transform a biased robust model into one with high fairness with lit- tle cost for average accuracy. We believe our research pro- vides a meaningful contribution to the discourse on robust- ness and fairness in machine learning, deepening our insight into the model’s behaviors under adversarial settings. Acknowledgement The research is supported by the UK EPSRC under project EnnCORE [EP/T026995/1]. References [1] Alekh Agarwal, Alina Beygelzimer, Miroslav Dud´ık, John Langford, and Hanna Wallach. A reductions approach to fair classification. In International Conference on Machine Learning, pages 60–69. PMLR, 2018. 2 [2] Aharon Ben-Tal, Laurent El Ghaoui, and Arkadi Ne- mirovski. Robust optimization. Princeton university press, 2009. 3 [3] Aharon Ben-Tal, Dick Den Hertog, Anja De Waegenaere, Bertrand Melenberg, and Gijs Rennen. Robust solutions of optimization problems affected by uncertain probabilities. Management Science, 59(2):341–357, 2013. 2, 3 [4] Amine Bennouna and Bart Van Parys. Holistic robust data- driven decisions. arXiv preprint arXiv:2207.09560, 2022. 3 [5] Amine Bennouna, Ryan Lucas, and Bart Van Parys. Certi- fied robust neural networks: Generalization and corruption resistance. arXiv preprint arXiv:2303.02251, 2023. 3, 4 [6] Philipp Benz, Chaoning Zhang, Adil Karjauv, and In So Kweon. Robustness may be at odds with fairness: An em- pirical study on class-wise accuracy. In NeurIPS 2020 Work- shop on Pre-registration in Machine Learning, pages 325– 342. PMLR, 2021. 3 [7] Dimitris Bertsimas, David B Brown, and Constantine Cara- manis. Theory and applications of robust optimization. SIAM review, 53(3):464–501, 2011. 3 [8] Tuan Anh Bui, Trung Le, Quan Tran, He Zhao, and Dinh Phung. A unified wasserstein distributional robust- ness framework for adversarial training. arXiv preprint arXiv:2202.13437, 2022. 3 [9] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label- distribution-aware margin loss. Advances in neural informa- tion processing systems, 32, 2019. 5 [10] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39–57. Ieee, 2017. 5 [11] Francesco Croce and Matthias Hein. Reliable evalua- tion of adversarial robustness with an ensemble of diverse parameter-free attacks. In International conference on ma- chine learning, pages 2206–2216. PMLR, 2020. 1, 5 [12] John C Duchi, Peter W Glynn, and Hongseok Namkoong. Statistics of robust optimization: A generalized empirical likelihood approach. Mathematics of Operations Research, 46(3):946–969, 2021. 2, 3 [13] Julien Ferry, Ulrich Aivodji, S´ebastien Gambs, Marie-Jos´e Huguet, and Mohamed Siala. Improving fairness generaliza- tion through a sample-robust optimization method. Machine Learning, pages 1–62, 2022. 3 [14] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014. 1 [15] Tatsunori Hashimoto, Megha Srivastava, Hongseok Namkoong, and Percy Liang. Fairness without demo- graphics in repeated loss minimization. In International Conference on Machine Learning, pages 1929–1938. PMLR, 2018. 2, 3 [16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 7 [17] Weihua Hu, Gang Niu, Issei Sato, and Masashi Sugiyama. Does distributionally robust supervised learning give robust classifiers? In International Conference on Machine Learn- ing, pages 2029–2037. PMLR, 2018. 3 [18] Xiaowei Huang, Daniel Kroening, Wenjie Ruan, James Sharp, Youcheng Sun, Emese Thamo, Min Wu, and Xinping Yi. A survey of safety and trustworthiness of deep neural net- works: Verification, testing, adversarial attack and defence, and interpretability. Computer Science Review, 37:100270, 2020. 1 [19] Xiaowei Huang, Wenjie Ruan, Wei Huang, Gaojie Jin, Yi Dong, Changshun Wu, Saddek Bensalem, Ronghui Mu, Yi Qi, Xingyu Zhao, et al. A survey of safety and trustworthi- ness of large language models through the lens of verification and validation. arXiv preprint arXiv:2305.11391, 2023. 1 [20] Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry Vetrov, and Andrew Gordon Wilson. Averaging weights leads to wider optima and better generalization. arXiv preprint arXiv:1803.05407, 2018. 7 [21] Gaojie Jin, Xinping Yi, Wei Huang, Sven Schewe, and Xi- aowei Huang. Enhancing adversarial training with second- order statistics of weights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15273–15283, 2022. 1 [22] Gaojie Jin, Xinping Yi, Dengyu Wu, Ronghui Mu, and Xi- aowei Huang. Randomized adversarial training via taylor expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16447– 16457, 2023. 1 [23] Sangwon Jung, Taeeon Park, Sanghyuk Chun, and Taesup Moon. Re-weighting based group fairness regularization via classwise robust optimization. In The Eleventh International Conference on Learning Representations, 2022. 3 [24] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 5 [25] Boqi Li and Weiwei Liu. Wat: improve the worst-class ro- bustness in adversarial training. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 14982–14990, 2023. 2, 3, 7, 8 [26] Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, and Bo Li. Distributionally robust learning with sta- ble adversarial training. IEEE Transactions on Knowledge and Data Engineering, 2022. 3 [27] Xinsong Ma, Zekai Wang, and Weiwei Liu. On the trade- off between robustness and fairness. In Advances in Neural Information Processing Systems, 2022. 2 [28] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learn- ing models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017. 1, 3, 5, 7 [29] Yonatan Oren, Shiori Sagawa, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust language modeling. arXiv preprint arXiv:1909.02060, 2019. 3 [30] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015. 1 [31] Shiori Sagawa, Pang Wei Koh, Tatsunori B Hashimoto, and Percy Liang. Distributionally robust neural networks for group shifts: On the importance of regularization for worst- case generalization. arXiv preprint arXiv:1911.08731, 2019. 3, 4 [32] Aman Sinha, Hongseok Namkoong, and John Duchi. Certi- fying some distributional robustness with principled adver- sarial training. In International Conference on Learning Representations, 2018. 3 [33] Matthew Staib and Stefanie Jegelka. Distributionally robust deep learning as a generalization of adversarial training. In NIPS workshop on Machine Learning and Computer Secu- rity, page 4, 2017. 3 [34] Chunyu Sun, Chenye Xu, Chengyuan Yao, Siyuan Liang, Yichao Wu, Ding Liang, XiangLong Liu, and Aishan Liu. Improving robust fairness via balance adversarial training. arXiv preprint arXiv:2209.07534, 2022. 2, 3 [35] Hieu Vu, Toan Tran, Man-Chung Yue, and Viet Anh Nguyen. Distributionally robust fair principal components via geodesic descents. In International Conference on Learning Representations, 2021. 3 [36] Fu Wang, Yanghao Zhang, Yanbin Zheng, and Wenjie Ruan. Dynamic efficient adversarial training guided by gradient magnitude. In Progress and Challenges in Building Trust- worthy Embodied AI, 2022. 1 [37] Hongjun Wang and Yisen Wang. Self-ensemble adver- sarial training for improved robustness. arXiv preprint arXiv:2203.09678, 2022. 7 [38] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In International conference on learning representations, 2019. 5 [39] Zeming Wei, Yifei Wang, Yiwen Guo, and Yisen Wang. Cfa: Class-wise calibrated fair adversarial training. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8193–8201, 2023. 2, 3, 7, 8 [40] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chau- mond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R´emi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceed- ings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38–45, 2020. 1 [41] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. Advances in Neural Information Processing Systems, 33:2958–2969, 2020. 5, 6 [42] Han Xu, Xiaorui Liu, Yaxin Li, Anil Jain, and Jiliang Tang. To be robust or to be fair: Towards fairness in adversarial training. In International Conference on Machine Learning, pages 11492–11501. PMLR, 2021. 1, 2, 3, 5, 6, 8 [43] Sergey Zagoruyko and Nikos Komodakis. Wide residual net- works. arXiv preprint arXiv:1605.07146, 2016. 1, 5 [44] Hongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Lau- rent El Ghaoui, and Michael Jordan. Theoretically principled trade-off between robustness and accuracy. In International conference on machine learning, pages 7472–7482. PMLR, 2019. 5, 7 [45] Yanghao Zhang, Wenjie Ruan, Fu Wang, and Xiaowei Huang. Generalizing universal adversarial perturbations for deep neural networks. Machine Learning, 112(5):1597– 1626, 2023. 1 "
}