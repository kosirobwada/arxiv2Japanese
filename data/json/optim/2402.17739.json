{
    "optim": "reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use\nSusobhan Ghosh1 , Yongyi Guo2 , Pei-Yao Hung3 , Lara Coughlin4 , Erin Bonar4 ,\nInbal Nahum-Shani3 , Maureen Walton4 , Susan Murphy1\n1Department of Computer Science, Harvard University\n2Department of Statistics, University of Wisconsin-Madison\n3Institute for Social Research, University of Michigan\n4Department of Psychiatry, University of Michigan\nsusobhan ghosh@g.harvard.edu, guo98@wisc.edu, peiyaoh@umich.edu, laraco@med.umich.edu, erinbona@med.umich.edu,\ninbal@umich.edu, waltonma@med.umich.edu, yongyiguo@fas.harvard.edu\nAbstract\nThe escalating prevalence of cannabis use, and as-\nsociated cannabis-use disorder (CUD), poses a sig-\nnificant public health challenge globally.\nWith\na notably wide treatment gap, especially among\nemerging adults (EAs; ages 18-25), addressing\ncannabis use and CUD remains a pivotal objec-\ntive within the 2030 United Nations Agenda for\nSustainable Development Goals (SDG). In this\nwork, we develop an online reinforcement learn-\ning (RL) algorithm called reBandit which will be\nutilized in a mobile health study to deliver per-\nsonalized mobile health interventions aimed at re-\nducing cannabis use among EAs. reBandit utilizes\nrandom effects and informative Bayesian priors to\nlearn quickly and efficiently in noisy mobile health\nenvironments. Moreover, reBandit employs Em-\npirical Bayes and optimization techniques to au-\ntonomously update its hyper-parameters online. To\nevaluate the performance of our algorithm, we con-\nstruct a simulation testbed using data from a prior\nstudy, and compare against commonly used algo-\nrithms in mobile health studies.\nWe show that\nreBandit performs equally well or better than all\nthe baseline algorithms, and the performance gap\nwidens as population heterogeneity increases in the\nsimulation environment, proving its adeptness to\nadapt to diverse population of study participants.\n1\nIntroduction & Motivation\nAddressing at-risk substance use, including cannabis use, is\na pivotal objective within the 2030 UN Agenda for Sustain-\nable Development Goals (SDG)1. Within this agenda, SDG\n3 focuses on ensuring healthy lives and well-being across the\nlifespan, yet, increasing use of cannabis, third in global preva-\nlence after alcohol and nicotine, threatens this goal [Pea-\ncock et al., 2018].\nHence, as highlighted in target 3.5 of\nthe agenda, strengthening the prevention and treatment of\ncannabis use and cannabis use disorder (CUD) is crucial. Un-\nfortunately, this coincides with a decreased public perception\nof the risks associated with cannabis use, likely influenced\n1https://sdgs.un.org/2030agenda\nby ongoing decriminalization efforts and greater access to\ncannabis products [Carliner et al., 2017], further worsened\nby one of the largest treatment gaps of any medical condi-\ntion, with one study showing only 5% of those with CUD\nreceiving treatment [Lapham et al., 2019].\nIn the US, the prevalence of cannabis use is highest among\nemerging adults (EAs; age 18-25) [SAMHSA, 2023], mark-\ning it as a significant concern within the growing landscape of\ncannabis use. Particularly worrisome is the fact that early ini-\ntiation of cannabis use links to an array of physical and mental\nhealth repercussions, as well as escalated risk for developing\nCUD [Volkow et al., 2014; Hall, 2009; Chan et al., 2021;\nHasin et al., 2016]. Given that cannabis use frequently com-\nmences during adolescence and peaks in emerging adulthood,\nthis is a critical developmental period for early intervention\nstrategies to prevent transitions into CUD.\nMobile health technologies, such as health apps and sen-\nsors, can potentially serve as support tools to help individuals\nmanage their cannabis use. Using these tools, individuals can\ntrack their cannabis consumption, receive personalized inter-\nventions, and provide objective data for early detection of is-\nsues. These technologies enable the delivery of just-in-time\nadaptive interventions (JITAIs) [Nahum-Shani et al., 2018],\nwhich leverage rapidly changing information about a person’s\nstate and context to decide whether and how to intervene in\ndaily life. JITAIs have been successful for many domains of\nbehavioral health [Jaimes et al., 2015; Clarke et al., 2017;\nGolbus et al., 2021], whilst JITAIs for cannabis use among\nEAs are currently lacking evidence despite promising early\ndata [Shrier et al., 2018].\nIn this work, we develop an RL algorithm called reBan-\ndit which will be utilized in the MiWaves pilot study (Sec-\ntion 1.1). MiWaves focuses on developing a JITAI for reduc-\ning cannabis use among emerging adults (EAs) (ages 18-25).\nThis JITAI leverages reBandit to determine the likelihood of\ndelivering an intervention message.\n1.1\nMiWaves pilot study\nThe MiWaves pilot study focuses on developing a personal-\nizing Just-In-Time Adaptive Intervention (pJITAI), namely a\nJITAI that integrates an RL algorithm. In this study, EAs are\nrandomized to receive a mobile-based intervention message\nor no message, twice daily. The RL algorithm is designed\nto learn from a participant’s history, and personalize the like-\narXiv:2402.17739v1  [cs.AI]  27 Feb 2024\nlihood of intervention delivery based on a participant’s cur-\nrent context. By combining technology, behavioral science,\nand data-driven decision-making, MiWaves aims to empower\nemerging adults with the digital tools to help reduce their\ncannabis use. The MiWaves pilot study has been registered\non ClinicalTrials.gov (NCT05824754), and is scheduled to\nstart in March 2024. Figure 1 provides a visual overview of\nthe MiWaves pilot study.\n1.2\nChallenges, Contributions and Overview\nDeploying RL algorithms in mHealth studies like MiWaves\npresent a multitude of challenges that must be addressed,\nwhich include:\nC1 Limited Data: Many sequential decision making prob-\nlems in mHealth involve scarce data, forcing RL algo-\nrithms to learn and perform well under strict data con-\nstraints [Trella et al., 2022].\nC2 After-study analysis and evaluation: The RL algo-\nrithms deployed in mHealth studies need to be devel-\noped in a way to facilitate after-study analysis and off-\npolicy evaluation.\nC3 Autonomy and Stability: The intervention protocol in\nclinical studies is pre-specified. Since the RL algorithm\nis part of the intervention, scientists do not have the flex-\nibility to change the RL algorithm while the study is run-\nning. RL algorithms must exhibit robustness in the face\nof noisy data, ensuring consistent and reliable perfor-\nmance throughout the study [Trella et al., 2022].\nC4 Explainability: It is imperative that RL algorithms are\ninterpretable and comprehensible to behavioral scien-\ntists and medical professionals to enhance their ability\nto critique RL performance and to enhance the possibil-\nity of larger scale implementation.\nC5 Delayed Effects: In mobile health studies, each inter-\nvention message sent to the user has a delayed effect.\nUsers may perceive burden upon receiving an interven-\ntion message, which influences their future behavior.\nC6 Reproducibility: Any algorithm used as part of the in-\ntervention in a clinical study needs to be reproducible in\norder for health scientists to evaluate and implement the\nintervention package in practice. Hence, the decisions\ntaken by the RL algorithm must be reproducible, allow-\ning for scrutiny and verification of its effectiveness.\nTo that end, we introduce reBandit, an online RL algorithm\nwhich utilizes random effects to address the challenges men-\ntioned above. When used as part of an RL algorithm, random\neffects allow the algorithm to learn quickly and efficiently by\nmaking use of other participant’s data in the population while\nsimultaneously personalizing treatment for a given partici-\npant. Moreover, reBandit employs an informative Bayesian\nprior formulated from pre-existing data to act as a warm-\nstart.\nCarefully designed priors incorporate previous (do-\nmain) knowledge, which help algorithms learn quickly and\nefficiently. Both random effects and informative priors can\nhelp reBandit to tackle challenge C1.\nThe most commonly used RL algorithms in mHealth set-\ntings are bandit algorithms. In mHealth settings, predictions\nof the value of next state (eg. using [Jiang et al., 2015]) can\nbe very noisy. Bandit algorithms are, thus, preferred due to\ntheir performance in such noisy environments.\nMoreover,\nthey are computationally less complex, and hence are able\nto run stably and reliably in an online environment. Further,\nlinear models are often considered interpretable due to their\nsimplicity of representing the role of various factors, and can\nalso be stably updated. reBandit utilizes both these concepts\n- it uses a bandit framework, along with a linear model (with\nrandom effects) to model the reward. We derive the formula\nto update reBandit’s parameters and hyper-parameters online\n(Sec 4.1). We show that we are able to autonomously update\nthese parameters and hyper-parameters within a reasonable\ntime-limit in an online environment. Moreover, to facilitate\nafter-study analysis, we utilize a smooth variant of posterior\nsampling, and clip the probabilities of taking an action (Sec\n4.2). This way, reBandit is able to overcome challenges C2,\nC3 and C4.\nTo address delayed effects (C5), one can use RL algorithms\nto model a full Markov Decision Process (MDP). However, in\nmobile health settings, such approaches are not feasible due\nto limited data and noisy outcomes [Trella et al., 2023]. On\nthe other hand, the classical bandit framework alone is also\ninsufficient, as it is designed to optimize for immediate re-\nward, and thus, cannot account for the delayed effects of ac-\ntions. To that end, we engineer the reward used to update re-\nBanditparameters and hyper-parameters (Sec 4.3), to account\nfor delayed effects of actions.\nFinally, to tackle challenge C6, we have made our imple-\nmentation of reBandit publicly available2. To ensure repro-\nducibility, we employ a seeded pseudo-random number gen-\nerator to make every stochastic decision in reBandit. Addi-\ntionally, all intermediate results and values used to make deci-\nsions are programatically stored in a database for reproducing\nthe results obtained during any run of the algorithm.\n2\nRL Framework and Notation\nThis section provides a brief overview of the Reinforcement\nLearning (RL) [Sutton and Barto, 2018] setup used in this\nwork, and the specifics of the RL setup with respect to the\nMiWaves pilot study.\nWe approximate the pilot study environment as a bandit\nenvironment. We represent it as a Markov Decision Process\n(MDP) where in the RL algorithm (eg. the mobile app) in-\nteracts with the environment (eg. the user). The MDP is\nspecified by the tuple ⟨S, A, r, P, T⟩, where S is the state-\nspace of the algorithm, A is the action-space, r(s, a) is the\nreward function defined for a given state s ∈ S and action\na ∈ A, P(s, a, s′) is the transition function for a given state\ns ∈ S, action a ∈ A and next state s′ ∈ S, and T is the\ntotal number of decision times. A user trajectory is given by\nH(T +1)\ni\n= {S(t), a(t), R(t)}T\nt=1, where S(t) denotes the state\nat decision time t, a(t) the action assigned by the RL algo-\nrithm at time t, and R(t) the reward collected after selection\nof the action. In the case of MiWaves we have:\n2https://github.com/StatisticalReinforcementLearningLab/\nmiwaves rl service\nFigure 1: Summary of the MiWaves pilot study. m = 120 EAs are expected to be recruited through social media ads. Each EA will be in\nthe trial for 30 days, and will be asked to self-report twice daily - once in the morning and once in the evening. Upon completion or time\nexpiration of the self-reporting, the RL algorithm will decide whether to send or not send an intervention message.\nActions: Binary action space, i.e. A = {0, 1} - to not send\n(0) or to send (1) an intervention message.\nDecision points: T decision points per user. The study is set\nto run for D = 30 days, and each day is supposed to have\n2 decision points per day. Therefore, we expect to have 60\ndecision points per user, i.e. T = 60.\nReward: We denote the reward for user i at decision time t\nby R(t)\ni . For the MiWaves pilot study, we have discrete re-\nwards {0, 1, 2, 3}, which increase linearly with user engage-\nment. We utilize engagement as our reward because engage-\nment is critical to assess effectiveness of interventions after\nthe study is over [Nahum-Shani et al., 2022].\nStates: Let us denote the state observation of the user i at\ndecision time t as S(t)\ni . A given state S = (S1, S2, S3) is de-\nfined as a 3-tuple of the following binary variables (omitting\nthe user and time index for brevity):\n• S1: Recent engagement - set to 1 if the average of past\n3 observed rewards is greater than or equal to 2 (high\nengagement), and set to 0 otherwise (low engagement).\nAt decision point t = 1, we set S1 to 0, as there is no\nengagement by the user at the start of the pilot study.\n• S2: Time of day of the decision point - Morning (0) vs.\nEvening (1).\n• S3: Recent cannabis use - set to 0 if the participant re-\nported using cannabis during their self-monitoring, and\n1 otherwise. If the user fails to self-report, we set S3 to\nbe 0. We do so because we expect the participant in the\nMiWaves pilot study to be using cannabis regularly (at\nleast 3 times a week).\nOverall, we represent the favorable states as 1 (not using\ncannabis, high engagement), and the unfavorable states as 0\n(using cannabis, low engagement).\nNumber of users: We expect m = 120 users to participate\nduring the RL-powered MiWaves pilot study.\n3\nRelated Work\nRandom effects (and mixed-effects) models have been well-\nstudied in the statistical literature [Laird, 2004; Laird and\nWare, 1982; Robinson, 1991], mainly in the context of batch\ndata analysis. Mixed-effects models comprise of fixed and\nrandom effects - hence termed mixed effects. Laird and Ware\nintroduce the notion of random-effects models for longitu-\ndinal data, and describe an unified approach to fitting such\nmodels using empirical Bayes and maximum likelihood esti-\nmation using EM algorithm. Our work (which is in context\nof streaming / real-time data) draws inspiration from Laird\nand Ware to extend random-effects based models to real-time\ndecision making through RL in sequential decision making\nproblems.\nThere has been a myriad of works in optimizing inter-\nvention delivery in mHealth settings in recent years [Gol-\nbus et al., 2021; Kramer et al., 2019; Martin et al., 2018;\nRabbi et al., 2019; Trella et al., 2022; Walsh and Groarke,\n2019]. Bandit algorithms are the most commonly used RL\nalgorithms used in such high stakes online settings [Lang-\nford and Zhang, 2007; Tewari and Murphy, 2017; Wang et\nal., 2005] due to their simplicity and stability, and ability\nto perform in noisy environments.\nSuch algorithms have\nmainly used one of two approaches. The first approach is per-\nson specific (a.k.a. fully personalized) [Forman et al., 2019;\nJaimes et al., 2015; Liao et al., 2019; Rabbi et al., 2015]\nwhere a separate model is deployed for each user in the trial.\nThis approach is suitable when the population of users are\nhighly heterogeneous, but suffers greatly when data is scarce\nand/or noisy.\nNote that fully personalized approaches are\nnot feasible for the MiWaves pilot study, as it runs for only\n30 days (due to scarce data).\nThe second approach com-\npletely pools data (a.k.a. fully pooled) across all users in\nthe population [Clarke et al., 2017; Paredes et al., 2014;\nTrella et al., 2022; Yom-Tov et al., 2017; Zhou et al., 2018].\nOur algorithm, reBandit, strikes a balance between the two\napproaches - it adaptively pools data across users depend-\ning on the degree of heterogeneity in the population.\n4.1\ndescribes in detail as to how we achieve that balance using\nrandom effects.\nTomkins et al. also use random effects in their Thompson-\nSampling [Russo and Van Roy, 2014; Thompson, 1933] con-\ntextual bandit algorithm [Li et al., 2010], IntelligentPool-\ning [Tomkins et al., 2021].\nIntelligentPooling updates its\nhyper-parameters by modeling the problem as a Gaussian\nProcess (GP). However, IntelligentPooling fails to run au-\ntonomously and stably in an online environment (does not\novercome C3)3. Here we deal with this problem by updat-\ning the hyper-parameters in reBandit using empirical Bayes\n(similar to [Laird and Ware, 1982]), and solve the optimiza-\ntion problem using projected gradient descent. reBandit runs\nautonomously and stably in an online environment while also\nhaving more users (12x) and more features (8x) as compared\nto the environment described in IntelligentPooling.\nRecently, various approaches have been explored regard-\ning the application of mixed effects models within a bandit\nframework [Zhu and Kveton, 2022a; Zhu and Kveton, 2022b;\nAouali et al., 2023]. However, these works primarily focus\non utilizing mixed effects to capture the dependence and het-\nerogeneity of rewards associated with different actions, rather\nthan addressing the similarity and heterogeneity among mul-\ntiple users. For example, [Zhu and Kveton, 2022a] consider a\n(non-contextual) multi-arm bandit problem, where the agent\nchooses one of the K arms at each time t with the goal of\nmaximizing cumulative reward. The authors assume that the\nreward for the arms are correlated with each other and can\nbe expressed using a mixed-effects model, so that pulling an\narm gives some information about the reward of other arms as\nwell. A follow up work, [Zhu and Kveton, 2022b], adds con-\ntext into the reward model, and results in a linear mixed ef-\nfects model for the rewards of the arms. [Aouali et al., 2023]\nfurther generalizes the above works to a non-linear reward\nsetting. Our approach diverges from these studies by utiliz-\ning mixed-effects to model user similarity and heterogeneity,\nwhile making decisions for each user at each time point.\nIn the broader RL literature, there has been much work\non Thompson Sampling based bandit algorithms [Basu et\nal., 2021; Hong et al., 2022], especially in connection to\nmulti-task learning and meta learning [Peleg et al., 2022;\nSimchowitz et al., 2021; Wan et al., 2021; Wan et al., 2023].\nThe multi-task learning based approaches quantify the sim-\nilarity between arms and/or users from their policies - the\nextent to which one user’s data influences or contributes to\nanother user’s policy is a function of some similarity mea-\nsure. reBandit can be connected to multi-task learning, as\nit learns across multiple users (or tasks), and tries to max-\nimize rewards across all users (or tasks). However, due to\nits unique application in mobile health, reBandit adopts a\ndistinct set of assumptions on the structure of the similarity\nmeasures in comparison to the works mentioned above. The\nmeta-learning based approaches exploit the underlying struc-\nture of similar tasks to improve performance on new (or un-\nseen, but similar) tasks. While reBandit can be viewed as a\nform of meta-learning, where shared population parameters\nare learnt across users (or tasks), and user-specific parame-\nters are learnt to personalize to tasks, reBandit does not try to\nimprove performance on new or unseen users.\n4\nBandit algorithm: reBandit\nThis section details details the reBandit algorithm used in the\nMiWaves pilot study. Being an online RL algorithm, reBan-\ndit has two major components: (i) the online learning algo-\nrithm; and (ii) the action-selection procedure. Going forward,\n3We were unable to run their code published on GitHub, and\nunable to parse the code or replicate it due to poor documentation\nwe describe reBandit’s online learning algorithm in Section\n4.1, and it’s posterior sampling based action selection strat-\negy in Section 4.2. Finally, to address delayed effects, we de-\nscribe its reward engineering procedure in Section 4.3. The\nreBandit algorithm is summarized in Algorithm 1.\n4.1\nOnline Learning Algorithm\nThis section details the online learning algorithm - specifi-\ncally the algorithm’s reward approximating function and its\nmodel update procedure.\nReward Approximating Function\nOne of the key components of the online learning algorithm is\nits reward approximation function, through which it models\nthe participant’s reward. Recall that the reward function is the\nconditional mean of the reward given state and action. We\nchose a Bayesian Mixed Linear Model to model the reward.\nMixed models allow the RL algorithm to adaptively pool and\nlearn across users while simultaneously personalizing actions\nfor each user.\nLet us assume that for a given user i at decision time t, the\nRL algorithm receives the reward R(t)\ni\nafter taking action a(t)\ni\nThen, the reward model is written as:\nR(t)\ni\n= g(S(t)\ni\n)T αi + a(t)\ni f(S(t)\ni\n)T βi + ϵ(t)\ni\n(1)\nwhere ϵ(t)\ni\nis the noise, assumed to be gaussian i.e.\nϵ ∼\nN(0, σ2\nϵ Imt), and m is the total number of users who have\nbeen or are currently part of the study. Also αi, βi, and γi\nare weights that the algorithm wants to learn. g(S) and f(S)\nare functions of the RL state defined in Section 2. To enhance\nrobustness to misspecification of the baseline reward model\nwhen a(t)\ni\n= 0, g(S(t)\ni\n)T αi, we utilize action-centering\n[Greenewald et al., 2017] to learn an over-parameterized ver-\nsion of the above reward model:\nR(t)\ni\n= g(S(t)\ni\n)T αi + (a(t)\ni\n− π(t)\ni )f(S(t)\ni\n)T βi\n+ (π(t)\ni )f(S(t)\ni\n)T γi + ϵ(t)\ni\n(2)\nwhere π(t)\ni\nis the probability of taking action a(t)\ni\n= 1 in state\nS(t)\ni\nfor participant i at decision time t. We refer to the term\ng(S(t)\ni\n)T αi as the baseline, and f(S(t)\ni\n)T βi as the advan-\ntage (i.e. the advantage of taking action 1 over action 0).\nWe re-write the reward model as follows:\nR(t)\ni\n= ΦT\nitθi + ϵi,t\n(3)\nwhere ΦT\nit = Φ(S(t)\ni\n, a(t)\ni , π(t)\ni )T\n= [g(S(t)\ni\n)T , (a(t)\ni\n−\nπ(t)\ni )f(S(t)\ni\n)T , (π(t)\ni )f(S(t)\ni\n)T ] is the design matrix for given\nstate and action, and θi = [αi, βi, γi]T is the joint weight\nvector that the algorithm wants to learn. We further break\ndown the joint weight vector θi into two components:\nθi =\n\"αi\nβi\nγi\n#\n=\n\n\nαpop + uα,i\nβpop + uβ,i\nγpop + uγ,i\n\n = θpop + ui\n(4)\nHere, θpop\n= [αpop, βpop, γpop]T is the population level\nterm which is common across all the user’s reward mod-\nels and follows a normal prior distribution given by θpop ∼\nN(µprior, Σprior). On the other hand, ui = [uα,i, uβ,i, uγ,i]T\nare the individual level parameters, or the random effects, for\nany given user i. Note that the individual level parameters\nare assumed to be normal by definition, i.e. ui ∼ N(0, Σu),\nand independent of ϵi. Please refer to Appendix D for the\nprior calculation and initialization values.\nOnline model update procedure\nPosterior Update: We vectorize the terms across the m users\nin a study, and re-write the model as:\nR = ΦT θ + ϵ\n(5)\nRi =\nh\nR(1)\ni\n. . . R(t)\ni\niT\n(6)\nR =\n\u0002\nRT\n1 . . . RT\nm\n\u0003T\n(7)\nθ =\n\u0002\nθT\n1 . . . θT\nm\n\u0003T = 1m ⊗ θpop + u\n(8)\nu =\n\u0002\nuT\n1 . . . uT\nm\n\u0003T\n(9)\nϵi = [ϵi,1 . . . ϵi,t]T\n(10)\nϵ =\n\u0002\nϵT\n1 . . . ϵT\nm\n\u0003T\n(11)\nui ∼ N(0, Σu)\n(12)\nϵ ∼ N(0, σ2\nϵ Imt)\n(13)\nAs specified before, we assume a gaussian prior on the\npopulation level term θpop ∼ N(µprior, Σprior). The hyper-\nparameters of the above model, given the definition above,\nare the noise variance σ2\nϵ and the random effects variance Σu.\nNow, at a given decision point t, using estimated values of the\nhyper-parameters (σ2\nϵ,t is the estimate of σ2\nϵ and Σu,t is the\nestimate of Σu), the posterior mean and covariance matrix of\nthe parameter θ can be calculated as:\nµ(t)\npost =\n\u0000˜Σ−1\nθ,t + σ−2\nϵ,t A\n\u0001−1\u0000˜Σ−1\nθ,tµθ + σ−2\nϵ,t B\n\u0001\n(14)\nΣ(t)\npost =\n\u0000˜Σ−1\nθ,t + σ−2\nϵ,t A\n\u0001−1\n(15)\nwhere\nA = BlockDiag\n\u0000A1, . . . , Am\n\u0001\n(16)\nAi =\nXt\nτ=1 ΦiτΦT\niτ\n(17)\nBT = [BT\n1 . . . BT\nm]\n(18)\nBi =\nXt\nτ=1 ΦiτR(τ)\ni\n(19)\nµT\nθ = [µprior\nT . . . µprior\nT ]\n(20)\n˜Σθ,t = Im ⊗ Σu,t + Jm ⊗ Σprior\n(21)\nThe action-selection procedure (described in Section\n4.2) uses the Gaussian posterior distribution defined by the\nposterior mean µ(t)\npost and variance Σ(t)\npost to determine the\naction selection probability π(t+1) and the corresponding\nactions for the next time steps.\nHyper-parameter update: The hyper-parameters in the al-\ngorithm’s reward model are the noise variance σ2\nϵ and random\neffects variance Σu. In order to update these variance esti-\nmates at the end of decision time t, we use Empirical Bayes\n[Morris, 1983] to maximize the marginal likelihood of ob-\nserved rewards, marginalized over the parameters θ. So, in\norder to form Σu,t and σ2\nϵ,t, we solve the following optimiza-\ntion problem:\nΣu,t, σ2\nϵ,t = argmax l(Σu,t, σ2\nϵ,t; H)\n(22)\ns.t.\nΣu,t ≻ 0\n(23)\nσ2\nϵ,t ≥ 0\n(24)\nwhere,\nl(Σu,t, σ2\nϵ,t; H) = log(det(X)) − log(det(X + yA))\n+ mt log(y) − y\nX\nτ∈[t]\nX\ni∈[m](R(τ)\ni\n)2 − µT\nθ Xµθ\n+ (Xµθ + yB)T (X + yA)−1(Xµθ + yB)\n(25)\nNote that, X = ˜Σ−1\nθ,t (see Eq. 21) and y =\n1\nσ2\nϵ,t . We solve\nthe optimization problem using gradient descent.\n4.2\nAction selection procedure\nThe action selection procedure utilizes a modified posterior\nsampling algorithm called the smooth posterior sampling al-\ngorithm. Recall from Section 4.1, our model for the reward\nis a Bayesian linear mixed model with action centering (refer\nEq. 2) where π(t)\ni\nis the probability that the RL algorithm se-\nlects action a(t)\ni\n= 1 in state S(t)\ni\nfor participant i at decision\npoint t. The RL algorithm computes the probability π(t)\ni\nas\nfollows:\nπ(t)\ni\n= E ˜β∼N (µ(t−1)\npost,i ,Σ(t−1)\npost,i )[ρ(f(S(t)\ni\n)T ˜β)|H(t−1)\n1:m , S(t)\ni\n]\n(26)\nNotice that the last expectation above is over the draw of β\nfrom the posterior distribution parameterized by µ(t−1)\npost,i and\nΣ(t−1)\npost,i (see Eq. 14 and Eq. 15 for their definitions).\nClassical posterior sampling sets ρ(x) = I(x > 0). In\nthis case, the posterior sampling algorithm sets randomization\nprobabilities to the posterior probability that the treatment ef-\nfect is positive. However, when using a pooled algorithm,\nZhang et al. showed that between study statistical inference\nis enhanced if ρ is a smooth i.e. continuously differentiable\nfunction. Using a smooth function ensures that the random-\nization probabilities formed by the algorithm concentrate.\nConcentration enhances the replicability of the randomiza-\ntion probabilities if the study is repeated. Without concentra-\ntion, the randomization probabilities might fluctuate greatly\nbetween repetitions of the study [Deshpande et al., 2018;\nKalvit and Zeevi, 2021; Zhang et al., 2022]. In MiWaves ,\nwe choose ρ to be a generalized logistic function, defined as\nfollows:\nρ(x) = Lmin +\nLmax − Lmin\n1 + c exp(−bx)\n(27)\nwhere c = 5, and b = 21.053 (please refer to Appendix E for\nmore details). We set the lower and upper clipping probabil-\nities as Lmin = 0.2 and Lmax = 0.8 (i.e., 0.2 ≤ π(t)\ni\n≤ 0.8).\nThe probabilities are clipped to facilitate after-study analysis\nand off-policy evaluation [Zhang et al., 2022].\nAlgorithm 1: reBandit\nInput : m, D, µ(0)\npost = µprior, Σ(0)\npost = Σprior, Σu,0, σ2\nϵ,0,\nρ(x)\nfor d = 1 to D do\nfor j = 0 to 1 do\nCompute timestep τ = ((d − 1) × 2) + j\nfor i = 1 to m do\nObserve state S(τ)\ni\n;\nGet posteriors µ(d−1)\npost,i\nand Σ(d−1)\npost,i\nfor user i\nfrom µ(d−1)\npost\nand Σ(d−1)\npost\n;\nCompute action selection probability π(τ)\ni\nusing Eq. 26\nSample action a(t)\ni\n= Bern(π(τ)\ni\n)\nCollect reward R(τ)\ni\nend\nend\nUpdate Σu,d and σ2\nϵ,d using Eq. 22, with engineered\nrewards from Eq. 28;\nUpdate posteriors µ(d)\npost and Σ(d)\npost using Eq. 14 and Eq.\n15, with engineered rewards from Eq. 28\nend\n4.3\nReward Engineering\nTo account for delayed effects in the bandit framework, we\nengineer the reward for the RL algorithm. Note that this en-\ngineered reward is only utilized to update the RL algorithm’s\nparameters and hyper-parameters. We are still interested in\nmaximizing the reward defined in Sec. 2, and use it to evalu-\nate the algorithm’s performance.\nThe engineered reward ˆR(t)\ni\nfor user i at decision time t is\ndefined as:\nˆR(t)\ni\n= R(t)\ni\n− a(t)\ni cost(a(t)\ni )\n(28)\ncost(a(t)\ni ) = λ · σi,obs\n(29)\nwhere σi,obs is the standard deviation of the observed re-\nwards for a given user i, and λ is a tuned non-negative\nhyper-parameter. Note that the reward is not penalized when\na(t)\ni\n= 0. Intuitively, the cost function is designed to allow\nthe RL algorithm to optimize for user engagement, while si-\nmultaneously accounting for the delayed effect of sending an\nintervention message, i.e. a(t)\ni\n= 1.\n5\nExperimental Results\nIn this section, we detail the design of a simulation testbed\n(Sec. 5.1) to help evaluate the performance of our algorithm.\nOur experimental setup and the corresponding results are dis-\ncussed in Sec 5.2.\n5.1\nSimulation Testbed Design\nWe leverage data from the SARA [Rabbi et al., 2018] study,\nwhich trialed an mHealth app aimed at sustaining engage-\nment of substance use data collection from participants. Since\nthe SARA study focused on a similar demographic of EAs as\nthe MiWaves pilot study, it appears ideal for constructing a\nsimulation testbed. However, note that this data is impover-\nished. SARA had only 1 decision point per day, as compared\nto 2 per day in MiWaves . The goal of the messages sent to\nthe participants in SARA was to increase survey completion\nin order to collect substance use data. In contrast, the goal of\nsending intervention messages in MiWaves pilot study is to\nreduce the participant’s cannabis use through self-monitoring\nand mobile health engagement. Moreover, the daily cannabis\nuse data in SARA was collected retro-actively at the end of\neach week, which often resulted in participant’s noisy rec-\nollection of their cannabis use, and had missing cannabis\nuse data if the participant chose to not respond. In contrast,\nparticipants in MiWaves are asked to self-report twice daily,\nwhich reduces the amount of missing data if they fail to self-\nreport once. More details and differences are highlighted in\nAppendix A.1. We construct a base dataset of 42 users af-\nter cleaning and imputing the SARA data (please refer to ap-\npendix A for more details).\nBase Model: For the base model of the environment, we fit\nMultinomial Logistic Regression (MLR) models on each of\nthe 42 users in the base dataset. The learnt weights include\nweights for the baseline (when action is 0), and the advantage\n(added to the baseline when action is 1). These user models\nare overfit to learn the user behavior as well capture the noise\nin the environment. We choose MLR for our user models,\nas it is interpretable, and performs similar in comparison to a\ngeneric neural network (see Appendix A.6).\nVarying Treatment Effects (TE): The effect of the interven-\ntion message on a particular user is measured by their unique\ntreatment effect size. Given that the intervention messages\nin SARA had minimal treatment effect [Nahum-Shani et al.,\n2021], we introduce higher levels of treatment effect into the\nuser models by augmenting their weights. Higher levels of\ntreatment effect increase the likelihood of obtaining higher\nrewards when taking action 1. To that end, we construct TE\n= low and TE = high treatment effect variants for each MLR\nuser model. Further details on imputing these effect sizes can\nbe found in Appendix A.9.\nModeling Habituation (HB): To account for delayed effects\nin the environment, we introduce user habituation to repeated\nstimuli (multiple intervention messages sent to the user in a\nshort span of time) by adding a negative effect in the base-\nline weights of the MLR user models. To that end, we define\ndosage for each user at each decision point as the weighted\naverage of the number of intervention messages received in\nthe previous six decision points. The weights are decreased\nwith each past decision point, reflecting a diminishing impact\nof older intervention messages received by the user. Next, we\nimpute baseline weights for dosage in the MLR user mod-\nels in a way that higher dosage (more messages received)\nleads to higher likelihood of generating low rewards, and\nvice-versa. Note that this procedure simulates how the user\nmay experience habituation; if the RL algorithm does not\nsend many interventions to a user experiencing habituation,\nthe user may dis-habituate and recover their baseline behav-\nior. We construct two environment variants - HB = low and\nHB = high habituation effect - by varying the baseline weights\nfor dosage. Additionally, we simulate the proportion of users\nwho can experience habituation within a population - set at\nMinimal Treatment Effect\nLow Treatment Effect\nHB=Low\nHB=High\nHB=Low\nHB=High\nAlg.\nHB=None\nP=50%\nP=100%\nP=50%\nP=100%\nHB=None\nP=50%\nP=100%\nP=50%\nP=100%\nreBandit\n128.54±0.18\n127.23±0.18\n126.01±0.18\n123.22±0.19\n119.55±0.20\n129.44±0.17\n128.11±0.17\n126.80±0.18\n123.74±0.19\n120.12±0.20\nBLR\n127.78±0.16\n126.60±0.18\n125.78±0.18\n123.23±0.19\n119.60±0.20\n129.10±0.17\n127.85±0.17\n126.53±0.18\n123.75±0.19\n120.16±0.20\nrandom\n127.83±0.16\n126.52±0.18\n125.22±0.18\n119.03±0.21\n110.29±0.23\n128.97±0.17\n127.70±0.17\n126.45±0.18\n120.49±0.20\n112.06±0.22\nTable 1: Average total reward per user per simulated trial, averaged across 500 simulated trials and 120 users per trial, along with their 95%\nconfidence intervals (CI) for minimal and low treatment effect settings. HB refers to the level of habituation in the environment, while P is\nused to denote the proportion of the population who can experience habituation.\nHB=Low\nHB=High\nAlg.\nHB=None\nP=50%\nP=100%\nP=50%\nP=100%\nreBandit\n132.25±0.16\n130.95±0.17\n129.71±0.17\n124.70±0.19\n121.19±0.20\nBLR\n132.21±0.16\n130.94±0.17\n129.63±0.17\n124.70±0.19\n121.22±0.19\nrandom\n131.05±0.17\n129.88±0.17\n128.71±0.17\n123.19±0.20\n115.39±0.22\nTable 2: Average total reward per user per simulated trial along with\ntheir 95% CIs for the high treatment effect settings.\neither P = 50% or P = 100%. Further details on model-\ning habituation into simulation user models can be found in\nAppendix A.9.\n5.2\nSimulation Results\nWe construct 15 simulation environment variants using a\ncombination of techniques described in Sec. 5.1. For each en-\nvironment, we simulate 500 studies with m = 120 users each,\nover a period of D = 30 days (T = 60). The m = 120 users\nare drawn with replacement from the 42 MLR user models\nlearnt using SARA data.\nWe compare the performance of our algorithm to two com-\nmon approaches in mobile health studies.\nFirst, is a full\npooling algorithm called BLR. BLR utilizes Bayesian Lin-\near Regression [Liao et al., 2019] to pool data and learn a\nsingle model across all the users in a study, and select ac-\ntions according to the action selection procedure mentioned\nin Sec. 4.2. We use engineered rewards (Sec. 4.3) to up-\ndate BLR’s parameters and hyper-parameters. We also up-\ndate BLR hyper-parameters using Empirical Bayes, similar\nto the approach described in Sec. 4.1, for a fair comparison.\nFor both reBandit and BLR, we update the posteriors at the\nend of each simulated day (every 2 decision points), and the\nhyper-parameters at the end of each week (every 14 decision\npoints). We refer the reader to Appendix B for more details\nabout BLR’s implementation. In addition to BLR, we also\ncompare against the random algorithm, which utilizes an ac-\ntion selection probability of π(t)\ni\n= 0.5.\nFor each algorithm and simulation environment pair, we\ncalculate the average total reward per user per simulated trial,\naveraged across the 500 simulated trials and 120 users in each\ntrial. We also compute their 95% confidence intervals (CIs).\nWe summarize our findings in Table 1 and 2. In all the sim-\nulation environments, reBandit performs no worse than the\nbaseline algorithms. We highlight the environments where\nreBandit significantly outperforms other algorithms (CIs do\nnot overlap) in green. In the environments where the CIs for\nthe average total reward overlap for reBandit and BLR, we in-\ndividually compare each of the 500 seeded simulations, and\ncount the number of times reBandit achieved an average to-\ntal reward per user as compared to BLR. If this number is\ngreater than 50% of the simulations, i.e. greater than 250, we\nhighlight those environments in yellow, otherwise they are\nhighlighted in blue.\nThe primary takeaway from our simulation results in Ta-\nbles 1 and 2 is that reBandit is impactful - it performs better\nthan BLR in most environments, and even in the blue high-\nlighted environments where it performs slightly worse than\nBLR, the performance is still comparable. It is important to\nnote that our procedures to artificially introduce treatment ef-\nfects and user habituation into the user models reduces the\nheterogeneity among the user models. This is due to the fact\nthat our procedure to artificially inject treatment effect estab-\nlishes a non-negative effect of taking an action across all the\nusers in the user models. The same applies to the procedure\nfor introducing user habituation, as it establishes a clear neg-\native effect with respect to dosage across all users in the user\nmodels. However, in practice, higher levels of treatment ef-\nfect or user habituation effect may lead to more heterogene-\nity in the population. Given that limitation, it is easy to ob-\nserve that in our simulations, as levels of treatment effect or\nuser habituation effect are increased, the performance gap be-\ntween reBandit and BLR decreases. In simulation environ-\nments characterized by more pronounced heterogeneity due\nto lower levels of treatment and/or habituation effects, reBan-\ndit excels by adeptly identifying and leveraging the hetero-\ngeneity within the user population to personalize the likeli-\nhood of intervention message delivery and accrues greater re-\nwards.\n6\nConclusion\nIn this paper, we introduced reBandit, an online RL algo-\nrithm which will be a part of the upcoming mobile health\nstudy named MiWaves aimed at reducing cannabis use among\nemerging adults. We addressed the unique challenges inher-\nent in mobile health studies, including limited data, and re-\nquirement for algorithmic autonomy and stability, while de-\nsigning reBandit. We showed that reBandit utilizes random-\neffects and informative Bayesian priors to learn quickly and\nefficiently in noisy environments which are common in mo-\nbile health studies. The introduction of random effects allows\nreBandit to leverage the heterogeneity in the study population\nand deliver personalized interventions. To benchmark our al-\ngorithm, we detailed the design of a simulation testbed using\nprior data, and showed that reBandit performs equally well\nor better than two common approaches used in mobile health\nstudies. In the future, we aim to analyze the effectiveness of\nthe interventions in the MiWaves pilot study. In addition, we\naim to investigate the contribution of an individual’s data and\nthe study population data towards learning the individual’s\nparameters in the random effects model (see Appendix F).\nReferences\n[Aouali et al., 2023] Imad Aouali, Branislav Kveton, and Sumeet\nKatariya.\nMixed-effect thompson sampling.\nIn International\nConference on Artificial Intelligence and Statistics, pages 2087–\n2115. PMLR, 2023.\n[Basu et al., 2021] Soumya Basu, Branislav Kveton, Manzil Za-\nheer, and Csaba Szepesv´ari.\nNo regrets for learning the prior\nin bandits. Advances in neural information processing systems,\n34:28029–28041, 2021.\n[Carliner et al., 2017] Hannah Carliner, Qiana L Brown, Aaron L\nSarvet, and Deborah S Hasin. Cannabis use, attitudes, and legal\nstatus in the us: A review. Preventive medicine, 104:13–23, 2017.\n[Chan et al., 2021] Gary CK Chan, Denise Becker, Peter Butter-\nworth, Lindsey Hines, Carolyn Coffey, Wayne Hall, and George\nPatton.\nYoung-adult compared to adolescent onset of regular\ncannabis use: A 20-year prospective cohort study of later con-\nsequences. Drug and Alcohol Review, 40(4):627–636, 2021.\n[Clarke et al., 2017] Shanice Clarke, Luis G Jaimes, and Miguel A\nLabrador. mstress: A mobile recommender system for just-in-\ntime interventions for stress. In 2017 14th IEEE annual consumer\ncommunications & networking conference (CCNC), pages 1–5.\nIEEE, 2017.\n[Deshpande et al., 2018] Yash Deshpande, Lester Mackey, Vasilis\nSyrgkanis, and Matt Taddy. Accurate inference for adaptive lin-\near models. In International Conference on Machine Learning,\npages 1194–1203. PMLR, 2018.\n[Forman et al., 2019] Evan M Forman, Stephanie G Kerrigan,\nMeghan L Butryn, Adrienne S Juarascio, Stephanie M Manasse,\nSantiago Onta˜n´on, Diane H Dallal, Rebecca J Crochiere, and\nDanielle Moskow.\nCan the artificial intelligence technique of\nreinforcement learning use continuously-monitored digital data\nto optimize treatment for weight loss?\nJournal of behavioral\nmedicine, 42:276–290, 2019.\n[Golbus et al., 2021] Jessica R Golbus, Walter Dempsey, Eliza-\nbeth A Jackson, Brahmajee K Nallamothu, and Predrag Klasnja.\nMicrorandomized trial design for evaluating just-in-time adap-\ntive interventions through mobile health technologies for cardio-\nvascular disease. Circulation: Cardiovascular Quality and Out-\ncomes, 14(2):e006760, 2021.\n[Greenewald et al., 2017] Kristjan Greenewald, Ambuj Tewari, Su-\nsan Murphy, and Predag Klasnja.\nAction centered contextual\nbandits. Advances in neural information processing systems, 30,\n2017.\n[Hall, 2009] Wayne Hall. The adverse health effects of cannabis\nuse: what are they, and what are their implications for policy?\nInternational Journal of drug policy, 20(6):458–466, 2009.\n[Hasin et al., 2016] Deborah Hasin, Bradley Kerridge, Tulshi Saha,\nBoji Huang, Roger Pickering, Sharon Smith, Jeesun Jung, Haitao\nZhang, and Bridget Grant.\nPrevalence and correlates of dsm-\n5 cannabis use disorder, 2012-2013:\nFindings from the na-\ntional epidemiologic survey on alcohol and related conditions–iii.\nAmerican Journal of Psychiatry, 173(6):588–599, 2016.\n[Hong et al., 2022] Joey Hong, Branislav Kveton, Manzil Zaheer,\nand Mohammad Ghavamzadeh. Hierarchical bayesian bandits. In\nInternational Conference on Artificial Intelligence and Statistics,\npages 7724–7741. PMLR, 2022.\n[Jaimes et al., 2015] Luis G Jaimes, Martin Llofriu, and Andrew\nRaij. Preventer, a selection mechanism for just-in-time preven-\ntive interventions. IEEE Transactions on Affective Computing,\n7(3):243–257, 2015.\n[Jiang et al., 2015] Nan Jiang, Alex Kulesza, Satinder Singh, and\nRichard Lewis. The dependence of effective planning horizon on\nmodel accuracy. In Proceedings of the 2015 International Con-\nference on Autonomous Agents and Multiagent Systems, pages\n1181–1189, 2015.\n[Kalvit and Zeevi, 2021] Anand Kalvit and Assaf Zeevi. A closer\nlook at the worst-case behavior of multi-armed bandit algorithms.\nIn M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information\nProcessing Systems, volume 34, pages 8807–8819. Curran Asso-\nciates, Inc., 2021.\n[Kramer et al., 2019] Jan-Niklas Kramer, Florian K¨unzler, Varun\nMishra, Bastien Presset, David Kotz, Shawna Smith, Urte Scholz,\nTobias Kowatsch, et al. Investigating intervention components\nand exploring states of receptivity for a smartphone app to pro-\nmote physical activity:\nprotocol of a microrandomized trial.\nJMIR research protocols, 8(1):e11540, 2019.\n[Laird and Ware, 1982] Nan Laird and James Ware.\nRandom-\neffects models for longitudinal data. Biometrics, pages 963–974,\n1982.\n[Laird, 2004] Nan Laird.\nRandom effects and the linear mixed\nmodel. In Analysis of Longitudinal and Cluster-Correlated Data,\nvolume 8, pages 79–96. Institute of Mathematical Statistics,\n2004.\n[Langford and Zhang, 2007] John Langford and Tong Zhang. The\nepoch-greedy algorithm for contextual multi-armed bandits. Ad-\nvances in neural information processing systems, 20(1):96–1,\n2007.\n[Lapham et al., 2019] Gwen T Lapham, Cynthia I Campbell, Bobbi\nJo H Yarborough, Rulin C Hechter, Brian K Ahmedani, Irina V\nHaller, Andrea H Kline-Simon, Derek D Satre, Amy M Loree,\nConstance Weisner, et al. The prevalence of healthcare effective-\nness data and information set (hedis) initiation and engagement\nin treatment among patients with cannabis use disorders in 7 us\nhealth systems. Substance Abuse, 40(3):268–277, 2019.\n[Li et al., 2010] Lihong Li, Wei Chu, John Langford, and Robert E\nSchapire. A contextual-bandit approach to personalized news ar-\nticle recommendation. In Proceedings of the 19th international\nconference on World wide web, pages 661–670, 2010.\n[Liao et al., 2019] Peng Liao, Kristjan H. Greenewald, Predrag V.\nKlasnja, and Susan A. Murphy. Personalized heartsteps: A re-\ninforcement learning algorithm for optimizing physical activity.\nCoRR, abs/1909.03539, 2019.\n[Martin et al., 2018] Cesar A Martin, Daniel E Rivera, Eric B Hek-\nler, William T Riley, Matthew P Buman, Marc A Adams, and Ali-\ncia B Magann. Development of a control-oriented model of social\ncognitive theory for optimized mhealth behavioral interventions.\nIEEE Transactions on Control Systems Technology, 28(2):331–\n346, 2018.\n[Morris, 1983] Carl N Morris. Parametric empirical bayes infer-\nence: theory and applications. Journal of the American statistical\nAssociation, 78(381):47–55, 1983.\n[Nahum-Shani et al., 2018] Inbal Nahum-Shani, Shawna N Smith,\nBonnie J Spring, Linda M Collins, Katie Witkiewitz, Ambuj\nTewari, and Susan A Murphy.\nJust-in-time adaptive interven-\ntions (jitais) in mobile health: key components and design prin-\nciples for ongoing health behavior support. Annals of Behavioral\nMedicine, pages 1–17, 2018.\n[Nahum-Shani et al., 2021] Inbal Nahum-Shani, Mashfiqui Rabbi,\nJamie Yap, Meredith L Philyaw-Kotov, Predrag Klasnja, Erin E\nBonar, Rebecca M Cunningham, Susan A Murphy, and Mau-\nreen A Walton.\nTranslating strategies for promoting engage-\nment in mobile health: A proof-of-concept microrandomized\ntrial. Health Psychology, 40(12):974, 2021.\n[Nahum-Shani et al., 2022] Inbal Nahum-Shani, Steven D Shaw,\nStephanie M Carpenter, Susan A Murphy, and Carolyn Yoon. En-\ngagement in digital interventions. American Psychologist, 2022.\n[Paredes et al., 2014] Pablo Paredes, Ran Gilad-Bachrach, Mary\nCzerwinski, Asta Roseway, Kael Rowan, and Javier Hernandez.\nPoptherapy: Coping with stress through pop-culture. In Proceed-\nings of the 8th international conference on pervasive computing\ntechnologies for healthcare, pages 109–117, 2014.\n[Peacock et al., 2018] Amy Peacock, Janni Leung, Sarah Larney,\nSamantha Colledge, Matthew Hickman, J¨urgen Rehm, Gary A\nGiovino, Robert West, Wayne Hall, Paul Griffiths, et al. Global\nstatistics on alcohol, tobacco and illicit drug use: 2017 status re-\nport. Addiction, 113(10):1905–1926, 2018.\n[Peleg et al., 2022] Amit Peleg, Naama Pearl, and Ron Meir. Met-\nalearning linear bandits by prior update. In International Confer-\nence on Artificial Intelligence and Statistics, pages 2885–2926.\nPMLR, 2022.\n[Rabbi et al., 2015] Mashfiqui Rabbi, Min Hane Aung, Mi Zhang,\nand Tanzeem Choudhury. Mybehavior: automatic personalized\nhealth feedback from user behaviors and preferences using smart-\nphones. In Proceedings of the 2015 ACM international joint con-\nference on pervasive and ubiquitous computing, pages 707–718,\n2015.\n[Rabbi et al., 2018] Mashfiqui Rabbi, Meredith Philyaw Kotov, Re-\nbecca Cunningham, Erin E Bonar, Inbal Nahum-Shani, Predrag\nKlasnja, Maureen Walton, Susan Murphy, et al. Toward increas-\ning engagement in substance use data collection: development of\nthe substance abuse research assistant app and protocol for a mi-\ncrorandomized trial using adolescents and emerging adults. JMIR\nresearch protocols, 7(7):e9850, 2018.\n[Rabbi et al., 2019] Mashfiqui Rabbi, Predrag Klasnja, Tanzeem\nChoudhury, Ambuj Tewari, and Susan Murphy.\nOptimizing\nmhealth interventions with a bandit. Digital Phenotyping and\nMobile Sensing: New Developments in Psychoinformatics, pages\n277–291, 2019.\n[Robinson, 1991] George K Robinson. That blup is a good thing:\nthe estimation of random effects. Statistical science, pages 15–\n32, 1991.\n[Russo and Van Roy, 2014] Daniel Russo and Benjamin Van Roy.\nLearning to optimize via posterior sampling. Mathematics of Op-\nerations Research, 39(4):1221–1243, 2014.\n[SAMHSA, 2023] SAMHSA.\n2021\nNSDUH\nDe-\ntailed\nTables.\nhttps://www.samhsa.gov/data/report/\n2021-nsduh-detailed-tables, 2023. Accessed: 2024-02-14.\n[Shrier et al., 2018] Lydia A Shrier, Pamela J Burke, Meredith\nKells, Emily A Scherer, Vishnudas Sarda, Cassandra Jonestrask,\nZiming Xuan, and Sion Kim Harris. Pilot randomized trial of\nmoment, a motivational counseling-plus-ecological momentary\nintervention to reduce marijuana use in youth. Mhealth, 4, 2018.\n[Simchowitz et al., 2021] Max Simchowitz, Christopher Tosh, Ak-\nshay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro\nDudik, and Robert Schapire.\nBayesian decision-making un-\nder misspecified priors with applications to meta-learning. Ad-\nvances in Neural Information Processing Systems, 34:26382–\n26394, 2021.\n[Sutton and Barto, 2018] Richard S Sutton and Andrew G Barto.\nReinforcement learning: An introduction. MIT press, 2018.\n[Tewari and Murphy, 2017] Ambuj Tewari and Susan A Murphy.\nFrom ads to interventions: Contextual bandits in mobile health.\nMobile health:\nsensors, analytic methods, and applications,\npages 495–517, 2017.\n[Thompson, 1933] William R Thompson. On the likelihood that\none unknown probability exceeds another in view of the evidence\nof two samples. Biometrika, 25(3-4):285–294, 1933.\n[Tomkins et al., 2021] Sabina Tomkins, Peng Liao, Predrag Klas-\nnja, and Susan Murphy.\nIntelligentpooling: Practical thomp-\nson sampling for mhealth. Machine learning, 110(9):2685–2727,\n2021.\n[Trella et al., 2022] Anna Trella, Kelly Zhang, Inbal Nahum-Shani,\nVivek Shetty, Finale Doshi-Velez, and Susan Murphy. Designing\nreinforcement learning algorithms for digital interventions: pre-\nimplementation guidelines. Algorithms, 15(8):255, 2022.\n[Trella et al., 2023] Anna Trella, Kelly Zhang, Inbal Nahum-Shani,\nVivek Shetty, Finale Doshi-Velez, and Susan Murphy. Reward\ndesign for an online reinforcement learning algorithm supporting\noral self-care. In Proceedings of the AAAI Conference on Artifi-\ncial Intelligence, volume 37, pages 15724–15730, 2023.\n[Volkow et al., 2014] Nora D Volkow, Ruben D Baler, Wilson M\nCompton, and Susan RB Weiss. Adverse health effects of mar-\nijuana use. New England Journal of Medicine, 370(23):2219–\n2227, 2014.\n[Walsh and Groarke, 2019] Jane C Walsh and Jenny M Groarke. In-\ntegrating behavioral science with mobile (mhealth) technology to\noptimize health behavior change interventions. European Psy-\nchologist, 2019.\n[Wan et al., 2021] Runzhe Wan, Lin Ge, and Rui Song. Metadata-\nbased multi-task bandits with bayesian hierarchical models. Ad-\nvances in Neural Information Processing Systems, 34:29655–\n29668, 2021.\n[Wan et al., 2023] Runzhe Wan, Lin Ge, and Rui Song. Towards\nscalable and robust structured bandits: A meta-learning frame-\nwork. In International Conference on Artificial Intelligence and\nStatistics, pages 1144–1173. PMLR, 2023.\n[Wang et al., 2005] Chih-Chun Wang, Sanjeev R Kulkarni, and\nH Vincent Poor. Bandit problems with side observations. IEEE\nTransactions on Automatic Control, 50(3):338–355, 2005.\n[Yom-Tov et al., 2017] Elad Yom-Tov, Guy Feraru, Mark Kozdoba,\nShie Mannor, Moshe Tennenholtz, and Irit Hochberg. Encourag-\ning physical activity in patients with diabetes: Intervention using\na reinforcement learning system. Journal of medical Internet re-\nsearch, 19(10):e338, 2017.\n[Zhang et al., 2022] Kelly W Zhang, Lucas Janson, and Susan A\nMurphy. Statistical inference after adaptive sampling for longi-\ntudinal data. arXiv preprint arXiv:2202.07098, 2022.\n[Zhou et al., 2018] Mo Zhou, Yonatan Mintz, Yoshimi Fukuoka,\nKen Goldberg, Elena Flowers, Philip Kaminsky, Alejandro\nCastillejo, and Anil Aswani. Personalizing mobile fitness apps\nusing reinforcement learning. In CEUR workshop proceedings,\nvolume 2068. NIH Public Access, 2018.\n[Zhu and Kveton, 2022a] Rong Zhu and Branislav Kveton. Ran-\ndom effect bandits. In International Conference on Artificial In-\ntelligence and Statistics, pages 3091–3107. PMLR, 2022.\n[Zhu and Kveton, 2022b] Rong Zhu and Branislav Kveton.\nRo-\nbust contextual linear bandits. arXiv preprint arXiv:2210.14483,\n2022.\nAppendices\nA\nSimulation Testbed\nThis section details how we transform prior data to construct a dataset, and utilize the dataset to develop the MiWaves simulation\ntestbed. The testbed is used to develop and evaluate the design of the RL algorithm for the MiWaves pilot study. The base\nsimulator or the vanilla testbed is constructed using the SARA [Rabbi et al., 2018] study dataset. The SARA dataset consists\nof N = 70 users, and the SARA study was for 30 days, 1 decision point per day. For each user, the dataset contains their daily\nand weekly survey responses about substance use, along with daily app interactivity logs and notification logs. We will now\ndetail the procedure to construct this base simulator.\nA.1\nSARA vs MiWaves\nThe Substance Use Research Assistant (SARA) [Rabbi et al., 2018] study trialed an mHealth app aimed at sustaining engage-\nment of substance use data collection from participants. Since the SARA study focused on a similar demographic of emerging\nadults (ages 18-25) as the MiWaves pilot study, we utilized the data gathered from the SARA study to construct the simulation\ntestbed. We highlight the key differences between the SARA study and the MiWaves pilot study in Table 3.\nSARA\nMiWaves\nm = 70\nm = 120\nT = 30\nT = 60\nπ(t)\ni\n= 0.5\nπ(t)\ni\ndetermined by RL\nCannabis use data\nself-reported weekly\nCannabis use data\nself-reported twice daily\nInspirational and reminder\nmessages to increase\nsurvey completion and\ncollect substance use data\nMessages to prompt\ncannabis use reduction\nthrough self-monitoring\nand improve engagement\nTable 3: SARA vs MiWaves : Key differences\nA.2\nData Extraction\nFirst, we will detail the steps to extract the relevant data from the SARA dataset:\n1. App Usage: The SARA dataset has a detailed log of the user’s app activity for each day in the trial, since their first login.\nWe calculate the time spent by each user between a normal entry and an app paused log entry, until 12 AM midnight, to\ndetermine the amount of time (in seconds) spent by the user in the app on a given day. To determine the time spent by\nthe user in the evening, we follow the same procedure but we start from any logged activity after the 4 PM timestamp, till\nmidnight (12 AM).\n2. Survey Completion: The SARA dataset contains a CSV file for each user detailing their daily survey completion status\n(completed or not). We use this binary information directly to construct the survey completion feature.\n3. Action: The SARA dataset contains a CSV file for each user detailing whether they got randomized (with 0.5 probability)\nto receive a notification at 4 PM, and whether the notification was actually pushed and displayed on the user’s device. We\nuse this CSV file to derive the latter information, i.e. whether the app showed the notification on the user’s device (yes or\nno).\n4. Cannabis Use: Unlike MiWaves , the SARA trial did not ask users to self-report cannabis use through the daily surveys.\nHowever, the users were prompted to respond to a weekly survey at the end of each week (on Sundays). Through these\nweekly surveys, the users were asked to retroactively report on their cannabis use in the last week, from Monday through\nSunday. We use these weekly surveys to retroactively build the reported cannabis-use for each user in the study. The\ncannabis use was reported in grams of cannabis used, taking values of 0g, 0.25g, 0.5g, 1g, 1.5g, 2g and 2.5g+. Users who\nwere not sure about their use on a particular day, reported their use as “Not sure”. Meanwhile, users who did not respond\nto the weekly survey were classified to have “Unknown” use for the entire week. The distribution of reported cannabis use\nin SARA across all the users, across all 30 days, can be viewed in Figure 2.\nWe build a database for all users using the three features specified above.\nFigure 2: Distribution of cannabis use across all users, across all data points\nA.3\nData Cleaning\nNext, we will specify the steps to clean this data, and deal with outliers:\n1. Users with insufficient data: We remove users who had more than 20 undetermined (i.e. either “Unknown” or “Not sure”)\ncannabis use entries. Upon removing such users, we are left with N = 42 users. The updated distribution of reported\ncannabis use in the remaining data across all the users across all 30 days, is demonstrated in Figure 3.\n(a) Dataset cannabis-use distribution\n10\n15\n20\n25\n30\nCount of self-reported cannabis use datapoints\n0\n1\n2\n3\n4\n5\n6\nNumber of users\nHistogram of count of self-reported cannabis use datapoints\n(b) Count of non-missing cannabis-use datapoints vs number of users\nFigure 3: Distribution of cannabis use across all users, across all data points, after removing users with insufficient data\n2. Outliers in app usage information: The average app usage in a given day across all users, comes out to be 244 seconds\n(please refer to Section A.2 as to how app usage is calculated from the raw data). However, due to some technical error\nduring the data collection process, sometimes, a user ended up with greater than 1000 seconds of app use in a day, with\nthe highest reaching around 67000 seconds. We observe a similar issue with post-4PM app use data. To deal with such\noutliers, we clip any post-4PM app use higher than 700 seconds. There are 20 such data points in the dataset.\nA.4\nReward computation\nNext, we calculate the reward for each data point in the dataset. It is calculated as follows:\n• 0: User did not complete the daily survey, nor used the app.\n• 1: User did not complete the daily survey, but used the app outside of the daily survey.\n• 2: User completed the survey.\nWe transform the [0 − 2] reward above to a [0 − 3] reward defined in MiWaves (refer Section 2) by randomly transforming\nthe data points with 2 reward to 3 with 50% probability.\nA.5\nDataset for training user models\nWe use the post-4PM data (i.e. data from 4 PM to 12 AM midnight) (dubbed as the evening data) to create a dataset to train the\nindividual user models. The dataset contains the following features:\n• Day In Study ∈ [1, 30]\n• Cannabis Use ∈ [0, 2.5g]\n• (Evening) App usage ∈ [0, 700]\n• Survey completion (binary)\n• Weekend indicator (binary)\n• Action (binary)\n• Reward ∈ {0, 1, 2, 3}\nWe detail the steps to create this evening dataset:\n1. Evening cannabis-use: The SARA study documented cannabis use for a given user for a whole day. In contrast, MiWaves\nwill be asking users to self-report their cannabis use twice a day. To mimic the same, after discussions among the scientific\nteam, we split a given user’s daily cannabis use from SARA into morning and evening use, and multiply a factor of 0.67 to\ngenerate their evening cannabis use. Also, the MiWaves study will be recruiting users who typically use cannabis at least\n3 times a week. We expect their use to be much higher than that of users in SARA. So, we multiply the evening cannabis\nuse by a factor of 1.5. Thus, we generate the evening cannabis use from the user’s daily use reported in SARA as follows:\nEvening CB Use = That Day’s SARA CB Use × 0.67 × 1.5\n(30)\n2. Feature normalization: The resulting dataset’s features are then normalized as follows:\n• Day in study is normalized into a range of [−1, 1] as follows\nDay in study (normalized) = Day in study − 15.5\n14.5\n(31)\n• App usage is normalized into a range of [−1, 1] as follows\nApp usage (normalized) = App usage − 350\n350\n(32)\n• Cannabis use (evening) is normalized into a range of [−1, 1] as follows\nCannabis use (normalized) = Cannabis use − 1.3\n1.35\n(33)\nA.6\nTraining User Models\nAs specified above in Sec. A.5, we use to evening dataset to train our user models. This dataset has the following features:\n• Day In Study ∈ [−1, 1]: Negative values refer to the first half of the study, while positive values refer to the second half of\nthe study. A value of 0 means that the user is in the middle of the study. −1 means that the user has just begun the study,\nwhile 1 means they are at the end of the 30 day study.\n• Cannabis Use ∈\n[−1, 1]: Negative values refer to the user’s cannabis use being lower than the population’s average\ncannabis use value, while positive values refer to user’s cannabis use being higher than the study population’s average\ncannabis use value. A value of 0 means that the user’s cannabis use is the average value of cannabis use in the study\npopulation. Meanwhile, −1 means that the user is not using cannabis, and 1 means that the user used the highest amount\nof cannabis reported by the study population.\n• (Evening) App usage ∈\n[−1, 1]: Negative values refer to the user’s app use being lower than the population’s average\napp use value, while positive values refer to user’s app use being higher than the study population’s average app use\nvalue. A value of 0 means that the user’s app usage is the average amount of app usage observed in the study population.\nMeanwhile, −1 means that the user’s app usage is non-existent (i.e. zero). On the other hand, 1 means that the user’s app\nusage is the highest among the observed app usage values in the study population.\n• Survey completion ∈ {0, 1}: A value of 0 refers to the case where the user has not finished the decision point’s EMA,\nwhile a value of 1 refers to the case where the user has responded to the decision point’s EMA.\n• Weekend indicator ∈ {0, 1}: A value of 0 refers to the case where the decision point falls on a weekday, while a value of\n1 refers to the case where the decision point falls on a weekend.\n• Action ∈ {0, 1}: A value of 0 means that action was not taken (i.e. no notification or intervention message was shown),\nwhile 1 means that an action was taken (i.e. a notification or intervention message was shown).\n• Reward ∈ {0, 1, 2, 3}: Same as defined in Section A.4\nWe morph these features into a 12 dimensional feature vector, defined as follows:\n1. Intercept (1)\n2. Survey Completion\n3. Standardized App Usage\n4. Standardized Cannabis Use\n5. Weekend Indicator\n6. Standardized Day in Study\n7. Action taken multiplied by the Intercept (1)\n8. Action taken multiplied by Survey Completion\n9. Action taken multiplied by Standardized App Usage\n10. Action taken multiplied by Standardized Cannabis Use\n11. Action taken multiplied by Weekend Indicator\n12. Action taken multiplied by Standardized Day in Study\nIn the rest of the section, the weights corresponding to features 1 − 6 are referred to as the baseline weights, and the weights\nfor 7 − 12 are referred to as the advantage weights.\nWe fit the reward using our user models. Before we train user models, we do a complete-case analysis on the evening\ndataset. It involves removing all the data points which have any missing feature. This can either be a missing “cannabis use”\nvalue, or a missing “action” (which is the case on the first day of the study).\nGiven that our target variable i.e.\nthe reward is categorical (0-3) in nature, we consider two options for our generative\nuser models:\n• Multinomial Logistic Regression (MLR) - We fit a multinomial logistic regression model on our dataset. Given K\nclasses (K=4 in our case), the probability for a data point i belonging to a class c is given by:\nP(Yi = c|Xi) =\neβc·Xi\nPK\nj=1 eβj·Xi\n(34)\nwhere Xi are the features of the given data point i, and βj is the learned coefficient for a given reward class j. Each data\npoint refers to a user-decision point, we use these terms interchangibly throughout the document.\nThe model is optimized using the following objective:\nmin\nβ −\nn\nX\ni=1\nK\nX\nc=1\n1{Yi=c} log(P(Yi = c|Xi)) + 1\n2||β||2\nF\n(35)\nWe use python’s scikit-learn package for training the multinomial logistic regression model (more information\nhere). It makes sure that P\nj βj = 0. We use the following parameters for training the model using scikit-learn:\n– Penalty: L2\n– Solver: LBFGS\n– Max. iterations: 200\n– Multi-class: Multionomial\n• Multi-layer perceptron (MLP) Classifier - We fit a simple neural network on our dataset, and use the last layer’s logits\nas probabilities for each class.\nWe use python’s scikit-learn package for training the MLP Classifier. We use the following parameters for training\nthe model -\n– Hidden layer configuration: (7, )\n– Activation function: Logistic\n– Solver: Adam\n– Max. iterations: 500\nWe choose MLR for our user models, as it is interpretable, and offers similar (if not better) performance as compared to a\ngeneric neural network (see Figure 5). Interpretability is important here since we would like to vary the treatment effect in our\ngenerative user models.\nFigure 4 represents the learnt coefficients of the MLR user model for classes 1 to 3, relative to class 0’s coefficients. Note\nthat both survey completion and app usage seem to exhibit strong relationship wrt the target variable for most of the users. To\nbe specific, in Figure 4, coefficients of both survey completion and app usage are mostly positive across most of the N = 42\nusers, both in baseline and advantage. The magnitude of the relative weights of these features keeps increasing as the reward\nclass increases. This signifies that if a user is engaged (completing surveys, using the app), they are more likely to generate\na non-zero reward as compared to a reward of 0. We use the probabilities as weights from the MLR models to stochastically\ngenerate the reward during the data generation process. More on that in Section A.8.\n1\n0\n1\n2\nCoefficient\nClass 1 intercept\nClass 1 survey_completion\nClass 1 std_app_usage\nClass 1 std_cannabis_use\nClass 1 weekend\nClass 1 std_day\n1\n0\n1\n2\nCoefficient\nClass 1 act_intercept\nClass 1 act_survey_completion Class 1 act_std_app_usage Class 1 act_std_cannabis_use\nClass 1 act_weekend\nClass 1 act_std_day\n1\n0\n1\n2\nCoefficient\nClass 2 intercept\nClass 2 survey_completion\nClass 2 std_app_usage\nClass 2 std_cannabis_use\nClass 2 weekend\nClass 2 std_day\n1\n0\n1\n2\nCoefficient\nClass 2 act_intercept\nClass 2 act_survey_completion Class 2 act_std_app_usage Class 2 act_std_cannabis_use\nClass 2 act_weekend\nClass 2 act_std_day\n1\n0\n1\n2\nCoefficient\nClass 3 intercept\nClass 3 survey_completion\nClass 3 std_app_usage\nClass 3 std_cannabis_use\nClass 3 weekend\nClass 3 std_day\n0\n10\n20\n30\n40\nUser ID\n1\n0\n1\n2\nCoefficient\nClass 3 act_intercept\n0\n10\n20\n30\n40\nUser ID\nClass 3 act_survey_completion\n0\n10\n20\n30\n40\nUser ID\nClass 3 act_std_app_usage\n0\n10\n20\n30\n40\nUser ID\nClass 3 act_std_cannabis_use\n0\n10\n20\n30\n40\nUser ID\nClass 3 act_weekend\n0\n10\n20\n30\n40\nUser ID\nClass 3 act_std_day\nFigure 4: Bar plot of coefficients of features in the MLR user models relative to coefficients of class 0, across all N = 42 users.\nA.7\nDataset for generative process\nNext, we will create a dataset for the generative process for a simulation. To that end, we impute missing values in the evening\ndataset, and also create the morning dataset. We describe the procedure for both below.\n• Imputation in evening dataset: As stated before, the vanilla evening dataset has a few missing values for “cannabis use”\nand “action” values. We impute the values for “cannabis use” as follows - for a given missing value, we first determine\nthe day of the week, and then replace the unknown/missing value with the average of the cannabis use across the available\ndata of the user for that day of the week.\nNote that during the simulation, the action used by the user models for reward generation will be determined by the RL\nalgorithm in the simulation. Hence, we do not need to impute “action” here.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42\nUser ID\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nLog Loss\nMultinomial Logistic Regression\nMLP Classifier\nFigure 5: Comparison of log loss between the two models across all users\n• Generating morning dataset: Similar to the evening dataset, we generate a morning dataset per user to mimic the data\nwe would receive from MiWaves (2 decision points a day). We generate the following features:\n– Cannabis Use: We generate the morning cannabis use as follows:\nMorning CB Use = That Day’s SARA CB Use × 0.33 × 1.5\n(36)\n– App Usage: Since there are less than 30 evening app usage values per user in the SARA dataset, we decide to use\nthese values as an empirical distribution, and resample, with replacement, from the 30 values for each user at each\nmorning. The sampled value is the user’s morning app usage.\n– Survey Completion: For each user, we determine the proportion of times they responded to the daily survey during\nthe SARA study. Using this ratio as our probability of survey completion, we sample from a binomial distribution to\nconstruct the Survey Completion feature for the morning dataset for each user for each day.\n– Day In Study and Weekend indicator: We create one morning data point per day, so the day in study and weekend\nindicator features mimic the evening dataset.\nA.8\nSimulations: Data generation\nIn this section, we detail our data generation process for a simulation.\nWe assume that any given user i, her\ntrajectory in an RL simulation environment with T total decision points has the following structure:\nH(T )\ni\n=\n{S(1)\ni\n, a(1)\ni , R(1)\ni , · · · , S(T )\ni\n, a(T )\ni\n, R(T )\ni\n}.\nWe also assume that the combined dataset has the following structure - for any given decision point t (t ∈ [1, 60]), the state,\nS(t)\ni\n, is constructed (partially) based on: the user’s app usage from t−1 to t, the survey completion indicator (whether user fills\nthe survey after receiving action at t) for decision point t, and the cannabis use of the user from t − 1 to t. The data at decision\npoint t from the combined dataset is used to generate R(t)\ni , which in turn helps generate features to form S(t+1)\ni\n(refer Section\n2).\n1. Given a set number of users (parameter of the simulator) to simulate, we sample users with replacement from the N = 42\nusers in the combined dataset.\n2. We start the simulation in the morning of a given day. From decision point t = 1 to T in the simulation (T = 60), for each\nsampled user i (refer to the RL framework in Section 2):\n(a) If t > 1, given previously-generated reward R(t−1)\ni\n, we construct the following features - survey completion, app\nusage indicator, and activity question (which will help form S(t)\ni\naccording to Section 2), according to the following\nrules:\nSurvey Completion =\n(\n1\nif R(t−1)\ni\n≥ 2\n0\nif R(t−1)\ni\n< 2\n(37)\nApp Usage Indicator =\n(\n1\nif R(t−1)\ni\n≥ 1\n0\nif R(t−1)\ni\n= 0\n(38)\nActivity Question =\n\u001a\n1\nif R(t−1)\ni\n= 3\n0\notherwise\n(39)\nWe communicate the aforementioned calculated features (Equation 37, 38, and 39), along with the cannabis use from\nt − 2 to t − 1 to the RL algorithm. This is similar to how a user would self-report and convey this information to the\napp, to help the algorithm derive the user current state S(t)\ni\n.\nAt t = 1, since there is no R(0)\ni , note that the RL algorithm uses some initial state S(1)\ni\n. Since we intend to simulate\nMiWaves , all RL algorithms will use the following S(1)\ni\n:\n• S(1)\ni,1 : Set to 0. At the start of the trial, there is no engagement by the user.\n• S(1)\ni,2 : Set to 0. The first decision point is the morning, which corresponds to 0.\n• S(1)\ni,3 : Set to 1. The users in MiWaves use cannabis regularly (at least 3x a week), so we ascertain the users in the\ntrial to have high cannabis use at the start of the trial.\n(b) We ask the RL algorithm for the action a(t)\ni\nto be taken at the current decision point t for user i.\n(c) We take the data from the tth decision point in combined dataset of the user i - specifically the user’s survey completion\nat t, the user’s app usage from t − 1 to t, and the user’s cannabis use from the t − 1 to t. We calculate the weekend\nindicator by checking whether the decision point t falls on Saturday or Sunday. We feed this data, along with the\naction a(t)\ni\nfrom the previous step, to the user i’s trained user model from Section A.6 to obtain a reward R(t)\ni .\nA.9\nEnvironment Variants\nWe will now describe the design of different environment variants for our simulator, and how we go about operationalizing\nthem. Since we know that the MiWaves pilot study will have 120 participants, all our simulation environments will have\nN = 120 participants. We sample these participants with replacement from the N = 42 participants from the combined dataset\nduring the data generation process.\n1. Varying size of the treatment effect: We construct three variants wrt the size of the treatment effect:\n• Minimal treatment effect size: We keep the treatment effect i.e. advantage coefficients in the MLR model that we\nlearn from the users in the SARA data.\nFor the other two variants, we set the advantage intercept weight for each of the MLR user models as follows: we find the\nminimum advantage intercept weight across all classes learnt using the SARA user data, and if it is not assigned to class\n0 - we swap the weight with that of class 0. Then we set the advantage intercept weight of class 2 and class 3 to be the\naverage of both. The reason we do so is because that we believe that at any given point, taking an action will always have\na non-negative immediate treatment effect. To that end, at any given time, taking an action will always result in higher\nprobabilities for generating a non-zero reward, and lower probabilities for generating a zero reward, as compared to not\ntaking an action. Setting class weights by assigning the minimum weight to reward class 0 helps us achieve that. Also,\nsince we trained the user models from SARA data where we assigned 2 and 3 reward with equal probability wherever\nreward of 2 was assigned, we set their corresponding advantage intercept weights to be equal. Hence, we set them to be\nthe average of the observed weights of the two reward classes. Moreover, we decide to work with the user’s learnt weights\nfrom SARA to preserve heterogeneity among users.\nWe then multiply the advantage intercept weights for all the classes by a factor, which we refer to as the multiplier. We\nselect the multipliers for our other variants after observing the standardized advantage intercept weights. To do so, for each\nmultiplier, we first generate a dataset of trajectories for each of the N = 42 users, by sampling action with probability\n0.5. We generate 500 sets of such trajectories (i.e. 500 simulations with N = 42 users in each simulation). For each set\nof trajectories (i.e each simulation) of all N = 42 users, we fit a GEE linear model with fixed effects and robust standard\nerrors. The model is given as:\nE\nh\nR(t+1)\ni\n|S1, S2, S3, a\ni\n= α0 + α1S1 + α2S2 + α3S3 + α4S1S2 + α5S1S3 + α6S2S3 + α7S1S2S3\n+ a(β0 + β1S1 + β2S2 + β3S3 + β4S1S2 + β5S1S3 + β6S2S3 + β7S1S2S3).\n(40)\nWe take the learned weight of β0 and divide it by the sample standard deviation of observed rewards for that set of\ntrajectories (i.e. for that simulation) of N = 42 users - in order to obtain the standardized action intercept weight. We\ndo so for all the different multipliers we consider, and the results are summarized in Fig 6. We first check the minimal\neffect (weights from SARA), and see that the standardized treatment effect for the minimum effect setting comes out to be\naround 0.12. We want our low treatment effect setting to be closer to 0.15 and our higher to be around 0.3. So we tinker\nour weights as mentioned in the procedure above, and re-run the simulations. Since we do not scale the weights (yet) by\nany factor, this is equivalent to a multiplier setting of 1.0. We observe a standardized treatment effect of 0.18 in this case.\nFigure 6: Comparison of mean standardized action intercept weight vs advantage intercept multiplier. Mean is taken across 500 simulations\nwith N = 42 users (from SARA) in each simulation\nUsing this as a reference point, we try a lower and higher multiplier, and we observe that 0.7 and 2.5 give us the desired\nstandardized treatment effect for low effect and high effect settings respectively.\nNow, the remaining variants are described as follows:\n• Low: We multiply the advantage coefficients for each of the MLR user models for each class by 0.7.\n• High: We multiply the advantage coefficients for each of the MLR user models (as mentioned above) for each class\nby 2.5. This way, we are increasing the size of all advantage intercepts in the MLR user model, and in turn further\nskews the probability of getting a higher reward when taking an action as compared to a zero reward.\nDiscussion: Given the way we model our user models using MLR, they are not complex enough to support higher stan-\ndardized effect sizes (through advantage intercepts). Recall that the probability of a data point belonging to a particular\nreward class c is defined using our MLR model as follows:\nP(Yi = c|Xi) =\neβc·Xi\nPK\nj=1 eβj·Xi\nNote that we use a linear model in the exponent of the above MLR model, i.e. βjXi is linear. This is due to our design\nchoices of trying to keep our models simple and interpretable. Since the weights are constrained (always sum up to 1),\nthere is a threshold upto which one can increase the standardized treatment effect through these user models. We see this\nbehavior in Figure 6, where we observe that the standardized effect size hovers around 0.35 − 0.4, even though we are\nscaling the advantage intercept weights by larger multipliers.\n2. Habituation (affecting the baseline effect): In order to incorporate user habituation to repeated stimuli (i.e. intervention\nmessages) into the user models, we first define dosage for a user i at time t as Q(t)\ni\n= dκ\nP6\nj=1 κj−1a(t−j)\ni\n, i.e. weighted\naverage of the number of actions sent to the user i in the recent 6 decision points prior to decision point t. We set κ = 5\n6\nto represent looking back 6 decision points, and scale each sum by a constant dκ =\n1−κ\n1−κ6 so that weights add up to 1. To\nincorporate habituation into the MLR user models, we add a new baseline weight for dosage. For a given reward class c\n(where c ∈ {0, 1, 2, 3}), we set this weight in the MLR user model as follows:\nβc,dosage =\n\n\n\n\n\n\n\n\n\n\n\n6\nP\ni=1\nβc,i\nη\nif\n6P\ni=1\nβ0,i ≥ 0\n−\n6\nP\ni=1\nβc,i\nη\nif\n6P\ni=1\nβ0,i < 0\n(41)\nwhere the numerator is the sum of all the baseline weights (since there are 6 baseline weights, see Appendix A.6), and η is\nused to control the intensity of habituation. Notice that we flip the sign of the weight if the sum of the baseline weights for\nreward class 0 is negative. This is done to ensure that higher dosage (i.e. higher levels of repeated stimuli in the near past)\nleads to an increase in likelihood of generating lower rewards (namely reward classes 0 and 1). We set the weight in this\nway (by summing the baseline weights) to keep the user models as heterogeneous as possible. However, since all the user\nmodels now have a clear negative effect wrt dosage, these modified user models are perceived to be less heterogeneous\nthan the ones originally learnt from the SARA data.\nWe construct two variants by modifying the value of η, to vary the intensity of habituation in the user models:\n• Low: Low intensity habituation effect is set by using η = 6.\n• High: High intensity habituation effect is set by using η = 1.\nThe higher the habituation effect, the higher the likelihood of getting lower rewards when dosage Q(t)\ni\nis non-zero.\n3. Proportion of Users with habituation: For the environments with habituation, we vary the proportion of the population\nwho can experience habituation. To that end, we construct two variants with respect to the proportion of the simulated\nstudy population who can experience habituation (i.e. proportion of the simulated population whose models are augmented\nto have weights for dosage as described above) - (i) 50% and (ii) 100%\nUsing a combination of the user model variants mentioned above, we design a total of 15 environment variants:\n1. Minimal treatment effect without habituation\n2. Minimal treatment effect with low habituation in 50% of the population\n3. Minimal treatment effect with low habituation in 100% of the population\n4. Minimal treatment effect with high habituation in 50% of the population\n5. Minimal treatment effect with high habituation in 100% of the population\n6. Low treatment effect without habituation\n7. Low treatment effect with low habituation in 50% of the population\n8. Low treatment effect with low habituation in 100% of the population\n9. Low treatment effect with high habituation in 50% of the population\n10. Low treatment effect with high habituation in 100% of the population\n11. High treatment effect without habituation\n12. High treatment effect with low habituation in 50% of the population\n13. High treatment effect with low habituation in 100% of the population\n14. High treatment effect with high habituation in 50% of the population\n15. High treatment effect with high habituation in 100% of the population\nB\nFull-pooling algorithm - Bayesian Linear Regression (BLR)\nFor a given user i at decision point t, the action-centered training model for Bayesian Linear Regression (BLR) is defined as:\nR(t)\ni\n= g(S(t)\ni )T α + (a(t)\ni\n− π(t)\ni )f(S(t)\ni )T β + (π(t)\ni )f(S(t)\ni )T γ + ϵ(t)\ni\n(42)\nwhere π(t)\ni\nis the probability of taking an active action i.e. sending an intervention message (a(t)\ni\n= 1). g(S(t)\ni ) and f(S(t)\ni )\nare the baseline and advantage vectors respectively. ϵ(t)\ni\nis the error term, assumed to be independent and identically distributed\n(i.i.d.) Gaussian noise with mean 0 and variance σ2\nϵ .\nThe probability π(t)\ni\nof sending a intervention message (a(t)\ni\n= 1) is calculated as:\nπ(t)\ni\n= E ˜β∼N (µ(t−1)\npost\n,Σ(t−1)\npost\n)[ρ(f(S(t)\ni )T ˜β)|H(t−1)\n1:m , S(t)\ni ]\n(43)\nwhere H(T )\ni\n= {S(1)\ni\n, a(1)\ni , R(1)\ni , · · · , S(T )\ni\n, a(T )\ni\n, R(T )\ni\n} is the trajectory for a given user i upto time t.\nThe Bayesian model requires the specification of prior values for α, β and γ. While we do not assume the terms α, β and γ\nto be independent, we have no information about the correlation between the α, β and γ terms, and hence we set the correlation\nto be 0 in our prior variance Σprior. Hence, Σprior = diag(Σα, Σβ, Σβ) is the prior variance of all the parameters (note that\nthe posterior var-covariance matrix will have off diagonal elements). Next µprior = (µα, µβ, µβ) is the prior mean of all the\nparameters. The prior of each parameter is assumed to be normal and given as:\nα ∼ N(µα, Σα)\n(44)\nβ ∼ N(µβ, Σβ)\n(45)\nγ ∼ N(µβ, Σβ)\n(46)\nSince the priors are Gaussian, and the error term is Gaussian, the posterior distribution of all the parameters, given the\nhistory of state-action-reward tuples (trajectory) up until time t is also Gaussian. Let us denote all the parameters using θT =\n(αT , βT , γT ). The posterior distribution of θ given the current history H(t) = {S(τ), a(τ), R(τ)}τ≤t (data of all users) is\ndenoted by N(µ(t)\npost, Σ(t)\npost), where:\nΣ(t)\npost =\n\u0012 1\nσ2ϵ\nΦT\n1:tΦ1:t + Σ−1\nprior\n\u0013−1\n(47)\nµ(t)\npost = Σ(t)\npost\n\u0012 1\nσ2ϵ\nΦT\n1:tR1:t + Σ−1\npriorµprior\n\u0013\n(48)\nΦ1:t is a stacked vector of Φ(S, a)T = [g(S(t))T , (a(t) − π(t))f(S(t))T , π(t)f(S(t))T ] for all users, for all decision points\nfrom 1 to t. R1:t is a stacked vector of rewards for all users, for all decision points from 1 to t. σ2\nϵ is the noise variance,\nΣprior = diag(Σα, Σβ, Σβ) is the prior variance of all the parameters, and µprior = (µα, µβ, µβ) is the prior mean of all the\nparameters.\nWe initialize the noise variance by σ2\nϵ = 0.85; see Equation (49) for the update equation. See Appendix D for the choice of\n0.85. Then we update the noise variance by solving the following optimization problem, that is maximizing the marginal (log)\nlikelihood of the observed rewards, marginalized over the parameters:\nσ2\nϵ = argmax[− log(det(X + yA)) + mt log(y) − y\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2\n+ (Xµprior + yB)T (X + yA)−1(Xµprior + yB)]\n(49)\nwhere X = Σ−1\nprior, y =\n1\nσ2\nϵ , A = ΦT\n1:tΦ1:t, B = ΦT\n1:tR1:t, t is current total number of decision points, and m is the total\nnumber of users.\nC\nBaseline and Advantage Functions\nThe baseline and advantage functions g(S) and f(S) are defined as follows:\ng(S) = [1, S1, S2, S3, S1S2, S2S3, S1S3, S1S2S3]\n(50)\nf(S) = [1, S1, S2, S3, S1S2, S2S3, S1S3, S1S2S3]\n(51)\nD\nInitialization values and Bayesian Priors\nParameter\nSignificance (S/I/M)\nMean\nVariance\nIntercept\nS\n2.12\n(0.78)2\nS1\nI\n0.00\n(0.38)2\nS2\nM\n0.00\n(0.62)2\nS3\nS\n−0.69\n(0.98)2\nS1S2\nM\n0.00\n(0.16)2\nS1S3\nI\n0.00\n(0.1)2\nS2S3\nM\n0.00\n(0.16)2\nS1S2S3\nM\n0.00\n(0.1)2\na Intercept\nI\n0.00\n(0.27)2\naS1\nI\n0.00\n(0.33)2\naS2\nM\n0.00\n(0.3)2\naS3\nI\n0.00\n(0.32)2\naS1S2\nM\n0.00\n(0.1)2\naS1S3\nI\n0.00\n(0.1)2\naS2S3\nM\n0.00\n(0.1)2\naS1S2S3\nM\n0.00\n(0.1)2\nTable 4: Prior values for the RL algorithm informed using the SARA data set. The significant column signifies the significant terms\nfound during the analysis using S, insignificant terms using I, and missing terms with M. Values are rounded to the nearest 2 decimal places.\nThis section details how we calculate the initial values and priors using SARA data (Sec. A) to estimate the reward, given\nthe state. First, we define the model for the conditional mean of the reward R(t)\ni\nof a given user i at time t, given the current\nstate S = {S1, S2, S3} (dropping the user index and time for brevity):\nE\nh\nR(t)\ni |S1, S2, S3, a\ni\n= α0 + α1S1 + α2S2 + α3S3 + α4S1S2 + α5S1S3 + α6S2S3 + α7S1S2S3\n+ a(β0 + β1S1 + β2S2 + β3S3 + β4S1S2 + β5S1S3 + β6S2S3 + β7S1S2S3).\n(52)\nNote that the reward model in Eq. 52 is non-parametric, i.e. there are 16 unique weights, and E\nh\nR(t)\ni |S1, S2, S3, a\ni\nhas 16\ndimensions. We follow the methods described in [Liao et al., 2019] to form our priors using the SARA dataset, which involved\nfitting linear models like Eq. 52 using GEE. We do a complete-case analysis on the SARA data, and transform it into State-\nAction-Reward tuples to mimic our RL algorithm setup. However, as noted in the previous section, the SARA dataset does\nnot account for the entire state-space, specifically S2, i.e. time of day, as the users in the study were requested to self-report\non a daily survey just once a day. To that end, we omit all terms from Eq. 52 which contain S2 when forming our analysis to\ndetermine priors. Hence, our reward estimation model while forming priors is given as:\nE\nh\nR(t)\ni |S1, S3, a\ni\n= α0 + α1S1 + α3S3 + α5S1S3 + a(β0 + β1S1 + β3S3 + β5S1S3).\n(53)\nD.1\nState formation and imputation\nWe operationalize the states of the RL algorithm from the SARA dataset as follows:\n• S1: Same as defined in Section 2\n• S2: This is not present in the SARA data set.\n• S3: Same as defined in Section 2\nWe work with complete-case data. So, whenever there is missing “cannabis use” in the past decision point (since Y = 1 only),\nwe impute and set S3 to 1. This is because participants in the MiWaves study are expected to often use cannabis (at least 3\ntimes a week). Also, since the data is complete-case, we do not use the first day’s (i.e. first data point for a user) reward to fit\nthe model, as we choose to not impute the state of the user on the first day while forming our priors.\nD.2\nFeature significance\nWe run a GEE linear regression analysis with robust standard errors to determine the feature significance. We categorize a\nfeature to be significant when it’s corresponding p-value is less than 0.05. The GEE Regression results are summarized in Fig.\n7. Using the criteria mentioned above, we classify the intercept (α0), and the S3 term (α3) to be significant.\nFigure 7: GEE Results\nD.3\nInitial value of Noise variance (σ2\nϵ )\nTo form an initial of the noise variance, we fit a GEE linear regression model (Eq. 53) per user, and compute the residuals.\nWe set the initial noise variance value as the average of the variance of the residuals across the N = 42 user models; that is\nσ2\nϵ,0 = 0.85 in our simulations.\nParameter\nValue\nσ2\nϵ,0\n0.85\nTable 5: Initial value of noise variance\n(a) Variant 0: Posterior means\n(b) Variant 0: Posterior variance\nFigure 8: Average posterior means and variances in the minimal treatment effect environment with no habituation\nD.4\nPrior mean for α and β\nTo compute the prior means, we first fit a single GEE regression model (Eq. 53) across all the users combined. For the\nsignificant features (intercept and S3), we choose the point estimates of α0, and α3 to be their corresponding feature prior\nmeans. For the insignificant features, we set the prior mean to 0 (α1, α5, β0, β1, β3, and β5). For the prior means of the weights\non the S2 terms which are not present in the GEE model, we also set them to 0.\nD.5\nPrior standard deviation for α and β\nTo compute the prior standard deviation, we first fit user-specific GEE regression models (Equation 53), one per user. We\nchoose the standard deviation of significant features, corresponding to α0, and α3, across the N = 42 user models to be their\ncorresponding prior standard deviations. For the insignificant features, we choose the standard deviation of α1, α5, β0, β1,\nβ3, and β5 across the N = 42 user models, and set the prior standard deviation to be half of their corresponding values. The\nrationale behind setting the mean to 0 and shrinking the prior standard deviation is to ensure stability in the algorithm; we do\nnot expect these terms to play a significant impact on the action selection, unless there is a strong signal in the user data during\nthe trial. In other words, we are reducing the SD of the non-significant weights because we want to provide more shrinkage to\nthe prior mean of 0 (i.e. more data is needed to overcome the prior). For the prior standard deviation of the S2 terms which\nare in the baseline (i.e. all the α terms), we set it to the average of the prior standard deviations of the other baseline α terms.\nSimilarly, we set the prior standard deviation of the S2 advantage (β) terms as the average of the prior standard deviations of\nthe other advantage β terms. We further shrink the standard deviation of all the two-way interactions by a factor of 4, and the\nthree-way interactions by a factor of 8 - with a minimum prior standard deviation of 0.1. Recall that we expect little to no signal\nfrom the data wrt. the higher order interactions of the binary variables, thus this decision. Unless there is strong evidence in the\nuser data during the MiWaves pilot study, we do not expect these interaction terms to play a significant role in action selection.\nD.6\nInitial values for variance of random effects Σu\nreBandit assumes that each weight αi,j is split into a population term αi,pop and an individual term ui,j (random effects) for\na given feature i and user j. These random effects capture the user heterogeneity in the study population, and the variance of\nthe random effects are denoted by Σu. We set the initial values for Σu to be Σu,0 = (0.1)2 × IK, where K is the number\nof random effects terms. We shrink this variance as we allow the data to provide evidence for user heterogeneity. We set the\noff-diagonal entries in the initial covariance matrix to 0 as we do not have any information about the covariance between the\nrandom effects.\nD.7\nEmpirical check for prior variance shrinkage\nDuring on our prior formulation, we shrink the variance of higher order interactions. This shrinkage allows us to deploy\ncomplex models without many drawbacks. However, it relies on the idea that if there is evidence of higher order interactions\nin the study/data, the algorithm will learn and identify that signal. This empirical check is to establish that our algorithms are\nable to do so, when the study environment does provide evidence of such interactions. In this empirical check, we are primarily\nconcerned with the variance of the action intercept or the advantage intercept coefficient.\n(a) Variant 0: Posterior means\n(b) Variant 0: Posterior variance\nFigure 9: Average posterior means and variances in high treatment effect environment with no habituation\nFirst, we run experiments with minimal effect/signal. To that end, we run 500 simulated clinical trials in the minimal\ntreatment effect environment with no habituation, with each trial consisting of N = 120 simulated users and lasting T = 60\ndecision points (30 days). We use smooth posterior sampling to determine the action selection probability (described in Section\n4.2). We also use the BLR algorithm (described in Appendix B). Next, we run experiments with evidence/signal in the data, in\norder to check whether we have shrunk the prior variances too much. To that end, we check if our pooled algorithm is able to\nidentify the signal in the high treatment effect environment with no habituation.\nWe compare the three variants in terms of the learnt average posterior means and variances (Figure 9). We see that the\nalgorithm is able to learn the signal added to the action intercept weight. This demonstrates that the shrinkage of the prior\nvariance of the action intercept term is appropriate for learning, when the user data supplies strong evidence of a signal.\nEvidence of the algorithm being able to pick up these signals allow us to run more complex models (in terms of the\nbaseline g(S) and advantage f(S) functions) without many drawbacks. We are able to do so because we shrink the prior\nvariances of the high order interactions by a lot.\nE\nSmooth Posterior Sampling\nRecall that the probability of sending an intervention message is computed as:\nπ(t)\ni\n= E ˜β∼N (µ(t−1)\npost\n,Σ(t−1)\npost\n)[ρ(f(S(t)\ni )T ˜β)|H(t−1)\n1:N , S(t)\ni ]\n(54)\nρ(x) = Lmin +\nLmax − Lmin\n1 + c exp(−bx)\n(55)\nwhere Lmin = 0.2 and Lmax = 0.8 are the lower and upper clipping probabilities. For now, we set c = 5. Larger values of\nc > 0 shifts the value of ρ(0) to the right. This choice implies that ρ(0) = 0.3. Intuitively, the probability of taking an action\nwhen treatment effect is 0, is 0.3.\nWe define b =\nB\nσres . Larger values of b > 0 makes the slope of the curve more “steep”. σres is the reward residual standard\ndeviation obtained from fitting our reward model on data from SARA. We have σres in the denominator, as it helps us standardize\nthe treatment effect. The intuition is that the denominator in ρ(f(s)β) has the term β/σres in the exponential, which becomes\nthe standardized treatment effect.\nWe currently set σres = 0.95. In order to arrive at this value, we first simulated each of N = 42 unique users from SARA\nin a MiWaves simulation with low treatment effect and no habituation effect. The reason we introduce low treatment effect, is\nbecause we see that SARA itself had minimal to no treatment effect. Actions are selected with 0.5 probability. We ran 500 such\nsimulations, and fit each simulation’s data into a GEE model, and computed the standard deviation of the residuals. 0.95 was\nthe mean of the residual standard deviation across these 500 simulations.\nWe also set B = 20. We arrive at this value after comparing the performance of reBandit with B = 10 and B = 20 across\nmultiple simulation environments. Hence, we set b =\nB\nσres =\n20\n0.95 = 21.053.\nF\nImplicit state-features\nThe random effects model, described in Sec 4.1, induces implicit state-features. The posterior distribution for an individual\nuser can be written in terms of converging population statistics (which use the data across the entire trial population), and the\nindividual user’s data (morphed into an implicit feature). While this implicit feature for a user is not apparent from the posterior\nexpressions in Eq. 14 and Eq. 15, we derive the joint posterior of the parameters θpop and ui (refer Appendix F.1). It turns out\nthat the posterior distribution of\n\u0014θpop\nui\n\u0015\nis jointly normal:\n\u0014θpop\nui\n\u0015\n∼ N\n\u0012 \u0014\nλ\nM\n\u0015\n,\n\u0014\nV1\nV2\nV3\nV4\n\u0015 \u0013\n(56)\nwhere\nU = ψ−1\ni\n(−Aiλ + Bi)\n(57)\nV1 = (mE)−1\n(58)\nV2 = −(mE)−1Aiψ−1\ni\n(59)\nV3 = −ψ−1\ni\nAi(mE)−1\n(60)\nV4 = σ2\nϵ ψ−1\ni\n+ ψ−1\ni\nAi(mE)−1Aiψ−1\ni\n(61)\nλ = E−1\n\u0012 1\nmΣ−1\npriorµprior + 1\nσ2ϵ\nζ1 − 1\nσ2ϵ\nζ2\n\u0013\n(62)\nE = 1\nmΣ−1\nprior + 1\nσ2ϵ\nζ3 − 1\nσ2ϵ\nζ4\n(63)\nζ1 = 1\nm\nX\ni\nBi\n(64)\nζ2 = 1\nm\nX\ni\nAiψ−1\ni\nBi\n(65)\nζ3 = 1\nm\nX\ni\nAi\n(66)\nζ4 = 1\nm\nX\ni\nAiψ−1\ni\nAi\n(67)\nψi = σ2\nϵ Σ−1\nu\n+ Ai\n(68)\nAi =\nt\nX\nτ=1\nΦiτΦT\niτ\n(69)\nBi =\nt\nX\nτ=1\nΦiτR(τ)\ni\n(70)\nWe observe that the random effects model induces the state-features Ai and Bi described in Eq. 69 and Eq. 70 for each user,\nwhich depends on the data for that particular user. It is also evident that the posterior depends on the statistics ζ1, ζ2, ζ3, and\nζ4 - all of which can be expected to converge in probability as m increases. These implicit features facilitate the after-study\nanalysis of reBandit, which is left for future work.\nF.1\nDerivation\nTo start, the mixed effects model can be written as:\nR(t)\ni\n= ΦT\ni,tθi + ϵi,t\n(71)\n= ΦT\ni,t(θpop + ui) + ϵi,t\n(72)\nAnd by definition, the random effects are normal with mean 0 and variance Σu, i.e. ui ∼ N(0, Σu).\nAs described before, we assume the noise to be Gaussian i.e. ϵ ∼ N(0, σ2\nϵ Imt). We also assume the following prior on the\npopulation term:\nθpop ∼ N(µprior, Σprior)\n(73)\nTherefore, we can write the prior distribution as:\nlog(prior) ∝ −1\n2(θpop − µprior)T Σ−1\nprior(θpop − µprior) +\nm\nX\ni=1\n−1\n2uT\ni Σ−1\nu ui\n(74)\nThe likelihood is given as:\nlog(likelihood) ∝\nm\nX\ni=1\nt\nX\nτ=1\n− 1\n2σ2ϵ\n(Riτ − ΦT\niτθpop − ΦT\niτui)2\n(75)\nGiven the prior and the likelihood, the posterior distribution is given as follows:\nlog(posterior) = const − 1\n2(θpop − µprior)T Σ−1\nprior(θpop − µprior) +\nm\nX\ni=1\n−1\n2uT\ni Σ−1\nu ui\n−\nm\nX\ni=1\nt\nX\nτ=1\n− 1\n2σ2ϵ\n(R(τ)\ni\n− ΦT\niτθpop − ΦT\niτui)2\n(76)\nDefine Ai = P\nτ ΦiτΦT\niτ, and Bi = P\nτ ΦiτRiτ. And,\nA =\n\n\nPt\nτ=1 Φ1τΦT\n1τ\n0\n· · ·\n0\n0\nPt\nτ=1 Φ2τΦT\n2τ\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nPt\nτ=1 ΦmτΦT\nmτ\n\n\n(77)\nB =\n\n\nB1...\nBm\n\n =\n\n\nPt\nτ=1 Φ1τR(τ)\n1\n...\nPt\nτ=1 ΦmτR(τ)\nm\n\n\n(78)\nThen, rewriting the posterior, we get:\n= const − 1\n2(θpop − µprior)T Σ−1\nprior(θpop − µprior) − 1\n2\nm\nX\ni=1\nuT\ni Σ−1\nu ui\n−\n1\n2σ2ϵ\nθT\npop\n\u0012 X\ni,τ\nAi\n\u0013\nθpop −\n1\n2σ2ϵ\nX\ni\nuT\ni Aiui\n− 1\nσ2ϵ\nθT\npop\n\u0012 X\ni\nAiui\n\u0013\n+ 1\nσ2ϵ\nX\ni\nBT\ni θpop + 1\nσ2ϵ\nX\ni\nBT\ni ui\n(79)\n= const − 1\n2θT\npop\n\u0012\nΣ−1\nprior + 1\nσ2ϵ\nX\ni\nAi\n\u0013\nθpop +\n\u0012\nΣ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBT\ni\n\u0013\nθpop\n− 1\nσ2ϵ\nθT\npop ˜\nAu − 1\n2uT Du + 1\nσ2ϵ\nBT u\n(80)\nwhere ˜\nA = [A1, . . . , Am] ∈ Rp×mp, p = dim(Φit)\nand\nD =\n\n\nΣ−1\nu\n+\n1\nσ2\nϵ A1\n. . .\n0\n0\n0\nΣ−1\nu\n+\n1\nσ2ϵ A2\n. . .\n0\n...\n...\n...\n...\n0\n0\n. . .\nΣ−1\nu\n+\n1\nσ2ϵ Am\n\n\n(81)\nContinuing the derivation,\nlog(posterior) = const − 1\n2\n\u0014θpop\nu\n\u0015T \"\nΣ−1\nprior +\n1\nσ2\nϵ\nP\ni Ai\n1\nσ2\nϵ\n˜A\n1\nσ2ϵ\n˜\nAT\nD\n# \u0014θpop\nu\n\u0015\n(82)\n+\n\"\nΣ−1\npriorµprior +\n1\nσ2ϵ\nP\ni Bi\n1\nσ2\nϵ B\n#T \u0014θpop\nu\n\u0015\n(83)\nContinuing the derivation,\nlog(posterior) = const − 1\n2\n\u0014θpop\nu\n\u0015T \"\nΣ−1\nprior +\n1\nσ2ϵ\nP\ni Ai\n1\nσ2ϵ\n˜A\n1\nσ2ϵ\n˜\nAT\nD\n# \u0014θpop\nu\n\u0015\n(84)\n+\n\"\nΣ−1\npriorµprior +\n1\nσ2\nϵ\nP\ni Bi\n1\nσ2ϵ B\n#T \u0014θpop\nu\n\u0015\n(85)\nDefine\nI =\n\"\nΣ−1\nprior +\n1\nσ2ϵ\nP\ni Ai\n1\nσ2ϵ\n˜A\n1\nσ2ϵ\n˜\nAT\nD\n#\n(86)\nJT =\n\"\nΣ−1\npriorµprior +\n1\nσ2ϵ\nP\ni Bi\n1\nσ2ϵ B\n#T\n(87)\nThen we get,\nlog(posterior) = const − 1\n2\n\u0014θpop\nu\n\u0015T\nI\n\u0014θpop\nu\n\u0015\n+ JT\n\u0014θpop\nu\n\u0015\n(88)\n= const − 1\n2\n \u0014θpop\nu\n\u0015\n− I−1J\n!T\nI\n \u0014θpop\nu\n\u0015\n− I−1J\n!\n(89)\nHence, the joint posterior distribution of θpop and u is jointly normal, as is given as:\n\u0014θpop\nu\n\u0015\n∼ N(I−1J, I−1)\n(90)\nNow, we can write,\nI =\n\"\nΣ−1\nprior +\n1\nσ2ϵ\nP\ni Ai\n1\nσ2ϵ\n˜A\n1\nσ2ϵ\n˜\nAT\nD\n#\n=\n\u0014\nC22\nC21\nC12\nC11\n\u0015\n(91)\n=⇒ I−1 =\n\"\n\u0000C22 − C21C−1\n11 C12\n\u0001−1\n−\n\u0000C22 − C21C−1\n11 C12\n\u0001−1 C21C−1\n11\n−C−1\n11 C12\n\u0000C22 − C21C−1\n11 C12\n\u0001−1\nC−1\n11 + C−1\n11 C12\n\u0000C22 − C21C−1\n11 C12\n\u0001−1 C21C−1\n11\n#\n(92)\n≜\n\u0014\n(mE)−1\n−(mE)−1C21C−1\n11\n−C−1\n11 C12(mE)−1\nC−1\n11 + C−1\n11 C12(mE)−1C21C−1\n11\n\u0015\n(93)\nWe evaluate the expression for each block in the block matrix. Then, we get\nmE = Σ−1\nprior + 1\nσ2ϵ\nX\ni\nAi − 1\nσ4ϵ\n˜AD ˜A\nT\n(94)\n= Σ−1\nprior + 1\nσ2ϵ\nX\ni\nAi − 1\nσ4ϵ\nX\ni\nAi(Σ−1\nu\n+ 1\nσ2ϵ\nAi)−1Ai\n(95)\n= Σ−1\nprior + 1\nσ2ϵ\nX\ni\nAi − 1\nσ2ϵ\nX\ni\nAiψ−1\ni\nAi\n(96)\n=⇒ E = 1\nm\n\u0012\nΣ−1\nprior + 1\nσ2ϵ\nX\ni\nAi − 1\nσ2ϵ\nX\ni\nAiψ−1\ni\nAi\n\u0013\n(97)\nwhere ψi = σ2\nϵ Σ−1\nu\n+ Ai.\nNext,\n−C−1\n11 C12(mE)−1 = − 1\nσ2ϵ\nD−1 ˜A\nT (mE)−1\n(98)\n= −\n\n\nψ−1\n1 A1\n...\nψ−1\nm Am\n\n (mE)−1\n(99)\nNext,\n−(mE)−1C21C−1\n11 = −(mE)−1 \u0002\nA1ψ−1\n1\n. . .\nAmψ−1\nm\n\u0003\n(100)\nAnd finally,\nC−1\n11 + C−1\n11 C12(mE)−1C21C−1\n11 = D−1 +\n\"\nψ−1\ni\nAi(mE)−1Ajψ−1\nj\n#\ni,j∈[m]\n(101)\nPlugging in these values, we get\nI−1J =\n\u0014\n(mE)−1\n−(mE)−1C21C−1\n11\n−C−1\n11 C12(mE)−1\nC−1\n11 + C−1\n11 C12(mE)−1C21C−1\n11\n\u0015 \"\nΣ−1\npriorµprior +\n1\nσ2ϵ\nP\ni Bi\n1\nσ2ϵ B\n#\n(102)\n=\n\"\n(mE)−1(Σ−1\npriorµprior +\n1\nσ2ϵ\nP\ni Bi) −\n1\nσ2ϵ (mE)−1C21C−1\n11 B\n−C−1\n11 C12(mE)−1(Σ−1\npriorµprior +\n1\nσ2ϵ\nP\ni Bi) +\n1\nσ2ϵ (C−1\n11 + C−1\n11 C12(mE)−1C21C−1\n11 )B\n#\n(103)\nThen, the upper block matrix evaluates to:\n(mE)−1(Σ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBi) − 1\nσ2ϵ\n(mE)−1C21C−1\n11 B\n(104)\n= (mE)−1(Σ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBi) − 1\nσ2ϵ\n(mE)−1 \u0002\nA1ψ−1\n1\n. . .\nAmψ−1\nm\n\u0003\nB\n(105)\n= (mE)−1(Σ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBi) − 1\nσ2ϵ\n(mE)−1 X\ni\nAiψ−1\ni\nBi\n(106)\n= (E)−1 1\nm(Σ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBi) − 1\nσ2ϵ\n(E)−1 1\nm\nX\ni\nAiψ−1\ni\nBi\n(107)\nNext, the bottom term in the block matrix of 103 evaluates to:\n− C−1\n11 C12 (mE)−1\n \nΣ−1\npriorµprior + 1\nσ2ϵ\nX\ni\nBi\n!\n|\n{z\n}\n≜f\n+ 1\nσ2ϵ\n\u0000C−1\n11 + C−1\n11 C22(mE)−1C21C−1\n11\n\u0001\nB\n(108)\n= −C−1\n11 C12f + 1\nσ2ϵ\nD−1B + 1\nσ2ϵ\n\"\nψ−1\ni\nAi(mE)−1Ajψ−1\nj\n#\ni,j∈[m]\nB\n(109)\n= −\n\n\nψ−1\n1 A1\n...\nψ−1\nm Am\n\n f +\n\n\nψ−1\n1 B1\n...\nψ−1\nm Bm\n\n +\n\n\nψ−1\n1 A1\n...\nψ−1\nm Am\n\n 1\nσ2ϵ\n(mE)−1 X\ni\nAiψ−1\ni\nBi\n|\n{z\n}\n≜g\n(110)\n=\n\n\nψ−1\n1 A1\n...\nψ−1\nm Am\n\n (g − f) +\n\n\nψ−1\n1 B1\n...\nψ−1\nm Bm\n\n\n(111)\nThus, posterior distribution of\n\u0014θpop\nui\n\u0015\nis jointly normal with posterior mean:\n\u0014θpop\nui\n\u0015\n∼ N\n\u0012 \u0014\nλ\nψ−1\ni\n(−Aiλ + Bi)\n\u0015\n,\n\u0014\n(mE)−1\n−(mE)−1Aiψ−1\ni\n−ψ−1\ni\nAi(mE)−1\nσ2\nϵ ψ−1\ni\n+ ψ−1\ni\nAi(mE)−1Aiψ−1\ni\n\u0015 \u0013\n(112)\nwhere\nλ = E−1\n\u0012 1\nmΣ−1\npriorµprior + 1\nσ2ϵ\nζ1 − 1\nσ2ϵ\nζ2\n\u0013\n(113)\nE = 1\nmΣ−1\nprior + 1\nσ2ϵ\nζ3 − 1\nσ2ϵ\nζ4\n(114)\nζ1 = 1\nm\nX\ni\nBi\n(115)\nζ2 = 1\nm\nX\ni\nAiψ−1\ni\nBi\n(116)\nζ3 = 1\nm\nX\ni\nAi\n(117)\nζ4 = 1\nm\nX\ni\nAiψ−1\ni\nAi\n(118)\nψi = σ2\nϵ Σ−1\nu\n+ Ai\n(119)\nAi =\nX\nτ\nΦiτΦT\niτ\n(120)\nBi =\nX\nτ\nΦiτR(τ)\ni\n(121)\nG\nPosterior update derivation\nThe posterior distribution can be derived as follows. Given the following,\nθ = θpop + u\n(122)\nθpop ∼ N(µprior, Σprior)\n(123)\nui ∼ N(0, Σu)\n(124)\n=⇒ θ ∼ N(µθ, Σθ)\n(125)\nwhere\nµθ =\n\n\nµprior\nµprior\n...\nµprior\n\n\n(126)\nΣθ =\n\n\nΣprior + Σu\nΣprior\n· · ·\nΣprior\nΣprior\nΣprior + Σu\n· · ·\nΣprior\n...\n...\n...\n...\nΣprior\nΣprior\n· · ·\nΣprior + Σu\n\n\n(127)\nHere dim(µprior) = p, dim(Σprior) = p × p, dim(µθ) = pm and dim(Σθ) = pm × pm, where p = dim(Φ). Then, define\n˜Σθ,t =\n\n\nΣprior + Σu,t\nΣprior\n· · ·\nΣprior\nΣprior\nΣprior + Σu,t\n· · ·\nΣprior\n...\n...\n...\n...\nΣprior\nΣprior\n· · ·\nΣprior + Σu,t\n\n\n(128)\nSo, we can write:\nprior ∝ exp\n\u0012\n− 1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ)\n\u0013\n(129)\nlikelihood ∝\nY\ni,τ\nexp\n\u0012\n−\n1\n2σ2\nϵ,t\n(R(τ)\ni\n− ΦT\ni,τθi)2\n\u0013\n(130)\nposterior ∝ prior × likelihood\n(131)\n=⇒ log(posterior) ∝ −1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ) −\n1\n2σ2\nϵ,t\nm\nX\ni=1\nt\nX\nτ=1\n(R(τ)\ni\n− ΦT\ni,τθi)2\n(132)\n∝ −1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ) −\n1\n2σ2\nϵ,t\nm\nX\ni=1\n(θi\nT (\nt\nX\nτ=1\nΦiτΦT\ni,τ)θi\n− 2(\nn\nX\nτ=1\nΦiτR(τ)\ni\n)T θi)\n(133)\nNow, we can define:\nA =\n\n\nPt\nτ=1 Φ1τΦT\n1τ\n0\n· · ·\n0\n0\nPt\nτ=1 Φ2τΦT\n2τ\n· · ·\n0\n...\n...\n...\n...\n0\n0\n· · ·\nPt\nτ=1 ΦmτΦT\nmτ\n\n\n(134)\nB =\n\n\nPt\nτ=1 Φ1τR(τ)\n1\n...\nPt\nτ=1 ΦmτR(τ)\nm\n\n\n(135)\nThen, we can rewrite the log posterior as:\nlog(posterior) ∝ −1\n2θT ˜Σ−1\nθ,tθ + (˜Σ−1\nθ,tµθ)T θ −\n1\n2σ2\nϵ,t\nθT Aθ +\n1\nσ2\nϵ,t\nBT θ\n(136)\n= −1\n2θT\n\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013\nθ +\n\u0012\n(˜Σ−1\nθ,tµθ)T +\n1\nσ2\nϵ,t\nBT\n\u0013\nθ\n(137)\nTherefore, we can write the posterior mean and variance of the parameters θ at time t as:\nµpost,t =\n\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\u0012\n˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB\n\u0013\n(138)\nΣpost,t =\n\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\n(139)\nH\nHyper-parameter update derivation\nUsing Empirical Bayes to maximize the marginal likelihood of observed rewards, marginalized over the parameters θ, we get:\nl(Σu, σ2\nϵ,t; H) =\nZ\nθ\nY\nτ∈[t]\nY\ni∈[m]\nl(R(τ)\ni\n|θi) Pr(θ|Σu, σ2\nϵ,t)dθ\n(140)\nNow,\nPr(θ|Σu, σ2\nϵ,t)dθ = Pr(θ|Σu)dθ\n(141)\n=\n1\nq\n(2π)2m det(˜Σθ,t)\nexp\n\u0012\n−1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ)\n\u0013\n(142)\nAnd,\nY\nτ∈[t]\nY\ni∈[m]\nl(R(τ)\ni\n|θi) =\nY\nτ∈[t]\nY\ni∈[m]\n1\nq\n2πσ2\nϵ,t\nexp\n\u0012\n−\n1\n2σ2\nϵ,t\n(R(τ)\ni\n− ΦT\niτθi)2\n\u0013\n(143)\n=\n1\n(2πσ2\nϵ,t)\nmt\n2 exp\n\u0012\n−\n1\n2σ2\nϵ,t\n(θT Aθ − 2BT θ +\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2)\n\u0013\n(144)\nTherefore, we can write:\nl(Σu, σ2\nϵ,t; H) =\nZ\nθ\n1\nq\n(2π)2m det(˜Σθ,t)\nexp\n\u0012\n−1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ)\n\u0013\n1\n(2πσ2\nϵ,t)\nmt\n2 exp\n\u0012\n−\n1\n2σ2\nϵ,t\n(θT Aθ − 2BT θ +\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2)\n\u0013\ndθ\n(145)\n=\nZ\nθ\nC1\nz\n}|\n{\n\u0012\n1\nq\n(2π)2m det(˜Σθ,t)\n\u0013\n·\n\u0012\n1\n(2πσ2\nϵ,t)\nmt\n2\n\u0013\n· exp\n\u0012\n−\n1\n2σ2\nϵ,t\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2\n\u0013\nexp\n\u0012\n−1\n2(θ − µθ)T ˜Σ−1\nθ,t(θ − µθ) −\n1\n2σ2\nϵ,t\n(θT Aθ − 2BT θ)\n\u0013\ndθ\n(146)\n= C1\nZ\nθ\nC2\nz\n}|\n{\nexp\n\u00121\n2(˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB)T (˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA)−1(˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB) − 1\n2µT\nθ ˜Σ−1\nθ,tµθ\n\u0013\nexp\n\u0012\n− 1\n2\n\u0012\nθ −\n\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\u0012\n˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB\n\u0013\u0013T\n\u0012\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\u0013−1\u0012\nθ −\n\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\u0012\n˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB\n\u0013\u0013\u0013\ndθ\n(147)\n= C1C2\ns\n(2π)2m det\n\u0012\u0012\n˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA\n\u0013−1\u0013\n(148)\n=\n \n1\ns\n(2π)mt det\n\u0012\nI +\n1\nσ2\nϵ,t\n˜Σθ,tA\n\u0013\n! \n1\n(σ2\nϵ,t)\nmt\n2\n!\nexp\n\u0012\n−\n1\n2σ2\nϵ,t\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2 − 1\n2µT\nθ ˜Σ−1\nθ,tµθ\n\u0013\nexp\n\u00121\n2(˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB)T (˜Σ−1\nθ,t +\n1\nσ2\nϵ,t\nA)−1(˜Σ−1\nθ,tµθ +\n1\nσ2\nϵ,t\nB)\n\u0013\n(149)\n=⇒ log(l(Σu, σ2\nϵ,t; H)) ∝ log(det(X)) − log(det(X + yA)) + mt log(y) − y\nX\nτ∈[t]\nX\ni∈[m]\n(R(τ)\ni\n)2\n− µT\nθ Xµθ + (Xµθ + yB)T (X + yA)−1(Xµθ + yB)\n(150)\nwhere X = ˜Σ−1\nθ,t and y =\n1\nσ2\nϵ,t .\n"
}