{
    "optim": "reBandit: Random Effects based Online RL algorithm for Reducing Cannabis Use Susobhan Ghosh1 , Yongyi Guo2 , Pei-Yao Hung3 , Lara Coughlin4 , Erin Bonar4 , Inbal Nahum-Shani3 , Maureen Walton4 , Susan Murphy1 1Department of Computer Science, Harvard University 2Department of Statistics, University of Wisconsin-Madison 3Institute for Social Research, University of Michigan 4Department of Psychiatry, University of Michigan susobhan ghosh@g.harvard.edu, guo98@wisc.edu, peiyaoh@umich.edu, laraco@med.umich.edu, erinbona@med.umich.edu, inbal@umich.edu, waltonma@med.umich.edu, yongyiguo@fas.harvard.edu Abstract The escalating prevalence of cannabis use, and as- sociated cannabis-use disorder (CUD), poses a sig- nificant public health challenge globally. With a notably wide treatment gap, especially among emerging adults (EAs; ages 18-25), addressing cannabis use and CUD remains a pivotal objec- tive within the 2030 United Nations Agenda for Sustainable Development Goals (SDG). In this work, we develop an online reinforcement learn- ing (RL) algorithm called reBandit which will be utilized in a mobile health study to deliver per- sonalized mobile health interventions aimed at re- ducing cannabis use among EAs. reBandit utilizes random effects and informative Bayesian priors to learn quickly and efficiently in noisy mobile health environments. Moreover, reBandit employs Em- pirical Bayes and optimization techniques to au- tonomously update its hyper-parameters online. To evaluate the performance of our algorithm, we con- struct a simulation testbed using data from a prior study, and compare against commonly used algo- rithms in mobile health studies. We show that reBandit performs equally well or better than all the baseline algorithms, and the performance gap widens as population heterogeneity increases in the simulation environment, proving its adeptness to adapt to diverse population of study participants. 1 Introduction & Motivation Addressing at-risk substance use, including cannabis use, is a pivotal objective within the 2030 UN Agenda for Sustain- able Development Goals (SDG)1. Within this agenda, SDG 3 focuses on ensuring healthy lives and well-being across the lifespan, yet, increasing use of cannabis, third in global preva- lence after alcohol and nicotine, threatens this goal [Pea- cock et al., 2018]. Hence, as highlighted in target 3.5 of the agenda, strengthening the prevention and treatment of cannabis use and cannabis use disorder (CUD) is crucial. Un- fortunately, this coincides with a decreased public perception of the risks associated with cannabis use, likely influenced 1https://sdgs.un.org/2030agenda by ongoing decriminalization efforts and greater access to cannabis products [Carliner et al., 2017], further worsened by one of the largest treatment gaps of any medical condi- tion, with one study showing only 5% of those with CUD receiving treatment [Lapham et al., 2019]. In the US, the prevalence of cannabis use is highest among emerging adults (EAs; age 18-25) [SAMHSA, 2023], mark- ing it as a significant concern within the growing landscape of cannabis use. Particularly worrisome is the fact that early ini- tiation of cannabis use links to an array of physical and mental health repercussions, as well as escalated risk for developing CUD [Volkow et al., 2014; Hall, 2009; Chan et al., 2021; Hasin et al., 2016]. Given that cannabis use frequently com- mences during adolescence and peaks in emerging adulthood, this is a critical developmental period for early intervention strategies to prevent transitions into CUD. Mobile health technologies, such as health apps and sen- sors, can potentially serve as support tools to help individuals manage their cannabis use. Using these tools, individuals can track their cannabis consumption, receive personalized inter- ventions, and provide objective data for early detection of is- sues. These technologies enable the delivery of just-in-time adaptive interventions (JITAIs) [Nahum-Shani et al., 2018], which leverage rapidly changing information about a person’s state and context to decide whether and how to intervene in daily life. JITAIs have been successful for many domains of behavioral health [Jaimes et al., 2015; Clarke et al., 2017; Golbus et al., 2021], whilst JITAIs for cannabis use among EAs are currently lacking evidence despite promising early data [Shrier et al., 2018]. In this work, we develop an RL algorithm called reBan- dit which will be utilized in the MiWaves pilot study (Sec- tion 1.1). MiWaves focuses on developing a JITAI for reduc- ing cannabis use among emerging adults (EAs) (ages 18-25). This JITAI leverages reBandit to determine the likelihood of delivering an intervention message. 1.1 MiWaves pilot study The MiWaves pilot study focuses on developing a personal- izing Just-In-Time Adaptive Intervention (pJITAI), namely a JITAI that integrates an RL algorithm. In this study, EAs are randomized to receive a mobile-based intervention message or no message, twice daily. The RL algorithm is designed to learn from a participant’s history, and personalize the like- arXiv:2402.17739v1  [cs.AI]  27 Feb 2024 lihood of intervention delivery based on a participant’s cur- rent context. By combining technology, behavioral science, and data-driven decision-making, MiWaves aims to empower emerging adults with the digital tools to help reduce their cannabis use. The MiWaves pilot study has been registered on ClinicalTrials.gov (NCT05824754), and is scheduled to start in March 2024. Figure 1 provides a visual overview of the MiWaves pilot study. 1.2 Challenges, Contributions and Overview Deploying RL algorithms in mHealth studies like MiWaves present a multitude of challenges that must be addressed, which include: C1 Limited Data: Many sequential decision making prob- lems in mHealth involve scarce data, forcing RL algo- rithms to learn and perform well under strict data con- straints [Trella et al., 2022]. C2 After-study analysis and evaluation: The RL algo- rithms deployed in mHealth studies need to be devel- oped in a way to facilitate after-study analysis and off- policy evaluation. C3 Autonomy and Stability: The intervention protocol in clinical studies is pre-specified. Since the RL algorithm is part of the intervention, scientists do not have the flex- ibility to change the RL algorithm while the study is run- ning. RL algorithms must exhibit robustness in the face of noisy data, ensuring consistent and reliable perfor- mance throughout the study [Trella et al., 2022]. C4 Explainability: It is imperative that RL algorithms are interpretable and comprehensible to behavioral scien- tists and medical professionals to enhance their ability to critique RL performance and to enhance the possibil- ity of larger scale implementation. C5 Delayed Effects: In mobile health studies, each inter- vention message sent to the user has a delayed effect. Users may perceive burden upon receiving an interven- tion message, which influences their future behavior. C6 Reproducibility: Any algorithm used as part of the in- tervention in a clinical study needs to be reproducible in order for health scientists to evaluate and implement the intervention package in practice. Hence, the decisions taken by the RL algorithm must be reproducible, allow- ing for scrutiny and verification of its effectiveness. To that end, we introduce reBandit, an online RL algorithm which utilizes random effects to address the challenges men- tioned above. When used as part of an RL algorithm, random effects allow the algorithm to learn quickly and efficiently by making use of other participant’s data in the population while simultaneously personalizing treatment for a given partici- pant. Moreover, reBandit employs an informative Bayesian prior formulated from pre-existing data to act as a warm- start. Carefully designed priors incorporate previous (do- main) knowledge, which help algorithms learn quickly and efficiently. Both random effects and informative priors can help reBandit to tackle challenge C1. The most commonly used RL algorithms in mHealth set- tings are bandit algorithms. In mHealth settings, predictions of the value of next state (eg. using [Jiang et al., 2015]) can be very noisy. Bandit algorithms are, thus, preferred due to their performance in such noisy environments. Moreover, they are computationally less complex, and hence are able to run stably and reliably in an online environment. Further, linear models are often considered interpretable due to their simplicity of representing the role of various factors, and can also be stably updated. reBandit utilizes both these concepts - it uses a bandit framework, along with a linear model (with random effects) to model the reward. We derive the formula to update reBandit’s parameters and hyper-parameters online (Sec 4.1). We show that we are able to autonomously update these parameters and hyper-parameters within a reasonable time-limit in an online environment. Moreover, to facilitate after-study analysis, we utilize a smooth variant of posterior sampling, and clip the probabilities of taking an action (Sec 4.2). This way, reBandit is able to overcome challenges C2, C3 and C4. To address delayed effects (C5), one can use RL algorithms to model a full Markov Decision Process (MDP). However, in mobile health settings, such approaches are not feasible due to limited data and noisy outcomes [Trella et al., 2023]. On the other hand, the classical bandit framework alone is also insufficient, as it is designed to optimize for immediate re- ward, and thus, cannot account for the delayed effects of ac- tions. To that end, we engineer the reward used to update re- Banditparameters and hyper-parameters (Sec 4.3), to account for delayed effects of actions. Finally, to tackle challenge C6, we have made our imple- mentation of reBandit publicly available2. To ensure repro- ducibility, we employ a seeded pseudo-random number gen- erator to make every stochastic decision in reBandit. Addi- tionally, all intermediate results and values used to make deci- sions are programatically stored in a database for reproducing the results obtained during any run of the algorithm. 2 RL Framework and Notation This section provides a brief overview of the Reinforcement Learning (RL) [Sutton and Barto, 2018] setup used in this work, and the specifics of the RL setup with respect to the MiWaves pilot study. We approximate the pilot study environment as a bandit environment. We represent it as a Markov Decision Process (MDP) where in the RL algorithm (eg. the mobile app) in- teracts with the environment (eg. the user). The MDP is specified by the tuple ⟨S, A, r, P, T⟩, where S is the state- space of the algorithm, A is the action-space, r(s, a) is the reward function defined for a given state s ∈ S and action a ∈ A, P(s, a, s′) is the transition function for a given state s ∈ S, action a ∈ A and next state s′ ∈ S, and T is the total number of decision times. A user trajectory is given by H(T +1) i = {S(t), a(t), R(t)}T t=1, where S(t) denotes the state at decision time t, a(t) the action assigned by the RL algo- rithm at time t, and R(t) the reward collected after selection of the action. In the case of MiWaves we have: 2https://github.com/StatisticalReinforcementLearningLab/ miwaves rl service Figure 1: Summary of the MiWaves pilot study. m = 120 EAs are expected to be recruited through social media ads. Each EA will be in the trial for 30 days, and will be asked to self-report twice daily - once in the morning and once in the evening. Upon completion or time expiration of the self-reporting, the RL algorithm will decide whether to send or not send an intervention message. Actions: Binary action space, i.e. A = {0, 1} - to not send (0) or to send (1) an intervention message. Decision points: T decision points per user. The study is set to run for D = 30 days, and each day is supposed to have 2 decision points per day. Therefore, we expect to have 60 decision points per user, i.e. T = 60. Reward: We denote the reward for user i at decision time t by R(t) i . For the MiWaves pilot study, we have discrete re- wards {0, 1, 2, 3}, which increase linearly with user engage- ment. We utilize engagement as our reward because engage- ment is critical to assess effectiveness of interventions after the study is over [Nahum-Shani et al., 2022]. States: Let us denote the state observation of the user i at decision time t as S(t) i . A given state S = (S1, S2, S3) is de- fined as a 3-tuple of the following binary variables (omitting the user and time index for brevity): • S1: Recent engagement - set to 1 if the average of past 3 observed rewards is greater than or equal to 2 (high engagement), and set to 0 otherwise (low engagement). At decision point t = 1, we set S1 to 0, as there is no engagement by the user at the start of the pilot study. • S2: Time of day of the decision point - Morning (0) vs. Evening (1). • S3: Recent cannabis use - set to 0 if the participant re- ported using cannabis during their self-monitoring, and 1 otherwise. If the user fails to self-report, we set S3 to be 0. We do so because we expect the participant in the MiWaves pilot study to be using cannabis regularly (at least 3 times a week). Overall, we represent the favorable states as 1 (not using cannabis, high engagement), and the unfavorable states as 0 (using cannabis, low engagement). Number of users: We expect m = 120 users to participate during the RL-powered MiWaves pilot study. 3 Related Work Random effects (and mixed-effects) models have been well- studied in the statistical literature [Laird, 2004; Laird and Ware, 1982; Robinson, 1991], mainly in the context of batch data analysis. Mixed-effects models comprise of fixed and random effects - hence termed mixed effects. Laird and Ware introduce the notion of random-effects models for longitu- dinal data, and describe an unified approach to fitting such models using empirical Bayes and maximum likelihood esti- mation using EM algorithm. Our work (which is in context of streaming / real-time data) draws inspiration from Laird and Ware to extend random-effects based models to real-time decision making through RL in sequential decision making problems. There has been a myriad of works in optimizing inter- vention delivery in mHealth settings in recent years [Gol- bus et al., 2021; Kramer et al., 2019; Martin et al., 2018; Rabbi et al., 2019; Trella et al., 2022; Walsh and Groarke, 2019]. Bandit algorithms are the most commonly used RL algorithms used in such high stakes online settings [Lang- ford and Zhang, 2007; Tewari and Murphy, 2017; Wang et al., 2005] due to their simplicity and stability, and ability to perform in noisy environments. Such algorithms have mainly used one of two approaches. The first approach is per- son specific (a.k.a. fully personalized) [Forman et al., 2019; Jaimes et al., 2015; Liao et al., 2019; Rabbi et al., 2015] where a separate model is deployed for each user in the trial. This approach is suitable when the population of users are highly heterogeneous, but suffers greatly when data is scarce and/or noisy. Note that fully personalized approaches are not feasible for the MiWaves pilot study, as it runs for only 30 days (due to scarce data). The second approach com- pletely pools data (a.k.a. fully pooled) across all users in the population [Clarke et al., 2017; Paredes et al., 2014; Trella et al., 2022; Yom-Tov et al., 2017; Zhou et al., 2018]. Our algorithm, reBandit, strikes a balance between the two approaches - it adaptively pools data across users depend- ing on the degree of heterogeneity in the population. 4.1 describes in detail as to how we achieve that balance using random effects. Tomkins et al. also use random effects in their Thompson- Sampling [Russo and Van Roy, 2014; Thompson, 1933] con- textual bandit algorithm [Li et al., 2010], IntelligentPool- ing [Tomkins et al., 2021]. IntelligentPooling updates its hyper-parameters by modeling the problem as a Gaussian Process (GP). However, IntelligentPooling fails to run au- tonomously and stably in an online environment (does not overcome C3)3. Here we deal with this problem by updat- ing the hyper-parameters in reBandit using empirical Bayes (similar to [Laird and Ware, 1982]), and solve the optimiza- tion problem using projected gradient descent. reBandit runs autonomously and stably in an online environment while also having more users (12x) and more features (8x) as compared to the environment described in IntelligentPooling. Recently, various approaches have been explored regard- ing the application of mixed effects models within a bandit framework [Zhu and Kveton, 2022a; Zhu and Kveton, 2022b; Aouali et al., 2023]. However, these works primarily focus on utilizing mixed effects to capture the dependence and het- erogeneity of rewards associated with different actions, rather than addressing the similarity and heterogeneity among mul- tiple users. For example, [Zhu and Kveton, 2022a] consider a (non-contextual) multi-arm bandit problem, where the agent chooses one of the K arms at each time t with the goal of maximizing cumulative reward. The authors assume that the reward for the arms are correlated with each other and can be expressed using a mixed-effects model, so that pulling an arm gives some information about the reward of other arms as well. A follow up work, [Zhu and Kveton, 2022b], adds con- text into the reward model, and results in a linear mixed ef- fects model for the rewards of the arms. [Aouali et al., 2023] further generalizes the above works to a non-linear reward setting. Our approach diverges from these studies by utiliz- ing mixed-effects to model user similarity and heterogeneity, while making decisions for each user at each time point. In the broader RL literature, there has been much work on Thompson Sampling based bandit algorithms [Basu et al., 2021; Hong et al., 2022], especially in connection to multi-task learning and meta learning [Peleg et al., 2022; Simchowitz et al., 2021; Wan et al., 2021; Wan et al., 2023]. The multi-task learning based approaches quantify the sim- ilarity between arms and/or users from their policies - the extent to which one user’s data influences or contributes to another user’s policy is a function of some similarity mea- sure. reBandit can be connected to multi-task learning, as it learns across multiple users (or tasks), and tries to max- imize rewards across all users (or tasks). However, due to its unique application in mobile health, reBandit adopts a distinct set of assumptions on the structure of the similarity measures in comparison to the works mentioned above. The meta-learning based approaches exploit the underlying struc- ture of similar tasks to improve performance on new (or un- seen, but similar) tasks. While reBandit can be viewed as a form of meta-learning, where shared population parameters are learnt across users (or tasks), and user-specific parame- ters are learnt to personalize to tasks, reBandit does not try to improve performance on new or unseen users. 4 Bandit algorithm: reBandit This section details details the reBandit algorithm used in the MiWaves pilot study. Being an online RL algorithm, reBan- dit has two major components: (i) the online learning algo- rithm; and (ii) the action-selection procedure. Going forward, 3We were unable to run their code published on GitHub, and unable to parse the code or replicate it due to poor documentation we describe reBandit’s online learning algorithm in Section 4.1, and it’s posterior sampling based action selection strat- egy in Section 4.2. Finally, to address delayed effects, we de- scribe its reward engineering procedure in Section 4.3. The reBandit algorithm is summarized in Algorithm 1. 4.1 Online Learning Algorithm This section details the online learning algorithm - specifi- cally the algorithm’s reward approximating function and its model update procedure. Reward Approximating Function One of the key components of the online learning algorithm is its reward approximation function, through which it models the participant’s reward. Recall that the reward function is the conditional mean of the reward given state and action. We chose a Bayesian Mixed Linear Model to model the reward. Mixed models allow the RL algorithm to adaptively pool and learn across users while simultaneously personalizing actions for each user. Let us assume that for a given user i at decision time t, the RL algorithm receives the reward R(t) i after taking action a(t) i Then, the reward model is written as: R(t) i = g(S(t) i )T αi + a(t) i f(S(t) i )T βi + ϵ(t) i (1) where ϵ(t) i is the noise, assumed to be gaussian i.e. ϵ ∼ N(0, σ2 ϵ Imt), and m is the total number of users who have been or are currently part of the study. Also αi, βi, and γi are weights that the algorithm wants to learn. g(S) and f(S) are functions of the RL state defined in Section 2. To enhance robustness to misspecification of the baseline reward model when a(t) i = 0, g(S(t) i )T αi, we utilize action-centering [Greenewald et al., 2017] to learn an over-parameterized ver- sion of the above reward model: R(t) i = g(S(t) i )T αi + (a(t) i − π(t) i )f(S(t) i )T βi + (π(t) i )f(S(t) i )T γi + ϵ(t) i (2) where π(t) i is the probability of taking action a(t) i = 1 in state S(t) i for participant i at decision time t. We refer to the term g(S(t) i )T αi as the baseline, and f(S(t) i )T βi as the advan- tage (i.e. the advantage of taking action 1 over action 0). We re-write the reward model as follows: R(t) i = ΦT itθi + ϵi,t (3) where ΦT it = Φ(S(t) i , a(t) i , π(t) i )T = [g(S(t) i )T , (a(t) i − π(t) i )f(S(t) i )T , (π(t) i )f(S(t) i )T ] is the design matrix for given state and action, and θi = [αi, βi, γi]T is the joint weight vector that the algorithm wants to learn. We further break down the joint weight vector θi into two components: θi = \"αi βi γi # =   αpop + uα,i βpop + uβ,i γpop + uγ,i   = θpop + ui (4) Here, θpop = [αpop, βpop, γpop]T is the population level term which is common across all the user’s reward mod- els and follows a normal prior distribution given by θpop ∼ N(µprior, Σprior). On the other hand, ui = [uα,i, uβ,i, uγ,i]T are the individual level parameters, or the random effects, for any given user i. Note that the individual level parameters are assumed to be normal by definition, i.e. ui ∼ N(0, Σu), and independent of ϵi. Please refer to Appendix D for the prior calculation and initialization values. Online model update procedure Posterior Update: We vectorize the terms across the m users in a study, and re-write the model as: R = ΦT θ + ϵ (5) Ri = h R(1) i . . . R(t) i iT (6) R = \u0002 RT 1 . . . RT m \u0003T (7) θ = \u0002 θT 1 . . . θT m \u0003T = 1m ⊗ θpop + u (8) u = \u0002 uT 1 . . . uT m \u0003T (9) ϵi = [ϵi,1 . . . ϵi,t]T (10) ϵ = \u0002 ϵT 1 . . . ϵT m \u0003T (11) ui ∼ N(0, Σu) (12) ϵ ∼ N(0, σ2 ϵ Imt) (13) As specified before, we assume a gaussian prior on the population level term θpop ∼ N(µprior, Σprior). The hyper- parameters of the above model, given the definition above, are the noise variance σ2 ϵ and the random effects variance Σu. Now, at a given decision point t, using estimated values of the hyper-parameters (σ2 ϵ,t is the estimate of σ2 ϵ and Σu,t is the estimate of Σu), the posterior mean and covariance matrix of the parameter θ can be calculated as: µ(t) post = \u0000˜Σ−1 θ,t + σ−2 ϵ,t A \u0001−1\u0000˜Σ−1 θ,tµθ + σ−2 ϵ,t B \u0001 (14) Σ(t) post = \u0000˜Σ−1 θ,t + σ−2 ϵ,t A \u0001−1 (15) where A = BlockDiag \u0000A1, . . . , Am \u0001 (16) Ai = Xt τ=1 ΦiτΦT iτ (17) BT = [BT 1 . . . BT m] (18) Bi = Xt τ=1 ΦiτR(τ) i (19) µT θ = [µprior T . . . µprior T ] (20) ˜Σθ,t = Im ⊗ Σu,t + Jm ⊗ Σprior (21) The action-selection procedure (described in Section 4.2) uses the Gaussian posterior distribution defined by the posterior mean µ(t) post and variance Σ(t) post to determine the action selection probability π(t+1) and the corresponding actions for the next time steps. Hyper-parameter update: The hyper-parameters in the al- gorithm’s reward model are the noise variance σ2 ϵ and random effects variance Σu. In order to update these variance esti- mates at the end of decision time t, we use Empirical Bayes [Morris, 1983] to maximize the marginal likelihood of ob- served rewards, marginalized over the parameters θ. So, in order to form Σu,t and σ2 ϵ,t, we solve the following optimiza- tion problem: Σu,t, σ2 ϵ,t = argmax l(Σu,t, σ2 ϵ,t; H) (22) s.t. Σu,t ≻ 0 (23) σ2 ϵ,t ≥ 0 (24) where, l(Σu,t, σ2 ϵ,t; H) = log(det(X)) − log(det(X + yA)) + mt log(y) − y X τ∈[t] X i∈[m](R(τ) i )2 − µT θ Xµθ + (Xµθ + yB)T (X + yA)−1(Xµθ + yB) (25) Note that, X = ˜Σ−1 θ,t (see Eq. 21) and y = 1 σ2 ϵ,t . We solve the optimization problem using gradient descent. 4.2 Action selection procedure The action selection procedure utilizes a modified posterior sampling algorithm called the smooth posterior sampling al- gorithm. Recall from Section 4.1, our model for the reward is a Bayesian linear mixed model with action centering (refer Eq. 2) where π(t) i is the probability that the RL algorithm se- lects action a(t) i = 1 in state S(t) i for participant i at decision point t. The RL algorithm computes the probability π(t) i as follows: π(t) i = E ˜β∼N (µ(t−1) post,i ,Σ(t−1) post,i )[ρ(f(S(t) i )T ˜β)|H(t−1) 1:m , S(t) i ] (26) Notice that the last expectation above is over the draw of β from the posterior distribution parameterized by µ(t−1) post,i and Σ(t−1) post,i (see Eq. 14 and Eq. 15 for their definitions). Classical posterior sampling sets ρ(x) = I(x > 0). In this case, the posterior sampling algorithm sets randomization probabilities to the posterior probability that the treatment ef- fect is positive. However, when using a pooled algorithm, Zhang et al. showed that between study statistical inference is enhanced if ρ is a smooth i.e. continuously differentiable function. Using a smooth function ensures that the random- ization probabilities formed by the algorithm concentrate. Concentration enhances the replicability of the randomiza- tion probabilities if the study is repeated. Without concentra- tion, the randomization probabilities might fluctuate greatly between repetitions of the study [Deshpande et al., 2018; Kalvit and Zeevi, 2021; Zhang et al., 2022]. In MiWaves , we choose ρ to be a generalized logistic function, defined as follows: ρ(x) = Lmin + Lmax − Lmin 1 + c exp(−bx) (27) where c = 5, and b = 21.053 (please refer to Appendix E for more details). We set the lower and upper clipping probabil- ities as Lmin = 0.2 and Lmax = 0.8 (i.e., 0.2 ≤ π(t) i ≤ 0.8). The probabilities are clipped to facilitate after-study analysis and off-policy evaluation [Zhang et al., 2022]. Algorithm 1: reBandit Input : m, D, µ(0) post = µprior, Σ(0) post = Σprior, Σu,0, σ2 ϵ,0, ρ(x) for d = 1 to D do for j = 0 to 1 do Compute timestep τ = ((d − 1) × 2) + j for i = 1 to m do Observe state S(τ) i ; Get posteriors µ(d−1) post,i and Σ(d−1) post,i for user i from µ(d−1) post and Σ(d−1) post ; Compute action selection probability π(τ) i using Eq. 26 Sample action a(t) i = Bern(π(τ) i ) Collect reward R(τ) i end end Update Σu,d and σ2 ϵ,d using Eq. 22, with engineered rewards from Eq. 28; Update posteriors µ(d) post and Σ(d) post using Eq. 14 and Eq. 15, with engineered rewards from Eq. 28 end 4.3 Reward Engineering To account for delayed effects in the bandit framework, we engineer the reward for the RL algorithm. Note that this en- gineered reward is only utilized to update the RL algorithm’s parameters and hyper-parameters. We are still interested in maximizing the reward defined in Sec. 2, and use it to evalu- ate the algorithm’s performance. The engineered reward ˆR(t) i for user i at decision time t is defined as: ˆR(t) i = R(t) i − a(t) i cost(a(t) i ) (28) cost(a(t) i ) = λ · σi,obs (29) where σi,obs is the standard deviation of the observed re- wards for a given user i, and λ is a tuned non-negative hyper-parameter. Note that the reward is not penalized when a(t) i = 0. Intuitively, the cost function is designed to allow the RL algorithm to optimize for user engagement, while si- multaneously accounting for the delayed effect of sending an intervention message, i.e. a(t) i = 1. 5 Experimental Results In this section, we detail the design of a simulation testbed (Sec. 5.1) to help evaluate the performance of our algorithm. Our experimental setup and the corresponding results are dis- cussed in Sec 5.2. 5.1 Simulation Testbed Design We leverage data from the SARA [Rabbi et al., 2018] study, which trialed an mHealth app aimed at sustaining engage- ment of substance use data collection from participants. Since the SARA study focused on a similar demographic of EAs as the MiWaves pilot study, it appears ideal for constructing a simulation testbed. However, note that this data is impover- ished. SARA had only 1 decision point per day, as compared to 2 per day in MiWaves . The goal of the messages sent to the participants in SARA was to increase survey completion in order to collect substance use data. In contrast, the goal of sending intervention messages in MiWaves pilot study is to reduce the participant’s cannabis use through self-monitoring and mobile health engagement. Moreover, the daily cannabis use data in SARA was collected retro-actively at the end of each week, which often resulted in participant’s noisy rec- ollection of their cannabis use, and had missing cannabis use data if the participant chose to not respond. In contrast, participants in MiWaves are asked to self-report twice daily, which reduces the amount of missing data if they fail to self- report once. More details and differences are highlighted in Appendix A.1. We construct a base dataset of 42 users af- ter cleaning and imputing the SARA data (please refer to ap- pendix A for more details). Base Model: For the base model of the environment, we fit Multinomial Logistic Regression (MLR) models on each of the 42 users in the base dataset. The learnt weights include weights for the baseline (when action is 0), and the advantage (added to the baseline when action is 1). These user models are overfit to learn the user behavior as well capture the noise in the environment. We choose MLR for our user models, as it is interpretable, and performs similar in comparison to a generic neural network (see Appendix A.6). Varying Treatment Effects (TE): The effect of the interven- tion message on a particular user is measured by their unique treatment effect size. Given that the intervention messages in SARA had minimal treatment effect [Nahum-Shani et al., 2021], we introduce higher levels of treatment effect into the user models by augmenting their weights. Higher levels of treatment effect increase the likelihood of obtaining higher rewards when taking action 1. To that end, we construct TE = low and TE = high treatment effect variants for each MLR user model. Further details on imputing these effect sizes can be found in Appendix A.9. Modeling Habituation (HB): To account for delayed effects in the environment, we introduce user habituation to repeated stimuli (multiple intervention messages sent to the user in a short span of time) by adding a negative effect in the base- line weights of the MLR user models. To that end, we define dosage for each user at each decision point as the weighted average of the number of intervention messages received in the previous six decision points. The weights are decreased with each past decision point, reflecting a diminishing impact of older intervention messages received by the user. Next, we impute baseline weights for dosage in the MLR user mod- els in a way that higher dosage (more messages received) leads to higher likelihood of generating low rewards, and vice-versa. Note that this procedure simulates how the user may experience habituation; if the RL algorithm does not send many interventions to a user experiencing habituation, the user may dis-habituate and recover their baseline behav- ior. We construct two environment variants - HB = low and HB = high habituation effect - by varying the baseline weights for dosage. Additionally, we simulate the proportion of users who can experience habituation within a population - set at Minimal Treatment Effect Low Treatment Effect HB=Low HB=High HB=Low HB=High Alg. HB=None P=50% P=100% P=50% P=100% HB=None P=50% P=100% P=50% P=100% reBandit 128.54±0.18 127.23±0.18 126.01±0.18 123.22±0.19 119.55±0.20 129.44±0.17 128.11±0.17 126.80±0.18 123.74±0.19 120.12±0.20 BLR 127.78±0.16 126.60±0.18 125.78±0.18 123.23±0.19 119.60±0.20 129.10±0.17 127.85±0.17 126.53±0.18 123.75±0.19 120.16±0.20 random 127.83±0.16 126.52±0.18 125.22±0.18 119.03±0.21 110.29±0.23 128.97±0.17 127.70±0.17 126.45±0.18 120.49±0.20 112.06±0.22 Table 1: Average total reward per user per simulated trial, averaged across 500 simulated trials and 120 users per trial, along with their 95% confidence intervals (CI) for minimal and low treatment effect settings. HB refers to the level of habituation in the environment, while P is used to denote the proportion of the population who can experience habituation. HB=Low HB=High Alg. HB=None P=50% P=100% P=50% P=100% reBandit 132.25±0.16 130.95±0.17 129.71±0.17 124.70±0.19 121.19±0.20 BLR 132.21±0.16 130.94±0.17 129.63±0.17 124.70±0.19 121.22±0.19 random 131.05±0.17 129.88±0.17 128.71±0.17 123.19±0.20 115.39±0.22 Table 2: Average total reward per user per simulated trial along with their 95% CIs for the high treatment effect settings. either P = 50% or P = 100%. Further details on model- ing habituation into simulation user models can be found in Appendix A.9. 5.2 Simulation Results We construct 15 simulation environment variants using a combination of techniques described in Sec. 5.1. For each en- vironment, we simulate 500 studies with m = 120 users each, over a period of D = 30 days (T = 60). The m = 120 users are drawn with replacement from the 42 MLR user models learnt using SARA data. We compare the performance of our algorithm to two com- mon approaches in mobile health studies. First, is a full pooling algorithm called BLR. BLR utilizes Bayesian Lin- ear Regression [Liao et al., 2019] to pool data and learn a single model across all the users in a study, and select ac- tions according to the action selection procedure mentioned in Sec. 4.2. We use engineered rewards (Sec. 4.3) to up- date BLR’s parameters and hyper-parameters. We also up- date BLR hyper-parameters using Empirical Bayes, similar to the approach described in Sec. 4.1, for a fair comparison. For both reBandit and BLR, we update the posteriors at the end of each simulated day (every 2 decision points), and the hyper-parameters at the end of each week (every 14 decision points). We refer the reader to Appendix B for more details about BLR’s implementation. In addition to BLR, we also compare against the random algorithm, which utilizes an ac- tion selection probability of π(t) i = 0.5. For each algorithm and simulation environment pair, we calculate the average total reward per user per simulated trial, averaged across the 500 simulated trials and 120 users in each trial. We also compute their 95% confidence intervals (CIs). We summarize our findings in Table 1 and 2. In all the sim- ulation environments, reBandit performs no worse than the baseline algorithms. We highlight the environments where reBandit significantly outperforms other algorithms (CIs do not overlap) in green. In the environments where the CIs for the average total reward overlap for reBandit and BLR, we in- dividually compare each of the 500 seeded simulations, and count the number of times reBandit achieved an average to- tal reward per user as compared to BLR. If this number is greater than 50% of the simulations, i.e. greater than 250, we highlight those environments in yellow, otherwise they are highlighted in blue. The primary takeaway from our simulation results in Ta- bles 1 and 2 is that reBandit is impactful - it performs better than BLR in most environments, and even in the blue high- lighted environments where it performs slightly worse than BLR, the performance is still comparable. It is important to note that our procedures to artificially introduce treatment ef- fects and user habituation into the user models reduces the heterogeneity among the user models. This is due to the fact that our procedure to artificially inject treatment effect estab- lishes a non-negative effect of taking an action across all the users in the user models. The same applies to the procedure for introducing user habituation, as it establishes a clear neg- ative effect with respect to dosage across all users in the user models. However, in practice, higher levels of treatment ef- fect or user habituation effect may lead to more heterogene- ity in the population. Given that limitation, it is easy to ob- serve that in our simulations, as levels of treatment effect or user habituation effect are increased, the performance gap be- tween reBandit and BLR decreases. In simulation environ- ments characterized by more pronounced heterogeneity due to lower levels of treatment and/or habituation effects, reBan- dit excels by adeptly identifying and leveraging the hetero- geneity within the user population to personalize the likeli- hood of intervention message delivery and accrues greater re- wards. 6 Conclusion In this paper, we introduced reBandit, an online RL algo- rithm which will be a part of the upcoming mobile health study named MiWaves aimed at reducing cannabis use among emerging adults. We addressed the unique challenges inher- ent in mobile health studies, including limited data, and re- quirement for algorithmic autonomy and stability, while de- signing reBandit. We showed that reBandit utilizes random- effects and informative Bayesian priors to learn quickly and efficiently in noisy environments which are common in mo- bile health studies. The introduction of random effects allows reBandit to leverage the heterogeneity in the study population and deliver personalized interventions. To benchmark our al- gorithm, we detailed the design of a simulation testbed using prior data, and showed that reBandit performs equally well or better than two common approaches used in mobile health studies. In the future, we aim to analyze the effectiveness of the interventions in the MiWaves pilot study. In addition, we aim to investigate the contribution of an individual’s data and the study population data towards learning the individual’s parameters in the random effects model (see Appendix F). References [Aouali et al., 2023] Imad Aouali, Branislav Kveton, and Sumeet Katariya. Mixed-effect thompson sampling. In International Conference on Artificial Intelligence and Statistics, pages 2087– 2115. PMLR, 2023. [Basu et al., 2021] Soumya Basu, Branislav Kveton, Manzil Za- heer, and Csaba Szepesv´ari. No regrets for learning the prior in bandits. Advances in neural information processing systems, 34:28029–28041, 2021. [Carliner et al., 2017] Hannah Carliner, Qiana L Brown, Aaron L Sarvet, and Deborah S Hasin. Cannabis use, attitudes, and legal status in the us: A review. Preventive medicine, 104:13–23, 2017. [Chan et al., 2021] Gary CK Chan, Denise Becker, Peter Butter- worth, Lindsey Hines, Carolyn Coffey, Wayne Hall, and George Patton. Young-adult compared to adolescent onset of regular cannabis use: A 20-year prospective cohort study of later con- sequences. Drug and Alcohol Review, 40(4):627–636, 2021. [Clarke et al., 2017] Shanice Clarke, Luis G Jaimes, and Miguel A Labrador. mstress: A mobile recommender system for just-in- time interventions for stress. In 2017 14th IEEE annual consumer communications & networking conference (CCNC), pages 1–5. IEEE, 2017. [Deshpande et al., 2018] Yash Deshpande, Lester Mackey, Vasilis Syrgkanis, and Matt Taddy. Accurate inference for adaptive lin- ear models. In International Conference on Machine Learning, pages 1194–1203. PMLR, 2018. [Forman et al., 2019] Evan M Forman, Stephanie G Kerrigan, Meghan L Butryn, Adrienne S Juarascio, Stephanie M Manasse, Santiago Onta˜n´on, Diane H Dallal, Rebecca J Crochiere, and Danielle Moskow. Can the artificial intelligence technique of reinforcement learning use continuously-monitored digital data to optimize treatment for weight loss? Journal of behavioral medicine, 42:276–290, 2019. [Golbus et al., 2021] Jessica R Golbus, Walter Dempsey, Eliza- beth A Jackson, Brahmajee K Nallamothu, and Predrag Klasnja. Microrandomized trial design for evaluating just-in-time adap- tive interventions through mobile health technologies for cardio- vascular disease. Circulation: Cardiovascular Quality and Out- comes, 14(2):e006760, 2021. [Greenewald et al., 2017] Kristjan Greenewald, Ambuj Tewari, Su- san Murphy, and Predag Klasnja. Action centered contextual bandits. Advances in neural information processing systems, 30, 2017. [Hall, 2009] Wayne Hall. The adverse health effects of cannabis use: what are they, and what are their implications for policy? International Journal of drug policy, 20(6):458–466, 2009. [Hasin et al., 2016] Deborah Hasin, Bradley Kerridge, Tulshi Saha, Boji Huang, Roger Pickering, Sharon Smith, Jeesun Jung, Haitao Zhang, and Bridget Grant. Prevalence and correlates of dsm- 5 cannabis use disorder, 2012-2013: Findings from the na- tional epidemiologic survey on alcohol and related conditions–iii. American Journal of Psychiatry, 173(6):588–599, 2016. [Hong et al., 2022] Joey Hong, Branislav Kveton, Manzil Zaheer, and Mohammad Ghavamzadeh. Hierarchical bayesian bandits. In International Conference on Artificial Intelligence and Statistics, pages 7724–7741. PMLR, 2022. [Jaimes et al., 2015] Luis G Jaimes, Martin Llofriu, and Andrew Raij. Preventer, a selection mechanism for just-in-time preven- tive interventions. IEEE Transactions on Affective Computing, 7(3):243–257, 2015. [Jiang et al., 2015] Nan Jiang, Alex Kulesza, Satinder Singh, and Richard Lewis. The dependence of effective planning horizon on model accuracy. In Proceedings of the 2015 International Con- ference on Autonomous Agents and Multiagent Systems, pages 1181–1189, 2015. [Kalvit and Zeevi, 2021] Anand Kalvit and Assaf Zeevi. A closer look at the worst-case behavior of multi-armed bandit algorithms. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 8807–8819. Curran Asso- ciates, Inc., 2021. [Kramer et al., 2019] Jan-Niklas Kramer, Florian K¨unzler, Varun Mishra, Bastien Presset, David Kotz, Shawna Smith, Urte Scholz, Tobias Kowatsch, et al. Investigating intervention components and exploring states of receptivity for a smartphone app to pro- mote physical activity: protocol of a microrandomized trial. JMIR research protocols, 8(1):e11540, 2019. [Laird and Ware, 1982] Nan Laird and James Ware. Random- effects models for longitudinal data. Biometrics, pages 963–974, 1982. [Laird, 2004] Nan Laird. Random effects and the linear mixed model. In Analysis of Longitudinal and Cluster-Correlated Data, volume 8, pages 79–96. Institute of Mathematical Statistics, 2004. [Langford and Zhang, 2007] John Langford and Tong Zhang. The epoch-greedy algorithm for contextual multi-armed bandits. Ad- vances in neural information processing systems, 20(1):96–1, 2007. [Lapham et al., 2019] Gwen T Lapham, Cynthia I Campbell, Bobbi Jo H Yarborough, Rulin C Hechter, Brian K Ahmedani, Irina V Haller, Andrea H Kline-Simon, Derek D Satre, Amy M Loree, Constance Weisner, et al. The prevalence of healthcare effective- ness data and information set (hedis) initiation and engagement in treatment among patients with cannabis use disorders in 7 us health systems. Substance Abuse, 40(3):268–277, 2019. [Li et al., 2010] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to personalized news ar- ticle recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670, 2010. [Liao et al., 2019] Peng Liao, Kristjan H. Greenewald, Predrag V. Klasnja, and Susan A. Murphy. Personalized heartsteps: A re- inforcement learning algorithm for optimizing physical activity. CoRR, abs/1909.03539, 2019. [Martin et al., 2018] Cesar A Martin, Daniel E Rivera, Eric B Hek- ler, William T Riley, Matthew P Buman, Marc A Adams, and Ali- cia B Magann. Development of a control-oriented model of social cognitive theory for optimized mhealth behavioral interventions. IEEE Transactions on Control Systems Technology, 28(2):331– 346, 2018. [Morris, 1983] Carl N Morris. Parametric empirical bayes infer- ence: theory and applications. Journal of the American statistical Association, 78(381):47–55, 1983. [Nahum-Shani et al., 2018] Inbal Nahum-Shani, Shawna N Smith, Bonnie J Spring, Linda M Collins, Katie Witkiewitz, Ambuj Tewari, and Susan A Murphy. Just-in-time adaptive interven- tions (jitais) in mobile health: key components and design prin- ciples for ongoing health behavior support. Annals of Behavioral Medicine, pages 1–17, 2018. [Nahum-Shani et al., 2021] Inbal Nahum-Shani, Mashfiqui Rabbi, Jamie Yap, Meredith L Philyaw-Kotov, Predrag Klasnja, Erin E Bonar, Rebecca M Cunningham, Susan A Murphy, and Mau- reen A Walton. Translating strategies for promoting engage- ment in mobile health: A proof-of-concept microrandomized trial. Health Psychology, 40(12):974, 2021. [Nahum-Shani et al., 2022] Inbal Nahum-Shani, Steven D Shaw, Stephanie M Carpenter, Susan A Murphy, and Carolyn Yoon. En- gagement in digital interventions. American Psychologist, 2022. [Paredes et al., 2014] Pablo Paredes, Ran Gilad-Bachrach, Mary Czerwinski, Asta Roseway, Kael Rowan, and Javier Hernandez. Poptherapy: Coping with stress through pop-culture. In Proceed- ings of the 8th international conference on pervasive computing technologies for healthcare, pages 109–117, 2014. [Peacock et al., 2018] Amy Peacock, Janni Leung, Sarah Larney, Samantha Colledge, Matthew Hickman, J¨urgen Rehm, Gary A Giovino, Robert West, Wayne Hall, Paul Griffiths, et al. Global statistics on alcohol, tobacco and illicit drug use: 2017 status re- port. Addiction, 113(10):1905–1926, 2018. [Peleg et al., 2022] Amit Peleg, Naama Pearl, and Ron Meir. Met- alearning linear bandits by prior update. In International Confer- ence on Artificial Intelligence and Statistics, pages 2885–2926. PMLR, 2022. [Rabbi et al., 2015] Mashfiqui Rabbi, Min Hane Aung, Mi Zhang, and Tanzeem Choudhury. Mybehavior: automatic personalized health feedback from user behaviors and preferences using smart- phones. In Proceedings of the 2015 ACM international joint con- ference on pervasive and ubiquitous computing, pages 707–718, 2015. [Rabbi et al., 2018] Mashfiqui Rabbi, Meredith Philyaw Kotov, Re- becca Cunningham, Erin E Bonar, Inbal Nahum-Shani, Predrag Klasnja, Maureen Walton, Susan Murphy, et al. Toward increas- ing engagement in substance use data collection: development of the substance abuse research assistant app and protocol for a mi- crorandomized trial using adolescents and emerging adults. JMIR research protocols, 7(7):e9850, 2018. [Rabbi et al., 2019] Mashfiqui Rabbi, Predrag Klasnja, Tanzeem Choudhury, Ambuj Tewari, and Susan Murphy. Optimizing mhealth interventions with a bandit. Digital Phenotyping and Mobile Sensing: New Developments in Psychoinformatics, pages 277–291, 2019. [Robinson, 1991] George K Robinson. That blup is a good thing: the estimation of random effects. Statistical science, pages 15– 32, 1991. [Russo and Van Roy, 2014] Daniel Russo and Benjamin Van Roy. Learning to optimize via posterior sampling. Mathematics of Op- erations Research, 39(4):1221–1243, 2014. [SAMHSA, 2023] SAMHSA. 2021 NSDUH De- tailed Tables. https://www.samhsa.gov/data/report/ 2021-nsduh-detailed-tables, 2023. Accessed: 2024-02-14. [Shrier et al., 2018] Lydia A Shrier, Pamela J Burke, Meredith Kells, Emily A Scherer, Vishnudas Sarda, Cassandra Jonestrask, Ziming Xuan, and Sion Kim Harris. Pilot randomized trial of moment, a motivational counseling-plus-ecological momentary intervention to reduce marijuana use in youth. Mhealth, 4, 2018. [Simchowitz et al., 2021] Max Simchowitz, Christopher Tosh, Ak- shay Krishnamurthy, Daniel Hsu, Thodoris Lykouris, Miro Dudik, and Robert Schapire. Bayesian decision-making un- der misspecified priors with applications to meta-learning. Ad- vances in Neural Information Processing Systems, 34:26382– 26394, 2021. [Sutton and Barto, 2018] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018. [Tewari and Murphy, 2017] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. Mobile health: sensors, analytic methods, and applications, pages 495–517, 2017. [Thompson, 1933] William R Thompson. On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3-4):285–294, 1933. [Tomkins et al., 2021] Sabina Tomkins, Peng Liao, Predrag Klas- nja, and Susan Murphy. Intelligentpooling: Practical thomp- son sampling for mhealth. Machine learning, 110(9):2685–2727, 2021. [Trella et al., 2022] Anna Trella, Kelly Zhang, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, and Susan Murphy. Designing reinforcement learning algorithms for digital interventions: pre- implementation guidelines. Algorithms, 15(8):255, 2022. [Trella et al., 2023] Anna Trella, Kelly Zhang, Inbal Nahum-Shani, Vivek Shetty, Finale Doshi-Velez, and Susan Murphy. Reward design for an online reinforcement learning algorithm supporting oral self-care. In Proceedings of the AAAI Conference on Artifi- cial Intelligence, volume 37, pages 15724–15730, 2023. [Volkow et al., 2014] Nora D Volkow, Ruben D Baler, Wilson M Compton, and Susan RB Weiss. Adverse health effects of mar- ijuana use. New England Journal of Medicine, 370(23):2219– 2227, 2014. [Walsh and Groarke, 2019] Jane C Walsh and Jenny M Groarke. In- tegrating behavioral science with mobile (mhealth) technology to optimize health behavior change interventions. European Psy- chologist, 2019. [Wan et al., 2021] Runzhe Wan, Lin Ge, and Rui Song. Metadata- based multi-task bandits with bayesian hierarchical models. Ad- vances in Neural Information Processing Systems, 34:29655– 29668, 2021. [Wan et al., 2023] Runzhe Wan, Lin Ge, and Rui Song. Towards scalable and robust structured bandits: A meta-learning frame- work. In International Conference on Artificial Intelligence and Statistics, pages 1144–1173. PMLR, 2023. [Wang et al., 2005] Chih-Chun Wang, Sanjeev R Kulkarni, and H Vincent Poor. Bandit problems with side observations. IEEE Transactions on Automatic Control, 50(3):338–355, 2005. [Yom-Tov et al., 2017] Elad Yom-Tov, Guy Feraru, Mark Kozdoba, Shie Mannor, Moshe Tennenholtz, and Irit Hochberg. Encourag- ing physical activity in patients with diabetes: Intervention using a reinforcement learning system. Journal of medical Internet re- search, 19(10):e338, 2017. [Zhang et al., 2022] Kelly W Zhang, Lucas Janson, and Susan A Murphy. Statistical inference after adaptive sampling for longi- tudinal data. arXiv preprint arXiv:2202.07098, 2022. [Zhou et al., 2018] Mo Zhou, Yonatan Mintz, Yoshimi Fukuoka, Ken Goldberg, Elena Flowers, Philip Kaminsky, Alejandro Castillejo, and Anil Aswani. Personalizing mobile fitness apps using reinforcement learning. In CEUR workshop proceedings, volume 2068. NIH Public Access, 2018. [Zhu and Kveton, 2022a] Rong Zhu and Branislav Kveton. Ran- dom effect bandits. In International Conference on Artificial In- telligence and Statistics, pages 3091–3107. PMLR, 2022. [Zhu and Kveton, 2022b] Rong Zhu and Branislav Kveton. Ro- bust contextual linear bandits. arXiv preprint arXiv:2210.14483, 2022. Appendices A Simulation Testbed This section details how we transform prior data to construct a dataset, and utilize the dataset to develop the MiWaves simulation testbed. The testbed is used to develop and evaluate the design of the RL algorithm for the MiWaves pilot study. The base simulator or the vanilla testbed is constructed using the SARA [Rabbi et al., 2018] study dataset. The SARA dataset consists of N = 70 users, and the SARA study was for 30 days, 1 decision point per day. For each user, the dataset contains their daily and weekly survey responses about substance use, along with daily app interactivity logs and notification logs. We will now detail the procedure to construct this base simulator. A.1 SARA vs MiWaves The Substance Use Research Assistant (SARA) [Rabbi et al., 2018] study trialed an mHealth app aimed at sustaining engage- ment of substance use data collection from participants. Since the SARA study focused on a similar demographic of emerging adults (ages 18-25) as the MiWaves pilot study, we utilized the data gathered from the SARA study to construct the simulation testbed. We highlight the key differences between the SARA study and the MiWaves pilot study in Table 3. SARA MiWaves m = 70 m = 120 T = 30 T = 60 π(t) i = 0.5 π(t) i determined by RL Cannabis use data self-reported weekly Cannabis use data self-reported twice daily Inspirational and reminder messages to increase survey completion and collect substance use data Messages to prompt cannabis use reduction through self-monitoring and improve engagement Table 3: SARA vs MiWaves : Key differences A.2 Data Extraction First, we will detail the steps to extract the relevant data from the SARA dataset: 1. App Usage: The SARA dataset has a detailed log of the user’s app activity for each day in the trial, since their first login. We calculate the time spent by each user between a normal entry and an app paused log entry, until 12 AM midnight, to determine the amount of time (in seconds) spent by the user in the app on a given day. To determine the time spent by the user in the evening, we follow the same procedure but we start from any logged activity after the 4 PM timestamp, till midnight (12 AM). 2. Survey Completion: The SARA dataset contains a CSV file for each user detailing their daily survey completion status (completed or not). We use this binary information directly to construct the survey completion feature. 3. Action: The SARA dataset contains a CSV file for each user detailing whether they got randomized (with 0.5 probability) to receive a notification at 4 PM, and whether the notification was actually pushed and displayed on the user’s device. We use this CSV file to derive the latter information, i.e. whether the app showed the notification on the user’s device (yes or no). 4. Cannabis Use: Unlike MiWaves , the SARA trial did not ask users to self-report cannabis use through the daily surveys. However, the users were prompted to respond to a weekly survey at the end of each week (on Sundays). Through these weekly surveys, the users were asked to retroactively report on their cannabis use in the last week, from Monday through Sunday. We use these weekly surveys to retroactively build the reported cannabis-use for each user in the study. The cannabis use was reported in grams of cannabis used, taking values of 0g, 0.25g, 0.5g, 1g, 1.5g, 2g and 2.5g+. Users who were not sure about their use on a particular day, reported their use as “Not sure”. Meanwhile, users who did not respond to the weekly survey were classified to have “Unknown” use for the entire week. The distribution of reported cannabis use in SARA across all the users, across all 30 days, can be viewed in Figure 2. We build a database for all users using the three features specified above. Figure 2: Distribution of cannabis use across all users, across all data points A.3 Data Cleaning Next, we will specify the steps to clean this data, and deal with outliers: 1. Users with insufficient data: We remove users who had more than 20 undetermined (i.e. either “Unknown” or “Not sure”) cannabis use entries. Upon removing such users, we are left with N = 42 users. The updated distribution of reported cannabis use in the remaining data across all the users across all 30 days, is demonstrated in Figure 3. (a) Dataset cannabis-use distribution 10 15 20 25 30 Count of self-reported cannabis use datapoints 0 1 2 3 4 5 6 Number of users Histogram of count of self-reported cannabis use datapoints (b) Count of non-missing cannabis-use datapoints vs number of users Figure 3: Distribution of cannabis use across all users, across all data points, after removing users with insufficient data 2. Outliers in app usage information: The average app usage in a given day across all users, comes out to be 244 seconds (please refer to Section A.2 as to how app usage is calculated from the raw data). However, due to some technical error during the data collection process, sometimes, a user ended up with greater than 1000 seconds of app use in a day, with the highest reaching around 67000 seconds. We observe a similar issue with post-4PM app use data. To deal with such outliers, we clip any post-4PM app use higher than 700 seconds. There are 20 such data points in the dataset. A.4 Reward computation Next, we calculate the reward for each data point in the dataset. It is calculated as follows: • 0: User did not complete the daily survey, nor used the app. • 1: User did not complete the daily survey, but used the app outside of the daily survey. • 2: User completed the survey. We transform the [0 − 2] reward above to a [0 − 3] reward defined in MiWaves (refer Section 2) by randomly transforming the data points with 2 reward to 3 with 50% probability. A.5 Dataset for training user models We use the post-4PM data (i.e. data from 4 PM to 12 AM midnight) (dubbed as the evening data) to create a dataset to train the individual user models. The dataset contains the following features: • Day In Study ∈ [1, 30] • Cannabis Use ∈ [0, 2.5g] • (Evening) App usage ∈ [0, 700] • Survey completion (binary) • Weekend indicator (binary) • Action (binary) • Reward ∈ {0, 1, 2, 3} We detail the steps to create this evening dataset: 1. Evening cannabis-use: The SARA study documented cannabis use for a given user for a whole day. In contrast, MiWaves will be asking users to self-report their cannabis use twice a day. To mimic the same, after discussions among the scientific team, we split a given user’s daily cannabis use from SARA into morning and evening use, and multiply a factor of 0.67 to generate their evening cannabis use. Also, the MiWaves study will be recruiting users who typically use cannabis at least 3 times a week. We expect their use to be much higher than that of users in SARA. So, we multiply the evening cannabis use by a factor of 1.5. Thus, we generate the evening cannabis use from the user’s daily use reported in SARA as follows: Evening CB Use = That Day’s SARA CB Use × 0.67 × 1.5 (30) 2. Feature normalization: The resulting dataset’s features are then normalized as follows: • Day in study is normalized into a range of [−1, 1] as follows Day in study (normalized) = Day in study − 15.5 14.5 (31) • App usage is normalized into a range of [−1, 1] as follows App usage (normalized) = App usage − 350 350 (32) • Cannabis use (evening) is normalized into a range of [−1, 1] as follows Cannabis use (normalized) = Cannabis use − 1.3 1.35 (33) A.6 Training User Models As specified above in Sec. A.5, we use to evening dataset to train our user models. This dataset has the following features: • Day In Study ∈ [−1, 1]: Negative values refer to the first half of the study, while positive values refer to the second half of the study. A value of 0 means that the user is in the middle of the study. −1 means that the user has just begun the study, while 1 means they are at the end of the 30 day study. • Cannabis Use ∈ [−1, 1]: Negative values refer to the user’s cannabis use being lower than the population’s average cannabis use value, while positive values refer to user’s cannabis use being higher than the study population’s average cannabis use value. A value of 0 means that the user’s cannabis use is the average value of cannabis use in the study population. Meanwhile, −1 means that the user is not using cannabis, and 1 means that the user used the highest amount of cannabis reported by the study population. • (Evening) App usage ∈ [−1, 1]: Negative values refer to the user’s app use being lower than the population’s average app use value, while positive values refer to user’s app use being higher than the study population’s average app use value. A value of 0 means that the user’s app usage is the average amount of app usage observed in the study population. Meanwhile, −1 means that the user’s app usage is non-existent (i.e. zero). On the other hand, 1 means that the user’s app usage is the highest among the observed app usage values in the study population. • Survey completion ∈ {0, 1}: A value of 0 refers to the case where the user has not finished the decision point’s EMA, while a value of 1 refers to the case where the user has responded to the decision point’s EMA. • Weekend indicator ∈ {0, 1}: A value of 0 refers to the case where the decision point falls on a weekday, while a value of 1 refers to the case where the decision point falls on a weekend. • Action ∈ {0, 1}: A value of 0 means that action was not taken (i.e. no notification or intervention message was shown), while 1 means that an action was taken (i.e. a notification or intervention message was shown). • Reward ∈ {0, 1, 2, 3}: Same as defined in Section A.4 We morph these features into a 12 dimensional feature vector, defined as follows: 1. Intercept (1) 2. Survey Completion 3. Standardized App Usage 4. Standardized Cannabis Use 5. Weekend Indicator 6. Standardized Day in Study 7. Action taken multiplied by the Intercept (1) 8. Action taken multiplied by Survey Completion 9. Action taken multiplied by Standardized App Usage 10. Action taken multiplied by Standardized Cannabis Use 11. Action taken multiplied by Weekend Indicator 12. Action taken multiplied by Standardized Day in Study In the rest of the section, the weights corresponding to features 1 − 6 are referred to as the baseline weights, and the weights for 7 − 12 are referred to as the advantage weights. We fit the reward using our user models. Before we train user models, we do a complete-case analysis on the evening dataset. It involves removing all the data points which have any missing feature. This can either be a missing “cannabis use” value, or a missing “action” (which is the case on the first day of the study). Given that our target variable i.e. the reward is categorical (0-3) in nature, we consider two options for our generative user models: • Multinomial Logistic Regression (MLR) - We fit a multinomial logistic regression model on our dataset. Given K classes (K=4 in our case), the probability for a data point i belonging to a class c is given by: P(Yi = c|Xi) = eβc·Xi PK j=1 eβj·Xi (34) where Xi are the features of the given data point i, and βj is the learned coefficient for a given reward class j. Each data point refers to a user-decision point, we use these terms interchangibly throughout the document. The model is optimized using the following objective: min β − n X i=1 K X c=1 1{Yi=c} log(P(Yi = c|Xi)) + 1 2||β||2 F (35) We use python’s scikit-learn package for training the multinomial logistic regression model (more information here). It makes sure that P j βj = 0. We use the following parameters for training the model using scikit-learn: – Penalty: L2 – Solver: LBFGS – Max. iterations: 200 – Multi-class: Multionomial • Multi-layer perceptron (MLP) Classifier - We fit a simple neural network on our dataset, and use the last layer’s logits as probabilities for each class. We use python’s scikit-learn package for training the MLP Classifier. We use the following parameters for training the model - – Hidden layer configuration: (7, ) – Activation function: Logistic – Solver: Adam – Max. iterations: 500 We choose MLR for our user models, as it is interpretable, and offers similar (if not better) performance as compared to a generic neural network (see Figure 5). Interpretability is important here since we would like to vary the treatment effect in our generative user models. Figure 4 represents the learnt coefficients of the MLR user model for classes 1 to 3, relative to class 0’s coefficients. Note that both survey completion and app usage seem to exhibit strong relationship wrt the target variable for most of the users. To be specific, in Figure 4, coefficients of both survey completion and app usage are mostly positive across most of the N = 42 users, both in baseline and advantage. The magnitude of the relative weights of these features keeps increasing as the reward class increases. This signifies that if a user is engaged (completing surveys, using the app), they are more likely to generate a non-zero reward as compared to a reward of 0. We use the probabilities as weights from the MLR models to stochastically generate the reward during the data generation process. More on that in Section A.8. 1 0 1 2 Coefficient Class 1 intercept Class 1 survey_completion Class 1 std_app_usage Class 1 std_cannabis_use Class 1 weekend Class 1 std_day 1 0 1 2 Coefficient Class 1 act_intercept Class 1 act_survey_completion Class 1 act_std_app_usage Class 1 act_std_cannabis_use Class 1 act_weekend Class 1 act_std_day 1 0 1 2 Coefficient Class 2 intercept Class 2 survey_completion Class 2 std_app_usage Class 2 std_cannabis_use Class 2 weekend Class 2 std_day 1 0 1 2 Coefficient Class 2 act_intercept Class 2 act_survey_completion Class 2 act_std_app_usage Class 2 act_std_cannabis_use Class 2 act_weekend Class 2 act_std_day 1 0 1 2 Coefficient Class 3 intercept Class 3 survey_completion Class 3 std_app_usage Class 3 std_cannabis_use Class 3 weekend Class 3 std_day 0 10 20 30 40 User ID 1 0 1 2 Coefficient Class 3 act_intercept 0 10 20 30 40 User ID Class 3 act_survey_completion 0 10 20 30 40 User ID Class 3 act_std_app_usage 0 10 20 30 40 User ID Class 3 act_std_cannabis_use 0 10 20 30 40 User ID Class 3 act_weekend 0 10 20 30 40 User ID Class 3 act_std_day Figure 4: Bar plot of coefficients of features in the MLR user models relative to coefficients of class 0, across all N = 42 users. A.7 Dataset for generative process Next, we will create a dataset for the generative process for a simulation. To that end, we impute missing values in the evening dataset, and also create the morning dataset. We describe the procedure for both below. • Imputation in evening dataset: As stated before, the vanilla evening dataset has a few missing values for “cannabis use” and “action” values. We impute the values for “cannabis use” as follows - for a given missing value, we first determine the day of the week, and then replace the unknown/missing value with the average of the cannabis use across the available data of the user for that day of the week. Note that during the simulation, the action used by the user models for reward generation will be determined by the RL algorithm in the simulation. Hence, we do not need to impute “action” here. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 User ID 0.0 0.2 0.4 0.6 0.8 1.0 1.2 Log Loss Multinomial Logistic Regression MLP Classifier Figure 5: Comparison of log loss between the two models across all users • Generating morning dataset: Similar to the evening dataset, we generate a morning dataset per user to mimic the data we would receive from MiWaves (2 decision points a day). We generate the following features: – Cannabis Use: We generate the morning cannabis use as follows: Morning CB Use = That Day’s SARA CB Use × 0.33 × 1.5 (36) – App Usage: Since there are less than 30 evening app usage values per user in the SARA dataset, we decide to use these values as an empirical distribution, and resample, with replacement, from the 30 values for each user at each morning. The sampled value is the user’s morning app usage. – Survey Completion: For each user, we determine the proportion of times they responded to the daily survey during the SARA study. Using this ratio as our probability of survey completion, we sample from a binomial distribution to construct the Survey Completion feature for the morning dataset for each user for each day. – Day In Study and Weekend indicator: We create one morning data point per day, so the day in study and weekend indicator features mimic the evening dataset. A.8 Simulations: Data generation In this section, we detail our data generation process for a simulation. We assume that any given user i, her trajectory in an RL simulation environment with T total decision points has the following structure: H(T ) i = {S(1) i , a(1) i , R(1) i , · · · , S(T ) i , a(T ) i , R(T ) i }. We also assume that the combined dataset has the following structure - for any given decision point t (t ∈ [1, 60]), the state, S(t) i , is constructed (partially) based on: the user’s app usage from t−1 to t, the survey completion indicator (whether user fills the survey after receiving action at t) for decision point t, and the cannabis use of the user from t − 1 to t. The data at decision point t from the combined dataset is used to generate R(t) i , which in turn helps generate features to form S(t+1) i (refer Section 2). 1. Given a set number of users (parameter of the simulator) to simulate, we sample users with replacement from the N = 42 users in the combined dataset. 2. We start the simulation in the morning of a given day. From decision point t = 1 to T in the simulation (T = 60), for each sampled user i (refer to the RL framework in Section 2): (a) If t > 1, given previously-generated reward R(t−1) i , we construct the following features - survey completion, app usage indicator, and activity question (which will help form S(t) i according to Section 2), according to the following rules: Survey Completion = ( 1 if R(t−1) i ≥ 2 0 if R(t−1) i < 2 (37) App Usage Indicator = ( 1 if R(t−1) i ≥ 1 0 if R(t−1) i = 0 (38) Activity Question = \u001a 1 if R(t−1) i = 3 0 otherwise (39) We communicate the aforementioned calculated features (Equation 37, 38, and 39), along with the cannabis use from t − 2 to t − 1 to the RL algorithm. This is similar to how a user would self-report and convey this information to the app, to help the algorithm derive the user current state S(t) i . At t = 1, since there is no R(0) i , note that the RL algorithm uses some initial state S(1) i . Since we intend to simulate MiWaves , all RL algorithms will use the following S(1) i : • S(1) i,1 : Set to 0. At the start of the trial, there is no engagement by the user. • S(1) i,2 : Set to 0. The first decision point is the morning, which corresponds to 0. • S(1) i,3 : Set to 1. The users in MiWaves use cannabis regularly (at least 3x a week), so we ascertain the users in the trial to have high cannabis use at the start of the trial. (b) We ask the RL algorithm for the action a(t) i to be taken at the current decision point t for user i. (c) We take the data from the tth decision point in combined dataset of the user i - specifically the user’s survey completion at t, the user’s app usage from t − 1 to t, and the user’s cannabis use from the t − 1 to t. We calculate the weekend indicator by checking whether the decision point t falls on Saturday or Sunday. We feed this data, along with the action a(t) i from the previous step, to the user i’s trained user model from Section A.6 to obtain a reward R(t) i . A.9 Environment Variants We will now describe the design of different environment variants for our simulator, and how we go about operationalizing them. Since we know that the MiWaves pilot study will have 120 participants, all our simulation environments will have N = 120 participants. We sample these participants with replacement from the N = 42 participants from the combined dataset during the data generation process. 1. Varying size of the treatment effect: We construct three variants wrt the size of the treatment effect: • Minimal treatment effect size: We keep the treatment effect i.e. advantage coefficients in the MLR model that we learn from the users in the SARA data. For the other two variants, we set the advantage intercept weight for each of the MLR user models as follows: we find the minimum advantage intercept weight across all classes learnt using the SARA user data, and if it is not assigned to class 0 - we swap the weight with that of class 0. Then we set the advantage intercept weight of class 2 and class 3 to be the average of both. The reason we do so is because that we believe that at any given point, taking an action will always have a non-negative immediate treatment effect. To that end, at any given time, taking an action will always result in higher probabilities for generating a non-zero reward, and lower probabilities for generating a zero reward, as compared to not taking an action. Setting class weights by assigning the minimum weight to reward class 0 helps us achieve that. Also, since we trained the user models from SARA data where we assigned 2 and 3 reward with equal probability wherever reward of 2 was assigned, we set their corresponding advantage intercept weights to be equal. Hence, we set them to be the average of the observed weights of the two reward classes. Moreover, we decide to work with the user’s learnt weights from SARA to preserve heterogeneity among users. We then multiply the advantage intercept weights for all the classes by a factor, which we refer to as the multiplier. We select the multipliers for our other variants after observing the standardized advantage intercept weights. To do so, for each multiplier, we first generate a dataset of trajectories for each of the N = 42 users, by sampling action with probability 0.5. We generate 500 sets of such trajectories (i.e. 500 simulations with N = 42 users in each simulation). For each set of trajectories (i.e each simulation) of all N = 42 users, we fit a GEE linear model with fixed effects and robust standard errors. The model is given as: E h R(t+1) i |S1, S2, S3, a i = α0 + α1S1 + α2S2 + α3S3 + α4S1S2 + α5S1S3 + α6S2S3 + α7S1S2S3 + a(β0 + β1S1 + β2S2 + β3S3 + β4S1S2 + β5S1S3 + β6S2S3 + β7S1S2S3). (40) We take the learned weight of β0 and divide it by the sample standard deviation of observed rewards for that set of trajectories (i.e. for that simulation) of N = 42 users - in order to obtain the standardized action intercept weight. We do so for all the different multipliers we consider, and the results are summarized in Fig 6. We first check the minimal effect (weights from SARA), and see that the standardized treatment effect for the minimum effect setting comes out to be around 0.12. We want our low treatment effect setting to be closer to 0.15 and our higher to be around 0.3. So we tinker our weights as mentioned in the procedure above, and re-run the simulations. Since we do not scale the weights (yet) by any factor, this is equivalent to a multiplier setting of 1.0. We observe a standardized treatment effect of 0.18 in this case. Figure 6: Comparison of mean standardized action intercept weight vs advantage intercept multiplier. Mean is taken across 500 simulations with N = 42 users (from SARA) in each simulation Using this as a reference point, we try a lower and higher multiplier, and we observe that 0.7 and 2.5 give us the desired standardized treatment effect for low effect and high effect settings respectively. Now, the remaining variants are described as follows: • Low: We multiply the advantage coefficients for each of the MLR user models for each class by 0.7. • High: We multiply the advantage coefficients for each of the MLR user models (as mentioned above) for each class by 2.5. This way, we are increasing the size of all advantage intercepts in the MLR user model, and in turn further skews the probability of getting a higher reward when taking an action as compared to a zero reward. Discussion: Given the way we model our user models using MLR, they are not complex enough to support higher stan- dardized effect sizes (through advantage intercepts). Recall that the probability of a data point belonging to a particular reward class c is defined using our MLR model as follows: P(Yi = c|Xi) = eβc·Xi PK j=1 eβj·Xi Note that we use a linear model in the exponent of the above MLR model, i.e. βjXi is linear. This is due to our design choices of trying to keep our models simple and interpretable. Since the weights are constrained (always sum up to 1), there is a threshold upto which one can increase the standardized treatment effect through these user models. We see this behavior in Figure 6, where we observe that the standardized effect size hovers around 0.35 − 0.4, even though we are scaling the advantage intercept weights by larger multipliers. 2. Habituation (affecting the baseline effect): In order to incorporate user habituation to repeated stimuli (i.e. intervention messages) into the user models, we first define dosage for a user i at time t as Q(t) i = dκ P6 j=1 κj−1a(t−j) i , i.e. weighted average of the number of actions sent to the user i in the recent 6 decision points prior to decision point t. We set κ = 5 6 to represent looking back 6 decision points, and scale each sum by a constant dκ = 1−κ 1−κ6 so that weights add up to 1. To incorporate habituation into the MLR user models, we add a new baseline weight for dosage. For a given reward class c (where c ∈ {0, 1, 2, 3}), we set this weight in the MLR user model as follows: βc,dosage =            6 P i=1 βc,i η if 6P i=1 β0,i ≥ 0 − 6 P i=1 βc,i η if 6P i=1 β0,i < 0 (41) where the numerator is the sum of all the baseline weights (since there are 6 baseline weights, see Appendix A.6), and η is used to control the intensity of habituation. Notice that we flip the sign of the weight if the sum of the baseline weights for reward class 0 is negative. This is done to ensure that higher dosage (i.e. higher levels of repeated stimuli in the near past) leads to an increase in likelihood of generating lower rewards (namely reward classes 0 and 1). We set the weight in this way (by summing the baseline weights) to keep the user models as heterogeneous as possible. However, since all the user models now have a clear negative effect wrt dosage, these modified user models are perceived to be less heterogeneous than the ones originally learnt from the SARA data. We construct two variants by modifying the value of η, to vary the intensity of habituation in the user models: • Low: Low intensity habituation effect is set by using η = 6. • High: High intensity habituation effect is set by using η = 1. The higher the habituation effect, the higher the likelihood of getting lower rewards when dosage Q(t) i is non-zero. 3. Proportion of Users with habituation: For the environments with habituation, we vary the proportion of the population who can experience habituation. To that end, we construct two variants with respect to the proportion of the simulated study population who can experience habituation (i.e. proportion of the simulated population whose models are augmented to have weights for dosage as described above) - (i) 50% and (ii) 100% Using a combination of the user model variants mentioned above, we design a total of 15 environment variants: 1. Minimal treatment effect without habituation 2. Minimal treatment effect with low habituation in 50% of the population 3. Minimal treatment effect with low habituation in 100% of the population 4. Minimal treatment effect with high habituation in 50% of the population 5. Minimal treatment effect with high habituation in 100% of the population 6. Low treatment effect without habituation 7. Low treatment effect with low habituation in 50% of the population 8. Low treatment effect with low habituation in 100% of the population 9. Low treatment effect with high habituation in 50% of the population 10. Low treatment effect with high habituation in 100% of the population 11. High treatment effect without habituation 12. High treatment effect with low habituation in 50% of the population 13. High treatment effect with low habituation in 100% of the population 14. High treatment effect with high habituation in 50% of the population 15. High treatment effect with high habituation in 100% of the population B Full-pooling algorithm - Bayesian Linear Regression (BLR) For a given user i at decision point t, the action-centered training model for Bayesian Linear Regression (BLR) is defined as: R(t) i = g(S(t) i )T α + (a(t) i − π(t) i )f(S(t) i )T β + (π(t) i )f(S(t) i )T γ + ϵ(t) i (42) where π(t) i is the probability of taking an active action i.e. sending an intervention message (a(t) i = 1). g(S(t) i ) and f(S(t) i ) are the baseline and advantage vectors respectively. ϵ(t) i is the error term, assumed to be independent and identically distributed (i.i.d.) Gaussian noise with mean 0 and variance σ2 ϵ . The probability π(t) i of sending a intervention message (a(t) i = 1) is calculated as: π(t) i = E ˜β∼N (µ(t−1) post ,Σ(t−1) post )[ρ(f(S(t) i )T ˜β)|H(t−1) 1:m , S(t) i ] (43) where H(T ) i = {S(1) i , a(1) i , R(1) i , · · · , S(T ) i , a(T ) i , R(T ) i } is the trajectory for a given user i upto time t. The Bayesian model requires the specification of prior values for α, β and γ. While we do not assume the terms α, β and γ to be independent, we have no information about the correlation between the α, β and γ terms, and hence we set the correlation to be 0 in our prior variance Σprior. Hence, Σprior = diag(Σα, Σβ, Σβ) is the prior variance of all the parameters (note that the posterior var-covariance matrix will have off diagonal elements). Next µprior = (µα, µβ, µβ) is the prior mean of all the parameters. The prior of each parameter is assumed to be normal and given as: α ∼ N(µα, Σα) (44) β ∼ N(µβ, Σβ) (45) γ ∼ N(µβ, Σβ) (46) Since the priors are Gaussian, and the error term is Gaussian, the posterior distribution of all the parameters, given the history of state-action-reward tuples (trajectory) up until time t is also Gaussian. Let us denote all the parameters using θT = (αT , βT , γT ). The posterior distribution of θ given the current history H(t) = {S(τ), a(τ), R(τ)}τ≤t (data of all users) is denoted by N(µ(t) post, Σ(t) post), where: Σ(t) post = \u0012 1 σ2ϵ ΦT 1:tΦ1:t + Σ−1 prior \u0013−1 (47) µ(t) post = Σ(t) post \u0012 1 σ2ϵ ΦT 1:tR1:t + Σ−1 priorµprior \u0013 (48) Φ1:t is a stacked vector of Φ(S, a)T = [g(S(t))T , (a(t) − π(t))f(S(t))T , π(t)f(S(t))T ] for all users, for all decision points from 1 to t. R1:t is a stacked vector of rewards for all users, for all decision points from 1 to t. σ2 ϵ is the noise variance, Σprior = diag(Σα, Σβ, Σβ) is the prior variance of all the parameters, and µprior = (µα, µβ, µβ) is the prior mean of all the parameters. We initialize the noise variance by σ2 ϵ = 0.85; see Equation (49) for the update equation. See Appendix D for the choice of 0.85. Then we update the noise variance by solving the following optimization problem, that is maximizing the marginal (log) likelihood of the observed rewards, marginalized over the parameters: σ2 ϵ = argmax[− log(det(X + yA)) + mt log(y) − y X τ∈[t] X i∈[m] (R(τ) i )2 + (Xµprior + yB)T (X + yA)−1(Xµprior + yB)] (49) where X = Σ−1 prior, y = 1 σ2 ϵ , A = ΦT 1:tΦ1:t, B = ΦT 1:tR1:t, t is current total number of decision points, and m is the total number of users. C Baseline and Advantage Functions The baseline and advantage functions g(S) and f(S) are defined as follows: g(S) = [1, S1, S2, S3, S1S2, S2S3, S1S3, S1S2S3] (50) f(S) = [1, S1, S2, S3, S1S2, S2S3, S1S3, S1S2S3] (51) D Initialization values and Bayesian Priors Parameter Significance (S/I/M) Mean Variance Intercept S 2.12 (0.78)2 S1 I 0.00 (0.38)2 S2 M 0.00 (0.62)2 S3 S −0.69 (0.98)2 S1S2 M 0.00 (0.16)2 S1S3 I 0.00 (0.1)2 S2S3 M 0.00 (0.16)2 S1S2S3 M 0.00 (0.1)2 a Intercept I 0.00 (0.27)2 aS1 I 0.00 (0.33)2 aS2 M 0.00 (0.3)2 aS3 I 0.00 (0.32)2 aS1S2 M 0.00 (0.1)2 aS1S3 I 0.00 (0.1)2 aS2S3 M 0.00 (0.1)2 aS1S2S3 M 0.00 (0.1)2 Table 4: Prior values for the RL algorithm informed using the SARA data set. The significant column signifies the significant terms found during the analysis using S, insignificant terms using I, and missing terms with M. Values are rounded to the nearest 2 decimal places. This section details how we calculate the initial values and priors using SARA data (Sec. A) to estimate the reward, given the state. First, we define the model for the conditional mean of the reward R(t) i of a given user i at time t, given the current state S = {S1, S2, S3} (dropping the user index and time for brevity): E h R(t) i |S1, S2, S3, a i = α0 + α1S1 + α2S2 + α3S3 + α4S1S2 + α5S1S3 + α6S2S3 + α7S1S2S3 + a(β0 + β1S1 + β2S2 + β3S3 + β4S1S2 + β5S1S3 + β6S2S3 + β7S1S2S3). (52) Note that the reward model in Eq. 52 is non-parametric, i.e. there are 16 unique weights, and E h R(t) i |S1, S2, S3, a i has 16 dimensions. We follow the methods described in [Liao et al., 2019] to form our priors using the SARA dataset, which involved fitting linear models like Eq. 52 using GEE. We do a complete-case analysis on the SARA data, and transform it into State- Action-Reward tuples to mimic our RL algorithm setup. However, as noted in the previous section, the SARA dataset does not account for the entire state-space, specifically S2, i.e. time of day, as the users in the study were requested to self-report on a daily survey just once a day. To that end, we omit all terms from Eq. 52 which contain S2 when forming our analysis to determine priors. Hence, our reward estimation model while forming priors is given as: E h R(t) i |S1, S3, a i = α0 + α1S1 + α3S3 + α5S1S3 + a(β0 + β1S1 + β3S3 + β5S1S3). (53) D.1 State formation and imputation We operationalize the states of the RL algorithm from the SARA dataset as follows: • S1: Same as defined in Section 2 • S2: This is not present in the SARA data set. • S3: Same as defined in Section 2 We work with complete-case data. So, whenever there is missing “cannabis use” in the past decision point (since Y = 1 only), we impute and set S3 to 1. This is because participants in the MiWaves study are expected to often use cannabis (at least 3 times a week). Also, since the data is complete-case, we do not use the first day’s (i.e. first data point for a user) reward to fit the model, as we choose to not impute the state of the user on the first day while forming our priors. D.2 Feature significance We run a GEE linear regression analysis with robust standard errors to determine the feature significance. We categorize a feature to be significant when it’s corresponding p-value is less than 0.05. The GEE Regression results are summarized in Fig. 7. Using the criteria mentioned above, we classify the intercept (α0), and the S3 term (α3) to be significant. Figure 7: GEE Results D.3 Initial value of Noise variance (σ2 ϵ ) To form an initial of the noise variance, we fit a GEE linear regression model (Eq. 53) per user, and compute the residuals. We set the initial noise variance value as the average of the variance of the residuals across the N = 42 user models; that is σ2 ϵ,0 = 0.85 in our simulations. Parameter Value σ2 ϵ,0 0.85 Table 5: Initial value of noise variance (a) Variant 0: Posterior means (b) Variant 0: Posterior variance Figure 8: Average posterior means and variances in the minimal treatment effect environment with no habituation D.4 Prior mean for α and β To compute the prior means, we first fit a single GEE regression model (Eq. 53) across all the users combined. For the significant features (intercept and S3), we choose the point estimates of α0, and α3 to be their corresponding feature prior means. For the insignificant features, we set the prior mean to 0 (α1, α5, β0, β1, β3, and β5). For the prior means of the weights on the S2 terms which are not present in the GEE model, we also set them to 0. D.5 Prior standard deviation for α and β To compute the prior standard deviation, we first fit user-specific GEE regression models (Equation 53), one per user. We choose the standard deviation of significant features, corresponding to α0, and α3, across the N = 42 user models to be their corresponding prior standard deviations. For the insignificant features, we choose the standard deviation of α1, α5, β0, β1, β3, and β5 across the N = 42 user models, and set the prior standard deviation to be half of their corresponding values. The rationale behind setting the mean to 0 and shrinking the prior standard deviation is to ensure stability in the algorithm; we do not expect these terms to play a significant impact on the action selection, unless there is a strong signal in the user data during the trial. In other words, we are reducing the SD of the non-significant weights because we want to provide more shrinkage to the prior mean of 0 (i.e. more data is needed to overcome the prior). For the prior standard deviation of the S2 terms which are in the baseline (i.e. all the α terms), we set it to the average of the prior standard deviations of the other baseline α terms. Similarly, we set the prior standard deviation of the S2 advantage (β) terms as the average of the prior standard deviations of the other advantage β terms. We further shrink the standard deviation of all the two-way interactions by a factor of 4, and the three-way interactions by a factor of 8 - with a minimum prior standard deviation of 0.1. Recall that we expect little to no signal from the data wrt. the higher order interactions of the binary variables, thus this decision. Unless there is strong evidence in the user data during the MiWaves pilot study, we do not expect these interaction terms to play a significant role in action selection. D.6 Initial values for variance of random effects Σu reBandit assumes that each weight αi,j is split into a population term αi,pop and an individual term ui,j (random effects) for a given feature i and user j. These random effects capture the user heterogeneity in the study population, and the variance of the random effects are denoted by Σu. We set the initial values for Σu to be Σu,0 = (0.1)2 × IK, where K is the number of random effects terms. We shrink this variance as we allow the data to provide evidence for user heterogeneity. We set the off-diagonal entries in the initial covariance matrix to 0 as we do not have any information about the covariance between the random effects. D.7 Empirical check for prior variance shrinkage During on our prior formulation, we shrink the variance of higher order interactions. This shrinkage allows us to deploy complex models without many drawbacks. However, it relies on the idea that if there is evidence of higher order interactions in the study/data, the algorithm will learn and identify that signal. This empirical check is to establish that our algorithms are able to do so, when the study environment does provide evidence of such interactions. In this empirical check, we are primarily concerned with the variance of the action intercept or the advantage intercept coefficient. (a) Variant 0: Posterior means (b) Variant 0: Posterior variance Figure 9: Average posterior means and variances in high treatment effect environment with no habituation First, we run experiments with minimal effect/signal. To that end, we run 500 simulated clinical trials in the minimal treatment effect environment with no habituation, with each trial consisting of N = 120 simulated users and lasting T = 60 decision points (30 days). We use smooth posterior sampling to determine the action selection probability (described in Section 4.2). We also use the BLR algorithm (described in Appendix B). Next, we run experiments with evidence/signal in the data, in order to check whether we have shrunk the prior variances too much. To that end, we check if our pooled algorithm is able to identify the signal in the high treatment effect environment with no habituation. We compare the three variants in terms of the learnt average posterior means and variances (Figure 9). We see that the algorithm is able to learn the signal added to the action intercept weight. This demonstrates that the shrinkage of the prior variance of the action intercept term is appropriate for learning, when the user data supplies strong evidence of a signal. Evidence of the algorithm being able to pick up these signals allow us to run more complex models (in terms of the baseline g(S) and advantage f(S) functions) without many drawbacks. We are able to do so because we shrink the prior variances of the high order interactions by a lot. E Smooth Posterior Sampling Recall that the probability of sending an intervention message is computed as: π(t) i = E ˜β∼N (µ(t−1) post ,Σ(t−1) post )[ρ(f(S(t) i )T ˜β)|H(t−1) 1:N , S(t) i ] (54) ρ(x) = Lmin + Lmax − Lmin 1 + c exp(−bx) (55) where Lmin = 0.2 and Lmax = 0.8 are the lower and upper clipping probabilities. For now, we set c = 5. Larger values of c > 0 shifts the value of ρ(0) to the right. This choice implies that ρ(0) = 0.3. Intuitively, the probability of taking an action when treatment effect is 0, is 0.3. We define b = B σres . Larger values of b > 0 makes the slope of the curve more “steep”. σres is the reward residual standard deviation obtained from fitting our reward model on data from SARA. We have σres in the denominator, as it helps us standardize the treatment effect. The intuition is that the denominator in ρ(f(s)β) has the term β/σres in the exponential, which becomes the standardized treatment effect. We currently set σres = 0.95. In order to arrive at this value, we first simulated each of N = 42 unique users from SARA in a MiWaves simulation with low treatment effect and no habituation effect. The reason we introduce low treatment effect, is because we see that SARA itself had minimal to no treatment effect. Actions are selected with 0.5 probability. We ran 500 such simulations, and fit each simulation’s data into a GEE model, and computed the standard deviation of the residuals. 0.95 was the mean of the residual standard deviation across these 500 simulations. We also set B = 20. We arrive at this value after comparing the performance of reBandit with B = 10 and B = 20 across multiple simulation environments. Hence, we set b = B σres = 20 0.95 = 21.053. F Implicit state-features The random effects model, described in Sec 4.1, induces implicit state-features. The posterior distribution for an individual user can be written in terms of converging population statistics (which use the data across the entire trial population), and the individual user’s data (morphed into an implicit feature). While this implicit feature for a user is not apparent from the posterior expressions in Eq. 14 and Eq. 15, we derive the joint posterior of the parameters θpop and ui (refer Appendix F.1). It turns out that the posterior distribution of \u0014θpop ui \u0015 is jointly normal: \u0014θpop ui \u0015 ∼ N \u0012 \u0014 λ M \u0015 , \u0014 V1 V2 V3 V4 \u0015 \u0013 (56) where U = ψ−1 i (−Aiλ + Bi) (57) V1 = (mE)−1 (58) V2 = −(mE)−1Aiψ−1 i (59) V3 = −ψ−1 i Ai(mE)−1 (60) V4 = σ2 ϵ ψ−1 i + ψ−1 i Ai(mE)−1Aiψ−1 i (61) λ = E−1 \u0012 1 mΣ−1 priorµprior + 1 σ2ϵ ζ1 − 1 σ2ϵ ζ2 \u0013 (62) E = 1 mΣ−1 prior + 1 σ2ϵ ζ3 − 1 σ2ϵ ζ4 (63) ζ1 = 1 m X i Bi (64) ζ2 = 1 m X i Aiψ−1 i Bi (65) ζ3 = 1 m X i Ai (66) ζ4 = 1 m X i Aiψ−1 i Ai (67) ψi = σ2 ϵ Σ−1 u + Ai (68) Ai = t X τ=1 ΦiτΦT iτ (69) Bi = t X τ=1 ΦiτR(τ) i (70) We observe that the random effects model induces the state-features Ai and Bi described in Eq. 69 and Eq. 70 for each user, which depends on the data for that particular user. It is also evident that the posterior depends on the statistics ζ1, ζ2, ζ3, and ζ4 - all of which can be expected to converge in probability as m increases. These implicit features facilitate the after-study analysis of reBandit, which is left for future work. F.1 Derivation To start, the mixed effects model can be written as: R(t) i = ΦT i,tθi + ϵi,t (71) = ΦT i,t(θpop + ui) + ϵi,t (72) And by definition, the random effects are normal with mean 0 and variance Σu, i.e. ui ∼ N(0, Σu). As described before, we assume the noise to be Gaussian i.e. ϵ ∼ N(0, σ2 ϵ Imt). We also assume the following prior on the population term: θpop ∼ N(µprior, Σprior) (73) Therefore, we can write the prior distribution as: log(prior) ∝ −1 2(θpop − µprior)T Σ−1 prior(θpop − µprior) + m X i=1 −1 2uT i Σ−1 u ui (74) The likelihood is given as: log(likelihood) ∝ m X i=1 t X τ=1 − 1 2σ2ϵ (Riτ − ΦT iτθpop − ΦT iτui)2 (75) Given the prior and the likelihood, the posterior distribution is given as follows: log(posterior) = const − 1 2(θpop − µprior)T Σ−1 prior(θpop − µprior) + m X i=1 −1 2uT i Σ−1 u ui − m X i=1 t X τ=1 − 1 2σ2ϵ (R(τ) i − ΦT iτθpop − ΦT iτui)2 (76) Define Ai = P τ ΦiτΦT iτ, and Bi = P τ ΦiτRiτ. And, A =   Pt τ=1 Φ1τΦT 1τ 0 · · · 0 0 Pt τ=1 Φ2τΦT 2τ · · · 0 ... ... ... ... 0 0 · · · Pt τ=1 ΦmτΦT mτ   (77) B =   B1... Bm   =   Pt τ=1 Φ1τR(τ) 1 ... Pt τ=1 ΦmτR(τ) m   (78) Then, rewriting the posterior, we get: = const − 1 2(θpop − µprior)T Σ−1 prior(θpop − µprior) − 1 2 m X i=1 uT i Σ−1 u ui − 1 2σ2ϵ θT pop \u0012 X i,τ Ai \u0013 θpop − 1 2σ2ϵ X i uT i Aiui − 1 σ2ϵ θT pop \u0012 X i Aiui \u0013 + 1 σ2ϵ X i BT i θpop + 1 σ2ϵ X i BT i ui (79) = const − 1 2θT pop \u0012 Σ−1 prior + 1 σ2ϵ X i Ai \u0013 θpop + \u0012 Σ−1 priorµprior + 1 σ2ϵ X i BT i \u0013 θpop − 1 σ2ϵ θT pop ˜ Au − 1 2uT Du + 1 σ2ϵ BT u (80) where ˜ A = [A1, . . . , Am] ∈ Rp×mp, p = dim(Φit) and D =   Σ−1 u + 1 σ2 ϵ A1 . . . 0 0 0 Σ−1 u + 1 σ2ϵ A2 . . . 0 ... ... ... ... 0 0 . . . Σ−1 u + 1 σ2ϵ Am   (81) Continuing the derivation, log(posterior) = const − 1 2 \u0014θpop u \u0015T \" Σ−1 prior + 1 σ2 ϵ P i Ai 1 σ2 ϵ ˜A 1 σ2ϵ ˜ AT D # \u0014θpop u \u0015 (82) + \" Σ−1 priorµprior + 1 σ2ϵ P i Bi 1 σ2 ϵ B #T \u0014θpop u \u0015 (83) Continuing the derivation, log(posterior) = const − 1 2 \u0014θpop u \u0015T \" Σ−1 prior + 1 σ2ϵ P i Ai 1 σ2ϵ ˜A 1 σ2ϵ ˜ AT D # \u0014θpop u \u0015 (84) + \" Σ−1 priorµprior + 1 σ2 ϵ P i Bi 1 σ2ϵ B #T \u0014θpop u \u0015 (85) Define I = \" Σ−1 prior + 1 σ2ϵ P i Ai 1 σ2ϵ ˜A 1 σ2ϵ ˜ AT D # (86) JT = \" Σ−1 priorµprior + 1 σ2ϵ P i Bi 1 σ2ϵ B #T (87) Then we get, log(posterior) = const − 1 2 \u0014θpop u \u0015T I \u0014θpop u \u0015 + JT \u0014θpop u \u0015 (88) = const − 1 2  \u0014θpop u \u0015 − I−1J !T I  \u0014θpop u \u0015 − I−1J ! (89) Hence, the joint posterior distribution of θpop and u is jointly normal, as is given as: \u0014θpop u \u0015 ∼ N(I−1J, I−1) (90) Now, we can write, I = \" Σ−1 prior + 1 σ2ϵ P i Ai 1 σ2ϵ ˜A 1 σ2ϵ ˜ AT D # = \u0014 C22 C21 C12 C11 \u0015 (91) =⇒ I−1 = \" \u0000C22 − C21C−1 11 C12 \u0001−1 − \u0000C22 − C21C−1 11 C12 \u0001−1 C21C−1 11 −C−1 11 C12 \u0000C22 − C21C−1 11 C12 \u0001−1 C−1 11 + C−1 11 C12 \u0000C22 − C21C−1 11 C12 \u0001−1 C21C−1 11 # (92) ≜ \u0014 (mE)−1 −(mE)−1C21C−1 11 −C−1 11 C12(mE)−1 C−1 11 + C−1 11 C12(mE)−1C21C−1 11 \u0015 (93) We evaluate the expression for each block in the block matrix. Then, we get mE = Σ−1 prior + 1 σ2ϵ X i Ai − 1 σ4ϵ ˜AD ˜A T (94) = Σ−1 prior + 1 σ2ϵ X i Ai − 1 σ4ϵ X i Ai(Σ−1 u + 1 σ2ϵ Ai)−1Ai (95) = Σ−1 prior + 1 σ2ϵ X i Ai − 1 σ2ϵ X i Aiψ−1 i Ai (96) =⇒ E = 1 m \u0012 Σ−1 prior + 1 σ2ϵ X i Ai − 1 σ2ϵ X i Aiψ−1 i Ai \u0013 (97) where ψi = σ2 ϵ Σ−1 u + Ai. Next, −C−1 11 C12(mE)−1 = − 1 σ2ϵ D−1 ˜A T (mE)−1 (98) = −   ψ−1 1 A1 ... ψ−1 m Am   (mE)−1 (99) Next, −(mE)−1C21C−1 11 = −(mE)−1 \u0002 A1ψ−1 1 . . . Amψ−1 m \u0003 (100) And finally, C−1 11 + C−1 11 C12(mE)−1C21C−1 11 = D−1 + \" ψ−1 i Ai(mE)−1Ajψ−1 j # i,j∈[m] (101) Plugging in these values, we get I−1J = \u0014 (mE)−1 −(mE)−1C21C−1 11 −C−1 11 C12(mE)−1 C−1 11 + C−1 11 C12(mE)−1C21C−1 11 \u0015 \" Σ−1 priorµprior + 1 σ2ϵ P i Bi 1 σ2ϵ B # (102) = \" (mE)−1(Σ−1 priorµprior + 1 σ2ϵ P i Bi) − 1 σ2ϵ (mE)−1C21C−1 11 B −C−1 11 C12(mE)−1(Σ−1 priorµprior + 1 σ2ϵ P i Bi) + 1 σ2ϵ (C−1 11 + C−1 11 C12(mE)−1C21C−1 11 )B # (103) Then, the upper block matrix evaluates to: (mE)−1(Σ−1 priorµprior + 1 σ2ϵ X i Bi) − 1 σ2ϵ (mE)−1C21C−1 11 B (104) = (mE)−1(Σ−1 priorµprior + 1 σ2ϵ X i Bi) − 1 σ2ϵ (mE)−1 \u0002 A1ψ−1 1 . . . Amψ−1 m \u0003 B (105) = (mE)−1(Σ−1 priorµprior + 1 σ2ϵ X i Bi) − 1 σ2ϵ (mE)−1 X i Aiψ−1 i Bi (106) = (E)−1 1 m(Σ−1 priorµprior + 1 σ2ϵ X i Bi) − 1 σ2ϵ (E)−1 1 m X i Aiψ−1 i Bi (107) Next, the bottom term in the block matrix of 103 evaluates to: − C−1 11 C12 (mE)−1   Σ−1 priorµprior + 1 σ2ϵ X i Bi ! | {z } ≜f + 1 σ2ϵ \u0000C−1 11 + C−1 11 C22(mE)−1C21C−1 11 \u0001 B (108) = −C−1 11 C12f + 1 σ2ϵ D−1B + 1 σ2ϵ \" ψ−1 i Ai(mE)−1Ajψ−1 j # i,j∈[m] B (109) = −   ψ−1 1 A1 ... ψ−1 m Am   f +   ψ−1 1 B1 ... ψ−1 m Bm   +   ψ−1 1 A1 ... ψ−1 m Am   1 σ2ϵ (mE)−1 X i Aiψ−1 i Bi | {z } ≜g (110) =   ψ−1 1 A1 ... ψ−1 m Am   (g − f) +   ψ−1 1 B1 ... ψ−1 m Bm   (111) Thus, posterior distribution of \u0014θpop ui \u0015 is jointly normal with posterior mean: \u0014θpop ui \u0015 ∼ N \u0012 \u0014 λ ψ−1 i (−Aiλ + Bi) \u0015 , \u0014 (mE)−1 −(mE)−1Aiψ−1 i −ψ−1 i Ai(mE)−1 σ2 ϵ ψ−1 i + ψ−1 i Ai(mE)−1Aiψ−1 i \u0015 \u0013 (112) where λ = E−1 \u0012 1 mΣ−1 priorµprior + 1 σ2ϵ ζ1 − 1 σ2ϵ ζ2 \u0013 (113) E = 1 mΣ−1 prior + 1 σ2ϵ ζ3 − 1 σ2ϵ ζ4 (114) ζ1 = 1 m X i Bi (115) ζ2 = 1 m X i Aiψ−1 i Bi (116) ζ3 = 1 m X i Ai (117) ζ4 = 1 m X i Aiψ−1 i Ai (118) ψi = σ2 ϵ Σ−1 u + Ai (119) Ai = X τ ΦiτΦT iτ (120) Bi = X τ ΦiτR(τ) i (121) G Posterior update derivation The posterior distribution can be derived as follows. Given the following, θ = θpop + u (122) θpop ∼ N(µprior, Σprior) (123) ui ∼ N(0, Σu) (124) =⇒ θ ∼ N(µθ, Σθ) (125) where µθ =   µprior µprior ... µprior   (126) Σθ =   Σprior + Σu Σprior · · · Σprior Σprior Σprior + Σu · · · Σprior ... ... ... ... Σprior Σprior · · · Σprior + Σu   (127) Here dim(µprior) = p, dim(Σprior) = p × p, dim(µθ) = pm and dim(Σθ) = pm × pm, where p = dim(Φ). Then, define ˜Σθ,t =   Σprior + Σu,t Σprior · · · Σprior Σprior Σprior + Σu,t · · · Σprior ... ... ... ... Σprior Σprior · · · Σprior + Σu,t   (128) So, we can write: prior ∝ exp \u0012 − 1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) \u0013 (129) likelihood ∝ Y i,τ exp \u0012 − 1 2σ2 ϵ,t (R(τ) i − ΦT i,τθi)2 \u0013 (130) posterior ∝ prior × likelihood (131) =⇒ log(posterior) ∝ −1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) − 1 2σ2 ϵ,t m X i=1 t X τ=1 (R(τ) i − ΦT i,τθi)2 (132) ∝ −1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) − 1 2σ2 ϵ,t m X i=1 (θi T ( t X τ=1 ΦiτΦT i,τ)θi − 2( n X τ=1 ΦiτR(τ) i )T θi) (133) Now, we can define: A =   Pt τ=1 Φ1τΦT 1τ 0 · · · 0 0 Pt τ=1 Φ2τΦT 2τ · · · 0 ... ... ... ... 0 0 · · · Pt τ=1 ΦmτΦT mτ   (134) B =   Pt τ=1 Φ1τR(τ) 1 ... Pt τ=1 ΦmτR(τ) m   (135) Then, we can rewrite the log posterior as: log(posterior) ∝ −1 2θT ˜Σ−1 θ,tθ + (˜Σ−1 θ,tµθ)T θ − 1 2σ2 ϵ,t θT Aθ + 1 σ2 ϵ,t BT θ (136) = −1 2θT \u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013 θ + \u0012 (˜Σ−1 θ,tµθ)T + 1 σ2 ϵ,t BT \u0013 θ (137) Therefore, we can write the posterior mean and variance of the parameters θ at time t as: µpost,t = \u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1\u0012 ˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B \u0013 (138) Σpost,t = \u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1 (139) H Hyper-parameter update derivation Using Empirical Bayes to maximize the marginal likelihood of observed rewards, marginalized over the parameters θ, we get: l(Σu, σ2 ϵ,t; H) = Z θ Y τ∈[t] Y i∈[m] l(R(τ) i |θi) Pr(θ|Σu, σ2 ϵ,t)dθ (140) Now, Pr(θ|Σu, σ2 ϵ,t)dθ = Pr(θ|Σu)dθ (141) = 1 q (2π)2m det(˜Σθ,t) exp \u0012 −1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) \u0013 (142) And, Y τ∈[t] Y i∈[m] l(R(τ) i |θi) = Y τ∈[t] Y i∈[m] 1 q 2πσ2 ϵ,t exp \u0012 − 1 2σ2 ϵ,t (R(τ) i − ΦT iτθi)2 \u0013 (143) = 1 (2πσ2 ϵ,t) mt 2 exp \u0012 − 1 2σ2 ϵ,t (θT Aθ − 2BT θ + X τ∈[t] X i∈[m] (R(τ) i )2) \u0013 (144) Therefore, we can write: l(Σu, σ2 ϵ,t; H) = Z θ 1 q (2π)2m det(˜Σθ,t) exp \u0012 −1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) \u0013 1 (2πσ2 ϵ,t) mt 2 exp \u0012 − 1 2σ2 ϵ,t (θT Aθ − 2BT θ + X τ∈[t] X i∈[m] (R(τ) i )2) \u0013 dθ (145) = Z θ C1 z }| { \u0012 1 q (2π)2m det(˜Σθ,t) \u0013 · \u0012 1 (2πσ2 ϵ,t) mt 2 \u0013 · exp \u0012 − 1 2σ2 ϵ,t X τ∈[t] X i∈[m] (R(τ) i )2 \u0013 exp \u0012 −1 2(θ − µθ)T ˜Σ−1 θ,t(θ − µθ) − 1 2σ2 ϵ,t (θT Aθ − 2BT θ) \u0013 dθ (146) = C1 Z θ C2 z }| { exp \u00121 2(˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B)T (˜Σ−1 θ,t + 1 σ2 ϵ,t A)−1(˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B) − 1 2µT θ ˜Σ−1 θ,tµθ \u0013 exp \u0012 − 1 2 \u0012 θ − \u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1\u0012 ˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B \u0013\u0013T \u0012\u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1\u0013−1\u0012 θ − \u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1\u0012 ˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B \u0013\u0013\u0013 dθ (147) = C1C2 s (2π)2m det \u0012\u0012 ˜Σ−1 θ,t + 1 σ2 ϵ,t A \u0013−1\u0013 (148) =   1 s (2π)mt det \u0012 I + 1 σ2 ϵ,t ˜Σθ,tA \u0013 !  1 (σ2 ϵ,t) mt 2 ! exp \u0012 − 1 2σ2 ϵ,t X τ∈[t] X i∈[m] (R(τ) i )2 − 1 2µT θ ˜Σ−1 θ,tµθ \u0013 exp \u00121 2(˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B)T (˜Σ−1 θ,t + 1 σ2 ϵ,t A)−1(˜Σ−1 θ,tµθ + 1 σ2 ϵ,t B) \u0013 (149) =⇒ log(l(Σu, σ2 ϵ,t; H)) ∝ log(det(X)) − log(det(X + yA)) + mt log(y) − y X τ∈[t] X i∈[m] (R(τ) i )2 − µT θ Xµθ + (Xµθ + yB)T (X + yA)−1(Xµθ + yB) (150) where X = ˜Σ−1 θ,t and y = 1 σ2 ϵ,t . "
}