{
    "optim": "ζ-QVAE: A Quantum Variational Autoencoder utilizing Regularized Mixed-state Latent Representations Gaoyuan Wang1,2† Jonathan Warrell 1,2† Prashant S. Emani1,2 Mark Gerstein1,2,3,4,5∗ 1 Program in Computational Biology and Bioinformatics, 2 Department of Molecular Biophysics and Biochemistry, 3 Department of Computer Science, 4 Department of Statistics & Data Science, 5 Department of Biomedical Informatics & Data Science, Yale University, New Haven, Connecticut 06520, USA † These authors contributed equally to this work. *Corresponding author: pi@gersteinlab.org Abstract A major challenge in near-term quantum computing is its application to large real-world datasets due to scarce quantum hardware resources. One approach to enabling tractable quantum models for such datasets involves compressing the original data to manageable dimensions while still representing essential information for downstream analysis. In classical machine learning, variational autoencoders (VAEs) facilitate efficient data compression, representation learning for subsequent tasks, and novel data generation. However, no model has been proposed that exactly captures all of these features for direct application to quantum data on quantum computers. Some existing quantum models for data compression lack regularization of latent representations, thus preventing direct use for generation and control of generalization. Others are hybrid models with only some internal quantum components, impeding direct training on quantum data. To bridge this gap, we present a fully quantum framework, ζ-QVAE, which encompasses all the capabilities of classical VAEs and can be directly applied for both classical and quantum data compression. Our model utilizes regularized mixed states to attain optimal latent representations. It accommodates various divergences for reconstruction and regularization. Furthermore, by accommodating mixed states at every stage, it can utilize the full-data density matrix and allow for a “global” training objective. Doing so, in turn, makes efficient optimization possible and has potential implications for private and federated learning. In addition to exploring the theoretical properties of ζ-QVAE, we demonstrate its performance on representative genomics and synthetic data. Our results consistently indicate that ζ-QVAE exhibits similar or better performance compared to matched classical models. 1 Introduction Autoencoders play an important role in current machine learning systems, enabling compression of data, learning latent representations, and as generative models. Classical variational autoencoders (VAEs) provide a unified modeling framework which combines these strengths, and more recent classical models have extended this framework to allow a trade-off between reconstruction and 1 arXiv:2402.17749v1  [quant-ph]  27 Feb 2024 information captured by the latent space [1], to maximize the coverage of the latent space and hence avoid generating spurious patterns [2], and to incorporate more complex encoders and decoders [3, 4] In the Noisy Intermediate-Scale Quantum (NISQ) era, quantum technologies are progressing rapidly, and classical machine learning methods are rapidly being generalized to operate in a quantum machine learning setting. Yet, the limited availability of quantum hardware and restrictions on the number of qubits in actual quantum devices underscores the need to minimize quantum resource requirements. In this work, we introduce a fully generalized quantum variational autoencoder (QVAE) framework, which answers the challenges above by allowing efficient quantum data compression. Our framework preserves or generalizes all the key features of classical VAE models, while directly operating on quantum data, to which classical compression methods cannot be directly applied. Notably, our proposed framework is valuable not just for quantum datasets but also for classical datasets due to the following potential advantages: (1) Quantum superposition offers the inherent advantage of a much richer representation space than classical binary bits. This enables potentially more efficient representations of data, crucial for compression into a compact latent space; (2) The entanglement of qubits can be utilized to capture intricate dependencies in the original data via the encoding into latent states, which classical methods may be unable to represent efficiently; (3) Our framework employs quantum probability in place of classical distributions; for instance, we replace the classical Gaussian distributions typically used in VAEs with quantum mixed states. A large number of proposals have been made to provide quantum analogues of autoencoder models [5, 6, 7, 8, 9]. Mostly, such models learn a quantum circuit to directly maximize the reconstruction of input quantum states. However, this approach has several shortcomings. While such quantum autoencoder analogs are optimized for the reconstruction of quantum states, they do not include a regularization term over the latent space, and hence cannot be used directly for generation and do not explicitly control generalization. We note that such models are ‘variational’ in the sense that their quantum circuits may be trained using an approximate Ansatz, which differs from the approximation of a prior distribution which induces the regularization term in a classical VAE objective; we will thus refer to this type of model as a Quantum Autoencoder (QAE). Further, the training for such QAEs assumes a particular form for the reconstruction error (quantum fidelity), and hence cannot be directly generalized if other forms of objective are required. In the quantum context, many different measures of similarity between quantum states have been proposed in addition to fidelity; the restriction to a particular similarity measure is thus undesirable. Another shortcoming of such QAEs is that the input, encoded and output states are all assumed to be pure states; this sacrifices a unique potential advantage of quantum models for handling large datasets in parallel and embedding information using mixed quantum states. An alternative kind of model is a hybrid quantum-classical analogue, such as the Ref. [10], which is a classical VAE with a quantum Boltzmann distribution imposed upon its latent variables. Although such hybrid models are trained as generative models, their objective is defined using a bound on the classical log-likelihood (hence they are trained to generate and reconstruct classical data). They cannot therefore be trained directly on quantum inputs, and moreover sacrifice the potential virtues of handling data efficiently through mixed quantum states. Hence, none of the current models is able to integrate the advantages of mixed-state input and latent representations, flexible objectives and effective, fully quantum regularization. However, such capabilities are particularly important for scaling up quantum models to handle real-world datasets with large feature spaces by compressing them down to dimensions feasible for NISQ quantum hardware. We therefore introduce a gate-based quantum variational autoencoder framework with a training objective that includes a latent space regularization term (and hence may be viewed as a generalized form of probabilistic generative models). We refer to our model as a ζ-QVAE, where ζ represents the density matrix of the mixed-state latent representation in our model (analogous to the classical 2 latent state, Z). The ζ−QVAE allows for regularized latent representations to exist as mixed states within the Bloch sphere, which further enriches the representation space, and potentially allows more efficient data compression. In our framework, the encoder and decoder pairs are modeled as quantum operations (completely positive trace-preserving (CPTP) linear maps), to provide mixed- state latent representations for quantum inputs (which may be mixed or pure states). Our model also offers multiple divergence options for the reconstruction and regularization losses, since specific losses are commonly used for particular applications of Quantum Machine Learning. Specifically, we show how i) fidelity [11], frequently utilized in quantum state tomography [12], ii) quantum relative entropy/quantum Jensen-Shannon divergence, significant in the field of quantum information theory [13, 14, 15], and iii) quantum Wasserstein distance, applied in generative models [16], can be integrated into our framework. Furthermore, we show that a quantum information-theoretic analogue to the classical evidence lower bound (ELBO) exists for the regularized reconstruction loss when the quantum relative entropy is used. Moreover, we formulate both local and global versions of each divergence, which allow models to be optimized for reconstruction of individual data points or the dataset as a whole, respectively. In particular, the quantum Wasserstein distance can be shown to give rise to equivalent optimal models under both global and instance-based objectives, while the other global divergences were observed to give similarly good performance under both on a real-world genomics dataset. The global version of each divergence allows efficient optimization by reducing the number of repeated quantum operations during the training. It also has implications for private and federated learning, since it requires only aggregate information about the dataset as opposed to individual data points. Further, we discuss suitable application domains for the ζ-QVAE, as well as addressing the challenges associated with implementing our framework on NISQ hardware. 2 Theoretical Framework We assume that our data live in an input Hilbert space, X, over NX qubits, and that we wish to learn an encoder/decoder pair to compress our dataset to Hilbert space, Z, over NZ qubits (NZ ≤ NX). Our dataset consists of a finite set of N pure states, |ψ1⟩ ... |ψN⟩ (which may themselves be generated from classical data-points, e.g. by amplitude encoding, or may be generated from an intrinsically quantum source). We use the following definitions for the input density matrices of individual datapoints (indexed by i), and a global density matrix representing the entire dataset: ρi = |ψi⟩ ⟨ψi| ρglob = 1 N X i ρi (1) We note that our dataset may be considered a finite sample from a distribution across pure states over X, and hence ρglob may be considered a finite-sample approximation to an underlying data distribution. Our goal is to learn quantum operations (completely positive trace-preserving (CPTP) linear maps) (E, D) corresponding to an encoder and decoder respectively, where these have the respective signatures E : D(X) → D(Z) and D : D(Z) → D(X) (here, D(X) denotes the set of density matrices over finite Hilbert space X; we note also that, due to circuit constraints, we may have E ∈ SE and D ∈ SD, where SE and SD are subsets of CPTP linear maps having a predefined maximum circuit complexity). Given an (E, D) pair, we define: 3 ζi = E(ρi) σi = D(ζi) (2) where ζi and σi represent the latent and reconstructed state respectively associated with input state ρi, and similarly, ζglob = E(ρglob) and σglob = D(ζglob). Further, we assume we have a predefined ‘prior’ density matrix over the latent space, ζgen; below, we will take this to be the maximally mixed state, ζgen = (1/2NZ)I2NZ . As in the classical case, this prior, ζgen, is transformed by the decoder to produce a generative approximation of the data distribution, σgen = D(ζgen). For concreteness, to define our encoder and decoder (see Fig. 1 and Fig. 2), we append NA auxiliary qubits to our input Hilbert space X, and NT reference qubits along with NB auxiliary qubits to our latent Hilbert space Z (NT = NX − NZ, where NT denotes the number of ‘trash’ qubits). Then, we can use the following definitions: E(ρ) = TrNA+NT (U −1(ρ ⊗ |0NA⟩ ⟨0NA|)U) D(ζ) = TrNB(V −1(ζ ⊗ |0NB+NT ⟩ ⟨0NB+NT |)V ) (3) where U and V are unitary matrix representations of the encoder and decoder circuits (E and D) respectively, and TrN(.) denotes the trace over the final N qubits (where, in general, the qubits may be ordered/indexed arbitrarily, although below we assume that the auxiliary qubits are ordered after those in X and Z). In Appendix A (Prop. 1), we prove that setting NA = NB = NZ is sufficient to allow arbitrary pairs of quantum operations (E, D) to be learned (assuming no circuit complexity constraints). Global Training Objective: To derive a training loss for the model above, we assume that we are interested in learning a model which minimizes a completely general loss L1(a, b) (the only assumptions being that it is non-negative and 0 iff a = b, but not necessarily symmetric, i.e., a divergence) between the implicit generative model and the global data density matrix (note that we derive an alternative instance-based objective below); hence we seek to optimize: min D L1(ρglob, σgen) (4) We note that Eq. 4 involves only the decoder, D. In analogy with the classical VAE, to simultaneously learn a representation of our data in the latent space, we introduce a variational density parameterized by our encoder E, which we assume (temporarily) to be expressive enough to fulfill the condition ζglob = ζgen: min D L1(ρglob, σgen) = min E,D s.t.ζglob=ζgen L1(ρglob, σglob) (5) We can reformulate Eq. 5 as a constrained optimization problem, introducing a second (regularization) loss L2 (with the same conditions as L1): min E,D L1(ρglob, σglob) L2(ζglob, ζgen) ≤ ϵ (6) 4 To account for the fact that our class of encoders may not allow L2(ζglob, ζgen) = 0 to be fulfilled, we introduce the constant ϵ = minE(L2(ζglob, ζgen)) in Eq. 6. Finally, introducing the Lagrange multiplier β ≥ 0, we derive our training objective F for a global input density matrix, ρglob: min E,D s.t.L2(ζglob,ζgen)≤ϵ L1(ρglob, σglob) ≥ max β min E,D Fglob(E, D, β) Fglob(E, D, β) = L1(ρglob, σglob) + β(L2(ζglob, ζgen) − ϵ) (7) In place of the maximization across β on the RHS (upper) of Eq. 7, we treat β as a hyperparameter when optimizing Fglob, and we disregard the constant −βϵ for the objectives used in the ζ-QVAE (as defined in Eq. 22 and Eq. 23). We also show, in Appendix A (Prop. 2), that when L1 and L2 are the quantum relative entropy, β = 1 and ϵ = 0, that Fglob(E, D, β) forms an analogue of the classical Evidence Lower-Bound (ELBO), as in the classical VAE [17] (we note that this bound is distinct from the Q-ELBO bound in [10], since the Q-ELBO is a bound on the classical log-likelihood, while Prop. 2 is a bound on the quantum relative entropy). The ζ-QVAE objective therefore optimizes the original objective in Eq. 4 in the case that either L1 and L2 are the quantum relative entropy with β = 1, or β = β∗, where β∗ is the optimum value of β in the RHS (upper) of Eq. 7. Instance-based Training Objective: In the above, Eq. 7 provides an objective for training (E, D) based on the generation and reconstruction of the global data density matrix ρglob. However, we may be interested in the reconstruction of individual data points, which is not explicitly optimized in Eq. 7. For this reason, we consider the following optimization problem for instance-level generation/reconstruction: min D X i L1(ρi, σgen) (8) By a similar argument to above, this leads to the following instance-level objective, Finst: min E,D s.t.L2(ζi,ζgen)≤ϵ,∀i X i L1(ρi, σi) ≥ max β1...N min E,D Finst(E, D, β1...N) Finst(E, D, β1...N) = X i (L1(ρi, σi) + βi(L2(ζi, ζgen) − ϵ)) (9) As above, we treat the β’s as a hyperparameter, using a common β = β1 = ... = βN, and ignore the constant terms −βiϵ. Here, ϵ = minE(maxi L2(ζi, ζgen)), and Eq. 9 forms a strict lower-bound on Eq. 8 when ϵ = 0. In general, Fglob and Finst will lead to different optimization problems, and hence different solutions for (E, D); however, in Appendix A (Prop. 3), we show that for certain losses, the optimization problems in Eq. 7 and Eq. 9 become equivalent. We note finally that, if we have an auxiliary loss function L′ 1 for which: min E,D s.t.D(E(ρ))=σ L′ 1(ρ, E, D) = L1(ρ, σ), (10) 5 we may use the following alternative definitions of Fglob and Finst in Eq. 7 and Eq. 9: F ′ glob(E, D, β) = L′ 1(ρglob, E, D) + β(L2(ζglob, ζgen) − ϵ) F ′ inst(E, D, β1...N) = X i (L′ 1(ρi, E, D) + βi(L2(ζi, ζgen) − ϵ)) (11) This version of the bound will used below when L1 is the Wasserstein divergence. 3 Model 3.1 ζ-QVAE architecture The overall architecture of the proposed QVAE is given in Fig. 1 with an example of NX = 2 input qubits, a latent space of NZ = 1 qubit, and one auxiliary qubit (d1) in both the encoder and decoder (hence, NA = NB = 1). The encoder and decoder are defined by quantum circuits, with trainable parameters θe and θd respectively. The corresponding unitary matrices are denoted U(θe) and V (θd) respectively. Additionally, the unitary matrix Ai performs the conversion from a classical source to a quantum representation for data-point i (e.g. using amplitude or angle embedding). Hence, |ψi⟩ = Ai |0⟩, and ρi = |ψi⟩ ⟨ψi|. After the embedding and encoder circuits have been applied to the initial |0⟩ state, both the NA auxiliary qubits and the NT = NX − NZ trash qubits are discarded by a partial trace operation, and the remaining qubit q1 is considered the latent state. The encoder E as a whole therefore has the form defined by Eq. 3. To reconstruct original information from the latent state, NT + NB zero state qubits are added to the remaining qubits, and a final partial trace is performed across the auxiliary qubit d1; hence the decoder D is of the form in Eq. 3. In Fig. 2, the encoder circuit U(θe) we used in this study is shown for one trainable layer i.e. Nl = 1 (note that we use a data embedding circuit, Ai, to first project classical data to a quantum state, e.g. via amplitude encoding; this is not formally part of the ζ-QVAE encoder, and can be removed if a quantum data source provides the input state). The Ansatz (marked in beige) was introduced by [18] and contains Rzz entangling gates and single qubit Ry rotations. The decoder circuit contains the same Ansatz as the encoder. 3.2 Training objectives Reconstruction loss, L1: We provide here the explicit forms of all the divergences we consider for the reconstruction loss. As in Sec. 2, a divergence between two density matrices ρ and σ over the same Hilbert space is a non-negative function, L(ρ, σ), which is zero iff ρ = σ, but unlike a metric, need not be symmetric. For generality, we write all divergences below for arbitrary ρ and σ. However, we are particularly interested in the cases L(ρglob, σglob) and L(ρi, σi), denoting the divergence between input and output density matrices for the global and instance level objectives respectively (see Eq. 7 and Eq. 9). For these cases, ρi = |ψi⟩ ⟨ψi|, where |ψi⟩ is the state vector of the i-th input data-point, ρglob = (1/N) P i ρi, σi = D(E(ρi)), and σglob = D(E(ρglob)), where E and D are the quantum operation representations of the encoder and decoder, as in Sec. 2. The particular losses we consider for the reconstruction loss, L1(ρ, σ), are summarized below: • Fidelity loss: Lf 1(ρ, σ) = 1 − \u0012 Tr q√σρ√σ \u00132 , (12) 6 (a) (b) Figure 1: (a) Illustration of the architecture and objective function of the ζ-QVAE; arrows represent transformations between mixed states, ζgen is the maximally mixed state, and L1 and L2 are the reconstruction and regularization loss respectively. (b) ζ-QVAE overall circuit representation. q1 : |0⟩ Embedding Ai Rzz(θ1) Rzz(θ3) Ry(θ4) q2 : |0⟩ Rzz(θ2) Ry(θ5) d1 : |0⟩ Ry(θ6) Figure 2: Encoder circuit where, for a pure state ρ = |ψρ⟩ ⟨ψρ|, this reduces to: Lf 1(ρ, σ) = ⟨ψρ| σ |ψρ⟩. • Quantum relative entropy (KLD): Lkl 1 (ρ, σ) = S(ρ|σ) = S(ρ, σ) − S(ρ) = − Tr(ρ log(σ)) − S(ρ), (13) where S(ρ) = − Tr(ρ log(ρ)) and S(ρ, σ) = − Tr(ρ log(σ)) . • Symmetric quantum relative entropy (JSD) [15]: Ljsd 1 (ρ, σ) = S \u0010 ρ \f\f\f 1 2 [ρ + σ] \u0011 + S \u0010 σ \f\f\f 1 2 [ρ + σ] \u0011 . (14) 7 • Quantum Wasserstein-distance loss: Lw 1 (ρ, σ) = min T :T (ρ)=σ Tr(π(ρ, T )C), (15) where T is a quantum operation, and: π(ρ, T ) := X i pi(T (|ei⟩ ⟨ei|)) ⊗ (|ei⟩ ⟨ei|) (16) with |ei⟩ an orthogonal basis for ρ, hence ρ = P i pi |ei⟩ ⟨ei|, and C is defined as in [16]. As discussed in Sec. 2, we introduce the following auxiliary loss function in place of Lw 1 for the reconstruction loss when using the Quantum Wasserstein-distance: (L′ 1)w(ρ, E, D) = Tr(π(ρ, D ◦ E)C), (17) Clearly, we have: min E,D s.t.D(E(ρ))=σ (L′ 1)w(ρ, E, D) = Lw 1 (ρ, σ), (18) and so we can use the alternative form of the training objectives in Eq. 11 to optimize Lw 1 (ρ, σ). Regularization loss, L2: We write the regularization loss below in the general form L2(ζ, ζgen), i.e. a divergence between a mixed-state latent representation ζ and the analog of the classical generative ‘prior’ on the latent space, ζgen. As discussed in Sec. 2, we use ζgen = 1 λI, where I is the identity operator and λ the dimension of the latent Hilbert space. This represents the maximally mixed state, i.e. the quantum state with the maximal entropy. In principle, all the divergences above could be used for the regularization loss, L2. However, we exclude the quantum Wasserstein loss, since this would require us to minimize over an auxiliary circuit to find the lowest-cost transformation T between ζ and ζgen. We briefly summarize the remaining divergences used for L2, with the simplifications induced by setting ζgen = 1 λI. • Fidelity loss: Lf 2(ζ, ζgen) = 1 − \u0012 Tr qp ζgenζ p ζgen \u00132 . (19) • Quantum relative entropy (KLD): Lkl 2 (ζ, ζgen) = S(ζ, ζgen) − S(ζ) = Tr(ζ log(ζ)) − log(1/λ) = −S(ζ) + c, (20) where c = − log(1/λ). • Symmetric quantum relative entropy (JSD): Ljsd 2 (ζ, ζgen) = S \u0010 ζ \f\f\f 1 2 [ζ + ζgen] \u0011 + S \u0010 ζgen \f\f\f 1 2 [ζ + ζgen] \u0011 (21) Overall training objectives: For explicitness, we collect together the specific forms of the overall global and instance based training objectives used to train our model, based on Eq. 7 and Eq. 9 respectively: 8 Lglob(θe, θd, β) = L1(ρglob, σglob) + βL2(ζglob, ζgen) Linst(θe, θd, β) = X i (L1(ρi, σi) + βL2(ζi, ζgen)) (22) along with the alternative forms used for the Wasserstein reconstruction loss based on Eq. 11 and Eq. 17: L′ glob(θe, θd, β) = L′ 1(ρglob, E(θe), D(θd)) + βL2(ζglob, ζgen) L′ inst(θe, θd, β) = X i (L′ 1(ρi, E(θe), D(θd)) + βL2(ζi, ζgen)) . (23) 3.3 QSVC classifier In addition to the ability of the ζ-QVAE to reconstruct the original states, we are also interested in how well the latent and reconstructed states belonging to different classes can be effectively distinguished. In other words, we want to evaluate the classification performance on the latent and reconstructed states in comparison to the original input states. To evaluate this, we implemented a quantum kernel based classifier[18] with amplitude embedding. Our QSVC classifier is illustrated in Fig. 3 using an example with one trainable layer. We use the same Ansatz, which includes alternating Rzz and Ry gates for the quantum kernel, as employed in the encoder and decoder. The similarity kernel of our QSVC is obtained as the quantum fidelity between each data pair. Due to the nature of the gene expression data and the normalizations we applied to our data for the amplitude embedding, we have observed a concentration of fidelity scores towards the higher end rather than being spread across the entire range from zero to one. To address this, we introduced a scaling function f \u0010 ⟨vi|vj⟩ \u0011 = tan \u0010 π 2.03 ⟨vi|vj⟩ \u0011 to enhance resolution within the densely populated region. q1 : |0⟩ Embedding Ai Rzz(θ1) Rzz(θ3) Ry(θ4) QSV C q2 : |0⟩ Rzz(θ2) Ry(θ5) q3 : |0⟩ Ry(θ6) Figure 3: The overall QSVC architecture. 4 Experiments 4.1 Dataset We test our model on a synthetic dataset that is designed to be compressible and a large and noisy real-world gene expression dataset (including schizophrenia patients and controls) from the PsychENCODE project [19]. 9 4.1.1 PsychENCODE gene expression data In this dataset, the schizophrenia status for patients and controls is given together with the quantile normalized expression values of 16 selected genes, generated from RNAseq data from the prefrontal cortex of ∼ 1500 postmortem subjects from the PsychENCODE consortium [19]. These genes were selected from a panel of 555 genes, including pre-identified high-confidence schizophrenia genes and transcription factors. The 16 genes selected were those found to have the highest variance across patients. To allow for possible future applications of angle embedding on this dataset, we first conducted a global normalization on the data ⃗xi → π × [⃗xi − min(X)]/[max(X) − min(X)], where X is the entire dataset matrix (including all feature vectors from all data points) and ⃗xi the gene expression vector of the i-th data point. We employed amplitude embedding in this study; we therefore performed an additional per-data-point L2 normalization, which allowed us to process the data as state vectors. We randomly picked equal number of cases (patients) and controls to create a training and test partition of size 695 and 298 respectively. 4.1.2 Swiss Roll synthetic dataset In the interest of understanding the generalizability to different datasets, we run the ζ-QVAE and QSVC classifier on 1000 data points from the Swiss Roll dataset [20] as implemented in Python’s scikit-learn package. The Swiss Roll dataset involves a helically distributed sheet of 3-dimensional points that can be compressed to a 2-dimensional manifold. To adapt the dataset to our context, we take the 3-dimensional dataset and append 5 additional dimensions by adding Gaussian-distributed noise terms (zero-centered, standard deviation = 0.2). We applied a per-data-point L2 normalization to make the inputs suitable for the quantum circuit. Furthermore, we set up a classification task by designating approximately half of the points as “cases” and the other half as “controls”; the task is designed to allow perfect classification along the 2-dimensional manifold, thus serving to evaluate how well we capture the 2-dimensional manifold. Figure 4: The 3-dimensional Swiss Roll dataset. The colors indicate the labels for the classification task. 10 4.2 Model setup To train the ζ-QVAE, we employed the COBYLA optimizer, utilizing a training duration of 60 epochs with a patience setting of 20 epochs throughout the study. Additionally, we kept the number of layers Nl identical within both the encoder and decoder, as well as the count of auxiliary qubits in the encoder and decoder (NA and NB). When running the ζ-QVAE with NT trash qubits, we select the first NT qubits as trash qubits. All the performance results, including ζ-QVAE reconstruction rate and QSVC classification accuracies are averaged over five random initializations. We determined the number of layers of the QSVC classifier Ncl based on its performance on the input datasets. Notably, we observed that varying Ncl between one and three had negligible impact for the datasets comprising 16 input features. Nevertheless, to account for potential larger input feature dimensions, where a greater Ncl might be essential, we opted to set Ncl = 3 for the remainder of the study. Throughout this study, the test accuracy serves as the metric for the classification performance. 5 Results In this section, we begin by subsec. 5.1 offering an overview of different objective functions introduced in subsec. 3.2. Subsequently, we focus on the specific case of our model in which the negative fidelity serves as the reconstruction loss, complemented by the JSD as the regularization loss. We will refer to this specific objective function as Fid+JSD in the following. In subsec. 5.2 and subsec. 5.3, we provide a thorough investigation of the impact of the model architecture on the regularization, and consequently, on the quantum state reconstruction and downstream classification tasks using the Fid+JSD objective function. In subsec. 5.4 we test our framework with the global level objective function. Then in subsec. 5.5, we test the ζ−QVAE on the synthetic dataset. Finally in subsec. 5.6, we compare the ζ−QVAE with QAE and classical VAE models. In our model, the architecture is controlled by several hyperparameters, including the number of layers Nl in the encoder and decoder, the β-value and the number of auxiliary qubits in the encoder and decoder NA = NB. As we show, the impacts of these hyperparameters are not independent from each other. We evaluate the performance of each model using the fidelity reconstruction rate of the ζ-QVAE and the accuracy of the QSVC on the downstream classification tasks. The notation employed in this section is as follows: f(NA, Nl) represents the fidelity reconstruction rate of a given model with NA auxiliary qubits and Nl layers. Similarly, l(NA, Nl) signifies the QSVC test accuracy using the latent states of the corresponding model with NA auxiliary qubits and Nl layers as input, while r(NA, Nl) denotes the QSVC test accuracy using the reconstructed states as input. 5.1 Objective function choice: Choice of reconstruction loss We begin with the evaluation of the three different forms of recon- struction loss - fidelity, Wasserstein and JSD - at β = 0, which implies that the regularization term is excluded from the objective function. The results are presented in Tab. 1. We found that all three types of reconstruction loss behave qualitatively similarly at β = 0, which can be briefly summarized as follows (further details are elaborated in subsec. 5.2 and subsec. 5.3): • In the case no auxiliary qubits are employed in the decoder, the reconstructed state is identical to the latent state (extended by reference |0⟩ trash-qubits) up to a unitary transformation and thus results in substantially the same fidelity-based quantum kernel matrix for QSVC. Thus, the classification performance on the latent and reconstructed states is observed to be effectively the same. 11 Table 1: Comparison of the three types of reconstruction losses at β = 0. fidelity wasserstein JSD f(0, 3) 0.844 ± 0.043 0.853 ± 0.033 0.839 ± 0.037 l(0, 3) 0.647 ± 0.01 0.652 ± 0.007 0.652 ± 0.013 r(0, 3) 0.644 ± 0.01 0.65 ± 0.005 0.653 ± 0.012 f(0, 2) 0.898 ± 0.008 0.882 ± 0.013 0.875 ± 0.012 l(0, 2) 0.636 ± 0.008 0.65 ± 0.009 0.653 ± 0.011 r(0, 2) 0.637 ± 0.008 0.65 ± 0.01 0.654 ± 0.007 f(0, 1) 0.953 ± 0 0.771 ± 0.223 0.953 ± 0.0 l(0, 1) 0.606 ± 0.002 0.62 ± 0.019 0.605 ± 0.002 r(0, 1) 0.607 ± 0.003 0.622 ± 0.023 0.606 ± 0.005 f(1, 3) 0.742 ± 0.046 0.695 ± 0.024 0.686 ± 0.048 l(1, 3) 0.655 ± 0.003 0.65 ± 0.008 0.651 ± 0.005 r(1, 3) 0.627 ± 0.021 0.607 ± 0.026 0.63 ± 0.016 f(1, 2) 0.851 ± 0.029 0.838 ± 0.026 0.841 ± 0.03 l(1, 2) 0.653 ± 0.013 0.657 ± 0.011 0.653 ± 0.014 r(1, 2) 0.613 ± 0.019 0.601 ± 0.023 0.622 ± 0.019 f(1, 1) 0.886 ± 0.009 0.894 ± 0.005 0.9 ± 0.007 l(1, 1) 0.629 ± 0.018 0.64 ± 0.027 0.636 ± 0.023 r(1, 1) 0.574 ± 0.021 0.573 ± 0.03 0.571 ± 0.032 • Regardless of the presence of auxiliary qubits, with an increasing number of layers, the fidelity reconstruction rate decreases while the test accuracy of classification tasks improves for both latent and reconstructed states. This observation implies that increasing the number of layers may implicitly regularize the model, since the larger parameter search space increases the difficulty for the COBYLA optimizer of finding solutions with high reconstruction fidelity. • For any given value of Nl, incorporating auxiliary qubits results in a lower fidelity recon- struction rate compared to the case where no auxiliary qubits are used. However, the classification accuracy on the latent states improves slightly while the classification accuracy on the reconstructed states drops especially for smaller Nl. We observe that, generally, the negative fidelity loss is able to achieve better reconstruction perfor- mance, while performing comparably to the other losses on classification tasks; we therefore use fidelity reconstruction loss in the following sections. Choice of regularization loss: Next, we examine the different choices of regularization loss choices at different values of β. The results are shown in Tab. 2. Given the variations in overall scale among the different forms of regularization loss, our focus shifts to slightly different ranges of β for each. Recalling the baseline results at β = 0 from Tab. 1, the performance of the fidelity reconstruction loss is as follows: f(0, 3) = 0.844 ± 0.043, l(0, 3) = 0.647 ± 0.01 and r(0, 3) = 0.644 ± 0.01. Here, we observe that KLD has the least favorable performance among the regularization loss options in terms of reconstruction rate. All three regularization loss options seem to be comparable in classification test accuracy, although the model utilizing JSD performs is slightly better for the optimal β. Consequently, our focus in the next section is on the combination of negative fidelity reconstruction loss and JSD regularization loss. 5.2 Understanding the determinants of regularization and reconstruction in ζ−QVAE Models with appropriately tuned regularization, leading to an optimal degree of disentanglement in their latent representations, have been shown to outperform those lacking such adjustments due 12 Table 2: Negative fidelity reconstruction loss with three different regularization loss options. β = 0.5 β = 1 β = 1.5 β = 2 β = 2.2 β = 2.5 β = 2.7 f(0, 3) 0.876 ± 0.025 0.843 ± 0.02 0.813 ± 0.017 0.815 ± 0.013 0.763 ± 0.043 0.765 ± 0.038 0.728 ± 0.035 l(0, 3) 0.651 ± 0.007 0.651 ± 0.011 0.654 ± 0.012 0.653 ± 0.009 0.658 ± 0.005 0.669 ± 0.005 0.661 ± 0.011 r(0, 3) 0.653 ± 0.005 0.649 ± 0.009 0.652 ± 0.01 0.655 ± 0.01 0.661 ± 0.005 0.665 ± 0.003 0.061 ± 0.011 (a) JSD regularization loss β = 0.5 β = 1 β = 1.5 β = 2 β = 2.2 β = 2.5 f(0, 3) 0.773 ± 0.039 0.714 ± 0.037 0.664 ± 0.064 0.522 ± 0.085 0.53 ± 0.076 0.425 ± 0.068 l(0, 3) 0.654 ± 0.014 0.653 ± 0.012 0.652 ± 0.009 0.653 ± 0.014 0.656 ± 0.009 0.661 ± 0.007 r(0, 3) 0.654 ± 0.011 0.652 ± 0.012 0.652 ± 0.008 0.65 ± 0.014 0.658 ± 0.014 0.661 ± 0.007 (b) KLD regularization loss β = 0.5 β = 1 β = 1.1 β = 1.2 β = 1.5 β = 2 β = 2.5 f(0, 3) 0.863 ± 0.009 0.864 ± 0.025 0.847 ± 0.015 0.849 ± 0.015 0.834 ± 0.027 0.829 ± 0.033 0.798 ± 0.03 l(0, 3) 0.648 ± 0.007 0.646 ± 0.015 0.657 ± 0.008 0.661 ± 0.01 0.655 ± 0.005 0.653 ± 0.008 0.648 ± 0.019 r(0, 3) 0.649 ± 0.007 0.646 ± 0.014 0.658 ± 0.01 0.658 ± 0.01 0.657 ± 0.006 0.656 ± 0.01 0.651 ± 0.018 (c) Negative fidelity regularization loss to their ability to capture the independent underlying latent factors effectively [1]. In this section, we investigate how the degree of regularization (explicit and implicit, as discussed below) and the reconstruction rate of the ζ−QVAE are influenced by the interplay of several factors: the β-value, the presence of auxiliary qubits and the circuit complexity. We impose different circuit complexity constraints by varying the number of layers Nl in the encoder and decoder and studied a range of β-values from zero to six, while considering all combinations with and without one auxiliary qubit. The reconstruction ability of the model is estimated using the fidelity reconstruction rate. To quantify the regularization effect, we analyze the distribution of the latent states in the latent space by calculating the regularization loss. In addition, we take into account downstream classification performance as an additional metric for the evaluation of the reconstruction rate and degree of regularization. We noticed that the models with Nl = 1 and no auxiliary qubits often failed to converge at non-zero β values, leading to the large standard deviation of f(0, 1) in Tab. 3. For example, among the five random initializations at β = 2, two exhibited a test fidelity reconstruction rate around 0.5 while the remaining three had a fidelity of approximately 0.88. Similarly, at β = 1, three had fidelity around 0.5, and the remaining two showed a fidelity near 0.93. Further, for the models with Nl = 1 and one auxiliary qubit, we found that for the range of β ∈ [0, 6], the reconstruction fidelity remained constant within the error range as suggested by f(1, 1) in Tab. 3. Below we provide several key findings based on results obtained for Nl = 2 and 3. Regularization is controlled by β-value, model complexity and number of auxiliary qubits: Varying β is the most direct way to adjust the degree of the regularization. In Fig. 5, the fidelity reconstruction rate (which is 1 for a perfect reconstruction) and regularization loss (where 0 implies stronger regularization / smaller regularization loss) are shown as a function of β. We see that for the entire range of β considered in this study, higher β values lead to stronger regularization and worse fidelity reconstruction rates. One can also see in Fig. 5, that the regularization loss is smaller when using one auxiliary qubit compared to the scenario without any auxiliary qubits, across all values of β. In addition, the model complexity controlled by Nl can also influence the degree of the regularization. As we can see in Tab. 3 in the case where no auxiliary qubits are present, similar to our observation at β = 0, increasing the number of layers in the encoder and decoder also leads to a decreased reconstruction rate for β = 1 and 2, accompanied by improved classification performance. Increasing 13 Table 3: Fid+JSD objective function β = 0 β = 1 β = 2 f(0, 3) 0.844 ± 0.043 0.843 ± 0.02 0.815 ± 0.013 l(0, 3) 0.647 ± 0.01 0.651 ± 0.011 0.653 ± 0.009 r(0, 3) 0.644 ± 0.01 0.649 ± 0.009 0.655 ± 0.01 f(0, 2) 0.898 ± 0.008 0.883 ± 0.018 0.832 ± 0.027 l(0, 2) 0.636 ± 0.008 0.644 ± 0.009 0.653 ± 0.009 r(0, 2) 0.637 ± 0.008 0.644 ± 0.004 0.655 ± 0.007 f(0, 1) 0.953 ± 0.0 0.673 ± 0.212 0.728 ± 0.186 l(0, 1) 0.606 ± 0.002 0.646 ± 0.006 0.653 ± 0.008 r(0, 1) 0.607 ± 0.003 0.647 ± 0.003 0.653 ± 0.01 f(1, 3) 0.742 ± 0.046 0.649 ± 0.061 0.561 ± 0.066 l(1, 3) 0.655 ± 0.003 0.651 ± 0.011 0.651 ± 0.016 r(1, 3) 0.627 ± 0.021 0.613 ± 0.004 0.605 ± 0.011 f(1, 2) 0.851 ± 0.029 0.815 ± 0.03 0.686 ± 0.044 l(1, 2) 0.653 ± 0.013 0.638 ± 0.01 0.644 ± 0.005 r(1, 2) 0.613 ± 0.019 0.6 ± 0.027 0.593 ± 0.024 f(1, 1) 0.886 ± 0.009 0.887 ± 0.009 0.887 ± 0.006 l(1, 1) 0.629 ± 0.018 0.648 ± 0.005 0.648 ± 0.012 r(1, 1) 0.574 ± 0.021 0.588 ± 0.014 0.562 ± 0.022 the number of layers or the number of auxiliary qubits thus has a similar effect to increasing β, resulting in a form of implicit regularization as noted above in subsec. 5.1. At β = 0, the effect of increasing number of layers is more noticeable than at a higher value of β. The number of layers, auxiliary qubits and β can thus be viewed as jointly contributing to the regularization of the model. Figure 5: The two components of the objective function are plotted as a function of β for the case with one trash qubit and Nl = 3. 14 Reconstruction and regularization are strongly dependent in the absence of auxiliary qubits: In the case where no auxiliary qubits are used, the reconstructed states are obtained from the latent states by a unitary (linear) operation. This means effectively, both the reconstruction constraints and the regularization constraints are imposed to the same space (since the latent states are mapped to a linear subspace of the output space with the same intrinsic dimensionality). The reconstructed state thus inherits directly the same regularization as the latent state. This can be seen in the left panels in Fig. 6, where a lower reconstruction loss at smaller β can only be achieved by sacrificing the regularization loss, i.e. by allowing a higher regularization loss. Reconstruction and regularization are substantially decoupled in the presence of auxiliary qubits: In the presence of auxiliary qubits, we observed simultaneous decreasing curves for the regularization loss and the reconstruction loss in right panels of Fig. 6 (one should note that the reconstruction loss is the negative counterpart of the reconstruction fidelity shown in Fig. 5). This is because the presence of auxiliary qubits allows for non-unitary transformations from latent states to reconstructed states, thus allowing a separate optimization of the regularization loss and reconstruction loss. This feature of ζ−QVAE has no classical analogue and can be potentially utilized to devise a framework for controlling the degree of coupling between latent and reconstructed states, thus enabling a flexible trade-off between regularization and reconstruction rates. Reconstructing the original state poses challenges in the presence of auxiliary qubits: As shown in Tab. 3, the fidelity reconstruction rate is lower in the presence of one auxiliary qubit compared to its absence. In addition, we note a decrease in the classification performance on the reconstructed states when one auxiliary qubit is used compared to when no auxiliary qubits are used. Whereas for the latent states, the classification performance remains similar. The observed phenomenon may be explained by the removal of the constraint imposed by the coupled reconstruction loss and regularization loss when employing one auxiliary qubit, which introduces a greater challenge to the optimization process of the model parameters[21]. This difficulty in optimization may be exacerbated by the Barren plateau effect intensified by the inclusion of an additional qubit [22]. 5.3 Optimal representations for downstream classification tasks An optimal degree of regularization exists for the downstream classification performance: In Fig. 7 (a) and (b), we plot the downstream classification performance against β. We noticed that when no auxiliary qubits are used, an optimal range of β is associated with higher classification accuracy. For the one trash qubit case, i.e. NT = 1, shown in (a), the optimal β is found to be around 2.5 for both Nl = 2 and Nl = 3. We also note that the model using three layers achieved slightly higher classification accuracy than the two-layer model. On the other hand, when one auxiliary qubit is used, the regularization seems to have no clear impact on the downstream classification performance (red and green points). Nevertheless, we cannot conclude less significant improvements cannot be identified since there is a large range of uncertainty in the performance. In panel (b) where two trash qubits are used, the optimal β occurs around 2 for the scenario without auxiliary qubits. Although it is still difficult to determine if there exists an optimal range of β in the scenarios with one auxiliary qubit, we can see that the performance of the models with auxiliary qubits is slightly better than that without auxiliary qubits. In panels (c) and (d), we plot the test accuracy directly against the regularization loss. For both NT = 1 and NT = 2 without auxiliary qubits, we see a clear optimal range of regularization loss at 0.6 (for NT = 1) and 0.11 (for NT = 2). For models with auxiliary qubits, we included also negative βs, indicated by green points. This is motivated by the observation that, even for β = 0, the regularization was already stronger than the optimal range observed for cases without auxiliary qubits case (blue points). As shown in (c), l(1, 3) (red points) improves slightly with increasing regularization loss in the NT = 1 case, and this trend persists with a slight improvement for negative 15 (a) Nl = 2, no auxiliary qubits (b) Nl = 2, one auxiliary qubit (c) Nl = 3, no auxiliary qubits (d) Nl = 3, one auxiliary qubit Figure 6: The optimization process of the reconstruction loss and regularization loss are shown separately. In the absence of auxiliary qubits, the two components of the objective function are mutually dependent. Utilizing auxiliary qubits allows for decoupling and thus simultaneous improvement of both terms. β values. In (d), The data suggests an upward trend as the regularization loss decreases. However, this observed trend is less pronounced compared to models without auxiliary qubits and remains suggestive rather than conclusive. Regularization is more advantageous for smaller latent space: For NT = 1, the latent space is an eight-dimensional Hilbert space formed by three qubits while for NT = 2, the latent space is four-dimensional formed by two qubits. As shown in Fig. 7 (a) and (b), for the NT = 1 case, the regularization improves the classification performance by ≈ 4% while for NT = 2, the improvement is over 7.5%. 16 Classification performance on the latent states is similar to that on the input states: The classification performance of the employed QSVC on the input states is 0.675 ± 0.003.1 For one trash qubit case, i.e. compressing to half of the original dimensionality, the best classification performance achieved on the latent states is 0.669 ± 0.005 for β = 2.5, Nl = 3 and no auxiliary qubits. Notably, this is only 0.9% lower than that achieved with the full original states. For two trash qubits, i.e. compressing to a quarter of the original dimensionality, we achieved a classification performance of 0.63 ± 0.015 for β = 6, Nl = 3 and one auxiliary qubit. 5.4 Training using global objective Recall from Eq. 1, that the global state is defined as a mixed state over the entire input dataset. In this section, we test the performance of the ζ-QVAE using the global density matrix. We consider only the setup where negative fidelity serves as the reconstruction loss and JSD acts as regularization loss. In this scenario, our quantum circuit is trained on a single global input state, while the model construction is identical to that of the instance-level model. Hence, through the training phase, one single latent state and one output state are present. Following the completion of quantum circuit training, each individual instance-level input data point will be fed through the optimized model. For each data point within the original dataset, the associated latent state and reconstructed state are computed. Subsequently, calculations for the fidelity reconstruction rate calculation and downstream classification tasks are executed on the instance-level input, latent and reconstructed states. We tested a range of βs on the global ζ−QVAE and the results are shown in Fig. 8. While the reconstruction rate is slightly lower for nearly all βs, the overall pattern of the curve with respect to β is very similar to that of the instance-level trained models. In the down-stream classification tasks, the QSVC test accuracy achieved on the latent and reconstructed states remains comparable for ζ−QVAE models trained on both global and instance-level data. For l(0, 3), where an optimal β of 2.5 was observed for the instance-level trained models, the globally trained models exhibit an optimal β of three. Nevertheless, the disparity in performance falls within the error range. 5.5 Application to the Swiss Roll dataset To evaluate performance on the Swiss Roll dataset (Fig. 9), we considered the case where our 8-dimensional input state is mapped to 3 qubits and the latent state is determined by 1 qubit. For the ζ-QVAE, we set NT = 2, Nl = 3, β = {0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5}, and test the scenario with zero auxiliary qubits to one with 1 auxiliary qubit added to the encoder and decoder. The results yield similar conclusions to those of the gene expression dataset. The reconstruction fidelity on the leave-out test set steadily decreases with an increase in β, irrespective of the number of auxiliary qubits (Fig. 9b). In contrast, the test accuracy in the classification task using the latent state achieves a peak at β = 1.5 for the 0-auxiliary-qubits case, showing an improved test accuracy of 0.75 ± 0.04 relative to the accuracy of 0.60 ± 0.02 at β = 0. On the other hand, there is no clear benefit of a non-zero β for the 1-auxiliary-qubit case, at least at the values screened here (Fig. 9c). This may be due to the fact that the implicit regularization due to the inclusion of the auxiliary qubit is already quite strong at β = 0. Overall, the test accuracy remains reasonably high, with a maximum of 0.75 ± 0.04 (based on both the latent and reconstructed states; test AUC = 0.82 ± 0.05) 1We also tested a classical SVC with RBF kernel on the input states and obtained a classification performance of 0.648 ± 0.016, lower than that of the QSVC. The error range in this case is obtained by averaging over various data partitions. 17 (a) NT = 1 (b) NT = 2 (c) NT = 1, including negative β in green (d) NT = 2 Figure 7: (a) and (b): Classification performance is plotted against β. For both NT = 1 and 2, the no auxiliary qubit cases (orange and blue line) clearly show an optimal β with improved classification performance, while in the one auxiliary qubit case the optimal β range is unclear. For NT = 2 using auxiliary qubits is advantageous compared to no auxiliary qubits. (c) and (d): Plots test accuracy directly against regularization loss to eliminate uncertainties caused by the intermediate parameter β. For both NT = 1 and 2, while in the no auxiliary qubit case, there is clearly an optimal range for regularization loss, for the one auxiliary qubit case this is less clear. 18 (a) Fidelity reconstruction rate (b) Classification performance Figure 8: For NT = 1 and Nl = 3, we compare the globally trained with the instance-level trained model. The shaded areas represent the error range. for 0 auxiliary qubits and 0.77 ± 0.01 (based on the latent state; test AUC = 0.82 ± 0.02) for 1 auxiliary qubit. (a) Fidelity reconstruction rate (b) Classification performance Figure 9: NT = 2, Nl = 3. We evaluate the performance of the ζ-QVAE (a) and QSVC classifier on the latent states (b) using the Swiss Roll dataset. The shaded areas represent the error range. 5.6 Comparison to QAE and classical VAEs Quantum Autoencoder (QAE): At β = 0, ζ-QVAE without the regularization term and with a fidelity-based reconstruction loss is similar to the QAE introduced in Ref. [5] with the following minor differences: (1) The objective function to be maximized in Ref. [5], i.e. fidelity on the trash state F(ρt, |0⟩), serves as an upper bound of the actual reconstruction fidelity F(ρi, D(E(ρi))), which we optimize directly; (2) The decoder in QAE is the inverse of encoder, which is a special instance 19 of our decoder, whose parameters are independent from that of the encoder. Our results show that the ζ-QVAE achieves improved classification performance at β > 0, suggesting that models with regularization offer advantages compared to the QAE. Classical VAEs: We compare two types of classical β-VAEs [1] to the ζ−QVAE. The first type has a single linear layer without an activation function for both the encoder and decoder. The second type is a two-layer β-VAE with 12 hidden nodes and a RELU activation function. We consider classical β-VAEs with 8-, 4-, 2- and 1-dimensional latent spaces. Across these cases, we conducted tests over a wide range of β and presented the highest classification performance overall βs in Tab. 4. It is important to note that in the classical β-VAE, each dimension in the latent space includes a mean and a variance, resulting in two degrees of freedom per dimension. Therefore, a four/two/one latent dimensional classical VAE is comparable to three/two/one latent qubits in the ζ−QVAE, respectively. On the gene expression data, the fully quantum compression and classification scheme (QVAE+QSVC) reached a classification accuracy of 0.669 ± 0.005 using three latent qubits and 0.63 ± 0.015 with two latent qubits, outperforming the fully classical compression and classification scheme (VAE+SVC). On the synthetic Swiss Roll dataset, the ζ−QVAE with one latent qubit achieved a classification accuracy of 0.77±0.01, which is slightly higher than that of the classical β−VAE with one-dimensional latent space. In addition to the improved classification accuracy on the latent states, the number of parameters used by the ζ−QVAE is also much smaller than that of the classical VAE. For example, in the case of 16 input features, a single-layer classical β−VAE with a 4-dimensional latent space has 216 free parameters. In contrast, a 3-layer ζ−QVAE has only 60 free parameters for the same input features, but benefits from the high dimensionality of the Hilbert space associated with the latent state. It is also possible that the entanglement between the qubits employed by the model contributes to a reduction in the number of parameters needed for encoding the original data. Table 4: Classification performance on the latent representations gene expression data Swiss Roll dataset linear VAE standard VAE linear VAE standard VAE latent dim. = 8 0.653 ± 0.027 0.659 ± 0.01 − − latent dim. = 4 0.643 ± 0.017 0.646 ± 0.006 − − latent dim. = 2 0.615 ± 0.01 0.612 ± 0.019 0.784 ± 0.003 0.775 ± 0.006 latent dim. = 1 − − 0.708 ± 0.004 0.768 ± 0.004 6 Advantages of the ζ-QVAE framework In general, the application domains of ζ−QVAE are not expected to differ significantly from those of classical VAEs. However, certain distinctive features of ζ−QVAE can offer specific advantages in select applications. Application to large-scale datasets. Our framework addresses key challenges in applying quantum models to fields involving large-scale datasets by allowing big datasets with large feature spaces to be compressed into a smaller latent space while preserving essential information crucial for downstream analyses, such as classification. This reduces the necessary (quantum) data storage capacity and addresses the limited availability of quantum hardware by allowing subsequent analysis to be carried out by quantum devices with a small number of qubits. 20 Application to privacy-aware computation. Our formulation of global objectives holds potential for privacy-preserving computation, as it potentially eliminates the need for access to all the original data points during model training. Instead, only the global density matrix may be required, or alternatively samples may be provided from any equivalent quantum ensemble with the same density matrix (for instance, the eigenvectors in the basis which the data density matrix diagonalizes, weighted by their eigenvalues). Given that mixed states are composed of classical mixtures of pure states, which may not necessarily be orthogonal, it is possible for different sets of pure states to yield the same mixed state. As a consequence, the decomposition of a mixed state into an ensemble of pure states is not unique. Consequently, if only the global mixed state density matrix is provided, individual-level data cannot be recovered. Moreover, the global objective also offers potential for application in federated learning. In this scenario, the sub-ensembles of each actor may be transformed independently, as their density matrices can be combined additively to generate the full data matrix. Application to genomics studies. Combining the specific advantages of our framework, we are particularly driven by potential applications in genomics studies. Genomics studies involve large- scale datasets that are diverse in data modalities and often contain sensitive information. Our framework addresses key challenges in the integration of quantum computing within such domains by: 1. enabling the compression of large-scale data into a compact representation, 2. offering flexible selection of problem-specific objectives for various data types, and 3. providing methods to conceal sensitive training data, for instance in scenarios involving individual-level genomics and clinical data. 7 Implementation on near-term quantum devices It is important to note that implementing our framework on NISQ hardware presents challenges not addressed in this manuscript, such as implementing circuits to input amplitude-encoded state vectors [23], reading out latent mixed states with sufficient accuracy, and storage of the resulting density matrices. Specifically for our framework, efficient methods are needed for the divergence calculations between pairs of quantum states in the objective function, and for quantum state tomography, which is required for latent state readout and storage. For the latter, while there are several generally applicable approaches based on matrix-state tomography [24], neural-network-based tomography [25, 26], or the efficient calculation of density matrix properties [12], the question remains of how well these methods scale for the states learned by our model. Frameworks like ours, which primarily utilize mixed states, may encounter additional practical difficulties due to the large number of parameters required to fully characterize the state, which in turn would impact the number of state samples needed for accurate readout of the states. In our current simulation-based implementation, the latent states are represented by a small number of qubits, but scaling up could demand the incorporation of additional methods when applied to NISQ hardware. 8 Discussion We have introduced a novel fully quantum VAE architecture, named ζ-QVAE, which utilizes mixed- state latent representation and provides a flexible framework in which a wide range of quantum reconstruction losses and regularizers can be combined in a unified way. Further, a theoretical analysis can be given of the objective functions we introduce, which optimize quantum analogues of the variation bounds underlying the classical β-VAE and Wasserstein-AE. A notable feature of our framework is that mixed states are treated analogously to classical distributions, significantly generalizing previous QAE architectures. Our results show that our model outperforms classical and alternative QAE models with matched architectures on reconstruction and classification tasks. Moreover, we show that the ability to fine-tune the regularization of the latent states allows our 21 model to optimize its representations for down-stream classification tasks, and there is a complex interplay between regularization and model architecture (including circuit complexity, latent space dimensionality and the inclusion of auxiliary qubits) in determining performance on downstream tasks. We further show that our model performs consistently well when trained using a global mixed-state to represent the data, as opposed to individual pure states per data point, thus indicating promising application potential in private and federated learning settings. With such considerations in mind, we propose that our framework may be ideally suited to construct- ing practical quantum models in application areas involving large-scale, heterogeneous and potentially privacy-aware dataset such as genomics. In future work, we intend to further investigate how to utilize the observed interaction between model architecture, explicit and implicit regularization, and downstream task performance from the point of view of representational complexity [27]. Further, we intend to investigate how explicit privacy guarantees and federated versions of our approach may be derived for training our model based on our global objective. Finally, we will investigate the potential of our model to provide efficient compression of intrinsically quantum sources, and implementations of our approach on quantum hardware. Code availability The code to run ζ-QVAE is available at https://github.com/gaoyuanwang1976/QVAE.git. Acknowledgement We acknowledge support from the NIH and from the AL Williams Professorship funds. We would also like to thank Huan-hsin Tseng and Aram Harrow for valuable discussions. References [1] Irina Higgins et al. “beta-vae: Learning basic visual concepts with a constrained variational framework”. In: International conference on learning representations. 2016. [2] Ilya Tolstikhin et al. “Wasserstein auto-encoders”. In: arXiv preprint arXiv:1711.01558 (2017). [3] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks for biomedical image segmentation”. In: Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18. Springer. 2015, pp. 234–241. [4] Jonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models”. In: Advances in neural information processing systems 33 (2020), pp. 6840–6851. [5] Jonathan Romero, Jonathan P Olson, and Alan Aspuru-Guzik. “Quantum autoencoders for efficient compression of quantum data”. In: Quantum Science and Technology 2.4 (2017), p. 045001. [6] Hailan Ma et al. “On compression rate of quantum autoencoders: Control design, numerical and experimental realization”. In: Automatica 147 (2023), p. 110659. issn: 0005-1098. [7] Stefano Mangini et al. “Quantum neural network autoencoder and classifier applied to an industrial case study”. In: Quantum Machine Intelligence 4.2 (June 2022). issn: 2524-4914. [8] Carlos Bravo-Prieto. “Quantum autoencoders with enhanced data encoding”. In: Machine Learning: Science and Technology 2.3 (July 2021), p. 035028. [9] Pablo Rivas, Liang Zhao, and Javier Orduz. “Hybrid Quantum Variational Autoencoders for Representation Learning”. In: 2021 International Conference on Computational Science and Computational Intelligence (CSCI). IEEE, Dec. 2021. 22 [10] Amir Khoshaman et al. “Quantum variational autoencoder”. In: Quantum Science and Tech- nology 4.1 (2018), p. 014001. [11] Richard Jozsa. “Fidelity for Mixed Quantum States”. In: Journal of Modern Optics 41.12 (1994), pp. 2315–2323. [12] Hsin-Yuan Huang, Richard Kueng, and John Preskill.“Predicting many properties of a quantum system from very few measurements”. In: Nature Physics 16.10 (June 2020), pp. 1050–1057. issn: 1745-2481. [13] V. Vedral. “The role of relative entropy in quantum information theory”. In: Rev. Mod. Phys. 74 (1 Mar. 2002), pp. 197–234. [14] Hamza Fawzi and Omar Fawzi. “Efficient optimization of the quantum relative entropy”. In: Journal of Physics A: Mathematical and Theoretical 51.15 (2018), p. 154003. [15] A. P. Majtey, P. W. Lamberti, and D. P. Prato. “Jensen-Shannon divergence as a measure of distinguishability between mixed quantum states”. In: Physical Review A 72.5 (Nov. 2005). issn: 1094-1622. [16] Shouvanik Chakrabarti et al. “Quantum Wasserstein generative adversarial networks”. In: Advances in Neural Information Processing Systems 32 (2019). [17] Diederik P Kingma and Max Welling. “Auto-encoding variational bayes”. In: arXiv preprint arXiv:1312.6114 (2013). [18] Seth Lloyd et al. Quantum embeddings for machine learning. 2020. arXiv: 2001.03622 [quant- ph]. [19] Daifeng Wang et al. “Comprehensive functional genomic resource and integrative model for the human brain”. In: Science 362.6420 (2018), eaat8464. [20] Stephen Marsland. Machine Learning: An Algorithmic Perspective. 2nd. CRC Press, 2014. Chap. 6. [21] Michael Ragone et al. Representation Theory for Geometric Quantum Machine Learning. 2023. arXiv: 2210.07980 [quant-ph]. [22] Andrew Arrasmith et al. “Effect of barren plateaus on gradient-free optimization”. In: Quantum 5 (Oct. 2021), p. 558. issn: 2521-327X. [23] Israel F. Araujo et al. “A divide-and-conquer algorithm for quantum state preparation”. In: Scientific Reports 11.6329 (2021). [24] Marcus Cramer et al. “Efficient quantum state tomography”. In: Nature Communications 1.149 (2010). [25] Giacomo Torlai et al. “Neural-network quantum state tomography”. In: Nature Physics 14 (5 2018), pp. 447–450. [26] Juan Carrasquilla et al. “Reconstructing quantum states with generative models”. In: Nature Machine Intelligence 1 (3 2019), pp. 155–161. [27] Maria Schuld and Francesco Petruccione. Machine learning with quantum computers. Springer, 2021. 9 Appendix A We provide here further details and proofs regarding the theoretical properties of our framework. The first relates to the number of qubits required to achieve arbitrary mappings in our encoder and decoder: 23 Proposition 1: Setting NA = NB = NZ is sufficient to allow arbitrary pairs of quantum operations (E, D) to be learned in our framework. Proof: An arbitrary quantum channel may be represented by a Choi matrix of rank between 1 and dimX · dimY, where X and Y are the input and output Hilbert spaces of the channel respectively. For E, the input and output spaces have dimension 2NX and 2NZ respectively; hence the maximum rank of the Choi matrix is 2NX · 2NZ = 2NX+NZ, corresponding to a system requiring NX + NZ qubits, which can be achieved by setting NA = NZ. For D, the input and output spaces are swapped, and so the maximum rank is again 2NX+NZ, leading to an identical setting for NB. Second, we show that, as in the classical case, the regularized reconstruction loss objective we use is also a lower-bound on the negative quantum relative entropy (the analogue of the classical log-likelihood), when using the quantum relative entropy for both the reconstruction and regularization terms in our objective, and setting β = 1 and ϵ = 0. Proposition 2: −S(ρglob|σgen) ≥ −S(ρglob|σglob) − S(ζglob|ζgen) Proof: We let ρglob = P i pi |vi⟩ ⟨vi|, ζgen = (1/2NZ) P j |wj⟩ ⟨wj| and ζglob = E(ρglob) = P j qj |wj⟩ ⟨wj|. Notice that we choose to express ζgen in the same basis as ζglob, which is possible, since the former is the maximally mixed state, which diagonalizes in any basis. We can express the LHS of the proposition as: −S(ρglob|σgen) = Tr{ρglob log σgen} + S(ρglob) = X i pi Tr{|vi⟩ ⟨vi| log σgen} + S(ρglob) (24) To derive the proposition, we will bound each of the summands Tr{|vi⟩ ⟨vi| log σgen}. We begin by observing the following: Tr{|vi⟩ ⟨vi| σgen} = Ej∼Categ(1/2NZ )[Tr{|vi⟩ ⟨vi| D(|wj⟩ ⟨wj|)}] = Ej∼Categ(q1...q2NZ )[Tr{|vi⟩ ⟨vi| D(|wj⟩ ⟨wj|)} · 2−NZ qj ] = Tr \u001a |vi⟩ ⟨vi| Ej∼Categ(q1...q2NZ )[D(|wj⟩ ⟨wj|) · 2−NZ qj ] \u001b (25) Hence, introducing logs and applying Jensen’s trace inequality (lines 2-3), we have: 24 Tr{|vi⟩ ⟨vi| log σgen} = Tr \u001a |vi⟩ ⟨vi| log Ej∼Categ(q1...q2NZ )[D(|wj⟩ ⟨wj|) · 2−NZ qj ] \u001b ≥ Tr \u001a |vi⟩ ⟨vi| Ej∼Categ(q1...q2NZ )[log D(|wj⟩ ⟨wj|) · 2−NZ qj ] \u001b = Tr{|vi⟩ ⟨vi| Ej∼Q[log D(|wj⟩ ⟨wj|)]} − Ej∼Q[log qj] + log 2−NZ = Tr{|vi⟩ ⟨vi| log σglob} + S(ζglob) − S(ζgen) (26) Substituting Eq. 26 into Eq. 24 and summing across i, we thus have: −S(ρglob|σgen) ≥ X i pi(Tr{|vi⟩ ⟨vi| log σglob} + S(ζglob) − S(ζgen)) + S(ρglob) = −S(ρglob|σglob) + S(ζglob) − S(ζgen) (27) and the proposition follows, since S(ζglob|ζgen) = S(ζgen) − S(ζglob). Finally, we show that our global and local objectives are equivalent for linear divergences in the following sense: Proposition 3: Our global and local objectives have identical minimizers for E and D, when they can be expressed in the form given in Eq. 23, and L′ 1 and L2 are linear functions their first arguments. Proof: We can express ρglob = (1/N) P i ρi, and ζglob = (1/N) P i E(ρi) = (1/N) P i ζi, where ρi are the pure states associated with each data-point, and ζi are the associated mixed-state latent representations. Hence, if L′ 1 and L2 are linear in their first arguments, we have: L′ glob(θe, θd, β) = L′ 1(ρglob, E(θe), D(θd)) + βL2(ζglob, ζgen) = (1/N) X i L′ 1(ρi, E(θe), D(θd)) + (1/N) X i βL2(ζi, ζgen) = (1/N)L′ inst(θe, θd, β) (28) Hence, the two objectives are equivalent up to the factor (1/N), leading to identical minimizers. In particular, Prop. 3 implies that setting L′ 1 to the form given in Eq. 17 for the Quantum Wasserstein loss, and β = 0 (or setting L2 to the Quantum Wasserstein loss with respect to the ζgen), results in identical global and local optimization problems. 25 "
}