{
    "optim": "GraphMatch: Subgraph Query Processing on FPGAs Jonas Dann jonas.dann@sap.com Heidelberg University & SAP Walldorf (Baden), Germany Tobias Götz goetzt@in.tum.de Technical University of Munich & SAP Munich, Germany Daniel Ritter daniel.ritter@sap.com SAP Walldorf (Baden), Germany Jana Giceva jana.giceva@in.tum.de Technical University of Munich Munich, Germany Holger Fröning holger.froening@ziti.uni- heidelberg.de Heidelberg University Heidelberg, Germany ABSTRACT Efficiently finding subgraph embeddings in large graphs is crucial for many application areas like biology and social network anal- ysis. Set intersections are the predominant and most challenging aspect of current join-based subgraph query processing systems for CPUs. Previous work has shown the viability of utilizing FPGAs for acceleration of graph and join processing. In this work, we propose GraphMatch, the first genearl-purpose stand-alone subgraph query processing accelerator based on worst- case optimal joins (WCOJ) that is fully designed for modern, field programmable gate array (FPGA) hardware. For efficient processing of various graph data sets and query graph patterns, it leverages a novel set intersection approach, called AllCompare, tailor-made for FPGAs. We show that this set intersection approach efficiently solves multi-set intersections in subgraph query processing, su- perior to CPU-based approaches. Overall, GraphMatch achieves a speedup of over 2.68× and 5.16×, compared to the state-of-the-art systems GraphFlow and RapidMatch, respectively. KEYWORDS Graph pattern matching, FPGA, Hardware accelerator ACM Reference Format: Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning. 2021. GraphMatch: Subgraph Query Processing on FPGAs. In Booktitle. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/1122445.1122456 1 INTRODUCTION Subgraph query processing, used e. g., for graph pattern matching, is an important workload in many application areas [22], like social network analysis [23] and protein interaction network analysis [20], where all embeddings identical to a given query graph are found in a data graph. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Somewhere, ..., 2021, Virtual © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06...$15.00 https://doi.org/10.1145/1122445.1122456 pt wt yt go db az ep wv Graph 0 25 50 75 100 Runtime (%) (a) RapidMatch Runtime pt wt yt go db az ep wv Graph 0.00 0.25 0.50 0.75 Runtime (ms) (b) Intersection Runtime Preprocessing Encoding Set Intersections RapidMatch (CPU) GraphMatch (FPGA) Figure 1: RapidMatch (CPU) runtime (left), and RapidMatch and GraphMatch (FPGA) intersection operators (right). Subgraph query processing was mainly approached with two kinds of algorithms in related work on CPUs, namely backtracking [3, 26] and join-based approaches [1, 17]. A recent study by Sun et al. [24] has shown that backtracking is efficient for large and sparse query graphs, and join approaches for smaller, dense query graphs. For instance, one of the most advanced CPU-based sys- tems, RapidMatch [25], is mainly based on joins, while considering graph structural information as in backtracking. When looking into join-based approaches like RapidMatch, set intersections are the most expensive operation, as shown in Fig. 1(a). Despite vectorized processing, this is due to challenges like limited parallelism, expen- sive round trips to main memory, and cache pollution in current general-purpose CPU hardware. Specialized hardware like field programmable gate arrays (FP- GAs) showed that they can solve these challenges with massive, unstructured data and pipeline parallelism and flexible data move- ment through configurable data flow architectures (e. g., in related domains like graph processing [5], JSON parsing [7], and relational join processing [13]). For instance, for subgraph matching, data movement could be more efficient by keeping partial matchings mostly on the FPGA chip. The benefits of such hardware are shown in Fig. 1(b), which denotes a comparison of RapidMatch using SIMD- based set intersection (cf. [11]) and an FPGA-based implementation that will be explained in more detail subsequently. However, in a recent survey on non-relational data processing, Dann et al. [6] identified a gap for subgraph query processing on FPGAs, which was also acknowledged and partially addressed by Jin et al. [12] on subgraph query processing on a hybrid CPU-FPGA system. arXiv:2402.17559v1  [cs.DB]  27 Feb 2024 Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning DRAM modules I/O Block RAMs (BRAM) Digital Signal Processors (DSP) Programmable interconnect Look-Up Tables (LUT) & registers Mem. controller Figure 2: FPGA architecture (taken from [7]). Recent work on worst-case optimal join (WCOJ)-based subgraph query processing on CPUs [17] was very promising. Thus, in this work, we focus on join-based subgraph query processing on FP- GAs using WCOJs. We propose GraphMatch, a graph processing accelerator for subgraph query processing fully implemented on an FPGA. In GraphMatch, we first conceptually adapt the widely- used LeapFrog algorithm [27] to FPGAs, before specifying a novel, FPGA-native AllCompare algorithm, which leverages the FPGA’s massive pipelining to its extreme. GraphMatch is designed for (i) dense, small query graphs, (ii) supporting subgraph homomorphism and isomorphism, (iii) parallel, efficient set intersections, and (iv) directed and undirected graphs. For that, GraphMatch implements a set of intersection operators with configurable number of input sets which are connected with partial matching multiplexers and demultiplexers able to dynamically switch matchings to a mem- ory sink. The GraphMatch query parser can generate query plans for arbitrary subgraph queries on-the-fly. With these query plans we select how we chain together and execute the operators. With GraphMatch, whose FPGA and subgraph query processing foun- dations and related work are introduced in Sect. 2, we make the following contributions (Cx): C1 We specify and implement two novel intersection accelera- tors tailored to FPGAs (LeapFrog and AllCompare). (Sect. 3) C2 We design a subgraph query processing accelerator based on worst-case optimal joins, called GraphMatch. (Sect. 4.1) C3 We make GraphMatch configurable for dynamic queries and propose three performance optimizations. (Sect. 4.2) The resulting GraphMatch system shows promising scalabil- ity with an average speedup of 2.68× over GraphFlow and 5.16× over RapidMatch with a maximum speedup of over 100× (Sect. 5). Overall, we conjecture that FPGAs are well suited to solve the set intersection bottleneck of CPU-based subgraph query processing systems. However, we still see areas of improvements for instance work balancing and for highly degree-skewed graphs (Sect. 6). 2 BACKGROUND AND RELATED WORK In this section, we briefly introduce FPGAs and fundamentals of subgraph query processing, and discuss GraphMatch in the context of related work. 2.1 Field Programmable Gate Arrays Field programmable gate arrays (short FPGAs) map custom digi- tal circuit designs (a set of logic gates and their connections) to a grid of resources (i. e., look-up tables, registers) connected with a programmable interconnection network (cf. Fig. 2). For frequently Query graph GQ q0 q2 q1 d1 d0 d2 d3 Subgraph query  processing Data graph GD Sub graph S0 Sub graph S1 d1 d0 d2 d1 d0 d3 Figure 3: Subgraph query processing example and all its iso- morphisms. used complex functionality like floating point computation, FPGAs contain digital signal processors. Access to off-chip resources like DRAM and network controllers is possible over I/O pins. The mem- ory hierarchy of FPGAs is split into on-chip and off-chip memory. On-chip, FPGAs implement distributed memory, which is made up of (a) single registers mostly used as storage for working val- ues, and (b) block RAM (BRAM) in the form of SRAM memory components, mostly used for fast storage of data structures. On modern FPGAs, there is about as much BRAM as cumulative cache on CPUs (all cache levels combined), but contrary to the fixed cache hierarchies of CPUs, BRAM is finely configurable to the needs of a given application. 2.2 Subgraph Query Processing Graphs are abstract data structures (𝐺 = (𝑉, 𝐸)) comprising of a vertex set 𝑉 , and an edge set 𝐸 ⊆ 𝑉 × 𝑉 . The edges of directed graphs are denoted by a set 𝐸 of tuples. Figure 3 shows an example of an unlabeled subgraph query pro- cessing task for directed graphs, given a data graph 𝐺𝐷 = (𝑉𝐷, 𝐸𝐷) with four vertices and seven edges, and an input query graph 𝐺𝑄 = (𝑉𝑄, 𝐸𝑄) with three vertices and three edges: 𝑉𝐷 ={𝑑0,𝑑1,𝑑2,𝑑3} 𝐸𝐷 ={(𝑑0,𝑑1), (𝑑1,𝑑2), (𝑑2,𝑑3), (𝑑2,𝑑2), (𝑑3,𝑑0), (𝑑0,𝑑2), (𝑑3,𝑑1)} 𝑉𝑄 ={𝑞0,𝑞1,𝑞2} 𝐸𝑄 ={(𝑞0,𝑞1), (𝑞0,𝑞2), (𝑞2,𝑞1)} . The task of subgraph query processing needs to compute either the homomorphisms or isomorphisms of the query graph within the data graph [25]. Both denote subgraphs of the same shape as the query graph, but homomorphisms allow duplicate vertices within subgraphs, while isomorphisms do not [25]. In the given example, we identify all ismorphisms of the trian- gular 𝐺𝑄 within 𝐺𝐷. Thus, all results of the task are triangular subgraphs of 𝐺𝐷, where edges of the same direction exist in the data graph and no vertex is used multiple times. The result contains two graphs: 𝑆0 = (𝑉𝑆0, 𝐸𝑆0) :=({𝑑0,𝑑2,𝑑1}, {(𝑑0,𝑑2), (𝑑0,𝑑1), (𝑑1,𝑑2)}) 𝑆1 = (𝑉𝑆1, 𝐸𝑆1) :=({𝑑3,𝑑1,𝑑0}, {(𝑑3,𝑑1), (𝑑3,𝑑0), (𝑑0,𝑑1)}) . GraphMatch: Subgraph Query Processing on FPGAs Somewhere, ..., 2021, Virtual The homomorphisms in the example Fig. 3 include 𝑆0 and 𝑆1 and four subgraphs with multiple occurrences of vertex 𝑑2: 𝑆2 = (𝑉𝑆2, 𝐸𝑆2) :=({𝑑0,𝑑2}, {(𝑑0,𝑑2), (𝑑2,𝑑2)}) 𝑆3 = (𝑉𝑆3, 𝐸𝑆3) :=({𝑑1,𝑑2}, {(𝑑1,𝑑2), (𝑑2,𝑑2)}) 𝑆4 = (𝑉𝑆4, 𝐸𝑆4) :=({𝑑2}, {(𝑑2,𝑑2)}) 𝑆5 = (𝑉𝑆5, 𝐸𝑆5) :=({𝑑2,𝑑3}, {(𝑑2,𝑑3), (𝑑2,𝑑2)}) . All results consist of vertices and edges in 𝐺𝐷 and have the same shape as 𝐺𝑄. In this example, we colored each vertex of the data graph 𝐺𝐷 differently, for easier validation of the results. In general, all subgraphs must guarantee ∀𝑖 : 𝑉𝑆𝑖 ⊆ 𝑉𝐷 and ∀𝑖 : 𝐸𝑆𝑖 ⊆ 𝐸𝐷. Exploration-Based Subgraph Query Processing. Exploration-based algorithms (also known as backtracking) are one of the two main approaches used to process subgraph queries. The general idea is to explore the entire graph and create candidate sets of appropriate data vertices for each query vertex. The next step takes those sets and enumerates all valid isomorphisms. In the literature, three different enumeration variations are known [25]: direct-, index-, and preprocessing-enumeration. Their under- lying ideas are the same, but they avoid or handle the start of the algorithm differently (e. g., enumerate subgraphs directly, create an index structure on the data graph beforehand). The general back- tracking algorithm works similar for all variations. A candidate set along a query vertex ordering (QVO) is computed containing all data vertices that might be a valid entry for the given position [25]. Additional information, like edge connections between candidates, is also collected in separate data structures [25]. Afterwards, the algorithm uses the collected information and data structures to enumerate all subgraph isomorphisms along the query vertex order recursively [25]. Each iteration computes a local candidate set by taking the connections between previous and future vertices into account [25]. The recursion ends after the whole query graph is pro- cessed. The algorithm can also be adapted to find homomorphisms instead by allowing duplicate vertices during the enumeration [25]. Join-Based Subgraph Query Processing. Worst-case optimal joins (WCOJ) limit their running time to the worst-case output size of the algorithm [8, 18, 25]. Join-based subgraph query processing is based on the WCOJ algorithm Generic Join [9, 17–19, 25]. Generic Join describes an iterative approach to construct all homomorphisms by joining one vertex at a time to a temporary subgraph (partial matching). A known variation of Generic Join is Leapfrog Triejoin [27]. Leapfrog starts with a simple subgraph and extends it step by step. To find valid join candidates, it intersects the corresponding neigh- bourhoods of all previously matched vertices that share an edge with the new vertex in the query graph. All elements of the result set are joined to the current subgraph. This process creates multiple new graphs for the next iteration with the trie structure of the algorithm. After each of the subgraphs represents a valid matching for the full query graph or no subgraph can be extended anymore, the algorithm terminates. The intersections of Leapforg Triejoin are computed by its Leapfrog Join algorithm. This algorithm searches for potential result values in ordered sets. It leaps from one value to the other within sets and jumps from set to set to exclude values that can not be part of the intersection result. To compute isomorphisms with a join-based approach, an ad- ditional check during the join phase is required [25]. This paper focuses on identifying isomorphisms with a Join-based approach on FPGA, with simple reconfiguration to homomorphisms for com- parison to the related GraphFlow [17] system. 2.3 Related Work Computing subgraph isomorphisms and homomorphisms is an im- portant task studied in related work. Surveys, like Sun et al. [24], explored different methods, approaches, and optimizations of sub- graph matching. Lee at al. [14] introduced generalized models and techniques for the general case of subgraph queries. The result of the increased attention in research for subgraph query processing is a variety of approaches and systems. Table 1 depicts an overview of the current state-of-the-art of subgraph query processing systems. For backtracking-based systems, CFLMatch [3] and DAF [10] are instrumental and use their custom candidate data structures to allow subgraph matching of general query graphs. CFLMatch decomposes the query graph into a core, a forest and leaves that are both matched in that order because of their decreasing selec- tivity. DAF uses a candidate space data structure with dynamic programming, an adaptive query vertex ordering, and failing set pruning. The system FAST [12] introduces a graph processing sys- tem for FPGA. It computes its candidate search tree data structure on CPU prior to FPGA computation [12]. After the transfer of the structure to the FPGA’s BRAM, the FPGA enumerates all subgraphs of the data graph for the given query [12]. Additionally, it allows concurrent computation with the host CPU to further speedup the computation [12]. GraphZero [16] is a compilation based approach that tries to eliminate redundant computations in a nested loop structure. EmptyHeaded [1] introduced the WCOJ approach to subgraph query processing systems as a compilation-based system and uses its trie data structure to support subgraph queries on directed graphs. Additionally, EmptyHeaded supports graph processing. GraphFlow [17] extends EmptyHeaded’s approach by combining it with binary joins into a hybrid approach with a query optimizer. This allows query plans which combine multiple partial embed- dings with the binary join. CECI [2] splits up the data graph into embedding clusters to parallelize execution and emplys pruning to reduce the number of intermediate matchings. RapidMatch [25] is the most recent subgraph query processing system that proves that the backtracking and WCOJ approaches are complexity-wise equal. Based on this observation, it combines a join-based approach with backtracking-like candidate pruning. We propose GraphMatch, a pure FPGA design utilizing the WCOJ approach which can compute subgraph isomorphism and homo- morphisms on directed or undirected graphs in parallel. 3 INTERSECTIONS ON FPGAS In this section we focus on designing a parallel, efficient set intersec- tor for FPGAs motivated by the observation that set intersections are the most expensive operation of subgraph query processing (cf. Fig. 1(a)). We describe the different set intersection approaches for CPUs and FPGAs on a spectrum from a software engineer’s perspective to a hardware engineer’s perspective (contribution C1). Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning Table 1: Subgraph query processing systems, design principles, and properties. Name Platform Approach Key data structure General Parallel Dir. Hom. Iso. CFLMatch [3] CPU Backtracking Compact path index \u0006 , , , \u0006 DAF [10] CPU Backtracking Candidate space \u0006 \u0006 , , \u0006 FAST [12] FPGA & CPU Backtracking Candidate search tree \u0006 \u0006 , , \u0006 GraphZero [16] CPU Nested loops Adjacency lists , , \u0006 , \u0006 EmptyHeaded [1] CPU WCOJ Trie , \u0006 \u0006 \u0006 , GraphFlow [17] CPU WCOJ & binary joins Adjacency lists \u0006 \u0006 \u0006 \u0006 , CECI [2] CPU Intersections Compact embedding cluster index \u0006 \u0006 \u0006 , \u0006 RapidMatch [25] CPU WCOJ & hash joins Encoded trie \u0006 , , \u0006 \u0006 GraphMatch FPGA WCOJ Compressed sparse row \u0006 \u0006 \u0006 \u0006 \u0006 Dir.: Directed graphs, Hom.: Subgraph homomorphisms, Iso.: Subgraph isomorphisms, \u0006: yes, ,: no 12 13 20 21 3 7 11 11 13 15 16 22 1 2 3 6 = > > > 13 20 21 11 11 < 13 15 16 22 12 7 < < = 13 15 16 22 < < 13 20 21 Compare Compare Compare Flush > > 22 12 13 20 21 3 7 11 11 13 15 16 22 1 2 3 6 3 13 20 21 7 11 11 15 16 22 2 3 6 7 12 13 20 21 11 11 15 16 22 13 12 13 13 21 11 16 17 20 12 < = < 13 20 15 17 21 21 16 Search Sync Search Sync Search Sync Search Sync Search Flush > > > > < < > > > > > > > > > > > > > 0 > > > > > > > > > > < = (a) LeapFrog set intersection (b) AllCompare set intersection 12 < Input set 0 Input set 1 Input set 0 Input set 1 Figure 4: LeapFrog and AllCompare intersection approaches. Thereafter, we introduce a highly FPGA-optimized implementation of AllCompare set intersection. Lastly, we show how the set in- tersection approaches compare and characterize the performance dimensions of the AllCompare set intersector. 3.1 Intersector Approaches for FPGAs Figure 4 compares LeapFrog as the dominant set intersection ap- proach for CPUs to the novel AllCompare set intersection approach specifically designed for FPGAs. LeapFrog Set Intersection. LeapFrog processes set intersections in turns of searching for a new search item and syncing the search item (Fig. 4(a) shows an example). The execution starts with 0 as the search item (green box in the middle). In each search step, the current search item is compared against each element in each input set (two in this example). For the sync step, the next biggest element of each input set is communicated and compared to form the next search item and all elemets that are smaller than the previous search item are discarded. This process is repeated until the search item is bigger than all remaining elements of on of the input sets. The rest of the set elements are then flushed. In each search and sync loop, LeapFrog has a guaranteed progress of only one element per set while the actual progress may be higher for real world sets. We implement a parallelized version of LeapFrog set intersection in Buffered  fetcher Controller Control interface Intersect Line maxer Buffered  fetcher Line maxer Buffered  fetcher Line maxer Buffered  fetcher Line maxer max max ==== < Line maxer Intersect Line maxer Intersect  Figure 5: AllCompare set intersector architecture. OneAPI that is able to perform all comparisons in the search step in one clock cycle and implement a VHDL version that does the same parallelization and additionally does input set prefetching which is not easily implementable in OneAPI. AllCompare Set Intersection. With the observation in mind, that on an FPGA we can implement many comparison operators in par- allel, we propose the novel AllCompare set intersection approach for FPGAs. Fig. 4(b) shows how AllCompare is able to massively reduce the number of steps for the same set intersection that is shown for LeapFrog. In each compare step, AllCompare compares all elements of both input sets against each other. Elements that are smaller than an element in the other set are discarded, elements that have an equal element in the other sets are put out and dis- carded and all other elements remain. In the end, the last remaining elements are flushed. AllCompare guarantees progress of at least one full line of one of the input sets and is thus able to process the same intersection in under half of the clock cycles in this example. 3.2 AllCompare Intersector Architecture Fig. 5 shows the AllCompare set intersector architecture for four input sets. Four buffered fetchers read the input data into the inter- sector and feed the input lines (16 elements per line on the FPGA 4 elements per line in this example) through line maxers. The line maxers find the maximum of the line which may not be the last element because not all elements of a line have to be valid (e. g., set is smaller than line width). These lines with extracted maximums are fed into the intersect operators. In each intersect operator, in GraphMatch: Subgraph Query Processing on FPGAs Somewhere, ..., 2021, Virtual Buffered  fetcher Cache 0 1 2 3 Controller addr count Figure 6: Cached fetcher architecture. each cycle, all elements are equal compared against all elements from the other input set to determine which elements should be put out as the result of the set intersection. Additionally, the maximums of both lines are discarded and the line with the smaller maximum is completeley discarded as all elements have to be smaller than at least one of the other line. The results are fed into a demultiplexer which either forwards the output to the next line maxer and in- tersect operator or the output port. As the last component, each AllCompare set intersector contains a controller that is connected to the control interface. The user may define the switches that switch the demultiplexers and the multiplexer before the execution. This allows dynamic reconfiguration of number of input sets during runtime. Especially during subgraph query processing, oftentimes the same vertex neighborhoods are accessed as input sets of the set intersector repeatedly. Thus, we propose a cached fetcher architec- ture that stores the most recently accessed input set in a cache and serves subsequent requests to the same input set from the cache. Fig. 6 shows the cached fetcher architecture. The controller receives the requests and stores the last address and number of elements accessed in registers. If the request is equal to the previous one, it is flagged as cached and directly inserted into a FIFO queue which multiplexes the output port. If the request cannot be served through the cache, it is forwarded to a buffered fetcher which send the re- quest to memory and flagged as fetched and also inserted into the FIFO queue. When the request is served by memory, the data is written into the cache (implemented as BRAM) as whole lines of memory starting from cache address 0. The FIFO queue is contin- uously observed. If a new request arrives, the flag if it is cached or not is read out and either the data from the buffered fetcher is directly forwarded or the respective number of memory lines are read from the cache again starting at address 0. The cached fetcher has the same interface as the buffered fetcher and may thus directly replace those in the AllCompare set intersector architecture. 3.3 Intersector Performance Characteristics Finally, we discuss different performance characteristics of set in- tersection on FPGAs starting with a comparison of the RapidMatch intersection function, LeapFrog implemented in OneAPI, LeapFrog implemented in VHDL, and the AllCompare intersector. Thereafter, we discuss the performance of AllCompare in detail for character- istics such as input set size, output set size, and number of input sets and degree of caching with the cached fetcher. The benefits of adapting algorithms to FPGAs. Figure 7 shows the comparison of the CPU-based RapidMatch intersection function pt wt yt go db az ep wv Graph 0.0 0.5 1.0 1.5 Runtime (ms) RapidMatch LeapFrogOneAPI LeapFrogVHDL AllCompare Figure 7: CPU (RapidMatch) vs. FPGA intersection operators. 1 4 8 12 16 20 24 28 32 36 40 44 48 52 56 60 64 Input Set Size 0.1 0.2 0.3 Runtime (ms) 0% 15% 20% 30% AllCompare(2) AllCompare(3) AllCompare(4) Figure 8: Runtime of set intersection with AllCompare for two to four input sets over input set size and output size as percentage of input size. against the different FPGA-based implementations of intersectors that we introduced. The benchmark environment as well as the graphs (cf. Tab. 3) used are described in Sect. 5.1. For LeapFrogOneAPI, an even newer Agilex FPGA is used which, however, was not made available by Intel for the VHDL-based design flow during the work on this paper. The benchmark shows the runtime in milliseconds of 5000 intersections of neighborhoods of random vertices of the respective graph. Overall, we observe that LeapFrogOneAPI performs similar to the RapidMatch intersection function for some graphs and worse for others. The RapidMatch intersection function performance benefits from very small graphs that fit mostly into the cache hierarchy of the CPU (i. e., the epinions (ep) and wiki-vote (wv) graphs). The per- formance of LeapFrogOneAPI is heavily influenced by average degree of the graph, thus, the performance for the amazon (az) and wiki- vote (wv) graphs are noticeably worse than for the other graphs. Additionally, the VHDL FPGA implementations LeapFrogVHDL and AllCompare (i. e., specifically tailored to FPGAs) perform signifi- cantly better than the CPU and OneAPI implementations. AllCom- pare always outperforms LeapFrogVHDL. Thus, we can see the pro- gression from adopting a CPU algorithm with LeapFrogOneAPI in a software engineer-friendly environment over a VHDL implementa- tion of the CPU algorithm (LeapFrogVHDL) to a highly optimized FPGA implementation in AllCompare. In the remainder of this paper, we proceed with the tailor-made AllCompare set intersector. Characteristics of the tailor-made FPGA intersector. Figure 8 shows the runtime of AllCompare in milliseconds with two, three, and Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning 2 3 4 Input Sets 0.0 0.1 0.2 Runtime (ms) 0% 20% 40% 60% 80% Figure 9: Input set caching for percentage of accesses cached by number of input sets. four input sets on 5000 set intersections with varying input set and output set size. For AllCompare on two input sets, where 0% of the input set are part of the output set (blue solid line) forms a memory-bound baseline. The baseline is not influenced by fine- granular input set size but by the number of lines of memory it has to fetch. For this implementation, 16 elements of an input set fit into on memory line. Thus, the runtime increases each time the last element is part of a new memory line. For the measurements where 15%, 20%, and 30% of the input sets are in the output sets, the runtime is also bounded by the output set size because AllCompare only puts out one output set element per clock cycle. More is not required by GraphMatch. For three and four input sets, the runtime is higher and increasingly more bound by memory because more data has to be fetched from memory per set intersection. For four input sets, only the measurement where 30% of the input set are part of the output set (green dotted line) is sometimes bound by output size. Figure 9 shows how input set caching (cf. Fig. 6) influences set intersection performance for the AllCompare set intersector for different number of input sets. Again, we measure the runtime in milliseconds over 5000 set intersections. Each set intersection intersects sets of size 64 without overlap such that output size does not influence performance. Additionally, we vary cache hit rate from 0% to 80%. Overall, we observe that at the latest of 80% cache hit rate, the performance reaches the same baseline for each number of input sets denoting a lower bound set by the cycles the FPGA logic needs to perform the intersection itself. For larger number of input sets this baseline is reached with higher cache hit rate. This is because more data has to be fetched from memory in the first place. It is important to note that AllCompare has the same runtime irregardless of nubmer of input sets if memory requests do not play a role. 3.4 Discussion This section provided a detailed look at the hardware design process in parts from the perspective of a software engineer to understand where FPGAs can provide benefits and that algorithms and data structures have to be specifically designed for the FPGA to provide good performance. From the benchmarks, we conclude that the novel AllCompare set intersection approach provides the best per- formance and is thus used for our full subgraph query processing system GraphMatch. The AllCompare set intersector performance is bound by memory which we optimize with a cached fetcher Matching source Instance  Controller GraphMatch  instance Matching  sink Memory Matchings Outgoing pointers Outgoing neighbors Matching ﬁlter Control interface Incoming pointers Incoming neighbors Matching extender Matching extender Matching extender Figure 10: GraphMatch instance architecture. implementation that is able to provide increased performance for intersections with repeated input sets. 4 GRAPHMATCH In this section, we first introduce the instance architecture of Graph- Match, our subgraph query processor, and its components (con- tribution C2). We then describe how subgraph queries can be dy- namically switched in GraphMatch during runtime, and suitable performance optimizations that we applied to the base system (con- tribution C3). 4.1 GraphMatch Instance Architecture Figure 10 depicts the architecture of a GraphMatch instance for subgraph matchings with up to five levels / vertices. The flow of matchings through the architecture’s components is depicted with bold arrows. It starts at the matching source – producing match- ings with two query vertices – runs through a matching filter and multiple matching extenders (i. e., a configuration with three ex- tenders results in five levels overall), demultiplexers and multiplex- ers (cf. trapezoids) and ends at the matching sink. The matching source reads all outgoing pointers and neighbors and combines them as edges, thereby forming the initial partial matchings. Then the matching filter discards initial matchings that do not fit certain criteria like vertices not being distinct in the case of graph isomor- phisms. Each matching extender extends input partial matchings by one query vertex in order of the query vertex ordering, which most often means set intersections. After each matching extender, there is a matching demultiplexer, which either forwards the incom- ing partial matchings to the next matching extender, or through a matching multiplexer to the matching sink. Finally, the matching sink writes complete matchings to the designated matchings array in the FPGA’s on-board memory. The on-board memory contains in total five data arrays: one CSR data structure consisting of a pointers array and a neighbors array for both incoming and outgoing edges of each vertex, and the matchings array. The matching source and matching extenders only read from memory, whereas the matching sink only writes to memory. These accesses are shown in Fig. 10 as dashed lines and are combined into one request stream fed to memory by a request merger (cf. white oval box). The request merger also routes the memory read responses to the corresponding requesters. GraphMatch: Subgraph Query Processing on FPGAs Somewhere, ..., 2021, Virtual Matching extender Control interface Matching ﬁlter Matching ﬁlter ID Vertex 0 Left Size ID Vertex 1 Left Size ID Vertex 2 Left Size ... Matching Matching intersector Pointer fetcher Combine AllCompare  intersector Mapping Combine Mapping l r Fetch. l s Figure 11: Matching data type and matching extender com- ponent. The query graph is configured by providing GraphMatch with parameters. All system components with white dots have parame- ters which are connected to the control interface operated by the CPU (shown by the dotted lines). The matching source is parameter- ized with the addresses of the arrays it has to read. The matching filter can be turned on and off with parameters and the match- ing extenders are also parameterized with memory addresses but also, for example, with the number of neighborhoods and which neighborhoods they should intersect. Finally, the instance controller manages the query’s execution. It has a parameter for query size that switches the demultiplexers and multiplexer, is responsbile to trigger the execution when all components are ready, and finally returns relevant statistics to the CPU. Matching Data Type. Figure 11 depicts the component design of a matching extender in the context of the matching data type. Matchings each consist of a configurable number of vertices with a vertex identifier, left bound for pointers, and size that denotes the neighborhood size. The number of vertices in the matching data type is equal to the number of levels in the GraphMatch in- stance (e. g., five for the instance in Fig. 10). The left bound and neighborhood size are kept in the matching as metadata between matching extenders. Only when the query first requires intersec- tion on the outbound edges of a vertex and then an intersection on the inbound edges or the other way around, do we have to fetch new metadata from the respective pointers array. The metadata is discarded when writing to memory in the matching sink, since only the vertex identifiers are relevant for the result . Matching Extender. The matching extender component at level 𝑙 takes a matching with 𝑙 vertices where vertex at position 𝑙 is still missing the metadata. It then fetches the required information and tries to extend the matching to vertex 𝑙 + 1. To do this, the partial matching is acquired through a pointer fetcher, retrieving required metadata, a first matching filter, a matching intersector perform- ing the set intersection required and extending the matching by a vertex, and lastly another matching filter. The pointer fetcher takes a partial matching and a mapping as a parameter and fetches the respective metadata. Dictated by the mapping, a buffered fetcher (Fetch.) fetches the lines containing the pointers at positions 𝑣 and 𝑣 + 1, where 𝑣 is the vertex identifier. These form the left (𝑙) and right (𝑟) bound of the neighborhood. The pointer fetcher subtracts 𝑙 q0 q1 q3 q2 - Intersect q0 and q1 - Intersect q1 and q2 Level 0 & 1 (matching source) - Fetch outgoing pointers and neighbors q0 q1 - Filter neighborhood q0 < 2 q0 q1 q2 Level 2 (matching extender) q0 q1 q3 q2 Example query graph Level 3 (matching extender) - Fetch q1 outgoing pointers - Fetch q2 incoming pointers - Filter empty set - Filter empty set Figure 12: GraphMatch query parser for query graph Q5. from 𝑟 to get the neighborhood size and, finally, combines the new metadata with the partial matching. The first matching filter filters out empty sets, i. e., sets where any of the neighborhood sizes used in the following intersection are 0. The matching intersector maps the matching vertices to intersector spots in the AllCompare inter- sector, specified in Sect. 3, based on mapping parameter extracted from the query graph. For example, if there is an intersection be- tween vertices 0 and 2, the AllCompare intersector is configured to do a 2 set intersection mapping vertex 0 to spot 0 and vertex 1 to spot 1. During the intersection, the partial matching is stored in a FIFO queue and the combined subcomponent extends the current partial matching until the intersection is finished. It then proceeds with the next partial matching from the FIFO queue. Depending on whether the workload is a graph isomorphism or homomor- phism, the second matching filter sieves out matchings for which the newly added vertex is different from all vertices already part of the matching. 4.2 Dynamic Queries & Optimizations Based on the GraphMatch architecture, we specify dynamic queries with our query parser and transformations of a query graph into query parameters for GraphMatch. We also explain our three key optimizations for GraphMatch: input set caching, failing set prun- ing, and stride mapping. Dynamic Queries. Figure 12 shows how the example query on the left side is deconstructed by the query parser to get the GraphMatch parameters to map the query graph to the system. The instance controller receives the number of query vertices and address of the matchings array. Each query starts with two vertices connected via an edge. This forms levels 0 and 1 of the GraphMatch instance in the matching source which is parameterized with the addresses of the outgoing pointers and neighbors of 𝑞0. Additionally, the matching filter after the matching source receives the neighborhood size of 𝑞0 in the complete query graph as a parameter. On level 2, the first matching extender is parameterized with a mapping to fetch the metadata for 𝑞1 and the address to outgoing pointers for the pointer fetcher. Additionally, the matching intersector receives a mapping to intersect the neighborhoods of 𝑞0 and 𝑞1 as a two- set intersection on outgoing neighbors. Finally, for level 3, the pointer fetcher should load the incoming pointers for 𝑞2 and we pass a mapping to intersect the neighborhoods of 𝑞1 and 𝑞2 as a Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning Instance 0 Control interface Memory GraphMatch Instance 1 Instance 2 Instance 3 Data graph Channel 0 Data graph Channel 2 Data graph Channel 3 Data graph Channel 1 Figure 13: GraphMatch multi-instance scaling. two set intersection on outgoing neighbors. If a query is larger than the number of levels of the instance, we can materialize the partial matchings into memory, read them back to the beginning of the matching extender pipeline and feed them through the levels. However, details of this are beyond the scope of this work. Input set caching. As a first optimization, we implement caching (cf. Sect. 3.3). We suspect that locality of reference exists in the input set of the AllCompare intersector, as found in each matching intersector, as well as in the input of the pointer fetcher. This is es- pecially the case when new metadata for existing vertices has to be loaded for a partial matching. Thus, all instances of the AllCompare intersector and the pointer fetcher employ caching. Failing set pruning. As a second optimization, we introduce fail- ing set pruning [10]. In GraphMatch we implement it inside the matching filter, after the matching source, and in the first matching filter of each matching extender. In addition to filtering out empty sets, we can also filter partial matchings when the neighborhood of a vertex is not at least as big as the corresponding vertices neigh- borhood in the query graph. For example, for graph isomorphisms on Q5 (cf. Fig. 15), for 𝑞0 the neighborhood size needs to be at least 2. Thus, we parametarize the matching filters for each vertex, so we can customize them for the query vertex neighborhood size. Parallelism. Figure 13 shows the GraphMatch system scaled to four instances. Each instance is assigned its own memory channel and otherwise only connected to the control interface. The data graph is copied to each memory channel in whole such that the instances can work truly independent. The system can, thus, either process a query on one data graph in parallel such that the vertex set is split up into four intervals each assigned to the one of the four instances, or process different combinations of queries and data graphs on the four instances independently. Stride mapping. As a last optimization, we apply stride mapping [4] to improve workload load balancing across instances of Graph- Match. Load balancing was found to be crucial when processing complex datasets. As the GraphMatch instances cannot communi- cate partial matching among each other, each vertex interval should require roughly equal amount of work. Unfortunately, this is not a realistic assumption when working with real world graphs [5], which are often skewed. Here our optimization for stride mapping comes into play, by doing a light-weight vertex reordering. Stride mapping is a technique for semi-random shuffling of the vertex identifiers before partitioning to create a new vertex ordering with a constant stride. In our case we use a stride of 100, which results in a new vertex order 𝑣0, 𝑣100, 𝑣200, ..., which virtually results in each vertex being mapped to a different hardware instance. 80GB/s 128GB/s 0.5GB/s Intel PAC D5005 Board CPU (Intel Xeon Platinum) FPGA (Intel S10 GX) DDR4 System Memory Disk 1 Data graph Edge list PCIe 13GB/s DDR4 FPGA Memory GraphMatch Host code Graph loader 2 3 FPGA manager Data graph Query parser Query Query parameters  Figure 14: Overall system architecture (incl. host, device, memory). 5 EVALUATION In this section, we first introduce the system used for the evaluation, the benchmark setup and key metrics such as resource utilization and clock frequency of the design, the graph data sets, and the graph queries. We then report benchmark results scaling Graph- Match from 1 up to 4 instances, compare GraphMatch against the state-of-the-art CPU-based systems GraphFlow and RapidMatch, and evaluate the effects of different optimizations employed in GraphMatch on overall performance. 5.1 Setup System Architecture. Figure 14 shows how GraphMatch is de- ployed in the system context. In principle, the system features a CPU and an accelerator board – connected via PCIe – which hosts the FPGA, running GraphMatch itself, and memory, used as inter- mediate data storage for the data graph during subgraph query processing. The CPU loads and prepares the data graph, parses and programs the query graph to GraphMatch and manages the execution on the FPGA. To execute a particular workload with a particular graph, the GraphMatch framework is first synthesized with the query parameter registers, a fixed number of instances and maximum query size (i. e., levels). Afterwards, the synthesized design is programmed to the FPGA. Execution Flow. For the execution of a particular subgraph query on a particular graph data set, the edge list (or any other represen- tation) of the graph is read from disk to the CPU and brought into two CSR data structures, one for outgoing edges and one for incom- ing edges of each vertex (cf. step (1)). During loading of the data graph, we transform the set of vertex identifiers to be dense (i. e., excluding vertices that have degree 0). The data graph is then repli- cated to each channel of the FPGA memory (cf. step (2)). Thereafter, the query graph is parsed and the respective parameter registers of GraphMatch are programmed via a control interface such that GraphMatch performs the subgraph query matching the query graph. The host code also triggers the execution via the control interface (cf. step (3)). After the execution finished, the results can be read back to CPU memory and used for further processing. If desired, the data graph or parameter register values can be used multiple times in a row by loading new parameters or a new data graph respectively and again triggering the execution. Hardware Details. For our experiments, we work with a server equipped with an Intel FPGA Programmable Accelerator Card (PAC) GraphMatch: Subgraph Query Processing on FPGAs Somewhere, ..., 2021, Virtual Table 2: Resource utilization and clock frequency by graph problem and number of graph cores. System 𝑝 𝑙 𝑐 LUTs Regs. BRAM DSPs Clock freq. GraphMatch 4 6 \u0007 48% 26% 21% 0% 187MHz 4 6 - 54% 33% 34% 0% 191MHz LUTs: Look-up tables; Regs.: Registers; BRAM: Block RAM; DSPs.: Digital signal processors; Clock freq.: Clock frequency Table 3: Graphs used often by systems in Tab. 1 (real-world graphs from [15] and [21]). Name |𝑉 | |𝐸| 𝐷𝑎𝑣𝑔 ø SCC patents (pt) 3.8M 16.5M 4.34 22 1.00 wiki-talk (wt) 2.4M 5.0M 2.10 11 0.05 youtube (yt) 1.2M 3.0M 5.16 20 0.98 google (go) 875.7K 5.1M 5.82 21 0.50 dblp (db) 426.0K 1.0M 4.93 21 0.74 amazon (az) 403.3K 3.4M 8.43 21 0.98 epinions (ep) 75.9K 508.8K 6.70 14 0.43 wiki-vote (wv) 7, 115 103.7K 14.56 7 0.18 syn𝑛,𝑑 𝑛 𝑛 · 𝑑 𝑑 - 1 SCC: Ratio of vertices in the largest strongly-connected component to 𝑛; -: yes, \u0007: no D5005 attached via PCIe version 3. The system features two Intel Xeon Gold 6142 CPUs at 2.6GHz and 384GB of DDR4-2666 memory, while the D5005 board is equipped with four channels of DDR4-2400 memory with a total capacity of 32GB and a resulting bandwidth of 76.8GB/s. The design itself is based on the Intel Open Programmable Execution Engine (OPAE) platform and is synthesized with Quartus version 19.4. Prototype Configurations. Tab. 2 shows the two different system configurations used for the benchmarks. We synthesized one system variant without input set caching (𝑐) and one with input set caching for the pointer fetchers and set intersectors. Both variants have 𝑝 = 4 instances of GraphMatch, one for each memory channel, with a maximum query graph size of 𝑙 = 6. Note that, without further resource utilization optimization, up to 8 instances could be placed onto the chip – derived from Tab. 2 – however, which could not adequately leverage the available memory channels (cf. discussion on HBM in ??). Since the instances are not connected, the resource utilization scales linearly excluding the fixed resource utilization of the OPAE wrapper. All types including pointers and vertex identifiers are 32-bit unsigned integers. Lastly, the depth of the reorder stage is set to 32. The resource utilization leaves room to scale to modern high-bandwidth memory or include more functionality, like graph processing [5], in the accelerator. Data and Query Graphs. Tab. 3 show the graph data sets used to benchmark GraphMatch. This selection represents the most im- portant graphs considered by the other state-of-the-art subgraph query processing systems (cf. Sect. 2.3). We additionally show graph properties like size (|𝑉 | and |𝐸|), average degree (𝐷𝑎𝑣𝑔), and ratio of vertices in the largest strongly-connected component (SCC) that are useful to explain different performance effects observed in the (a) Q1 (b) Q2 (c) Q3 (d) Q4 (e) Q5 (f) Q6 (g) Q7 q0 q1 q2 q0 q1 q3 q2 q0 q1 q3 q2 q0 q1 q3 q2 q0 q1 q3 q2 q0 q1 q3 q2 q0 q1 q2 q3 q4 Figure 15: Query graphs (adopted from [17]). benchmarks. For the intersection benchmark, we generated dif- ferent configurations of a synthetic graph syn𝑛,𝑑 with another parameter for output size of the resulting intersection between two adjacent vertices. Fig. 15 shows the query graphs we use in our evaluation, taken from [17]. These can be classified as cliques (Q1, Q6, and Q7), cycles (Q1, Q2, and Q3), and other graphs (Q4 and Q5). 5.2 GraphMatch Scalability Fig. 16 shows the scalability of GraphMatch as we increase from a single up to four instances. We report the speedup over a baseline using a single instance for all data graphs and queries. When using a single instance, the stride mapping optimization is disabled be- cause it makes no difference for measurements on a single instance. Otherwise, all optimizations discussed in Sect. 4.2 are enabled by default. The measurements show that the speedup is mostly depen- dent on the properties of the data set itself. For example, we observe a linear scalability on the patents and amazon graphs, which is not the case for the other graphs, where scalability is influenced by the number of intermediate (and actual matchings) in the range of ver- tices assigned to an instance. This effect is particularly pronounced for the wiki-talk and youtube graphs. Scalability could be improved in future work, e. g., with work-stealing where an idling instance takes over partial matchings from a busy instance to balance out the load. However, this would also introduce dependencies between the instances and could lead to higher design complexity. 5.3 Comparison to GraphFlow and RapidMatch We recall from Sect. 2 that the predominant CPU-based subgraph query processing systems are GraphFlow and RapidMatch, which we compare GraphMatch to subsequently. GraphFlow. Fig. 17 compares the performance of GraphMatch with four instances to GraphFlow [17] running on 16 threads (a full CPU on our server) with numactrl that restricts all threads to one NUMA node. The plots show the performance in seconds of runtime on a logarithmic scale. For GraphMatch, we tried out different QVOs for each query and data graph combination and show the best one. This makes a huge difference and as future work can be added as a query optimizer step to the query parser. Graph- Flow also uses directed graphs, however, only supporting subgraph Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning Q1 Q2 Q3 Q4 Q5 Q6 Q7 0 1 2 3 4 Speedup (a) patents Q1 Q2 Q3 Q4 Q5 Q6 Q7 0 1 2 3 4 (b) wiki-talk Q1 Q2 Q3 Q4 Q5 Q6 Q7 0 1 2 3 4 (c) youtube Q1 Q2 Q3 Q4 Q5 Q6 Q7 0 1 2 3 4 (d) google Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 1 2 3 4 Speedup (e) dblp Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 1 2 3 4 (f) amazon Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 1 2 3 4 (g) epinions Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 1 2 3 4 (h) wiki-vote GraphMatch(1) GraphMatch(2) GraphMatch(4) Figure 16: Scalability of GraphMatch from 1 to 4 instances. Q1 Q2 Q3 Q4 Q5 Q6 Q7 10 1 100 Seconds (a) patents Q1 Q2 Q3 Q4 Q5 Q6 Q7 100 101 102 (b) wiki-talk Q1 Q2 Q3 Q4 Q5 Q6 Q7 10 1 100 (c) youtube Q1 Q2 Q3 Q4 Q5 Q6 Q7 10 1 100 (d) google Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 2 10 1 100 Seconds (e) dblp Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 1 100 (f) amazon Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 2 10 1 100 (g) epinions Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 2 10 1 (h) wiki-vote GraphFlow(16) GraphMatch(4) Figure 17: GraphMatch (4 instances) vs. GraphFlow (16 threads) on directed graphs computing subgraph homomorphisms. homomorphisms. Thus, to set the ground for a fair comparison, we have turned off distinct vertex checking and changed the failing set pruning optimizations to match the workload in GraphMatch. We note that for GraphFlow, we had to execute three scripts that prepare the graphs beforehand for each graph. It was not clear to us how much these scripts optimize the graph layout. When looking at the performance numbers, overall we observe big performance improvements with GraphMatch. This is especially pronounced for Q1 on all data graphs. GraphMatch also performs exceptionally well for Q4-Q6 on many graphs. Another interesting observation is that Q2 and Q3 pose a challenge for GraphMatch on all graphs. We attribute that to their low intermediate selectivity, which leads to a large number of partial matchings after extending to 𝑞2. Finally, we note that the graph structures of wiki-talk and youtube are more problematic for GraphMatch than for GraphFlow. These graphs ex- hibit highly exponential (i. e., skewed) degree distributions, which lead to some very large vertex neighborhoods which are used often in partial matchings. These cannot be cached with our design which has relatively small caches and are able to be cached by the more sophisticated cache hierarchy of CPUs. In particular, GraphMatch outperforms GraphFlow by an average of 2.68× for all graphs with a maximum of 100× for query-1 on wiki-vote. RapidMatch. Fig. 18 compares GraphMatch ran with four in- stances to RapidMatch [25]. We were not able to find any config- uration parameter to make RapidMatch run in parallel, so it only uses vectorized instructions for intra-intersection parallelism. The plots show the runtime of both systems in seconds, and on a log- arithmic scale. RapidMatch only works with undirected graphs, so we also make the graphs undirected for GraphMatch and com- pute sub-graph isomorphisms (even though RapidMatch can also compute homomorphisms). Overall, we observe significantly better GraphMatch: Subgraph Query Processing on FPGAs Somewhere, ..., 2021, Virtual Q1 Q2 Q3 Q4 Q5 Q6 Q7 100 101 Seconds (a) patents Q1 Q2 Q3 Q4 Q5 Q6 Q7 101 102 103 (b) wiki-talk Q1 Q2 Q3 Q4 Q5 Q6 Q7 100 101 102 (c) youtube Q1 Q2 Q3 Q4 Q5 Q6 Q7 100 101 (d) google Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 100 102 Seconds (e) dblp Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 1 100 101 (f) amazon Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 1 100 101 (g) epinions Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 10 1 101 (h) wiki-vote RapidMatch GraphMatch(4) Figure 18: GraphMatch (4 instances) vs. RapidMatch on undirected graphs when computing subgraph isomorphisms. Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 2 4 Seconds (a) patents Q1 Q2 Q3 Q4 Q5 Q6 Q7 Query 0 1 2 (b) youtube None Failing Set Pruning Stride Mapping Caching All Figure 19: Effects of GraphMatch optimizations for patents and youtube data graphs. performance for GraphMatch compared to RapidMatch, with an average speedup of 5.16×. Similar to the comparison to GraphFlow, GraphMatch performs exceptionally well for Q1. As before, the system performs less well for Q2, Q3, and Q7. We further note that the performance results for Q2 and Q3, as well as for Q4 and Q5 are similar, because in an undirected graph the orientation of the query edges makes no difference. Once again, we note the wiki-talk and youtube graphs are challenging for GraphMatch because of their heavy exponential degree distribution. 5.4 Effects of GraphMatch Optimizations Fig. 19 shows the effects of the failing set pruning, stride mapping, and input set caching optimizations on performance on the example of the patents and the youtube query graphs. The baseline is Graph- Match with four instances with all optimizations turned off (None). The failing set pruning optimization effectiveness depends on the query graph. For most query graphs it has a small effect but gets more effective with larger query graphs (e. g., Q7). Stride mapping has the largest effect because it balances out the work required between the GraphMatch instances. The effectiveness of input set caching mostly depends on the data graph. It has the biggest effect on the youtube data graph benchmarks. The optimizations work well together to provide a significant performance boost when all of them are turned on (All). 5.5 Performance Model The effectiveness of input set caching showed that memory accesses are the bottleneck of GraphMatch. Thus, we propose a performance model that is based on the number of memory requests with 𝑙 denoting the number of vertex identifiers or pointers that fit into the width of the memory interface. For instance, for 32bit values and 512bit wide memory interface, 𝑙 equals 16. To materialize the initial edges, we need the following number of memory requests: (|𝑉 | + 1)/𝑙 + |𝐸|/𝑙 We need to read |𝑉 | +1 pointers and |𝐸| edges sequentially. Thus, we can divide these numbers by 𝑙. For each extension of this partial matching we need approximately this additional amount of memory requests, where 𝑓 is the number of vertices for which new pointers have to be fetched, 𝑚 is the number of partial matchings going into this extension, and 𝑠 is the number of sets being intersected: 𝑓 · 𝑚 + 𝑠 · (𝑚 · 𝐷𝑎𝑣𝑔/min(𝑙, 𝐷𝑎𝑣𝑔)) We need to fetch 𝑓 · 𝑚 pairs of pointers which however most of the time are part of one line of memory such that we only need one request. For each intersector, we need to make𝑚·𝐷𝑎𝑣𝑔/min(𝑙, 𝐷𝑎𝑣𝑔) requests. The minimum term is because if the neighbor hood is smaller than 𝑙, we still have to fetch a whole line of memory. How- ever, this may be lowered with caching. For example, 𝑠 = 1 for an extension by one edge, and 𝑚 = |𝐸| for the first extension after the initial edges. 5.6 Discussion This section provided an in-depth analysis of the performance of GraphMatch on various graph datasets. We observe that the system exhibits linear scalability when the graphs have close to uniform degree distribution; and that the relative performance can Somewhere, ..., 2021, Virtual Jonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning be significantly improved with our proposed optimizations (up to 5x on the patents dataset and close to 3x on youtube). A detailed ablation study has demonstrated the effect of each individual opti- mization strategy and how their effects differ depending on a given query and input graph. When compared to state-of-the-art systems (GraphFlow and RapidMatch), we have shown that GraphMatch has superior performance, on average outperforming GraphFlow by 2.68×, and RapidMatch by 5.16× across all queries and datasets. When zooming in on the performance for individual queries, we observe that GraphMatch exhibits an excellent performance for all queries when working with graph datasets that have an only slightly skewed degree distribution, and that there is room for im- provement when handling cyclic queries like Q2 and Q3 on highly skewed graphs (e.g., youtube and wiki-talk). 6 CONCLUSION We proposed GraphMatch, an FPGA-based efficient subgraph query processing accelerator. GraphMatch is inspired by the insight that current CPU-based subgraph query processing systems are limited by set intersection which FPGAs are well suited for. We showed the potential of such a design by first introducing a novel set inter- sector AllCompare specifically developed for FPGAs that is able to outperform all state-of-the-art CPU intersectors and FPGA adapta- tions of CPU set intersection algorithms. Therafter, we introduce the GraphMatch architecture with dynamic query reconfiguration and three key performance optimizations. Our experimental per- formance evaluation showed scalability and superior performance of GraphMatch compared to state-of-the-art CPU-based subgraph query processing systems GraphFlow and RapidMatch with an average speedup of 2.68× and up to 5.16× respectively. In future work, we want to extend GraphMatch to benchmarks with labeled graphs and further improve performance for highly de- gree skewed graphs with more sophisticated caching. Additionally, we consider connecting the GraphMatch instances with a work- stealing enabling matching crossbar and exploring query plan opti- mization for GraphMatch could unlock even higher performance. REFERENCES [1] Christopher R. Aberger, Andrew Lamb, Susan Tu, Andres Nötzli, Kunle Oluko- tun, and Christopher Ré. 2017. EmptyHeaded: A Relational Engine for Graph Processing. ACM Trans. Database Syst. 42, 4 (2017), 20:1–20:44. [2] Bibek Bhattarai, Hang Liu, and H. Howie Huang. 2019. CECI: Compact Embedding Cluster Index for Scalable Subgraph Matching. In SIGMOD. ACM, 1447–1462. [3] Fei Bi, Lijun Chang, Xuemin Lin, Lu Qin, and Wenjie Zhang. 2016. Efficient Subgraph Matching by Postponing Cartesian Products. In SIGMOD. ACM, 1199– 1214. [4] Guohao Dai, Tianhao Huang, Yuze Chi, Ningyi Xu, Yu Wang, and Huazhong Yang. 2017. ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA Architecture. In FPGA. 217–226. [5] Jonas Dann, Daniel Ritter, and Holger Fröning. 2022. GraphScale: Scalable Bandwidth-Efficient Graph Processing on FPGAs. In FPL. IEEE, 24–32. [6] Jonas Dann, Daniel Ritter, and Holger Fröning. 2023. Non-relational Databases on FPGAs: Survey, Design Decisions, Challenges. ACM Comput. Surv. 55, 11 (2023), 225:1–225:37. [7] Jonas Dann, Royden Wagner, Daniel Ritter, Christian Faerber, and Holger Fröning. 2022. PipeJSON: Parsing JSON at Line Speed on FPGAs. In DaMoN. 3:1–3:7. [8] Michael J. Freitag, Maximilian Bandle, Tobias Schmidt, Alfons Kemper, and Thomas Neumann. 2020. Adopting Worst-Case Optimal Joins in Relational Database Systems. PVLDB 13, 11 (2020), 1891–1904. [9] Per Fuchs, Peter A. Boncz, and Bogdan Ghit. 2020. EdgeFrame: Worst-Case Optimal Joins for Graph-Pattern Matching in Spark. In GRADES-NDA, Akhil Arora, Semih Salihoglu, and Nikolay Yakovets (Eds.). ACM, 4:1–4:11. [10] Myoungji Han, Hyunjoon Kim, Geonmo Gu, Kunsoo Park, and Wook-Shin Han. 2019. Efficient Subgraph Matching: Harmonizing Dynamic Programming, Adap- tive Matching Order, and Failing Set Together. In SIGMOD. ACM, 1429–1446. [11] Shuo Han, Lei Zou, and Jeffrey Xu Yu. 2018. Speeding Up Set Intersections in Graph Algorithms using SIMD Instructions. In SIGMOD. ACM, 1587–1602. [12] Xin Jin, Zhengyi Yang, Xuemin Lin, Shiyu Yang, Lu Qin, and You Peng. 2021. FAST: FPGA-based Subgraph Matching on Massive Graphs. In 37th IEEE International Conference on Data Engineering, ICDE 2021, Chania, Greece, April 19-22, 2021. IEEE, 1452–1463. https://doi.org/10.1109/ICDE51399.2021.00129 [13] Robert Lasch, Mehdi Moghaddamfar, Norman May, Süleyman Sirri Demirsoy, Christian Färber, and Kai-Uwe Sattler. 2022. Bandwidth-optimal Relational Joins on FPGAs. In EDBT. 1:27–1:39. [14] Jinsoo Lee, Wook-Shin Han, Romans Kasperovics, and Jeong-Hoon Lee. 2012. An In-depth Comparison of Subgraph Isomorphism Algorithms in Graph Databases. Proc. VLDB Endow. 6, 2 (2012), 133–144. [15] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data. [16] Daniel Mawhirter, Sam Reinehr, Connor Holmes, Tongping Liu, and Bo Wu. 2021. GraphZero: A High-Performance Subgraph Matching System. ACM SIGOPS Oper. Syst. Rev. 55, 1 (2021), 21–37. [17] Amine Mhedhbi and Semih Salihoglu. 2019. Optimizing Subgraph Queries by Combining Binary and Worst-Case Optimal Joins. PVLDB 12, 11 (2019), 1692– 1704. [18] Hung Q. Ngo, Ely Porat, Christopher Ré, and Atri Rudra. 2018. Worst-case Optimal Join Algorithms. J. ACM 65, 3 (2018), 16:1–16:40. [19] Hung Q. Ngo, Christopher Ré, and Atri Rudra. 2013. Skew strikes back: new developments in the theory of join algorithms. SIGMOD Rec. 42, 4 (2013), 5–16. [20] Natasa Przulj, Derek G. Corneil, and Igor Jurisica. 2006. Efficient estimation of graphlet frequency distributions in protein-protein interaction networks. Bioin- form. 22, 8 (2006), 974–980. [21] Ryan A. Rossi and Nesreen K. Ahmed. 2015. The Network Data Repository with Interactive Graph Analytics and Visualization. http://networkrepository.com. In AAAI. [22] Siddhartha Sahu, Amine Mhedhbi, Semih Salihoglu, Jimmy Lin, and M. Tamer Özsu. 2020. The ubiquity of large graphs and surprising challenges of graph processing: extended survey. VLDB J. 29, 2-3 (2020), 595–618. [23] Tom AB Snijders, Philippa E Pattison, Garry L Robins, and Mark S Handcock. 2006. New specifications for exponential random graph models. Sociological methodology 36, 1 (2006), 99–153. [24] Shixuan Sun and Qiong Luo. 2020. In-Memory Subgraph Matching: An In-depth Study. In SIGMOD. ACM, 1083–1098. [25] Shixuan Sun, Xibo Sun, Yulin Che, Qiong Luo, and Bingsheng He. 2020. Rapid- Match: A Holistic Approach to Subgraph Query Processing. Proc. VLDB Endow. 14, 2 (2020), 176–188. [26] Julian R. Ullmann. 1976. An Algorithm for Subgraph Isomorphism. J. ACM 23, 1 (1976), 31–42. [27] Todd L. Veldhuizen. 2012. Leapfrog Triejoin: a worst-case optimal join algorithm. CoRR abs/1210.0481 (2012). "
}