{
    "optim": "GraphMatch: Subgraph Query Processing on FPGAs\nJonas Dann\njonas.dann@sap.com\nHeidelberg University & SAP\nWalldorf (Baden), Germany\nTobias Götz\ngoetzt@in.tum.de\nTechnical University of Munich\n& SAP\nMunich, Germany\nDaniel Ritter\ndaniel.ritter@sap.com\nSAP\nWalldorf (Baden), Germany\nJana Giceva\njana.giceva@in.tum.de\nTechnical University of Munich\nMunich, Germany\nHolger Fröning\nholger.froening@ziti.uni-\nheidelberg.de\nHeidelberg University\nHeidelberg, Germany\nABSTRACT\nEfficiently finding subgraph embeddings in large graphs is crucial\nfor many application areas like biology and social network anal-\nysis. Set intersections are the predominant and most challenging\naspect of current join-based subgraph query processing systems\nfor CPUs. Previous work has shown the viability of utilizing FPGAs\nfor acceleration of graph and join processing.\nIn this work, we propose GraphMatch, the first genearl-purpose\nstand-alone subgraph query processing accelerator based on worst-\ncase optimal joins (WCOJ) that is fully designed for modern, field\nprogrammable gate array (FPGA) hardware. For efficient processing\nof various graph data sets and query graph patterns, it leverages\na novel set intersection approach, called AllCompare, tailor-made\nfor FPGAs. We show that this set intersection approach efficiently\nsolves multi-set intersections in subgraph query processing, su-\nperior to CPU-based approaches. Overall, GraphMatch achieves a\nspeedup of over 2.68× and 5.16×, compared to the state-of-the-art\nsystems GraphFlow and RapidMatch, respectively.\nKEYWORDS\nGraph pattern matching, FPGA, Hardware accelerator\nACM Reference Format:\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning.\n2021. GraphMatch: Subgraph Query Processing on FPGAs. In Booktitle. ACM,\nNew York, NY, USA, 12 pages. https://doi.org/10.1145/1122445.1122456\n1\nINTRODUCTION\nSubgraph query processing, used e. g., for graph pattern matching,\nis an important workload in many application areas [22], like social\nnetwork analysis [23] and protein interaction network analysis\n[20], where all embeddings identical to a given query graph are\nfound in a data graph.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nSomewhere, ..., 2021, Virtual\n© 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/10.1145/1122445.1122456\npt wt yt go db az ep wv\nGraph\n0\n25\n50\n75\n100\nRuntime (%)\n(a) RapidMatch Runtime\npt wt yt go db az ep wv\nGraph\n0.00\n0.25\n0.50\n0.75\nRuntime (ms)\n(b) Intersection Runtime\nPreprocessing\nEncoding\nSet Intersections\nRapidMatch (CPU)\nGraphMatch (FPGA)\nFigure 1: RapidMatch (CPU) runtime (left), and RapidMatch\nand GraphMatch (FPGA) intersection operators (right).\nSubgraph query processing was mainly approached with two\nkinds of algorithms in related work on CPUs, namely backtracking\n[3, 26] and join-based approaches [1, 17]. A recent study by Sun\net al. [24] has shown that backtracking is efficient for large and\nsparse query graphs, and join approaches for smaller, dense query\ngraphs. For instance, one of the most advanced CPU-based sys-\ntems, RapidMatch [25], is mainly based on joins, while considering\ngraph structural information as in backtracking. When looking into\njoin-based approaches like RapidMatch, set intersections are the\nmost expensive operation, as shown in Fig. 1(a). Despite vectorized\nprocessing, this is due to challenges like limited parallelism, expen-\nsive round trips to main memory, and cache pollution in current\ngeneral-purpose CPU hardware.\nSpecialized hardware like field programmable gate arrays (FP-\nGAs) showed that they can solve these challenges with massive,\nunstructured data and pipeline parallelism and flexible data move-\nment through configurable data flow architectures (e. g., in related\ndomains like graph processing [5], JSON parsing [7], and relational\njoin processing [13]). For instance, for subgraph matching, data\nmovement could be more efficient by keeping partial matchings\nmostly on the FPGA chip. The benefits of such hardware are shown\nin Fig. 1(b), which denotes a comparison of RapidMatch using SIMD-\nbased set intersection (cf. [11]) and an FPGA-based implementation\nthat will be explained in more detail subsequently. However, in a\nrecent survey on non-relational data processing, Dann et al. [6]\nidentified a gap for subgraph query processing on FPGAs, which\nwas also acknowledged and partially addressed by Jin et al. [12] on\nsubgraph query processing on a hybrid CPU-FPGA system.\narXiv:2402.17559v1  [cs.DB]  27 Feb 2024\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\nDRAM modules\nI/O\nBlock RAMs (BRAM)\nDigital Signal Processors (DSP)\nProgrammable interconnect\nLook-Up Tables (LUT) & registers\nMem. controller\nFigure 2: FPGA architecture (taken from [7]).\nRecent work on worst-case optimal join (WCOJ)-based subgraph\nquery processing on CPUs [17] was very promising. Thus, in this\nwork, we focus on join-based subgraph query processing on FP-\nGAs using WCOJs. We propose GraphMatch, a graph processing\naccelerator for subgraph query processing fully implemented on\nan FPGA. In GraphMatch, we first conceptually adapt the widely-\nused LeapFrog algorithm [27] to FPGAs, before specifying a novel,\nFPGA-native AllCompare algorithm, which leverages the FPGA’s\nmassive pipelining to its extreme. GraphMatch is designed for (i)\ndense, small query graphs, (ii) supporting subgraph homomorphism\nand isomorphism, (iii) parallel, efficient set intersections, and (iv)\ndirected and undirected graphs. For that, GraphMatch implements\na set of intersection operators with configurable number of input\nsets which are connected with partial matching multiplexers and\ndemultiplexers able to dynamically switch matchings to a mem-\nory sink. The GraphMatch query parser can generate query plans\nfor arbitrary subgraph queries on-the-fly. With these query plans\nwe select how we chain together and execute the operators. With\nGraphMatch, whose FPGA and subgraph query processing foun-\ndations and related work are introduced in Sect. 2, we make the\nfollowing contributions (Cx):\nC1 We specify and implement two novel intersection accelera-\ntors tailored to FPGAs (LeapFrog and AllCompare). (Sect. 3)\nC2 We design a subgraph query processing accelerator based\non worst-case optimal joins, called GraphMatch. (Sect. 4.1)\nC3 We make GraphMatch configurable for dynamic queries and\npropose three performance optimizations. (Sect. 4.2)\nThe resulting GraphMatch system shows promising scalabil-\nity with an average speedup of 2.68× over GraphFlow and 5.16×\nover RapidMatch with a maximum speedup of over 100× (Sect. 5).\nOverall, we conjecture that FPGAs are well suited to solve the set\nintersection bottleneck of CPU-based subgraph query processing\nsystems. However, we still see areas of improvements for instance\nwork balancing and for highly degree-skewed graphs (Sect. 6).\n2\nBACKGROUND AND RELATED WORK\nIn this section, we briefly introduce FPGAs and fundamentals of\nsubgraph query processing, and discuss GraphMatch in the context\nof related work.\n2.1\nField Programmable Gate Arrays\nField programmable gate arrays (short FPGAs) map custom digi-\ntal circuit designs (a set of logic gates and their connections) to a\ngrid of resources (i. e., look-up tables, registers) connected with a\nprogrammable interconnection network (cf. Fig. 2). For frequently\nQuery graph GQ\nq0\nq2\nq1\nd1\nd0\nd2\nd3\nSubgraph query \nprocessing\nData graph GD\nSub graph S0\nSub graph S1\nd1\nd0\nd2\nd1\nd0\nd3\nFigure 3: Subgraph query processing example and all its iso-\nmorphisms.\nused complex functionality like floating point computation, FPGAs\ncontain digital signal processors. Access to off-chip resources like\nDRAM and network controllers is possible over I/O pins. The mem-\nory hierarchy of FPGAs is split into on-chip and off-chip memory.\nOn-chip, FPGAs implement distributed memory, which is made\nup of (a) single registers mostly used as storage for working val-\nues, and (b) block RAM (BRAM) in the form of SRAM memory\ncomponents, mostly used for fast storage of data structures. On\nmodern FPGAs, there is about as much BRAM as cumulative cache\non CPUs (all cache levels combined), but contrary to the fixed cache\nhierarchies of CPUs, BRAM is finely configurable to the needs of a\ngiven application.\n2.2\nSubgraph Query Processing\nGraphs are abstract data structures (𝐺 = (𝑉, 𝐸)) comprising of a\nvertex set 𝑉 , and an edge set 𝐸 ⊆ 𝑉 × 𝑉 . The edges of directed\ngraphs are denoted by a set 𝐸 of tuples.\nFigure 3 shows an example of an unlabeled subgraph query pro-\ncessing task for directed graphs, given a data graph 𝐺𝐷 = (𝑉𝐷, 𝐸𝐷)\nwith four vertices and seven edges, and an input query graph\n𝐺𝑄 = (𝑉𝑄, 𝐸𝑄) with three vertices and three edges:\n𝑉𝐷 ={𝑑0,𝑑1,𝑑2,𝑑3}\n𝐸𝐷 ={(𝑑0,𝑑1), (𝑑1,𝑑2), (𝑑2,𝑑3), (𝑑2,𝑑2), (𝑑3,𝑑0), (𝑑0,𝑑2), (𝑑3,𝑑1)}\n𝑉𝑄 ={𝑞0,𝑞1,𝑞2}\n𝐸𝑄 ={(𝑞0,𝑞1), (𝑞0,𝑞2), (𝑞2,𝑞1)} .\nThe task of subgraph query processing needs to compute either the\nhomomorphisms or isomorphisms of the query graph within the\ndata graph [25]. Both denote subgraphs of the same shape as the\nquery graph, but homomorphisms allow duplicate vertices within\nsubgraphs, while isomorphisms do not [25].\nIn the given example, we identify all ismorphisms of the trian-\ngular 𝐺𝑄 within 𝐺𝐷. Thus, all results of the task are triangular\nsubgraphs of 𝐺𝐷, where edges of the same direction exist in the\ndata graph and no vertex is used multiple times. The result contains\ntwo graphs:\n𝑆0 = (𝑉𝑆0, 𝐸𝑆0) :=({𝑑0,𝑑2,𝑑1}, {(𝑑0,𝑑2), (𝑑0,𝑑1), (𝑑1,𝑑2)})\n𝑆1 = (𝑉𝑆1, 𝐸𝑆1) :=({𝑑3,𝑑1,𝑑0}, {(𝑑3,𝑑1), (𝑑3,𝑑0), (𝑑0,𝑑1)}) .\nGraphMatch: Subgraph Query Processing on FPGAs\nSomewhere, ..., 2021, Virtual\nThe homomorphisms in the example Fig. 3 include 𝑆0 and 𝑆1 and\nfour subgraphs with multiple occurrences of vertex 𝑑2:\n𝑆2 = (𝑉𝑆2, 𝐸𝑆2) :=({𝑑0,𝑑2}, {(𝑑0,𝑑2), (𝑑2,𝑑2)})\n𝑆3 = (𝑉𝑆3, 𝐸𝑆3) :=({𝑑1,𝑑2}, {(𝑑1,𝑑2), (𝑑2,𝑑2)})\n𝑆4 = (𝑉𝑆4, 𝐸𝑆4) :=({𝑑2}, {(𝑑2,𝑑2)})\n𝑆5 = (𝑉𝑆5, 𝐸𝑆5) :=({𝑑2,𝑑3}, {(𝑑2,𝑑3), (𝑑2,𝑑2)}) .\nAll results consist of vertices and edges in 𝐺𝐷 and have the same\nshape as 𝐺𝑄. In this example, we colored each vertex of the data\ngraph 𝐺𝐷 differently, for easier validation of the results. In general,\nall subgraphs must guarantee ∀𝑖 : 𝑉𝑆𝑖 ⊆ 𝑉𝐷 and ∀𝑖 : 𝐸𝑆𝑖 ⊆ 𝐸𝐷.\nExploration-Based Subgraph Query Processing. Exploration-based\nalgorithms (also known as backtracking) are one of the two main\napproaches used to process subgraph queries. The general idea is\nto explore the entire graph and create candidate sets of appropriate\ndata vertices for each query vertex. The next step takes those sets\nand enumerates all valid isomorphisms.\nIn the literature, three different enumeration variations are known\n[25]: direct-, index-, and preprocessing-enumeration. Their under-\nlying ideas are the same, but they avoid or handle the start of the\nalgorithm differently (e. g., enumerate subgraphs directly, create an\nindex structure on the data graph beforehand). The general back-\ntracking algorithm works similar for all variations. A candidate set\nalong a query vertex ordering (QVO) is computed containing all\ndata vertices that might be a valid entry for the given position [25].\nAdditional information, like edge connections between candidates,\nis also collected in separate data structures [25]. Afterwards, the\nalgorithm uses the collected information and data structures to\nenumerate all subgraph isomorphisms along the query vertex order\nrecursively [25]. Each iteration computes a local candidate set by\ntaking the connections between previous and future vertices into\naccount [25]. The recursion ends after the whole query graph is pro-\ncessed. The algorithm can also be adapted to find homomorphisms\ninstead by allowing duplicate vertices during the enumeration [25].\nJoin-Based Subgraph Query Processing. Worst-case optimal joins\n(WCOJ) limit their running time to the worst-case output size of the\nalgorithm [8, 18, 25]. Join-based subgraph query processing is based\non the WCOJ algorithm Generic Join [9, 17–19, 25]. Generic Join\ndescribes an iterative approach to construct all homomorphisms\nby joining one vertex at a time to a temporary subgraph (partial\nmatching).\nA known variation of Generic Join is Leapfrog Triejoin [27].\nLeapfrog starts with a simple subgraph and extends it step by step.\nTo find valid join candidates, it intersects the corresponding neigh-\nbourhoods of all previously matched vertices that share an edge\nwith the new vertex in the query graph. All elements of the result\nset are joined to the current subgraph. This process creates multiple\nnew graphs for the next iteration with the trie structure of the\nalgorithm. After each of the subgraphs represents a valid matching\nfor the full query graph or no subgraph can be extended anymore,\nthe algorithm terminates. The intersections of Leapforg Triejoin are\ncomputed by its Leapfrog Join algorithm. This algorithm searches\nfor potential result values in ordered sets. It leaps from one value\nto the other within sets and jumps from set to set to exclude values\nthat can not be part of the intersection result.\nTo compute isomorphisms with a join-based approach, an ad-\nditional check during the join phase is required [25]. This paper\nfocuses on identifying isomorphisms with a Join-based approach\non FPGA, with simple reconfiguration to homomorphisms for com-\nparison to the related GraphFlow [17] system.\n2.3\nRelated Work\nComputing subgraph isomorphisms and homomorphisms is an im-\nportant task studied in related work. Surveys, like Sun et al. [24],\nexplored different methods, approaches, and optimizations of sub-\ngraph matching. Lee at al. [14] introduced generalized models and\ntechniques for the general case of subgraph queries. The result of\nthe increased attention in research for subgraph query processing is\na variety of approaches and systems. Table 1 depicts an overview of\nthe current state-of-the-art of subgraph query processing systems.\nFor backtracking-based systems, CFLMatch [3] and DAF [10]\nare instrumental and use their custom candidate data structures\nto allow subgraph matching of general query graphs. CFLMatch\ndecomposes the query graph into a core, a forest and leaves that\nare both matched in that order because of their decreasing selec-\ntivity. DAF uses a candidate space data structure with dynamic\nprogramming, an adaptive query vertex ordering, and failing set\npruning. The system FAST [12] introduces a graph processing sys-\ntem for FPGA. It computes its candidate search tree data structure\non CPU prior to FPGA computation [12]. After the transfer of the\nstructure to the FPGA’s BRAM, the FPGA enumerates all subgraphs\nof the data graph for the given query [12]. Additionally, it allows\nconcurrent computation with the host CPU to further speedup the\ncomputation [12]. GraphZero [16] is a compilation based approach\nthat tries to eliminate redundant computations in a nested loop\nstructure.\nEmptyHeaded [1] introduced the WCOJ approach to subgraph\nquery processing systems as a compilation-based system and uses\nits trie data structure to support subgraph queries on directed\ngraphs. Additionally, EmptyHeaded supports graph processing.\nGraphFlow [17] extends EmptyHeaded’s approach by combining it\nwith binary joins into a hybrid approach with a query optimizer.\nThis allows query plans which combine multiple partial embed-\ndings with the binary join. CECI [2] splits up the data graph into\nembedding clusters to parallelize execution and emplys pruning to\nreduce the number of intermediate matchings. RapidMatch [25] is\nthe most recent subgraph query processing system that proves that\nthe backtracking and WCOJ approaches are complexity-wise equal.\nBased on this observation, it combines a join-based approach with\nbacktracking-like candidate pruning.\nWe propose GraphMatch, a pure FPGA design utilizing the WCOJ\napproach which can compute subgraph isomorphism and homo-\nmorphisms on directed or undirected graphs in parallel.\n3\nINTERSECTIONS ON FPGAS\nIn this section we focus on designing a parallel, efficient set intersec-\ntor for FPGAs motivated by the observation that set intersections\nare the most expensive operation of subgraph query processing\n(cf. Fig. 1(a)). We describe the different set intersection approaches\nfor CPUs and FPGAs on a spectrum from a software engineer’s\nperspective to a hardware engineer’s perspective (contribution C1).\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\nTable 1: Subgraph query processing systems, design principles, and properties.\nName\nPlatform\nApproach\nKey data structure\nGeneral\nParallel\nDir.\nHom.\nIso.\nCFLMatch [3]\nCPU\nBacktracking\nCompact path index\n\u0006\n,\n,\n,\n\u0006\nDAF [10]\nCPU\nBacktracking\nCandidate space\n\u0006\n\u0006\n,\n,\n\u0006\nFAST [12]\nFPGA & CPU\nBacktracking\nCandidate search tree\n\u0006\n\u0006\n,\n,\n\u0006\nGraphZero [16]\nCPU\nNested loops\nAdjacency lists\n,\n,\n\u0006\n,\n\u0006\nEmptyHeaded [1]\nCPU\nWCOJ\nTrie\n,\n\u0006\n\u0006\n\u0006\n,\nGraphFlow [17]\nCPU\nWCOJ & binary joins\nAdjacency lists\n\u0006\n\u0006\n\u0006\n\u0006\n,\nCECI [2]\nCPU\nIntersections\nCompact embedding cluster index\n\u0006\n\u0006\n\u0006\n,\n\u0006\nRapidMatch [25]\nCPU\nWCOJ & hash joins\nEncoded trie\n\u0006\n,\n,\n\u0006\n\u0006\nGraphMatch\nFPGA\nWCOJ\nCompressed sparse row\n\u0006\n\u0006\n\u0006\n\u0006\n\u0006\nDir.: Directed graphs, Hom.: Subgraph homomorphisms, Iso.: Subgraph isomorphisms, \u0006: yes, ,: no\n12\n13\n20\n21\n3\n7\n11\n11\n13\n15\n16\n22\n1\n2\n3\n6\n=\n>\n>\n>\n13\n20\n21\n11\n11\n<\n13\n15\n16\n22\n12\n7\n<\n<\n=\n13\n15\n16\n22\n<\n<\n13\n20\n21\nCompare\nCompare\nCompare\nFlush\n>\n>\n22\n12 13 20 21\n3\n7\n11 11\n13 15 16 22\n1\n2\n3\n6\n3\n13 20 21\n7\n11 11\n15 16 22\n2\n3\n6\n7\n12\n13\n20 21\n11 11\n15 16 22\n13\n12\n13\n13\n21\n11\n16 17\n20\n12\n<\n=\n<\n13\n20\n15\n17\n21\n21\n16\nSearch\nSync\nSearch\nSync\nSearch\nSync\nSearch\nSync\nSearch\nFlush\n>\n>\n>\n>\n<\n<\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n0\n>\n>\n>\n>\n>\n>\n>\n>\n>\n>\n<\n=\n(a) LeapFrog set intersection\n(b) AllCompare set intersection\n12\n<\nInput set 0\nInput set 1\nInput set 0\nInput set 1\nFigure 4: LeapFrog and AllCompare intersection approaches.\nThereafter, we introduce a highly FPGA-optimized implementation\nof AllCompare set intersection. Lastly, we show how the set in-\ntersection approaches compare and characterize the performance\ndimensions of the AllCompare set intersector.\n3.1\nIntersector Approaches for FPGAs\nFigure 4 compares LeapFrog as the dominant set intersection ap-\nproach for CPUs to the novel AllCompare set intersection approach\nspecifically designed for FPGAs.\nLeapFrog Set Intersection. LeapFrog processes set intersections\nin turns of searching for a new search item and syncing the search\nitem (Fig. 4(a) shows an example). The execution starts with 0 as\nthe search item (green box in the middle). In each search step, the\ncurrent search item is compared against each element in each input\nset (two in this example). For the sync step, the next biggest element\nof each input set is communicated and compared to form the next\nsearch item and all elemets that are smaller than the previous search\nitem are discarded. This process is repeated until the search item\nis bigger than all remaining elements of on of the input sets. The\nrest of the set elements are then flushed. In each search and sync\nloop, LeapFrog has a guaranteed progress of only one element per\nset while the actual progress may be higher for real world sets. We\nimplement a parallelized version of LeapFrog set intersection in\nBuffered \nfetcher\nController\nControl interface\nIntersect\nLine maxer\nBuffered \nfetcher\nLine maxer\nBuffered \nfetcher\nLine maxer\nBuffered \nfetcher\nLine maxer\nmax\nmax\n====\n<\nLine maxer\nIntersect\nLine maxer\nIntersect \nFigure 5: AllCompare set intersector architecture.\nOneAPI that is able to perform all comparisons in the search step in\none clock cycle and implement a VHDL version that does the same\nparallelization and additionally does input set prefetching which is\nnot easily implementable in OneAPI.\nAllCompare Set Intersection. With the observation in mind, that\non an FPGA we can implement many comparison operators in par-\nallel, we propose the novel AllCompare set intersection approach\nfor FPGAs. Fig. 4(b) shows how AllCompare is able to massively\nreduce the number of steps for the same set intersection that is\nshown for LeapFrog. In each compare step, AllCompare compares\nall elements of both input sets against each other. Elements that are\nsmaller than an element in the other set are discarded, elements\nthat have an equal element in the other sets are put out and dis-\ncarded and all other elements remain. In the end, the last remaining\nelements are flushed. AllCompare guarantees progress of at least\none full line of one of the input sets and is thus able to process the\nsame intersection in under half of the clock cycles in this example.\n3.2\nAllCompare Intersector Architecture\nFig. 5 shows the AllCompare set intersector architecture for four\ninput sets. Four buffered fetchers read the input data into the inter-\nsector and feed the input lines (16 elements per line on the FPGA 4\nelements per line in this example) through line maxers. The line\nmaxers find the maximum of the line which may not be the last\nelement because not all elements of a line have to be valid (e. g., set\nis smaller than line width). These lines with extracted maximums\nare fed into the intersect operators. In each intersect operator, in\nGraphMatch: Subgraph Query Processing on FPGAs\nSomewhere, ..., 2021, Virtual\nBuffered \nfetcher\nCache\n0\n1\n2\n3\nController\naddr\ncount\nFigure 6: Cached fetcher architecture.\neach cycle, all elements are equal compared against all elements\nfrom the other input set to determine which elements should be put\nout as the result of the set intersection. Additionally, the maximums\nof both lines are discarded and the line with the smaller maximum\nis completeley discarded as all elements have to be smaller than at\nleast one of the other line. The results are fed into a demultiplexer\nwhich either forwards the output to the next line maxer and in-\ntersect operator or the output port. As the last component, each\nAllCompare set intersector contains a controller that is connected\nto the control interface. The user may define the switches that\nswitch the demultiplexers and the multiplexer before the execution.\nThis allows dynamic reconfiguration of number of input sets during\nruntime.\nEspecially during subgraph query processing, oftentimes the\nsame vertex neighborhoods are accessed as input sets of the set\nintersector repeatedly. Thus, we propose a cached fetcher architec-\nture that stores the most recently accessed input set in a cache and\nserves subsequent requests to the same input set from the cache.\nFig. 6 shows the cached fetcher architecture. The controller receives\nthe requests and stores the last address and number of elements\naccessed in registers. If the request is equal to the previous one, it\nis flagged as cached and directly inserted into a FIFO queue which\nmultiplexes the output port. If the request cannot be served through\nthe cache, it is forwarded to a buffered fetcher which send the re-\nquest to memory and flagged as fetched and also inserted into the\nFIFO queue. When the request is served by memory, the data is\nwritten into the cache (implemented as BRAM) as whole lines of\nmemory starting from cache address 0. The FIFO queue is contin-\nuously observed. If a new request arrives, the flag if it is cached\nor not is read out and either the data from the buffered fetcher is\ndirectly forwarded or the respective number of memory lines are\nread from the cache again starting at address 0. The cached fetcher\nhas the same interface as the buffered fetcher and may thus directly\nreplace those in the AllCompare set intersector architecture.\n3.3\nIntersector Performance Characteristics\nFinally, we discuss different performance characteristics of set in-\ntersection on FPGAs starting with a comparison of the RapidMatch\nintersection function, LeapFrog implemented in OneAPI, LeapFrog\nimplemented in VHDL, and the AllCompare intersector. Thereafter,\nwe discuss the performance of AllCompare in detail for character-\nistics such as input set size, output set size, and number of input\nsets and degree of caching with the cached fetcher.\nThe benefits of adapting algorithms to FPGAs. Figure 7 shows the\ncomparison of the CPU-based RapidMatch intersection function\npt\nwt\nyt\ngo\ndb\naz\nep\nwv\nGraph\n0.0\n0.5\n1.0\n1.5\nRuntime (ms)\nRapidMatch\nLeapFrogOneAPI\nLeapFrogVHDL\nAllCompare\nFigure 7: CPU (RapidMatch) vs. FPGA intersection operators.\n1 4\n8\n12 16 20 24 28 32 36 40 44 48 52 56 60 64\nInput Set Size\n0.1\n0.2\n0.3\nRuntime (ms)\n0%\n15%\n20%\n30%\nAllCompare(2)\nAllCompare(3)\nAllCompare(4)\nFigure 8: Runtime of set intersection with AllCompare for\ntwo to four input sets over input set size and output size as\npercentage of input size.\nagainst the different FPGA-based implementations of intersectors\nthat we introduced.\nThe benchmark environment as well as the graphs (cf. Tab. 3)\nused are described in Sect. 5.1. For LeapFrogOneAPI, an even newer\nAgilex FPGA is used which, however, was not made available by\nIntel for the VHDL-based design flow during the work on this\npaper. The benchmark shows the runtime in milliseconds of 5000\nintersections of neighborhoods of random vertices of the respective\ngraph.\nOverall, we observe that LeapFrogOneAPI performs similar to the\nRapidMatch intersection function for some graphs and worse for\nothers. The RapidMatch intersection function performance benefits\nfrom very small graphs that fit mostly into the cache hierarchy of\nthe CPU (i. e., the epinions (ep) and wiki-vote (wv) graphs). The per-\nformance of LeapFrogOneAPI is heavily influenced by average degree\nof the graph, thus, the performance for the amazon (az) and wiki-\nvote (wv) graphs are noticeably worse than for the other graphs.\nAdditionally, the VHDL FPGA implementations LeapFrogVHDL and\nAllCompare (i. e., specifically tailored to FPGAs) perform signifi-\ncantly better than the CPU and OneAPI implementations. AllCom-\npare always outperforms LeapFrogVHDL. Thus, we can see the pro-\ngression from adopting a CPU algorithm with LeapFrogOneAPI in a\nsoftware engineer-friendly environment over a VHDL implementa-\ntion of the CPU algorithm (LeapFrogVHDL) to a highly optimized\nFPGA implementation in AllCompare. In the remainder of this\npaper, we proceed with the tailor-made AllCompare set intersector.\nCharacteristics of the tailor-made FPGA intersector. Figure 8 shows\nthe runtime of AllCompare in milliseconds with two, three, and\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\n2\n3\n4\nInput Sets\n0.0\n0.1\n0.2\nRuntime (ms)\n0%\n20%\n40%\n60%\n80%\nFigure 9: Input set caching for percentage of accesses cached\nby number of input sets.\nfour input sets on 5000 set intersections with varying input set\nand output set size. For AllCompare on two input sets, where 0%\nof the input set are part of the output set (blue solid line) forms\na memory-bound baseline. The baseline is not influenced by fine-\ngranular input set size but by the number of lines of memory it has\nto fetch. For this implementation, 16 elements of an input set fit\ninto on memory line. Thus, the runtime increases each time the\nlast element is part of a new memory line. For the measurements\nwhere 15%, 20%, and 30% of the input sets are in the output sets, the\nruntime is also bounded by the output set size because AllCompare\nonly puts out one output set element per clock cycle. More is not\nrequired by GraphMatch. For three and four input sets, the runtime\nis higher and increasingly more bound by memory because more\ndata has to be fetched from memory per set intersection. For four\ninput sets, only the measurement where 30% of the input set are\npart of the output set (green dotted line) is sometimes bound by\noutput size.\nFigure 9 shows how input set caching (cf. Fig. 6) influences set\nintersection performance for the AllCompare set intersector for\ndifferent number of input sets. Again, we measure the runtime\nin milliseconds over 5000 set intersections. Each set intersection\nintersects sets of size 64 without overlap such that output size does\nnot influence performance. Additionally, we vary cache hit rate\nfrom 0% to 80%. Overall, we observe that at the latest of 80% cache\nhit rate, the performance reaches the same baseline for each number\nof input sets denoting a lower bound set by the cycles the FPGA\nlogic needs to perform the intersection itself. For larger number of\ninput sets this baseline is reached with higher cache hit rate. This\nis because more data has to be fetched from memory in the first\nplace. It is important to note that AllCompare has the same runtime\nirregardless of nubmer of input sets if memory requests do not play\na role.\n3.4\nDiscussion\nThis section provided a detailed look at the hardware design process\nin parts from the perspective of a software engineer to understand\nwhere FPGAs can provide benefits and that algorithms and data\nstructures have to be specifically designed for the FPGA to provide\ngood performance. From the benchmarks, we conclude that the\nnovel AllCompare set intersection approach provides the best per-\nformance and is thus used for our full subgraph query processing\nsystem GraphMatch. The AllCompare set intersector performance\nis bound by memory which we optimize with a cached fetcher\nMatching\nsource\nInstance \nController\nGraphMatch \ninstance\nMatching \nsink\nMemory\nMatchings\nOutgoing pointers\nOutgoing neighbors\nMatching\nﬁlter\nControl interface\nIncoming pointers\nIncoming neighbors\nMatching\nextender\nMatching\nextender\nMatching\nextender\nFigure 10: GraphMatch instance architecture.\nimplementation that is able to provide increased performance for\nintersections with repeated input sets.\n4\nGRAPHMATCH\nIn this section, we first introduce the instance architecture of Graph-\nMatch, our subgraph query processor, and its components (con-\ntribution C2). We then describe how subgraph queries can be dy-\nnamically switched in GraphMatch during runtime, and suitable\nperformance optimizations that we applied to the base system (con-\ntribution C3).\n4.1\nGraphMatch Instance Architecture\nFigure 10 depicts the architecture of a GraphMatch instance for\nsubgraph matchings with up to five levels / vertices. The flow of\nmatchings through the architecture’s components is depicted with\nbold arrows. It starts at the matching source – producing match-\nings with two query vertices – runs through a matching filter and\nmultiple matching extenders (i. e., a configuration with three ex-\ntenders results in five levels overall), demultiplexers and multiplex-\ners (cf. trapezoids) and ends at the matching sink. The matching\nsource reads all outgoing pointers and neighbors and combines\nthem as edges, thereby forming the initial partial matchings. Then\nthe matching filter discards initial matchings that do not fit certain\ncriteria like vertices not being distinct in the case of graph isomor-\nphisms. Each matching extender extends input partial matchings\nby one query vertex in order of the query vertex ordering, which\nmost often means set intersections. After each matching extender,\nthere is a matching demultiplexer, which either forwards the incom-\ning partial matchings to the next matching extender, or through a\nmatching multiplexer to the matching sink. Finally, the matching\nsink writes complete matchings to the designated matchings array\nin the FPGA’s on-board memory.\nThe on-board memory contains in total five data arrays: one\nCSR data structure consisting of a pointers array and a neighbors\narray for both incoming and outgoing edges of each vertex, and\nthe matchings array. The matching source and matching extenders\nonly read from memory, whereas the matching sink only writes to\nmemory. These accesses are shown in Fig. 10 as dashed lines and\nare combined into one request stream fed to memory by a request\nmerger (cf. white oval box). The request merger also routes the\nmemory read responses to the corresponding requesters.\nGraphMatch: Subgraph Query Processing on FPGAs\nSomewhere, ..., 2021, Virtual\nMatching extender\nControl interface\nMatching\nﬁlter\nMatching\nﬁlter\nID\nVertex 0\nLeft\nSize\nID\nVertex 1\nLeft\nSize\nID\nVertex 2\nLeft\nSize\n...\nMatching\nMatching intersector\nPointer fetcher\nCombine\nAllCompare \nintersector\nMapping\nCombine\nMapping\nl\nr\nFetch.\nl\ns\nFigure 11: Matching data type and matching extender com-\nponent.\nThe query graph is configured by providing GraphMatch with\nparameters. All system components with white dots have parame-\nters which are connected to the control interface operated by the\nCPU (shown by the dotted lines). The matching source is parameter-\nized with the addresses of the arrays it has to read. The matching\nfilter can be turned on and off with parameters and the match-\ning extenders are also parameterized with memory addresses but\nalso, for example, with the number of neighborhoods and which\nneighborhoods they should intersect. Finally, the instance controller\nmanages the query’s execution. It has a parameter for query size\nthat switches the demultiplexers and multiplexer, is responsbile to\ntrigger the execution when all components are ready, and finally\nreturns relevant statistics to the CPU.\nMatching Data Type. Figure 11 depicts the component design\nof a matching extender in the context of the matching data type.\nMatchings each consist of a configurable number of vertices with\na vertex identifier, left bound for pointers, and size that denotes\nthe neighborhood size. The number of vertices in the matching\ndata type is equal to the number of levels in the GraphMatch in-\nstance (e. g., five for the instance in Fig. 10). The left bound and\nneighborhood size are kept in the matching as metadata between\nmatching extenders. Only when the query first requires intersec-\ntion on the outbound edges of a vertex and then an intersection on\nthe inbound edges or the other way around, do we have to fetch\nnew metadata from the respective pointers array. The metadata\nis discarded when writing to memory in the matching sink, since\nonly the vertex identifiers are relevant for the result .\nMatching Extender. The matching extender component at level 𝑙\ntakes a matching with 𝑙 vertices where vertex at position 𝑙 is still\nmissing the metadata. It then fetches the required information and\ntries to extend the matching to vertex 𝑙 + 1. To do this, the partial\nmatching is acquired through a pointer fetcher, retrieving required\nmetadata, a first matching filter, a matching intersector perform-\ning the set intersection required and extending the matching by a\nvertex, and lastly another matching filter. The pointer fetcher takes\na partial matching and a mapping as a parameter and fetches the\nrespective metadata. Dictated by the mapping, a buffered fetcher\n(Fetch.) fetches the lines containing the pointers at positions 𝑣 and\n𝑣 + 1, where 𝑣 is the vertex identifier. These form the left (𝑙) and\nright (𝑟) bound of the neighborhood. The pointer fetcher subtracts 𝑙\nq0\nq1\nq3\nq2\n- Intersect q0 and q1\n- Intersect q1 and q2\nLevel 0 & 1 (matching source)\n- Fetch outgoing pointers and neighbors\nq0\nq1\n- Filter neighborhood q0 < 2\nq0\nq1\nq2\nLevel 2 (matching extender)\nq0\nq1\nq3\nq2\nExample query\ngraph\nLevel 3 (matching extender)\n- Fetch q1 outgoing pointers\n- Fetch q2 incoming pointers\n- Filter empty set\n- Filter empty set\nFigure 12: GraphMatch query parser for query graph Q5.\nfrom 𝑟 to get the neighborhood size and, finally, combines the new\nmetadata with the partial matching. The first matching filter filters\nout empty sets, i. e., sets where any of the neighborhood sizes used\nin the following intersection are 0. The matching intersector maps\nthe matching vertices to intersector spots in the AllCompare inter-\nsector, specified in Sect. 3, based on mapping parameter extracted\nfrom the query graph. For example, if there is an intersection be-\ntween vertices 0 and 2, the AllCompare intersector is configured to\ndo a 2 set intersection mapping vertex 0 to spot 0 and vertex 1 to\nspot 1. During the intersection, the partial matching is stored in a\nFIFO queue and the combined subcomponent extends the current\npartial matching until the intersection is finished. It then proceeds\nwith the next partial matching from the FIFO queue. Depending\non whether the workload is a graph isomorphism or homomor-\nphism, the second matching filter sieves out matchings for which\nthe newly added vertex is different from all vertices already part of\nthe matching.\n4.2\nDynamic Queries & Optimizations\nBased on the GraphMatch architecture, we specify dynamic queries\nwith our query parser and transformations of a query graph into\nquery parameters for GraphMatch. We also explain our three key\noptimizations for GraphMatch: input set caching, failing set prun-\ning, and stride mapping.\nDynamic Queries. Figure 12 shows how the example query on the\nleft side is deconstructed by the query parser to get the GraphMatch\nparameters to map the query graph to the system. The instance\ncontroller receives the number of query vertices and address of the\nmatchings array. Each query starts with two vertices connected via\nan edge. This forms levels 0 and 1 of the GraphMatch instance in the\nmatching source which is parameterized with the addresses of the\noutgoing pointers and neighbors of 𝑞0. Additionally, the matching\nfilter after the matching source receives the neighborhood size of\n𝑞0 in the complete query graph as a parameter. On level 2, the\nfirst matching extender is parameterized with a mapping to fetch\nthe metadata for 𝑞1 and the address to outgoing pointers for the\npointer fetcher. Additionally, the matching intersector receives a\nmapping to intersect the neighborhoods of 𝑞0 and 𝑞1 as a two-\nset intersection on outgoing neighbors. Finally, for level 3, the\npointer fetcher should load the incoming pointers for 𝑞2 and we\npass a mapping to intersect the neighborhoods of 𝑞1 and 𝑞2 as a\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\nInstance 0\nControl interface\nMemory\nGraphMatch\nInstance 1\nInstance 2\nInstance 3\nData graph\nChannel 0\nData graph\nChannel 2\nData graph\nChannel 3\nData graph\nChannel 1\nFigure 13: GraphMatch multi-instance scaling.\ntwo set intersection on outgoing neighbors. If a query is larger\nthan the number of levels of the instance, we can materialize the\npartial matchings into memory, read them back to the beginning of\nthe matching extender pipeline and feed them through the levels.\nHowever, details of this are beyond the scope of this work.\nInput set caching. As a first optimization, we implement caching\n(cf. Sect. 3.3). We suspect that locality of reference exists in the\ninput set of the AllCompare intersector, as found in each matching\nintersector, as well as in the input of the pointer fetcher. This is es-\npecially the case when new metadata for existing vertices has to be\nloaded for a partial matching. Thus, all instances of the AllCompare\nintersector and the pointer fetcher employ caching.\nFailing set pruning. As a second optimization, we introduce fail-\ning set pruning [10]. In GraphMatch we implement it inside the\nmatching filter, after the matching source, and in the first matching\nfilter of each matching extender. In addition to filtering out empty\nsets, we can also filter partial matchings when the neighborhood of\na vertex is not at least as big as the corresponding vertices neigh-\nborhood in the query graph. For example, for graph isomorphisms\non Q5 (cf. Fig. 15), for 𝑞0 the neighborhood size needs to be at least\n2. Thus, we parametarize the matching filters for each vertex, so\nwe can customize them for the query vertex neighborhood size.\nParallelism. Figure 13 shows the GraphMatch system scaled to\nfour instances. Each instance is assigned its own memory channel\nand otherwise only connected to the control interface. The data\ngraph is copied to each memory channel in whole such that the\ninstances can work truly independent. The system can, thus, either\nprocess a query on one data graph in parallel such that the vertex\nset is split up into four intervals each assigned to the one of the\nfour instances, or process different combinations of queries and\ndata graphs on the four instances independently.\nStride mapping. As a last optimization, we apply stride mapping\n[4] to improve workload load balancing across instances of Graph-\nMatch. Load balancing was found to be crucial when processing\ncomplex datasets. As the GraphMatch instances cannot communi-\ncate partial matching among each other, each vertex interval should\nrequire roughly equal amount of work. Unfortunately, this is not\na realistic assumption when working with real world graphs [5],\nwhich are often skewed. Here our optimization for stride mapping\ncomes into play, by doing a light-weight vertex reordering. Stride\nmapping is a technique for semi-random shuffling of the vertex\nidentifiers before partitioning to create a new vertex ordering with\na constant stride. In our case we use a stride of 100, which results in\na new vertex order 𝑣0, 𝑣100, 𝑣200, ..., which virtually results in each\nvertex being mapped to a different hardware instance.\n80GB/s\n128GB/s\n0.5GB/s\nIntel PAC D5005 Board\nCPU (Intel Xeon Platinum)\nFPGA (Intel S10 GX)\nDDR4 System Memory\nDisk\n1\nData graph\nEdge list\nPCIe\n13GB/s\nDDR4 FPGA Memory\nGraphMatch\nHost code\nGraph\nloader\n2\n3\nFPGA\nmanager\nData graph\nQuery\nparser\nQuery\nQuery parameters \nFigure 14: Overall system architecture (incl. host, device,\nmemory).\n5\nEVALUATION\nIn this section, we first introduce the system used for the evaluation,\nthe benchmark setup and key metrics such as resource utilization\nand clock frequency of the design, the graph data sets, and the\ngraph queries. We then report benchmark results scaling Graph-\nMatch from 1 up to 4 instances, compare GraphMatch against the\nstate-of-the-art CPU-based systems GraphFlow and RapidMatch,\nand evaluate the effects of different optimizations employed in\nGraphMatch on overall performance.\n5.1\nSetup\nSystem Architecture. Figure 14 shows how GraphMatch is de-\nployed in the system context. In principle, the system features a\nCPU and an accelerator board – connected via PCIe – which hosts\nthe FPGA, running GraphMatch itself, and memory, used as inter-\nmediate data storage for the data graph during subgraph query\nprocessing. The CPU loads and prepares the data graph, parses\nand programs the query graph to GraphMatch and manages the\nexecution on the FPGA. To execute a particular workload with a\nparticular graph, the GraphMatch framework is first synthesized\nwith the query parameter registers, a fixed number of instances\nand maximum query size (i. e., levels). Afterwards, the synthesized\ndesign is programmed to the FPGA.\nExecution Flow. For the execution of a particular subgraph query\non a particular graph data set, the edge list (or any other represen-\ntation) of the graph is read from disk to the CPU and brought into\ntwo CSR data structures, one for outgoing edges and one for incom-\ning edges of each vertex (cf. step (1)). During loading of the data\ngraph, we transform the set of vertex identifiers to be dense (i. e.,\nexcluding vertices that have degree 0). The data graph is then repli-\ncated to each channel of the FPGA memory (cf. step (2)). Thereafter,\nthe query graph is parsed and the respective parameter registers\nof GraphMatch are programmed via a control interface such that\nGraphMatch performs the subgraph query matching the query\ngraph. The host code also triggers the execution via the control\ninterface (cf. step (3)). After the execution finished, the results can\nbe read back to CPU memory and used for further processing. If\ndesired, the data graph or parameter register values can be used\nmultiple times in a row by loading new parameters or a new data\ngraph respectively and again triggering the execution.\nHardware Details. For our experiments, we work with a server\nequipped with an Intel FPGA Programmable Accelerator Card (PAC)\nGraphMatch: Subgraph Query Processing on FPGAs\nSomewhere, ..., 2021, Virtual\nTable 2: Resource utilization and clock frequency by graph\nproblem and number of graph cores.\nSystem\n𝑝\n𝑙\n𝑐\nLUTs\nRegs.\nBRAM\nDSPs\nClock freq.\nGraphMatch\n4\n6\n\u0007\n48%\n26%\n21%\n0%\n187MHz\n4\n6\n-\n54%\n33%\n34%\n0%\n191MHz\nLUTs: Look-up tables; Regs.: Registers; BRAM: Block RAM; DSPs.: Digital\nsignal processors; Clock freq.: Clock frequency\nTable 3: Graphs used often by systems in Tab. 1 (real-world\ngraphs from [15] and [21]).\nName\n|𝑉 |\n|𝐸|\n𝐷𝑎𝑣𝑔\nø\nSCC\npatents (pt)\n3.8M\n16.5M\n4.34\n22\n1.00\nwiki-talk (wt)\n2.4M\n5.0M\n2.10\n11\n0.05\nyoutube (yt)\n1.2M\n3.0M\n5.16\n20\n0.98\ngoogle (go)\n875.7K\n5.1M\n5.82\n21\n0.50\ndblp (db)\n426.0K\n1.0M\n4.93\n21\n0.74\namazon (az)\n403.3K\n3.4M\n8.43\n21\n0.98\nepinions (ep)\n75.9K\n508.8K\n6.70\n14\n0.43\nwiki-vote (wv)\n7, 115\n103.7K\n14.56\n7\n0.18\nsyn𝑛,𝑑\n𝑛\n𝑛 · 𝑑\n𝑑\n-\n1\nSCC: Ratio of vertices in the largest strongly-connected component to 𝑛;\n-: yes, \u0007: no\nD5005 attached via PCIe version 3. The system features two Intel\nXeon Gold 6142 CPUs at 2.6GHz and 384GB of DDR4-2666 memory,\nwhile the D5005 board is equipped with four channels of DDR4-2400\nmemory with a total capacity of 32GB and a resulting bandwidth of\n76.8GB/s. The design itself is based on the Intel Open Programmable\nExecution Engine (OPAE) platform and is synthesized with Quartus\nversion 19.4.\nPrototype Configurations. Tab. 2 shows the two different system\nconfigurations used for the benchmarks. We synthesized one system\nvariant without input set caching (𝑐) and one with input set caching\nfor the pointer fetchers and set intersectors. Both variants have\n𝑝 = 4 instances of GraphMatch, one for each memory channel,\nwith a maximum query graph size of 𝑙 = 6. Note that, without\nfurther resource utilization optimization, up to 8 instances could\nbe placed onto the chip – derived from Tab. 2 – however, which\ncould not adequately leverage the available memory channels (cf.\ndiscussion on HBM in ??). Since the instances are not connected,\nthe resource utilization scales linearly excluding the fixed resource\nutilization of the OPAE wrapper. All types including pointers and\nvertex identifiers are 32-bit unsigned integers. Lastly, the depth\nof the reorder stage is set to 32. The resource utilization leaves\nroom to scale to modern high-bandwidth memory or include more\nfunctionality, like graph processing [5], in the accelerator.\nData and Query Graphs. Tab. 3 show the graph data sets used\nto benchmark GraphMatch. This selection represents the most im-\nportant graphs considered by the other state-of-the-art subgraph\nquery processing systems (cf. Sect. 2.3). We additionally show graph\nproperties like size (|𝑉 | and |𝐸|), average degree (𝐷𝑎𝑣𝑔), and ratio\nof vertices in the largest strongly-connected component (SCC) that\nare useful to explain different performance effects observed in the\n(a) Q1\n(b) Q2\n(c) Q3\n(d) Q4\n(e) Q5\n(f) Q6\n(g) Q7\nq0\nq1\nq2\nq0\nq1\nq3\nq2\nq0\nq1\nq3\nq2\nq0\nq1\nq3\nq2\nq0\nq1\nq3\nq2\nq0\nq1\nq3\nq2\nq0\nq1\nq2\nq3\nq4\nFigure 15: Query graphs (adopted from [17]).\nbenchmarks. For the intersection benchmark, we generated dif-\nferent configurations of a synthetic graph syn𝑛,𝑑 with another\nparameter for output size of the resulting intersection between two\nadjacent vertices.\nFig. 15 shows the query graphs we use in our evaluation, taken\nfrom [17]. These can be classified as cliques (Q1, Q6, and Q7), cycles\n(Q1, Q2, and Q3), and other graphs (Q4 and Q5).\n5.2\nGraphMatch Scalability\nFig. 16 shows the scalability of GraphMatch as we increase from a\nsingle up to four instances. We report the speedup over a baseline\nusing a single instance for all data graphs and queries. When using\na single instance, the stride mapping optimization is disabled be-\ncause it makes no difference for measurements on a single instance.\nOtherwise, all optimizations discussed in Sect. 4.2 are enabled by\ndefault. The measurements show that the speedup is mostly depen-\ndent on the properties of the data set itself. For example, we observe\na linear scalability on the patents and amazon graphs, which is not\nthe case for the other graphs, where scalability is influenced by the\nnumber of intermediate (and actual matchings) in the range of ver-\ntices assigned to an instance. This effect is particularly pronounced\nfor the wiki-talk and youtube graphs. Scalability could be improved\nin future work, e. g., with work-stealing where an idling instance\ntakes over partial matchings from a busy instance to balance out\nthe load. However, this would also introduce dependencies between\nthe instances and could lead to higher design complexity.\n5.3\nComparison to GraphFlow and RapidMatch\nWe recall from Sect. 2 that the predominant CPU-based subgraph\nquery processing systems are GraphFlow and RapidMatch, which\nwe compare GraphMatch to subsequently.\nGraphFlow. Fig. 17 compares the performance of GraphMatch\nwith four instances to GraphFlow [17] running on 16 threads (a\nfull CPU on our server) with numactrl that restricts all threads\nto one NUMA node. The plots show the performance in seconds\nof runtime on a logarithmic scale. For GraphMatch, we tried out\ndifferent QVOs for each query and data graph combination and\nshow the best one. This makes a huge difference and as future work\ncan be added as a query optimizer step to the query parser. Graph-\nFlow also uses directed graphs, however, only supporting subgraph\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\n0\n1\n2\n3\n4\nSpeedup\n(a) patents\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\n0\n1\n2\n3\n4\n(b) wiki-talk\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\n0\n1\n2\n3\n4\n(c) youtube\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\n0\n1\n2\n3\n4\n(d) google\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n1\n2\n3\n4\nSpeedup\n(e) dblp\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n1\n2\n3\n4\n(f) amazon\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n1\n2\n3\n4\n(g) epinions\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n1\n2\n3\n4\n(h) wiki-vote\nGraphMatch(1)\nGraphMatch(2)\nGraphMatch(4)\nFigure 16: Scalability of GraphMatch from 1 to 4 instances.\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n10 1\n100\nSeconds\n(a) patents\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n100\n101\n102\n(b) wiki-talk\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n10 1\n100\n(c) youtube\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n10 1\n100\n(d) google\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 2\n10 1\n100\nSeconds\n(e) dblp\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 1\n100\n(f) amazon\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 2\n10 1\n100\n(g) epinions\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 2\n10 1\n(h) wiki-vote\nGraphFlow(16)\nGraphMatch(4)\nFigure 17: GraphMatch (4 instances) vs. GraphFlow (16 threads) on directed graphs computing subgraph homomorphisms.\nhomomorphisms. Thus, to set the ground for a fair comparison, we\nhave turned off distinct vertex checking and changed the failing\nset pruning optimizations to match the workload in GraphMatch.\nWe note that for GraphFlow, we had to execute three scripts that\nprepare the graphs beforehand for each graph. It was not clear to us\nhow much these scripts optimize the graph layout. When looking\nat the performance numbers, overall we observe big performance\nimprovements with GraphMatch. This is especially pronounced\nfor Q1 on all data graphs. GraphMatch also performs exceptionally\nwell for Q4-Q6 on many graphs. Another interesting observation is\nthat Q2 and Q3 pose a challenge for GraphMatch on all graphs. We\nattribute that to their low intermediate selectivity, which leads to a\nlarge number of partial matchings after extending to 𝑞2. Finally, we\nnote that the graph structures of wiki-talk and youtube are more\nproblematic for GraphMatch than for GraphFlow. These graphs ex-\nhibit highly exponential (i. e., skewed) degree distributions, which\nlead to some very large vertex neighborhoods which are used often\nin partial matchings. These cannot be cached with our design which\nhas relatively small caches and are able to be cached by the more\nsophisticated cache hierarchy of CPUs. In particular, GraphMatch\noutperforms GraphFlow by an average of 2.68× for all graphs with\na maximum of 100× for query-1 on wiki-vote.\nRapidMatch. Fig. 18 compares GraphMatch ran with four in-\nstances to RapidMatch [25]. We were not able to find any config-\nuration parameter to make RapidMatch run in parallel, so it only\nuses vectorized instructions for intra-intersection parallelism. The\nplots show the runtime of both systems in seconds, and on a log-\narithmic scale. RapidMatch only works with undirected graphs,\nso we also make the graphs undirected for GraphMatch and com-\npute sub-graph isomorphisms (even though RapidMatch can also\ncompute homomorphisms). Overall, we observe significantly better\nGraphMatch: Subgraph Query Processing on FPGAs\nSomewhere, ..., 2021, Virtual\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n100\n101\nSeconds\n(a) patents\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n101\n102\n103\n(b) wiki-talk\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n100\n101\n102\n(c) youtube\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\n100\n101\n(d) google\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n100\n102\nSeconds\n(e) dblp\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 1\n100\n101\n(f) amazon\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 1\n100\n101\n(g) epinions\nQ1 Q2 Q3 Q4 Q5 Q6 Q7\nQuery\n10 1\n101\n(h) wiki-vote\nRapidMatch\nGraphMatch(4)\nFigure 18: GraphMatch (4 instances) vs. RapidMatch on undirected graphs when computing subgraph isomorphisms.\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n2\n4\nSeconds\n(a) patents\nQ1\nQ2\nQ3\nQ4\nQ5\nQ6\nQ7\nQuery\n0\n1\n2\n(b) youtube\nNone\nFailing Set Pruning\nStride Mapping\nCaching\nAll\nFigure 19: Effects of GraphMatch optimizations for patents\nand youtube data graphs.\nperformance for GraphMatch compared to RapidMatch, with an\naverage speedup of 5.16×. Similar to the comparison to GraphFlow,\nGraphMatch performs exceptionally well for Q1. As before, the\nsystem performs less well for Q2, Q3, and Q7. We further note that\nthe performance results for Q2 and Q3, as well as for Q4 and Q5 are\nsimilar, because in an undirected graph the orientation of the query\nedges makes no difference. Once again, we note the wiki-talk and\nyoutube graphs are challenging for GraphMatch because of their\nheavy exponential degree distribution.\n5.4\nEffects of GraphMatch Optimizations\nFig. 19 shows the effects of the failing set pruning, stride mapping,\nand input set caching optimizations on performance on the example\nof the patents and the youtube query graphs. The baseline is Graph-\nMatch with four instances with all optimizations turned off (None).\nThe failing set pruning optimization effectiveness depends on the\nquery graph. For most query graphs it has a small effect but gets\nmore effective with larger query graphs (e. g., Q7). Stride mapping\nhas the largest effect because it balances out the work required\nbetween the GraphMatch instances. The effectiveness of input set\ncaching mostly depends on the data graph. It has the biggest effect\non the youtube data graph benchmarks. The optimizations work\nwell together to provide a significant performance boost when all\nof them are turned on (All).\n5.5\nPerformance Model\nThe effectiveness of input set caching showed that memory accesses\nare the bottleneck of GraphMatch. Thus, we propose a performance\nmodel that is based on the number of memory requests with 𝑙\ndenoting the number of vertex identifiers or pointers that fit into\nthe width of the memory interface. For instance, for 32bit values\nand 512bit wide memory interface, 𝑙 equals 16. To materialize the\ninitial edges, we need the following number of memory requests:\n(|𝑉 | + 1)/𝑙 + |𝐸|/𝑙\nWe need to read |𝑉 | +1 pointers and |𝐸| edges sequentially. Thus,\nwe can divide these numbers by 𝑙. For each extension of this partial\nmatching we need approximately this additional amount of memory\nrequests, where 𝑓 is the number of vertices for which new pointers\nhave to be fetched, 𝑚 is the number of partial matchings going into\nthis extension, and 𝑠 is the number of sets being intersected:\n𝑓 · 𝑚 + 𝑠 · (𝑚 · 𝐷𝑎𝑣𝑔/min(𝑙, 𝐷𝑎𝑣𝑔))\nWe need to fetch 𝑓 · 𝑚 pairs of pointers which however most of\nthe time are part of one line of memory such that we only need one\nrequest. For each intersector, we need to make𝑚·𝐷𝑎𝑣𝑔/min(𝑙, 𝐷𝑎𝑣𝑔)\nrequests. The minimum term is because if the neighbor hood is\nsmaller than 𝑙, we still have to fetch a whole line of memory. How-\never, this may be lowered with caching. For example, 𝑠 = 1 for an\nextension by one edge, and 𝑚 = |𝐸| for the first extension after the\ninitial edges.\n5.6\nDiscussion\nThis section provided an in-depth analysis of the performance\nof GraphMatch on various graph datasets. We observe that the\nsystem exhibits linear scalability when the graphs have close to\nuniform degree distribution; and that the relative performance can\nSomewhere, ..., 2021, Virtual\nJonas Dann, Tobias Götz, Daniel Ritter, Jana Giceva, and Holger Fröning\nbe significantly improved with our proposed optimizations (up to\n5x on the patents dataset and close to 3x on youtube). A detailed\nablation study has demonstrated the effect of each individual opti-\nmization strategy and how their effects differ depending on a given\nquery and input graph. When compared to state-of-the-art systems\n(GraphFlow and RapidMatch), we have shown that GraphMatch\nhas superior performance, on average outperforming GraphFlow\nby 2.68×, and RapidMatch by 5.16× across all queries and datasets.\nWhen zooming in on the performance for individual queries, we\nobserve that GraphMatch exhibits an excellent performance for\nall queries when working with graph datasets that have an only\nslightly skewed degree distribution, and that there is room for im-\nprovement when handling cyclic queries like Q2 and Q3 on highly\nskewed graphs (e.g., youtube and wiki-talk).\n6\nCONCLUSION\nWe proposed GraphMatch, an FPGA-based efficient subgraph query\nprocessing accelerator. GraphMatch is inspired by the insight that\ncurrent CPU-based subgraph query processing systems are limited\nby set intersection which FPGAs are well suited for. We showed\nthe potential of such a design by first introducing a novel set inter-\nsector AllCompare specifically developed for FPGAs that is able to\noutperform all state-of-the-art CPU intersectors and FPGA adapta-\ntions of CPU set intersection algorithms. Therafter, we introduce\nthe GraphMatch architecture with dynamic query reconfiguration\nand three key performance optimizations. Our experimental per-\nformance evaluation showed scalability and superior performance\nof GraphMatch compared to state-of-the-art CPU-based subgraph\nquery processing systems GraphFlow and RapidMatch with an\naverage speedup of 2.68× and up to 5.16× respectively.\nIn future work, we want to extend GraphMatch to benchmarks\nwith labeled graphs and further improve performance for highly de-\ngree skewed graphs with more sophisticated caching. Additionally,\nwe consider connecting the GraphMatch instances with a work-\nstealing enabling matching crossbar and exploring query plan opti-\nmization for GraphMatch could unlock even higher performance.\nREFERENCES\n[1] Christopher R. Aberger, Andrew Lamb, Susan Tu, Andres Nötzli, Kunle Oluko-\ntun, and Christopher Ré. 2017. EmptyHeaded: A Relational Engine for Graph\nProcessing. ACM Trans. Database Syst. 42, 4 (2017), 20:1–20:44.\n[2] Bibek Bhattarai, Hang Liu, and H. Howie Huang. 2019. CECI: Compact Embedding\nCluster Index for Scalable Subgraph Matching. In SIGMOD. ACM, 1447–1462.\n[3] Fei Bi, Lijun Chang, Xuemin Lin, Lu Qin, and Wenjie Zhang. 2016. Efficient\nSubgraph Matching by Postponing Cartesian Products. In SIGMOD. ACM, 1199–\n1214.\n[4] Guohao Dai, Tianhao Huang, Yuze Chi, Ningyi Xu, Yu Wang, and Huazhong\nYang. 2017. ForeGraph: Exploring Large-scale Graph Processing on Multi-FPGA\nArchitecture. In FPGA. 217–226.\n[5] Jonas Dann, Daniel Ritter, and Holger Fröning. 2022.\nGraphScale: Scalable\nBandwidth-Efficient Graph Processing on FPGAs. In FPL. IEEE, 24–32.\n[6] Jonas Dann, Daniel Ritter, and Holger Fröning. 2023. Non-relational Databases\non FPGAs: Survey, Design Decisions, Challenges. ACM Comput. Surv. 55, 11\n(2023), 225:1–225:37.\n[7] Jonas Dann, Royden Wagner, Daniel Ritter, Christian Faerber, and Holger Fröning.\n2022. PipeJSON: Parsing JSON at Line Speed on FPGAs. In DaMoN. 3:1–3:7.\n[8] Michael J. Freitag, Maximilian Bandle, Tobias Schmidt, Alfons Kemper, and\nThomas Neumann. 2020. Adopting Worst-Case Optimal Joins in Relational\nDatabase Systems. PVLDB 13, 11 (2020), 1891–1904.\n[9] Per Fuchs, Peter A. Boncz, and Bogdan Ghit. 2020. EdgeFrame: Worst-Case\nOptimal Joins for Graph-Pattern Matching in Spark. In GRADES-NDA, Akhil\nArora, Semih Salihoglu, and Nikolay Yakovets (Eds.). ACM, 4:1–4:11.\n[10] Myoungji Han, Hyunjoon Kim, Geonmo Gu, Kunsoo Park, and Wook-Shin Han.\n2019. Efficient Subgraph Matching: Harmonizing Dynamic Programming, Adap-\ntive Matching Order, and Failing Set Together. In SIGMOD. ACM, 1429–1446.\n[11] Shuo Han, Lei Zou, and Jeffrey Xu Yu. 2018. Speeding Up Set Intersections in\nGraph Algorithms using SIMD Instructions. In SIGMOD. ACM, 1587–1602.\n[12] Xin Jin, Zhengyi Yang, Xuemin Lin, Shiyu Yang, Lu Qin, and You Peng. 2021. FAST:\nFPGA-based Subgraph Matching on Massive Graphs. In 37th IEEE International\nConference on Data Engineering, ICDE 2021, Chania, Greece, April 19-22, 2021.\nIEEE, 1452–1463. https://doi.org/10.1109/ICDE51399.2021.00129\n[13] Robert Lasch, Mehdi Moghaddamfar, Norman May, Süleyman Sirri Demirsoy,\nChristian Färber, and Kai-Uwe Sattler. 2022. Bandwidth-optimal Relational Joins\non FPGAs. In EDBT. 1:27–1:39.\n[14] Jinsoo Lee, Wook-Shin Han, Romans Kasperovics, and Jeong-Hoon Lee. 2012. An\nIn-depth Comparison of Subgraph Isomorphism Algorithms in Graph Databases.\nProc. VLDB Endow. 6, 2 (2012), 133–144.\n[15] Jure Leskovec and Andrej Krevl. 2014. SNAP Datasets: Stanford Large Network\nDataset Collection. http://snap.stanford.edu/data.\n[16] Daniel Mawhirter, Sam Reinehr, Connor Holmes, Tongping Liu, and Bo Wu. 2021.\nGraphZero: A High-Performance Subgraph Matching System. ACM SIGOPS Oper.\nSyst. Rev. 55, 1 (2021), 21–37.\n[17] Amine Mhedhbi and Semih Salihoglu. 2019. Optimizing Subgraph Queries by\nCombining Binary and Worst-Case Optimal Joins. PVLDB 12, 11 (2019), 1692–\n1704.\n[18] Hung Q. Ngo, Ely Porat, Christopher Ré, and Atri Rudra. 2018. Worst-case\nOptimal Join Algorithms. J. ACM 65, 3 (2018), 16:1–16:40.\n[19] Hung Q. Ngo, Christopher Ré, and Atri Rudra. 2013. Skew strikes back: new\ndevelopments in the theory of join algorithms. SIGMOD Rec. 42, 4 (2013), 5–16.\n[20] Natasa Przulj, Derek G. Corneil, and Igor Jurisica. 2006. Efficient estimation of\ngraphlet frequency distributions in protein-protein interaction networks. Bioin-\nform. 22, 8 (2006), 974–980.\n[21] Ryan A. Rossi and Nesreen K. Ahmed. 2015. The Network Data Repository with\nInteractive Graph Analytics and Visualization. http://networkrepository.com. In\nAAAI.\n[22] Siddhartha Sahu, Amine Mhedhbi, Semih Salihoglu, Jimmy Lin, and M. Tamer\nÖzsu. 2020. The ubiquity of large graphs and surprising challenges of graph\nprocessing: extended survey. VLDB J. 29, 2-3 (2020), 595–618.\n[23] Tom AB Snijders, Philippa E Pattison, Garry L Robins, and Mark S Handcock.\n2006. New specifications for exponential random graph models. Sociological\nmethodology 36, 1 (2006), 99–153.\n[24] Shixuan Sun and Qiong Luo. 2020. In-Memory Subgraph Matching: An In-depth\nStudy. In SIGMOD. ACM, 1083–1098.\n[25] Shixuan Sun, Xibo Sun, Yulin Che, Qiong Luo, and Bingsheng He. 2020. Rapid-\nMatch: A Holistic Approach to Subgraph Query Processing. Proc. VLDB Endow.\n14, 2 (2020), 176–188.\n[26] Julian R. Ullmann. 1976. An Algorithm for Subgraph Isomorphism. J. ACM 23, 1\n(1976), 31–42.\n[27] Todd L. Veldhuizen. 2012. Leapfrog Triejoin: a worst-case optimal join algorithm.\nCoRR abs/1210.0481 (2012).\n"
}