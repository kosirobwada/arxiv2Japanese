{
    "optim": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and\nOptimization\nWenqi Zhang1, Ke Tang2,3,6, Hai Wu2,4,6, Mengna Wang2,5, Yongliang Shen1\nGuiyang Hou1, Zeqi Tan1, Peng Li2,6†, Yueting Zhuang1, Weiming Lu1\n1College of Computer Science and Technology, Zhejiang University\n2Institute of Software, Chinese Academy of Sciences\n3Nanjing University of Posts and Telecommunications\n4Nanjing University of Information Science and Technology 5Beijing University of Technology\n6University of Chinese Academy of Sciences, Nanjing\nzhangwenqi@zju.edu.cn, lipeng@iscas.ac.cn\nAbstract\nLarge Language Models (LLMs) exhibit ro-\nbust problem-solving capabilities for diverse\ntasks. However, most LLM-based agents are\ndesigned as specific task solvers with sophis-\nticated prompt engineering, rather than agents\ncapable of learning and evolving through inter-\nactions. These task solvers necessitate manu-\nally crafted prompts to inform task rules and\nregulate LLM behaviors, inherently incapacitat-\ning to address complex dynamic scenarios e.g.,\nlarge interactive games. In light of this, we pro-\npose Agent-Pro: an LLM-based Agent with\nPolicy-level Reflection and Optimization that\ncan learn a wealth of expertise from interactive\nexperiences and progressively elevate its behav-\nioral policy. Specifically, it involves a dynamic\nbelief generation and reflection process for pol-\nicy evolution. Rather than action-level reflec-\ntion, Agent-Pro iteratively reflects on past tra-\njectories and beliefs, \"fine-tuning\" its irrational\nbeliefs for a better policy. Moreover, a depth-\nfirst search is employed for policy optimization,\nensuring continual enhancement in policy pay-\noffs. Agent-Pro is evaluated across two games:\nBlackjack and Texas Hold’em, outperforming\nvanilla LLM and specialized models. Our re-\nsults show Agent-Pro can learn and evolve in\ncomplex and dynamic scenes, which also bene-\nfits numerous LLM-based applications 1.\n1\nIntroduction\nDesigning a human-level agent with robust\nproblem-solving abilities has long been a vision\nin the academic community. This necessitates the\nagent to possess learning and generalization capa-\nbilities across a diverse array of tasks. The advent\nof Large Language Models (LLMs) (Chowdhery\net al., 2022; Zhang et al., 2022a; Zeng et al., 2023;\nTouvron et al., 2023a; OpenAI, 2022, 2023; Tou-\nvron et al., 2023b) has shed light on this vision,\n*Corresponding author.\n1Code is in https://github.com/zwq2018/Agent-Pro\nPlease defeat your opponents \nin the game …\nTask Environment\nObs k\nPrompt1\nLLM\nPolicy Evolution\nGoal: To win the game, you need to win more chips in winning \nmatches and lose fewer chips in losing matches…\nStrategy: Firstly, you calculate the value of your cards and then \nanalyze your opponents’ behaviors. If conservative players \nconsistently raise their bets, it means they may have a strong hand.\nDemonstrations: some good and bad examples…\nBehavior Policy\nYou’re playing Texas \nHold’em with three players…\nTask Description\nBehavior Policy\nYou’re playing limited Texas Hold’em with three players. Player-\n1 seems to have an aggressive style with some deceptive bluffs, \nwhile Player 2 is usually conservative with many early folds.\nTask Description\nInteractive\nScenario\nAction k\n…\nImperfect\nInformation Game\n• Environment\n• Other Players\n• Rules\nWorld-Belief\nSelf-Belief\n• Plan\n• State\n• Risk\nBelief k\nBelief k-1\nPolicy1\nPolicy2\nPolicyN\nAgent with Policy1\n• Task Description\n• Behavior Policy\n• Output Format\nAgent with Policy N\n• Task Description\n• Behavior Policy\n• Output Format\nPrompt N\nLLM\nPast Experience\nTrajectory\nResult\nBelief\nPolicy-Level\nReflection\nNew Prompt\nPolicy\nEvolution\nMulti-Agent\nCoexistence\nFigure 1:\nFor interactive tasks, e.g., imperfect-\ninformation games, we propose a versatile agent frame-\nwork capable of self-learning and evolving. Firstly,\nour agent constructs beliefs about itself and the envi-\nronment. Then it autonomously updates its prompts\nthrough policy-level reflection on past trajectories and\nbeliefs, evolving a better behavioral strategy.\nespecially they can be rapidly generalized across\na wide range of tasks with only a few demonstra-\ntions (Wei et al., 2022a,b). Benefiting from this,\nmany systems built upon LLMs have showcased\nmarkedly enhanced performance such as question-\nanswering (Yao et al., 2022; Schick et al., 2023;\nShen et al., 2023a; Zhang et al., 2023d; Madaan\net al., 2023), code generation (Hong et al., 2023;\nWu et al., 2023b), and real-world application (Qin\net al., 2023b; Zhang et al., 2023b).\nDespite these achievements, building a human-\nlevel agent remains a challenging endeavor. First,\nmost LLM-based agents are designed for specific\ntasks through sophisticated prompts, including de-\ntailed task descriptions and behavioral specifica-\ntions. However, numerous real-world tasks, e.g.,\nbusiness, company negotiations, and security, are\nmore intricate with imperfect information, necessi-\narXiv:2402.17574v1  [cs.AI]  27 Feb 2024\ntating laborious efforts to design strategic behavior.\nSecond, most LLM-based agents do not consider\ninteracting with task scenarios, and more critically,\ncannot learn from past experiences and evolve their\nbehavioral strategies during interactions. In con-\ntrast, humans often learn and adjust their behaviors\nthrough interaction, especially in novel scenarios.\nIn light of these, a promising yet under-explored\ntopic emerges: Can LLM-based agents learn and\nelevate behavioral strategies by interacting with\nthe environment like humans? It should be an\nindispensable ability of a human-level agent.\nRecently, numerous studies (Shinn et al., 2023;\nWang et al., 2023a; Zhang et al., 2023a; Zhao et al.,\n2023; Qian et al., 2024) undertake intriguing explo-\nrations, e.g., utilizing feedback for self-correction\nat the action-level. Besides, several efforts also ex-\nplore deploying LLM in interactive games, includ-\ning StarCraft (Ma et al., 2023), Minecraft (Wang\net al., 2023a), strategy-based gaming (Bakhtin et al.,\n2022; Guo et al., 2023a; Xu et al., 2023a,b).\nSimilarly, we first evaluate LLM-based agents\nwith the self-correction strategy in dynamic interac-\ntive scenarios, such as multi-player Texas Hold’em,\nwhich is a zero-sum game with imperfect informa-\ntion. However, we observe that it loses most of the\nrounds to its opponents, even the most advanced\nLLMs. Upon examining its reasoning thoughts and\nactions, we find that it often adopts irrational be-\nhaviors and is unable to deduce effective strategies\nfrom long action sequences.\nTo answer the above question, the Theory of\nMind (ToM) (Premack and Woodruff, 1978) may\nprovide some insight. In this framework, each hu-\nman develops perceptions of himself (self-belief)\nand the external environment (social-belief) in the\nsocial context, and then grounds their decisions\non these beliefs, or adjusts incorrect beliefs in re-\nsponse to external feedback. Inspired by this, we\nadvocate Agent-Pro: a LLM-based Agent with\nPolicy-level Reflection and Optimization. Agent-\nPro is endowed with the capacity to learn and\nevolve within environments, i.e., autonomously re-\nflect on past experiences, calibrate its beliefs about\nitself and the environment, and optimize its behav-\nior policy without parameter tuning.\nConcretely, as shown in Figure 1, an LLM-based\nagent involves an LLM as the foundational model\nand some instructions in the prompt to regulate\nits behavior (policy). Upon observing partial in-\nformation from the scenarios, Agent-Pro first up-\ndates its self-belief and world-belief, then makes\ndecisions based on these beliefs. After exploring\ntasks, Agent-Pro performs a policy-level reflection\nand optimization on past trajectories, beliefs, and\nresults. It autonomously \"fine-tunes\" its beliefs,\nsearches for useful prompt instructions, and con-\nsolidates them into a new behavior policy.\nThe experiments in two zero-sum games, Black-\njack and Texas Hold’em, demonstrate that Agent-\nPro, after evolution, can defeat vanilla LLMs and\nspecialized models, improving the game’s payoffs.\nIt indicates that Agent-Pro enhances its capabili-\nties through interaction and reflection without hu-\nman guidance. As depicted in Figure 1, the initial\nprompt is quite simple (Left Bottom), but after\nlearning and evolution, the Agent-Pro generates\nmany practical instructions (Right Bottom). For\ninstance, Agent-Pro records estimations of each op-\nponent’s style in Task Description and adds spe-\ncific Goals, Strategies in Behavior Policy.\nThe contributions of this work are as follows:\n• We introduce Agent-Pro, a framework capa-\nble of learning and evolving within interactive\ngames, empowering LLM-based agents to effi-\nciently adapt to more complex dynamic tasks.\n• We devise a belief-aware decision-making pro-\ncess with self and world-belief, enhancing its\ncapabilities for intricate tasks, i.e., generating\nmore rational actions in interactive scenarios.\n• We utilize policy-level reflection and optimiza-\ntion to iteratively update prompt instructions,\nwhich empower Agent-Pro to progressively\nevolve from a novice to a skilled veteran with\nmany strategic behaviors.\n• After learning, Agent-Pro is evaluated in mul-\ntiplayer games and defeats specialized models,\ngaining notable progress. It develops strategic\nskills like humans, e.g., actively cutting losses,\nbluffing, or disguising to influence others.\nNot just in card games, similar scenarios abound\nin the real world as well. Through self-learning\nand evolution, Agent-Pro can enhance deployment\neffectiveness in those scenarios, expanding the ca-\npability boundaries of LLM-based agents notably.\n2\nProblem Definition\nOur study focuses on multi-player imperfect infor-\nmation interactive games, with two characteristics:\nImperfect Information. Unlike perfect infor-\nmation games (e.g., chess), imperfect information\nscenarios are characterized by agents only having\naccess to their own states and public information,\nwithout knowing the states of others, e.g., in Texas\nHold’em, players cannot observe others’ cards,\nwhich is dissimilar to many LLM-based tasks.\nDynamic Interaction. There may be multiple\nagents in the environment, and they may influence\neach other. That is, the actions of one agent may\nlead to changes in the environment, which are un-\npredictable for other agents.\nIn real-world contexts, such as competition, com-\npany negotiations, and security, these scenarios can\noften be abstracted as multi-agent interactive sce-\nnarios with imperfect information. Research on\nthis can offer viable solutions to many real-world\nproblems. We select two games as our testbed:\nBlackjack and Limit Texas Hold’em with multi-\nplayer. Please refer to Appendix B for details.\n3\nMethods\nTo empower agents to learn in interactive con-\ntexts, a typical method is reinforcement learn-\ning (Zhang et al., 2021, 2022b). This involves\nexploring highly rewarding actions through trial\nand error and solidifying these experiences into\nmodel parameters. Nonetheless, the training over-\nhead for LLMs is substantial. Therefore, we em-\nploy a gradient-free \"exploration-learning\" strategy\nthat enables LLM-based agents to learn through\nin-context learning. Specifically, we convert the\npolicy learning into a prompt optimization process,\ni.e., LLM autonomously reflects and updates the\nprompt’s instructions based on its exploration expe-\nrience, solidifying the high-reward strategies into\nthe prompts. Benefiting from LLM’s generaliza-\ntion capabilities, our agent can summarize rules\nand learn specialized skills from a small number\nof samples like humans, making it well-suited for\nmany real-world scenarios.\nAs shown in Figure 2, Agent-Pro comprises\nthree components: (1) A Belief-Aware Decision-\nMaking process. It first updates beliefs about the\nworld and itself, rendering more coherent and con-\nsistent decisions in dynamic and imperfect game\nscenarios. (2) A Policy-Level Reflection. Rather\nthan reflecting on a single action, our design em-\npowers LLMs to self-reflect on irrational beliefs\nfrom failed experiences. Then, it summarizes these\nerroneous beliefs into specific prompt instructions,\nlike acting strategy (Behavioral Guideline), descrip-\ntions of the task world, and conjectures about other\nplayers (World Modeling), etc, which can calibrate\nits incorrect beliefs, evolving into a better policy.\n(3) A Prompt Optimization process ensures that the\nagent’s policy evolves for a higher payoff following\na DFS-based search.\n3.1\nBelief-aware Decision-Making Process\nTo develop an LLM-based agent better suited for\ninteractive environments, we draw inspiration from\nthe Theory of Mind (ToM) (Premack and Woodruff,\n1978; Li et al., 2023b; Guo et al., 2023a). In this\nframework, human condenses perceptions of them-\nselves (self-belief) and the external environment\n(social-belief) and then ground their decisions on\nthese beliefs, or adjust incorrect beliefs in response\nto external feedback. We also design a belief-aware\ndecision-making process for Agent-Pro, simulating\nhuman cognitive processes in social contexts.\nFirst, we need to define the policy of an LLM-\nbased agent, which refers to a specific behavioral\nstrategy guiding the agent to interact and com-\nplete tasks. It often involves complex prompts\ndesigned by experts, covering task rules, strate-\ngies, and output formats.\nIn a zero-sum game\nwith K+1 players (assuming playing order is\n(op1, our, op2, .., opK)), we denote the policy of\nour agent as π with some observable information,\ncontaining agent’s private information st, public\ninformation ot, our own action at, and the actions\nof all opponents aop1\nt\n, aop2\nt\n, .., aopK\nt\n, where t means\nt-th rounds of a game. Therefore a complete game\ntrajectory spanning t rounds:\nH0:t = {(s0, o0, aop1\n0\n, a0, aop2\n0\n, ..., aopK\n0\n),\n...\n(st, ot, aop1\nt\n, at, aop2\nt\n, ..., aopK\nt\n)}\n(1)\nAs shown in Figure 2, when making a deci-\nsion, Agent-Pro first generates a dynamic belief\nξ about itself (self-belief) and opponents (world-\nbelief) in natural language. Then, it predicts an\naction based on the latest beliefs. For instance,\nfor Texas Hold’em, Agent-Pro’s understanding of\nits hand cards, plan, and potential risk constitutes\nits self-belief, while the conjectures about the op-\nponents form its world-belief. These beliefs are\nupdated in each decision-making cycle. Equipped\nwith this, Agent-Pro can generate more coherent\nand consistent actions:\nξt+1, at+1 ∼ π(H0:t, st+1, ot+1, aop1\nt+1, ξt)\n(2)\n!!\n…\n!\"\n\"\n!#\n\"\n!$\n\"\n!#\n$\n!$\n$\n!\"\n$\n…\nScore: -1.7\nScore: 0.9\n0.4\n0.8\n0.3\n0.8\n1.2\n…      …      …\nPolicy Generation\nPolicy Evaluation\nSearching Path:\n!!\n!\"\n\"\n!\"\n$\n!$\n$\n!#\n$\n!$\n\"\n!#\n\"\n….\nPolicy Evaluation For K2 times\nBelief t+1\nAction t+1\nBelief t+2\nAction t+2\nAgent-Pro\nOther Agents\nEnvironment\n!!\"#\n!!\"#\n$%\nPrivate State\nPublic State\nPrivate State\n\"!\"#\n#!\"#\n\"!\"&\n#!\"&\nObservable\nUnobservable\ns!\"#\n#!\"#\nBelief t\n...\nResults\nTrajectory t\nPolicy-Level Reflection:\n• Please check that the belief is consistent with the final result.\n• Make sure the action and the belief are consistent at each step…\n• Assess the rationality of each belief … . Correct any erroneous beliefs…\n• Whether the beliefs accurately reflect the underlying intentions behind the opponents\nAction\nEnv\nFeedback\nAction-Level Reflection\nAction\nEnv\nFeedback\nPolicy-Level Reflection\nWorld-Belief\nSelf-Belief\nBehavioral\nGuideline\nWorld \nModeling\nBelief\nNew Action\nPolicy\nNew Policy\nVerification\nWorld Modeling\nI perceive that Player 1 seems adept at psychological warfare. Despite holding a weak \nhand, he managed to bluff effectively, coercing me into folding prematurely… Next \ntime, I must be cautious and not be misled by him.\nPlayer 2 also likes to raise, it seems like his style is more aggressive.\nBehavioral Guideline\nGoal: To win more chips, I need to maximize gains when holding a strong hand and \nminimize losses when holding a weak hand …\nStrategy: I need to learn to bluff. Even if my hand is weak, I can test my opponents' \nreactions by raising it in the early stages ...\nDemonstration: This game is typical. I should record it for future reference.\n{Label: I got a weak hand..., Detail: …}\nSelf-Belief: Currently, my hand is weak (State). I need to \nwait for the next community card reveal. Besides, I must \nobserve my opponent’s actions closely (Plan). If they appear \nstrong, keeping calling may lead to more losses (Risk). ……\nWorld-Belief: In my impression, player-1 is relatively \nconservative. However, he has been consistently raising, \nwhich may indicate a strong hand (Opponent). The final \ncommunity card will be revealed shortly, and it might still be \na weak one (Environment). If Player-1 raises again, \naccording to the rules, I can only raise, call, or fold (Rule).\nBehavioral\nGuideline\nWorld \nModeling\nScores\nPlayer1\nPlayer2\nAgent-Pro\nPlayer3\nPlayer1\nPlayer2\nAgent-Pro\nPlayer3\nSwap\nCard\nSwap\nPosition\nPlayer1\nPlayer2\nPlayer3\nAgent-Pro\nDealer\nBlind\nDealer\nBlind\nDealer\nBlind\nDFS-based Policy Evolution\nBelief-aware Decision-Making\nPolicy-Level Reflection\nFigure 2: In a competitive multiplayer game with imperfect information, Agent-Pro designs a dynamic belief\nto enhance decision-making capabilities. It first updates its beliefs about the world and itself, then generates\nmore coherent actions. To achieve policy-level reflection, Agent-Pro examines the beliefs associated with failed\ntrajectories. It then summarizes prompt instructions, including World Modeling and Behavioral Guideline to calibrate\nincorrect beliefs. Lastly, Agent-Pro employs a DFS-based search to incrementally enhance policy effectiveness.\nWhen a game is over, we acquire the observable\nstate R (e.g., private hand cards after showdown)\nand the final scores S of all players. The objective\nis to find an optimal π∗ to maximize S(our).\n3.2\nPolicy-Level Reflection\nEquipped with an initial policy (a simple prompt)\nand a dynamic belief, Agent-Pro already possesses\nbasic capabilities for game exploration. To fur-\nther enhance Agent-Pro’s capabilities, we design a\nlearning mechanism via a policy-level reflection.\nSpecifically, many text-based tasks have em-\nployed reflection strategies and immediate environ-\nmental feedback to correct prior actions. However,\nin many typical interaction scenarios with longer\ndecision-making processes, action-level reflections\nare not directly applicable due to delayed feedback.\nTherefore, for such a long-horizon interaction pro-\ncess, Agent-Pro is instructed to focus on the ratio-\nnality of beliefs and underlying behavioral policies\nrather than individual actions.\nBelief Calibration As depicted in Figure 2, un-\nder the guidance of the current behavior policy,\nAgent-Pro generates actions based on self-belief\nand world-belief. If these beliefs are inaccurate,\nthey may lead to irrational actions and eventual\nfailure. Therefore, Agent-Pro examines the ratio-\nnality of these beliefs based on the final results and\nreflectss on the reasons for the final failure.\nCorrectness: Whether its beliefs about\nitself , the game , and its opponents\nalign with the final results.\nConsistency: Whether each belief and\naction is self -contradictory.\nRationality: Whether the beliefs\naccurately reflect the underlying\nintentions behind the opponents.\nReasons: Reflect on why it lost to its\nopponents , which beliefs are problematic\n, and what the underlying reasons are.\nLastly, to calibrate the incorrect beliefs, Agent-\nPro summarizes these reflections and analyses\nabout itself and the external world into specific\ninstructions: Behavioral Guideline and World\nModeling, where the former represents general-\nized behavioral strategies for this task, and the latter\nsignifies its understanding and conjectures about\nthe game world. For instance, in Texas Hold’em,\nAgent-Pro summarizes the following contents:\nBehavioral Guideline\n1-Please summarize a detailed goal based\non your reflection on beliefs. {Goal}\n2-What strategy helps you build correct\nbelief and win at similar .. {Strategy}\n3-Can this game be considered a typical\nexample for future ... {Demonstration}\nWorld Modeling\n1-Accurately model each player to help\nbuild more precise beliefs about them ,\nincluding action , and style.{Opponent}\n2-Describe any game rules or details\nthat are easy to overlook ...{Rule}\nAgent-Pro summarizes high-level strategies within\nthe Behavioral Guideline and describes the task\nand opponents in World Modeling. These instruc-\ntions can calibrate previous incorrect beliefs and\nimprove policy performance. The entire process\ncan be formalized as follows:\nInstructionn+1 ←LLM (Hn\n0:T ,{ξn\n1 , ξn\n2 ..},Rn,Sn)\n(3)\nwhere Hn\n0:T denotes a complete trajectory at the\nn-th match, {ξn\n1 , ξn\n2 , ...} denotes the belief se-\nquence, Rn and Sn means the final results and\nscore.\nInstructionn+1 denotes new generated\nBehavioral Guideline and World Modeling.\nVerification After extracting these instructions,\nAgent-Pro verifies its efficacy. Agent-Pro incorpo-\nrates these generated Behavioral Guideline and\nWorld Modeling into the prompt and then replays\nthe same game again, i.e., the same opponents and\ninitial conditions. If the final score improves, we\nretain them in the prompt. Otherwise, we regener-\nate a new one. If it fails to pass verification after\nthree retries, we discard this trajectory Hn:\nπn+1 V erify\n←−−−−πn ∪ Instructionn+1\n(4)\nwhere ∪ means incorporates new instructions into\nthe previous prompt for πn+1. This new policy\nencompasses more effective instructions, empower-\ning Agent-Pro to establish accurate self- and world\nbeliefs and generate more rational actions.\n3.3\nDFS-based Policy Evolution\nTo iteratively update the policy, we devise a policy\noptimization process based on depth-first search\n(DFS). It encompasses a policy evaluation process\nto assess the generalization ability of the new policy\nin novel game scenarios and a search mechanism\nto progressively find a better policy.\nPolicy Evaluation Each time the policy is\nupdated, Agent-Pro is required to evaluate the\nnew strategies.\nThis evaluation process is dis-\ntinct from the previous Verification step, as the\nVerification repeatedly utilizes the \"training\"\ndata for evaluation and can not ensure the gener-\nalizability of the new policy. Hence, Agent-Pro\nconducts a thorough assessment of the new policy\nin novel trajectories. Besides, it is imperative to\neliminate the influence of random factors when pol-\nicy evaluation, e.g., a poor initial hand due to bad\nluck or an unfavorable playing order.\nTherefore, we first randomly generate a new\ngame for K+1 players.\nThen we sequentially\nswap both the hand cards and the playing order\nof each player, generating a total of (K+1)2 combi-\nnations. To eliminate randomness, we concurrently\nuse these (K+1)2 games to evaluate Agent-Pro’s\nnew policy. We calculate the average score over\nthe (K+1)2 games for each player. Since the influ-\nences of hand-card quality and playing order are\nmitigated, the average score of all combinations\ncan represent the true capabilities of each player.\nLastly, we calculate the evaluating metrics:\n∆=\n1\n(K+1)2\n(K+1)2\nX\nj\n\u0014\nSj(our)−max\ni\nSj(opi)\n\u0015\n(5)\nwhere i ∈ {1, . . . , K} denotes the index of an op-\nponent, and j denotes the index of the games within\n(K+1)2 combinations. The ∆ assesses both the ab-\nsolute gains of the policy and its gains relative to\nthe strongest opponent, providing a comprehensive\nevaluation in multiplayer gaming scenarios.\nPolicy Search Inevitably, sometimes the new\npolicy does not bring an improvement in ∆ in the\nnew scenario. In such cases, we employ DFS to\nsearch for a better policy from other branches (i.e.,\nother candidate policies). As shown in Figure 2,\nwhen updating old policy πn, we generate B can-\ndidate policies {πn+1\n1\n, πn+1\n2\n,..., πn+1\nB\n}, forming B\nWin Rate ↑(%)\nBased Models\nStrategy\nQwen-72B Llama2-70B GPT3.5 GPT4\nVanilla LLM\n0.5\n0.3\n27.9\n34\nRadical LLM\n0.6\n0.4\n1.8\n11.5\nReAct\n30.9\n11.8\n36.6\n40.9\nReflexion\n32.3\n12.1\n36.7\n40.8\nAgent-Pro\n36.2 ↑3.9\n23.1 ↑11.0\n38.2 ↑1.5 40.4 ↓0.5\n- w/o Learning\n34.1\n8.0\n37.4\n40.6\nTable 1: All agents compete independently against the\ndealer and then we calculate their win rates. w/o means\nonly with belief-aware decision-making process.\n↑\nshows the difference compared to the best baseline.\nbranches. Then, we first calculate ∆n+1\n1\nfor new\npolicy πn+1\n1\nand compare it with ∆n. If ∆n+1\n1\nis\ngreater than ∆n, we accept this evolutionary. Oth-\nerwise, we reject πn+1\n1\nand consider πn+1\n2\n. If none\nof the B candidate policies πn+1 enhance Agent-\nPro’s performance, we backtrack to πn and con-\nsider its sibling nodes πn\n2 . Similarly, Agent-Pro ex-\nplores the environment using πn\n2 , then also updates\nB candidate policies and searches in a depth-first\nmanner. Ultimately, we select the policy with the\nhighest ∆ across the entire policy tree.\n4\nGame: Blackjack\nEnvironment Settings We employ the RL-\nCard (Zha et al., 2019) as our simulators for two\ngames. We train two reinforcement learning agents\nas opponents: DQN (Mnih et al., 2015), and Deep\nMonte Carlo Search (DMC) (Zha et al., 2021).\nPlease refer to Appendix A for more details.\n4.1\nResults\nAs shown in Table 1, we report the win rates of\neach agent against the dealer over 900 games. We\nalso provide the results of RL-based models and a\nhuman player in Table C3 for reference.\nAgent-Pro Significantly Surpasses the Base-\nline Agents Across most LLMs. The results show\nthat Agent-Pro significantly surpasses most base-\nline agents with an average advantage of +4%. For\nexample, On Qwen-72B and Llama2-70B, Agent-\nPro significantly surpasses Reflexion with increases\nof +3.9% and +11%, respectively.\nFor GPT-4,\nBlackjack is relatively simple, so the win rates of\ndifferent strategies are quite similar.\nWhat has Agent-Pro learned from evolution?\nCompared to ReAct and Reflexion, Agent-Pro is\nmore robust. We find that this is due to the effective\nbehavioral guidelines summarized by policy-level\nreflection. For instance, Agent-Pro summarizes\nVanilla LLM\nReAct\nReflexion\nAgentPro\nVanilla LLM\nReAct\nReflexion\nAgentPro\nDealer's face-up card has a low point\n90\n80\n70\n60\n50\n40\n30\n20\n10\nAgent-Pro << Baselines\nAgent-Pro << Baselines\nAgent-Pro < Baselines\nHit Rate By Agent(%)\nC2\nA2\nC1\nDealer's face-up card has a high point\n90\n80\n70\n60\n50\n40\n30\n20\n10\n4    5   6\n7 \n8\n9    10\n11\n12\n13  14   15    16\n17\n18 19   20   21\nInitial Point Holding by Agent\nHit Rate By Agent(%)\nA1\nAgent-Pro = Baselines\nB2\nB1\nAgent-Pro > Baselines\nAgent-Pro > Baselines\nFigure 3: We analyze the hit rates of the agents under\ndifferent initial point totals. Upper Figure: The dealer’s\nface-up card has a low point. Lower figure: The dealer’s\nface-up card has a high point.\ntwo instructions as follows: 1-When you have\nachieved a relatively stable total hand\nvalue,\nchoosing\nnot\nto\ntake\nrisks\nis\na good decision.\n2-Analyze the dealer\ncards\nin\nWorld-belief,...,\nexcessive\nrisk-taking\ncan\nlead\nto\nunfavorable\noutcomes... These self-summarized instructions\ncan alert Agent-Pro to the risks associated with\naction Hit, thus making more rational decisions.\n4.2\nAnalyisis\nAgent-Pro is More Rational than Baselines. We\nfurther analyze the Hit rates of the agents under\ndifferent initial point totals, i.e., the sum of the\ninitial two cards. The hit rate represents whether\nthe agent is willing to take risks to draw cards.\nAt this point, the player needs to consider both\ntheir own hand and the dealer’s hand to decide\nwhether to take the risk. However, in Figure 3,\nwe observe that the baseline seems to only focus\non its own hand, with no significant difference in\nbehavior when the dealer’s cards are high or low,\nwhereas Agent-Pro is much more reasonable. For\ninstance, for Agent-Pro, areas B1 and B2 show a\nclear difference. It tends to Stand when the dealer\nhas high cards and Hit when the dealer has low\ncards. Because it believes the dealer is more likely\nto bust with high cards, making it not worth the\nrisk for itself. We provide some detailed cases\nin Figures F1 to F4 to show their difference.\n5\nGame: Limit Texas Hold’em\nSetups In Limit Texas Hold’em, each player has\ntwo private cards and chooses from four actions:\nFold, Check, Call, Raise. We set up matches\nAgent Strategy\nBased Model = GPT3.5\nBased Model = GPT4\nBased Model = Llama2-70B\nDQN\nDMC\nGPT3.5 Agent\nDQN\nDMC\nGPT3.5 Agent\nDQN\nDMC\nGPT3.5 Agent\nHuman\n-4.0\n0.7\n-2.4\n5.7\n-4.0\n0.7\n-2.4\n5.7\n-4.0\n0.7\n-2.4\n5.7\nVanilla LLM\n-0.3\n2.2\n-0.8\n-1.1\n-2.2\n1.7\n-0.9\n1.4\n-0.8\n3.4\n-0.4\n-2.2\nAggressive LLM\n-0.4\n3.0\n-0.5\n-2.1\n-2.0\n2.8\n-1.0\n0.2\n-1.6\n7.6\n-1.2\n-4.8\nConservative LLM -0.7\n2.9\n-0.9\n-1.3\n-1.6\n2.7\n-1.6\n0.5\n-0.5\n3.4\n-0.8\n-2.1\nSelf-Consistency\n-0.5\n1.9\n-0.8\n-0.6\n-2.8\n2\n-0.7\n1.5\n-1.0\n3.8\n-0.9\n-1.9\nReAct\n-0.7\n1.7\n-0.7\n-0.3\n-2.4\n1.3\n-1.1\n2.2\n-1.1\n3.9\n-0.8\n-2.0\nReflexion\n-0.1\n2.5\n-0.9\n-1.5\n-2.6\n2.1\n-0.7\n1.2\n-1.2\n4.7\n-0.9\n-2.6\nMulti-Agent\n-1.1\n2.3\n-0.3\n-0.9\n-1.8\n1.9\n-1.2\n1.1\n-0.7\n3.5\n-1.0\n-1.8\nAgent-Pro\n-1.5↓1.2 1.4 ↓0.8 -1.1 ↓0.3 1.2 ↑2.3 -3.9↓1.7 1.1 ↓0.6 -1.5 ↓0.6 4.3 ↑2.9 -1.2 ↓0.4 3.1 ↓0.3 -0.5 ↓0.1 -1.4 ↑0.8\n- w/o Learning\n-0.7\n1.8\n-1.0\n-0.1↑1\n-3\n1.5\n-1.2\n2.7↑1.3 -0.3\n3.3\n-1.2\n-1.8↑0.4\nTable 2: Each game contains four players. The first three are fixed as DQN, DMC, GPT-3.5, and the last one is the\nagent we need to evaluate: Agent-Pro or baselines. Arrow means comparison with Vanilla LLM.\namong four players: DQN, DMC, GPT-3.5, and X,\nwhere X represents the LLM-based agent we aim\nto evaluate, including Agent-Pro and baselines (Ap-\npendix A). The prompts for baselines and Agent-\nPro in Appendices E.3 and E.4. To enable Agent-\nPro to learn within the game, we employ a total of\n167 \"training\" game hands and 20 evaluation hands.\nPlease refer Appendix A.4 for detail.\nMetrics Similar to Section 3.3, we sample 100\nnew game hands and allocate them to players. The\nplayers sequentially swap their hands and positions,\ngenerating 16 distinct permutations to eliminate\nthe impact of chance and playing order. Lastly,\nwe acquire 1600 games as the test set in total and\ncalculate the average chip counts for four players.\nWe provide detailed statistics in Table B1 regarding\n\"training\", evaluation, and test set.\n5.1\nResults\nAs shown in Table 2, we report the final chip counts\nof various LLM-based agents against the other\nthree players (DQN, DMC, GPT-3.5). The results\nindicate that Agent-Pro consistently outperforms\nRL-based agents e.g., DMC, and surpasses other\nLLM-based agents across numerous LLMs.\nAgent-Pro Surpasses LLM-based Agents and\nalso Defeats RL-based Agents. We observe that\nAgent-Pro achieves significant progress on GPT-\n3.5, GPT-4, and Llama2-70B, with an average score\nincrease of +2 points. Besides, it surpasses special-\nized agents (DMC) on GPT-4, with an advantage\nof +3.2 points, and outperforms other LLM-based\nagents by a large margin (larger than 2.0 points).\nBy analyzing the actions of Agent-Pro, we notice\nthat it has learned to use multiple game techniques\nlike humans. For instance, based on the analysis\nof the opponent’s style in the World Modeling,\nit may coerce some cautious players into folding\nby bluffing or sometimes it may disguise itself to\nentice aggressive opponents to raise their bets.\nBelief Enhances Decision-making Capabili-\nties in Dynamic Scenario. Even without the learn-\ning process (policy-level reflection), Agent-Pro\nalso can improve Vanilla LLM’s performance by\n+0.9 points. For instance, on GPT-3.5 and GPT-\n4, it led to improvements of +1 points and +1.3\npoints, respectively, which already slightly sur-\npasses most LLM-based agents. This improve-\nment stems from the dynamic belief, which en-\nables agents to promptly capture updates in com-\nmunity cards, changes in opponents’ strategies, etc.,\nthereby making more rational decisions. From the\nperspective of ReAct, our belief can also be seen\nas a dynamic thought process constructed based\non the ToM framework, which endows agents with\nthe ability to actively perceive internal and external\nbeliefs and how they may change over time.\nBesides, in Table B2, we explore whether our\nevolution process could be replaced by few-shot\nlearning, i,e., we add some demonstrations to the\nprompt of Vanilla LLM, and evaluate its results.\nWe find that failed game trajectories can slightly\nimprove its effectiveness, but not as significantly as\nour evolution strategy. In Table B2, we also ablate\nthe belief component from Agent-Pro but remain\nlearning process. It shows that directly reflecting\non the action sequence is quite unstable, and results\nin some vague and verbose behavioral instructions.\n5.2\nAnalysis on Learning Process\nWe analyze the performance of Agent-Pro through-\nout the whole learning process. As shown in Fig-\nure 4, Agent-Pro is evaluated every 10 iterations.\nDifferent LLM-based Agent-Pro Develops Di-\nverse Strategies. We observe that the learning\ncurves of the three Agent-Pros exhibit significant\nFlexible Strategy:\nBluffing, Deceiving, and \nActively Probing Opponents\nRisk-Averse Strategy:\nFolding at Beginning to\nMinimize Loss\nCautious Strategy:\nInitially Cautious, only \nChecking or Actively \nFolding in the Later Stages\nWhen dealt the same bad hand\nChip of Agent-Pro\nFigure 4: We report the relations between iteration num-\nber and the performance (average chips and its std).\n2) More Rational\n1) Irrational\n2) More Cautious\n3) Proactive\n3) More Flexible\nFigure 5: We analyze the Fold and Raise frequencies\nof three agents to illustrate the evolution of the strategy.\ndifferences. Agent-Pro based on GPT-4 and GPT-\n3.5 rapidly improves their performance in the early\nstages of learning, with a maximum increase of\n+2.1 and 2.3 chips respectively. In contrast, Llama-\n2-70B exhibits a dissimilar learning process, with\nperformance initially declining in the first half and\nthen improving (+0.6 chips) in the latter half. An-\nalyzing the behaviors of the three agents, we dis-\ncover that their strategic styles are entirely different.\nWhen dealt the same bad hand, the GPT-4-based\nAgent-Pro is relatively flexible and may bluff to\nprobe opponents. GPT-3.5-based Agent-Pro tends\nto be cautious and may actively fold in the later\nstages, whereas the Llama-based Agent-Pro devel-\nops a highly conservative, risk-averse strategy. It\nconcedes at the beginning of the game by opting to\nFold, thereby losing only the initial few chips.\n5.3\nAnalysis on Policy Evolution\nWe manually select 20 challenging games (Details\nin Table C4). Then, we test three agents on these\n20 games: Agent-Pro in the early learning phase\n(Agent-Pro-Early), Agent-Pro, and Vanilla LLM.\nHow the Strategy Evolved. We calculate the\nfrequency of the most conservative action (Fold)\nand the most aggressive action (Raise) during the\nfour stages of the game: PreFlop, Flop, Turn,\nRiver. As shown in Figure 5, we discuss how\nthe strategy evolved. 1) The behavior of Vanilla\n-4\n0\n4\n8\n12\n16\n20\n24\n28\n32\nStrong Hand\n(20%)\nWeak Hand\n(60%)\nModerate Hand\n(20%)\nFinal Chips\nVanilla LLM\nAgent-Pro-Early\nAgent-Pro\nFigure 6: We categorize the agent’s hands into three\ntypes: strong, medium-strength, and weak hands.\nLLM is rather rigid, Folding early in the game\n(Preflop stage) and ignoring subsequent commu-\nnity cards. 2) As learning progresses, Agent-Pro-\nEarly becomes more rational, with a noticeable de-\ncrease in Folding frequency during the Preflop\nstage. It can observe the public cards in subse-\nquent phases before deciding to Folding. Besides,\nAgent-Pro-Early is more cautious, with a signifi-\ncant decrease in the frequency of Raising. 3) After\nlearning, Agent-Pro exhibits flexible and proactive\nbehavior. Compared to Agent-Pro-Early, its Fold\nfrequency in Preflop continues to decrease, but\nthe frequency of Raising in all four stages has re-\nbounded. This result demonstrates the evolution\nof the strategy: from irrational to rational, from\nconservative to flexible. A detailed case study is\nshown in Appendices F and F.2.\nWin More, Lose Less. As shown in Figure 6,\nwe categorize the hands dealt to the agent into three\ntypes: strong, medium, and weak hands, and record\ntheir performance separately. The results show that\nAgent-Pro can win more chips with strong hands\nand lose fewer chips with weak hands compared to\nVanilla LLM. Notably, Agent-Pro significantly im-\nproves performance (> 80%) with medium-strength\nhands, which indicates that it learns advanced skills,\nexpanding its capability boundaries.\n6\nConclusion\nWe design an LLM-based agent, Agent-Pro, ca-\npable of learning and evolution in complex in-\nteractive tasks. It first constructs a dynamic be-\nlief for decision-making in uncertain scenarios.\nThen Agent-Pro reflects on its interactive experi-\nences, corrects irrational beliefs, and summarizes\nits reflections into two instructions: behavioral\nguidelines and world descriptions for a new pol-\nicy. Lastly, we evaluate Agent-Pro in two zero-sum\ngames and observe that its decision-making capa-\nbilities significantly improve after learning from\nhistorical experiences.\nLimitations\nAgent-Pro has presented a novel paradigm for de-\nsigning an evolvable LLM-based agent, but we\nwant to highlight that there remain some limita-\ntions or improvement spaces: 1) Dependency: the\nlearning process of the Agent-Pro heavily relies\non the capability of the foundational model, es-\npecially its reasoning and reflection abilities. In\nTexas Hold’em, the GPT-4-based Agent-Pro ap-\nproaches the level of human players and surpasses\nDMC clearly, while GPT-3.5 and Llama2-70B-\nbased Agent-Pro are still weaker than DMC, de-\nspite notable improvements. We plan to continue\nenhancing the capabilities of Agent-Pro based on\nweaker LLMs, aiming to achieve greater improve-\nments even on smaller LLM models. 2) Perfor-\nmance: despite indispensable improvements, there\nmay still be a significant gap between Agent-Pro\nand state-of-the-art algorithms (such as CFR plus)\nin gaming scenarios. In the future, we will con-\ntinue to explore this issue and establish a set of\nbenchmarks to evaluate their behaviors more com-\nprehensively.\nReferences\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nAnton Bakhtin, Noam Brown, Emily Dinan, Gabriele\nFarina, Colin Flaherty, Daniel Fried, Andrew Goff,\nJonathan Gray, Hengyuan Hu, Athul Paul Jacob,\nMojtaba Komeili, Karthik Konath, Minae Kwon,\nAdam Lerer, Mike Lewis, Alexander H. Miller, San-\ndra Mitts, Adithya Renduchintala, Stephen Roller,\nDirk Rowe, Weiyan Shi, Joe Spisak, Alexander Wei,\nDavid J. Wu, Hugh Zhang, and Markus Zijlstra. 2022.\nHuman-level play in the game of diplomacy by com-\nbining language models with strategic reasoning. Sci-\nence, 378:1067 – 1074.\nMaciej Besta, Nils Blach, Ales Kubicek, Robert Ger-\nstenberger, Lukas Gianinazzi, Joanna Gajda, Tomasz\nLehmann, Michal Podstawski, Hubert Niewiadomski,\nPiotr Nyczyk, et al. 2023. Graph of thoughts: Solv-\ning elaborate problems with large language models.\narXiv preprint arXiv:2308.09687.\nEthan Brooks, Logan A Walls, Richard Lewis, and Satin-\nder Singh. 2023. Large language models can imple-\nment policy iteration. In Thirty-seventh Conference\non Neural Information Processing Systems.\nLiting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan,\nFangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan\nRajmohan, et al. 2023a. Introspective tips: Large lan-\nguage model for in-context decision making. arXiv\npreprint arXiv:2305.11598.\nWeize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,\nChenfei Yuan, Cheng Qian, Chi-Min Chan, Yujia Qin,\nYa-Ting Lu, Ruobing Xie, Zhiyuan Liu, Maosong\nSun, and Jie Zhou. 2023b. Agentverse: Facilitating\nmulti-agent collaboration and exploring emergent\nbehaviors in agents. ArXiv, abs/2308.10848.\nJiale Cheng, Xiao Liu, Kehan Zheng, Pei Ke, Hongning\nWang, Yuxiao Dong, Jie Tang, and Minlie Huang.\n2023.\nBlack-box prompt optimization: Aligning\nlarge language models without model training. arXiv\npreprint arXiv:2311.04155.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sut-\nton, Sebastian Gehrmann, and others. 2022. Palm:\nScaling language modeling with pathways. ArXiv,\nabs/2204.02311.\nYilun Du, Shuang Li, Antonio Torralba, Joshua B.\nTenenbaum, and Igor Mordatch. 2023. Improving\nfactuality and reasoning in language models through\nmultiagent debate. ArXiv, abs/2305.14325.\nZane Durante, Bidipta Sarkar, Ran Gong, Rohan\nTaori, Yusuke Noda, Paul Tang, Ehsan Adeli,\nShrinidhi Kowshika Lakshmikanth, Kevin Schulman,\nArnold Milstein, et al. 2024. An interactive agent\nfoundation model. arXiv preprint arXiv:2402.05929.\nCaoyun Fan, Jindou Chen, Yaohui Jin, and Hao He.\n2023. Can large language models serve as rational\nplayers in game theory? a systematic analysis. arXiv\npreprint arXiv:2312.05488.\nYao Fu, Hao-Chun Peng, Tushar Khot, and Mirella La-\npata. 2023. Improving language model negotiation\nwith self-play and in-context learning from ai feed-\nback. ArXiv, abs/2305.10142.\nRan Gong, Qiuyuan Huang, Xiaojian Ma, Hoi Vo, Zane\nDurante, Yusuke Noda, Zilong Zheng, Song-Chun\nZhu, Demetri Terzopoulos, Li Fei-Fei, et al. 2023.\nMindagent: Emergent gaming interaction.\narXiv\npreprint arXiv:2309.09971.\nJiaxian Guo, Bo Yang, Paul Yoo, Bill Yuchen\nLin, Yusuke Iwasawa, and Yutaka Matsuo. 2023a.\nSuspicion-agent:\nPlaying imperfect information\ngames with theory of mind aware gpt-4.\narXiv\npreprint arXiv:2309.17277.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nSong, Xu Tan, Guoqing Liu, Jiang Bian, and Yujiu\nYang. 2023b. Connecting large language models\nwith evolutionary algorithms yields powerful prompt\noptimizers. arXiv preprint arXiv:2309.08532.\nSirui Hong, Xiawu Zheng, Jonathan P. Chen, Yuheng\nCheng, Ceyao Zhang, Zili Wang, Steven Ka Shing\nYau, Zi Hen Lin, Liyang Zhou, Chenyu Ran,\nLingfeng Xiao, and Chenglin Wu. 2023. Metagpt:\nMeta programming for multi-agent collaborative\nframework. ArXiv, abs/2308.00352.\nCho-Jui Hsieh, Si Si, Felix X Yu, and Inderjit S Dhillon.\n2023. Automatic engineering of long prompts. arXiv\npreprint arXiv:2311.10117.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu,\nXuezhi Wang, Hongkun Yu, and Jiawei Han. 2022.\nLarge language models can self-improve.\nArXiv,\nabs/2210.11610.\nGuohao Li, Hasan Abed Al Kader Hammoud, Hani\nItani, Dmitrii Khizbullin, and Bernard Ghanem.\n2023a. Camel: Communicative agents for \"mind\" ex-\nploration of large language model society. In Thirty-\nseventh Conference on Neural Information Process-\ning Systems.\nHuao Li, Yu Chong, Simon Stepputtis, Joseph Camp-\nbell, Dana Hughes, Charles Lewis, and Katia Sycara.\n2023b. Theory of mind for multi-agent collabora-\ntion via large language models. In Proceedings of\nthe 2023 Conference on Empirical Methods in Natu-\nral Language Processing, pages 180–192, Singapore.\nAssociation for Computational Linguistics.\nNunzio Lorè and Babak Heydari. 2023. Strategic behav-\nior of large language models: Game structure vs. con-\ntextual framing. arXiv preprint arXiv:2309.05898.\nWeiyu Ma, Qirui Mi, Xue Yan, Yuqiao Wu, Runji Lin,\nHaifeng Zhang, and Jun Wang. 2023. Large lan-\nguage models play starcraft ii: Benchmarks and a\nchain of summarization approach. arXiv preprint\narXiv:2312.11865.\nAman Madaan, Niket Tandon, Prakhar Gupta, Skyler\nHallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,\nNouha Dziri, Shrimai Prabhumoye, Yiming Yang,\nSean Welleck,\nBodhisattwa Prasad Majumder,\nShashank Gupta, Amir Yazdanbakhsh, and Peter\nClark. 2023. Self-refine: Iterative refinement with\nself-feedback. ArXiv, abs/2303.17651.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver,\nAndrei A Rusu, Joel Veness, Marc G Bellemare,\nAlex Graves, Martin Riedmiller, Andreas K Fidje-\nland, Georg Ostrovski, et al. 2015. Human-level\ncontrol through deep reinforcement learning. nature,\n518(7540):529–533.\nOpenAI. 2022. Chatgpt.\nOpenAI. 2023. Gpt-4 technical report.\nLiangming Pan, Michael Stephen Saxon, Wenda Xu,\nDeepak Nathani, Xinyi Wang, and William Yang\nWang. 2023.\nAutomatically correcting large lan-\nguage models: Surveying the landscape of diverse\nself-correction strategies. ArXiv, abs/2308.03188.\nJoon Sung Park, Joseph C. O’Brien, Carrie J. Cai,\nMeredith Ringel Morris, Percy Liang, and Michael S.\nBernstein. 2023. Generative agents: Interactive simu-\nlacra of human behavior. In In the 36th Annual ACM\nSymposium on User Interface Software and Technol-\nogy (UIST ’23), UIST ’23, New York, NY, USA.\nAssociation for Computing Machinery.\nDebjit Paul, Mete Ismayilzada, Maxime Peyrard, Beat-\nriz Borges, Antoine Bosselut, Robert West, and Boi\nFaltings. 2023. Refiner: Reasoning feedback on in-\ntermediate representations. ArXiv, abs/2304.01904.\nDavid Premack and Guy Woodruff. 1978. Does the\nchimpanzee have a theory of mind? Behavioral and\nbrain sciences, 1(4):515–526.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with\" gradient descent\" and\nbeam search. arXiv preprint arXiv:2305.03495.\nCheng Qian, Shihao Liang, Yujia Qin, Yining Ye, Xin\nCong, Yankai Lin, Yesai Wu, Zhiyuan Liu, and\nMaosong Sun. 2024. Investigate-consolidate-exploit:\nA general strategy for inter-task agent self-evolution.\narXiv preprint arXiv:2401.13996.\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, Yi Ren Fung, Yusheng Su,\nHuadong Wang, Cheng Qian, Runchu Tian, Kunlun\nZhu, Shihao Liang, Xingyu Shen, Bokai Xu, Zhen\nZhang, Yining Ye, Bowen Li, Ziwei Tang, Jing Yi,\nYuzhang Zhu, Zhenning Dai, Lan Yan, Xin Cong,\nYaxi Lu, Weilin Zhao, Yuxiang Huang, Junxi Yan,\nXu Han, Xian Sun, Dahai Li, Jason Phang, Cheng\nYang, Tongshuang Wu, Heng Ji, Zhiyuan Liu, and\nMaosong Sun. 2023a. Tool learning with foundation\nmodels.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023b. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, M. Lomeli, Luke Zettlemoyer, Nicola Can-\ncedda, and Thomas Scialom. 2023. Toolformer: Lan-\nguage Models Can Teach Themselves to Use Tools.\nArXiv, abs/2302.04761.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023a. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhugging face. In Advances in Neural Information\nProcessing Systems.\nYongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang,\nKan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li,\nand Yueting Zhuang. 2023b. Taskbench: Benchmark-\ning large language models for task automation. arXiv\npreprint arXiv:2311.18760.\nNoah Shinn, Beck Labash, and Ashwin Gopinath. 2023.\nReflexion: an autonomous agent with dynamic mem-\nory and self-reflection. ArXiv, abs/2303.11366.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aur’elien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023a. Llama: Open\nand Efficient Foundation Language Models. ArXiv,\nabs/2302.13971.\nHugo Touvron, Louis Martin, Kevin R. Stone, Peter\nAlbert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Daniel M. Bikel, Lukas Blecher, Cris-\ntian Cantón Ferrer, Moya Chen, Guillem Cucurull,\nDavid Esiobu, Jude Fernandes, Jeremy Fu, Wenyin\nFu, Brian Fuller, Cynthia Gao, Vedanuj Goswami,\nNaman Goyal, Anthony S. Hartshorn, Saghar Hos-\nseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel M. Kloumann, A. V.\nKorenev, Punit Singh Koura, Marie-Anne Lachaux,\nThibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai\nLu, Yuning Mao, Xavier Martinet, Todor Mihaylov,\nPushkar Mishra, Igor Molybog, Yixin Nie, Andrew\nPoulton, Jeremy Reizenstein, Rashi Rungta, Kalyan\nSaladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, R. Subramanian, Xia Tan, Binh Tang, Ross\nTaylor, Adina Williams, Jian Xiang Kuan, Puxin\nXu, Zhengxu Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aure-\nlien Rodriguez, Robert Stojnic, Sergey Edunov, and\nThomas Scialom. 2023b. Llama 2: Open foundation\nand fine-tuned chat models. ArXiv, abs/2307.09288.\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-\ndlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and An-\nima Anandkumar. 2023a. Voyager: An open-ended\nembodied agent with large language models. arXiv\npreprint arXiv:2305.16291.\nShenzhi Wang, Chang Liu, Zilong Zheng, Siyuan\nQi,\nShuo Chen,\nQisen Yang,\nAndrew Zhao,\nChaofei Wang, Shiji Song, and Gao Huang. 2023b.\nAvalon’s game of thoughts: Battle against decep-\ntion through recursive contemplation. arXiv preprint\narXiv:2310.01320.\nXinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai,\nHaotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P\nXing, and Zhiting Hu. 2023c.\nPromptagent:\nStrategic planning with language models enables\nexpert-level prompt optimization.\narXiv preprint\narXiv:2310.16427.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Huai hsin Chi, and Denny Zhou. 2022. Self-\nconsistency improves chain of thought reasoning in\nlanguage models. ArXiv, abs/2203.11171.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022a. Emer-\ngent abilities of large language models.\nCoRR,\nabs/2206.07682.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of Thought Prompt-\ning Elicits Reasoning in Large Language Models. In\nConference on Neural Information Processing Sys-\ntems (NeurIPS).\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023a.\nVisual chatgpt:\nTalking, drawing and editing\nwith visual foundation models.\narXiv preprint\narXiv:2303.04671.\nQingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu,\nShaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang,\nXiaoyun Zhang, and Chi Wang. 2023b. Autogen:\nEnabling next-gen llm applications via multi-agent\nconversation framework. ArXiv, abs/2308.08155.\nZhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen\nDing, Boyang Hong, Ming Zhang, Junzhe Wang,\nSenjie Jin, Enyu Zhou, et al. 2023. The rise and\npotential of large language model based agents: A\nsurvey. arXiv preprint arXiv:2309.07864.\nYuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xi-\naolong Wang, Weidong Liu, and Yang Liu. 2023a.\nExploring large language models for communica-\ntion games: An empirical study on werewolf. arXiv\npreprint arXiv:2309.04658.\nZelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu.\n2023b. Language agents with reinforcement learn-\ning for strategic play in the werewolf game. arXiv\npreprint arXiv:2310.18940.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu,\nQuoc V Le, Denny Zhou, and Xinyun Chen. 2023.\nLarge language models as optimizers. arXiv preprint\narXiv:2309.03409.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L. Griffiths,\nYuan Cao,\nand Karthik\nNarasimhan. 2023.\nTree of thoughts: Deliberate\nproblem solving with large language models. ArXiv,\nabs/2305.10601.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. ArXiv, abs/2210.03629.\nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and\nFereshte Khani. 2023. Prompt engineering a prompt\nengineer. arXiv preprint arXiv:2311.05661.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma,\nYufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan\nLiu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023.\nGlm-130b: An Open Bilingual Pre-trained Model.\nICLR 2023 poster.\nDaochen Zha, Kwei-Herng Lai, Yuanpu Cao, Songyi\nHuang, Ruzhe Wei, Junyu Guo, and Xia Hu. 2019.\nRlcard: A toolkit for reinforcement learning in card\ngames. arXiv preprint arXiv:1910.04376.\nDaochen Zha, Jingru Xie, Wenye Ma, Sheng Zhang, Xi-\nangru Lian, Xia Hu, and Ji Liu. 2021. Douzero: Mas-\ntering doudizhu with self-play deep reinforcement\nlearning. In international conference on machine\nlearning, pages 12333–12344. PMLR.\nCeyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang,\nGuanghe Li, Yihang Sun, Cheng Zhang, Zhaowei\nZhang, Anji Liu, Song-Chun Zhu, et al. 2023a. Proa-\ngent: Building proactive cooperative ai with large\nlanguage models. arXiv preprint arXiv:2308.11339.\nChi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin\nChen, Zebiao Huang, Bin Fu, and Gang Yu. 2023b.\nAppagent: Multimodal agents as smartphone users.\nDanyang Zhang, Lu Chen, Situo Zhang, Hongshen\nXu, Zihan Zhao, and Kai Yu. 2023c.\nLarge lan-\nguage model is semi-parametric reinforcement learn-\ning agent. arXiv preprint arXiv:2306.07929.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022a. Opt: Open\nPre-trained Transformer Language Models. ArXiv,\nabs/2205.01068.\nWenqi Zhang, Yongliang Shen, Weiming Lu, and Yuet-\ning Zhuang. 2023d.\nData-copilot: Bridging bil-\nlions of data and humans with autonomous workflow.\narXiv preprint arXiv:2306.07209.\nWenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying\nPeng, Jun Wang, Yueting Zhuang, and Weiming\nLu. 2024. Self-contrast: Better reflection through\ninconsistent solving perspectives.\narXiv preprint\narXiv:2401.02009.\nWenqi Zhang, Kai Zhao, Peng Li, Xiao Zhu, Yongliang\nShen, Yanna Ma, Yingfeng Chen, and Weiming Lu.\n2022b. A closed-loop perception, decision-making\nand reasoning mechanism for human-like naviga-\ntion. In Proceedings of the Thirty-First International\nJoint Conference on Artificial Intelligence, IJCAI-22,\npages 4717–4724. International Joint Conferences on\nArtificial Intelligence Organization. Main Track.\nWenqi Zhang, Kai Zhao, Peng Li, Xiaochun Zhu, Fap-\ning Ye, Wei Jiang, Huiqiao Fu, and Tao Wang. 2021.\nLearning to navigate in a vuca environment: Hierar-\nchical multi-expert approach. 2021 IEEE/RSJ Inter-\nnational Conference on Intelligent Robots and Sys-\ntems (IROS), pages 9254–9261.\nAndrew Zhao, Daniel Huang, Quentin Xu, Matthieu\nLin, Yong-Jin Liu, and Gao Huang. 2023. Expel:\nLlm agents are experiential learners. arXiv preprint\narXiv:2308.10144.\nDenny Zhou, Nathanael Scharli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Huai hsin\nChi. 2022a. Least-to-most prompting enables com-\nplex reasoning in large language models.\nArXiv,\nabs/2205.10625.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2022b. Large language models are human-level\nprompt engineers. arXiv preprint arXiv:2211.01910.\nAppendix\nA\nExperiment Details\nA.1\nLLMs\nWe employ the GPT-3.5-Tubor-0613, GPT4-0613,\nLlama2-Chat-70B (Touvron et al., 2023b) and\nQwen-72B (Bai et al., 2023) to construct our agent.\nTo make a fair comparison, we uniformly set the\ntemperature to 1.0 for all experiments. For each\ntest, we repeat it five times and report the average.\nA.2\nBaselines\nWe compare Agent-Pro with many common LLM-\nbased agent strategies, including Vanilla LLM, Re-\nAct (Yao et al., 2022), Reflexion (Shinn et al.,\n2023), Self-Consistency (Wang et al., 2022), Multi-\nagent Debate (Du et al., 2023).\nA.3\nSetups For Blackjack\nIn Blackjack, players must decide to hit or stand\nbased on their own hand, the dealer’s face-up\ncard.\nWe simplify our approach by not incor-\nporating Verification and DFS-based Policy\nEvolution since Blackjack is simple with a small\nstate space. We collect 50 failed games for pol-\nicy learning. We evaluate Agent-Pro and baselines\non newly sampled 900 games. All prompts are\npresented in Appendices E.1 and E.2.\nA.4\nDetailed Setup For Texas Hold’em\nThe whole learning process is as follows: We first\nrandomly allocate 500 game hands for 4 players\nand then select these failed game hands in which\nthe agent loses a significant number of chips. We\ncollect a total of 167 challenging game hands as\nthe \"training\" set and 20 game hands as a devel-\nopment set for policy evaluation. Then Agent-\nPro is instructed to conduct a learning process on\nthese \"training\" instances, containing three phases:\nExploration-Reflection-Evolution.\nExploration (§ 3.1): It randomly selects a game\nfrom \"training\" set to play with the latest policy\nand the belief-aware decision-making process.\nReflection (§ 3.2): If Agent-Pro loses to its op-\nponents, it immediately performs Policy-Level\nReflection on this game and then updates to the\nnew policy after passing Verification.\nEvolution (§ 3.3): We first sample 2 game hands\nfrom the development set to evaluate the new policy\nand calculate its ∆ with B=8 for DFS. The process\nends when the policy cannot be further improved,\nor all samples have been explored.\nB\nIntroduction of Two Games\nWe selected the following two games as interactive\nenvironments.\nB.1\nBlackjack\nBlackjack 2 , also known as 21, is a popular card\ngame that involves a dealer and a player. Players\nmust decide whether to hit or stand based on\ntheir own hand, the dealer’s face-up card, and the\ndealer’s one hidden card. The objective is to beat\nthe dealer without exceeding 21 points. For this\ngame, we observe whether LLM-based agents can\nmake rational decisions under uncertain scenarios.\nB.2\nLimit Texas Hold’em\nLimit Texas Hold’em is a popular card game 3\n. The game commences with each player being\ndealt two private cards, which belong exclusively\nto the player and remain hidden from the others.\nFive community cards are then dealt face-up in a\nseries of stages: a three-card Flop, followed by\na single card on the Turn and another single card\non the River. The player can choose from four\nactions: Fold, Check, Call, Raise. They aim\nto construct the best five-card poker hand possible\nusing any combination of their private cards and\ncommunity cards.\nB.3\nThe Challenging of two Games\nTwo games can evaluate the agent’s capabilities\nfrom multiple dimensions:\nHandling Uncertainty in Environment: Both\ngames are imperfect information games and be\nused to assess the performance of LLM-based\nagents in face of uncertainty. For instance, in the\ngame of Blackjack, the card hidden by the dealer\nintroduces significant uncertainty. The agent needs\nto assess the risk and make decisions accordingly.\nAddressing Dynamic Environment: Most real-\nworld scenarios are dynamic rather than static.\nThis requires the agent to capture environmental\nchanges and adapt to them. For instance, in Texas\nHold’em, in addition to one’s own hand, the actions\nof opponents also greatly influence the agent’s deci-\nsions. We evaluate whether Agent-Pro can handle\nsuch dynamic environments.\n2https://en.wikipedia.org/wiki/Blackjack\n3https://www.winamax.fr/en/poker-school_rules_\nlimit-texas-hold--em\nGame\n#Train #Dev #Test\nBlackjack\n50\n-\n900\nTexas Hold’em 167\n20\n1600\nTable B1: The sample sizes of the Training, Develop-\nment, and Testing sets for the two games, where the\ntraining set is utilized for exploration and reflection, the\ndevelopment set for policy evaluation, and the test set\nfor assessing the effectiveness of all methods.\nStrategy\nDQN DMC GPT-3.5 Agent\nVanilla LLM\n-2.2\n1.7\n-0.9\n1.4\n- w/ 3 win shots\n-2.4\n2.6\n-1.1\n0.9 ↓0.5\n- w/ 3 lose shots\n-2.6\n1.8\n-1.2\n2.0 ↑+0.6\n- w/ 3 win+3 lose shots -1.9\n1.9\n-1.5\n1.5 ↑+0.1\nAgent-Pro\n-3.9\n1.1\n-1.5\n4.3 ↑2.9\n- w/o Belief\n-3.3\n1.5\n-0.7\n2.5 1.1\n- w/o Learning\n-3\n1.5\n-1.2\n2.7↑1.3\nTable B2: Up: We compare the performance of Agent-\nPro with Vanilla LLM with few-shot demonstrations.\nEach demonstration contains a complete trajectory and\nfinal results. Down: We ablate the dynamic belief or\nlearning process from Agent-Pro and evaluate its results.\nAddressing Complexity: Blackjack is relatively\nsimple, with an InfoSet number of 1000. In con-\ntrast, multi-player Limit Texas Hold’em is very\ncomplex, with its Infoset number reaching up to\n1014 (Zha et al., 2019). We analyze Agent-Pro’s\nlearning capacity in such intricate scenarios.\nC\nComplementary Experiments\nTo better investigate the performance of Agent-Pro,\nwe design some ablation experiments.\nC.1\nWhether Few-shot Learning Can Handle\nSuch Complex Interaction\nFirst, we compare Agent-Pro with Few-shot Agent:\nwe randomly select some winning and losing game\ntrajectories and their final results as demonstra-\ntions in the prompt. Then we evaluate them on\nthe test set. As shown in Table B2, we observe\nthat winning trajectories seem to have no effect (-\n0.5), while losing trajectories can slightly improve\nthe final performance (+0.6). This phenomenon\nis quite intriguing, indicating that these winning\ndemonstrations do not seem to enhance the agent’s\ndecision-making ability in such complex scenarios.\nThis may be because these winning demonstrations\nare relatively simple for the vanilla agent, which\nis already capable of winning these games, so in-\ncluding them in the prompt does not provide it with\nany additional insights. In contrast, those failing\nStrategy\nDQN\nDMC\nHuman\nWin-rate ↑(%)\n40.0\n41.9\n37.1\nTable C3: We evaluate two RL-based agents, and the\nhuman player on the same 900 games.\ntrajectories instead promote agents to reflect and\nadjust their behaviors, improving the final results.\nC.2\nPolicy-Level Reflection without Belief\nWe ablate the dynamic belief module, i.e., conduct-\ning policy-level reflection directly on the action\nsequences, state sequences, and final results. Then\nit also summarizes prompt instructions for policy\nupdates. As shown in Table B2, we observe that\nafter removing, Agent-Pro’s chips drop from 4.3\nto 2.5, although there is still an improvement of\n+1.1 compared to Vanilla LLM. Upon closely ex-\namining the Behavioral Guideline and World\nModeling it generated, we observe that compared\nto Agent-Pro, its content is rather vague and ver-\nbose, lacking in specificity and conciseness. These\nresults indicate that dynamic belief can enhance\nboth decision-making and policy-level reflection\ncapabilities. Below, we provide two similar in-\nstructions, one from Agent-Pro and the other from\nAgent-Pro without Belief.\nA Learned Instruction From Agent-Pro\nWhen holding a weak hand , adopting a\nconservative approach and waiting for\nthe flop can be wise. This strategy\nallows for the possibility of the\ncommunity cards improving your hand.\nHowever , if the flop doesn ’t enhance\nyour hand ’s strength , folding to\nminimize losses becomes the prudent\nchoice. During this period , maintaining\na low profile and avoiding aggressive\nactions like raising is advisable.\nA Similar Instruction From Agent-Pro w/o Belief\nIn situations where the strength of one ’\ns hand isn ’t exactly what one might call\nrobust or particularly promising , it\ncould potentially be somewhat beneficial\n, or at least not entirely\ndisadvantageous , to entertain the notion\nof adopting a stance that leans more\ntowards the side of caution .... the\nunveiling of the community cards .....\nC.3\nDetailed Analysis Experiments\nAs shown in Table C4, we manually select 20 chal-\nlenging sets of hands, each with a significant differ-\nence in the hands of four players, and then assess\nthe performance of Agent-Pro and Vanilla LLM.\nHand\nStrength\nHand\nCommunity Cards\nDQN\nDMC\nGPT3.5\nAgent\nFlop\nTurn\nRiver\nStrong\nH5\nS4\nD6\nDQ\nS9\nC2\nCA\nHA\nH7\nCQ\nCT\nH3\nS3\nDK\nS5\nHK\nD2\nS7\nH4\nDA\nHA\nDQ\nD9\nDT\nC6\nD7\nD3\nC8\nHA\nHT\nH5\nS9\nDQ\nDJ\nD4\nCK\nH7\nCQ\nC5\nSA\nH9\nC6\nS8\nS3\nSJ\nHT\nCK\nD7\nC5\nC4\nC3\nD2\nModerate\nHJ\nCQ\nS7\nSA\nC3\nD5\nH3\nCA\nC2\nH9\nS3\nD9\nC8\nH5\nC5\nDJ\nH9\nS6\nD2\nHK\nH2\nDA\nDK\nSK\nC3\nH8\nH5\nD6\nDT\nCT\nC9\nC4\nS9\nC8\nS6\nS7\nHT\nHQ\nHJ\nC3\nD5\nH3\nCA\nHJ\nCQ\nS7\nSA\nC2\nH9\nS3\nD9\nC8\nWeak\nS3\nSJ\nHT\nCK\nSA\nH9\nC6\nS8\nD7\nC5\nC4\nC3\nD2\nS3\nC8\nH7\nS2\nDA\nCA\nD7\nD5\nH8\nHJ\nSJ\nST\nD9\nDK\nD5\nSJ\nC6\nD9\nS3\nH2\nC8\nCA\nCQ\nCT\nD8\nC2\nH5\nH8\nHA\nS9\nC6\nD9\nC5\nH3\nD3\nC3\nHQ\nS3\nDA\nHA\nS9\nC6\nD9\nC5\nH3\nH5\nH8\nD3\nC3\nHQ\nS3\nDA\nH2\nC8\nDK\nD5\nSJ\nC6\nD9\nS3\nCA\nCQ\nCT\nD8\nC2\nH7\nS2\nDA\nCA\nD7\nD5\nS3\nC8\nH8\nHJ\nSJ\nST\nD9\nHA\nHT\nH5\nS9\nDQ\nDJ\nD3\nC8\nD4\nCK\nH7\nCQ\nC5\nDA\nHA\nDK\nS5\nHK\nD2\nS7\nH4\nDQ\nD9\nDT\nC6\nD7\nDT\nCT\nC9\nC4\nS9\nC8\nH5\nD6\nS6\nS7\nHT\nHQ\nHJ\nD6\nDQ\nS9\nC2\nCA\nHA\nH5\nS4\nH7\nCQ\nCT\nH3\nS3\nHK\nH2\nH5\nC5\nDJ\nH9\nS6\nD2\nDA\nDK\nSK\nC3\nH8\nTable C4: Each card consists of a letter representing the suit (\"S\", \"H\", \"D\", \"C\") and a number representing size\n(\"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"T\", \"J\", \"Q\", \"K\", \"A\"). Among them, \"S\" represents Spade, \"H\" represents\nHeart, \"D\" represents Diamond, \"C\" represents Club, and \"T\" represents 10.\nD\nRelated Works\nD.1\nLLM-based Application\nLarge language models (LLMs), pre-trained on\nextensive corpora, have demonstrated robust lan-\nguage comprehension and reasoning capabilities.\nBenefiting from this, researchers have designed a\nplethora of agent systems built upon LLM, achiev-\ning promising results (Xi et al., 2023). Schick\net al. (2023); Wu et al. (2023a); Shen et al.\n(2023a) have harnessed the planning capabili-\nties of LLMs to invoke specialized models and\ntools for task-solving. Some open-source projects,\ne.g., AutoGPT4, gentGPT5, BabyAGI6, BMTools7,\nChatArena8, LangChain9 have developed an LLM-\nbased assistant.\nFurther, (Qin et al., 2023a,b;\nShen et al., 2023b) have empowered LLM to au-\ntonomously invoke the APIs for daily life scenarios.\nBesides, leveraging the code generation capabilities\nof LLMs, researchers have designed multi-agent\ncollaborative systems (Li et al., 2023a; Chen et al.,\n2023b; Hong et al., 2023; Wu et al., 2023b) for\ncomplex tasks, such as software development. Un-\n4https://github.com/Significant-Gravitas/\nAuto-GPT\n5https://github.com/reworkd/AgentGPT\n6https://github.com/yoheinakajima/babyagi\n7https://github.com/OpenBMB/BMTools\n8https://github.com/chatarena/chatarena\n9https://github.com/hwchase17/langchain\nlike these task-specific agents that require manually\nspecified behavior protocols, our agents can under-\nstand tasks through interaction with the environ-\nment. It can optimize its behavioral strategy from\npast experiences, accomplishing the task more ef-\nfectively.\nD.2\nLLMs For Interactive Scenarios\nBeyond these applications, LLMs have also been\nutilized in interactive settings (Durante et al., 2024).\nReAct (Yao et al., 2022) integrates reasoning, ac-\ntion, and observation into the problem-solving pro-\ncess. Park et al. (2023) introduces generative agents\nthat can simulate human behavior. Fu et al. (2023)\nshow LLMs can improve each other in a nego-\ntiation scenario. Zhao et al. (2023); Chen et al.\n(2023a) propose an experiential learner gathering\nexperiences and extracting from a collection of\ntraining tasks. Fan et al. (2023) explored the capa-\nbility of LLMs to make rational decisions in game-\ntheoretic scenarios. Besides, some studies have\ndesigned sophisticated LLM-based agents for large-\nscale games, including StarCraft (Ma et al., 2023),\nMinecraft (Wang et al., 2023a; Gong et al., 2023),\nLeduc Hold’em (Guo et al., 2023a), strategy-based\ngaming (Bakhtin et al., 2022; Xu et al., 2023a;\nWang et al., 2023b; Xu et al., 2023b; Lorè and\nHeydari, 2023).\nD.3\nImproving the Quality of LLM Responses\nEnhancing the quality of responses from LLMs has\ngarnered significant attention within the commu-\nnity. We categorize the strategies into two method-\nologies: 1. Developing superior reasoning architec-\ntures. First, Chain-of-Thoughts (Wei et al., 2022b)\nelicits LLM’s reasoning ability. Works as Least-to-\nMost (Zhou et al., 2022a), Tree of Thoughts (Yao\net al., 2023), Graph of Thoughts (Besta et al.,\n2023) have explored diverse problem-solving pro-\ncedures and reasoning architectures, significantly\nenhancing the performance of LLM-based agents.\n2.\nRefining the output of LLMs.\nResearchers\nhave proposed post-hoc prompting strategies to\niteratively refine the outputs of LLMs (Pan et al.,\n2023), including Reflexion (Shinn et al., 2023),\nSelf-Refine (Madaan et al., 2023; Paul et al., 2023;\nHuang et al., 2022), Self-Contrast (Zhang et al.,\n2024), etc. However, these self-correction strate-\ngies are performed at the action-level, whereas our\nagent operates at the policy-level, making it more\nsuited for interactive environments.\nD.4\nAutomatic Prompt Optimization\nIn addition to optimizing the outputs of LLMs,\nmany researchers also enhance the performance\nof LLMs by searching for a more effective\nprompt (Zhou et al., 2022b; Hsieh et al., 2023; Guo\net al., 2023b; Wang et al., 2023c). APO (Pryzant\net al., 2023) emulates the process of gradient opti-\nmization. It calculates the \"gradients\" of the cur-\nrent prompt by analyzing the instances that are\ninaccurately predicted by this prompt. Further-\nmore, Yang et al. (2023) and Ye et al. (2023) eval-\nuate each candidate prompt using the training set\nand iteratively optimize the prompts based on the\nevaluation results.\nCheng et al. (2023) train a\nSequence-to-Sequence model to translate an im-\nperfect prompt into a better one. Brooks et al.\n(2023) and Zhang et al. (2023c) combine reinforce-\nment learning with prompt updating, demonstrating\npromising results. We extend these prompt opti-\nmization techniques to more complex interactive\ngaming environments, learning a robust behavioral\nstrategy through policy-level reflection and search.\nFurthermore, our agent must constantly consider\nchanges in the environment and the styles of oppo-\nnents, thereby dynamically adjusting the content of\nthe prompts.\nE\nDetailed Prompts\nWe provide detailed prompt designs for two\ngames, including baselines and Agent-Pro in Ap-\npendix E.1, E.2, E.3 and E.4.\nF\nCase Study\nAs shown in Figure F3, F4, F1, F2 we provide four\ncases for Blackjack. We visualize the difference\nin their solving steps between Agent-Pro and Re-\nAct when using Qwen-72B. Besides, we also pro-\nvide four cases for Limited Texas Hold’em in Ap-\npendix F.2. These cases demonstrate that Agent-\nPro, after learning, has significantly improved in\nunderstanding task rules, mastering techniques, and\ndealing with uncertain environments.\nE.1\nBaseline’s Prompts For Blackjack\nGame Rule:\n### Game Rules\n1. Please try to get your card total to as close to 21 as possible , without going\nover , and still having a higher total than the dealer.\n2. If anyone ’s point total exceeds 21, he or she loses the game.\n3. You can only choose one of the following two actions: {\" Stand\", \"Hit \"}. If you\nchoose to Stand , you will stop taking cards and wait for the dealer to finish. If\nyou choose to Hit , you can continue to take a card , but there is also the risk of\nlosing the game over 21 points.\n4. After all players have completed their hands , the dealer reveals their hidden\ncard. Dealers must hit until their cards total 17 or higher.\nGame Information:\nThe dealer ’s face -up card is {Dealer-Card}. The dealer has another\nhidden card. You don ’t know what it is. Your current cards are {Player-Card}.\n---------- Prompt For Vanilla LLM --------------\nYou are a player in blackjack. Please beat the dealer and win the game.\n### {Game Rules}\n### {Game Information}\n### Please output your action in following format: ###My action is {Your action},\nwithout any other text.\n------------- Prompt For Radical LLM --------------\nYou are an aggressive player of blackjack who likes to take risks to earn high\nreturns. Please beat the dealer and win the game.\n### {Game Rules}\n### {Game Information}\n### Please output your action in following format: ###My action is {Your action},\nwithout any other text.\n------------- Prompt For ReAct --------------\nYou are a player in blackjack. Please beat the dealer and win the game.\n### {Game Rules}\n### {Game Information}\n### Please first think and reason about the current hand and then generate your\naction as follows: ###My thought is {Your Thought }. My action is {Your action }.\n------------- Prompt For Reflexion --------------\nYou are a player in blackjack. Please beat the dealer and win the game.\n### {Game Rules}\n### {Game Information}\n### Please first think about the current hand and then generate your action in\nfollowing format: ###My thought is {Your thought }. My action is {Your action }.\nAssistant: {LLM Response}. My action is {LLM Response}\n### Please carefully check the response you just output , and then refine your answer\n. The final output is also in following format: ###My thought is {Your thought }. My\naction is {Your action }.\nE.2\nAgent-Pro’s Prompt For Blackjack\nGame Rule:\n### Game Rules\n1. Please try to get your card total to as close to 21 as possible , without going\nover , and still having a higher total than the dealer.\n2. If anyone ’s point total exceeds 21, he or she loses the game.\n3. You can only choose one of the following two actions: {\" Stand\", \"Hit \"}. If you\nchoose to Stand , you will stop taking cards and wait for the dealer to finish. If\nyou choose to Hit , you can continue to take a card , but there is also the risk of\nlosing the game over 21 points.\n4. After all players have completed their hands , the dealer reveals their hidden\ncard. Dealers must hit until their cards total 17 or higher.\nGame Information:\nThe dealer ’s face -up card is {Dealer-Card}. The dealer has another\nhidden card. You don ’t know what it is. Your current cards are {Player-Card}.\n------------- Prompt For Agent-Pro --------\nYou are a player in blackjack. Please beat the dealer and win the game.\n### {Game Rules}\n### {Game Information}\n### {Behavioral Guideline: Goal, Strategy, Demonstration}\n### {World Modeling: Rule Description}\n### Please read the behavoiral guideline and world modeling carefully. Then you\nshould analyze your own cards and your strategies in Self -belief and then analyze\nthe dealer cards in World -belief. Lastly , please select your action from {\" Stand\", \"\nHit \"}.\n### Output Format: Self -Belief is {Belief about youself }. World -Belief is {Belief\nabout the dealer }. My action is {Your action }. Please output in the given format.\n------------- Prompt For Policy-Level Reflection --------------\n### {Game Rules}\n### Game Record: {Game Record, Belief Sequences, Final Result}\n### You are a seasoned blackjack expert , and you need to carefully reflect on the\nfollowing record of this losing game:\nCorrectness: Whether its beliefs about yourself , the game , and the dealer align with\nthe final results.\nConsistency: Whether each belief and action is self -contradictory.\nReasons: Reflect on why you lost to your dealer , which beliefs and actions are\nproblematic , and what the underlying reasons are.\n### Output Format: I analyze this game as follows: {Your analysis about the game and\nbelief }.\n------------- Prompt For Generating Behavioral Guideline and World Modeling --------------\n### Game Record: {Game Record, Belief Sequences, Final Result}\n### Policy -Level Reflection: {Reflection}\nFollowing the previous rigorous analysis , you should distill and articulate a set of\nBehavioral Guidelines and World Modeling. The Behavioral Guideline is about what\nyou consider to be a more reasonable and effective behavioral strategy and\nsuggestions. World Modeling is about the description of the game and the dealer.\nHere are some suggestions for you:\nBehavioral Guideline\n1-Goal: Please summarize the detailed goal based on your reflection ...\n2-Strategy: What kind of strategy can lead you to win in similar games ...\n3-Demonstration: Can this game be considered a typical example to be preserved for\nfuture reference ...\nWorld Modeling\n1-Rule-Description: Based on the recent reflection , describe any game rules or details\nthat are easy to overlook ...\nE.3\nBaseline’s Prompts For Limited Texas Hold’em\nGame Settings:\n1- You are playing the Limit Texas poker game. In this game , there are 4 players\nfrom 0 to 3, and your role is player 3.\n2- The number of chips every player has is infinite.\n3- You just need to win new chips in the competition as much as possible.\n4- The actions you can choose are [’call ’, ’raise ’, ’fold ’, ’check ’]\nGame Information:\nYour current hands are {Private Cards}.\nThe current stage: {Stage}. Public cards are {Public Cards}.\nNumber of chips all players have invested are {Inveseted Chip List}.\nAvailable actions you can choose are {Available Actions}.\nPrevious actions of all players are: {Preflop: Actions Sequences, Flop: Actions ...}.\n---------- Prompt For Vanilla LLM --------------\nYou are a player in Limited Texas Hold ’em. Beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### Output your action in following format: {\" action \": \" \"} without any other text.\n---------- Prompt For Aggressive LLM --------------\nYou are an aggressive player of limited Texas Hold ’em who likes to take risks to\nearn high returns. Please beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### Output your action in following format: {\" action \": \" \"} without any other text.\n---------- Prompt For Conservative LLM --------------\nYou are a conservative player of limited Texas Hold ’em who is risk averse and\nprefers more certainty. Please beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### Output your action in following format: {\" action \": \" \"} without any other text.\n---------- Prompt For ReAct --------------\nYou are a player in Limited Texas Hold ’em. Beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### Please first think and reason about the current state and then generate your\naction as follows: ###My thought is {Your Thought}, and my action is {\" action \": \" \"}\n---------- Prompt For Reflection --------------\nYou are a player in Limited Texas Hold ’em. Beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### Please first think and reason about the current state and then generate your\naction as follows: ###My thought is {Your Thought}, and my action is {\" action \": \"\n\"}.\nAssistant: {LLM Response}.\n### Please carefully check the thought and the action you just output , and then\nrefine your answer. The final output is also in the same format: ###My revised\nthought is {Your Thought }. My revised action is {\" action \": \" \"}.\nE.4\nAgent-Pro’s Prompt For Limited Texas Hold’em\nGame Settings:\n1- You are playing the Limit Texas poker game. In this game , there are 4 players\nfrom 0 to 3, and your role is player 3.\n2- The number of chips every player has is infinite.\n3- You just need to win new chips in the competition as much as possible.\n4- The actions you can choose are [’call ’, ’raise ’, ’fold ’, ’check ’]\nGame Information:\nYour current hands are {Private Cards}.\nThe current stage: {Stage}. Public cards are {Public Cards}.\nNumber of chips all players have invested are {Inveseted Chip List}.\nAvailable actions you can choose are {Available Actions}.\nPrevious actions of all players are: {Preflop: Actions Sequences, Flop: Actions ...}.\n------------- Prompt For Agent-Pro --------\nYou are a player in Limited Texas Hold ’em. Beat your opponents and win the game.\n### {Game Rules}\n### {Game Information}\n### {Behavioral Guideline: Goal, Strategy, Demonstration}\n### {World Modeling: Rule Description, Opponents Description}\n### Please read the behavoiral guideline and world modeling carefully. Then ,\nfollowing their instructions , you should analyze your own cards and your strategies\nin Self -belief and then analyze and reason about your opponents in World -belief.\nLastly , output your action.\n### Output Format: Self -Belief is {belief about youself }. World -Belief is {belief\nabout the all opponents }. My action is {\" action \": \" \"}. Please output in the given\nformat.\n------------- Prompt For Policy-Level Reflection --------------\n### {Game Rules}\n### {Game Information}\n### Game Record: {Game Record, Belief Sequences, Final Result}\n### You are a seasoned Limited Texas Hold ’em expert , and you need to carefully\nreflect on the following record of this losing game:\nCorrectness: Whether its beliefs about yourself , the game , and the opponents align\nwith the final results.\nConsistency: Whether each belief and action is self -contradictory.\nRationality: Whether the beliefs accurately reflect the underlying intentions behind\nthe opponents.\nReasons: Reflect on why you lost to your opponents , which beliefs and actions are\nproblematic , and what the underlying reasons are.\n### Output Format: I analyze this game as follows: {Your analysis about the game and\nbelief }.\n------------- Prompt For Generating Behavioral Guideline and World Modeling --------------\n### Game Record: {Game Record, Belief Sequences, Final Result}\n### Policy -Level Reflection: {Reflection}\nFollowing the previous rigorous analysis , you should distill and articulate a set of\nBehavioral Guidelines and World Modeling. The Behavioral Guideline is about what\nyou consider to be a more reasonable and effective behavioral strategy and\nsuggestions. World Modeling is about the description of the game and all opponents.\nHere are some suggestions for you:\nBehavioral Guideline\n1-Goal: Please summarize the detailed goal based on your reflection ...\n2-Strategy: What kind of strategy can lead you to win in similar games ...\n3-Demonstration: Can this game be considered a typical example to be preserved for\nfuture reference ...\nWorld Modeling\n1-Rule-Description: Based on the recent reflection , describe any game rules or details\nthat are easy to overlook ...\n2-Opponent-Modeling: Based on each opponent ’s action and the real hands shown at the\nend of the game , what do you think their style and strategy are ...\nF.1\nCases For Blackjack\nAgent-Pro is Better at Handling the Uncertainties of the Environment.\nState\nReAct Vs. Dealer\nAgent-Pro Vs. Dealer\nInitial Card\nThe dealer’s face-up card is Spade 4.\nYour current cards are Heart 5, Club J.\nThought\nMy thought is I currently have a total of 15 points\nwith my two cards, which is not a very strong\nhand. The dealer’s up-card is a 4, which is a rel-\natively low value card. Based on basic strategy\nin blackjack, I should hit if my total is less than\n17, so I should take a hit to improve my hand.\nSelf-Belief is I currently have a total of 15 points, with\na Heart 5 and a Club J. I am slightly below the ideal\ntotal of 21, but I am also less likely to go bust if I draw\nanother card. I think my best chance of winning is to\nstand and hope that the dealer goes bust or ends up with\na lower total.\nWorld-Belief is The dealer currently has a Spade 4 show-\ning, which means their total is likely to be between 4 and\n14 points. Since the dealer must draw more cards if their\ntotal is below 17, they are at a higher risk of going bust.\nHowever, if the dealer has a strong hidden card, such\nas a face card or an ace, they could end up with a high\ntotal and win the game. Given this uncertainty, standing\nseems like a safer bet.\nAction\nMy action is Hit.\nMy action is Stand.\nNext Round\nThe final hand is Dealer: Spade 4, Diamond 4.\nYour: Heart 5, Club J, Club 8 (23 point)\nThe final hand is Dealer: Spade 4, Diamond 4, Club 8,\nSpade 7 (23 point).\nYour: Heart 5, Club J (15 point).\nResult\nLose\nWin\nFigure F1: Some cases for Blackjack by ReAct and Agent-Pro using Qwen-72B. ReAct behavioral strategies are\ntoo aggressive. However, Agent-Pro guesses the dealer’s next move and makes a reasoned decision.\nReAct is Aggressive, While Agent-Pro is More Rational.\nState\nReAct Vs. Dealer\nAgent-Pro Vs. Dealer\nInitial Card\nThe dealer’s face-up card is Spade 8.\nYour current cards are Heart 10, Club 8.\nThought\nMy thought is The sum of my cards is currently\n18. I am in a fairly safe position, not too likely to\nexceed 21 by hitting. However, since the dealer’s\nvisible card is an 8, there is a good chance that\nthe sum of the dealer’s cards will also be around\n18. In order to increase my chances of winning,\nI need to aim for a higher total.\nSelf-Belief is I currently have a total of 18 points, which\nis a relatively strong hand. I am confident that I can beat\nthe dealer with this hand. However, I should be cautious\nnot to exceed 21 points by hitting and risking busting.\nWorld-Belief is The dealer’s visible card is an 8, which\nis a moderate hand. It is likely that the dealer may still\nneed to hit to reach a total of 17 or higher. Therefore,\nI should be careful not to risk busting by hitting and\nexceeding 21 points.\nAction\nMy action is Hit.\nMy action is Stand.\nNext Round\nThe final hand is\nDealer: Spade 8, Heart 4. (12 points)\nYour: Heart 10, Club 8, Diamond J (28 points)\nThe final hand is\nDealer: Spade 8, Heart 4, Diamond J (22 points).\nYour: Heart 10, Club 8 (18 points).\nResult\nLose\nWin\nFigure F2: Some cases for Blackjack by ReAct and Agent-Pro using Qwen-72B. ReAct doesn’t consider the risk of\nexceeding 21 points.\nAgent-Pro Can Better Understand the Rules of the Game.\nState\nReAct Vs. Dealer\nAgent-Pro Vs. Dealer\nInitial Card\nThe dealer’s face-up card is Club 6.\nYour current cards are Club A, Diamond 4.\nThought\nMy thought is that I currently have a total of 15\npoints (Ace counts as 1) and the dealer has 6\npoints showing. To win the game, I need to get\nas close to 21 as possible without going over.\nWith the dealer’s current visible card being low,\nthey may have to hit again to reach 17 or higher,\nincreasing their chances of busting. Therefore,\nI should play conservatively and stand with my\ncurrent total of 15 to avoid risking busting my-\nself.\nSelf-Belief is I have a total of 15 points, with an Ace that\ncan be either 1 or 11, giving me some flexibility in my\nstrategy. I should aim to get closer to 21 without busting.\nWorld-Belief is The dealer’s visible card is a 6, which\nmeans they could have a total of anywhere from 16 to\n22, and may need to hit again. Given this information,\nI think my best strategy is to Hit and try to improve my\nhand.\nAction\nMy action is Stand.\nMy action is Hit.\nNext Round\nThe final hand is Dealer: Club 6, Club 5, Heart\n6.\nYour: Club A, Diamond 4.\nThe dealer’s face-up card is Club 6.\nYour current cards are Club A, Diamond 4, Heart 6\nThought\nLose\nDealer: Club 6 + Club 5 + Heart 6 = 17\nYou: Club A + Diamond 4 = 5 or 15\nSelf-Belief is I currently have three cards with a total\nvalue of 21. This is a strong hand and I am confident\nin my chances of winning the game. World-Belief is\nThe dealer currently has a visible card of 6, which is a\nrelatively low value. However, I do not know the value of\ntheir hidden card, so I cannot be too complacent. Based\non these factors, I believe that standing is the best course\nof action.\nAction\nLose\nMy action is Stand.\nNext Round\nLose\nThe final hand Dealer: Club 6, Club 5, Diamond 9 (20).\nYour: Club A, Diamond 4, Heart 6 (21).\nResult\nLose\nWin\nFigure F3: Some cases for Blackjack conducted by ReAct and Agent-Pro using Qwen-72B. ReAct does not\naccurately understand the meaning of \"A\" in the rules of the game.\nAgent-Pro can Better Understand the Rules of the Game.\nState\nReAct Vs. Dealer\nAgent-Pro Vs. Dealer\nInitial Card\nThe dealer’s face-up card is Spade 3.\nYour current cards are Heart A, Club 9.\nThought\nMy thought is that I currently have a total of 10\npoints with my Heart A and Club 9. Given that\nthe dealer’s visible card is a Spade 3, it is likely\nthat the dealer has a total of between 13 and 16\npoints, since they would usually stand on a hard\n17 or higher. Therefore, I should aim to get as\nclose to 21 as possible without going over, while\nconsidering the possibility that the dealer may\nhave a strong hand.\nSelf-Belief is I have a total of 20 points, which is a strong\nhand. I am confident that I can beat the dealer with this\nhand. World-Belief is The dealer’s visible card is a 3,\nwhich is a weak hand. However, the dealer may have a\nhidden card that could potentially change the outcome\nof the game. Taking unnecessary risks could lead to\nan unfavorable outcome. Therefore, I should choose to\nstand and wait for the dealer to reveal their hidden card.\nAction\nMy action is Hit.\nMy action is Stand.\nNext Round\nThe dealer’s face-up card is Spade 3.\nYour current cards are Heart A, Club 9, Club 5\nThe final hand is Dealer: Spade 3, Heart J, Club 5.\nYour: Heart A, Club 9.\nThought\nI have a total of 15 points from my two cards\n(Ace counts as 1), and the dealer shows a 3.\nGiven this situation, there is a good chance that\nthe dealer may bust if they draw more cards to\nreach at least 17. However, standing with 15 is\nrisky as well since the dealer could still make a\nstrong hand.\nWin\nHeart A + Club 9 = 20\nSpade 3 + Heart J + Club 5 = 18\nAction\nMy action is Hit.\nWin\nAction\nMy action is Hit.\nWin\nNext Round\nThe dealer’s face-up card is Spade 3, Heart J,\nDiamond 8 (21).\nYour current cards are Heart A, Club 9, Club 5,\nSpade K (25 or 35)\nWin\nResult\nLose\nWin\nFigure F4: Some cases for Blackjack by ReAct and Agent-Pro using Qwen-72B. Agent-Pro accurately understands\nthe rules of the game and makes rational decisions.\nF.2\nCases For Texas Hold’em\nCurrent game State:\nNow your hand is [’HT’, ’CK ’], and the public cards are []. The number of chips all\nplayers have invested is [4, 0, 1, 2]. Currently , Preflop.\nAction Sequence: Player 0 raises. Player 1 folds. Player 2 folds.\nThe Response of the Agent:\nSelf-Belief: As Player 3, I hold the Ten of Hearts and the King of Clubs in hand ,\nindicating a high card potential. However , no similar suits or consecutive cards\nsuggest a weak hand in terms of potential straight or flush. Nevertheless , the hand\nstill has some promising value given the presence of a King. Even though the\ncommunity cards have not yet been dealt , it ’s worth to consider a strategy of\nmaintaining some aggression to minimize the number of opponents in the later stages.\nWorld-Belief: Player 0 seemingly adopted an aggressive strategy and raised already ,\npotentially suggesting a strong hand , although it might also be a bluff. · · ·\nHowever ,\nwhether Player 0 continues his aggressive strategy need to be closely watched. · · ·\nAction: Given the current situation , I am not in a position to take an aggressive\naction , neither it is wise to fold at this point. Thus , I will choose to simply\nmatch the current bet to see the flop. Thus , my action would be Call.\n------------------Next Round--------------------------------------------------\nCurrent game State: Now your hand is [’HT ’, ’CK ’], and the public cards are []. The\nnumber of chips all players have invested is [4, 0, 1, 4].\nCurrently , Preflop. Action Sequence: Player 0 raises. Player 1 folds. Player 2 folds\n. Player 3 calls. Player 0 checks.\nThe Response of the Agent:\nSelf-Belief: As Player 3, my hand consists of the Ten of Hearts and the King of Clubs.\nThese are not strong cards , and they do not form any promising hands such as a\nstraight or flush. However , the King card is high rank and may provide some\nadvantages in the later stage of the game. A conservative strategy might be the most\nreasonable at this stage , but a certain level of aggression may also be necessary\nto exert pressure on my opponent.\nWorld-Belief: Player 0 appears to be employing an aggressive strategy , having raised\neven before any community cards have been dealt. This behavior suggests that he may\nhave a strong hand. However , it could also be possible that Player 0 is bluffing.\nSince players 1 and 2 have already folded , it appears that the game might turn into\na head -on confrontation between Player 0 and me. · · ·\nAction: Given this scenario , it would be wise to use a balanced strategy to counter\nPlayer 0’s aggressive play without risking too many chips on my not -so -strong hand.\nTherefore , I will choose {’action ’: ’check’}.\n------------------Next Round---------------------------------------------------\nCurrent game State: Now your hand is [’HT ’, ’CK ’], and the public cards are [’D7 ’, ’C5\n’, ’C4 ’]. The number of chips all players have invested is [4, 0, 1, 4].\nCurrently , Preflop. Player 0 raises. Player 1 folds. Player 2 folds. Player 3 calls.\nPlayer 0 checks. Player 3 checks.\nFlop. Player 0 checks.\nThe Response of the Agent:\nSelf-Belief: As Player 3, the cards in my hand are the Ten of Hearts and the King of\nClubs. Without a clear pattern or majority suit , my current hand doesn ’t offer a lot\nin terms of immediate strength. However , the King is a high -ranking card and could\npotentially provide a strategic advantage should the game progress towards a\nshowdown. Given the moderateness of my hand and the absence of an apparent\ncompetitive advantage , it might be wise to maintain a balanced\nstrategy, toggling between conservative and aggressive plays based on the situation.\nWorld-Belief:Now , looking at Player 0, who remains the only player I would engage in\nthis round , following the withdrawal of Players 1 and 2. In the series of events\nleading up to the Flop , Player 0 has exhibited an aggressive approach , choosing to\nraise at every opportunity. This could indicate the strength of his hand , or it\ncould be an attempt to bluff , setting up an environment where he may feed on the\nhesitance of the other players. Given this behavior , it seems logical to assume that\nhe will continue this aggressive strategy going forward.· · · \"\nAction: Given the current circumstances, a cautious but unexpected move on my part could potentially\nthrow off Player 0’s supposed strategy. Thus , I will go with {’action ’: ’raise ’}. This\ncould make him reconsider my playing style and the strength of my hand , potentially\ninducing unforeseen errors in future rounds.\n------------------Next Round--------------------------------------\nPlayer 0 fold\n------------------Game End---------------------------------------\nPlayer0 loses 2 points , Player1 loses 0, Player2 loses 0.5, Agent -Pro wins 2.5 points\n------------------Game End ---------------------------------------------------\nBaseline Action Sequence:\nPreflop: raises -> calls -> calls -> checks.\nFlop: checks -> checks\nTurn: calls -> raises ->raises -> raises -> checks.\nRiver: checks -> checks.\nBaseline loses by 13.0 chip(s).\nCurrent game State:\nNow your hand is [’HK’, ’H2 ’], and the public cards are []. The number of chips all\nplayers have invested is [2, 4, 4, 2].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 calls.\nThe Response of the Agent:\nSelf-Belief: As Player 3, my card combination is currently off -suit high -low hand with\nKing and 2 in Hearts. This is a decent beginning at the Preflop stage , especially\nconsidering the high card , King , which carries significant weight. However, it’s still\nin the early stages of the game and no community cards are revealed, thus full information is not\navailable to make a precise decision.\nWorld-Belief:\nPlayer 0, Player 1, and Player 2 have all chosen to remain in the game.\nPlayer 1 has raised the pot , indicating they could potentially hold a strong hand.\nPlayer 0 and Player 2 have both elected to call , suggesting they might be playing\nmore cautiously , possibly because they hold mediocre hands or they ’re waiting to see\nthe upcoming community cards before playing aggressively. · · ·\nAction: Given the present circumstances , I believe the smart play would be to call.\nWhile the high King gives my hand some potential, the accompanying 2 substantially weakens my initial\nstance. Meanwhile, an overly aggressive play could be detrimental if my opponents are holding strong\nhands, which is possible given Player 1’s raise. Hence , my move would be to call and wait\nfor the flop ’s revelation. Thus , I’d conclude with {’action ’: ’call’}.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’HK’, ’H2 ’], and the public cards are []. The number of chips all\nplayers have invested is [6, 8, 4, 4].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 calls. Player\n3 calls. Player 0 raises. Player 1 raises. Player 2 folds.\nThe Response of the Agent:\nSelf-Belief:\nAs Player 3, I currently hold a high card King and a low card 2, both in\nhearts. This off-suit combination is an okay starting point in the preflop round. I have to keep in\nmind that luck plays a role in the rounds to come and the community cards to be drawn\nyet. Moreover , an aggressive approach towards the end can tilt the scales towards me\n, deploying my king card , if it finally pairs up.\nWorld-Belief: · · ·\nAction: Considering these observations and trying to keep up the misleading image , I\nthink the best move for now would be to call , not showing too much aggression , yet\nstaying in the game , waiting for a potentially favorable turn or river. Therefore , I\nwill choose {’action ’: ’call’}.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’HK’, ’H2 ’], and the public cards are [’DA ’, ’DK ’, ’SK ’]. The\nnumber of chips all players have invested is [8, 8, 4, 8].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 calls. Player\n3 calls. Player 0 raises. Player 1 raises. Player 2 folds. Player 3 calls. Player 0\ncalls. Player 1 checks.\nThe Response of the Agent:\nSelf-Belief: · · ·\nWorld-Belief: · · ·\nAction: Based on this analysis , I opt for {’action ’: ’raise’} demonstrating a greater\nlevel of confidence in my hand and taking advantage of my opponents ’ potential\nuncertainty.\n------------------Next Round---------------------------------------------------\nNext Action Sequence: Player 0 raises. Player 1 raises. Player 3 raises. Player 0\ncalls. Player 1 calls. Player 3 checks.\nTurn , Community Cards: [’DA’, ’DK’, ’SK ’, ’C3 ’], Action Sequence: Player 0 raises.\nPlayer 1 calls. Player 3 raises. Player 0 raises. Player 1 calls. Player 3 raises.\nPlayer 0 calls. Player 1 calls. Player 3 checks.\nRiver , Community Cards: [’DA’, ’DK’, ’SK ’, ’C3 ’, ’H8 ’], Action Sequence: Player 0\nraises. Player 1 calls. Player 3 raises. Player 0 raises. Player 1 calls. Player 3\nraises. Player 0 calls. Player 1 calls. Player 3 checks.\n------------------Game End---------------------------------------------------\nPlayer0 loses 24.0 points\nPlayer1 lost 24.0 points\nPlayer2 lose 2.0 points\nAgent -Pro wins 50.0 points\n----------Baseline: Vanilla LLM with the Same Cards and Opponents ----------\nAction Sequence:\nPreflop: Calls -> Folds.\nBaseline loses by 2.0 points.\nCurrent game State:\nNow your hand is [’C5’, ’H3 ’], and the public cards are []. The number of chips all\nplayers have invested is [2, 4, 1, 2].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 folds.\nThe Response of the Agent:\nSelf-Belief: Currently , I am holding 5 of Clubs and 3 of Hearts , which is a low\nunsuited hand. At the preflop stage , without any public cards on the table , it ’s a\nbit risky to invest heavily with such a hand. My feasible strategies include playing\nit safe and calling , or folding to avoid unnecessary risk. Raising would be a more\naggressive move and less rational given my current hand.\nWorld-Belief: Looking at my competitors , Player 0 has chosen to call , indicating a\npossibility of moderate strength in his hand. It seems he is being cautious ,\npotentially having a mediocre hand. Player 1 raises , suggesting a high confidence in\nhis hand , although it could also be a bluff. As Player 2 has already folded , I only\nhave to consider Player 0 and Player 1 in terms of active competition. · · ·\nAction: Given the above analysis , my most reasonable action at this stage would be to\ncall , as raising with a weak hand would be an unnecessary risk , and folding might\nclose potential chances to improve my hand with the public cards to come. Hence , I\nwill choose {’action ’: ’call’}.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’C5’, ’H3 ’], and the public cards are []. The number of chips all\nplayers have invested is [6, 8, 1, 4].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 folds. Player\n3 calls. Player 0 raises. Player 1 raises.\nThe Response of the Agent:\nSelf-Belief: Currently , I am holding 5 of Clubs and 3 of Hearts , which is a weak hand ,\nespecially in the absence of any public cards. With this hand, my feasible strategies\ninclude calling, to maintain the game pace while minimizing potential loss, or folding to completely\navoid any risk. Raising, in this scenario, seems irrational given the weak hand.\nWorld-Belief: · · ·\nAction: Based on these analyses , the most reasonable action would be to play it safe.\nTherefore , I will choose {’action ’: ’call’}.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’C5’, ’H3 ’], and the public cards are []. The number of chips all\nplayers have invested is [10, 10, 1, 8].\nIn Preflop , Action Sequence: Player 0 calls. Player 1 raises. Player 2 folds. Player\n3 calls. Player 0 raises. Player 1 raises. Player 3 calls. Player 0 raises. Player\n1 calls.\nThe Response of the Agent: Currently , I am holding 5 of Clubs and 3 of Hearts , which is\na weak hand , especially in the absence of any public cards. With this hand , my\nfeasible strategies include calling , to maintain the game pace while minimizing\npotential loss , or folding to completely avoid any risk. Raising , in this scenario ,\nseems irrational given the weak hand.\nSelf-Belief: · · ·\nWorld-Belief: · · ·\nAction: Given all the above factors , I think the most reasonable action would be to\n{’action ’: ’call’}. Considering the infinite chips, my main goal is to stay in the game and watch\nhow it unfolds. This strategy is also in line with my conservatism and allows me to stay in the round\nwith minimal risk.\n------------------Next Round---------------------------------------------------\nNext Action Sequence: Player 0 raises. Player 1 calls. Player 3 raises. Player 0\ncalls. Player 1 calls. Player 3 checks.\nTurn , Community cards: [’D3’, ’C3’, ’HQ ’, ’S3 ’], Action Sequence: Player 0 raises.\nPlayer 1 calls. Player 3 raises. Player 0 raises. Player 1 calls. Player 3 raises.\nPlayer 0 calls. Player 1 calls. Player 3 checks.\nRiver , Community cards: [’D3’, ’C3’, ’HQ ’, ’S3 ’, ’DA ’], Action Sequence: Player 0\nraises. Player 1 calls. Player 3 raises. Player 0 raises. Player 1 calls. Player 3\nraises. Player 0 calls. Player 1 calls. Player 3 checks.\n------------------Game End--------------------------------\nPlayer0 loses 25.0 points\nPlayer1 loses 25.0 points\nPlayer2 loses 0.5 points\nAgent -Pro wins 50.5 points\n----------Baseline: Vanilla LLM with the Same Cards and Opponents ----------\nAction Sequence:\nPreflop: Folds.\nBaseline loses by 1.0 points..\nCurrent game State:\nNow your hand is [’HT’, ’CK ’], and the public cards are []. The number of chips all\nplayers have invested is [4, 0, 6, 2].\nIn Preflop , Action Sequence: Player 0 raises. Player 1 folds. Player 2 raises.\nThe Response of the Agent:\nSelf-Belief:\nMy hand cards are heart -ten and club -king which is considered as high\ncards. · · · My current strategy can be to try and match my King or Ten with the upcoming community\ncards or use my King card as the high card to potentially outrank my opponents.\nWorld-Belief: · · ·\nTheir betting strategy could be quite aggressive which can be\nimpactful in trying to potential opponents to fold , allowing them to scoop the pot\nor they have strong confidence in their hands. · · ·\nAction: · · ·\nI will choose {’action ’: ’call’}.\n------------------Next Round---------------------------------------------------\nNext Action Sequence:\nPlayer 0 raises. Player 2 calls. Player 3 calls. Player 0\nchecks.\nFlop , community cards is [’D7’, ’C5 ’, ’C4 ’], Action Sequence: Player 2 checks.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’HT’, ’CK ’], and the public cards are [’D7 ’, ’C5 ’, ’C4 ’]. The\nnumber of chips all players have invested is [8, 0, 8, 8].\nIn Preflop , Action Sequence: Player 0 raises. Player 1 folds. Player 2 raises.\nPlayer 3 calls. Player 0 raises. Player 2 calls. Player 3 calls. Player 0 checks.\nIn Flop , Action Sequence: Player 2 checks.\nThe Response of the Agent:\nSelf-Belief: · · ·\nBased on the Texas Hold ’em hand ranking , I only have a high card at\nthis moment. Nonetheless , if the Turn and the River bring clubs , I could potentially\nform a Flush with my King as the highest card. But, the probability of this happening is\nrelatively low, and relying solely on this possibility would be too risky and against the prudent\npoker strategy.\nWorld-Belief: · · ·\nAction: · · ·\nSo, {’action ’: ’check’} is my choice.\n------------------Next Round---------------------------------------------------\nNext Action Sequence: Player 0 checks. Player 2 raises. Player 3 calls. Player 0\nfolds. Player 2 raises. Player 3 calls. Player 2 checks. Player 3 checks.\nTurn , community cards: [’D7’, ’C5’, ’C4 ’, ’C3 ’], Action Sequence: Player 2 checks.\nPlayer 3 checks. Player 2 checks. Player 3 checks.\n------------------Next Round---------------------------------------------------\nCurrent game State:\nNow your hand is [’HT’, ’CK ’], and the public cards are [’D7 ’, ’C5 ’, ’C4 ’, ’C3 ’, ’D2\n’]. The number of chips all players have invested is [8, 0, 16, 12].\nIn Preflop , Action Sequence: Player 0 raises. Player 1 folds. Player 2 raises.\nPlayer 3 calls. Player 0 raises. Player 2 calls. Player 3 calls. Player 0 checks.\nIn Flop , Action Sequence: Player 2 checks. Player 3 checks. Player 0 checks. Player\n2 raises. Player 3 calls. Player 0 folds. Player 2 raises. Player 3 calls. Player 2\nchecks. Player 3 checks.\nIn Turn , Action Sequence: Player 2 checks. Player 3 checks. Player 2 checks. Player\n3 checks.\nIn River , Action Sequence: Player 2 raises.\nThe Response of the Agent:\nSelf-Belief: As Player 3, my hand consists of the ten of hearts and king of clubs. The\ncommunity cards , unfortunately , don ’t create any viable combinations for me except\nfor a potential straight if an Ace or 6 of any suit appears , but that ’s pushing my\nluck. My current strategy is to proceed with caution, bluff if necessary, and only call when my hand\nhas a good potential.\nWorld-Belief: Looking at my opponents , Player 0 and Player 2, they started strong by\nraising the stakes but have seemed to cool off a bit. This might signify that the\ncommunity cards didn ’t play into their hand as they would ’ve hoped. · · ·\nAction: Considering all the above , I think it ’s vital to maintain my poker face and\nkeep them guessing. I will {’action ’: ’call’}. I will show confidence while remaining\nthoughtful of the game situation.\n------------------Next Round---------------------------------------------------\nNext Action Sequence: Player 2 checks. Player 3 checks.\n------------------Game End-----------------------------------------\nPlayer0 loses 4.0 points\nPlayer1 loses 0.0 points\nPlayer2 wins 12.0 points\nAgent -Pro loses 8.0 points\n----------Baseline: Vanilla LLM with the Same Cards and Opponents ----------\nAction Sequence:\n1-Preflop: raises -> calls.\n2-Flop: checks.\n3-Turn: checks -> calls.\n4-River: checks -> calls -> calls -> raises -> raises -> checks.\nPlayer 3 loses by 15.0 points.\n"
}