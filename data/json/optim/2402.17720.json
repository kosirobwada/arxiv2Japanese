{
    "optim": "The SMART Approach to Instance-Optimal Online\nLearning\nSiddhartha Banerjee\nORIE, Cornell\nsbanerjee@cornell.edu\nAlankrita Bhatt\nCMS, Caltech\nabhatt@caltech.edu\nChristina Lee Yu\nORIE, Cornell\ncleeyu@cornell.edu\nAbstract\nWe devise an online learning algorithm – titled Switching via Monotone Adapted Regret\nTraces (SMART) – that adapts to the data and achieves regret that is instance optimal, i.e.,\nsimultaneously competitive on every input sequence compared to the performance of the follow-\nthe-leader (FTL) policy and the worst case guarantee of any other input policy ALGWC. We\nshow that the regret of the SMART policy on any input sequence is within a multiplicative\nfactor e/(e − 1) ≈ 1.58 of the smaller of: 1) the regret obtained by FTL on the sequence, and 2)\nthe upper bound on regret guaranteed by the given worst-case policy. This implies a strictly\nstronger guarantee than typical ‘best-of-both-worlds’ bounds as the guarantee holds for every\ninput sequence regardless of how it is generated. SMART is simple to implement as it begins by\nplaying FTL and switches at most once during the time horizon to ALGWC. Our approach and\nresults follow from an operational reduction of instance optimal online learning to competitive\nanaylsis for the ski-rental problem. We complement our competitive ratio upper bounds with a\nfundamental lower bound showing that over all input sequences, no algorithm can get better\nthan a 1.43-fraction of the minimum regret achieved by FTL and the minimax-optimal policy.\nWe also present a modification of SMART that combines FTL with a “small-loss” algorithm to\nachieve instance optimality between the regret of FTL and the small loss regret bound.\n1\nIntroduction\nOur work aims to develop algorithms for online learning that are instance optimal [Fagin et al.,\n2001],[Roughgarden, 2021, Chapter 3] with respect to the stochastic and minimax optimal algorithms\nfor a given setting. This is best motivated via a concrete example:\nExample 1 (Binary Prediction). We are given bit stream yn := y1, y2, . . . , yn ∈ {0, 1}n. At the\nstart of day t, before seeing yt, we choose (possibly randomized) prediction bYt ∼ Bernoulli(at)\n(for at ∈ [0, 1]) for the upcoming bit yt, given the history yt−1. Our resulting loss on day t is\nℓt(at) = P(bYt ̸= yt) = |at − yt|, and our total loss is Ln(ALG, yn) := Pn\nt=1 ℓt(at). The objective is\nto achieve low regret (i.e., additive loss) compared to the loss Ln(a, yn) = Pn\nt=1 ℓt(a) of the best\nfixed action a∗ ∈ [0, 1] in hindsight. As a∗ is the majority in yn between 0 and 1, it follows that\nLn(a∗, yn) = min\n\bPn\nt=1 yt, n − Pn\nt=1 yt\n\t\n. Formally, for sequence yn ∈ {0, 1}n, policy ALG incurs\nregret\nReg(ALG, yn) := Ln(ALG, yn) − Ln(a∗, yn) = Pn\nt=1 |at − yt| − mina∈[0,1]\nPT\nt=1 |a − yt|.\n(1)\nBinary prediction goes back to the seminal works of Blackwell [1956] and Hannan [1957]. The\ndefinition of regret is motivated by the case where yt is randomly generated as i.i.d. Bernoulli(p). If\n1\narXiv:2402.17720v1  [cs.LG]  27 Feb 2024\np is known, then the optimal policy is the ‘Bayes predictor’ aBayes = ⌊2p⌋ (i.e., nearest integer to p),\nwhich coincides with hindsight optimal a∗ with high probability when p is away from 1/2. When\np is unknown, the stochastic optimal policy is the Follow The Leader or FTL policy, which sets\nat = Majority(yt−1), i.e. the majority bit amongst the first t − 1 bits (at = 1/2 if both are equal1).\nA starting point for online learning is the observation that it is easy to construct a sequence\nyn such that FTL has poor regret: For example, if yn = (1, 0, 1, 0, 1, 0, . . .), i.e., alternate 1s and\n0s, then the regret of FTL grows linearly with n. In contrast, worst-case optimal online learning\npolicies such as those of Blackwell and Hannan, or more modern versions like Multiplicative\nWeights or Follow The Perturbed Leader (see Cesa-Bianchi and Lugosi [2006], Slivkins [2019])\nguarantee regret of Θ(√n) over all sequences. Indeed, for bit prediction, the exact minimax optimal\npolicy was established by Cover [1966], and this policy (which we refer to as Cover) achieves2\nReg(Cover, yn) = p n\n2π(1 + o(1)) under any yn ∈ {0, 1}n, implying it is an equalizer (achieves same\nregret over all sequences).\nWhile the above discussion seems a convincing endorsement of worst-case online learning\nalgorithms, the situation is more complicated. One problem is that while FTL has bad regret on\ncertain pathological sequences, on more ‘realistic’ sequences FTL performs orders of magnitude\nbetter than the minimax regret. As an example, with i.i.d. Bernoulli(p) input, Reg(FTL, yn) is\nactually independent of n (i.e., O(1)) as long as p is away from 1/2 with high probability. We\ndemonstrate this in Figure 1(a), where we see Reg(FTL) is much lower than Reg(Cover) ≈ 0.39√n\nunless p is very close to 1/2. This phenomena is known in more general settings [Huang et al., 2016],\nsuggesting that in practice one may be better off just using FTL. On the other hand, as Figure 1(b, c)\nindicates, we know how to generate sequences yn [Feder et al., 1992] for which Reg(FTL, yn) grows\nlinearly with n, and so the √n regret of Cover becomes appealing.\nNow suppose instead that a fictitious oracle is told beforehand which of FTL or Cover is better\nsuited for the upcoming sequence yn; the demand made by instance optimality is that we try to be\ncompetitive against such an oracle on every sequence yn.\nDefinition 1 (Instance Optimality). A binary prediction policy ALG is instance optimal with respect\nto the regret of FTL and Cover if there exists some universal γn ≥ 1 such that for all yn ∈ {0, 1}n:\nReg(ALG, yn) ≤ γn min{Reg(FTL, yn), Reg(Cover, yn)}\nWe henceforth refer to γn as the competitive ratio achieved by ALG; ideally we want this ratio\nto be a constant, i.e., γn = O(1). This necessitates that on sequences where FTL gets a constant\nregret, then ALG basically follows FTL throughout, while on sequences where FTL has high (in\nparticular, ω(√n)) regret, then ALG follows Cover in most rounds.\nThe challenge in designing instance optimal algorithms is that the regret of any algorithm\nis a quantity that is not adapted to the natural filtration, i.e. it may not be possible to track\nReg(ALG, yn) for any ALG from just the history (y1, y2, . . . , yt−1), since the hindsight optimal action\na∗ depends on the entire sequence yn. One proxy is to track an algorithm’s loss instead, leading\nto the idea of ‘corralling’ policies [Agarwal et al., 2017, Pacchiano et al., 2020, Dann et al., 2023],\nthat run online learning over the reference algorithms to get within O(poly(n)) of the smaller\nof the two losses. Such an approach can not ensure γn = O(1): for example, consider an i.i.d.\nsequence of Bernoulli(0.1) bits, where FTL has lower regret than Cover. With high probability on\nany such sequence we have small Reg(FTL, yn) = O(1) and yet high loss Ln(a∗, yn) = Θ(n); now\nany corralling algorithm (even a small loss one) must suffer O(poly(n)) regret, and hence ω(1)\n1We choose this specific tie-breaking rule for convenience; however, we can take any at ∈ [0, 1].\n2Here fn, the so-called Rademacher complexity of the setting, is a fixed function of n that does not depend on\nsequence yn. For binary prediction, fn =\nE | Pn\nt=1 Zt|\n2\n≈ p n\n2π where Zn ∼ Unif{1, −1} i.i.d.\n2\nFigure 1: Comparing regret of FTL, Cover and SMART on a collection of input sequences (for fixed n).\n• In Fig. (a), we consider i.i.d. Bernoulli(p) inputs for varying p. The regret of FTL is much lower than Cover for\np < 1/2; the regret of SMART tracks FTL closely (better than 2Reg(FTL), indicated by dotted line).\n• In Fig. (b) and (c), we consider ‘worst-case’ binary sequences (as per [Feder et al., 1992]) parameterized by the\nnumber of ‘lead-changes’: the sequence with parameter c comprises of c pairs ‘0, 1’ or ‘1, 0’, followed by n − 2c ‘1’s. In\nFig. (b), we consider SMART with a deterministic switching threshold (Theorem 1) and compare Reg(SMART) with\n2Reg(FTL) and 2Reg(Cover) (dotted lines); in Fig. (c), we use a randomized threshold (Theorem 2), and show the\naverage regret over the randomized threshold, as well as sample paths (plotted in green), and compare with\ne\ne−1 times\nReg(FTL) and Reg(Cover) (dotted lines).\ncompetitive ratio. This example also shows that achieving a constant factor guarantee with respect\nto the minimum of the two losses does not translate to a constant factor guarantee with respect to\nthe minimum of the two regrets.\nThe instance optimal guarantee is closely related to best-of-both-worlds guarantees, which aim for\nalgorithms that simultaneously achieve (up to constant factors) both the low pseudoregret guarantee\nof policies designed for stochastic inputs (as with FTL in our setting, or the Upper Confidence Bound\n(UCB) algorithm in bandits), as well as a per-sequence regret guarantee comparable to a worst-case\noptimal algorithm ALGWC (Eg. Cover or Hedge in online learning; EXP3 in bandits Auer et al.\n[2002a]). Such guarantees have been shown in a variety of settings, including online learning [De Rooij\net al., 2014, Orabona and P´al, 2015, Mourtada and Ga¨ıffas, 2019, Bilodeau et al., 2023] and bandit\nsettings [Bubeck and Slivkins, 2012, Zimmert and Seldin, 2019, Lykouris et al., 2018, Dann et al.,\n2023]. One problem though is that since pseudoregret and worst-case regret are very different\nquantities, the above results tend to be hard to interpret, and less predictive of good performance3.\nNote though that given a pair of stochastic/worst-case optimal algorithms, a policy that is γ-\ninstance-optimal w.r.t. these immediately satisfies a best-of-both-worlds guarantee with constant\nfactor γ. In this regard, instance optimality provides a stronger guarantee as it holds on every\nsequence yn regardless of how it is generated. Moreover, the parameter γ can also provide sharper\ncomparisons between algorithms, as well as admit hardness results on the limits of such guarantees.\n1.1\nOur Contributions\nWe consider a general online learning setting where at the beginning of each round t ∈ [n], a policy\nALG first plays an action at ∈ A, following which, a loss function ℓt : A → [0, 1] is revealed, resulting\nin a loss of ℓt(at). The regret is defined according to:\nReg(ALG, ℓn) = Pn\nt=1 ℓt(at) − infa∈A\nPn\nt=1 ℓt(a).\n(2)\n3As an example, Hedge has optimal pseudoregret in certain stochastic settings [Mourtada and Ga¨ıffas, 2019], but\nthis is known to be sensitive to perturbations in the distributions [Bilodeau et al., 2023].\n3\nMore generally, as in with bit prediction, we allow ALG to play in round t a measure wt ∈ ∆A\n(i.e., play {wt : A 7→ [0, 1]| P\na∈A wt(a) = 1}), resulting in an expected loss of P\na∈A wt(a)ℓt(a).\nFor notational convenience, we henceforth use (at, ℓt(at)) for the action/loss, and reserve use of\nexpectations for randomness in the algorithm and/or sequence.\nWe want to understand when is it possible to attain instance optimality as in Eq. (1) with respect\nto a given pair of algorithms. Ideally, we want the first to be optimal for stochastic instances, and\nthe second to be minimax optimal; unfortunately however exact optimal policies are unknown except\nin simple settings. To this end, we make two amendments to our goal: First, for the stochastic\noptimal policy, we use FTL; this is well defined in any online learning setting, and moreover, known\nto be optimal or near-optimal for a wide range of settings under minimal assumptions [Kot lowski,\n2018]. Second, instead of the minimax policy, we use as reference any policy ALGWC which has\na known worst case regret bound g(n). With these modifications in place, we have the following\nobjective.\nDefinition 2. Given FTL and any algorithm ALGWC with supℓn Reg(ALGWC, ℓn) ≤ g(n), we say a\npolicy ALG is instance optimal with respect to the pair if there exists some universal γn ≥ 1 (i.e.\nnot depending on yn) such that for every sequence of losses ℓn:\nReg(ALG, ℓn) ≤ γn min{Reg(FTL, ℓn), g(n)}\nWhile the above guarantee is not truly instance-optimal in that we are comparing against a\nworst-case regret bound g(n) for ALGWC rather than its performance on the instance ℓn, the two\nare the same if ALGWC is minimax optimal and hence attaining equal regret on all loss sequences;\nrecall this is true of Cover for binary prediction.\nTo realize the above goal, we propose the Switch via Monotone Adapted Regret Traces (SMART)\napproach, which at a high level is a black-box way to convert design of instance-optimal policies\ninto a simple optimal stopping problem. Our approach depends on just two ingredients: first, owing\nto the additive structure of online learning problems, we have that the minimax guarantee g(k)\nabove holds over any k ∈ Z and any (sub)sequence of n loss functions; second, we show that FTL\nadmits simple anytime regret estimator ΣFTL\nτ\n(see Lemma 1) which is monotone and adapted (i.e., a\nfunction only of historical data). Using these two observations, we can reduce the task of minimizing\nregret to a version of the ‘ski-rental’ problem [Karlin et al., 1994, Borodin and El-Yaniv, 2005], as\nfollows: we play FTL up to some stopping time τ, and then switch to ALGWC for the remaining n−τ\nperiods, resetting all losses to zero. This algorithm incurs a total regret bounded by ΣFTL\nτ\n+ g(n − τ),\nand using ideas from competitive analysis, we get that there is a simple way to choose the stopping\ntime τ to achieve an e/(e − 1) ≈ 1.58-competitive ratio guarantee with respect to the minimum\nbetween the regret of FTL and the worst case guarantee g(n).\nTheorem. (See Theorem 2) Let ALGWC have worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n)\nwhere g(n) is some monotonic function of n. An instantiation of SMART achieves\nReg(SMART, ℓn) ≤\ne\ne − 1 min{Reg(FTL, ℓn), g(n)} + 1.\n(3)\nA highlight of our approach is the surprising simplicity of the algorithm and analysis, despite the\nstrength of the instance optimality guarantee. In particular, our approach is modular, allowing one\nto plug in any ALGWC and corresponding worst case bound g(n), thus letting us handle any online\nlearning setting with known minimax bounds. This results in an entire family of instance optimal\npolicies for settings such as predictions with experts and online convex optimization. Moreover, the\napproach is easy to extend to get more complex guarantees; as an example, if ALGWC is designed\n4\nto get low regret for benign (i.e., ‘small-loss’) sequences ℓn, then we show how to use SMART as a\nsubroutine and achieve an instance optimal guarantee with respect to the regret of FTL and a small\nloss regret bound.\nCorollary 1 (Following Theorem 5). Consider the prediction with expert advice setting [Cesa-\nBianchi et al., 1997], where A = ∆m−1, the m−simplex for m ≥ 2, and ℓt(a) = ⟨a, ℓt⟩ for ℓt ∈ [0, 1]m.\nLet L∗ := minj\nPn\nt=1 ℓtj. An instantiation of SMART achieves\nReg(SMART, ℓn) ≤ 2 min\nn\nReg(FTL, ℓn), 10\np\n2L∗ log m\no\n+ O(log L∗ log m).\nFinally, studying instance optimality also lets us understand the fundamental limits of best-of-\nboth worlds algorithms. To this end, we provide a lower bound that shows our algorithm is nearly\noptimal in the competitive ratio. To the best of our knowledge, this is the first hardness result for\nbest-of-both-worlds guarantees in online learning.\nTheorem. (See Theorem 3) In the binary prediction setting, given any online algorithm ALG,\nthere exist sequences yn ∈ {0, 1}n such that:\nReg(ALG, yn) ≥ 1.43 min\n\b\nReg(FTL, yn), Reg(Cover, yn)\n\t\nNote again that in binary prediction, FTL achieves the optimal pseudoregret under i.i.d. inputs,\nwhile Cover is the true minimax policy; thus, this is a fundamental lower bound on best-of-both-worlds\nguarantee in any online learning setting.\n1.2\nRelated work\nThere have been many approaches towards combining stochastic and worst-case guarantees. As\nwe discussed before, there is a large literature on best-of-both-worlds algorithms for both full and\npartial information settings [Wei and Luo, 2018, Bubeck et al., 2019, Zimmert and Seldin, 2019,\nDann et al., 2023], and also more complex settings such as metrical task systems [Bhuyan et al.,\n2023] and control [Sabag et al., 2021, Goel et al., 2023]. Another line of work [Rakhlin et al., 2011,\nHaghtalab et al., 2022, Block et al., 2022, Bhatt et al., 2023] considers smoothed analysis, where the\nworst-case actions of the adversary are perturbed by nature. A third approach [Bubeck and Slivkins,\n2012, Lykouris et al., 2018, Amir et al., 2020, Zimmert and Seldin, 2019] interpolates between\nthe stochastic and adversarial settings by considering most ℓt to be i.i.d., interspersed with a few\nadversarially chosen instances (corruptions). Finally, another line considers the data-generating\ndistribution to come from a ball of specified radius around i.i.d. distributions [Mourtada and Ga¨ıffas,\n2019, Bilodeau et al., 2023]. While all these approaches provide useful insights into the gap between\naverage and worst-case guarantees, one can argue they are all imprecisely specified – given an\ninstance {ℓt}t∈[n] in hindsight, there is no clear sense as to which model best ‘explains’ the instance.\nOur focus on instance optimality instead follows the approach of better understanding and\nshaping the per-sequence regret landscape. The origins of this approach arguably come from the\nseminal work of Cover [1966] for binary prediction (we discuss this in more detail in Section 3), with\na later focus on better bounds for benign instances in general online learning [Auer et al., 2002b,\nCesa-Bianchi et al., 2005, Hazan and Kale, 2010]. More recently, a line of work [Koolen et al., 2014,\nVan Erven et al., 2015, Van Erven and Koolen, 2016, Gaillard et al., 2014] have studied designing\npolicies that can adapt to different types of data sequences and achieve multiple performance\nguarantees simultaneously. The main idea is to use multiple learning rates that are weighted\naccording to their empirical performance on the data. While the focus is still primarily on classifying\ninstances based on when they are easier/harder to learn, some of the resulting guarantees have an\n5\ninstance-optimality flavor; for example, Van Erven and Koolen [2016] show how to simultaneously\nmatch the performance guarantee (in terms of certain variance bounds) attained by different learning\nrates in Hedge. Such approaches however need to understand their baseline algorithms in great\ndetail, and use them in a ‘white-box’ way to get their guarantees.\nIn contrast, our approach fundamentally focuses on combining policies in a black-box way\nto get instance optimal outcomes. As we mention, this is similar in spirit to corralling bandit\nalgorithms Agarwal et al. [2017], Pacchiano et al. [2020], Dann et al. [2023], as well as more recent\nwork on online algorithms with predictions Bamas et al. [2020], Dinitz et al. [2022], Anand et al.\n[2022]; however, as we mention above, these all get guarantees with respect to the loss of the reference\nalgorithms, which is much weaker than our regret guarantees (though they do so in much more\ncomplex settings with partial information and/or state). To our knowledge, the only previous result\nwhich attains a comparable instance-optimality guarantee to ours is that of De Rooij et al. [2014]\nfor the experts problem, where the authors propose the FlipFlop policy which interleaves Hedge\n(with varying learning rates) and FTL to obtain a regret guarantee similar to that of Corollary 1.\nIn fact, their guarantee is stronger as FlipFlop is shown to be 5.64-competitive with respect to\nmin{Reg(FTL, ℓn), g(L∗)} where g(L∗) ≤ √L∗ log m [De Rooij et al., 2014, Corollary 16]. However,\nwhile FlipFlop depends on a clever choice of learning rates in Hedge, SMART can black-box interleave\nFTL with any worst-case/small-loss algorithm without knowing the inner workings of said algorithm,\nwhich we see as a significant engineering strength. More importantly, our approach to this problem\nis distinct as we focus on the fundamental limits (upper and lower bounds) on the competitive ratio\nthat must be incurred when combining FTL with a worst-case policy; to the best of our knowledge\nthis viewpoint, and the corresponding reduction to an optimal stopping problem, has not been\npreviously explored.\n2\nInstance Optimal Online Learning: Achievability via SMART\nGiven the setting and problem statement above, we can now present the SMART policy. We do this\nfor a general online learning problem, wherein we want to combine FTL with any given algorithm\nALGWC with a worst-case regret guarantee supℓn Reg(ALGWC, ℓn) ≤ g(n). Before presenting the\npolicy, we first need the following regret decomposition for FTL.\nLemma 1 (Regret of FTL). If Lt(·) := Pt\ni=1 ℓt(·), i.e. the cumulative loss function, then\nReg(FTL, ℓn) = Pn\nt=1(Lt(a∗\nt−1) − Lt(a∗\nt )).\nThis is reminiscent of the ‘be-the-leader’ lemma [Kalai and Vempala, 2005, Slivkins, 2019],\nalthough never stated explicitly as an exact decomposition.\nProof. Recall we define aFTL\nt\n= a∗\nt−1, and hence infa∈A\nPn\nt=1 ℓt(a) = Ln(a∗\nn). Now we have\nReg(FTL, ℓn) = Pn\nt=1 ℓt(a∗\nt−1) − Ln(a∗\nn)\n= Pn\nt=1(Lt(a∗\nt−1) − Lt−1(a∗\nt−1)) − Ln(a∗\nn)\n= Pn\nt=1(Lt(a∗\nt−1) − Lt(a∗\nt )) + Pn\nt=1(Lt(a∗\nt ) − Lt−1(a∗\nt−1)) − Ln(a∗\nn)\n(a)\n= Pn\nt=1(Lt(a∗\nt−1) − Lt(a∗\nt ))\nwhere (a) follows since Pn\nt=1(Lt(a∗\nt ) − Lt−1(a∗\nt−1)) = Ln(a∗\nn) by telescoping.\n6\nNext, let ΣFTL\nt\ndenote the anytime regret that FTL incurs up to round t (i.e., assuming the game\nends after round t). From Lemma 1 we have\nΣFTL\nt\n:= Reg(FTL, ℓt) = Pt\ni=1(Li(a∗\ni−1) − Li(a∗\ni ))\n(4)\nNow we make three critical observations:\n• ΣFTL\nt\nis adapted: it can be computed at the end of the tth round\n• ΣFTL\nt\nis monotone non-decreasing in t (since by definition Li(a∗\ni−1) − Li(a∗\ni ) ≥ 0)\n• ΣFTL\nt\nis an anytime lower bound for Reg(FTL, ℓn), with ΣFTL\nn\n= Reg(FTL, ℓn)\nFor an input threshold θ ≥ 0 and an algorithm ALGWC, we get the following (meta)algorithm.\nAlgorithm 1: Switching via Monotone Adapted Regret Traces (SMART)\nInput: Policies FTL, ALGWC, threshold θ\nInitialize ΣFTL\n0\n= 0, t = 1 ;\nwhile ΣFTL\nt−1 ≤ θ do\nSet at = a∗\nt−1;\n// Play FTL\nObserve ℓt(·);\nUpdate Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL\nt\n= ΣFTL\nt−1 + (Lt(a∗\nt−1) − Lt(a∗\nt )) and t = t + 1;\nend\nReset losses to 0 and play ALGWC for remaining rounds (See Figure 2(b)) ;\nWe now have the following performance guarantee for Algorithm 1 for θ = g(n).\nTheorem 1 (Regret of SMART with deterministic threshold). We are given FTL, and any other\npolicy ALGWC with worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n) for some monotone function g.\nThen, playing SMART with threshold θ = g(n) ensures\nReg(SMART, ℓn) ≤ 2 min{Reg(FTL, ℓn), g(n)} + 1.\n(5)\nAs we mention before, the structure of the SMART algorithm (and the resulting guarantee)\nparallels the standard 2-competitive guarantee for the ski-rental problem [Karlin et al., 1994]. This\nis a classical optimal stopping problem, where a principal faces an unknown horizon, and in each\nperiod must decide whether to rent a pair of skis for the period (for fixed cost $1) or buy the skis for\nthe remaining horizon (for fixed cost $B). The aim is to design a policy which is minimax optimal\n(over the unknown horizon) with regards to the ratio of the cost paid by the principal, and the\noptimal cost in hindsight. We further expand on this connection in Section 2.1 for the case of binary\nprediction. However, the connection suggests a natural follow-up question of whether randomized\nswitching can help (as is the case for ski-rental); the following result answers this in the affirmative.\nTheorem 2 (Regret of SMART with Randomized Thresholds). We are given FTL and any other\npolicy ALGWC with worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n) for some monotone function g.\nMoreover, given random sample U ∼ Unif[0, 1], suppose we set\nθ = g(n) ln(1 + (e − 1)U)\nThen playing the SMART policy (Algorithm 1) with random threshold θ ensures\nEθ\n\u0002\nReg(SMART, ℓn)\n\u0003\n≤\ne\ne − 1 min{Reg(FTL, ℓn), g(n)} + 1.\n7\nWhile we state the above as a randomized switching policy, this is more for interpretability – it\nis easier to view our policy as switching between two black-box algorithms rather than playing a\nconvex combination of the two. However, since we define the loss incurred by any ALG in round t to\nbe the expected loss when ALG plays a distribution wt over actions A (see Section 1.1), therefore\nwe can alternately implement the above by mixing between the actions of FTL and ALGWC. More\nspecifically, the above policy induces a monotone mixing rule, where over t, the weight on the action\nsuggested by ALGWC is non-decreasing.\nRemark 1 (Optimality over Monotone Mixing Policies). The competitive ratio of\ne\ne−1 is known to\nbe optimal for the ski-rental problem via Yao’s minmax theorem [Borodin and El-Yaniv, 2005]. A\ndirect corollary of this is the optimality of SMART over algorithms that are single switch (i.e., where\nthe weight on ALGWC is non-decreasing in t). One difference between our setting and ski-rental\nis that switching back-and-forth between FTL and ALGWC is possible; see for example the FlipFlop\npolicy of De Rooij et al. [2014]. In Section 3 we provide a fundamental lower bound of 1.43 on\nthe competitive ratio over all algorithms; this suggests that multiple switching can help get a better\ncompetitive ratio (since e/(e − 1) ≈ 1.58), but also, that single-switch policies are surprisingly close\nto optimal.\n2.1\nIllustrating the Reduction to Ski Rental in Binary Prediction\nBefore proving Theorems 1 and 2, we first illustrate the basic idea of our approach and reduction\nfor the binary prediction setting. This is aided by the observation that the regret of FTL in this\nsetting admits a simple geometric interpretation: for any sequence, and any time t, we have that\nΣFTL is equal to 1/2 times the number of ‘lead changes’ up to time t; where a lead change is a time\nstep i where the count of 1s and 0s in the (sub)sequence yi−1 is equal (see Figure 2(a)).\nCorollary 2 (FTL for binary prediction). In binary prediction, for any sequence y ∈ {0, 1}n and\nany time t ≤ n, define the lead-change counter\nc(yt) := |{0 ≤ j ≤ t s.t.\nj\nX\ni=1\nyi =\nj\nX\ni=1\n(1 − yi)}|.\nThen we have ΣFTL\nt\n= 1\n2c(yt−1) and thus Reg(FTL, yn) = 1\n2c(yn−1).\nCorollary 2 follows from the regret decomposition in Lemma 1. Furthermore, since the losses\nof 0s and 1s are equal at a lead change, it also follows that at a lead change t, the anytime regret\nΣFTL\nt\nis also equal to the hindsight regret incurred by FTL up to time t. Since ΣFTL\nt\nonly increases\nin value at lead changes, if the SMART algorithm switches to ALGWC, it will only happen after a\nlead change, and thus SMART behaves as if it had oracle knowledge of the regret of FTL from just\nthe history up to the current time.\nConsider the instantiation of SMART where ALGWC = Cover and g(n) = p n\n2π. As mentioned in\nSection 1, a remarkable property of Cover is that it is the true minimax optimal algorithm, where\nReg(Cover, yn) = p n\n2π(1+o(1)) for all sequences yn, such that g(n) is nearly equal to Reg(Cover, yn)\nregardless of the sequence [Cover, 1966].\nIt follows that SMART is equivalent to an algorithm which starts with FTL, plays it until the\nregret of FTL matches the minimax regret guaranteed by Cover for the remaining sequence, and then\nswitches to Cover until the end. Let tsw denote the last round SMART plays FTL before switching\nto ALGWC (with tsw = n if it never switches). For a single switch algorithm, the sequence which\nmaximizes the regret is one that maximizes the FTL regret before the switch at tsw, and minimizes\n8\nFigure 2: Figure 2(a) on the left shows the worst case instance in binary prediction for an algorithm which starts\nwith FTL and switches at most once during the time horizon to Cover. Figure2(b) on the right depicts in a prediction\nwith experts setting how SMART resets the losses after the switch from FTL to ALGWC.\nthe FTL after the switch, as depicted in Figure 2(a). For such a sequence, ΣFTL\nt\n= t/4 at lead\nchanges t, such that the regret incurred by the algorithm is linear before the switch, matching\nthe linear cost of renting skis in the ski rental problem. Note that tsw will necessarily be o(n) in\nsuch sequences as the time it takes until ΣFTL\nt\n≥ g(n) is linear in g(n) = o(n). After the switch,\nCover will incur regret\nq\nn−tsw\n2π (1 + o(1)) = g(n)(1 + o(1)), matching the fixed cost of buying skis\nat the switching point; Corollary 3 follows as a result of this analysis. Furthermore, in the binary\nprediction setting, SMART achieves the stronger notion of instance optimality stated in Definition 1.\nCorollary 3. For all yn ∈ {0, 1}n, SMART with ALGWC = Cover and θ = p n\n2π satisfies\nReg(SMART, yn) ≤ 2 min{Reg(FTL, yn), Reg(Cover, yn)} + 1.\nSMART with ALGWC = Cover and θ = p n\n2π ln(1 + (e − 1)U) for U ∼ Uniform[0, 1] satisfies\nReg(SMART, yn) ≤ 1.58 min{Reg(FTL, yn), Reg(Cover, yn)} + 1.\n2.2\nProofs for General Online Learning\nIn the general online learning setting, the proof is nearly the same, with the reduction to the\nski-rental problem captured by Lemma 2.\nLemma 2. Let tsw := min1≤t≤n−1 ΣFTL\nt\n> θ denote the last round SMART plays FTL before\nswitching to ALGWC (with tsw = n if it never switches). SMART incurs regret bounded by\nReg(SMART, ℓn) ≤ Reg(FTL, ℓtsw) + Reg(ALGWC, ℓn\ntsw+1) ≤ θ + Reg(ALGWC, ℓn\ntsw+1) + 1.\nProof. We separately bound the regret of SMART before the switch and after the switch,\nReg(SMART, ℓn) =\n\u0000 Ptsw\nt=1 ℓt(at) − Ptsw\nt=1 ℓt(a∗\nn)\n\u0001\n+\n\u0000 Pn\nt=tsw+1 ℓt(at) − Pn\nt=tsw+1 ℓt(a∗\nn)\n\u0001\n(a)\n≤\n\u0000 Ptsw\nt=1 ℓt(at) − Ptsw\nt=1 ℓt(a∗\ntsw)\n\u0001\n+\n\u0000 Pn\nt=tsw+1 ℓt(at) − Pn\nt=tsw+1 ℓt(a∗\ntsw+1:n)\n\u0001\n= Reg(FTL, ℓtsw) + Reg(ALGWC, ℓn\ntsw+1).\n9\nThe first term is upper bounded by Reg(FTL, ℓtsw) since Ltsw(a∗\nn) ≥ Ltsw(a∗\ntsw) by definition, as a∗\ntsw\nis the minimizer of Ltsw, and furthermore SMART always plays according to FTL in rounds up to\ntsw. The second term is upper bounded by Reg(ALGWC, ℓn\ntsw+1) because Pn\nt=tsw+1 ℓt(a∗\ntsw+1:n) ≤\nPn\nt=tsw+1 ℓt(a∗\nn) as a∗\ntsw+1:n is the minimizer of the losses after tsw, and at time t > tsw, SMART\nplays according to ALGWC on the sequence of losses limited to ℓn\nt+1. This illustrates the important\nrole of resetting the losses after the switch as depicted in Figure 2(b). Using the fact that ΣFTL\ntsw−1 ≤ θ\nand Ltsw−1(x∗\ntsw−1) ≤ Ltsw−1(x∗\ntsw), it follows that\nReg(FTL, ℓtsw) = ΣFTL\ntsw−1 + Ltsw−1(a∗\ntsw−1) − Ltsw−1(x∗\ntsw) + ℓtsw(a∗\ntsw−1) − ℓtsw(x∗\ntsw) ≤ θ + 1.\nThe reduction to ski-rental is again immediate due to the properties that ΣFTL\nt\n:= Reg(FTL, ℓtsw)\nis adapted, monotone, and is an anytime lower bound for Reg(FTL, ℓn), while remaining an upper\nbound on the true regret incurred by FTL up to time t. As a result, the algorithm can pretend\nthat it truly observes the regret it incurs at each time up to the switching time tsw. After the\nswitch, SMART incurs regret Reg(ALGWC, ℓn\ntsw+1) which is upper bounded by g(n − tsw) ≤ g(n) by\nassumption.\nProof of Theorem 1. This follows immediately from Lemma 2. For ℓn such that Reg(FTL, ℓn) ≤ g(n),\nSMART will never switch to Cover as ΣFTL\nt\n≤ Reg(FTL, ℓn) ≤ g(n) such that Reg(SMART, ℓn) =\nmin{Reg(FTL, ℓn), g(n)}. For ℓn such that Reg(FTL, ℓn) > g(n), by Lemma 2, Reg(SMART, ℓn) ≤\ng(n) + g(n − tsw) + 1 ≤ 2g(n) + 1.\nProof of Theorem 2. The proof uses a primal-dual approach, similar to that of Karlin et al. [1994] for\nski-rental. For a given sequence of loss functions ℓn, we use the shorthand r = Reg(FTL, ℓn) and g =\ng(n). Also, for our given choice of cumulative distribution function Fn, the corresponding probability\ndensity function is given by f(z) =\nez/g\ng(e−1) for z ∈ [0, g]. As before, let tsw := min1≤t≤n−1 ΣFTL\nt\n> θ\nbe the (random) round where SMART switches from FTL to ALGWC (with tsw = n if it never\nswitches). Then by Lemma 2 we have\nReg(SMART, ℓn) − 1\nmin{Reg(FTL, ℓn), g(n)} ≤\n\n\n\nθ+g\nmin{r,g}\nif tsw < n\n1\nif tsw = n\nwhere the second case follows from the fact that θ ∈ [0, g], and hence if we never switch, then\nr ≤ g(n). Taking expectation over θ, we have\nEθ\n\u0002\nReg(SMART, ℓn)\n\u0003\n− 1\nmin{Reg(FTL, ℓn), g}\n≤\n\n\n\nR r\n0\nx+g\nr f(x)dx + 1 − F(r)\nif r ≤ g\nR g\n0\nx+g\ng f(x)dx\nif r > g\nLet ϕ(z) :=\nR z\n0\n(x+g)\nz\nf(x)dx + 1 − F(z) for z ∈ [0, g]; then\nEθ[Reg(SMART,ℓn)]−1\nmin{Reg(FTL,ℓn),g}\n≤ maxz∈[0,g] ϕ(z).\nMoreover, we can differentiate to get z2ϕ′(z) = gzf(z) −\nR z\n0 (x + g)f(x)dx. Substituting our choice\nof f in this expression, we get\nz2dϕ(z)\ndz\n= zez/g\n(e − 1) −\nZ z\n0\n(x + g)\nex/g\ng(e − 1)dx = zez/g − g\nR z/g\n0\n(w + 1)ewdw\n(e − 1)\n= 0\nThus, ϕ(z) is constant for all z ∈ [0, g] and ϕ(g) =\n1\ng(e−1)\nR g\n0 (1 + x/g)ex/gdx =\ne\ne−1.\n10\n3\nInstance Optimal Online Learning: Converse\nIn this section, we investigate fundamental limits on the instance-optimal regret guarantees achievable\nby any algorithm. More precisely, in the setting of binary prediction, we ask what is the smallest\nvalue of γn satisfying\nReg(ALG, yn) ≤ γn min{Reg(FTL, yn), Reg(Cover, yn)} = γn min\nn\n1\n2c(yn−1), fn\no\n(6)\nfor all yn, where fn := Reg(Cover, yn) = p n\n2π(1 + o(1)). We show the following lower bound.\nTheorem 3 (Lower bound on the competitive ratio).\nlim\nn→∞ γn ≥\n\u0010\n1 − e−1/π + 2Q\n\u0000p\n2/π\n\u0001\u0011−1\n≈ 1.4335\nwhere the Q(·) function is Q(x) :=\n1\n√\n2π\nR ∞\nx e−t2/2.\nSince binary prediction is a specific online learning problem, this also yields a fundamental lower\nbound for instance-optimality for general online learning. Note particularly that γn > 1, implying\nthat (1 + o(1)) min{Reg(FTL), Reg(ALGWC)} regret is not possible to achieve. Thus, there is an\ninevitable multiplicative factor that must be paid in order to achieve an instance-optimal regret\nguarantee.\nAn equivalent way to state (6) is to find the smallest γn for which a predictor {at(yt−1)}n\nt=1\nsatisfies for all y ∈ {0, 1}n\nPn\nt=1 |at(yt−1) − yt| ≤ γn min\n\b 1\n2c(yn−1), fn\n\t\n+ min\n\b Pn\nt=1 yi, n − Pn\nt=1 yi\n\t\n.\n(7)\nIn order to establish the values of γn for which the loss function in the right hand side of (7) are\nachievable, we utilize the following result of Cover [1966], which provides an exact characterization of\nthe set of all loss functions achievable in binary prediction. Formally, we say a function ϕ : {0, 1}n →\nR+ is achievable in binary prediction if there exists a predictor/strategy at : yt−1 7→ [0, 1] that\nensures Pn\nt=1 |at(yt−1)−yt| = ϕ(yn)\n, ∀ yn ∈ {0, 1}n. Then, we have the following characterization.\nTheorem 4 (Cover [1966]). Let ϵn ∼ Bern\n\u0010\n1\n2\n\u0011\ni.i.d. For ϕ to be achievable, it must satisfy the\nfollowing:\n• Balance: E[ϕ(ϵn)] = n\n2 .\n• Stability: Let ϕt(yt) := E[ϕ(ytϵn\nt+1)]; then |ϕt(yt−10) − ϕt(yt−11)| ≤ 1 ∀t ∈ [n], yt ∈ {0, 1}t.\nFurther any ϕ satisfying the above is realized by predictor at(yt−1) = 1+ϕt(yt−10)−ϕt(yt−11)\n2\n.\nAs an immediate corollary, Theorem 4 equips us with the exact minimax optimal algorithm for\nbinary prediction alluded to in Section 1. Returning to our setting, from the balance condition\nin Theorem 4, for ϵn ∼ Bern(1/2) i.i.d.\nγn E\n\u0002\nmin\n\b 1\n2c(ϵn−1), fn\n\t\u0003\n+ E\n\u0002\nmin\n\b Pn\nt=1 ϵt, n − Pn\nt=1 ϵt\n\t\u0003\n≥ n\n2 .\nfor the function in (7) to be achievable. Using the definition of fn,\nγn ≥\nfn\nE\n\u0014\nmin\nn\n1\n2c(ϵn−1), fn\no\u0015 =\n2fn\nE\nh\nmin\n\b\nc(ϵn−1), 2fn\n\t\n]\ni.\nThe above bound immediately yields that γn ≥ 1 as expected. We can further sharply characterize\nthe asymptotics of γn, resulting in the stated lower bound. The full proof of Theorem 3 is provided\nin Appendix B.\n11\n4\nInstance-Optimal Algorithms in Small-Loss Settings\nSo far, we have presented specializations of SMART that achieve instance-optimality between FTL\nand the worst-case regret g(n). However, many worst-case algorithms can still adapt to the instance\nℓn and achieve regret guarantees that are a function of the ‘difficulty’ of the instance ℓn. A common\nway to quantify this is difficulty is via small-loss bounds, where the regret is upper bounded by\ng(L∗) where g(·) as earlier is a monotonic increasing function and L∗ := mina∈A\nPn\nt=1 ℓt(a) is the\nloss achieved by the best action. Such guarantees imply that for sequences where there exists an\naction achieving low loss, the corresponding regret achieved is also low. Thus, a natural question is\nwhether SMART can be specialized to yield an algorithm that is constant competitive with respect\nto min{Reg(FTL, ℓn), g(L∗)}.\nAs a starting point, if L∗ is known apriori, it is easy to achieve a\ne\ne−1 approximation by simply\nusing SMART with (random) threshold θ = g(L∗) ln(1 + (e − 1)U), U ∼ Unif[0, 1]; this is an\nimmediate corollary of Theorem 2. When L∗ is not known, we use a guess-and-double argument to\ndevise an algorithm that achieves the following instance-optimality guarantee.\nTheorem 5 (Regret of SMART for unknown small loss). Let ALGWC have small loss regret guarantees\nsatisfying Reg(ALGWC, ℓn) ≤ g(L∗) for any ℓn where L∗ = minj∈[m] Ltj, i.e. the loss achieved by the\nbest expert in hindsight. Then, if we play SMART for Small-Loss as stated in Algorithm 2, we have\nReg(SMART, ℓn) ≤ 2 min\n\u0000Reg(FTL, ℓn), Plog(1+L∗/ log m)+1\nz=1\ng(2z log m)\n\u0001\n+ O(log L∗/ log m)\nIn particular, in the prediction with expert advice setting, we know that Hedge with a time-varying\nlearning rate achieves g(L∗) ≡ 2√2L∗ log m + κ log m (where κ > 0 is an absolute constant) [Auer\net al., 2002b, Cesa-Bianchi et al., 2007]; this gives Corollary 1 in Section 1.\nThe intuitive idea behind the algorithm is to guess the value of L∗, and play SMART with\nthis guessed value while simultaneously keeping track of the regret incurred. Whenever the regret\nincurred exceeds the guarantee established by SMART with known L∗ double the guessed value and\nstart again. We use the notation ALGWC(ℓt2\nt1) to refer to the worst-case algorithm when the previously\nobserved sequence is ℓt2\nt1; in particular this would be equivalent to the action recommended at time\nt2 +1 after throwing away all the observed losses before t1. We let ΣFTL\nt1:t2 = Pt2\ni=t1(Li(a∗\ni−1)−Li(a∗\ni )),\nwhich grows as the number of leader changes within i ∈ [t1, t2]. The algorithm’s pseudocode is given\nin Algorithm 2 below, and a proof of Theorem 5 is provided in Appendix A.\n5\nConclusion\nIn this paper, we present SMART, a simple and black-box online learning algorithm that adapts\nto the data and achieves instance optimal regret with respect to FTL and any given worst-case\nalgorithm. We show that SMART only switches once from FTL to the worst-case algorithm, and\nattains a regret that is within a factor of e/(e − 1) ≈ 1.58 of the minimum of the regret of FTL\nand the minimax regret over all input sequences; we also show that any algorithm must incur an\nextra factor of at least 1.43 establishing that our simple approach is surprisingly close to optimal.\nFurthermore, we extend SMART to incorporate a small-loss algorithm and obtain instance optimality\nwith respect to the small-loss regret bound. Our approach relies on a novel reduction of instance\noptimal online learning to the ski-rental problem, and leverages tools from information theory and\ncompetitive analysis. Our work suggests several open problems for future research, such as finding\ninstance optimal algorithms for bandit settings, or designing algorithms that can adapt to multiple\nreference algorithms besides FTL and minimax algorithms.\n12\nAlgorithm 2: SMART for Small-Loss\nInput: Policies FTL, ALGWC; Small-loss bound g(·)\nfor z = 0, 1, . . . (epochs) do\nLet t = tz :=start time of zth epoch, L∗\nz := 2z log m (current guess for L∗), ΣFTL\ntz:tz−1 = 0 ;\nwhile ΣFTL\ntz:t−1 ≤ g(L∗\nz) do\nSet at = a∗\nt−1;\n// Play FTL\nObserve ℓt(·);\nUpdate Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL\ntz:t = ΣFTL\ntz:t−1 + (Lt(a∗\nt−1) − Lt(a∗\nt )) and t = t + 1;\nend\nLet τz := mint≥tz ΣFTL\ntz:t > g(L∗\nz) and t = τz + 1;\n// Check if loss incurred by ALGWC in this epoch violates the upper\nbound from L∗\nz is correct\nwhile Pt\nt=tz⟨at, ℓt⟩ ≤ L∗\nz + 2 min{ΣFTL\ntz:t , g(L∗\nz)} + 1 do\nSet at = ALGWC(ℓt−1\nτz+1) ;\n// Play ALGWC forgetting losses before τz + 1\nObserve ℓt(·);\nUpdate Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL\ntz:t = ΣFTL\ntz:t−1 + (Lt(a∗\nt−1) − Lt(a∗\nt )) and t = t + 1;\nend\nend\nAcknowledgements\nThis work is supported by NSF grants CNS-1955997, CCF-2337796 and ECCS-1847393, and AFOSR\ngrant FA9550-23-1-0301. This work was partially done when the authors were visitors at the Simons\nInstitute for the Theory of Computing, UC Berkeley.\nA\nOmitted proofs from Section 4\nIn this Section, we will establish the proofs of Theorem 5 and Corollary 1.\nRecall Algorithm 2, where ALGWC(ℓt2\nt1) refers to the worst-case algorithm when the previously\nobserved sequence is ℓt2\nt1; in particular this would be equivalent to the action recommended at time\nt2 +1 after throwing away all the observed losses before t1. We let ΣFTL\nt1:t2 = Pt2\ni=t1(Li(a∗\ni−1)−Li(a∗\ni )),\nwhich grows as the number of leader changes within i ∈ [t1, t2].\nWe first have the following decomposition of the regret for any algorithm ALG that plays the\nsequence of actions aALG\nt\nat time t.\nLemma 3. The regret incurred by any sequence of actions (aALG\nt\n)t∈[n+1] can be written as\nReg(ALG, ℓn) =\nn\nX\nt=1\n\u0010\nLt(aALG\nt\n) − Lt(aALG\nt+1 )\n\u0011\n,\n(8)\nwhere we let aALG\nn+1 := a∗\nn.\nProof.\nLn(ALG) =\nn\nX\nt=1\nℓt(aALG\nt\n)\n(9)\n13\n=\nn\nX\nt=1\n\u0010\nLt(aALG\nt\n) − Lt−1(aALG\nt\n)\n\u0011\n(10)\n= Ln(aALG\nn\n) +\nn−1\nX\nt=1\n\u0010\nLt(aALG\nt\n) − Lt(aALG\nt+1 )\n\u0011\n− L0(aALG\n1\n).\n(11)\nThis implies a regret decomposition of\nReg(ALG, ℓn) = Ln(ALG) − Ln(a∗\nn)\n(12)\n= Ln(aALG\nn\n) − Ln(a∗\nn) +\nn−1\nX\nt=1\n\u0010\nLt(aALG\nt\n) − Lt(aALG\nt+1 )\n\u0011\n− L0(aALG\n1\n)\n(13)\nAs L0(aALG\n1\n) = 0, and aALG\nn+1 := a∗\nn, it follows that\nReg(ALG, ℓn) =\nn\nX\nt=1\n\u0010\nLt(aALG\nt\n) − Lt(aALG\nt+1 )\n\u0011\n.\n(14)\nNext, we use this decomposition to establish the regret of any algorithm ALG that alternates\nbetween playing FTL and another algorithm ALGWC.\nLemma 4. Consider an algorithm ALG which alternates between playing FTL and ALGWC, where FTL\nis played in the intervals {[tz, τz]}z∈[zlast], and ALGWC is played in intervals {[τz +1, tz+1 −1]}z∈[zlast].\nThe regret of ALG is bounded by\nReg(ALG, ℓn) ≤\nX\nz\n\u0010\nΣFTL\ntz:τz−1 + Reg(ALGWC, ℓtz+1−1\nτz+1\n) + 1\n\u0011\n.\n(15)\nProof. We let tzlast = n + 1, and an+1 = a∗\nn. We use Lemma 3 and rearrange the terms by grouping\nthem by the FTL periods and the ALGWC periods.\nReg(ALG, ℓn)\n=\nX\nz\n\n\ntz+1−1\nX\nt=tz\n(Lt(at) − Lt(at+1))\n\n\n=\nX\nz\n\n\nτz−1\nX\nt=tz\n(Lt(a∗\nt−1) − Lt(a∗\nt )) + Lτz(a∗\nτz−1) − Lτz(aτz+1) +\ntz+1−1\nX\nt=τz+1\n(Lt(at) − Lt(at+1))\n\n\n=\nX\nz\n\n\nτz−1\nX\nt=tz\n(Lt(a∗\nt−1) − Lt(a∗\nt )) + Lτz(a∗\nτz−1) +\ntz+1−1\nX\nt=τz+1\nℓt(at) − Ltz+1−1(atz+1)\n\n\n=\nX\nz\nτz−1\nX\nt=tz\n(Lt(a∗\nt−1) − Lt(a∗\nt )) +\nX\nz\n\n\ntz+1−1\nX\nt=τz+1\nℓt(at) + Lτz(a∗\nτz−1) − Ltz+1−1(a∗\ntz+1−1)\n\n .\nWe bound the first term by the FTL regret. Recall the notation\nΣFTL\ntz:τz−1 :=\nτz−1\nX\nt=tz\n(Lt(a∗\nt−1) − Lt(a∗\nt )).\n(16)\n14\nBecause we are playing FTL at both time τz and τz − 1, it holds that\nLτz(a∗\nτz−1) = Lτz−1(a∗\nτz−1) + ℓτz(a∗\nτz−1) ≤ Lτz(a∗\nτz) + 1.\nTo bound the second term, we will show that\ntz+1−1\nX\nt=τz+1\nℓt(at) + Lτz(a∗\nτz−1) − Ltz+1−1(a∗\ntz+1−1)\n≤\ntz+1−1\nX\nt=τz+1\nℓt(at) + Lτz(a∗\nτz) − Ltz+1−1(a∗\ntz+1−1) + 1\n(17)\n=\ntz+1−1\nX\nt=τz+1\nℓt(at) + min\na Lτz(a) − min\na Ltz+1−1(a) + 1\n(18)\n≤\ntz+1−1\nX\nt=τz+1\nℓt(at) − min\na\n\u0000Ltz+1−1(a) − Lτz(a)\n\u0001\n+ 1\n(19)\n= Reg(ALGWC, ℓtz+1−1\nτz+1\n) + 1.\n(20)\nWe now complete the proof of Theorem 3 by showing that the conditions that determine the\nswitching time between FTL and ALGWC are appropriately chosen to upper bound ΣFTL\ntz:τz−1 and\nReg(ALGWC, ℓtz+1−1\nτz+1\n). Firstly, note that for any z < zlast, Reg(ALGWC, ℓtz+1−1\nτz+1\n) > g(L∗\nz), which\nimplies that L∗ > L∗\nz. In particular, the epoch zlast − 1 was exited, which implies that4\n2zlast−1 log m ≤ L∗ =⇒ zlast ≤ log\n\u0012 L∗\nlog m\n\u0013\n+ 1\nBy the stopping condition of the epoch, Reg(ALGWC, ℓtz+1−1\nτz+1\n) ≤ Reg(ALGWC, ℓtz+1−2\nτz+1\n) + 1 ≤\ng(L∗\nz) + 1, such that substituting into Lemma 4 implies\nReg(SMART, ℓn) ≤\nX\nz\n\u0010\nΣFTL\ntz:τz−1 + g(L∗\nz) + 2\n\u0011\n.\n(21)\nAlso, it always holds that ΣFTL\ntz:τz−1 ≤ g(L∗\nz) and for z < zlast, ΣFTL\ntz:τz > g(L∗\nz). Therefore\nzlast−1\nX\nz\ng(L∗\nz) <\nX\nz\nΣFTL\ntz:τz−1 ≤ Reg(FTL, ℓn)\nand P\nz ΣFTL\ntz:τz−1 ≤ Pzlast\nz\ng(L∗\nz).\nTo put it all together, if Pzlast\nz\ng(L∗\nz) ≤ Reg(FTL, ℓn), then\nReg(SMART, ℓn) ≤ 2\nX\nz\ng(L∗\nz) + 2zlast.\n(22)\n4Here we have implicitly assumed that L∗ > log m—if not, then there is only one epoch, zlast = 0 and the result is\nreadily implied by Corollary 2.\n15\nIf Reg(FTL, ℓn) < Pzlast\nz\ng(L∗\nz), then it must be that in the last epoch the algorithm never switches\nto ALGWC. If it switched to ALGWC it would imply that Reg(FTL, ℓn) ≥ P\nz ΣFTL\ntz:τz > Pzlast\nz\ng(L∗\nz)\nwhich would violate the assumption that Reg(FTL, ℓn) < Pzlast\nz\ng(L∗\nz). Therefore it must be that\nReg(SMART, ℓn) ≤ 2Reg(FTL, ℓn) + 2zlast.\n(23)\nAs a result, it follows that (putting the L∗ ≤ log m and the L∗ > log m cases together)\nReg(SMART, ℓn)\n≤ 2 min\n\n\n\nReg(FTL, ℓn),\nlog\n\u0010\n1+\nL∗\nlog m\n\u0011\n+1\nX\nz=0\ng(2z log m)\n\n\n\n + 2 log\n\u0012\n1 +\nL∗\nlog m\n\u0013\n+ 2.\n(24)\nA.1\nProof of Corollary 1\nThis follows from Theorem 5 and by calculating\nlog\n\u0010\n1+\nL∗\nlog m\n\u0011\n+1\nX\nz=0\ng(2z log m)\n= 2\n√\n2 log m\nlog\n\u0010\n1+\nL∗\nlog m\n\u0011\n+1\nX\nz=0\n2z/2 + κ log m log\n\u0012\n1 +\nL∗\nlog m\n\u0013\n+ κ log m\n≤ log m 4\n√\n2\n√\n2 − 1\ns\n1 +\nL∗\nlog m + κ log m log\n\u0012\n1 +\nL∗\nlog m\n\u0013\n+ κ log m\n≤ 10\nq\n2 log2 m + 2L∗ log m + κ log m log\n\u0012\n1 +\nL∗\nlog m\n\u0013\n+ κ log m\n(a)\n≤ 10\np\n2L∗ log m + κ log\n\u0012\n1 +\nL∗\nlog m\n\u0013\nlog m + 10\n√\n2 log m + κ log m\nwhere (a) follows since for nonnegative a, b\n√\na + b ≤ √a +\n√\nb.\nB\nProof of Theorem 3\nConsider a large even n. We then have for horizon size n + 1, Reg(FTL, yn+1) = c(yn)\n2\n. Moreover,\nReg(Cover, yn+1) = fn+1. Let5\npn,k := P[c(ϵn) = k + 1].\n(25)\nWe then have,\nE[min{c(ϵn), 2fn+1}] =\nn\nX\nk=1\nmin{k, 2fn+1} Pr[c(ϵn) = k]\n=\nn\nX\nk=1\nmin{k, 2fn+1}pn,k−1\n5Note that k in this definition does not counting the origin as as a line crossing.\n16\n=\nn\nX\nk=0\nmin{k + 1, 2fn+1}pn,k\n=\n⌊2fn+1⌋−1\nX\nk=0\n(k + 1)pn,k + 2fn+1P[c(ϵn) ≥ 2fn+1]\n=\n⌊2fn+1⌋−1\nX\nk=0\n(k + 1)pn,k + 2fn+1P[c(ϵn) ≥ 2fn+1] + P[c(ϵn) ≤ ⌊2fn+1⌋]\n=\n⌊2fn+1⌋−1\nX\nk=0\nkpn,k + 2fn+1\n\n1 −\n⌊2fn+1⌋−1\nX\nk=0\npn,k\n\n + P[c(ϵn) ≤ ⌊2fn+1⌋]\n(26)\nUpon dividing (26) by 2fn+1, we get\nE[min{c(ϵn), 2fn+1}]\n2fn+1\n=\nP⌊2fn+1⌋−1\nk=0\nkpn,k\n2fn+1\n+\n\n1 −\n⌊2fn+1⌋−1\nX\nk=0\npn,k\n\n + P[c(ϵn) ≤ ⌊2fn+1⌋]\n2fn+1\n.\n(27)\nNote that the third term vanishes since fn+1 → ∞ and 0 ≤ P[·] ≤ 1. We will now separately\nevaluate the first two terms in (27). To do this, we require an auxiliary lemma, the proof of which\nis provided later.\nLemma 5. If k ≤ C√n for an absolute constant C, then for large enough n (n ≥ 32C2 suffices) we\nhave\ne− 16C3\n√n ≤\npn,k\nq\n2\nnπe−k2/2n\n≤\ns\n1 − C/√n\n1 − 2C/√ne\n16C3\n√n .\n(28)\nThat is,\npn,k =\nr\n2\nnπe−k2/2n(1 + o(1)).\nWe now evaluate the first term in (27). Since ⌊2fn+1⌋−1 ≤ 2√n, we invoke Lemma 5 to evaluate\n⌊2fn+1⌋−1\nX\nk=0\nkpn,k = (1 + o(1))\n⌊2fn+1⌋−1\nX\nk=0\nk\nr\n2\nnπe− k2\n2n\n(29)\n= (1 + o(1))\nr\n2\nnπ\n⌊2fn+1⌋−1\nX\nk=0\nke− k2\n2n\n(30)\nNow, we note that since x 7→ xe− x2\n2n is increasing on (0, ⌊2fn+1⌋ − 1) , by a Riemann approximation,\nwe have that\nZ 2fn+1−2\n0\nxe− x2\n2n dx − 1 ≤\n⌊2fn+1⌋−1\nX\nk=0\nke− k2\n2n ≤\nZ 2fn+1\n0\nxe− x2\n2n dx\n(31)\nand evaluating\nZ 2fn+1\n0\nxe− x2\n2n dx = 1\n2\nZ 4f2\nn+1\n0\ne− t\n2n dt\n17\n= n\n \n1 − e−\n4f2\nn+1\n2n\n!\n= n\n\u0010\n1 − e− 1\nπ (1+o(1))\u0011\n.\n(32)\nEvaluating the lower bound in (31) analogously we have\n⌊2fn+1⌋−1\nX\nk=0\nke− k2\n2n = (1 + o(1))n\n\u0010\n1 − e− 1\nπ (1+o(1))\u0011\n(33)\nand therefore from (30),\nP⌊2fn+1⌋−1\nk=0\nkpn,k\n2fn+1\n= (1 + o(1))\n\u0010\n1 − e− 1\nπ (1+o(1))\u0011\nn\nq\n2\nnπ\nq\n2(n+1)\nπ\n= (1 + o(1))\n\u0010\n1 − e− 1\nπ (1+o(1))\u0011\n(34)\nand therefore, from (34), we have that\nlim\nn→∞\nP⌊2fn+1⌋−1\nk=0\nkpn,k\n2fn+1\n= 1 − e− 1\nπ .\n(35)\nWe now address the second term in (27) by invoking Lemma 5 and noting that\n⌊2fn+1⌋−1\nX\nk=0\npn,k = (1 + o(1))\nr\n2\nnπ\n⌊2fn+1⌋−1\nX\nk=0\ne− k2\n2n\n(a)\n= (1 + o(1))\nZ 2fn+1\n0\ne− x2\n2n dx\n= (1 + o(1))\nr\n2\nπ\nZ q\n2\nπ\n0\ne− t2\n2 dt\n=\n1\n√\n2π\nZ q\n2\nπ\n−\nq\n2\nπ\ne− t2\n2 dt\n= P\n \n−\nr\n2\nπ ≤ X ≤\nr\n2\nπ\n!\n(36)\nwhere in (36) X ∼ N(0, 1). Then,\n1 −\n⌊2fn+1⌋−1\nX\nk=0\npn,k → 1 − P\n \n−\nr\n2\nπ ≤ X ≤\nr\n2\nπ\n!\n= 2Q\n r\n2\nπ\n!\n.\n(37)\nSubstituting (35) and (37) in (27) yields that\n1\nγn\n= E[min{c(ϵn), 2fn+1}]\n2fn+1\n→ 1 − e− 1\nπ + 2Q\n r\n2\nπ\n!\n(38)\n18\nand therefore\nγn →\n1\n1 − e− 1\nπ + 2Q\n\u0012q\n2\nπ\n\u0013.\n(39)\nB.1\nProof of Lemma 5\nWe first note the following.\nProposition 1 (Feller, Chapter 3, Exercise 11).\npn,k =\n1\n2n−k\n\u0012n − k\nn/2\n\u0013\n.\nTherefore,\npn,k2n\n2k\n=\n\u0012n − k\nn/2\n\u0013\n=\n(n − k)!\nn\n2 !\n\u0000 n\n2 − k\n\u0001\n!.\nWe now use the Stirling approximation:\n√\n2πm\n\u0012m\ne\n\u0013m\ne\n1\n12m+1 ≤ m! ≤\n√\n2πm\n\u0012m\ne\n\u0013m\ne\n1\n12m .\n(40)\nUsing (40) we have\npn,k2n\n2k\n≤\np\n2π(n − k)\np\n2π(n/2)\np\n2π(n/2 − k)\n·\n(n − k)n−k\n(n/2)n/2(n/2 − k)n/2−k\n· exp\n\u0012\n1\n12(n − k) −\n1\n6n + 1 −\n1\n6n − 12k + 1\n\u0013\n=\nr\n2\nnπ\nr\nn − k\nn − 2k\n(n − k)n−k2n−k\nnn/2(n − 2k)n/2−k · exp\n\u0012\n1\n12n − k\n\u0013\n(a)\n≤\nr\n2\nnπ · 2n−k ·\n(n − k)n−k\nnn/2(n − 2k)n/2−k exp\n\u0012\n1\n12n − k\n\u0013\n·\ns\n1 − C/√n\n1 − 2C/√n\n=\nr\n2\nnπ · 2n−k\n(1 − k/n)n−k\n(1 − 2k/n)n/2−k\n|\n{z\n}\n=:T\nexp\n\u0012\n1\n12n − k\n\u0013\n·\ns\n1 − C/√n\n1 − 2C/√n.\n(41)\nWe now analyze the term T in (41) in more detail.\nProposition 2.\n− k2\n2n2 − ck3\nn2 ≤ ln T ≤ − k2\n2n2 + ck3\nn2\n(42)\nwhere c ≤ 15.\nProof. By Taylor theorem, we have\nln(1 − x) = −x − x2\n2 −\nx3\n3(1 − µ)2 for µ ∈ (0, x).\n(43)\n19\nTherefore, for k ≤ C√n and n ≥ 16C2 we have\nln\n\u0012\n1 − k\nn\n\u0013\n= −k\nn − k2\n2n2 − α1k3\nn3\n(44)\nln\n\u0012\n1 − 2k\nn\n\u0013\n= −2k\nn − 2k2\n2n2 − α2k3\nn3\n(45)\nfor α1, α2 ∈\n\u0010\n1\n3, 8\n3\ni\n. Evaluating ln T we have\nln T = (n − k) ln\n\u0012\n1 − k\nn\n\u0013\n−\n\u0012\nn − k\n2\n\u0013\nln\n\u0012\n1 − 2k\nn\n\u0013\n(a)\n= (n − k)\n \n−k\nn − k2\n2n2 − α1k3\nn3\n!\n−\n\u0012\nn − k\n2\n\u0013  \n−2k\nn − 2k2\n2n2 − α2k3\nn3\n!\n=\n \n− k2\n2n + k2\nn + k2\nn − 2k2\nn\n!\n+\n\u0012\n−α1 + 1\n2 + α2\n2 − 2\n\u0013 k3\nn2 + (α1 − α2) k4\nn3\n= − k2\n2n2 + c1\nk3\nn2 + c2\nk4\nn3\n(46)\nwhere (a) follows by substituting (44) and (45). Now, since k4\nn3 ≤ k3\nn2 we have\nln T ≤ − k2\n2n2 + (|c1| + |c2|)k3\nn2\n(47)\nand on the other hand for the same reason\nln T ≥ − k2\n2n − (|c1| + |c2|)k3\nn2\n(48)\nThe proposition follows by noticing |c1| + |c2| ≤ 15 by using that α1, α2 ∈\n\u0010\n1\n3, 8\n3\ni\n.\nUsing Proposition 2 in (41) we have the upper bound\npn,k ≤\nr\n2\nnπ · exp\n \n− k2\n2n + 15k3\nn2\n!\n· exp\n\u0012\n1\n12n − k\n\u0013\n·\ns\n1 − C/√n\n1 − 2C/√n\n(a)\n≤\nr\n2\nnπ · exp\n \n− k2\n2n\n!\n· exp\n \n16k3\nn2\n!\n·\ns\n1 − C/√n\n1 − 2C/√n\n(b)\n≤\nr\n2\nnπ · exp\n \n− k2\n2n\n!\n· exp\n \n16C3\n√n\n!\n·\ns\n1 − C/√n\n1 − 2C/√n\n(49)\nwhere (a) uses\n1\n12n−k + 15k3\nn2 ≤ 16k3\nn2 , and (b) uses the fact that k ≤ C√n, which yields the upper bound\nin (28). The lower bound follows analogously by the Stirling approximation (40) and Proposition 2.\nReferences\nRonald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation algorithms for middleware.\nIn Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of\ndatabase systems, pages 102–113, 2001.\n20\nTim Roughgarden. Beyond the worst-case analysis of algorithms. Cambridge University Press, 2021.\nD Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathematics,\n6(1):1–8, 1956.\nJames Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of\nGames, 3:97–139, 1957.\nNicolo Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge university\npress, 2006.\nAleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Machine\nLearning, 12(1-2):1–286, 2019.\nThomas M. Cover. Behavior of sequential predictors of binary sequences. In Transactions of the\nFourth Prague Conference on Information Theory, 1966.\nRuitong Huang, Tor Lattimore, Andr´as Gy¨orgy, and Csaba Szepesv´ari. Following the leader and\nfast rates in linear prediction: Curved constraint sets and other regularities. Advances in Neural\nInformation Processing Systems, 29, 2016.\nMeir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences. IEEE\ntransactions on Information Theory, 38(4):1258–1270, 1992.\nAlekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of\nbandit algorithms. In Conference on Learning Theory, pages 12–38. PMLR, 2017.\nAldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and\nCsaba Szepesvari. Model selection in contextual stochastic bandit problems. Advances in Neural\nInformation Processing Systems, 33:10328–10337, 2020.\nChristoph Dann, Chen-Yu Wei, and Julian Zimmert. Best of both worlds policy optimization. arXiv\npreprint arXiv:2302.09408, 2023.\nPeter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed\nbandit problem. SIAM journal on computing, 32(1):48–77, 2002a.\nSteven De Rooij, Tim Van Erven, Peter D Gr¨unwald, and Wouter M Koolen. Follow the leader if\nyou can, hedge if you must. The Journal of Machine Learning Research, 15(1):1281–1316, 2014.\nFrancesco Orabona and D´avid P´al. Scale-free algorithms for online linear optimization. In Interna-\ntional Conference on Algorithmic Learning Theory, pages 287–301. Springer, 2015.\nJaouad Mourtada and St´ephane Ga¨ıffas. On the optimality of the hedge algorithm in the stochastic\nregime. Journal of Machine Learning Research, 20:1–28, 2019.\nBlair Bilodeau, Jeffrey Negrea, and Daniel M Roy. Relaxing the iid assumption: Adaptively minimax\noptimal regret via root-entropic regularization. The Annals of Statistics, 51(4):1850–1876, 2023.\nS´ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial\nbandits.\nIn Conference on Learning Theory, pages 42–1. JMLR Workshop and Conference\nProceedings, 2012.\n21\nJulian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits.\nIn The 22nd International Conference on Artificial Intelligence and Statistics, pages 467–475.\nPMLR, 2019.\nThodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial\ncorruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing,\npages 114–122, 2018.\nWojciech Kot lowski. On minimaxity of follow the leader strategy in the stochastic setting. Theoretical\nComputer Science, 742:50–65, 2018.\nAnna R. Karlin, Mark S. Manasse, Lyle A. McGeoch, and Susan Owicki. Competitive randomized\nalgorithms for nonuniform problems. Algorithmica, 11(6):542–571, 1994.\nAllan Borodin and Ran El-Yaniv. Online computation and competitive analysis. cambridge university\npress, 2005.\nNicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and\nManfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427–485,\n1997.\nChen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference\nOn Learning Theory, pages 1263–1291. PMLR, 2018.\nS´ebastien Bubeck, Yuanzhi Li, Haipeng Luo, and Chen-Yu Wei. Improved path-length regret bounds\nfor bandits. In Conference On Learning Theory, pages 508–528. PMLR, 2019.\nNeelkamal Bhuyan, Debankur Mukherjee, and Adam Wierman. Best of both worlds: Stochastic\nand adversarial convex function chasing. arXiv preprint arXiv:2311.00181, 2023.\nOron Sabag, Gautam Goel, Sahin Lale, and Babak Hassibi. Regret-optimal controller for the\nfull-information problem. In 2021 American Control Conference (ACC), pages 4777–4782. IEEE,\n2021.\nGautam Goel, Naman Agarwal, Karan Singh, and Elad Hazan. Best of both worlds in online control:\nCompetitive ratio and policy regret. In Learning for Dynamics and Control Conference, pages\n1345–1356. PMLR, 2023.\nAlexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic, constrained,\nand smoothed adversaries. Advances in neural information processing systems, 24, 2011.\nNika Haghtalab, Tim Roughgarden, and Abhishek Shetty.\nSmoothed analysis with adaptive\nadversaries. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS),\npages 942–953. IEEE, 2022.\nAdam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed online learning is as\neasy as statistical learning. In Conference on Learning Theory, pages 1716–1786. PMLR, 2022.\nAlankrita Bhatt, Nika Haghtalab, and Abhishek Shetty. Smoothed analysis of sequential probability\nassignment. Neural Information Processing Systems, 2023.\nIdan Amir, Idan Attias, Tomer Koren, Yishay Mansour, and Roi Livni. Prediction with corrupted\nexpert advice. Advances in Neural Information Processing Systems, 33:14315–14325, 2020.\n22\nPeter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning\nalgorithms. Journal of Computer and System Sciences, 64(1):48–75, 2002b.\nNicolo Cesa-Bianchi, G´abor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction.\nIEEE Transactions on Information Theory, 51(6):2152–2162, 2005.\nElad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation\nin costs. Machine learning, 80:165–188, 2010.\nWouter M Koolen, Tim Van Erven, and Peter Gr¨unwald. Learning the learning rate for prediction\nwith expert advice. Advances in neural information processing systems, 27, 2014.\nTim Van Erven, Peter Grunwald, Nishant A Mehta, Mark Reid, Robert Williamson, et al. Fast\nrates in statistical and online learning. 2015.\nTim Van Erven and Wouter M Koolen. Metagrad: Multiple learning rates in online learning.\nAdvances in Neural Information Processing Systems, 29, 2016.\nPierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In\nConference on Learning Theory, pages 176–196. PMLR, 2014.\nEtienne Bamas, Andreas Maggiori, and Ola Svensson.\nThe primal-dual method for learning\naugmented algorithms. Advances in Neural Information Processing Systems, 33:20083–20094,\n2020.\nMichael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Al-\ngorithms with prediction portfolios. Advances in neural information processing systems, 35:\n20273–20286, 2022.\nKeerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms with multiple\npredictions. In International Conference on Machine Learning, pages 582–598. PMLR, 2022.\nAdam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of\nComputer and System Sciences, 71(3):291–307, 2005.\nNicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction\nwith expert advice. Machine Learning, 66:321–352, 2007.\nWilliam Feller. An introduction to probability theory and its applications, Volume 1, Third Edition.\nJohn Wiley & Sons, New York.\n23\n"
}