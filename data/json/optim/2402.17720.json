{
    "optim": "The SMART Approach to Instance-Optimal Online Learning Siddhartha Banerjee ORIE, Cornell sbanerjee@cornell.edu Alankrita Bhatt CMS, Caltech abhatt@caltech.edu Christina Lee Yu ORIE, Cornell cleeyu@cornell.edu Abstract We devise an online learning algorithm – titled Switching via Monotone Adapted Regret Traces (SMART) – that adapts to the data and achieves regret that is instance optimal, i.e., simultaneously competitive on every input sequence compared to the performance of the follow- the-leader (FTL) policy and the worst case guarantee of any other input policy ALGWC. We show that the regret of the SMART policy on any input sequence is within a multiplicative factor e/(e − 1) ≈ 1.58 of the smaller of: 1) the regret obtained by FTL on the sequence, and 2) the upper bound on regret guaranteed by the given worst-case policy. This implies a strictly stronger guarantee than typical ‘best-of-both-worlds’ bounds as the guarantee holds for every input sequence regardless of how it is generated. SMART is simple to implement as it begins by playing FTL and switches at most once during the time horizon to ALGWC. Our approach and results follow from an operational reduction of instance optimal online learning to competitive anaylsis for the ski-rental problem. We complement our competitive ratio upper bounds with a fundamental lower bound showing that over all input sequences, no algorithm can get better than a 1.43-fraction of the minimum regret achieved by FTL and the minimax-optimal policy. We also present a modification of SMART that combines FTL with a “small-loss” algorithm to achieve instance optimality between the regret of FTL and the small loss regret bound. 1 Introduction Our work aims to develop algorithms for online learning that are instance optimal [Fagin et al., 2001],[Roughgarden, 2021, Chapter 3] with respect to the stochastic and minimax optimal algorithms for a given setting. This is best motivated via a concrete example: Example 1 (Binary Prediction). We are given bit stream yn := y1, y2, . . . , yn ∈ {0, 1}n. At the start of day t, before seeing yt, we choose (possibly randomized) prediction bYt ∼ Bernoulli(at) (for at ∈ [0, 1]) for the upcoming bit yt, given the history yt−1. Our resulting loss on day t is ℓt(at) = P(bYt ̸= yt) = |at − yt|, and our total loss is Ln(ALG, yn) := Pn t=1 ℓt(at). The objective is to achieve low regret (i.e., additive loss) compared to the loss Ln(a, yn) = Pn t=1 ℓt(a) of the best fixed action a∗ ∈ [0, 1] in hindsight. As a∗ is the majority in yn between 0 and 1, it follows that Ln(a∗, yn) = min \bPn t=1 yt, n − Pn t=1 yt \t . Formally, for sequence yn ∈ {0, 1}n, policy ALG incurs regret Reg(ALG, yn) := Ln(ALG, yn) − Ln(a∗, yn) = Pn t=1 |at − yt| − mina∈[0,1] PT t=1 |a − yt|. (1) Binary prediction goes back to the seminal works of Blackwell [1956] and Hannan [1957]. The definition of regret is motivated by the case where yt is randomly generated as i.i.d. Bernoulli(p). If 1 arXiv:2402.17720v1  [cs.LG]  27 Feb 2024 p is known, then the optimal policy is the ‘Bayes predictor’ aBayes = ⌊2p⌋ (i.e., nearest integer to p), which coincides with hindsight optimal a∗ with high probability when p is away from 1/2. When p is unknown, the stochastic optimal policy is the Follow The Leader or FTL policy, which sets at = Majority(yt−1), i.e. the majority bit amongst the first t − 1 bits (at = 1/2 if both are equal1). A starting point for online learning is the observation that it is easy to construct a sequence yn such that FTL has poor regret: For example, if yn = (1, 0, 1, 0, 1, 0, . . .), i.e., alternate 1s and 0s, then the regret of FTL grows linearly with n. In contrast, worst-case optimal online learning policies such as those of Blackwell and Hannan, or more modern versions like Multiplicative Weights or Follow The Perturbed Leader (see Cesa-Bianchi and Lugosi [2006], Slivkins [2019]) guarantee regret of Θ(√n) over all sequences. Indeed, for bit prediction, the exact minimax optimal policy was established by Cover [1966], and this policy (which we refer to as Cover) achieves2 Reg(Cover, yn) = p n 2π(1 + o(1)) under any yn ∈ {0, 1}n, implying it is an equalizer (achieves same regret over all sequences). While the above discussion seems a convincing endorsement of worst-case online learning algorithms, the situation is more complicated. One problem is that while FTL has bad regret on certain pathological sequences, on more ‘realistic’ sequences FTL performs orders of magnitude better than the minimax regret. As an example, with i.i.d. Bernoulli(p) input, Reg(FTL, yn) is actually independent of n (i.e., O(1)) as long as p is away from 1/2 with high probability. We demonstrate this in Figure 1(a), where we see Reg(FTL) is much lower than Reg(Cover) ≈ 0.39√n unless p is very close to 1/2. This phenomena is known in more general settings [Huang et al., 2016], suggesting that in practice one may be better off just using FTL. On the other hand, as Figure 1(b, c) indicates, we know how to generate sequences yn [Feder et al., 1992] for which Reg(FTL, yn) grows linearly with n, and so the √n regret of Cover becomes appealing. Now suppose instead that a fictitious oracle is told beforehand which of FTL or Cover is better suited for the upcoming sequence yn; the demand made by instance optimality is that we try to be competitive against such an oracle on every sequence yn. Definition 1 (Instance Optimality). A binary prediction policy ALG is instance optimal with respect to the regret of FTL and Cover if there exists some universal γn ≥ 1 such that for all yn ∈ {0, 1}n: Reg(ALG, yn) ≤ γn min{Reg(FTL, yn), Reg(Cover, yn)} We henceforth refer to γn as the competitive ratio achieved by ALG; ideally we want this ratio to be a constant, i.e., γn = O(1). This necessitates that on sequences where FTL gets a constant regret, then ALG basically follows FTL throughout, while on sequences where FTL has high (in particular, ω(√n)) regret, then ALG follows Cover in most rounds. The challenge in designing instance optimal algorithms is that the regret of any algorithm is a quantity that is not adapted to the natural filtration, i.e. it may not be possible to track Reg(ALG, yn) for any ALG from just the history (y1, y2, . . . , yt−1), since the hindsight optimal action a∗ depends on the entire sequence yn. One proxy is to track an algorithm’s loss instead, leading to the idea of ‘corralling’ policies [Agarwal et al., 2017, Pacchiano et al., 2020, Dann et al., 2023], that run online learning over the reference algorithms to get within O(poly(n)) of the smaller of the two losses. Such an approach can not ensure γn = O(1): for example, consider an i.i.d. sequence of Bernoulli(0.1) bits, where FTL has lower regret than Cover. With high probability on any such sequence we have small Reg(FTL, yn) = O(1) and yet high loss Ln(a∗, yn) = Θ(n); now any corralling algorithm (even a small loss one) must suffer O(poly(n)) regret, and hence ω(1) 1We choose this specific tie-breaking rule for convenience; however, we can take any at ∈ [0, 1]. 2Here fn, the so-called Rademacher complexity of the setting, is a fixed function of n that does not depend on sequence yn. For binary prediction, fn = E | Pn t=1 Zt| 2 ≈ p n 2π where Zn ∼ Unif{1, −1} i.i.d. 2 Figure 1: Comparing regret of FTL, Cover and SMART on a collection of input sequences (for fixed n). • In Fig. (a), we consider i.i.d. Bernoulli(p) inputs for varying p. The regret of FTL is much lower than Cover for p < 1/2; the regret of SMART tracks FTL closely (better than 2Reg(FTL), indicated by dotted line). • In Fig. (b) and (c), we consider ‘worst-case’ binary sequences (as per [Feder et al., 1992]) parameterized by the number of ‘lead-changes’: the sequence with parameter c comprises of c pairs ‘0, 1’ or ‘1, 0’, followed by n − 2c ‘1’s. In Fig. (b), we consider SMART with a deterministic switching threshold (Theorem 1) and compare Reg(SMART) with 2Reg(FTL) and 2Reg(Cover) (dotted lines); in Fig. (c), we use a randomized threshold (Theorem 2), and show the average regret over the randomized threshold, as well as sample paths (plotted in green), and compare with e e−1 times Reg(FTL) and Reg(Cover) (dotted lines). competitive ratio. This example also shows that achieving a constant factor guarantee with respect to the minimum of the two losses does not translate to a constant factor guarantee with respect to the minimum of the two regrets. The instance optimal guarantee is closely related to best-of-both-worlds guarantees, which aim for algorithms that simultaneously achieve (up to constant factors) both the low pseudoregret guarantee of policies designed for stochastic inputs (as with FTL in our setting, or the Upper Confidence Bound (UCB) algorithm in bandits), as well as a per-sequence regret guarantee comparable to a worst-case optimal algorithm ALGWC (Eg. Cover or Hedge in online learning; EXP3 in bandits Auer et al. [2002a]). Such guarantees have been shown in a variety of settings, including online learning [De Rooij et al., 2014, Orabona and P´al, 2015, Mourtada and Ga¨ıffas, 2019, Bilodeau et al., 2023] and bandit settings [Bubeck and Slivkins, 2012, Zimmert and Seldin, 2019, Lykouris et al., 2018, Dann et al., 2023]. One problem though is that since pseudoregret and worst-case regret are very different quantities, the above results tend to be hard to interpret, and less predictive of good performance3. Note though that given a pair of stochastic/worst-case optimal algorithms, a policy that is γ- instance-optimal w.r.t. these immediately satisfies a best-of-both-worlds guarantee with constant factor γ. In this regard, instance optimality provides a stronger guarantee as it holds on every sequence yn regardless of how it is generated. Moreover, the parameter γ can also provide sharper comparisons between algorithms, as well as admit hardness results on the limits of such guarantees. 1.1 Our Contributions We consider a general online learning setting where at the beginning of each round t ∈ [n], a policy ALG first plays an action at ∈ A, following which, a loss function ℓt : A → [0, 1] is revealed, resulting in a loss of ℓt(at). The regret is defined according to: Reg(ALG, ℓn) = Pn t=1 ℓt(at) − infa∈A Pn t=1 ℓt(a). (2) 3As an example, Hedge has optimal pseudoregret in certain stochastic settings [Mourtada and Ga¨ıffas, 2019], but this is known to be sensitive to perturbations in the distributions [Bilodeau et al., 2023]. 3 More generally, as in with bit prediction, we allow ALG to play in round t a measure wt ∈ ∆A (i.e., play {wt : A 7→ [0, 1]| P a∈A wt(a) = 1}), resulting in an expected loss of P a∈A wt(a)ℓt(a). For notational convenience, we henceforth use (at, ℓt(at)) for the action/loss, and reserve use of expectations for randomness in the algorithm and/or sequence. We want to understand when is it possible to attain instance optimality as in Eq. (1) with respect to a given pair of algorithms. Ideally, we want the first to be optimal for stochastic instances, and the second to be minimax optimal; unfortunately however exact optimal policies are unknown except in simple settings. To this end, we make two amendments to our goal: First, for the stochastic optimal policy, we use FTL; this is well defined in any online learning setting, and moreover, known to be optimal or near-optimal for a wide range of settings under minimal assumptions [Kot lowski, 2018]. Second, instead of the minimax policy, we use as reference any policy ALGWC which has a known worst case regret bound g(n). With these modifications in place, we have the following objective. Definition 2. Given FTL and any algorithm ALGWC with supℓn Reg(ALGWC, ℓn) ≤ g(n), we say a policy ALG is instance optimal with respect to the pair if there exists some universal γn ≥ 1 (i.e. not depending on yn) such that for every sequence of losses ℓn: Reg(ALG, ℓn) ≤ γn min{Reg(FTL, ℓn), g(n)} While the above guarantee is not truly instance-optimal in that we are comparing against a worst-case regret bound g(n) for ALGWC rather than its performance on the instance ℓn, the two are the same if ALGWC is minimax optimal and hence attaining equal regret on all loss sequences; recall this is true of Cover for binary prediction. To realize the above goal, we propose the Switch via Monotone Adapted Regret Traces (SMART) approach, which at a high level is a black-box way to convert design of instance-optimal policies into a simple optimal stopping problem. Our approach depends on just two ingredients: first, owing to the additive structure of online learning problems, we have that the minimax guarantee g(k) above holds over any k ∈ Z and any (sub)sequence of n loss functions; second, we show that FTL admits simple anytime regret estimator ΣFTL τ (see Lemma 1) which is monotone and adapted (i.e., a function only of historical data). Using these two observations, we can reduce the task of minimizing regret to a version of the ‘ski-rental’ problem [Karlin et al., 1994, Borodin and El-Yaniv, 2005], as follows: we play FTL up to some stopping time τ, and then switch to ALGWC for the remaining n−τ periods, resetting all losses to zero. This algorithm incurs a total regret bounded by ΣFTL τ + g(n − τ), and using ideas from competitive analysis, we get that there is a simple way to choose the stopping time τ to achieve an e/(e − 1) ≈ 1.58-competitive ratio guarantee with respect to the minimum between the regret of FTL and the worst case guarantee g(n). Theorem. (See Theorem 2) Let ALGWC have worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n) where g(n) is some monotonic function of n. An instantiation of SMART achieves Reg(SMART, ℓn) ≤ e e − 1 min{Reg(FTL, ℓn), g(n)} + 1. (3) A highlight of our approach is the surprising simplicity of the algorithm and analysis, despite the strength of the instance optimality guarantee. In particular, our approach is modular, allowing one to plug in any ALGWC and corresponding worst case bound g(n), thus letting us handle any online learning setting with known minimax bounds. This results in an entire family of instance optimal policies for settings such as predictions with experts and online convex optimization. Moreover, the approach is easy to extend to get more complex guarantees; as an example, if ALGWC is designed 4 to get low regret for benign (i.e., ‘small-loss’) sequences ℓn, then we show how to use SMART as a subroutine and achieve an instance optimal guarantee with respect to the regret of FTL and a small loss regret bound. Corollary 1 (Following Theorem 5). Consider the prediction with expert advice setting [Cesa- Bianchi et al., 1997], where A = ∆m−1, the m−simplex for m ≥ 2, and ℓt(a) = ⟨a, ℓt⟩ for ℓt ∈ [0, 1]m. Let L∗ := minj Pn t=1 ℓtj. An instantiation of SMART achieves Reg(SMART, ℓn) ≤ 2 min n Reg(FTL, ℓn), 10 p 2L∗ log m o + O(log L∗ log m). Finally, studying instance optimality also lets us understand the fundamental limits of best-of- both worlds algorithms. To this end, we provide a lower bound that shows our algorithm is nearly optimal in the competitive ratio. To the best of our knowledge, this is the first hardness result for best-of-both-worlds guarantees in online learning. Theorem. (See Theorem 3) In the binary prediction setting, given any online algorithm ALG, there exist sequences yn ∈ {0, 1}n such that: Reg(ALG, yn) ≥ 1.43 min \b Reg(FTL, yn), Reg(Cover, yn) \t Note again that in binary prediction, FTL achieves the optimal pseudoregret under i.i.d. inputs, while Cover is the true minimax policy; thus, this is a fundamental lower bound on best-of-both-worlds guarantee in any online learning setting. 1.2 Related work There have been many approaches towards combining stochastic and worst-case guarantees. As we discussed before, there is a large literature on best-of-both-worlds algorithms for both full and partial information settings [Wei and Luo, 2018, Bubeck et al., 2019, Zimmert and Seldin, 2019, Dann et al., 2023], and also more complex settings such as metrical task systems [Bhuyan et al., 2023] and control [Sabag et al., 2021, Goel et al., 2023]. Another line of work [Rakhlin et al., 2011, Haghtalab et al., 2022, Block et al., 2022, Bhatt et al., 2023] considers smoothed analysis, where the worst-case actions of the adversary are perturbed by nature. A third approach [Bubeck and Slivkins, 2012, Lykouris et al., 2018, Amir et al., 2020, Zimmert and Seldin, 2019] interpolates between the stochastic and adversarial settings by considering most ℓt to be i.i.d., interspersed with a few adversarially chosen instances (corruptions). Finally, another line considers the data-generating distribution to come from a ball of specified radius around i.i.d. distributions [Mourtada and Ga¨ıffas, 2019, Bilodeau et al., 2023]. While all these approaches provide useful insights into the gap between average and worst-case guarantees, one can argue they are all imprecisely specified – given an instance {ℓt}t∈[n] in hindsight, there is no clear sense as to which model best ‘explains’ the instance. Our focus on instance optimality instead follows the approach of better understanding and shaping the per-sequence regret landscape. The origins of this approach arguably come from the seminal work of Cover [1966] for binary prediction (we discuss this in more detail in Section 3), with a later focus on better bounds for benign instances in general online learning [Auer et al., 2002b, Cesa-Bianchi et al., 2005, Hazan and Kale, 2010]. More recently, a line of work [Koolen et al., 2014, Van Erven et al., 2015, Van Erven and Koolen, 2016, Gaillard et al., 2014] have studied designing policies that can adapt to different types of data sequences and achieve multiple performance guarantees simultaneously. The main idea is to use multiple learning rates that are weighted according to their empirical performance on the data. While the focus is still primarily on classifying instances based on when they are easier/harder to learn, some of the resulting guarantees have an 5 instance-optimality flavor; for example, Van Erven and Koolen [2016] show how to simultaneously match the performance guarantee (in terms of certain variance bounds) attained by different learning rates in Hedge. Such approaches however need to understand their baseline algorithms in great detail, and use them in a ‘white-box’ way to get their guarantees. In contrast, our approach fundamentally focuses on combining policies in a black-box way to get instance optimal outcomes. As we mention, this is similar in spirit to corralling bandit algorithms Agarwal et al. [2017], Pacchiano et al. [2020], Dann et al. [2023], as well as more recent work on online algorithms with predictions Bamas et al. [2020], Dinitz et al. [2022], Anand et al. [2022]; however, as we mention above, these all get guarantees with respect to the loss of the reference algorithms, which is much weaker than our regret guarantees (though they do so in much more complex settings with partial information and/or state). To our knowledge, the only previous result which attains a comparable instance-optimality guarantee to ours is that of De Rooij et al. [2014] for the experts problem, where the authors propose the FlipFlop policy which interleaves Hedge (with varying learning rates) and FTL to obtain a regret guarantee similar to that of Corollary 1. In fact, their guarantee is stronger as FlipFlop is shown to be 5.64-competitive with respect to min{Reg(FTL, ℓn), g(L∗)} where g(L∗) ≤ √L∗ log m [De Rooij et al., 2014, Corollary 16]. However, while FlipFlop depends on a clever choice of learning rates in Hedge, SMART can black-box interleave FTL with any worst-case/small-loss algorithm without knowing the inner workings of said algorithm, which we see as a significant engineering strength. More importantly, our approach to this problem is distinct as we focus on the fundamental limits (upper and lower bounds) on the competitive ratio that must be incurred when combining FTL with a worst-case policy; to the best of our knowledge this viewpoint, and the corresponding reduction to an optimal stopping problem, has not been previously explored. 2 Instance Optimal Online Learning: Achievability via SMART Given the setting and problem statement above, we can now present the SMART policy. We do this for a general online learning problem, wherein we want to combine FTL with any given algorithm ALGWC with a worst-case regret guarantee supℓn Reg(ALGWC, ℓn) ≤ g(n). Before presenting the policy, we first need the following regret decomposition for FTL. Lemma 1 (Regret of FTL). If Lt(·) := Pt i=1 ℓt(·), i.e. the cumulative loss function, then Reg(FTL, ℓn) = Pn t=1(Lt(a∗ t−1) − Lt(a∗ t )). This is reminiscent of the ‘be-the-leader’ lemma [Kalai and Vempala, 2005, Slivkins, 2019], although never stated explicitly as an exact decomposition. Proof. Recall we define aFTL t = a∗ t−1, and hence infa∈A Pn t=1 ℓt(a) = Ln(a∗ n). Now we have Reg(FTL, ℓn) = Pn t=1 ℓt(a∗ t−1) − Ln(a∗ n) = Pn t=1(Lt(a∗ t−1) − Lt−1(a∗ t−1)) − Ln(a∗ n) = Pn t=1(Lt(a∗ t−1) − Lt(a∗ t )) + Pn t=1(Lt(a∗ t ) − Lt−1(a∗ t−1)) − Ln(a∗ n) (a) = Pn t=1(Lt(a∗ t−1) − Lt(a∗ t )) where (a) follows since Pn t=1(Lt(a∗ t ) − Lt−1(a∗ t−1)) = Ln(a∗ n) by telescoping. 6 Next, let ΣFTL t denote the anytime regret that FTL incurs up to round t (i.e., assuming the game ends after round t). From Lemma 1 we have ΣFTL t := Reg(FTL, ℓt) = Pt i=1(Li(a∗ i−1) − Li(a∗ i )) (4) Now we make three critical observations: • ΣFTL t is adapted: it can be computed at the end of the tth round • ΣFTL t is monotone non-decreasing in t (since by definition Li(a∗ i−1) − Li(a∗ i ) ≥ 0) • ΣFTL t is an anytime lower bound for Reg(FTL, ℓn), with ΣFTL n = Reg(FTL, ℓn) For an input threshold θ ≥ 0 and an algorithm ALGWC, we get the following (meta)algorithm. Algorithm 1: Switching via Monotone Adapted Regret Traces (SMART) Input: Policies FTL, ALGWC, threshold θ Initialize ΣFTL 0 = 0, t = 1 ; while ΣFTL t−1 ≤ θ do Set at = a∗ t−1; // Play FTL Observe ℓt(·); Update Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL t = ΣFTL t−1 + (Lt(a∗ t−1) − Lt(a∗ t )) and t = t + 1; end Reset losses to 0 and play ALGWC for remaining rounds (See Figure 2(b)) ; We now have the following performance guarantee for Algorithm 1 for θ = g(n). Theorem 1 (Regret of SMART with deterministic threshold). We are given FTL, and any other policy ALGWC with worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n) for some monotone function g. Then, playing SMART with threshold θ = g(n) ensures Reg(SMART, ℓn) ≤ 2 min{Reg(FTL, ℓn), g(n)} + 1. (5) As we mention before, the structure of the SMART algorithm (and the resulting guarantee) parallels the standard 2-competitive guarantee for the ski-rental problem [Karlin et al., 1994]. This is a classical optimal stopping problem, where a principal faces an unknown horizon, and in each period must decide whether to rent a pair of skis for the period (for fixed cost $1) or buy the skis for the remaining horizon (for fixed cost $B). The aim is to design a policy which is minimax optimal (over the unknown horizon) with regards to the ratio of the cost paid by the principal, and the optimal cost in hindsight. We further expand on this connection in Section 2.1 for the case of binary prediction. However, the connection suggests a natural follow-up question of whether randomized switching can help (as is the case for ski-rental); the following result answers this in the affirmative. Theorem 2 (Regret of SMART with Randomized Thresholds). We are given FTL and any other policy ALGWC with worst-case regret supℓn Reg(ALGWC, ℓn) ≤ g(n) for some monotone function g. Moreover, given random sample U ∼ Unif[0, 1], suppose we set θ = g(n) ln(1 + (e − 1)U) Then playing the SMART policy (Algorithm 1) with random threshold θ ensures Eθ \u0002 Reg(SMART, ℓn) \u0003 ≤ e e − 1 min{Reg(FTL, ℓn), g(n)} + 1. 7 While we state the above as a randomized switching policy, this is more for interpretability – it is easier to view our policy as switching between two black-box algorithms rather than playing a convex combination of the two. However, since we define the loss incurred by any ALG in round t to be the expected loss when ALG plays a distribution wt over actions A (see Section 1.1), therefore we can alternately implement the above by mixing between the actions of FTL and ALGWC. More specifically, the above policy induces a monotone mixing rule, where over t, the weight on the action suggested by ALGWC is non-decreasing. Remark 1 (Optimality over Monotone Mixing Policies). The competitive ratio of e e−1 is known to be optimal for the ski-rental problem via Yao’s minmax theorem [Borodin and El-Yaniv, 2005]. A direct corollary of this is the optimality of SMART over algorithms that are single switch (i.e., where the weight on ALGWC is non-decreasing in t). One difference between our setting and ski-rental is that switching back-and-forth between FTL and ALGWC is possible; see for example the FlipFlop policy of De Rooij et al. [2014]. In Section 3 we provide a fundamental lower bound of 1.43 on the competitive ratio over all algorithms; this suggests that multiple switching can help get a better competitive ratio (since e/(e − 1) ≈ 1.58), but also, that single-switch policies are surprisingly close to optimal. 2.1 Illustrating the Reduction to Ski Rental in Binary Prediction Before proving Theorems 1 and 2, we first illustrate the basic idea of our approach and reduction for the binary prediction setting. This is aided by the observation that the regret of FTL in this setting admits a simple geometric interpretation: for any sequence, and any time t, we have that ΣFTL is equal to 1/2 times the number of ‘lead changes’ up to time t; where a lead change is a time step i where the count of 1s and 0s in the (sub)sequence yi−1 is equal (see Figure 2(a)). Corollary 2 (FTL for binary prediction). In binary prediction, for any sequence y ∈ {0, 1}n and any time t ≤ n, define the lead-change counter c(yt) := |{0 ≤ j ≤ t s.t. j X i=1 yi = j X i=1 (1 − yi)}|. Then we have ΣFTL t = 1 2c(yt−1) and thus Reg(FTL, yn) = 1 2c(yn−1). Corollary 2 follows from the regret decomposition in Lemma 1. Furthermore, since the losses of 0s and 1s are equal at a lead change, it also follows that at a lead change t, the anytime regret ΣFTL t is also equal to the hindsight regret incurred by FTL up to time t. Since ΣFTL t only increases in value at lead changes, if the SMART algorithm switches to ALGWC, it will only happen after a lead change, and thus SMART behaves as if it had oracle knowledge of the regret of FTL from just the history up to the current time. Consider the instantiation of SMART where ALGWC = Cover and g(n) = p n 2π. As mentioned in Section 1, a remarkable property of Cover is that it is the true minimax optimal algorithm, where Reg(Cover, yn) = p n 2π(1+o(1)) for all sequences yn, such that g(n) is nearly equal to Reg(Cover, yn) regardless of the sequence [Cover, 1966]. It follows that SMART is equivalent to an algorithm which starts with FTL, plays it until the regret of FTL matches the minimax regret guaranteed by Cover for the remaining sequence, and then switches to Cover until the end. Let tsw denote the last round SMART plays FTL before switching to ALGWC (with tsw = n if it never switches). For a single switch algorithm, the sequence which maximizes the regret is one that maximizes the FTL regret before the switch at tsw, and minimizes 8 Figure 2: Figure 2(a) on the left shows the worst case instance in binary prediction for an algorithm which starts with FTL and switches at most once during the time horizon to Cover. Figure2(b) on the right depicts in a prediction with experts setting how SMART resets the losses after the switch from FTL to ALGWC. the FTL after the switch, as depicted in Figure 2(a). For such a sequence, ΣFTL t = t/4 at lead changes t, such that the regret incurred by the algorithm is linear before the switch, matching the linear cost of renting skis in the ski rental problem. Note that tsw will necessarily be o(n) in such sequences as the time it takes until ΣFTL t ≥ g(n) is linear in g(n) = o(n). After the switch, Cover will incur regret q n−tsw 2π (1 + o(1)) = g(n)(1 + o(1)), matching the fixed cost of buying skis at the switching point; Corollary 3 follows as a result of this analysis. Furthermore, in the binary prediction setting, SMART achieves the stronger notion of instance optimality stated in Definition 1. Corollary 3. For all yn ∈ {0, 1}n, SMART with ALGWC = Cover and θ = p n 2π satisfies Reg(SMART, yn) ≤ 2 min{Reg(FTL, yn), Reg(Cover, yn)} + 1. SMART with ALGWC = Cover and θ = p n 2π ln(1 + (e − 1)U) for U ∼ Uniform[0, 1] satisfies Reg(SMART, yn) ≤ 1.58 min{Reg(FTL, yn), Reg(Cover, yn)} + 1. 2.2 Proofs for General Online Learning In the general online learning setting, the proof is nearly the same, with the reduction to the ski-rental problem captured by Lemma 2. Lemma 2. Let tsw := min1≤t≤n−1 ΣFTL t > θ denote the last round SMART plays FTL before switching to ALGWC (with tsw = n if it never switches). SMART incurs regret bounded by Reg(SMART, ℓn) ≤ Reg(FTL, ℓtsw) + Reg(ALGWC, ℓn tsw+1) ≤ θ + Reg(ALGWC, ℓn tsw+1) + 1. Proof. We separately bound the regret of SMART before the switch and after the switch, Reg(SMART, ℓn) = \u0000 Ptsw t=1 ℓt(at) − Ptsw t=1 ℓt(a∗ n) \u0001 + \u0000 Pn t=tsw+1 ℓt(at) − Pn t=tsw+1 ℓt(a∗ n) \u0001 (a) ≤ \u0000 Ptsw t=1 ℓt(at) − Ptsw t=1 ℓt(a∗ tsw) \u0001 + \u0000 Pn t=tsw+1 ℓt(at) − Pn t=tsw+1 ℓt(a∗ tsw+1:n) \u0001 = Reg(FTL, ℓtsw) + Reg(ALGWC, ℓn tsw+1). 9 The first term is upper bounded by Reg(FTL, ℓtsw) since Ltsw(a∗ n) ≥ Ltsw(a∗ tsw) by definition, as a∗ tsw is the minimizer of Ltsw, and furthermore SMART always plays according to FTL in rounds up to tsw. The second term is upper bounded by Reg(ALGWC, ℓn tsw+1) because Pn t=tsw+1 ℓt(a∗ tsw+1:n) ≤ Pn t=tsw+1 ℓt(a∗ n) as a∗ tsw+1:n is the minimizer of the losses after tsw, and at time t > tsw, SMART plays according to ALGWC on the sequence of losses limited to ℓn t+1. This illustrates the important role of resetting the losses after the switch as depicted in Figure 2(b). Using the fact that ΣFTL tsw−1 ≤ θ and Ltsw−1(x∗ tsw−1) ≤ Ltsw−1(x∗ tsw), it follows that Reg(FTL, ℓtsw) = ΣFTL tsw−1 + Ltsw−1(a∗ tsw−1) − Ltsw−1(x∗ tsw) + ℓtsw(a∗ tsw−1) − ℓtsw(x∗ tsw) ≤ θ + 1. The reduction to ski-rental is again immediate due to the properties that ΣFTL t := Reg(FTL, ℓtsw) is adapted, monotone, and is an anytime lower bound for Reg(FTL, ℓn), while remaining an upper bound on the true regret incurred by FTL up to time t. As a result, the algorithm can pretend that it truly observes the regret it incurs at each time up to the switching time tsw. After the switch, SMART incurs regret Reg(ALGWC, ℓn tsw+1) which is upper bounded by g(n − tsw) ≤ g(n) by assumption. Proof of Theorem 1. This follows immediately from Lemma 2. For ℓn such that Reg(FTL, ℓn) ≤ g(n), SMART will never switch to Cover as ΣFTL t ≤ Reg(FTL, ℓn) ≤ g(n) such that Reg(SMART, ℓn) = min{Reg(FTL, ℓn), g(n)}. For ℓn such that Reg(FTL, ℓn) > g(n), by Lemma 2, Reg(SMART, ℓn) ≤ g(n) + g(n − tsw) + 1 ≤ 2g(n) + 1. Proof of Theorem 2. The proof uses a primal-dual approach, similar to that of Karlin et al. [1994] for ski-rental. For a given sequence of loss functions ℓn, we use the shorthand r = Reg(FTL, ℓn) and g = g(n). Also, for our given choice of cumulative distribution function Fn, the corresponding probability density function is given by f(z) = ez/g g(e−1) for z ∈ [0, g]. As before, let tsw := min1≤t≤n−1 ΣFTL t > θ be the (random) round where SMART switches from FTL to ALGWC (with tsw = n if it never switches). Then by Lemma 2 we have Reg(SMART, ℓn) − 1 min{Reg(FTL, ℓn), g(n)} ≤    θ+g min{r,g} if tsw < n 1 if tsw = n where the second case follows from the fact that θ ∈ [0, g], and hence if we never switch, then r ≤ g(n). Taking expectation over θ, we have Eθ \u0002 Reg(SMART, ℓn) \u0003 − 1 min{Reg(FTL, ℓn), g} ≤    R r 0 x+g r f(x)dx + 1 − F(r) if r ≤ g R g 0 x+g g f(x)dx if r > g Let ϕ(z) := R z 0 (x+g) z f(x)dx + 1 − F(z) for z ∈ [0, g]; then Eθ[Reg(SMART,ℓn)]−1 min{Reg(FTL,ℓn),g} ≤ maxz∈[0,g] ϕ(z). Moreover, we can differentiate to get z2ϕ′(z) = gzf(z) − R z 0 (x + g)f(x)dx. Substituting our choice of f in this expression, we get z2dϕ(z) dz = zez/g (e − 1) − Z z 0 (x + g) ex/g g(e − 1)dx = zez/g − g R z/g 0 (w + 1)ewdw (e − 1) = 0 Thus, ϕ(z) is constant for all z ∈ [0, g] and ϕ(g) = 1 g(e−1) R g 0 (1 + x/g)ex/gdx = e e−1. 10 3 Instance Optimal Online Learning: Converse In this section, we investigate fundamental limits on the instance-optimal regret guarantees achievable by any algorithm. More precisely, in the setting of binary prediction, we ask what is the smallest value of γn satisfying Reg(ALG, yn) ≤ γn min{Reg(FTL, yn), Reg(Cover, yn)} = γn min n 1 2c(yn−1), fn o (6) for all yn, where fn := Reg(Cover, yn) = p n 2π(1 + o(1)). We show the following lower bound. Theorem 3 (Lower bound on the competitive ratio). lim n→∞ γn ≥ \u0010 1 − e−1/π + 2Q \u0000p 2/π \u0001\u0011−1 ≈ 1.4335 where the Q(·) function is Q(x) := 1 √ 2π R ∞ x e−t2/2. Since binary prediction is a specific online learning problem, this also yields a fundamental lower bound for instance-optimality for general online learning. Note particularly that γn > 1, implying that (1 + o(1)) min{Reg(FTL), Reg(ALGWC)} regret is not possible to achieve. Thus, there is an inevitable multiplicative factor that must be paid in order to achieve an instance-optimal regret guarantee. An equivalent way to state (6) is to find the smallest γn for which a predictor {at(yt−1)}n t=1 satisfies for all y ∈ {0, 1}n Pn t=1 |at(yt−1) − yt| ≤ γn min \b 1 2c(yn−1), fn \t + min \b Pn t=1 yi, n − Pn t=1 yi \t . (7) In order to establish the values of γn for which the loss function in the right hand side of (7) are achievable, we utilize the following result of Cover [1966], which provides an exact characterization of the set of all loss functions achievable in binary prediction. Formally, we say a function ϕ : {0, 1}n → R+ is achievable in binary prediction if there exists a predictor/strategy at : yt−1 7→ [0, 1] that ensures Pn t=1 |at(yt−1)−yt| = ϕ(yn) , ∀ yn ∈ {0, 1}n. Then, we have the following characterization. Theorem 4 (Cover [1966]). Let ϵn ∼ Bern \u0010 1 2 \u0011 i.i.d. For ϕ to be achievable, it must satisfy the following: • Balance: E[ϕ(ϵn)] = n 2 . • Stability: Let ϕt(yt) := E[ϕ(ytϵn t+1)]; then |ϕt(yt−10) − ϕt(yt−11)| ≤ 1 ∀t ∈ [n], yt ∈ {0, 1}t. Further any ϕ satisfying the above is realized by predictor at(yt−1) = 1+ϕt(yt−10)−ϕt(yt−11) 2 . As an immediate corollary, Theorem 4 equips us with the exact minimax optimal algorithm for binary prediction alluded to in Section 1. Returning to our setting, from the balance condition in Theorem 4, for ϵn ∼ Bern(1/2) i.i.d. γn E \u0002 min \b 1 2c(ϵn−1), fn \t\u0003 + E \u0002 min \b Pn t=1 ϵt, n − Pn t=1 ϵt \t\u0003 ≥ n 2 . for the function in (7) to be achievable. Using the definition of fn, γn ≥ fn E \u0014 min n 1 2c(ϵn−1), fn o\u0015 = 2fn E h min \b c(ϵn−1), 2fn \t ] i. The above bound immediately yields that γn ≥ 1 as expected. We can further sharply characterize the asymptotics of γn, resulting in the stated lower bound. The full proof of Theorem 3 is provided in Appendix B. 11 4 Instance-Optimal Algorithms in Small-Loss Settings So far, we have presented specializations of SMART that achieve instance-optimality between FTL and the worst-case regret g(n). However, many worst-case algorithms can still adapt to the instance ℓn and achieve regret guarantees that are a function of the ‘difficulty’ of the instance ℓn. A common way to quantify this is difficulty is via small-loss bounds, where the regret is upper bounded by g(L∗) where g(·) as earlier is a monotonic increasing function and L∗ := mina∈A Pn t=1 ℓt(a) is the loss achieved by the best action. Such guarantees imply that for sequences where there exists an action achieving low loss, the corresponding regret achieved is also low. Thus, a natural question is whether SMART can be specialized to yield an algorithm that is constant competitive with respect to min{Reg(FTL, ℓn), g(L∗)}. As a starting point, if L∗ is known apriori, it is easy to achieve a e e−1 approximation by simply using SMART with (random) threshold θ = g(L∗) ln(1 + (e − 1)U), U ∼ Unif[0, 1]; this is an immediate corollary of Theorem 2. When L∗ is not known, we use a guess-and-double argument to devise an algorithm that achieves the following instance-optimality guarantee. Theorem 5 (Regret of SMART for unknown small loss). Let ALGWC have small loss regret guarantees satisfying Reg(ALGWC, ℓn) ≤ g(L∗) for any ℓn where L∗ = minj∈[m] Ltj, i.e. the loss achieved by the best expert in hindsight. Then, if we play SMART for Small-Loss as stated in Algorithm 2, we have Reg(SMART, ℓn) ≤ 2 min \u0000Reg(FTL, ℓn), Plog(1+L∗/ log m)+1 z=1 g(2z log m) \u0001 + O(log L∗/ log m) In particular, in the prediction with expert advice setting, we know that Hedge with a time-varying learning rate achieves g(L∗) ≡ 2√2L∗ log m + κ log m (where κ > 0 is an absolute constant) [Auer et al., 2002b, Cesa-Bianchi et al., 2007]; this gives Corollary 1 in Section 1. The intuitive idea behind the algorithm is to guess the value of L∗, and play SMART with this guessed value while simultaneously keeping track of the regret incurred. Whenever the regret incurred exceeds the guarantee established by SMART with known L∗ double the guessed value and start again. We use the notation ALGWC(ℓt2 t1) to refer to the worst-case algorithm when the previously observed sequence is ℓt2 t1; in particular this would be equivalent to the action recommended at time t2 +1 after throwing away all the observed losses before t1. We let ΣFTL t1:t2 = Pt2 i=t1(Li(a∗ i−1)−Li(a∗ i )), which grows as the number of leader changes within i ∈ [t1, t2]. The algorithm’s pseudocode is given in Algorithm 2 below, and a proof of Theorem 5 is provided in Appendix A. 5 Conclusion In this paper, we present SMART, a simple and black-box online learning algorithm that adapts to the data and achieves instance optimal regret with respect to FTL and any given worst-case algorithm. We show that SMART only switches once from FTL to the worst-case algorithm, and attains a regret that is within a factor of e/(e − 1) ≈ 1.58 of the minimum of the regret of FTL and the minimax regret over all input sequences; we also show that any algorithm must incur an extra factor of at least 1.43 establishing that our simple approach is surprisingly close to optimal. Furthermore, we extend SMART to incorporate a small-loss algorithm and obtain instance optimality with respect to the small-loss regret bound. Our approach relies on a novel reduction of instance optimal online learning to the ski-rental problem, and leverages tools from information theory and competitive analysis. Our work suggests several open problems for future research, such as finding instance optimal algorithms for bandit settings, or designing algorithms that can adapt to multiple reference algorithms besides FTL and minimax algorithms. 12 Algorithm 2: SMART for Small-Loss Input: Policies FTL, ALGWC; Small-loss bound g(·) for z = 0, 1, . . . (epochs) do Let t = tz :=start time of zth epoch, L∗ z := 2z log m (current guess for L∗), ΣFTL tz:tz−1 = 0 ; while ΣFTL tz:t−1 ≤ g(L∗ z) do Set at = a∗ t−1; // Play FTL Observe ℓt(·); Update Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL tz:t = ΣFTL tz:t−1 + (Lt(a∗ t−1) − Lt(a∗ t )) and t = t + 1; end Let τz := mint≥tz ΣFTL tz:t > g(L∗ z) and t = τz + 1; // Check if loss incurred by ALGWC in this epoch violates the upper bound from L∗ z is correct while Pt t=tz⟨at, ℓt⟩ ≤ L∗ z + 2 min{ΣFTL tz:t , g(L∗ z)} + 1 do Set at = ALGWC(ℓt−1 τz+1) ; // Play ALGWC forgetting losses before τz + 1 Observe ℓt(·); Update Lt(·) = Lt−1(·) + ℓt(·) and ΣFTL tz:t = ΣFTL tz:t−1 + (Lt(a∗ t−1) − Lt(a∗ t )) and t = t + 1; end end Acknowledgements This work is supported by NSF grants CNS-1955997, CCF-2337796 and ECCS-1847393, and AFOSR grant FA9550-23-1-0301. This work was partially done when the authors were visitors at the Simons Institute for the Theory of Computing, UC Berkeley. A Omitted proofs from Section 4 In this Section, we will establish the proofs of Theorem 5 and Corollary 1. Recall Algorithm 2, where ALGWC(ℓt2 t1) refers to the worst-case algorithm when the previously observed sequence is ℓt2 t1; in particular this would be equivalent to the action recommended at time t2 +1 after throwing away all the observed losses before t1. We let ΣFTL t1:t2 = Pt2 i=t1(Li(a∗ i−1)−Li(a∗ i )), which grows as the number of leader changes within i ∈ [t1, t2]. We first have the following decomposition of the regret for any algorithm ALG that plays the sequence of actions aALG t at time t. Lemma 3. The regret incurred by any sequence of actions (aALG t )t∈[n+1] can be written as Reg(ALG, ℓn) = n X t=1 \u0010 Lt(aALG t ) − Lt(aALG t+1 ) \u0011 , (8) where we let aALG n+1 := a∗ n. Proof. Ln(ALG) = n X t=1 ℓt(aALG t ) (9) 13 = n X t=1 \u0010 Lt(aALG t ) − Lt−1(aALG t ) \u0011 (10) = Ln(aALG n ) + n−1 X t=1 \u0010 Lt(aALG t ) − Lt(aALG t+1 ) \u0011 − L0(aALG 1 ). (11) This implies a regret decomposition of Reg(ALG, ℓn) = Ln(ALG) − Ln(a∗ n) (12) = Ln(aALG n ) − Ln(a∗ n) + n−1 X t=1 \u0010 Lt(aALG t ) − Lt(aALG t+1 ) \u0011 − L0(aALG 1 ) (13) As L0(aALG 1 ) = 0, and aALG n+1 := a∗ n, it follows that Reg(ALG, ℓn) = n X t=1 \u0010 Lt(aALG t ) − Lt(aALG t+1 ) \u0011 . (14) Next, we use this decomposition to establish the regret of any algorithm ALG that alternates between playing FTL and another algorithm ALGWC. Lemma 4. Consider an algorithm ALG which alternates between playing FTL and ALGWC, where FTL is played in the intervals {[tz, τz]}z∈[zlast], and ALGWC is played in intervals {[τz +1, tz+1 −1]}z∈[zlast]. The regret of ALG is bounded by Reg(ALG, ℓn) ≤ X z \u0010 ΣFTL tz:τz−1 + Reg(ALGWC, ℓtz+1−1 τz+1 ) + 1 \u0011 . (15) Proof. We let tzlast = n + 1, and an+1 = a∗ n. We use Lemma 3 and rearrange the terms by grouping them by the FTL periods and the ALGWC periods. Reg(ALG, ℓn) = X z   tz+1−1 X t=tz (Lt(at) − Lt(at+1))   = X z   τz−1 X t=tz (Lt(a∗ t−1) − Lt(a∗ t )) + Lτz(a∗ τz−1) − Lτz(aτz+1) + tz+1−1 X t=τz+1 (Lt(at) − Lt(at+1))   = X z   τz−1 X t=tz (Lt(a∗ t−1) − Lt(a∗ t )) + Lτz(a∗ τz−1) + tz+1−1 X t=τz+1 ℓt(at) − Ltz+1−1(atz+1)   = X z τz−1 X t=tz (Lt(a∗ t−1) − Lt(a∗ t )) + X z   tz+1−1 X t=τz+1 ℓt(at) + Lτz(a∗ τz−1) − Ltz+1−1(a∗ tz+1−1)   . We bound the first term by the FTL regret. Recall the notation ΣFTL tz:τz−1 := τz−1 X t=tz (Lt(a∗ t−1) − Lt(a∗ t )). (16) 14 Because we are playing FTL at both time τz and τz − 1, it holds that Lτz(a∗ τz−1) = Lτz−1(a∗ τz−1) + ℓτz(a∗ τz−1) ≤ Lτz(a∗ τz) + 1. To bound the second term, we will show that tz+1−1 X t=τz+1 ℓt(at) + Lτz(a∗ τz−1) − Ltz+1−1(a∗ tz+1−1) ≤ tz+1−1 X t=τz+1 ℓt(at) + Lτz(a∗ τz) − Ltz+1−1(a∗ tz+1−1) + 1 (17) = tz+1−1 X t=τz+1 ℓt(at) + min a Lτz(a) − min a Ltz+1−1(a) + 1 (18) ≤ tz+1−1 X t=τz+1 ℓt(at) − min a \u0000Ltz+1−1(a) − Lτz(a) \u0001 + 1 (19) = Reg(ALGWC, ℓtz+1−1 τz+1 ) + 1. (20) We now complete the proof of Theorem 3 by showing that the conditions that determine the switching time between FTL and ALGWC are appropriately chosen to upper bound ΣFTL tz:τz−1 and Reg(ALGWC, ℓtz+1−1 τz+1 ). Firstly, note that for any z < zlast, Reg(ALGWC, ℓtz+1−1 τz+1 ) > g(L∗ z), which implies that L∗ > L∗ z. In particular, the epoch zlast − 1 was exited, which implies that4 2zlast−1 log m ≤ L∗ =⇒ zlast ≤ log \u0012 L∗ log m \u0013 + 1 By the stopping condition of the epoch, Reg(ALGWC, ℓtz+1−1 τz+1 ) ≤ Reg(ALGWC, ℓtz+1−2 τz+1 ) + 1 ≤ g(L∗ z) + 1, such that substituting into Lemma 4 implies Reg(SMART, ℓn) ≤ X z \u0010 ΣFTL tz:τz−1 + g(L∗ z) + 2 \u0011 . (21) Also, it always holds that ΣFTL tz:τz−1 ≤ g(L∗ z) and for z < zlast, ΣFTL tz:τz > g(L∗ z). Therefore zlast−1 X z g(L∗ z) < X z ΣFTL tz:τz−1 ≤ Reg(FTL, ℓn) and P z ΣFTL tz:τz−1 ≤ Pzlast z g(L∗ z). To put it all together, if Pzlast z g(L∗ z) ≤ Reg(FTL, ℓn), then Reg(SMART, ℓn) ≤ 2 X z g(L∗ z) + 2zlast. (22) 4Here we have implicitly assumed that L∗ > log m—if not, then there is only one epoch, zlast = 0 and the result is readily implied by Corollary 2. 15 If Reg(FTL, ℓn) < Pzlast z g(L∗ z), then it must be that in the last epoch the algorithm never switches to ALGWC. If it switched to ALGWC it would imply that Reg(FTL, ℓn) ≥ P z ΣFTL tz:τz > Pzlast z g(L∗ z) which would violate the assumption that Reg(FTL, ℓn) < Pzlast z g(L∗ z). Therefore it must be that Reg(SMART, ℓn) ≤ 2Reg(FTL, ℓn) + 2zlast. (23) As a result, it follows that (putting the L∗ ≤ log m and the L∗ > log m cases together) Reg(SMART, ℓn) ≤ 2 min    Reg(FTL, ℓn), log \u0010 1+ L∗ log m \u0011 +1 X z=0 g(2z log m)     + 2 log \u0012 1 + L∗ log m \u0013 + 2. (24) A.1 Proof of Corollary 1 This follows from Theorem 5 and by calculating log \u0010 1+ L∗ log m \u0011 +1 X z=0 g(2z log m) = 2 √ 2 log m log \u0010 1+ L∗ log m \u0011 +1 X z=0 2z/2 + κ log m log \u0012 1 + L∗ log m \u0013 + κ log m ≤ log m 4 √ 2 √ 2 − 1 s 1 + L∗ log m + κ log m log \u0012 1 + L∗ log m \u0013 + κ log m ≤ 10 q 2 log2 m + 2L∗ log m + κ log m log \u0012 1 + L∗ log m \u0013 + κ log m (a) ≤ 10 p 2L∗ log m + κ log \u0012 1 + L∗ log m \u0013 log m + 10 √ 2 log m + κ log m where (a) follows since for nonnegative a, b √ a + b ≤ √a + √ b. B Proof of Theorem 3 Consider a large even n. We then have for horizon size n + 1, Reg(FTL, yn+1) = c(yn) 2 . Moreover, Reg(Cover, yn+1) = fn+1. Let5 pn,k := P[c(ϵn) = k + 1]. (25) We then have, E[min{c(ϵn), 2fn+1}] = n X k=1 min{k, 2fn+1} Pr[c(ϵn) = k] = n X k=1 min{k, 2fn+1}pn,k−1 5Note that k in this definition does not counting the origin as as a line crossing. 16 = n X k=0 min{k + 1, 2fn+1}pn,k = ⌊2fn+1⌋−1 X k=0 (k + 1)pn,k + 2fn+1P[c(ϵn) ≥ 2fn+1] = ⌊2fn+1⌋−1 X k=0 (k + 1)pn,k + 2fn+1P[c(ϵn) ≥ 2fn+1] + P[c(ϵn) ≤ ⌊2fn+1⌋] = ⌊2fn+1⌋−1 X k=0 kpn,k + 2fn+1  1 − ⌊2fn+1⌋−1 X k=0 pn,k   + P[c(ϵn) ≤ ⌊2fn+1⌋] (26) Upon dividing (26) by 2fn+1, we get E[min{c(ϵn), 2fn+1}] 2fn+1 = P⌊2fn+1⌋−1 k=0 kpn,k 2fn+1 +  1 − ⌊2fn+1⌋−1 X k=0 pn,k   + P[c(ϵn) ≤ ⌊2fn+1⌋] 2fn+1 . (27) Note that the third term vanishes since fn+1 → ∞ and 0 ≤ P[·] ≤ 1. We will now separately evaluate the first two terms in (27). To do this, we require an auxiliary lemma, the proof of which is provided later. Lemma 5. If k ≤ C√n for an absolute constant C, then for large enough n (n ≥ 32C2 suffices) we have e− 16C3 √n ≤ pn,k q 2 nπe−k2/2n ≤ s 1 − C/√n 1 − 2C/√ne 16C3 √n . (28) That is, pn,k = r 2 nπe−k2/2n(1 + o(1)). We now evaluate the first term in (27). Since ⌊2fn+1⌋−1 ≤ 2√n, we invoke Lemma 5 to evaluate ⌊2fn+1⌋−1 X k=0 kpn,k = (1 + o(1)) ⌊2fn+1⌋−1 X k=0 k r 2 nπe− k2 2n (29) = (1 + o(1)) r 2 nπ ⌊2fn+1⌋−1 X k=0 ke− k2 2n (30) Now, we note that since x 7→ xe− x2 2n is increasing on (0, ⌊2fn+1⌋ − 1) , by a Riemann approximation, we have that Z 2fn+1−2 0 xe− x2 2n dx − 1 ≤ ⌊2fn+1⌋−1 X k=0 ke− k2 2n ≤ Z 2fn+1 0 xe− x2 2n dx (31) and evaluating Z 2fn+1 0 xe− x2 2n dx = 1 2 Z 4f2 n+1 0 e− t 2n dt 17 = n   1 − e− 4f2 n+1 2n ! = n \u0010 1 − e− 1 π (1+o(1))\u0011 . (32) Evaluating the lower bound in (31) analogously we have ⌊2fn+1⌋−1 X k=0 ke− k2 2n = (1 + o(1))n \u0010 1 − e− 1 π (1+o(1))\u0011 (33) and therefore from (30), P⌊2fn+1⌋−1 k=0 kpn,k 2fn+1 = (1 + o(1)) \u0010 1 − e− 1 π (1+o(1))\u0011 n q 2 nπ q 2(n+1) π = (1 + o(1)) \u0010 1 − e− 1 π (1+o(1))\u0011 (34) and therefore, from (34), we have that lim n→∞ P⌊2fn+1⌋−1 k=0 kpn,k 2fn+1 = 1 − e− 1 π . (35) We now address the second term in (27) by invoking Lemma 5 and noting that ⌊2fn+1⌋−1 X k=0 pn,k = (1 + o(1)) r 2 nπ ⌊2fn+1⌋−1 X k=0 e− k2 2n (a) = (1 + o(1)) Z 2fn+1 0 e− x2 2n dx = (1 + o(1)) r 2 π Z q 2 π 0 e− t2 2 dt = 1 √ 2π Z q 2 π − q 2 π e− t2 2 dt = P   − r 2 π ≤ X ≤ r 2 π ! (36) where in (36) X ∼ N(0, 1). Then, 1 − ⌊2fn+1⌋−1 X k=0 pn,k → 1 − P   − r 2 π ≤ X ≤ r 2 π ! = 2Q  r 2 π ! . (37) Substituting (35) and (37) in (27) yields that 1 γn = E[min{c(ϵn), 2fn+1}] 2fn+1 → 1 − e− 1 π + 2Q  r 2 π ! (38) 18 and therefore γn → 1 1 − e− 1 π + 2Q \u0012q 2 π \u0013. (39) B.1 Proof of Lemma 5 We first note the following. Proposition 1 (Feller, Chapter 3, Exercise 11). pn,k = 1 2n−k \u0012n − k n/2 \u0013 . Therefore, pn,k2n 2k = \u0012n − k n/2 \u0013 = (n − k)! n 2 ! \u0000 n 2 − k \u0001 !. We now use the Stirling approximation: √ 2πm \u0012m e \u0013m e 1 12m+1 ≤ m! ≤ √ 2πm \u0012m e \u0013m e 1 12m . (40) Using (40) we have pn,k2n 2k ≤ p 2π(n − k) p 2π(n/2) p 2π(n/2 − k) · (n − k)n−k (n/2)n/2(n/2 − k)n/2−k · exp \u0012 1 12(n − k) − 1 6n + 1 − 1 6n − 12k + 1 \u0013 = r 2 nπ r n − k n − 2k (n − k)n−k2n−k nn/2(n − 2k)n/2−k · exp \u0012 1 12n − k \u0013 (a) ≤ r 2 nπ · 2n−k · (n − k)n−k nn/2(n − 2k)n/2−k exp \u0012 1 12n − k \u0013 · s 1 − C/√n 1 − 2C/√n = r 2 nπ · 2n−k (1 − k/n)n−k (1 − 2k/n)n/2−k | {z } =:T exp \u0012 1 12n − k \u0013 · s 1 − C/√n 1 − 2C/√n. (41) We now analyze the term T in (41) in more detail. Proposition 2. − k2 2n2 − ck3 n2 ≤ ln T ≤ − k2 2n2 + ck3 n2 (42) where c ≤ 15. Proof. By Taylor theorem, we have ln(1 − x) = −x − x2 2 − x3 3(1 − µ)2 for µ ∈ (0, x). (43) 19 Therefore, for k ≤ C√n and n ≥ 16C2 we have ln \u0012 1 − k n \u0013 = −k n − k2 2n2 − α1k3 n3 (44) ln \u0012 1 − 2k n \u0013 = −2k n − 2k2 2n2 − α2k3 n3 (45) for α1, α2 ∈ \u0010 1 3, 8 3 i . Evaluating ln T we have ln T = (n − k) ln \u0012 1 − k n \u0013 − \u0012 n − k 2 \u0013 ln \u0012 1 − 2k n \u0013 (a) = (n − k)   −k n − k2 2n2 − α1k3 n3 ! − \u0012 n − k 2 \u0013   −2k n − 2k2 2n2 − α2k3 n3 ! =   − k2 2n + k2 n + k2 n − 2k2 n ! + \u0012 −α1 + 1 2 + α2 2 − 2 \u0013 k3 n2 + (α1 − α2) k4 n3 = − k2 2n2 + c1 k3 n2 + c2 k4 n3 (46) where (a) follows by substituting (44) and (45). Now, since k4 n3 ≤ k3 n2 we have ln T ≤ − k2 2n2 + (|c1| + |c2|)k3 n2 (47) and on the other hand for the same reason ln T ≥ − k2 2n − (|c1| + |c2|)k3 n2 (48) The proposition follows by noticing |c1| + |c2| ≤ 15 by using that α1, α2 ∈ \u0010 1 3, 8 3 i . Using Proposition 2 in (41) we have the upper bound pn,k ≤ r 2 nπ · exp   − k2 2n + 15k3 n2 ! · exp \u0012 1 12n − k \u0013 · s 1 − C/√n 1 − 2C/√n (a) ≤ r 2 nπ · exp   − k2 2n ! · exp   16k3 n2 ! · s 1 − C/√n 1 − 2C/√n (b) ≤ r 2 nπ · exp   − k2 2n ! · exp   16C3 √n ! · s 1 − C/√n 1 − 2C/√n (49) where (a) uses 1 12n−k + 15k3 n2 ≤ 16k3 n2 , and (b) uses the fact that k ≤ C√n, which yields the upper bound in (28). The lower bound follows analogously by the Stirling approximation (40) and Proposition 2. References Ronald Fagin, Amnon Lotem, and Moni Naor. Optimal aggregation algorithms for middleware. In Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 102–113, 2001. 20 Tim Roughgarden. Beyond the worst-case analysis of algorithms. Cambridge University Press, 2021. D Blackwell. An analog of the minimax theorem for vector payoffs. Pacific Journal of Mathematics, 6(1):1–8, 1956. James Hannan. Approximation to bayes risk in repeated play. Contributions to the Theory of Games, 3:97–139, 1957. Nicolo Cesa-Bianchi and G´abor Lugosi. Prediction, learning, and games. Cambridge university press, 2006. Aleksandrs Slivkins. Introduction to multi-armed bandits. Foundations and Trends® in Machine Learning, 12(1-2):1–286, 2019. Thomas M. Cover. Behavior of sequential predictors of binary sequences. In Transactions of the Fourth Prague Conference on Information Theory, 1966. Ruitong Huang, Tor Lattimore, Andr´as Gy¨orgy, and Csaba Szepesv´ari. Following the leader and fast rates in linear prediction: Curved constraint sets and other regularities. Advances in Neural Information Processing Systems, 29, 2016. Meir Feder, Neri Merhav, and Michael Gutman. Universal prediction of individual sequences. IEEE transactions on Information Theory, 38(4):1258–1270, 1992. Alekh Agarwal, Haipeng Luo, Behnam Neyshabur, and Robert E Schapire. Corralling a band of bandit algorithms. In Conference on Learning Theory, pages 12–38. PMLR, 2017. Aldo Pacchiano, My Phan, Yasin Abbasi Yadkori, Anup Rao, Julian Zimmert, Tor Lattimore, and Csaba Szepesvari. Model selection in contextual stochastic bandit problems. Advances in Neural Information Processing Systems, 33:10328–10337, 2020. Christoph Dann, Chen-Yu Wei, and Julian Zimmert. Best of both worlds policy optimization. arXiv preprint arXiv:2302.09408, 2023. Peter Auer, Nicolo Cesa-Bianchi, Yoav Freund, and Robert E Schapire. The nonstochastic multiarmed bandit problem. SIAM journal on computing, 32(1):48–77, 2002a. Steven De Rooij, Tim Van Erven, Peter D Gr¨unwald, and Wouter M Koolen. Follow the leader if you can, hedge if you must. The Journal of Machine Learning Research, 15(1):1281–1316, 2014. Francesco Orabona and D´avid P´al. Scale-free algorithms for online linear optimization. In Interna- tional Conference on Algorithmic Learning Theory, pages 287–301. Springer, 2015. Jaouad Mourtada and St´ephane Ga¨ıffas. On the optimality of the hedge algorithm in the stochastic regime. Journal of Machine Learning Research, 20:1–28, 2019. Blair Bilodeau, Jeffrey Negrea, and Daniel M Roy. Relaxing the iid assumption: Adaptively minimax optimal regret via root-entropic regularization. The Annals of Statistics, 51(4):1850–1876, 2023. S´ebastien Bubeck and Aleksandrs Slivkins. The best of both worlds: Stochastic and adversarial bandits. In Conference on Learning Theory, pages 42–1. JMLR Workshop and Conference Proceedings, 2012. 21 Julian Zimmert and Yevgeny Seldin. An optimal algorithm for stochastic and adversarial bandits. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 467–475. PMLR, 2019. Thodoris Lykouris, Vahab Mirrokni, and Renato Paes Leme. Stochastic bandits robust to adversarial corruptions. In Proceedings of the 50th Annual ACM SIGACT Symposium on Theory of Computing, pages 114–122, 2018. Wojciech Kot lowski. On minimaxity of follow the leader strategy in the stochastic setting. Theoretical Computer Science, 742:50–65, 2018. Anna R. Karlin, Mark S. Manasse, Lyle A. McGeoch, and Susan Owicki. Competitive randomized algorithms for nonuniform problems. Algorithmica, 11(6):542–571, 1994. Allan Borodin and Ran El-Yaniv. Online computation and competitive analysis. cambridge university press, 2005. Nicolo Cesa-Bianchi, Yoav Freund, David Haussler, David P Helmbold, Robert E Schapire, and Manfred K Warmuth. How to use expert advice. Journal of the ACM (JACM), 44(3):427–485, 1997. Chen-Yu Wei and Haipeng Luo. More adaptive algorithms for adversarial bandits. In Conference On Learning Theory, pages 1263–1291. PMLR, 2018. S´ebastien Bubeck, Yuanzhi Li, Haipeng Luo, and Chen-Yu Wei. Improved path-length regret bounds for bandits. In Conference On Learning Theory, pages 508–528. PMLR, 2019. Neelkamal Bhuyan, Debankur Mukherjee, and Adam Wierman. Best of both worlds: Stochastic and adversarial convex function chasing. arXiv preprint arXiv:2311.00181, 2023. Oron Sabag, Gautam Goel, Sahin Lale, and Babak Hassibi. Regret-optimal controller for the full-information problem. In 2021 American Control Conference (ACC), pages 4777–4782. IEEE, 2021. Gautam Goel, Naman Agarwal, Karan Singh, and Elad Hazan. Best of both worlds in online control: Competitive ratio and policy regret. In Learning for Dynamics and Control Conference, pages 1345–1356. PMLR, 2023. Alexander Rakhlin, Karthik Sridharan, and Ambuj Tewari. Online learning: Stochastic, constrained, and smoothed adversaries. Advances in neural information processing systems, 24, 2011. Nika Haghtalab, Tim Roughgarden, and Abhishek Shetty. Smoothed analysis with adaptive adversaries. In 2021 IEEE 62nd Annual Symposium on Foundations of Computer Science (FOCS), pages 942–953. IEEE, 2022. Adam Block, Yuval Dagan, Noah Golowich, and Alexander Rakhlin. Smoothed online learning is as easy as statistical learning. In Conference on Learning Theory, pages 1716–1786. PMLR, 2022. Alankrita Bhatt, Nika Haghtalab, and Abhishek Shetty. Smoothed analysis of sequential probability assignment. Neural Information Processing Systems, 2023. Idan Amir, Idan Attias, Tomer Koren, Yishay Mansour, and Roi Livni. Prediction with corrupted expert advice. Advances in Neural Information Processing Systems, 33:14315–14325, 2020. 22 Peter Auer, Nicolo Cesa-Bianchi, and Claudio Gentile. Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48–75, 2002b. Nicolo Cesa-Bianchi, G´abor Lugosi, and Gilles Stoltz. Minimizing regret with label efficient prediction. IEEE Transactions on Information Theory, 51(6):2152–2162, 2005. Elad Hazan and Satyen Kale. Extracting certainty from uncertainty: Regret bounded by variation in costs. Machine learning, 80:165–188, 2010. Wouter M Koolen, Tim Van Erven, and Peter Gr¨unwald. Learning the learning rate for prediction with expert advice. Advances in neural information processing systems, 27, 2014. Tim Van Erven, Peter Grunwald, Nishant A Mehta, Mark Reid, Robert Williamson, et al. Fast rates in statistical and online learning. 2015. Tim Van Erven and Wouter M Koolen. Metagrad: Multiple learning rates in online learning. Advances in Neural Information Processing Systems, 29, 2016. Pierre Gaillard, Gilles Stoltz, and Tim Van Erven. A second-order bound with excess losses. In Conference on Learning Theory, pages 176–196. PMLR, 2014. Etienne Bamas, Andreas Maggiori, and Ola Svensson. The primal-dual method for learning augmented algorithms. Advances in Neural Information Processing Systems, 33:20083–20094, 2020. Michael Dinitz, Sungjin Im, Thomas Lavastida, Benjamin Moseley, and Sergei Vassilvitskii. Al- gorithms with prediction portfolios. Advances in neural information processing systems, 35: 20273–20286, 2022. Keerti Anand, Rong Ge, Amit Kumar, and Debmalya Panigrahi. Online algorithms with multiple predictions. In International Conference on Machine Learning, pages 582–598. PMLR, 2022. Adam Kalai and Santosh Vempala. Efficient algorithms for online decision problems. Journal of Computer and System Sciences, 71(3):291–307, 2005. Nicolo Cesa-Bianchi, Yishay Mansour, and Gilles Stoltz. Improved second-order bounds for prediction with expert advice. Machine Learning, 66:321–352, 2007. William Feller. An introduction to probability theory and its applications, Volume 1, Third Edition. John Wiley & Sons, New York. 23 "
}