{
    "optim": "Learning-Based Algorithms for Graph Searching Problems\nAdela Frances DePavia\nadepavia@uchicago.edu\nUniversity of Chicago\nErasmo Tani\netani@uchicago.edu\nUniversity of Chicago\nAli Vakilian\nvakilian@ttic.edu\nTTIC\nFebruary 28, 2024\nAbstract\nWe consider the problem of graph searching with prediction recently introduced by Banerjee et al.\n(2023). In this problem, an agent, starting at some vertex r has to traverse a (potentially unknown)\ngraph G to find a hidden goal node g while minimizing the total distance travelled. We study a setting in\nwhich at any node v, the agent receives a noisy estimate of the distance from v to g. We design algorithms\nfor this search task on unknown graphs. We establish the first formal guarantees on unknown weighted\ngraphs and provide lower bounds showing that the algorithms we propose have optimal or nearly-optimal\ndependence on the prediction error. Further, we perform numerical experiments demonstrating that in\naddition to being robust to adversarial error, our algorithms perform well in typical instances in which\nthe error is stochastic. Finally, we provide alternative simpler performance bounds on the algorithms of\nBanerjee et al. (2023) for the case of searching on a known graph, and establish new lower bounds for\nthis setting.\n1\nIntroduction\nSearching on graphs is a fundamental problem which models many real-world applications in autonomous\nnavigation. In a graph searching problem instance, an agent is initialized at some vertex r ∈ V (referred to as\nthe root) in some (potentially weighted, directed) graph G = (V, E). The agent’s task is to find a goal node\ng ∈ V . The agent searches for g by sequentially visiting adjacent nodes in the graph. The graph searching\nproblem terminates when the agent reaches the goal node, and the cost incurred by the agent is the total\namount of distance they travelled.\nThere are two main settings of interest. In the exploration setting, as the agent moves through the graph,\nit only learns the structure of G by observing the vertices and edges adjacent to the nodes it has visited, a\nmodel sometimes referred to as the fixed graph scenario (Komm et al., 2015; Kalyanasundaram and Pruhs,\n1994). In the strictly-easier planning setting, the agent is given the entire graph G ahead of time, but does\nnot know the identity of the goal node g.\nWithout additional information, in the worst-case the agent must resort to visiting the entire graph, a\ntask which amounts to finding an efficient tour in an unknown graph1(Berman, 2005; Dobrev et al., 2012;\nMegow et al., 2012; Eberle et al., 2022).\nRecently, Banerjee et al. (2023) consider the setting when an\nalgorithm for graph searching also receives some prediction function, f : V → R, representing some (noisy)\nestimate of the distance to the goal node g at any given node v ∈ V . This setup models applications in\nwhich the searcher receives advice from some machine-learning model designed to predict the distance to the\ngoal; this problem fits into the broader framework of learning-based algorithms, which exploit (potentially\nnoisy) advice from a machine learning model to enhance their performance.\nIn the case of planning, Banerjee et al. (2023) propose an intuitive strategy that can be deployed in\nweighted and unweighted graphs and analyze its performance in terms of structural properties of the instance\ngraph, such as its maximum-degree and its doubling-dimension. They establish formal guarantees on the\ncost incurred by their algorithm under different notions of prediction error. In contrast, their results in\nthe exploration setting are limited: they propose an algorithm for exploration in unweighted trees. Their\n1We note that historically the task of finding an efficient tour in an unknown graph has been referred to as graph exploration,\nbut following the conventions of Banerjee et al. (2023) we reserve this name for the graph search problem on an unknown graph.\n1\narXiv:2402.17736v1  [cs.DS]  27 Feb 2024\nalgorithm is tailored to this restricted class of graphs and their guarantees are parameterized by the number\nof incorrect predictions, which could in general be a poor measure of the accuracy of the prediction model.\nIn particular, this parameterization is not well suited for settings in which the prediction function incurs\nsome small error at every node.\nThis paper seeks to expand the understanding of graph exploration problems with predictions. We design\nalgorithms which can be deployed on a variety of weighted graphs and prove worst-case guarantees on their\nperformance. We also complement this analysis by providing lower-bounds for these problems, showing that\nthe algorithms studied in this work are optimal or nearly optimal. In particular, we focus on two different\nerror models. In the absolute error model, the magnitude of the error at a node is independent of that\nnode’s true distance to the goal, and guarantees are given in terms of the total magnitude of the error\nincurred at every node. In the relative error model, nodes further from the goal may have larger deviation\nbetween the prediction and the truth, and algorithmic guarantees are parameterized by the maximum ratio\nof the error to the true distance at any vertex.\nRelated Works\nOnline graph searching problems have long been used as basic models for problems in\nautonomous navigation Berman (2005). Searching with access to predictions, also referred to as “advice”\nor “heuristics” in different communities, is a commonly studied variant (Pelc, 2002; Dobrev et al., 2012;\nEberle et al., 2022; Banerjee et al., 2023). The problem and prediction settings considered in this work most\nclosely correspond to those considered by Banerjee et al. (2023). This setup models applications in which\npredictions are the output of some machine-learning model. Recent years have seen a marked increase in\nthe integration of machine learning techniques to enhance traditional algorithmic challenges (Angelopoulos\net al., 2020; Gupta et al., 2022; Mitzenmacher and Vassilvitskii, 2022; Antoniadis et al., 2023). More generic\nforms of advice and the advice-complexity of exploration tasks are long-standing subjects of study. Komm\net al. (2015) study the case when the the searcher receives generic advice, which can take the form of any\nbit string, and prove results about the advice complexity of this task. For a more detailed survey of related\nworks, we direct the reader to Section A.\nOrganization\nIn Section 1.1 we formally state the main results of the paper. In Section 2 we present\ntechnical preliminaries and define relevant notation. Sections 3 and 4 contain algorithms and analysis for\nexploration under absolute and relative error models respectively. In Section 5 we derive new bounds in the\nplanning setting via metric embeddings. Finally, in Section 6 we complement these results with numerical\nexperiments. All missing proofs are in Section B of the supplementary material.\n1.1\nSummary Of Results\nWe begin by formally describing the exploration and planning settings:\nThe Exploration Problem In this setting, both the graph G and the predictions f are initially unknown\nto the agent: the agent is initialized with access to the root node r, the neighbors of r, and the\npredictions at all of these nodes.\nAs the algorithm proceeds, on each iteration i it has access in\nmemory to a subgraph Gi ⊆ G containing the nodes it has visited, the neighbors of those nodes, and\nany edges between visited nodes and neighbors. The searcher can only query predictions from nodes\nin the subgraph Gi. This problem models exploration of an unknown environment.\nThe Planning Problem In this setting, both the graph G and the predictions f are fully known to the\nsearcher upon initialization.\nFor any algorithm which visits an ordered sequence of vertices v1, . . . , vT , we denote the algorithmic cost\nALG\ndef\n= P\ni∈T dGi(vi, vi+1). The guarantees in this paper compare ALG to the optimal cost a posteriori,\ndenoted by OPT\ndef\n= dG(r, g).\nExploration Under Absolute Error\nA natural way of measuring the error of some predictions is the\nmagnitude of the difference between the true distance-to-goal and prediction value at each node. In Section 3\nwe propose an algorithm for the exploration problem on weighted graphs and prove performance guarantees\nparameterized by these error measures. In particular, we prove the following theorem:\n2\nTheorem 1. There is an algorithm for searching arbitrary (potentially directed) graphs which finds the goal\ng by traveling a distance of at most OPT + E−\n1 + n · E+\n∞, where E−\n1\ndef\n= P\nv∈V max {0, d(v, g) − f(v)} and\nE+\n∞\ndef\n= maxv∈V max {0, f(v) − d(v, g)}.\nThis algorithm enjoys several advantages over the most recent results on the exploration problem by\nBanerjee et al. (2023). Their work introduces an involved combinatorial algorithm for exploration on un-\nweighted trees whose performance is parametrized by the ℓ0 norm of the vector of errors. In particular, the\nguarantees of their algorithm are trivial when most or all nodes contain some (possibly very small) error,\nand they do not apply when the graph being searched is not an unweighted tree. In contrast, the algorithm\nproposed in the present work is intuitive and easily implementable, and the guarantees obtained hold in a\nwide variety of settings, e.g. when the graph is weighted and/or directed. The performance of the algorithm\nis parameterized by the natural absolute deviation (ℓ1) error measure.\nWe also establish that under the above parameterization, the proposed algorithm is in some sense optimal\n(see Theorem 6). Further our analysis highlights that the same algorithm performs particularly well when\n(erroneous) predictions only underestimate distance to the true goal. Prediction functions with this property\nare referred to as admissible. They are key objects of study in path-finding literature and are well-motivated\nby applications (Dechter and Pearl, 1985; Eden et al., 2022; Ferguson et al., 2005; Pohl, 1969).\nRelative Errors\nOne realistic setting for applications is one in which the error is proportionate to the\nmagnitude of the distance to the goal. In order to capture this behavior, we consider a different error model\nin which the ratio of the error to true distance-to-goal at every vertex is assumed to be bounded by some\nvalue ε ∈ (0, 1):\n(1 − ε)d(v, g) ≤ f(v) ≤ (1 + ε)d(v, g).\n(1)\nWe do not place any restriction on the total amount of error in the graph beyond this condition.\nWe consider two regimes of multiplicative error. In the first setting, ε is assumed to be known to the\nsearcher a priori. We propose an algorithm and show that it achieves the following competitive ratio:\nTheorem 2. Consider the exploration problem on a weighted tree where predictions satisfy (1) with respect\nto ε ∈ (0, 1), and ε is known. Then there exists an algorithm which succeeds in finding the goal g and incurs\ncompetitive ratio at most\nALG\nOPT ≤\n1\n1 − ε + nε ·\n4\n(1 − ε)2 .\nIn particular, if the predictions are admissible, then the same algorithm incurs competitive ratio\nALG\nOPT ≤ 1 + nε ·\n2\n1 − ε.\nIn the second regime ε is assumed to be small (ε < 1/3) but its exact value is not assumed to be known.\nFor this setting, we design a different algorithm which allows us to prove the following result:\nTheorem 3. Given G a weighted tree with predictions f satisfying Equation (1) for some unknown ε < 1/3,\nthen Algorithm 3 with β = 2/3 incurs competitive ratio at most\nALG\nOPT ≤ 2 + O\n\u0012\nnε 5 + 3ε\n(1 − 3ε)2\n\u0013\n.\nIn Section 4, we describe our algorithms for these problems, prove the above theorems, and complement\nthese algorithmic guarantees with lowerbounds that show these algorithms are nearly optimal.\nPlanning Problems\nBanerjee et al. (2023) consider problems in which both the full graph and all predic-\ntions are available to the algorithm upon initialization. This setting is referred to as the planning problem.\nThey construct algorithms for this version of the problem under different error models and the guaran-\ntees they obtained are outlined in Table 1. In many regimes (e.g. planning on unweighted trees under\nerror parametrized by the ℓ0-norm of the vector of errors, denoted E0, and planning on graphs under error\nparametrized by the ℓ1-norm of the vector of errors, denoted E1) matching lowerbounds for their algorithms\n3\nE0, unweighted\nE1, positive weights\nTrees with\nmaximum degree ∆\nupper bound\nOPT + O(∆E0) (†)\nOPT + O(∆E2\n1)\n(integer distances)\nlower bound\nmax {OPT, Ω(∆E0)} (†)\nmax\n\b\nOPT, Ω(∆E2\n1)\n\t\nGraphs with\ndoubling dimension α\nupper bound\nOPT + 2O(α)O(E2\n0) (†)\nOPT + 2O(α)O(E1) (†)\nlower bound\nUnknown\nmax {OPT, Ω(2αE1)}\nGraphs with path-\nembedding distortion ρ\nupper bound\nOPT + O(ρE0)\nOPT + O(ρE1)\nlower bound\nUnknown\nmax {OPT, Ω(ρE1)}\nTable 1: Known results for planning problem in different settings. (†) denotes results from Banerjee et al.\n(2023). ∆ denotes the maximum degree of any vertex in the graph, α denotes the doubling-dimension of the\ngraph, and ρ denotes the distortion of the path-embedding on G. Banerjee et al. (2023) in their work note\nthe absence of a matching lowerbound in the graph planning problem. This gap motivates this work’s study\nof planning bounds parameterized by metric embeddings, which yields a reduced asymptotic dependency on\nE0. For full discussions of lowerbounds for E1 parametrized by ∆, α, and ρ, see Lemmas 11 and 13, and\nLemma 20 in Section B of the supplementary material.\ncan be established, as shown in the table. A notable exception is the case of planning on unweighted graphs\nunder E0 parametrization: they establish an upperbound of OPT + 2O(α)O(E2\n0) where α is the doubling\ndimension of the graph. In particular, the lowerbounds they provide fail to match the depedence on the\nquadratic term E2\n0 in their upperbound.\nWe provide an alternative analysis of their algorithm based on metric properties of the instance graph\nwhich shows that in some classes of graphs one can reduce the asymptotic dependence on E0 from a quadratic\nto a linear factor. In particular, we consider the distortion of embedding the instance graph into a weighted\npath or cycle graph, and establish the following guarantee:\nTheorem 4. Consider G an unweighted graph such that G admits an embedding into a weighted path or\na weighted cycle of distortion at most ρ. Then on G, the E0 planning algorithm of Banerjee et al. (2023)\nincurs cost at most OPT + O(ρE0).\nWe note that the results present in Table 1 which were not proved by Banerjee et al. (2023) are established\nin this paper in Section 5 and the proved in the supplementary material.\n2\nTechnical Preliminaries and Notation\nGraphs are assumed to be weighted and directed, unless otherwise specified.\n(Weighted) shortest-path\ndistances in a graph G are denoted by dG(·, ·). Given a set S ⊆ V , let ∂S be its external vertex boundary:\n∂S\ndef\n= {v ∈ G \\ S | ∃u ∈ S : v ∼ u}.\nFor a vertex set S ⊆ V , we denote by tourG(S) the (weighted) length of the shortest walk that visits all\nnodes in S:\ntourG(S)\ndef\n= max\nv∈S\nmin\nW ∈W(v,S) lengthG(W),\n(2)\nwhere W(v, S) is the set of walks in G starting at vertex v and visiting every vertex in S.\nGiven a metric space (X, dX) its doubling constant is the minimum number λ such that, for every r > 0,\nevery ball of radius r can be covered by at most λ balls of radius r/2 (Gupta et al., 2003). The doubling\nconstant of a graph G is the doubling constant of (G, d) where d is the shortest path distance on G. The\ndoubling dimension of the space, denoted α, is defined as α\ndef\n= log2 λ.\n4\nNotation For Exploration Algorithms\nRecall that in exploration problems, the true graph G is initially\nunknown to the searcher. Thus in the setting of exploration, one needs to distinguish between the shortest\nknown path distance between two vertices and the true shortest path distance. To this end, let Vi−1 be\nthe set of vertices visited by iteration i and let Gi be the subgraph of G containing: all of the vertices in\nVi−1 ∪ ∂Vi−1, and all of the edges adjacent to Vi−1. Throughout this paper, we emphasize the distinction\nbetween dGi and dG. In general, we have dGi ≥ dG.\nWhen analyzing performance, we denote the progress made on the ith iteration as\n∆i\ndef\n= dG(vi, g) − dG(vi+1, g).\n(3)\nObserve that, under the assumption that v0, v1, . . . , vT are nodes visited by some algorithm which originates\nat r and terminates at g (i.e. v0 = r and vT = g) we have:\nT −1\nX\ni=0\n∆i = dG(r, g) − 0 = OPT.\nMetric Embeddings\nWhen analyzing the planning algorithms of Banerjee et al. (2023), we consider\nmetric embeddings on graphs, and parametrize results in terms of the distortion of the relevant embedding:\nDefinition 1 (Distortion). Given a function τ : X → Y between two finite metric spaces (X, dX) and\n(Y, dY ), we define the distortion dist(τ) of τ as the minimum value ρ satisfying the following: there exists a\nconstant c > 0 such that for all x1, x2 ∈ X,\nc · dX(x1, x2) ≤ dY (τ(x1), τ(x2)) ≤ c · ρ · dX(x1, x2).\n3\nExploration Under Absolute Error\nIn this section we study the absolute error regime, in which the ℓ1 norm of the vector of errors is bounded by\nsome constant E1, not necessarily known to the searcher. We consider the following natural rule: on the ith\niteration, choose the next vertex to visit by picking the node vi ∈ ∂Vi−1 minimizing the sum of dGi(vi−1, vi)\nand f(vi) (See Algorithm 1). This iterative step can be interpreted as visiting the vertex that would be on\nthe shortest path to the goal if all the predictions f(v) were correct.\nAlgorithm 1 ℓ1-Greedy_Search(G, r)\nv0 ← r\ni ← 0\nV0 ← {r}\nwhile vi ̸= g do\ni ← i + 1\nvi ∈ argminv∈∂Vi−1 dGi(vi−1, v) + f(v)\nVi ← {v0, ..., vi}\nend while\nWe remark that because we are working in the exploration setting, Algorithm 1 does not have access to\nthe true distances dG(·, ·) and must instead make use of dGi(·, ·) the distances in the subgraph of observed\nvertices at iteration i.\nTheorem 1 parametrizes the worst-case guarantees for Algorithm 1 in terms of the total negative error E−\nand the maximum-occurring positive error E+\n∞. This asymmetry corresponds to the intuition that positive\nerrors can obstruct the search task more dramatically than negative errors by obscuring shortest paths to\nthe goal. Indeed, an immediate corollary of Theorem 1 is the following result about the performance of\nAlgorithm 1 in the setting in which the prediction function is admissible (i.e. error is only negative):\n5\nCorollary 5. Consider the problem of searching a weighted (possibly directed) graph with predictions f\nsatisfying f(v) ≤ dG(v, g) ∀v ∈ V Then there exists an algorithm which finds the goal g with cost at most\nOPT + E1, where E1 is the total ℓ1 error in the predictions, i.e. E1\ndef\n= P\nv∈V |f(v) − d(v, g)|.\nThe dependency on E− and E+\n∞ in Theorem 1 is optimal in the following sense:\nTheorem 6. For every E− > 0, there exist graph search instances with total negative error E− such that\nany algorithm for the exploration problem on these instances must incur cost at least OPT+E− in the worst\ncase. Additionally, for any n > 3 and any E+\n∞, there exist graph search instances on n nodes with maximum\npositive error E+\n∞ such that any algorithm for the exploration problem on these instances must incur cost at\nleast OPT + E+\n∞(n − 2) in the worst case.\nWe note that the lower bound in Theorem 6 also holds for the expected distance travelled of randomized\nsearch strategies, up to constant factors, as per the following result.\nProposition 7. For every E− > 0 there exists a graph search instance with total negative error E− such\nthat any randomized algorithm incurs expected costs at least OPT+ 1\n2E− on this instance. Moreover, for any\nn > 3 and any E+\n∞ there exists a graph search instance on n nodes with maximum positive error E+\n∞ such\nthat any randomized algorithm must incur expected cost at least OPT + (n − 2)E+\n∞/2 on this instance.\nThe full proof of Theorem 1 and the proof of Corollary 5, given in Section B, rely on a charging argument\nwhich shows that distance travelled away from the goal can be directly attributed to errors in the predictions\nof observed nodes. The proof of Theorem 6 and Proposition 7 are constructive and can also be found in\nSection B.\nFor completeness we now sketch the proof of Theorem 1. The proof considers the progress ∆i as in\nEquation (3), which measures how much closer the agent is to the goal after the ith iteration of the algorithm.\nThe cost of the algorithm is given by ALG = P\ni∈[T ] dGi(vi−1, vi). At each step i ∈ [T], one can show that\nthe distance travelled dGi(vi−1, vi) in the observed subgraph Gi is bounded above by the sum of three terms:\ndGi(vi−1, vi) ≤ ∆i + E−(vi) + E+(wi),\n(4)\nwhere E−(vi) is the negative error at vi, and E+(wi) is the positive error at some vertex wi on a shortest\npath from vi−1 to g. In particular, all three terms in this upper bound are independent of the observed\nsubgraph Gi. The statement of the theorem then follows by summing both sides of Equation (4) over all\ni ∈ [T].\n4\nExploration Under Relative Error\nIn this section, we consider the setting when the prediction function satisfies Equation (1) for every v ∈ V .\nWe assume that ε ∈ (0, 1): in particular, if ε ≥ 1, then f(v) = 0 is a valid prediction at every vertex and no\nexploration algorithm can avoid visiting the entire graph in the worst case.\nIf ε is known to the searcher a priori then given access to the prediction at a node, the searcher can\nconstruct an upper bound on the true distance-to-goal. On trees this allows one to limit exploration to a ball\nof some radius R (dependent on ε and OPT) around the initial vertex, effectively “pruning” distant nodes\nfrom the vertex set. In particular, one could limit their search to the set:\nSε,r\ndef\n=\n\u001a\nv ∈ V | dG(v, r) ≤\n1\n1 − εf(r)\n\u001b\n.\n(5)\nWe couple this observation with the algorithm in the previous section and obtain the following algorithm.\n6\nAlgorithm 2 ε-Known_Search(G, r, ε)\nv0 ← r\ni ← 0\nV0 ← {r}\nwhile vi ̸= g do\ni ← i + 1\nvi ∈ argminv∈∂Vi−1∩Sε,r dGi(vi−1, v) + f(v)\nVi ← {v0, ..., vi}\nend while\nIn Section B we leverage favorable properties of the set Sε,r to show that Algorithm 2 satisfies the\nguarantees of Theorem 2. We observe that in particular, Theorem 2 implies that for any n the competitive\nratio incurred by Algorithm 2 tends to 1 as ε → 0. The combination of the truncation with the shortest-path\nrule in Algorithm 2 was necessary to secure this property: for example, a simple scheme such as running\nbreadth-first-search on the truncated set Sε,r would not enjoy such a guarantee in the worst case.\nWe note that Algorithm 2 crucially relies on the fact that the searcher knows the value of ε and so\nit cannot be deployed in the setting where ε is unknown. For the latter regime we propose an alternative\nalgorithm, also based on Algorithm 1, which provably succeeds for unknown values of ε under the assumption\nthat ε is small, e.g. ε < 1/3.\nAlgorithm 3 ε-Unknown_Weighted_Search(G, r, β)\nv0 ← r\ni ← 0\nV0 ← {r}\nwhile vi ̸= g do\ni ← i + 1\nvi ∈ argminv∈∂Vi−1 βdGi(vi−1, v) + f(v)\nVi ← {v0, ..., vi}\nend while\nIn Section B we show that on trees this reweighting scheme ensures that Algorithm 3 never explores\nnodes which are far from the goal and use this property to establish the guarantees in Theorem 3 under the\nsetting where β = 2/3.\nWe give a lower bound to establish that Algorithm 2 is almost optimal. Specifically, we show that even\nwhen ε is known a-priori, the asymptotic dependence on n · ε is tight up to factors of 1/(1 − ε):\nTheorem 8. For all n sufficiently large (n ≥ 6) and for any ε ∈ (0, 1), there exists an instance I of the\nexploration on weighted trees G with predictions (1), such that any algorithm for the exploration problem on\nG must incur cost ALG ≥ (1 + nε)OPT on I.\nNote that this lower bound also applies to the regime addressed by Algorithm 3. Moreover, one can prove\nan analogous lower bound for the case of potentially randomized exploration strategies, as per the following\nresult.\nProposition 9. For all n sufficiently large (n ≥ 6) and for any ε ∈ (0, 1), there exists an instance I of the\nexploration on weighted trees G with predictions (1), such that any randomized algorithm for the exploration\nproblem on G must incur cost ALG ≥ (1 + nε\n2 )OPT on I.\n4.1\nPlanning With Relative Error\nUnder the model of relative error described by Equation (1), planning problems become either trivial or\nimpossible, with no intermediate regimes. In the context of predictions f(v) = (1 + εv)dG(v, g) for some\nεv ∈ [−ε, ε], we observe that it must be that f(g) = 0, independent of the value of ε. For the planning\nproblem to be nontrivial, there must also occur vertices v ̸= g such that f(v) = 0. Thus, for nontrivial\ninstances of the planning problem in this regime, ε ≥ 1. However, instances with such (large) values of ε are\n7\nhopeless in the worst case: such a setting allows for the prediction of every node to be set equal to 0, a case\nwhich forces the searcher to visit every node in the worst case.\n5\nPlanning\n5.1\nPlanning Bounds Via Metric Embeddings\nIn this section, we analyze the performance of the algorithm by Banerjee et al. (2023) for the planning\nproblem, as a function of how similar the target graph is to some graph G′ admitting inexpensive tours. In\nthe planning problem, the full graph G as well as all predictions f are made available to the algorithm upon\ninitialization. Banerjee et al. (2023) study the implied error functions ϕ0 : V → R and ϕ1 : V → R, defined\nas\nϕ0(v)\ndef\n= |{u ∈ V : f(u) ̸= dG(u, v)}|\nand\nϕ1(v)\ndef\n=\nX\nu∈V\n|f(u) − dG(u, v)|.\nBanerjee et al. (2023) consider an algorithm that iteratively visits sublevel sets of ϕ0 or ϕ1 respectively, for\ngeometrically increasing thresholds (see Algorithm 4). Their algorithm is very simple: for every threshold, it\nvisits every node in the sublevel set before increasing to the next threshold value. Algorithmically, Banerjee\net al. (2023) accomplish this by computing a minimum length Steiner tree of the sublevel set, which is in\ngeneral not computationally efficient. However, one can replace this computationally expensive procedure\nwith a polynomial-time constant factor approximation (see e.g. (Karlin et al., 2021)), which preserves the\nasymptotic upper bound on the algorithms’ performance.\nAlgorithm 4 FullInfoX (G, r, ϕ) from Banerjee et al. (2023)\nVi ← r\nλ ← 0\nwhile g ̸∈ Vi do\nVi ← Vi ∪ L−\nϕ (2λ)\nCompute a minimum length Steiner tree of Vi and perform an Euler tour of the tree\nλ ← λ + 1\nend while\nThis definition of Algorithm 4 motivates our analysis, which focuses on the distortion of embedding the\ninstance graph into some graph G′ which admits inexpensive tours. Recall the definition of tourG(S) as in\nEquation (2).\nDefinition 2 (Easily-tourable). A graph G′ = (V ′, E′) is c-easily-tourable for some c > 0 if for any S′ ⊆ V ′,\ntourG′ ≤ c · diam(S′).\nIn Section B, we establish that in easily-tourable graphs the algorithm of Banerjee et al. enjoys good\nperformance.\nWe then show that if a graph G can be embedded into an easily-tourable graph G′ with\ndistortion ρ, then G itself must be easily-tourable with tour costs that scale with ρ. This culminates in the\nfollowing result:\nLemma 10. Given G an unweighted graph, if G admits an embedding τ : G → G′ of distortion ρ for G′ some\ncG′-easily-tourable graph, then Algorithm 4 with objective ϕ = ϕ0 from Banerjee et al. (2023) incurs cost at\nmost OPT+O(ρ·cG′ ·E0). If G has integer-valued distances and admits an embedding of distortion ρ into G′\nsome easily-tourable graph, then Algorithm 4 with objective ϕ = ϕ1 incurs cost at most OPT + O(ρ · cG′ · E1).\nIn particular, (weighted) paths and cycles are easily-tourable with respect to constant c, resulting in the\nguarantees in Theorem 4. In Section C we give results suggesting that our analysis of planning problems via\nmetric embeddings is a refinement of the analysis of Banerjee et al. (2023).\n8\n1\n1\nr\n1\n1\n1\n1\n...\n...\n...\n...\n...\n...\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\ng\nFigure 1: The lower bound construction for the proof of Lemmas 11 and 13.\n5.2\nLowerbounds For Planning\nIn this section, we provide lowerbounds that extend the results in Banerjee et al. (2023). Consider the\nplanning problem on some weighted graph G with integer weights2 where prediction error is parameterized\nby E1. Banerjee et al. (2023) propose an algorithm which provably incurs cost at most OPT+O(E1poly(λ)),\nwhere λ is the doubling constant of the input graph. We provide complementary lower bounds by establishing\nthe following result:\nLemma 11. Let A be any algorithm for the planning problem which is guaranteed to incur cost: OPT +\nO(Ea\n1 λb) for some a, b ∈ R when run on a weighted graph G with doubling constant λ, and with a error vector\n⃗e such that ∥e∥1 = E1. Then it must be the case that a ≥ 1. Moreover, if a = 1, then b ≥ 1.\nOur lowerbounds are constructive, and consider a family of graphs illustrated in Figure 1. A full proof\ncan be found in Section B.\nIt is known that the doubling constant is always at least as large as the maximum degree but we note\nthat in general it may be much larger even if one restricts themselves to trees. We analyze the algorithm\nof Banerjee et al. (2023) and prove that in trees with integer weights performance of their algorithm can be\nbounded in terms of maximum degree, at the cost of paying a higher asymptotic dependence on E1:\nLemma 12. Given G a tree with integer-valued distances and maximum degree ∆, consider the planning\nproblem on G with predictions satisfying E1 ≥ 1. Then on this problem instance Algorithm 4 with objective\nϕ = ϕ1 from Banerjee et al. (2023) incurs cost at most OPT + O(∆E2\n1).\nWe establish corresponding lowerbounds via a similar construction to that in Lemma 11:\nLemma 13. Let A be any algorithm for the planning problem on trees with integer weights which is guar-\nanteed to incur cost: OPT + O(Ea\n1 ∆b) for some a, b ∈ R when run on a tree G with maximum degree ∆ and\nwith a prediction vector ⃗e such that ∥e∥1 = E1. Then it must be the case that a ≥ 1 and b ≥ 1. Moreover, it\nmust be that a + b ≥ 3.\n2We note that the analysis of Banerjee et al. for the above setting also appears to go through for the case non-integer\nweights, and that the lowerbound provided by Lemma 11 would then hold for that setting too.\n9\n6\nNumerical Experiments: Impact of Random Errors\nThroughout this paper we have focused on worst-case theoretical guarantees. In this section, we provide\nnumerical results exploring the effectiveness of Algorithms 1 and\n3 on exploration problems beyond the\nworst-case setting, particularly under random errors. The experiments show that in addition to being robust\nto adversarial error, the algorithms considered perform well in instances with stochastic error. Moreover,\nwe find that although the guarantees for Algorithm 3 were shown only for trees, empirically the algorithm\nalso succeeds on general (cyclic) graphs. These results suggest that Algorithms 1 and 3 could be deployed\neffectively for exploration problems in practice.\nWe study the performance of Algorithm 1 in the absolute regime and Algorithm 3 in the relative regime.\nIn the left subfigure of Figure 2 we plot the performance of Algorithm 1 for different graph topologies when\nthe error is sampled at random in an absolute fashion. We plot the performance against the total ℓ1-norm of\nthe error vector. In the right subfigure of Figure 2 we explore the performance of Algorithm 3 against relative\nerror. Once again, we find that the algorithm enjoys empirical performance superior to that predicted by\nthe worst-case upper bound. In particular, the gap between the worst-case bound and the average empirical\nperformance is consistent over families of graphs with very different topologies.\nIn Table 2 we report the average ratio of algorithmic cost incurred to value of the upper-bound in\nTheorem 1 (given as a percentage). We compute this percentage for different classes of graphs over 100 runs\nof these experiments when the error, initial node and goal node, and graph structure (when applicable) have\nbeen sampled at random.\nFigure 2: Performance of Algorithms 1 and 3 against random errors. Experimental procedures are detailed\nin Section D. The number of nodes is fixed over all graph topologies and error settings. LEFT: Average\nand standard deviation of ALG − OPT incurred by Algorithm 1 over 2000 independent random trials for\nvarying values of E1. RIGHT: Average and standard deviation of ALG/OPT incurred by Algorithm 3 over\n2000 independent random trials for varying values of ε.\nWe also empirically compare the performance of Algorithm 1 to another natural heuristic, which we call\nSmallest Prediction. In Smallest Prediction, the agent always travels to the vertex v in ∂Vi−1 with the\nsmallest value of the prediction function f(v). While our theoretical results already show that Algorithm 1\nachieves optimal performance in the worst-case, we show that our algorithm performs better than Smallest\nPrediction in the presence of random error across a variety of graph topologies.\nIn Figure 3, we plot\nthe average performance of Algorithm 1 against the performance of Smallest Prediction (measured as the\ndistance travelled by the agent, minus OPT, as a fraction of of OPT) in random trees with 100 vertices for\na growing value of the magnitude E1 of the error vector. More details on this comparison can be found in\nthe supplementary material.\nFurther details of our experiments, including details on the error models and the graph families being\nused in the experiments can be found in Section D in the supplementary material.\n10\nAlgorithm 1\nSmallest Prediction\n(ALG-OPT)/OPT\nFigure 3: A comparison of the performance of Algorithm 1 with the Smallest Prediction heuristic. We plot\nthe average and the standard deviation of the performance of Algorithm 1 and that of the Smallest Prediction\nheuristic against the magnitude of the error vector E1. Experiments in this figure were conducted on random\ntrees; for analogous results on other graph topologies, see Figure 9 in Appendix D.\nGRAPH\nFAMILY\nRandom\nLobster\nErdös\nRényi\nRandom\nTree\nCircular\nLadder\nCOST(%)\n2.4 ± 2.5\n3.1 ± 4.3\n1.7 ± 2.3\n0.7 ± 0.5\nTable 2: Average empirical cost of running Algorithm 1 as a percentage of the upperbound in Theorem 1\nfor different family of graphs. Experiments performed on graphs with 300 nodes. These results demonstrate\nthat when run with randomly-generated errors, the actual cost incurred by the algorithm is a very small\nfraction of the upperbound.\n7\nConclusions and Future Directions\nIn this work we have introduced new general algorithms for the problem of searching in an unknown graph.\nUnder the absolute error model we design algorithms which succeed in a broad class of graphs and prove\nthat these algorithms are optimal (Section 3). We then move beyond the absolute error regime and consider\nrelative error; to the best of the authors’ knowledge, this work is the first to address the exploration problem\nunder this natural error model. Within this setting we propose algorithms for the exploration problem on\nweighted trees and show that their performance is nearly-optimal (Section 4).\nWe complement our advances in the exploration setting by expanding the landscape of results for the\nplanning problem.\nWe extend the work of Banerjee et al. (2023) by providing alternative performance\nguarantees which establish a linear–rather than quadratic–dependency on the error parameter E0 in some\ngraph families, and which suggests that such a lower asymptotic dependency may be attainable in general\n(Theorem 4). We also complete the results of Banerjee et al. in the planning setting on integer-distance\ngraphs by proving one cannot improve the factor of E1 in their upperbound and that achieving this linear\ndependence on the error requires cost linear in the doubling constant λ of the instance graph (Lemma 11).\nThe work in this paper directly suggests several avenues for further study.\nWhile our lowerbounds\ndemonstrate the impossibility of uniformly improving the results in Theorem 1, it is possible the bound may\nbe overly pessimistic in certain classes of graphs; it would be interesting to consider whether making stronger\nstructural assumptions about the instance graph would yield better guarantees. In the setting of relative\nerror, an immediate open problem is whether the guarantees on Algorithms 2 and 3 can be extended to more\ngeneral graphs. In order to improve understanding of the planning problem, a complete characterization of\neasily-tourable graphs would better contextualize Lemma 10. Finally, while the numerical results suggest\nthe algorithms proposed in this paper perform well under random errors, formal guarantees studying this\n11\nsetting would be a valuable addition.\nReferences\nA. Aamand, J. Y. Chen, and P. Indyk. (Optimal) Online Bipartite Matching with Degree Information. In\nAdvances in Neural Information Processing Systems, volume 35, 2022.\nS. Alpern and S. Gal. The theory of search games and rendezvous, volume 55. Springer Science & Business\nMedia, 2006.\nK. Anand, R. Ge, and D. Panigrahi. Customizing ml predictions for online algorithms. In International\nConference on Machine Learning, pages 303–313, 2020.\nS. Angelopoulos, C. Dürr, S. Jin, S. Kamali, and M. Renault. Online computation with untrusted advice.\nIn 11th Innovations in Theoretical Computer Science Conference (ITCS 2020). Schloss Dagstuhl-Leibniz-\nZentrum für Informatik, 2020.\nA. Antoniadis, C. Coester, M. Eliáš, A. Polak, and B. Simon. Online metric algorithms with untrusted\npredictions. ACM Transactions on Algorithms, 19(2):1–34, 2023.\nE. Bamas, A. Maggiori, and O. Svensson.\nThe primal-dual method for learning augmented algorithms.\nAdvances in Neural Information Processing Systems, 33:20083–20094, 2020.\nS. Banerjee, V. Cohen-Addad, A. Gupta, and Z. Li. Graph searching with predictions. In 14th Innovations\nin Theoretical Computer Science Conference, ITCS, volume 251 of LIPIcs, pages 12:1–12:24, 2023.\nP. Berman. On-line searching and navigation. Online Algorithms: The State of the Art, pages 232–241,\n2005.\nA. Bhattacharya, B. Gorain, and P. S. Mandal. Treasure hunt in graph using pebbles. In International\nSymposium on Stabilizing, Safety, and Security of Distributed Systems, pages 99–113. Springer, 2022.\nS. Bouchard, Y. Dieudonne, A. Labourel, and A. Pelc.\nAlmost-optimal deterministic treasure hunt in\narbitrary graphs. In International Colloquium on Automata, Languages and Programming (ICALP) 2021,\n2021.\nJ. Y. Chen, T. Eden, P. Indyk, H. Lin, S. Narayanan, R. Rubinfeld, S. Silwal, T. Wagner, D. P. Woodruff,\nand M. Zhang. Triangle and four cycle counting with predictions in graph streams. In 10th International\nConference on Learning Representations, ICLR, 2022.\nE. Cohen, O. Geri, and R. Pagh. Composable sketches for functions of frequencies: Beyond the worst case.\nIn Proceedings of the 37th International Conference on Machine Learning, 2020.\nR. Dechter and J. Pearl. Generalized best-first search strategies and the optimality of A*. Journal of the\nACM (JACM), 32(3):505–536, 1985.\nI. Diakonikolas, V. Kontonis, C. Tzamos, A. Vakilian, and N. Zarifis.\nLearning online algorithms with\ndistributional advice. In International Conference on Machine Learning, pages 2687–2696, 2021.\nS. Dobrev, R. Královič, and E. Markou. Online graph exploration with advice. In International Colloquium\non Structural Information and Communication Complexity, pages 267–278. Springer, 2012.\nE. Du, F. Wang, and M. Mitzenmacher. Putting the “learning\" into learning-augmented algorithms for\nfrequency estimation. In Proceedings of the 38th International Conference on Machine Learning, pages\n2860–2869, 2021.\nF. Eberle, A. Lindermayr, N. Megow, L. Nölke, and J. Schlöter. Robustification of online graph exploration\nmethods. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 9732–9740,\n2022.\n12\nT. Eden, P. Indyk, S. Narayanan, R. Rubinfeld, S. Silwal, and T. Wagner. Learning-based support estimation\nin sublinear time. In 9th International Conference on Learning Representations, ICLR, 2021.\nT. Eden, P. Indyk, and H. Xu. Embeddings and labeling schemes for a. In 13th Innovations in Theoretical\nComputer Science Conference (ITCS 2022). Schloss Dagstuhl-Leibniz-Zentrum für Informatik, 2022.\nP. Erdős, A. Rényi, et al. On the evolution of random graphs. Publ. math. inst. hung. acad. sci, 5(1):17–60,\n1960.\nD. Ferguson, M. Likhachev, and A. Stentz. A guide to heuristic-based path planning. In Proceedings of the\ninternational workshop on planning under uncertainty for autonomous systems, international conference\non automated planning and scheduling (ICAPS), pages 9–18, 2005.\nP. Ferragina and G. Vinciguerra. Learned data structures. In Recent Trends in Learning From Data: Tutorials\nfrom the INNS Big Data and Deep Learning Conference (INNSBDDL2019), pages 5–41. Springer, 2020.\nD. Foead, A. Ghifari, M. B. Kusuma, N. Hanafiah, and E. Gunawan. A systematic literature review of A*\npathfinding. Procedia Computer Science, 179:507–514, 2021.\nB. Gorain, K. Mondal, H. Nayak, and S. Pandit. Pebble guided optimal treasure hunt in anonymous graphs.\nTheoretical Computer Science, 922:61–80, 2022.\nA. Gupta, R. Krauthgamer, and J. R. Lee. Bounded geometries, fractals, and low-distortion embeddings. In\n44th Annual IEEE Symposium on Foundations of Computer Science, 2003. Proceedings., pages 534–543.\nIEEE, 2003.\nA. Gupta, D. Panigrahi, B. Subercaseaux, and K. Sun.\nAugmenting online algorithms with ε-accurate\npredictions. Advances in Neural Information Processing Systems, 35:2115–2127, 2022.\nC. Hsu, P. Indyk, D. Katabi, and A. Vakilian. Learning-based frequency estimation algorithms. In 7th\nInternational Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019, 2019.\nP. Indyk, A. Vakilian, and Y. Yuan.\nLearning-based low-rank approximations.\nIn Advances in Neural\nInformation Processing Systems, pages 7400–7410, 2019.\nT. Jiang, Y. Li, H. Lin, Y. Ruan, and D. P. Woodruff. Learning-augmented data stream algorithms. In\nInternational Conference on Learning Representations, 2020.\nB. Kalyanasundaram and K. R. Pruhs. Constructing competitive tours from local information. Theoretical\nComputer Science, 130(1):125–138, 1994.\nA. R. Karlin, N. Klein, and S. O. Gharan. A (slightly) improved approximation algorithm for metric tsp. In\nProceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, pages 32–45, 2021.\nD. Komm, R. Královič, R. Královič, and J. Smula. Treasure hunt with advice. In Structural Information\nand Communication Complexity: 22nd International Colloquium, SIROCCO 2015, Montserrat, Spain,\nJuly 14-16, 2015. Post-Proceedings 22, pages 328–341. Springer, 2015.\nT. Kraska, A. Beutel, E. H. Chi, J. Dean, and N. Polyzotis. The case for learned index structures. In\nProceedings of the 2018 International Conference on Management of Data, pages 489–504, 2018.\nY. Li, H. Lin, S. Liu, A. Vakilian, and D. Woodruff. Learning the positions in countsketch. In The Eleventh\nInternational Conference on Learning Representations, 2023.\nH. Lin, T. Luo, and D. Woodruff. Learning augmented binary search trees. In International Conference on\nMachine Learning, pages 13431–13440. PMLR, 2022.\nT. Lykouris and S. Vassilvitskii. Competitive caching with machine learned advice. In International Con-\nference on Machine Learning, pages 3302–3311, 2018.\n13\nN. Megow, K. Mehlhorn, and P. Schweitzer. Online graph exploration: New results on old and new algo-\nrithms. Theoretical Computer Science, 463:62–72, 2012.\nM. Mitzenmacher. A model for learned bloom filters and optimizing by sandwiching. In Advances in Neural\nInformation Processing Systems, pages 464–473, 2018.\nM. Mitzenmacher and S. Vassilvitskii. Algorithms with predictions. Communications of the ACM, 65(7):\n33–35, 2022.\nP. Paliwal. A survey of a-star algorithm family for motion planning of autonomous vehicles. In 2023 IEEE\nInternational Students’ Conference on Electrical, Electronics and Computer Science (SCEECS), pages\n1–6. IEEE, 2023.\nA. Pelc. Searching games with errors—fifty years of coping with liars. Theoretical Computer Science, 270\n(1-2):71–109, 2002.\nI. Pohl. Bi-directional and heuristic search in path problems. Technical report, Stanford Linear Accelerator\nCenter, Calif., 1969.\nM. Purohit, Z. Svitkina, and R. Kumar. Improving online algorithms via ml predictions. In Advances in\nNeural Information Processing Systems, pages 9661–9670, 2018.\nL. H. O. Rios and L. Chaimowicz. A survey and classification of A* based best-first heuristic search algo-\nrithms. In Brazilian Symposium on Artificial Intelligence, pages 253–262. Springer, 2010.\nA. Wei and F. Zhang. Optimal robustness-consistency trade-offs for learning-augmented online algorithms.\nAdvances in Neural Information Processing Systems, 33:8042–8053, 2020.\nA. C.-C. Yao. Probabilistic computations: Toward a unified measure of complexity. In 18th Annual Sympo-\nsium on Foundations of Computer Science (sfcs 1977), pages 222–227. IEEE Computer Society, 1977.\nA\nSurvey of Related Works\nGraph search with distance-to-goal predictions\nThe problem and prediction settings considered in\nthis work most closely correspond to those considered by Banerjee et al. (2023). Banerjee et al. (2023) study\nexploration and planning under absolute error models. They consider two parametrizations of prediction\nerror: the first in terms of the number of nodes at which predictions are not equal to true distance-to-goal,\ndenoted E0, and the second in terms of the ℓ1 norm of the vector of errors, denoted E1 as in this work.\nThey develop an algorithm for exploration on unweighted trees, and prove guarantees on its performance\nin terms of E0. They also develop algorithms for planning on graphs: on unweighted graphs, they establish\nguarantees parameterized by E0, while in graphs with integer-valued distances their performance bounds are\nparameterized by E1.\nTreasure Hunt\nIn the treasure hunt problem, a mobile agent must traverse some unknown environment,\ncontinuous or discrete, to locate a stationary hidden goal (Alpern and Gal, 2006). When the search en-\nvironment is a graph, this problem shares many features with the exploration problem considered in this\npaper. Bouchard et al. (2021) study the graph treasure hunt problem when the searcher receives no addi-\ntional information. They establish lower bounds on the total cost incurred by any algorithm in terms of\nthe number of edges in the ball of radius OPT around the root node, and give algorithms with performance\nguarantees which asymptotically exceed these lower bounds by at most a factor of log(OPT). Graph treasure\nhunt problems have also been considered when the agent receives help with the task. Komm et al. (2015)\nstudy the case when the the searcher receives generic advice, which can take the form of any bit string.\nThey consider the advice complexity of the treasure hunt task; they prove that there is an algorithm which\nachieves competitive ratio r by receiving O(n/r) bits of advice along the search and moreover they establish\nthat any algorithm achieving a competitive ratio of r must receive Ω(n/r) bits of advice (see Theorems 4\nand 5 in Komm et al. (2015)). In the setting where graph vertices are anonymized, i.e. the searcher has no\n14\nFigure 4: Comparing Algorithm 1 versus A∗ search on a random tree with randomly generated errors. The\nsame set of predictions is provided to both algorithms. While the set of nodes visited by the two algorithms\nis comparable, the computational model in A∗ places no penalty on traversal distance so that algorithm has\na tendency to double-back on itself, leading to a more expensive tour. Nodes are colored by prediction value\nand labeled with the order in which they are first visited by the relevant algorithm. Tour cost is taken to be\nP d(vi, vi+1) for indices i ordered according to when a node is first visited: the tour cost for A∗ omits costs\nincurred by re-expanding a node within the execution of the algorithm.\nway to recognize whether a vertex has or has not previously been visited, recent work has studied the task\nof graph treasure hunt with access to advice from an omniscient oracle which marks vertices with binary\nlabels (Bhattacharya et al., 2022; Gorain et al., 2022).\nThe exploration model studied in this paper can be viewed as a graph treasure hunt problem with\nspecific kinds of advice (predictions of distance-to-goal). One main contrast with this work is that the advice\nconsidered can contain adversarial errors, and indeed the impact that different error models have on the\ngraph exploration task is a core topic investigated in this work.\nPath-Planning and A∗ Search\nDistance-to-goal predictions have been the subject of study in many\npath-planning and graph-traversal settings. Initialized with a root node and some known target node, path-\nplanning problems seek to learn a shortest path between the root and goal and common problem models\nassume access to a set of distance-to-goal predictions, referred to as “heuristics” within this literature Pohl\n(1969); Ferguson et al. (2005). A∗ search is a celebrated algorithm for the problem of path-finding and\ngraph-traversal, designed for cases when the entire graph G and all predictions f are accessible in memory,\nand has spawned many algorithmic variants (Rios and Chaimowicz, 2010; Foead et al., 2021; Paliwal, 2023).\nMuch of the theory of A∗ search focuses on cases when predictions have particular structural properties:\na prediction function f : V → R is called admissible if the prediction at every node v is never greater than\nthe actual distance to the goal from v. A prediction function f : V → R is consistent (or monotone) if for\nevery node v and every neighbor u of v, f(v) ≤ f(u)+d(v, u) and f(g) = 0. Consistency is studied so heavily\nin part because it implies admissibility. Admissible heuristics are well-motivated and occur in various other\nproblems. For example, Eden et al. (2022) consider access to an oracle that provides an underestimate for the\nprobabilities of any element in some discrete probability distribution. In addition to being well-motivated\nby applications, admissibility is a focus of literature because A∗ search with admissible heuristics enjoys\noptimality properties (Dechter and Pearl, 1985).\nWhile many of the problems and algorithms within path-planning may appear closely related to this\nwork at first-glance, we emphasize that the goals and cost models differentiate the graph searching problems\nconsidered in this work from those in path-planning. In particular, the design and algorithmic guarantees on\npath-planning algorithms like A∗ search implicitly assume that the full graph and predictions are available in\nmemory upon initialization of the algorithm. Performance guarantees and notions of optimality are proven\nin terms of computational procedures, rather than traversal distance. For example, when predictions are\n15\nadmissible A∗ is optimal in the sense that the set of nodes expanded (an operation analogous to visiting\na node) is minimal (Dechter and Pearl, 1985). However, the sequence in which these nodes are expanded\ncan incur high traversal distance, as illustrated in Figure 4. More generally, the goals of algorithms for\npath-planning differ substantially from that in the graph search problem: the path-finding problem seeks to\nlearn and return a shortest path even if the process required to find such a path is expensive in the sense\nof graph traversal, whereas the aim of a graph searching problem is finding the goal node in an inexpensive\nmanner and makes no demand that a shortest path from the root node to the goal be in the set of visited\nnodes upon termination of the algorithm. These differences in goals and algorithmic guarantees mean that\nin cases when the environment is unknown, i.e. when the graph and predictions are not available in memory\nbut must instead by accessed by traversal, path-planning algorithms may incur much higher traversal cost\nthan the graph search algorithms proposed in this paper.\nLearning-Based Algorithms.\nRecent years have seen a marked increase in the integration of machine\nlearning techniques to enhance traditional algorithmic challenges.\nSuch algorithms have been developed\nfor various topics including online algorithms Lykouris and Vassilvitskii (2018); Purohit et al. (2018); An-\ngelopoulos et al. (2020); Wei and Zhang (2020); Bamas et al. (2020); Aamand et al. (2022); Antoniadis et al.\n(2023); Anand et al. (2020); Diakonikolas et al. (2021); Gupta et al. (2022), data structures Kraska et al.\n(2018); Mitzenmacher (2018); Ferragina and Vinciguerra (2020); Lin et al. (2022), and streaming models Hsu\net al. (2019); Indyk et al. (2019); Jiang et al. (2020); Cohen et al. (2020); Du et al. (2021); Eden et al. (2021);\nChen et al. (2022); Li et al. (2023). For an extensive collection of learning-based algorithms, refer to the\nrepository at https://algorithms-with-predictions.github.io/.\nB\nMissing Proofs\nIn this section, we present detailed proofs of the results in the main body of the paper.\nB.1\nProofs For Section 3\nProof of Theorem 1 and Corollary 5. Algorithm 1 follows a shortest path in Gi from vi−1 to vi. A\nsimple consequence of this is that:\nALG =\nX\ni∈[T ]\ndGi(vi−1, vi).\nLet ∆i\ndef\n= dG(vi−1, g) − dG(vi, g), and let T be the number of iterations of the while loop executed, so that\nvT = g. We then have:\nX\ni∈[T ]\n∆i = dG(v0, g) − dG(vT , g) = dG(v0, g) − dG(g, g) = OPT .\nConsider any iteration i ∈ [T]. Let wi be the first vertex outside of Vi−1 encountered when traversing a\nshortest path from vi−1 to g. Note that wi ∈ ∂Vi−1, and hence, by the update rule in Algorithm 1 we have:\nf(vi) + dGi(vi−1, vi) ≤ f(wi) + dGi(vi−1, wi).\n(6)\nFurthermore, by the definition of wi, we have:\ndGi(vi−1, wi) = dG(vi−1, wi).\n(7)\nThe above follows from a simple contradiction argument, for the existence of a shorter path from vi−1 to wi\nin G which is not in Gi, would contradict the definition of wi. We also have:\ndG(vi−i, g) = dG(vi−1, wi) + dG(wi, g).\n(8)\nWe then have, for any i ∈ [T]:\ndG(vi, g) − f(vi)\n(6)\n≥ dG(vi, g) + dGi(vi−1, vi) − dGi(vi−1, wi) − f(wi)\n16\n(7)\n= dG(vi, g) + dGi(vi−1, vi) − dG(vi−1, wi) − f(wi)\n= dG(vi−1, g) − ∆i + dGi(vi−1, vi) − dG(vi−1, wi) − f(wi)\n(8)\n= dGi(vi−1, vi) − ∆i + dG(wi, g) − f(wi).\nNote that the vertices in the sequence {vi}i∈[T ]∪{0} are always distinct, while the vertices in the sequence\n{wi}i∈[T ] might not be. This also implies that T ≤ n. The above then implies:\nALG =\nX\ni∈[T ]\ndGi(vi−1, vi)\n≤\nX\ni∈[T ]\n∆i + dG(vi, g) − f(vi) + f(wi) − dG(wi, g)\n=\nX\ni∈[T ]\n∆i +\nX\ni∈[T ]\ndG(vi, g) − f(vi) +\nX\ni∈[T ]\nf(wi) − dG(wi, g)\n≤ OPT + E−\n1 + T · E+\n∞\n≤ OPT + E−\n1 + n · E+\n∞.\nThis completes the proof of Theorem 1. When predictions are admissible, E−\n1 = E1 and E+\n∞ = 0 so Corollary 5\nfollows.\nProof of Theorem 6. We begin by proving the first half of the theorem. Given E−\n1 > 0 one considers the\nthree-vertex graph path P3 where the two edges are weighted with weight w = E−/2, the start vertex / root\nis chosen to be the middle vertex and the goal is one of the other two vertices (see left side of Figure 5).\nNote that the value of OPT is dG(r, g) = w. When the predictions on the vertices are given by: f(v1) = 0 ,\nf(v2) = w and f(v3) = 0, the error is equal to E− and the graph looks completely symmetric to the searcher,\nand hence in the worst case to find the goal, the searcher has to incur a cost of 3w = w + 2w = OPT + E−\nas needed.\nFor the second part of the theorem, we construct a star on n vertices, where each edge has weight\nw = E+\n∞/2, the starting vertex is at the center of the star, and the goal is chosen arbitrarily among the other\nvertices. The prediction at the goal is then picked to equal E+\n∞ so that it equals the prediction in all other\nvertices, i.e. we set the predictions to f(r) = w and f(v) = 2w for all w ̸= r. Every algorithm will then\nhave to visit the entire star in the worst case, incurring a cost of E+\n∞/2 + E+\n∞(n − 2) = OPT + E+\n∞(n − 2) as\nneeded (See the right side of Figure 5).\nProof of Proposition 7. We apply Yao’s minimax principle (Yao, 1977) to the same constructions used\nin the proof of Theorem 6. For both constructions, we consider the distribution over instances produced by\nchoosing the goal node uniformly at random among the leaf nodes.\nIn particular, for the first statement, we consider the performance of any deterministic algorithm on the\ndistribution of instances given by taking the three-node path graph on the left-hand side of Figure 5, and\nplacing the goal g at either v1 or v3 with equal probability. We fix predictions f(v1) = f(v3) = 0 and\nf(v2) = w as in the proof of Theorem 6. The expected cost incurred by any deterministic algorithm over\nthis distribution of instances is 2w = OPT + w = OPT + 1/2E−. Yao’s minimax principle then implies the\nstated lower bound for all randomized algorithms.\nThe proof of the second result follows analogously by considering the second construction in the proof of\nTheorem 6.\nB.2\nProofs For Section 4\nB.2.1\nAlgorithmic Guarantees Under Relative Error\nWe begin the analysis of Algorithm 2 by establishing the following properties of the set Sε,r defined in\nEquation (5). For G a tree, let PG(u, v) denote the (unique) shortest path between nodes u and v in G.\nLemma 14. For Sε,r as defined in (5) and G a weighted tree, then the following hold:\n17\nw\nw\nv1\nv2\nv3\nr\ng\nw\nw\nv1\nv2\nv3\nr\ng\nw\nw\nv4\nw\nv5\nw\nv(n−1)\nvn\n· · ·\nFigure 5: The construction in the proof of Theorem 6.\n(i) ∀v ̸∈ Sε,r, dG(v, r) > OPT,\n(ii) ∀v ∈ Sε,r, dG(v, r) ≤ 1+ε\n1−ε · OPT,\n(iii) ∀v ∈ Sε,r, dG(v, g) ≤\n2\n1−ε · OPT,\n(iv) PG(r, g) ⊆ Sε,r,\n(v) For any v ∈ Sε,r, PG(v, g) ⊆ Sε,r.\nProof. The properties follow immediately from the definition of the relative error model in Equation (1) and\nthe definition of Sε,r in Equation (5).\n(i) ∀v ̸∈ Sε,r, f(v) >\n1\n1−εf(r) and by Equation (1), f(r) ≥ (1 − ε)dG(r, g) = (1 − ε)OPT.\n(ii) ∀v ∈ Sε,r, dG(v, r) ≤\n1\n1−εf(r), and by Equation (1), f(r) ≤ (1 + ε)OPT.\n(iii) By triangle inequality, for any v ∈ G\ndG(v, g) ≤ dG(v, r) + dG(r, g) = dG(v, r) + OPT\nand so by property (ii) above, for any v ∈ Sε,r the result follows.\n(iv) ∀v ∈ PG(r, g), dG(r, v) ≤ dG(r, g) = OPT. Thus property (i) above implies ∀v ∈ PG(r, g), v ∈ Sε,r.\n(v) Consider v ∈ Sε,r, and let u\ndef\n= argmin{dG(u, g) | u ∈ PG(v, r)}. Because G is a tree,\nPG(v, g) = PG(v, u) ∪ PG(u, g).\nIn particular, PG(v, u) ⊆ PG(v, r); observe that because v ∈ Sε,r, ∀w ∈ PG(v, r), dG(w, r) ≤ dG(v, r) ≤\n1\n1−εf(r) and thus by the definition of Sε,r, PG(v, r) ⊆ Sε,r. We’ve thus concluded that PG(v, u) ⊆ Sε,r.\nFor the second portion of the path, PG(u, g), observe that by the definition of u = argmin{dG(u, g) |\nu ∈ PG(v, r)}, it must be that u ∈ PG(r, g).\nThus PG(u, g) ⊆ PG(r, g) and so by property (iv),\nPG(u, g) ⊆ Sε,r.\nWith these properties, we now bound the distance travelled on the ith step of the algorithm:\nLemma 15. Let G be a weighted tree, with predictions satisfying Equation (1) with respect to parameter\nε < 1. Then, the distance travelled by Algorithm 2 on the ith iteration satisfies\n(1 − ε)dGi(vi−1, vi) ≤ ∆i + 2εdG(vi, g).\n(9)\nAdditionally, if the predictions on G are multiplicative and decremental,\ndGi(vi−1, vi) ≤ ∆i + εdG(vi, g).\n(10)\n18\nProof of Lemma 15. We observe that, by definition of Algorithm 2, for all iterations i, Vi−1 ⊂ Sε,r.\nConsider wi the first vertex outside of Vi−1 encountered when traversing P(vi−1, g), the shortest path from\nvi−1 to g. Note that wi ∈ ∂Vi−1, and that by property (v) of Lemma 14, wi ∈ Sϵ,r.\nThus, on iteration i, ∃wi ∈ ∂Vi−1 ∩ Sε,r ∩ P(vi−1, g), so by the definition of Algorithm 2 such wi satisfies\nf(vi) + dGi(vi−1, vi) ≤ f(wi) + dGi(vi−1, wi).\nIn particular, because wi ∈ P(vi−1, g) ∩ ∂Vi−1 and by the tree properties of G, we have\ndGi(vi−1, wi) = dG(vi−1, wi) = dG(vi−1, g) − dG(wi, g).\nWe can thus upper bound\ndGi(vi−1, vi) ≤ dG(vi−1, g) − dG(wi, g) + f(wi) − f(vi)\n= dG(vi−1, g) − dG(vi, g) +\n\u0000dG(vi, g) − f(vi)\n\u0001\n+\n\u0000f(wi) − dG(wi, g)\n\u0001\n= ∆i − εvidG(vi, g) + εwidG(wi, g)\nwhere εvi, εwi ∈ [−ε, ε] are the constants whose existence is implied by Equation (1) such that\nf(v) = (1 + εv)dG(v, g).\n(11)\nConsider two cases: first, consider the case when εwi > 0. Because wi ∈ P(vi−1, g),\ndG(wi, g) ≤ dG(vi−1, g) ≤ dG(vi−1, vi) + dG(vi, g).\nCombining this bound and the fact that εvi, εwi ∈ [−ε, ε] yields:\n−εvidG(vi, g) + εwidG(wi, g) ≤ 2εdG(vi, g) + εdG(vi−1, vi).\nThus in this case, the desired bound in (9) is satisfied.\nIn the second case, when εwi ≤ 0,\n−εvidG(vi, g) + εwidG(wi, g) ≤ −εvidG(vi, g) ≤ εdG(vi, g)\nwhich is trivially upper bounded by 2εdG(vi, g) + εdG(vi−1, vi). Thus, in both cases, the desired bound in\n(9) is satisfied.\nIn the case of decremental errors, εv ≤ 0 ∀v ∈ V . Thus the latter case always applies, so we can bound\ndGi(vi−1, vi) ≤ ∆i − εvidG(vi, g) + εwidG(wi, g) ≤ ∆i + εdG(vi, g)\nthus establishing the bound in (10).\nWe are now equipped to prove Theorem 2.\nProof of Theorem 2. We will show that Algorithm 2 achieves the desired competitive ratio. We begin\nby establishing that the algorithm will terminate at the goal node g: by Property (iv) in Lemma 14,\nP(r, g) ⊆ Sε,r, and further by definition P(r, g) is connected, so Algorithm 2 initialized at r will explore a\nconnected subgraph of G that contains g, and will thus terminate at g.\nWe now bound the total distance travelled by Algorithm 2. Consider the general case, when errors can\nbe incremental or decremental. Then, by Lemma 15,\nALG =\nX\ni∈[T ]\ndGi(vi−1, vi) =\nX\ni∈[T ]\n(1 − ε)dGi(vi−1, vi) + εdGi(vi−1, vi)\n≤\nX\ni∈[T ]\n∆i +\nX\ni∈[T ]\n2εdG(vi, g) +\nX\ni∈[T ]\nεdG(vi−1, vi) = OPT + 2ε\n\n X\ni∈[T ]\ndG(vi, g)\n\n + εALG.\n19\nBecause Vi ⊆ Sε,r for all iterations i, and by property (iii) in Lemma 14,\n2ε\nX\ni∈[T ]\ndG(vi, g) ≤ 2ε|Sε,r| ·\n2\n1 − εOPT.\nThus, re-arranging,\n(1 − ε)ALG ≤ OPT\n\u0012\n1 + |Sε,r|ε ·\n4\n1 − ε\n\u0013\n,\nyielding the claimed competitive ratio from the trivial upper bound |Sε,r| ≤ n.\nIn the case of decremental errors, Lemma 15 and a similar argument give\nALG =\nX\ni∈[T ]\ndGi(vi−1, vi) ≤\nX\ni∈[T ]\n∆i +\nX\ni∈[T ]\nεdG(vi, g) ≤ OPT\n\u0012\n1 + |Sε,r|ε ·\n2\n1 − ε\n\u0013\n,\nyielding the claimed competitive ratio.\nTo prove Theorem 3, we’ll use the following lemmas: the first (Lemma 16) establishes that certain\nalgorithms never explore nodes too far from g. We emphasize that the below lemma makes use of distances\nin the full G, not Gi. In the case of weighted trees, these two distances are always identical: on a tree, for\nall iterations i and for any u, v ∈ Vi ∪ ∂Vi, dGi(u, v) = dG(u, v). The second lemma (Lemma 17) bounds the\ndistance travelled by these algorithms on any given iteration.\nLemma 16. Consider the exploration problem on G a weighted, undirected graph with predictions f satisfying\nEquation (1). Consider the update rule used in Algorithm 3:\nvi = argmin\nv∈∂Vi−1\n{βdG(vi−1, v) + f(v)} ,\n(12)\nand assume β > 0 satisfies β < 1 − ε. Then, for every iteration i ∈ [T], the node vi visited by the algorithm\non the ith iteration satisfies\ndG(vi, g) ≤ 1 + ε + β\n1 − (ε + β)OPT.\nProof of Lemma 16. Let ri be the first vertex outside of Vi−1 encountered when traversing PG(r, g) a\nshortest path from the root r to g. Note that ri ∈ ∂Vi−1, and thus by Equation (12),\nβdG(vi−1, vi) + f(vi) ≤ βdG(vi−1, ri) + f(ri).\nIn particular, by the triangle inequality we can bound\nf(vi) ≤ β (dG(vi−1, ri) − dG(vi−1, vi)) + f(ri)\n≤ βdG(ri, vi) + f(ri)\n≤ β (dG(ri, g) + dG(vi, g)) + f(ri).\nGiven ri ∈ PG(r, g), dG(ri, g) ≤ dG(r, g) = OPT. Moreover, recalling that the predictions f must satisfy\nEquation (1), let εvi and εri be the relative errors at vertex vi and ri respectively (defined as in Equation (11)).\nThen we can rewrite the bound above as\n(1 + εvi)dG(vi, g) ≤ β (dG(ri, g) + dG(vi, g)) + (1 + εri)dG(ri, g),\nand hence:\n(1 + εvi − β)dG(vi, g) ≤ (β + 1 + εri)OPT.\nUsing the fact that εvi, εri ∈ [−ε, ε] and the assumption that β satisfies β < 1 − ε, we can use the above\nbound to conclude\ndG(vi, g) ≤ 1 + ε + β\n1 − (ε + β)OPT.\nIn particular, this holds on any iteration independently of i, so we obtain the desired result.\n20\nLemma 17. Consider the exploration problem on G a weighted, undirected graph with predictions f satisfying\nEquation (1). Assume β > 0 satisfies\n1 + ε\n2\n< β < 1 − ε.\n(13)\nThen the distance traversed by Algorithm 3 on the ith iteration is bounded by\ndGi(vi−1, vi) ≤\nβ\n2β − 1 − ε∆i +\n2ε\n2β − 1 − εdG(vi, g).\nProof of Lemma 17. Let PG(u, v) denote an (arbitrary) shortest path between nodes u and v in G. Algo-\nrithm 3 follows a shortest path in Gi from vi−1 to vi. Let wi be the first vertex outside of Vi−1 encountered\nwhen traversing PG(vi−1, g). Note that as a consequence, wi ∈ ∂Vi−1, and hence by (12) we have\nβdGi(vi−1, vi) + f(vi) ≤ βdGi(vi−1, wi) + f(wi).\nSince wi ∈ PG(vi−1, g),\ndGi(vi−1, wi) = dG(vi−1, wi) = dG(vi−1, g) − dG(wi, g).\nRe-arranging and using the above fact, we obtain\nβdGi(vi−1, vi) ≤ βdGi(vi−1, wi) + f(wi) − f(vi)\n= β\n\u0000dG(vi−1, g) − dG(wi, g)\n\u0001\n+ f(wi) − f(vi)\n= β\n\u0000dG(vi−1, g) − dG(vi, g)\n\u0001\n+\n\u0000βdG(vi, g) − f(vi)\n\u0001\n+\n\u0000f(wi) − βdG(wi, g)\n\u0001\n.\nLet εvi, εwi be the relative prediction errors at vi and wi respectively (defined as in Equation (11)), and\nrecall the definition of ∆i in (3). Then we can rewrite the above as\nβdGi(vi−1, vi) ≤ β∆i + (β − 1 − εvi)dG(vi, g) + (1 + εwi − β)dG(wi, g).\nBecause wi ∈ PG(vi−1, g), we have that\ndG(vi−1, wi) + dG(wi, g) = dG(vi−1, g) ≤ dG(vi−1, vi) + dG(vi, g).\nMoreover, by upper bound on β in (13), (1 + εwi − β) ≥ 0, so we can revise our upper bound:\nβdGi(vi−1, vi) ≤ β∆i + (β − 1 − εvi)dG(vi, g) + (1 + εwi − β)\n\u0000dG(vi−1, vi) + dG(vi, g)\n\u0001\n.\nRe-arranging and recalling εvi−1, εwi ∈ [−ε, ε] yields\n(2β − 1 − ε)dGi(vi−1, vi) ≤ β∆i + 2εdG(vi, g)\nLeveraging the lower bound on β in (13), we can divide to obtain the desired result.\nTheorem 3 follows immediately from Lemmas 16 and 17:\nProof of Theorem 3. Observe that for ε ∈ (0, 1/3), β = 2/3 always satisfies Equation (13), independently\nof the value of ε. Thus for this setting, Lemma 17 implies that the update cost on a single iteration of\nAlgorithm 3 is bounded as\ndGi(vi−1, vi) ≤\n2\n1 − 3ε∆i +\n6ε\n1 − 3εdG(vi, g).\n(14)\nIn particular, for G a weighted tree, on all iterations i, for all u, v ∈ Vi ∪ ∂Vi,\ndGi(u, v) = dG(u, v).\nThis, in combination with the choice of β = 2/3 implies that Lemma 16 applies, so for all vertices vi visited\nby the algorithm, we have\ndG(vi, g) ≤ 5 + 3ε\n1 − 3εOPT.\n(15)\n21\nWe can then upper bound the last term in Equation (14) and obtain:\ndGi(vi−1, vi) ≤\n2\n1 − 3ε∆i +\n6ε\n1 − 3ε · 5 + 3ε\n1 − 3εOPT\nThus, letting T denote the total number of iterations made by Algorithm 3, summing over all iterations\nyields\nALG =\nX\ni∈[T ]\ndGi(vi−1, vi) ≤\n2\n1 − 3ε\nX\ni∈[T ]\n∆i +\n6ε\n1 − 3ε · 5 + 3ε\n1 − 3εOPT · T.\nIn particular, P\ni∈[T ] ∆i = OPT, and as every iteration i must end at some distinct vi satisfying (15),\nT ≤\n\f\f\f\fB\n\u0012\ng, 5 + 3ε\n1 − 3εOPT\n\u0013\f\f\f\f ≤ n.\nWe then have:\nALG ≤\n2\n1 − 3εOPT +\n6ε\n1 − 3ε · 5 + 3ε\n1 − 3εOPT · T\n≤ OPT\n\u0012\n2 +\n6ε\n1 − 3ε +\n6ε\n1 − 3ε · 5 + 3ε\n1 − 3ε · n\n\u0013\n.\nGiving the result in the statement of the theorem.\nB.2.2\nLower bounds For Exploration With Relative Error\nProof of Theorem 8. We consider a star in which every edge has weight w1, to this, we add a new vertex\ng connected to one of the outside vertices by an edge of weight w2. We consider the case when the initial\nposition r is the central node of the star. In this case, the optimal algorithmic cost is\nOPT = dG(r, g) = w1 + w2.\nFor the given ε ∈ (0, 1), consider choice of w1 and w2 such that\nε =\nw1\nw1 + w2\n.\nNote that in particular, such a setting of weights allows for the following predictions: every node except for\nthe root and the goal can have prediction\nf(v) = (1 − ε)(2w1 + w2).\nFor the above setting of w1 and w2, this satisfies Equation 1 with respect to ε. In particular, for a searcher\nstarting at the root, all neighbors of the root appear identical.\nIn this error regime, every algorithm for the exploration problem has to explore all branches of the star\nbefore finding g in the worst-case. Thus any algorithm has to incur cost at least\nALG = 2w1 · (n − 3) + (w1 + w2) = w1(2(n − 3) + 1) + w2.\nThe competitive ratio in this instance is thus\nALG\nOPT = (2(n − 3) + 1)\nw1\nw1 + w2\n+\nw2\nw1 + w2\n= (2(n − 3) + 1) ε + (1 − ε) = Θ(1 + nε).\nProof of Proposition 9. Consider a distribution over instances obtained by taking the instance defined in\nthe proof of Theorem 8, selecting a neighbor v of the root vertex r uniformly at random, and replacing the\nedge incident to the goal vertex g with an edge gv of weight w2.\nAny deterministic algorithm for the exploration problem running on an instance sampled from this\ndistributions incurs expected cost at least:\nALG = (n − 3) · 1\n22w2 + OPT = ((n − 3)ε + 1) OPT ≥\n\u0010\n1 + nε\n2\n\u0011\nOPT.\nThe lower bound for randomized algorithms then follows from applying Yao’s minimax principle (Yao,\n1977).\n22\nB.3\nProofs For Section 5\nThroughout this subsection, we will use the following properties of ϕ0 and ϕ1 established by Banerjee et al.\n(2023):\nLemma 18. Corollary 5.4 and Lemma 5.10 in Banerjee et al. (2023). Given G an unweighted graph, for\nany u, v ∈ G,\nϕ0(u) + ϕ0(v) ≥ dG(u, v).\nFor G weighted,\nϕ1(u) + ϕ1(v) ≥ 2dG(u, v).\nB.3.1\nPlanning Bounds Via Metric Embeddings\nThe distortion of an embedding can be related to its Lipschitz constant and that of its inverse: the Lipschitz\nconstant of τ is defined as:\n∥τ∥Lip\ndef\n=\nmax\nx1,x2∈X\ndY (τ(x1), τ(x2))\ndX(x1, x2)\n.\nNote that any map with non-trivial distortion must be injective, and thus considering τ −1 : Y → X,\ndist(τ) = ∥τ∥Lip · ∥τ −1∥Lip.\nTo prove Lemma 10, we’ll use the following fact to relate tours in G to tours in some embedding.\nLemma 19. Consider an embedding τ : G → G′ for G = (V, E) and G′ = (V ′, E′). Then for any S ⊆ V ,\ntourG(S) ≤ ∥τ −1∥Lip · tourG′(τ(S)).\nProof of Lemma 19. Recall the definition of tourG(S) given in Equation (2):\ntourG(S)\ndef\n= max\nv∈S\nmin\nW ∈W(v,S) lengthG(W),\nwhere W(v, S) is the set of walks in G starting at vertex v and visiting every vertex in S. Consider any\nwalk W = (u1, ..., uk) in G′ starting at some u1 ∈ τ(S) and visiting all of τ(S). Let W ′ = (u′\n1, ..., u′\nk′) be the\nsubsequence of W containing only the points in τ(S). Note that W ′ contains all of the points in τ(S). We\nhave:\nlengthG′(W) =\nk−1\nX\ni=1\ndG′(ui, ui+1) ≥\nk′−1\nX\ni=1\ndG′(u′\ni, u′\ni+1)\n(16)\n≥\nk′−1\nX\ni=1\n1\n∥τ −1∥Lip\n· dG(τ −1(u′\ni), τ −1(u′\ni+1)).\n(17)\nSo, letting W ′′ be the walk visiting the vertices (τ −1(u′\ni))k′\ni=1 in order while walking the shortest path in\nG between them. We then have:\nlengthG′(W) ≥\nk′−1\nX\ni=1\n1\n∥τ −1∥Lip\n· dG(τ −1(u′\ni), τ −1(u′\ni+1)) =\n1\n∥τ −1∥Lip\n· lengthG(W ′′).\nIn particular, for any starting point v ∈ S and any walk in W ∈ W(τ(v), τ(S)) there exists some walk\nW ′′ ∈ W(v, S) such that:\nlengthG(W ′′) ≤ ∥τ −1∥Lip · lengthG′(W),\nso that, for every v ∈ S:\nmin\nW ∈W(v,S) lengthG(W) ≤ ∥τ −1∥Lip\nmin\nW ∈W(τ(v),τ(S)) lengthG′(W) ≤ ∥τ −1∥Lip max\nu∈τ(S)\nmin\nW ∈W(u,τ(S)) lengthG′(W),\nand hence:\nmax\nv∈S\nmin\nW ∈W(v,S) lengthG(W) ≤ ∥τ −1∥Lip max\nu∈τ(S)\nmin\nW ∈W(u,τ(S)) lengthG′(W),\ncompleting the proof.\n23\nWe now use Lemma 19 to establish Lemma 10.\nProof of Lemma 10. Given a real-valued function f : V → R, we denote the sublevel set of f about\nthreshold c as\nL−\nf (c)\ndef\n= {v ∈ V : f(v) ≤ c}.\nFor the first part of the result, let G be an unweighted graph and consider the sublevel set L−\nϕ0(λ). By\ndefinition of the Lipschitz constant of τ : G → G′, for all u, v ∈ G\ndG′(τ(u), τ(v)) ≤ ∥τ∥LipdG(u, v).\nThus by Lemma 18, the embedding of the sublevel set has bounded diameter: let u, v ∈ L−\nϕ0(λ) such that\ndiam\n\u0000τ(L−\nϕ0(λ))\n\u0001\n= dG′(τ(u), τ(v)). Then\ndiam\n\u0000τ(L−\nϕ0(λ))\n\u0001\n= dG′(τ(u), τ(v)) ≤ ∥τ∥LipdG(u, v) ≤ ∥τ∥Lip(ϕ0(u) + ϕ0(v)) ≤ ∥τ∥Lip · 2λ.\nUsing this result along with the bound from Lemma 19 and the assumption that G′ is cG′ easily-tourable,\nwe can bound\ntourG(L−\nϕ0)(λ) ≤ ∥τ −1∥Lip · tourG′\u0000τ(L−\nϕ0(λ)\n\u0001\n≤ ∥τ −1∥Lip · cG′diam\n\u0000τ(L−\nϕ0(λ))\n\u0001\n≤ ∥τ −1∥Lip · cG′ · 2λ∥τ∥Lip\n= 2ρcG′λ.\nWe now use this bound to analyze the cost of Algorithm 4. Algorithm 4 sequentially visits sublevel sets\nof ϕ0. On an iteration k corresponding to threshold λk, the algorithm visits each node in L−\nϕ0(λk) ⊆ V by\ncomputing a constant-factor approximation to the following problem: for {v1, . . . , v|L−\nϕ0(λk)|} the nodes of\nL−\nϕ0(λk) and Π(n) the set of permutations on integers 1, . . . , n, the algorithm computes\nσ∗\nλk\ndef\n=\nargmin\nσ∈Π(|L−\nϕ0(λk)|)\n|L−\nϕ0(λk)|\nX\ni=1\ndG(vσ(i), vσ(i+1)).\nThe total distance travelled on the iteration k is thus\n|L−\nϕ0(λk)|\nX\ni=1\ndG(vσ∗(i), vσ∗(i+1)) ≤\nmax\nv∈L−\nϕ0(λk)\nmin\nW ∈W(v,L−\nϕ0(λk))\nlengthG(W) = tourG(L−\nϕ0(λk)).\nAlgorithm 4 begins with λ0 = 1 and doubles the threshold on each iteration, such that λk = 2k. In particular,\nϕ0(g) = E0, so the algorithm is guaranteed to terminate by the time it has visited every node of L−\nϕ0(λk) for\nthe first sufficiently large threshold λk ≥ E0. The algorithmic cost can thus be bounded as\nALG ≤ dG(r, L−\nϕ0(1)) +\n⌈log2(E0)⌉\nX\nk=0\ntourG(L−\nϕ0(2k))\n≤ dG(r, L−\nϕ0(1)) + 2ρcG′\n⌈log2(E0)⌉\nX\nk=0\n2k\n≤ dG(r, L−\nϕ0(1)) + 2ρcG′(4E0 − 1).\nUsing Lemma 18, we can bound the transition cost from r to the first sublevel set as\ndG(r, L−\nϕ0(1)) ≤ dG(r, g) + dG(g, L−\nϕ0(1)) ≤ OPT + (ϕ0(g) + 1) = OPT + E0 + 1.\n24\nCombining these yields\nALG ≤ OPT + E0(8ρcG′ + 1) − 2ρcG′ + 1 = OPT + O(ρcG′E0),\nas desired.\nFor the second part of the result, let G be an graph with integer-valued distances consider the sublevel\nset L−\nϕ1(λ), and note that Lemma 19 holds for both weighted and unweighted graphs.\nThe proof then\nfollows analogously to the above argument, using the appropriate bound relating ϕ1 to distances in G from\nLemma 18.\nB.3.2\nLower Bounds For Planning Problems\n1\n1\nr\n1\n1\n1\n1\n...\n...\n...\n...\n...\n...\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\nW\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\ng\nFigure 6: Reproduction of Figure 1. The lower bound construction for the proof of Lemma 11\nProof of Lemma 11. We construct a family of graphs with uniform predictions and analyze the worst-case\ncost incurred by any algorithm for the planning problem.\nFor a given ∆ and W ∈ N let G∆,W be following graph: consider a root node r with ∆ child nodes\nv1, . . . , v∆.\nLet every edge (r, vi) have edge weight 1.\nEach child node vi then has ∆ − 1 descendants\nui\n1, . . . , ui\n∆−1 with edge weights 1 for each edge (vi, ui\nj). Each of these descendants has a single child node\nwi\nj to which ui\nj is attached with edge weight W. This construction is illustrated in Figure 1.\nWe consider the planning problem when the searcher is initialized at the root node r described above,\nand the goal is the leaf node w1\n1. We consider the case when error in the predictions is such that each\nsubtree rooted at vi appears to have the same predictions. In this construction, predictions are equal to\ntrue distance-to-goal for all nodes which are descendents of v1 for i ̸= 1, and error is allocated only over\ndescendents of v1. We first calculate the total error in such predictions:\nE1 = |f(v1) − dG(v1, g)| + |f(g)| +\n∆−1\nX\nj=2\n|f(u1\nj) − dG(u1\nj, g)| + |f(w1\nj) − dG(w1\nj, g)| = 2W + 6∆ + 4.\nUnder these predictions, all nodes on a given level from the root appear identical to the searcher. As a\nresult, in the worst-case any algorithm for this problem must visit every node and incur cost ALG at least\n25\nas large as the shortest tour of the graph starting at r and ending at g. Hence:\nALG ≥ W(2∆2 − 2∆ − 1) + 2(∆2 − 1).\nOn the other hand, we have OPT = dG(r, g) = W + 2. This gives:\nALG − OPT ≥ 2W(∆2 − ∆ − 1) + 2(∆2 − 2).\nIn order to understand how this cost scales with our parameters of interest, we now establish bounds on the\ndoubling constant of G that show that λ = Θ(∆2). Recall that the doubling constant is defined to be the\nminimum value of λ such that for any radius R, any ball of radius R can be covered with at most λ many\nballs of radius R/2. Observe that the number of nodes n always upper bounds the doubling constant, so\nλ ≤ 2∆2 + ∆ + 1. To lower bound the doubling constant, consider the ball of radius W + 2 centered at the\nroot node r, and note that this ball contains the entire graph. For W large (W ≥ 4), ∆2 + 1 many balls of\nradius W/2 + 1 are required to cover the nodes of the graph, hence ∆2 − 1 ≤ λ.\nThe fact that a ≥ 1 then follows from observing that λ is independent of W while ALG−OPT = Ω(W∆2).\nThe fact that if a = 1, b ≥ 1 similarly follows from the above along with E1 = Θ(W + ∆).\nProof of Lemma 13. To establish that a ≥ 1 and that a + b ≥ 3, we consider the same construction\noutlined in the proof of Lemma 11 above. The same argument implies a ≥ 1. Setting W = ∆ yields a family\nof problem instances on integer-weighted trees for which ALG − OPT = Ω(∆3) where E1 = Θ(∆). Thus on\nthis family of instances any algorithm which is guaranteed to incur cost ALG − OPT = O(Ea\n1 ∆b) must have\na + b ≥ 3.\nTo establish that b ≥ 1, we consider a construction in which E1 scales independently of ∆. Consider the\nweighted star with edge weights w, and assume the searcher is initialized at the central root node r and the\ngoal node g is a leaf node as illustrated on the right side of Figure 5. We consider the same predictions\nconstructed in the proof of Theorem 6: all nodes except for g have f(v) = dG(v, g), and f(g) = 2w so that\npredictions at all leaf nodes appear uniform. Then in the worst case, the searcher must visit every node in\nthe graph, incurring traversal cost\nALG = w(2∆ − 1).\nHowever, the total ℓ1 norm of the vector of errors is E1 = 2w independent of ∆, and OPT = w, so the result\nfollows.\nLemma 20. Let A be any algorithm for the planning problem on weighted trees which is guaranteed to incur\ncost: OPT + O(Ea\n1 ρb) on graph searching instance I, where ρ is the minimum distortion of embedding the\ninstance graph into the path. Then a ≥ 1 and b ≥ 1.\nProof. The proof is entirely analogous to that of the second part of Lemma 13. For any value of E1, one can\nmake use of the same construction (the weighted star, with r the central node and g a leaf) with weights\nw = E1/2 on each edge.\nThe result then follows from observing that for the family of constructed graphs, ρ = ∆ and the analogous\ncalculations.\nB.4\nPlanning On Trees With Integer-Valued Distances\nIn this section, we prove Lemma 12.\nThe result follows from arguments analogous to those outlined in\nBanerjee et al. (2023) Section 5.1: we first state and prove three necessary lemmas.\nLemma 21 (Analogous to Lemma 5.3 in Banerjee et al. (2023)). Given G with positive integer distances\nd : V × V → Z≥0, for any U ⊆ V\n|S \\ M(U)| ≤\nX\nu∈U\nφ1(u),\nwhere\nM(U)\ndef\n= {v ∈ V : d(v, u) = d(v, u′) ∀u, u′ ∈ U}.\n26\nProof Lemma 21. Given d : V × V → Z≥0, for any U ⊆ V , ∀w ̸∈ M(U) let uw, vw denote elements of U\nsuch that dG(w, uw) ̸= dG(w, vw). In particular, because d(·, ·) is integer-valued, ∀w ̸∈ M(U)\n|dG(w, uw) − dG(w, vw)| ≥ 1.\nIn particular, for any S ⊆ V\nX\nu∈U\nφ1(u) =\nX\nu∈U\nX\nw∈V\n|f(w) − dG(w, u)|\n≥\nX\nu∈U\nX\nw∈S\\M(U)\n|f(w) − dG(w, u)|\n≥\nX\nw∈S\\M(U)\n|f(w) − dG(w, uw)| + |f(w) − dG(w, vw)|\n≥\nX\nw∈S\\M(U)\n|dG(w, uw) − dG(w, vw)|\n≥ |S \\ M(U)|.\nLemma 22 (Generalization of Lemma 5.10 in Banerjee et al. (2023)). For any u, v ∈ V , we have:\nφ1(u) + φ1(v) ≥ 2d(u, v) +\nX\nw∈V \\{u,v}\n|d(u, w) − d(v, w)| .\nProof of Lemma 22. The proof is a straight-forward application of the triangle inequality:\nφ1(u) + φ1(v) =\nX\nw∈V\n|d(u, w) − f(w)| + |d(v, w) − f(w)|\n≥\nX\nw∈V\n|d(u, w) − f(w) − d(v, w) + f(w)|\n=\nX\nw∈V\n|d(u, w) − d(v, w)|\n= 2d(u, v) +\nX\nw∈V \\{u,v}\n|d(u, w) − d(v, w)| .\nWe also utilize the following bound on the size of the minimum Steiner tree of any sublevel set of ϕ1:\nrecall that for a real-valued function f : V → R, we denote the sublevel set of f about threshold c as\nL−\nf (c)\ndef\n= {v ∈ V : f(v) ≤ c}.\nLemma 23. For G a connected tree with at least three nodes, integer edge weights, and maximum degree ∆,\nlet Cλ denote the set of vertices in the minimum Steiner tree containing all vertices in L−\nϕ1(λ). Then\n|Cλ| ≤ λ∆.\nProof of Lemma 23. By definition, L−\nϕ1(λ) ⊆ Cλ. Let u1, u2 ∈ L−\nϕ1(λ) such that\nd(u1, u2) = diam(L−\nϕ1(λ))\nIf ̸ ∃w ∈ Cλ such that d(u1, w) = d(u2, w), then Lemma 21 implies\n|Cλ| = |Cλ \\ M(u1, u2)| ≤ φ1(u1) + φ1(u2) ≤ 2λ.\n27\nTj\nqj\nw\nu1\nu2\nx\nuj\n...\nFigure 7: Visual aid for proof of Lemma 23, for the case when ∃w ∈ Cλ such that d(u1, w) = d(u2, w).\nIn particular, for G connected with at least three nodes, ∆ ≥ 2, so the desired bound holds.\nWe now consider the case when ∃w ∈ Cλ such that d(u1, w) = d(u2, w).\nLet q1, . . . , qk denote the\nneighbors of w and let Ti ⊆ Cλ denote the subtree of descendants of w that contains qi. Assume without\nloss of generality that T1 ∋ u1 and T2 ∋ u2. Note that, because Cλ is defined to be minimal, ∀i ∈ [k]\nL−\nϕ1(λ) ∩ Ti ̸= ∅. Let u3, ..., uk be points such that ui ∈ L−\nϕ1(λ) ∩ Ti. Consider any x ∈ Cλ \\ {w}. Then\n∃j ∈ [k] such that x ∈ Tj. This case is illustrated in Figure 7.\nAssume without loss of generality that j ̸= 1 (this can be assumed WLOG because if x ∈ T1, then the\nbelow argument can be carried out with respect to u2). Because G is a tree and x ̸∈ T1,\nd(u1, x) = d(u1, w) + d(w, x)\nBy choice of u1, u2, diam(L−\nϕ1(λ)) = 2d(u1, w) so in particular d(u1, w) ≥ d(uj, w) ∀uj ∈ L−\nϕ1(λ). Thus\nd(u1, x) = d(u1, w) + d(w, x) ≥ d(uj, w) + d(w, x).\nMoreover, for all v ∈ Tj, d(v, w) = d(v, qj) + d(qj, w) by definition of subtree Tj, so for non-zero weights,\nd(uj, w) + d(w, x) > d(uj, qj) + d(x, qj) ≥ d(uj, x)\nWe thus conclude d(u1, x) > d(uj, x), which in particular implies x ̸∈ M({u1, . . . , uk}).\nThus for all (Cλ \\ {w}) ∩ M({u1, . . . , uk}) = ∅, so\n|Cλ| − 1 = |Cλ \\ M({u1, ..., uk}) ≤\nk\nX\ni=1\nφ1(ui) ≤ λ∆.\nWe have thus established the result in both cases (i.e. when Cλ ∩ M({u1, . . . , uk}) = ∅ and when Cλ ∩\nM({u1, . . . , uk}) ̸= ∅).\nProof of Lemma 12. Consider the cost of visiting every node in Cλ, a minimum Steiner tree containing\nthe sublevel set L−\nϕ1(λ). Because Cλ is minimal,\ndiam(Cλ) = diam(L−\nϕ1(λ)) ≤ λ\nwhere the last inequality follows from Lemma 22. In particular, traversing Cλ to visit every node incurs travel\ncost at most diam(Cλ) · |Cλ|. Combining the above bound and Lemma 23 implies diam(Cλ) · |Cλ| ≤ λ2∆.\nAlgorithm 4 with objective ϕ1 proceeds by iteratively visiting every node in the sublevel set L−\nϕ1(λ) by\ncomputing and traversing a minimum Steiner tree Cλ that contains the sublevel set. Algorithm 4 begins\nwith λ0 = 1 and doubles the threshold on each iteration, such that λk = 2k. In particular, ϕ1(g) = E1, so the\nalgorithm is guaranteed to terminate by the time it has visited every node of L−\nϕ1(λk) for the first sufficiently\nlarge threshold λk ≥ E1. The algorithmic cost of Algorithm 4 with objective ϕ1 is thus bounded by\nALG = d(r, L−\nϕ1(1)) +\n⌈log2(E1)⌉\nX\nk=0\ndiam(C2k) · |C2k|\n28\n≤ d(r, L−\nϕ1(1)) + ∆\n⌈log2(E1)⌉\nX\nk=0\n(2k)2\n≤ d(r, L−\nϕ1(1)) + ∆\n3 (16E2\n1 − 1)\nAdditionally, leveraging Lemma 22 and the fact that ϕ1(g) ≤ cE1,\nd(r, L−\nϕ1(1)) ≤ d(r, g) + d(g, L−\nϕ1(1)) ≤ OPT + 1\n2 (E1 + 1) .\nCombining these bounds yields the desired result in the regime E1 ≥ 1.\nC\nRelation Between Embedding Distortion and Doubling Dimen-\nsion\nWe show that that every graph with a low-distortion embedding into the path also has small doubling\ndimension / doubling constant, as per the following lemma. Recall that the doubling constant of a metric\nspace is the smallest value λ such that, for any choice of radius R ∈ R, every ball of radius R can be covered\nwith the union of λ balls of radius R/2, and that the doubling dimension is given by log2 λ.\nLemma 24. Let G be an undirected graph on n vertices admitting an embedding into a path on n vertices\nwith distortion ρ and let λ be the doubling constant of G. Then:\nλ ≤ ⌈8ρ⌉.\nIn contrast there exist graphs with constant doubling dimension that admit no embeddings into the\nunweighted path of distortion independent of n. For example, the 2D planar grid graph on n vertices has\nconstant doubling dimension / doubling constant, but a simple argument shows that every embedding of the\n2D planar grid into the path has distortion Ω(√n).\nProof of Lemma 24. Let G = (V, E) be an undirected graph which embeds into [n] with distortion ρ. Let\nτ be an embedding which achieves this distortion. For any R > 0, let BG(u, R) ⊆ V denote the ball in\nG centered at u of radius R, and let B[n](τ(u), R) denote the ball of radius R in [n] centered at τ(u). Let\nτ(S) ⊆ [n] denote the image of S ⊆ V under τ. For any radius R and any u ∈ V , by the definition of the\nLipschitz constant we can bound\nd[n](τ(u), τ(v)) ≤ ∥τ∥LipdG(u, v) ≤ ∥τ∥LipR\n∀v ∈ BG(u, R)\nso τ(BG(u, R)) ⊆ B[n](τ(u), ∥τ∥LipR). In particular, for S1 ⊆ V , τ(S1) ⊆ S2 implies S1 ⊆ τ −1(S2), so we\nconclude BG(u, R) ⊆ τ −1(B[n](τ(u), ∥τ∥LipR)).\nConsider B[n](τ(u), ∥τ∥LipR). Fix ϵ and let kϵ denote the cardinality of an ϵ-covering of B[n](τ(u), ∥τ∥LipR).\nObserve that for any c, ϵ > 0 and any v ∈ [n], B[n](v, c) admits an ϵ-covering of cardinality at most\n⌈2c/ϵ⌉, so kϵ ≤ ⌈2∥τ∥LipR/ϵ⌉.\nLet {x1, . . . , xkϵ} ⊆ [n] denote the centers of the covering balls.\nGiven\nBG(u, R) ⊆ τ −1(B[n](τ(u), ∥τ∥LipR)), we observe that\nBG(u, R) ⊆\nkε\n[\ni=1\nτ −1 \u0000B[n](xi, ϵ)\n\u0001\n.\nIn particular, ∀i ∈ [kϵ], for any x, y ∈ B[n](xi, ϵ), the definition of the Lipschitz constant implies\ndG(τ −1(x), τ −1(y)) ≤ ∥τ −1∥Lipd[n](x, y) ≤ ∥τ −1∥Lip · 2ϵ.\nThus diam\n\u0000τ −1 \u0000B[n](xi, ϵ)\n\u0001\u0001\n≤ 2ϵ∥τ −1∥Lip. In particular, this implies that ∀i ∈ [kϵ] such that τ −1 \u0000B[n](xi, ϵ)\n\u0001\n̸=\n∅, ∃vi ∈ V such that τ −1 \u0000B[n](xi, ϵ)\n\u0001\n⊆ BG(vi, 2ϵ∥τ −1∥Lip). Thus\nBG(u, R) ⊆\nkε\n[\ni=1\nτ −1 \u0000B[n](xi, ϵ)\n\u0001\n⊆\nkε\n[\ni=1\nBG(vi, 2ϵ∥τ −1∥Lip).\n29\nWe have thus produced a covering of BG(u, R) using kϵ balls of radius 2ϵ∥τ −1∥Lip. We now choose ϵ so that\n2ϵ∥τ −1∥Lip = R/2, namely let ϵ = R/(4∥τ −1∥Lip). The cardinality of the covering is then\nkϵ ≤\n&\n2∥τ∥LipR\nϵ\n'\n=\n&\n2∥τ∥LipR · 4∥τ −1∥Lip\nR\n'\n= ⌈8∥τ∥Lip∥τ −1∥Lip⌉\nUsing the fact that ρ = ∥τ∥Lip∥τ −1∥Lip we conclude that the doubling constant of G is at most ⌈8ρ⌉.\nD\nExperimental Details\nIn this section, we provide a detailed descriptions of the experiments discussed in Section 6 of the main\nbody of the paper. We outline two sets of experiments: the first set of experiments is used to evaluate\nthe performance of Algorithm 1 in the presence of absolute error, the second set is used to evaluate the\nperformance of Algorithm 2 in the presence of relative error.\nBoth these sets of experiments focus on\nstochastic error.\nAll experiments in this section were run on a 2019 MacBook Pro with a 1.4 GHz Quad-Core Intel Core\ni5 Processor with 16 GB of RAM. No GPUs were used for this experiment.\nFigure 8: A larger rendering of the left subfigure in Figure 2 in the main body of the paper.\nAbsolute Error\nThe first set of experiments corresponds to the left side of Figure 2 (replicated above as\nFigure 8). Here, we generate an error vector ⃗e ∈ Rn according to the following procedure: we fixed a value\nE1 representing the total desired ℓ1-norm of the vector of errors, and then we sampled a vector ⃗eunsigned\nuniformly at random from the scaled simplex with ℓ1-norm equal to E1, i.e.:\n⃗eunsigned ∼ E1 · ∆n\ndef\n= {⃗x ∈ Rn | ⃗x ≥ 0, ∥⃗x∥1 = E1}.\nWe then assign a random sign to each entry of ⃗eunsigned to obtain ⃗e, this is done by multiplying each\n⃗eunsigned[v] by a Rademacher random variable σv. Fixing a graph G, the predictions at each vertex v ∈ V\nare then given by f(v) = dG(v, g) + σv · ⃗eunsigned[v]. This is repeated over many instance graphs selected\nfrom four classes: Random Tree, Random Lobster, Erdos-Rényi and Circular Ladder (See paragraph Graph\n30\nFamilies below). Whenever the family of graphs chosen is stochastic, as it is the case for all classes except\nfor Circular Ladder, the graph is also resampled from its family at each iteration, so that the expectation\nis taken over the sampling of the graph topology as well as the random error. For each of these problem\ninstances, we run Algorithm 1 and record the difference between the total distance ALG travelled by the\nalgorithm to find g, and the true shortest-path distance OPT from the starting point to g, and we plot E1\nagainst it. We report mean and standard deviation of ALG − OPT over 2000 independent trials.\n(ALG-OPT)/OPT\nErdos-Renyi\nAlgorithm 1\nSmallest Prediction\n(ALG-OPT)/OPT\nRandom Lobster\nAlgorithm 1\nSmallest Prediction\n(ALG-OPT)/OPT\nCircular Ladder\nAlgorithm 1\nSmallest Prediction\nFigure 9: A comparison of the performance of Algorithm 1 with the Smallest Prediction heuristic. Each\nsubfigure represents one family of graphs: the top-left corresponds to random Erdös-Rényi graphs, the top-\nright corresponds to Random Lobster, and the middle one at the bottom corresponds to circular ladder (See\nGraph Families at the end of this section). In each figure, we plot the average and the standard deviation\nof the performance of Algorithm 1 and that of the Smallest Prediction heuristic against the magnitude of\nthe error vector E1.\nComparison to Smallest Prediction Heuristic\nWe then compare the performance of Algorithm 1 to\nthe Smallest Prediction heuristic defined in Section 6 (Figure 9). Recall that in Smallest Prediction, at\neach iteration i, the agent travels to the an arbitrary vertex vi ∈ argminv∈Vi−1 f(v). We consider the same\nfamilies of graphs as in the previous section. For each family, we compare the performance of our algorithm\nwith that of Smallest Prediction for different values of the error magnitude E1. The instances, including the\nerrors, are generated like in the previous set of experiments. Performance is measured as the total distance\ntravelled by the agent, minus the true distance OPT from r to g, as a fraction of OPT. Just like in the\nprevious experiments, we run 2000 trials for every value of E1 and report the average and standard deviation\nof the performance across those trials.\n31\nFigure 10: A larger rendering of the right subfigure of Figure 2 in the main body of the paper.\nRelative Error\nIn the right subfigure of Figure 2, for each value of ε the predictions are generated\nby setting f(v) = (1 + εv) · dG(v, g) where εv is sampled from a Gaussian distribution with mean 0 and\nstandard deviation ε/2 conditioned on the event: εv ∈ [−ε, ε]. We run Algorithm 3 and plot the value of\nthe competitive ratio ALG/OPT against the value of ε for ε ∈ [0, 0.3], and report the mean and standard\ndeviation incurred over 2000 independent trials.\nScaling with Number of Nodes\nFinally, we plot the performance of Algorithm 3 as a function of n\n(Figure 11). For this experiment we generate instances with different numbers of vertices and plot report the\naverage and standard deviation of the respective empirical competitive ratios (ALG/OPT). We run 2000\ntrials for each family of graphs and for each number n ∈ {50, 100, 500, 1000} of nodes. The error is generated\nas in the previous section with ε = 0.2.\nGraph Families\nIn the above experiments we consider the four graph families described below. All the\ngraphs considered are undirected and unweighted. In Figure 2, we sample the below graphs on n = 100\nnodes, and in Table 2, we sample them on n = 300 nodes.\n• Erdös-Renyi Random Graphs: Erdös-Rényi Gn,p graphs (Erdős et al., 1960) are a popular random\ngraph model in the literature. We sample from the distribution of Erdös-Rényi graphs with n nodes\nand edge probability p = 0.1, conditioned on the graph being connected;\n• Random Trees: Trees are just connected acyclic graphs. We sample trees on n vertices uniformly at\nrandom;\n• Random Lobster Graphs: A lobster graph is a tree which becomes a caterpillar graph when its leaves\nare removed, we sample random lobster graphs on n vertices;\n• Circular Ladder Graphs: A circular ladder graph is a graph obtained by gluing the endpoints of a\nladder graph, i.e. it’s a graph on vertices {1, ..., 2k} where the edges are of the form (i, i + 1) for\ni = 1, ..., k−1 and i = k+1, ..., 2k, and (i, k+i) for i = 1, ..., k as well as (1, k), (k+1, 2k). We consider\nthe circular ladder graph on n vertices.\n32\nALG/OPT\nNumber of Nodes\nFigure 11: The empirical competitive ratio of Algorithm 3 for different graph families and for different\nvalues of n.\nOn the x-axis: the number of vertices n in the instance graphs considered.\nWe consider\nn = 50, 100, 500, 1000. On the y-axis: the ratio between the distance travelled by the agent, and the true\ndistance OPT from r to g in G.\n33\n"
}