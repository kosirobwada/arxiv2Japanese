{
    "optim": "Multi-Agent Deep Reinforcement Learning for\nDistributed Satellite Routing\nFederico Lozano-Cuadra, Beatriz Soret Senior Member, IEEE\nAbstract—This paper introduces a Multi-Agent Deep Rein-\nforcement Learning (MA-DRL) approach for routing in Low\nEarth Orbit Satellite Constellations (LSatCs). Each satellite is\nan independent decision-making agent with a partial knowledge\nof the environment, and supported by feedback received from the\nnearby agents. Building on our previous work that introduced a\nQ-routing solution, the contribution of this paper is to extend it to\na deep learning framework able to quickly adapt to the network\nand traffic changes, and based on two phases: (1) An offline\nexploration learning phase that relies on a global Deep Neural\nNetwork (DNN) to learn the optimal paths at each possible\nposition and congestion level; (2) An online exploitation phase\nwith local, on-board, pre-trained DNNs. Results show that MA-\nDRL efficiently learns optimal routes offline that are then loaded\nfor an efficient distributed routing online.\nI. INTRODUCTION\nLow Earth Orbit (LEO) Satellite Constellations (LSatCs) are\none of the pillars of 6G ubiquitous and global connectivity,\nenhancing cellular coverage, supporting a global backbone,\nand enabling advanced applications [1]. Unlike terrestrial\nnetworks with stable links that can be handled with Dijkstra’s\nalgorithm and static routing tables, the unique characteristics\nof LSatCs calls for specific routing solutions. Specifically,\nLSatCs deal with rapidly moving satellites, predictable yet\ndynamic topology, significant propagation delays, and unbal-\nanced and unpredictable terrestrial traffic [2].\nThis paper introduces a novel approach for the End-to-\nEnd (E2E) packet routing in LSatCs, avoiding the dependence\nupon the ground infrastructure and aiming for a robust,\nlow-latency solution. Building on our previous work in Q-\nrouting [3], we extend it to a deep learning framework able to\nhandle a more complex state space, including local position\nand congestion information. This allows the agent to adapt\neasily to new situations. Our approach utilizes Multi-Agent\nDeep Reinforcement Learning (MA-DRL) where each satellite\nacts as a different agent with partial knowledge of the envi-\nronment, informed by feedback from nearby agents. Unlike\nprevious Machine Learning applications in routing, which\nhave struggled with dynamic queuing times and multi-agent\ninteractions [4]–[6], our MA-DRL algorithm incorporates a\ntwo-phase solution: (1) An offline exploration learning phase\nutilizing a global Deep Neural Network (DNNg) to learn\noptimal paths for each position and congestion condition (Fig.\n1.3); (2) An online exploitation phase with local, on-board,\nF. Lozano-Cuadra (flozano@ic.uma.es) and B. Soret are with the Telecom-\nmunications Research Institute, University of Malaga, 29071, Malaga, Spain.\nThis work is partially funded by ESA SatNEx V (prime contract no.\n4000130962/20/NL/NL/FE), and by the Spanish Ministerio de Ciencia, Inno-\nvaci´on y Universidades (PID2022-136269OB-I00).\nFig. 1. System model: 1) network graph; 2) multi-agent interaction needed\nto build the tuple experience (SARS); 3) and 4) are representations of the\noffline exploration and online exploitation phases, respectively.\npre-trained DNNs (Fig. 1.4). This paper presents a comprehen-\nsive model of LSatC and ground infrastructure, formulating\nrouting as a Partially Observable Markov Decision Problem\n(POMDP) [3], and demonstrates that our MA-DRL algorithm\nlearns and utilizes optimal routes for distributed routing.\nII. SYSTEM MODEL\nThe LSatC network, formed by both space and ground layers,\nis abstracted as a graph. The space segment consists of N\nsatellites across M orbital planes (Fig. 1.1), forming a finite\nset of satellite nodes, S, and a set of edges, E, representing\nthe transmission links between them. Each satellite Sati is\nequipped with 2 antennas for intra-plane and 2 for inter-plane\ncommunication, with their feasible edge set, Ei.\nThe ground segment includes a set of gateways (Fig. 1.1),\nG, located at key global positions (Fig. 1.1). These gateways\nmaintain a single ground-to-satellite link (GSL) with their\nnearest satellite, constituting the edge set EG. The data rate\nfor communication is determined by the highest modulation\nand coding scheme possible within the current Signal-to-Noise\nRatio (SNR), in line with DVB-S2 standards. Each gateway\ngathers the ground traffic and distributes it to each other\ngateway equally by injecting it to the LSatC.\nThe latency is computed considering the queue time at the\nsatellite, transmission time based on data rate and propagation\ntime over the link distance. This latency model accounts for\narXiv:2402.17666v1  [cs.LG]  27 Feb 2024\nvarying traffic loads, where propagation time dominates in\nnon-congested networks, but queue time quickly escalates\nunder high traffic conditions [2].\nBenchmarking involves comparing our MA-DRL algo-\nrithm with a traditional shortest path routing approach using\nDijkstra’s algorithm, where edge weights are proportional to\nthe slant range between nodes.\nIII. LEARNING FRAMEWORK\nIn our MA-DRL, each satellite works as an independent agent\nin a networked multi-agent system, where the decision-making\nprocess for routing data packets is based on a POMDP. Upon\npacket arrival, each agent observes the state St and makes\nan action at. The observed St includes information about\nthe agent’s position, neighboring agents positions, packet\ndestination and neighboring agents congestion levels. The at\nto take consists on selecting the next hop for forwarding the\npacket. Afterwards, the reward rt for the (St, at) pair is based\nslant range reduction from the packet to its destination after\nbeing forwarded and time spent on the receiving agent queue.\nIn conventional DRL, an agent i stores every tuple of\nexperiences (St, at, rt, St+1) in order to learn, where St+1\nis the state where i has transited to and rt is the reward after\ntaking at at the observed state St. The innovative aspect of\nthe MA-DRL algorithm lies in observing the impact of an\naction at from the perspective of a packet p with destination\nd, p(d). When p(d) is forwarded by an agent Sati to another\nagent Satj, it transits from the state St observed in Sati\nto S∗\nt+1 observed at Satj. This experience tuple SARS:\n(St, at, rt, S∗\nt+1) with states observed in the interacting agents\nis then stored in a experience buffer D and used to train a\nDNN that learns the optimal routing policy (Fig. 1.2).\nThe learning process involves two phases. The first is\nthe offline exploration (Fig. 1.3), where there is a global\nexperience buffer Dg where experiences from all the agents\nare stored and used to train a global DNNg. After DNNg\nlearns the optimal routing policy at every observed state the\nonline exploitation phase starts (Fig. 1.4). Here every agent\ni has its own DNNi onboard, which is a copy of the trained\nDNNg that dictates the routing policy. In both phases there\nis a minimal feedback needed between satellites where each\nagent i needs its neighboring agents congestion information\nin order to observe St. Moreover, each agent i needs the new\nstate S∗\nt+1 encountered at p(d)´s receiving agent j and the\ntime spent on its queue in order to compute rt (Fig 1.2). This\ninformation is then stored in the local experience buffer Di\nin order let i keep training DNNi with local data.\nIV. PRELIMINARY RESULTS\nDuring the exploration phase, the weights θg that parame-\nterize DNNg are initialized randomly. Coupled with a high\nexploration rate ϵ, DNNg tends to make random routing\nactions initially. Fig. 2 shows how this behaviour disappears\nas ϵ decreases: DNNg learns first sub-optimal paths and then\nconverges to the shortest path in less than 1 second of real\ntime simulated.\nFig. 2. E2E latency versus packet creation time. Comparison of our MA-DRL\nwith the Q-Routing algorithm [3] and the slant range shortest path benchmark.\nNote that the Q-learning method [3] converges faster than\nDNN due to simpler Q-Tables without positional data and\nreduced queue details, but it must continually converge to\nnew solutions, which limits its adaptability to congestion\nand location changes, unlike MA-DRL. The shortest path\nalgorithm has real time information about all the LSatC, while\nMA-DRL has only 1 hop neighboring information at every\nhop, which is a more realistic approach; it is impractical to\nhave real time information about the whole LSatC due to\nthe congestion caused by feedback messages and propagation\ntimes delays.\nV. CONCLUSIONS\nThe implementation of MA-DRL in LSatC demonstrates\npromising results in terms of efficient learning, as it quickly\nconverges to an optimal routing policy with minimal LSatC\nlocal status information at every hop, showcasing the effec-\ntiveness of the decentralized DNN-based approach. Notably,\nin terms of adaptability, MA-DRL has not only learned the\noptimal path but also a set of alternative paths during the\noffline exploration phase. This aspect becomes particularly\nadvantageous in more loaded LSatCs where the agents, being\naware of their neighbors’ congestion status, can seamlessly\nswitch to these alternative paths. This ability to adapt to\nchanging network conditions by selecting appropriate routes\nunderscores the robustness and practical utility of our MA-\nDRL approach in dynamic satellite environments.\nREFERENCES\n[1] I. Leyva-Mayorga, B. Soret, M. R¨oper et al., “LEO small-satellite\nconstellations for 5G and Beyond-5G communications,” IEEE Access,\nvol. 8, pp. 184 955–184 964, 2020.\n[2] J. W. Rabjerg, I. Leyva-Mayorga, B. Soret, and P. Popovski, “Exploiting\ntopology awareness for routing in LEO satellite constellations,” in Proc.\nIEEE GLOBECOM, 2021.\n[3] B. Soret, I. Leyva-Mayorga, F. Lozano-Cuadra, and M. D. Thorsager,\n“Q-learning for distributed routing in leo satellite constellations,” arXiv\npreprint arXiv:2306.01346, 2023.\n[4] Z. M. F. et al., “State-of-the-art deep learning: Evolving machine intel-\nligence toward tomorrow’s intelligent network traffic control systems,”\nIEEE Comms. Surveys & Tutorials, vol. 19, no. 4, pp. 2432–2455, 2017.\n[5] J. Liu and B. Z. et al., “DRL-ER: An intelligent energy-aware routing\nprotocol with guaranteed delay bounds in satellite mega-constellations,”\nIEEE Trans. on Netw Sci. and Eng., vol. 8, pp. 2872–2884, 2021.\n[6] D. Liu, J. Zhang, J. Cui et al., “Deep learning aided routing for space-air-\nground integrated networks relying on real satellite, flight, and shipping\ndata,” IEEE Wireless Communications, vol. 29, no. 2, pp. 177–184, 2022.\n"
}