{
    "optim": "Variational Learning is Effective for Large Deep Networks\nYuesong Shen * 1 Nico Daheim * 2 Bai Cong 3 Peter Nickl 4 Gian Maria Marconi 4 Clement Bazan 3\nRio Yokota 3 Iryna Gurevych 2 Daniel Cremers 1 Mohammad Emtiyaz Khan 4 Thomas M¨ollenhoff 4\nAbstract\nWe give extensive empirical evidence against the\ncommon belief that variational learning is inef-\nfective for large neural networks. We show that\nan optimizer called Improved Variational Online\nNewton (IVON) consistently matches or outper-\nforms Adam for training large networks such as\nGPT-2 and ResNets from scratch. IVON’s com-\nputational costs are nearly identical to Adam but\nits predictive uncertainty is better. We show sev-\neral new use cases of IVON where we improve\nfine-tuning and model merging in Large Language\nModels, accurately predict generalization error,\nand faithfully estimate sensitivity to data. We find\noverwhelming evidence in support of effective-\nness of variational learning.\nCode available at this link.\n1. Introduction\nVariational learning can potentially improve many aspects\nof deep learning, but there remain doubts about its effective-\nness for large-scale problems. Popular strategies (Graves,\n2011; Blundell et al., 2015) do not easily perform well,\neven on moderately-sized problems, which has led some\nto believe that it is impossible to get both good accuracy\nand uncertainty (Trippe & Turner, 2017; Foong et al., 2020;\nCoker et al., 2022). Variational methods generally have\nhigher costs or tricky implementations (Kingma et al., 2015;\nHern´andez-Lobato & Adams, 2015; Zhang et al., 2018;\nKhan et al., 2018; Osawa et al., 2019), and it is a struggle to\nkeep up with the ever-increasing scale of deep learning.\nCurrently, no variational method can accurately train Large\nLanguage Models (LLMs) from scratch at a cost, say, similar\nto Adam (Kingma & Ba, 2015). This is excluding methods\nsuch as MC-dropout (Gal & Ghahramani, 2016), stochas-\ntic weight averaging (SWAG) (Maddox et al., 2019), and\n*Equal contribution 1Technical University of Munich & Munich\nCenter for Machine Learning 2UKP Lab, Technical University of\nDarmstadt & hessian.AI 3Tokyo Institute of Technology 4RIKEN\nCenter for AI Project. Correspondence to: Thomas M¨ollenhoff\n<thomas.moellenhoff@riken.jp>.\nLaplace (MacKay, 1992), which do not directly optimize\nthe variational objective, even though they have variational\ninterpretations. Ideally, we want to know whether a direct\noptimization of the objective can match the accuracy of\nAdam-like methods without any increase in the cost, while\nalso yielding good weight-uncertainty to improve calibra-\ntion, model averaging, knowledge transfer, etc.\nIn this paper, we present the Improved Variational Online\nNewton (IVON) method, which adapts the method of Lin\net al. (2020) to large scale and obtains state-of-the-art accu-\nracy and uncertainty at nearly identical cost as Adam. Fig. 1\nshows some examples where, for training GPT-2 (773M\nparameters) from scratch, IVON gives 0.4 reduction in vali-\ndation perplexity over AdamW and, for ResNet-50 (25.6M\nparameters) on ImageNet, it gives around 2% more accurate\npredictions that are also better calibrated. For image classi-\nfication, we never observe severe overfitting like AdamW\nand consistently obtain better or comparable results to SGD.\nWe introduce practical tricks necessary to achieve good per-\nformance and present an Adam-like implementation (Alg. 1)\nwhich uses a simplified Hessian-estimation scheme to both\nadapt the learning rate and estimate weight-uncertainty. This\nalso makes IVON a unique second-order optimizer that con-\nsistently performs better than Adam at a similar cost. We\npresent extensive numerical experiments and new use cases\nto demonstrate its effectiveness. We find that,\n1. IVON gets better or comparable predictive uncertainty\nto alternatives, such as, MC-dropout and SWAG;\n2. It works well for finetuning LLMs and reduces the cost\nof model-merging;\n3. It can be used to faithfully predict generalization which\nis useful for diagnostics and early stopping;\n4. It is useful to understand sensitivity to data which is\noften challenging at large-scale due to ill-conditioning.\nOverall, we find overwhelming evidence that variational\nlearning is not only effective but also useful for large deep\nnetworks, especially LLMs. IVON is easily amenable to\nflexible posterior forms (Lin et al., 2019), and we expect it to\nhelp researchers further investigate the benefits of Bayesian\nprinciples to improve deep learning.\n1\narXiv:2402.17641v1  [cs.LG]  27 Feb 2024\nVariational Learning is Effective for Large Deep Networks\n0\n25\n50\n75\n100\nTrain step (x1,000)\n12\n18\n24\nValidation Perplexity\n24h\n125M\n355M\n773M\n12h\nIVON\nAdamW\n(a) GPT-2 on OpenWebText\n0\n50\n100\n150\n200\n250\nTrain step (x 1,000)\n75\n50\n25\nValidation Error\n12h\n24h\n(b) ResNet-50 on ImageNet\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\nMean predicted probability\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue probability\n(c) Calibration on ImageNet\nFigure 1: First two panels show that IVON closely matches the trajectory of AdamW (Loshchilov & Hutter, 2017) for\ntraining GPT-2 on OpenWebText and ResNet-50 on ImageNet. The computational costs of IVON and AdamW are nearly\nidentical. Runtime in hours (h) is indicated by the arrows. The third panel shows that the predictions are also better calibrated\nas the red curve is closer to diagonal. Comparisons to SGD on ImageNet are in Table 1. Final numbers for IVON vs AdamW\nare as follows: 12.6 vs. 13.0 perplexity (lower is better) on GPT-2 (773M), 14.1 vs 14.5 perplexity on GPT-2 (355M), 17.9\nvs 18.1 perplexity on GPT-2 (125M), 77.5 vs 75.2 accuracy and 0.022 vs 0.066 ECE (lower is better) on ResNet-50.\n2. Challenges of Variational Learning for\nLarge Deep Networks\nVariational learning is challenging for large networks due to\nfundamental differences in its objective to those commonly\nused in deep learning. Deep learning methods estimate\nnetwork weights θ ∈ RP by minimizing empirical risk\n¯ℓ(θ) = PN\ni=1 ℓi(θ)/N, which is an average over individual\nlosses ℓi(θ) for N examples. In contrast, variational meth-\nods estimate a distribution q(θ) over weights by minimizing\nL(q) = λEq(θ)\n\u0002¯ℓ(θ)\n\u0003\n+ DKL(q(θ) ∥ p(θ)),\n(1)\nwhere p(θ) is the prior, DKL(· ∥ ·) the Kullback-Leibler di-\nvergence and λ a scaling parameter often set to N, but other\nvalues are useful, for example, to handle model misspeci-\nfication. The objective in Eq. 1 coincides with variational\ninference when ¯ℓ(θ) is a proper likelihood. We use the term\nvariational learning to denote the general case.\nOptimization of L(q) is fundamentally different from that\nof ¯ℓ(θ). For instance, the number of parameters of q can\nbe much larger than the size of θ, making the problem\nharder. The number of parameters of q is doubled for a\ndiagonal-covariance Gaussian q(θ) = N(θ | m, diag(σ)2)\ndue to the estimation of two vectors of mean m ∈ RP and\nstandard deviation σ ∈ RP , respectively. The optimization\nis further complicated because of the expectation in Eq. 1,\nwhich adds additional noise during the optimization.\nDue to these differences, a direct optimization of Eq. 1\nremains challenging. The standard approach is to optimize\nit by using a standard deep learning method, say, SGD,\nm ← m − ρb∇mL\nσ ← σ − ρb∇σL,\nwhere ρ > 0 is the learning rate. This showed promising\nresults in early attempts of variational deep learning with\nseveral different stochastic gradient estimators b∇ (Graves,\n2011; Blundell et al., 2015). Unfortunately, these methods\nhave been unable to keep up with the growth in scale of\ndeep learning. The lack of progress has been attributed to\nvarious causes, such as high-variance in stochastic gradi-\nents (Kingma et al., 2015; Wen et al., 2018), issues with\nthe temperature parameter (Wenzel et al., 2020; Noci et al.,\n2021), and lack of a good prior (Fortuin et al., 2022). Multi-\nple thereotical studies have raised doubts whether variational\nlearning can ever work at all (Trippe & Turner, 2017; Foong\net al., 2020; Coker et al., 2022). Altogether, these have led\nto a belief that there exists an inherent trade-off between\naccuracy and uncertainty in Bayesian learning.\nProgress in variational learning has been made on a differ-\nent front by using natural-gradient methods (Sato, 2001;\nHoffman et al., 2013; Khan & Lin, 2017) which have shown\npromising results on ImageNet (Osawa et al., 2019). Their\nupdates resemble an Adam-like form which makes it easy\nto tune them at large scale. Yet, the implementation can\nbe tricky and the cost can be much higher than Adam. For\nexample, Osawa et al. (2019) build upon the Variational\nOnline Newton (VON) method of Khan et al. (2018) where\nthey replace the Hessian computation by a Gauss-Newton\nestimate. They implement the following Adam-like update:\nbh ← 1\n|B|\nX\ni∈B\n∇ℓi(θ)2,\nwhere θ ∼ q,\ng ← β1g + b∇¯ℓ(θ) + s0m/λ,\nh ← β2h + (1 − β2)bh,\nm ← m − αtg/(h + c),\nσ ← 1/\np\nλ(h + c).\n(2)\nHere, a prior p(θ) = N(θ | 0, I/s0) is used. The difficult\n2\nVariational Learning is Effective for Large Deep Networks\ncomputation is in the first line of Eq. 2 where a Gauss-\nNewton estimate over a minibatch B is computed at a sample\nfrom the Gaussian, while the rest is similar to Adam: the\nsecond line is gradient momentum, where s0m/λ is added\ndue to the prior. The third and fourth line are identical to\nthe scale and parameter vectors updates, respectively. The\nconstant c = γ +s0/λ where γ > 0 is a damping parameter.\nThe computation of the Gauss-Newton estimate is tricky\nbecause it requires per-example squaring, which is not a\nstandard operation in deep learning and could be difficult\nto implement. In Osawa et al. (2019, Fig. 1), this ends up\nincreasing the cost by a factor of two. The Gauss-Newton\nestimate also introduces an additional approximation in the\nvariational learning, even though it helps to ensure the posi-\ntivity of h. Another issue is the use of an additional damping\nparameter γ which departs from the Bayesian framework.\nIdeally, we want a method that directly optimizes Eq. 1 with-\nout additional approximations and also seamlessly fits into\nan Adam-like framework without any significant compu-\ntational overheads. Methods such as MC-dropout, SWAG,\nand Laplace do not solve this problem, and rather circum-\nvent it by relying on algorithms that optimize ¯ℓ, not L. The\ngoal of this paper is to propose a method that can match the\naccuracy of Adam while directly optimizing L.\n3. Improved Variational Online Newton\nWe present the Improved Variational Online Newton (IVON)\nmethod by adapting the method of Lin et al. (2020) and\nintroducing practical tricks necessary to achieve good per-\nformance at large scale. They propose an improved version\nof the Bayesian Learning Rule (Khan & Rue, 2021) which\nensures positivity of certain variational parameters, such as,\nthe Gaussian variance or scale parameter of Gamma distri-\nbution. For the Gaussian case, they propose an Adam-like\nupdate which makes the update in Eq. 2 simpler. Specifi-\ncally, they use the following Hessian estimate by using the\nreparameterization trick,\nbh ← b∇¯ℓ(θ) · θ − m\nσ2\n,\n(3)\nwhich does not require per-example gradient squares, rather\njust a single vector multiplication with the minibatch gra-\ndient. The above estimate is easy to compute but, unlike\nthe Gauss-Newton estimate, it is not always positive and\ncan make h in Eq. 2 negative (Khan et al., 2018, App. D).\nLin et al. (2020) solve this problem by using Riemannian\ngradient descent which ensures positivity by adding an extra\nterm in the update of h,\nh ← (1 − ρ)h + ρbh + 1\n2ρ2(h − bh)2/(h + s0/λ).\n(4)\nPositivity holds even when bh are negative, as shown in Lin\net al. (2020, Theorem 1).\nAlgorithm 1 Improved Variational Online Newton (IVON).\nHyperparameter setting is described in App. A.\nRequire: Learning rates {αt}, weight-decay δ > 0.\nRequire: Momentum parameters β1, β2 ∈ [0, 1).\nRequire: Hessian init h0 > 0.\nInit: m ← (NN-weights), h ← h0,\ng ← 0,\nλ ← N.\nInit: σ ← 1/\np\nλ(h + δ).\nOptional: αt ← (h0 + δ)αt for all t.\n1: for t = 1, 2, . . . do\n2:\nbg ← b∇¯ℓ(θ), where θ ∼ q\n3:\nbh ← bg · (θ − m)/σ2\n4:\ng ← β1g +(1−β1)bg\n5:\nh ← β2h+(1−β2)bh+ 1\n2(1 − β2)2(h − bh)2/(h + δ)\n6:\n¯g ← g/(1 − βt\n1)\n7:\nm ← m − αt(¯g + δm)/(h + δ)\n8:\nσ ← 1/\np\nλ(h + δ)\n9: end for\n10: return m, σ\nIn Alg. 1, we use the two modifications (highlighted in red)\nto get an improved version of VON, called IVON. The up-\ndates closely resemble Adam. One major change is the\nsampling step in line 2 and a minor difference is the lack\nof square-root over h + δ in line 7. IVON therefore uses a\nproper Newton-like update but, instead of using bh, it uses\nthe smoothed average h. This can reduce instability due\nto the noise in the Hessian estimate. The Hessian estima-\ntor in Eq. 3 is also less costly compared to other second-\norder optimizers (Dauphin et al., 2015; Yao et al., 2021; Liu\net al., 2023). It is valid even for losses that are not twice-\ndifferentiable (for example, for ReLU activations). These\naspects make IVON a unique second-order optimizer with\nnearly identical cost to Adam.\nBelow, we list a few practical tricks needed for good results.\n1. Instead of the prior precision s0, we use the weight-\ndecay regularizer δ as the prior. The scaling parameter\nλ is set to N, except for finetuning on small datasets.\n2. Unlike Lin et al. (2020, Fig. 1), the update of h does\nnot use δ. We also update h before m which has no\nimpact on the performance. Also, we do not debias h.\n3. The Hessian h is initialized with a constant h0. Lin\net al. (2020) most likely set it to 0 due to the debiasing\nstep used in their work. We find the initialization to be\nuseful. Too small values can destabilize the training\nwhile larger values may give poor performance.\n4. When training transformers, it can be helpful to clip the\npreconditioned gradient in line 7 entrywise to [−ξ, ξ].\n3\nVariational Learning is Effective for Large Deep Networks\nDataset / Model\nMethod\nTop-1 Acc. ↑\nTop-5 Acc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nAdamW\n(2%) 75.16±0.14\n92.37±0.03\n1.018±0.003\n0.066±0.002\n0.349±0.002\nSGD\n(1%) 76.63±0.45\n93.21±0.25\n0.917±0.026\n0.038±0.009\n0.326±0.006\nIVON@mean\n77.30±0.08\n93.58±0.05\n0.884±0.002\n0.035±0.002\n0.316±0.001\nImageNet\nResNet-50\n(26M params)\nIVON\n77.46±0.07\n93.68±0.04\n0.869±0.002\n0.022±0.002\n0.315±0.001\nAdamW\n(15%) 47.33±0.90\n71.54±0.95\n6.823±0.235\n0.421±0.008\n0.913±0.018\nSGD\n(1%) 61.39±0.18\n82.30±0.22\n1.811±0.010\n0.138±0.002\n0.536±0.002\nIVON@mean\n62.41±0.15\n83.77±0.18\n1.776±0.018\n0.150±0.005\n0.532±0.002\nTinyImageNet\nResNet-18\n(11M params)\nIVON\n62.68±0.16\n84.12±0.24\n1.528±0.010\n0.019±0.004\n0.491±0.001\nAdamW\n(11%) 50.65±0.0∗\n74.94±0.0∗\n4.487±0.0∗\n0.357±0.0∗\n0.812±0.0∗\nSGD\n(2%) 59.39±0.50\n81.34±0.30\n2.040±0.040\n0.176±0.006\n0.577±0.007\nIVON@mean\n60.85±0.39\n83.89±0.14\n1.584±0.009\n0.053±0.002\n0.514±0.003\nTinyImageNet\nPreResNet-110\n(4M params)\nIVON\n61.25±0.48\n84.13±0.17\n1.550±0.009\n0.049±0.002\n0.511±0.003\nAdamW\n(11%) 64.12±0.43\n86.85±0.51\n3.357±0.071\n0.278±0.005\n0.615±0.008\nSGD\n(1%) 74.46±0.17\n92.66±0.06\n1.083±0.007\n0.113±0.001\n0.376±0.001\nIVON@mean\n74.51±0.24\n92.74±0.19\n1.284±0.013\n0.152±0.003\n0.399±0.002\nCIFAR-100\nResNet-18\n(11M params)\nIVON\n75.14±0.34\n93.30±0.19\n0.912±0.009\n0.021±0.003\n0.344±0.003\nAdamW\n(10%) 65.88±0.84\n88.34±0.56\n2.893±0.088\n0.258±0.006\n0.578±0.014\nSGD\n(2%) 74.19±0.11\n92.41±0.14\n1.204±0.012\n0.137±0.002\n0.393±0.004\nIVON@mean\n75.23±0.23\n93.45±0.16\n1.149±0.010\n0.136±0.002\n0.380±0.003\nCIFAR-100\nPreResNet-110\n(4M params)\nIVON\n75.81±0.18\n93.93±0.19\n0.884±0.007\n0.030±0.003\n0.336±0.001\nTable 1: IVON improves both accuracy and uncertainty over SGD and AdamW. Improvements in accuracy by IVON are\nshown in red. The performance of AdamW is not good on the smaller datasets likely due to overfitting when training for 200\nepochs. IVON does not have this issue. Additional results are in the appendix in Tables 8 to 10 and Table 12.\n5. Optionally, we rescale αt by (h0 + δ) so that the first\nsteps of the algorithm have step-size close to the initial\nαt. When clipping is used, this step is omitted.\nMomentum β1, learning rate αt and weight-decay δ can\nbe set in the same fashion as for standard optimizers, as\nwell as minibatch size and clipping radius ξ. β2 typically\nneeds to be closer to one as in Adam, for instance, values of\nβ2 = 0.99995 work well. Setting of h0 and λ is also easy,\nas discussed above. This makes obtaining good results with\nIVON often very easy. A detailed guide for hyperparameter\nsetting is in App. A.\nWe implement IVON as a drop-in replacement for\nAdam in PyTorch1, where only two lines need to be\nadded (shown in red below) to sample noisy weights.\nfor inputs, targets in dataloader:\nfor _ in range(num_mc_samples):\nwith optimizer.sampled_params(train=True):\noptimizer.zero_grad()\noutputs = model(inputs)\nloss = loss_fn(outputs, targets)\nloss.backward()\noptimizer.step()\nIVON training can easily be generalized to using multiple\nMC samples. Furthermore, is is easliy parallelized, for\nexample, by using multiple GPUs.\nOn the other hand,\n1https://github.com/team-approx-bayes/ivon\nmemory-efficiency can be improved by using accumulation\nsteps for the gradient and Hessian estimate on each device.\nThis can be implemented by replacing the calculations\nof bg and bh in line 2 and 3 of Alg. 1, respectively, by the\nfollowing quantities:\nbg =\nX\nj,s\nαj b∇¯ℓ(θ(s)\nj ), bh =\nX\nj,s\nαj b∇¯ℓ(θ(s)\nj )\nθ(s)\nj\n− m\nσ2\n.\nHere, we use a different random weight θ(s)\nj\n∼ q on\neach device and accumulation step, where the index j\nranges over number of devices times accumulation steps\nand the index s over the number of per-device MC sam-\nples S. Using a different random noise is similar to the local\nreparametrization trick by Kingma et al. (2015) which leads\nto reduced variance. The weighting coefficients are given by\nαj = Bj/ P\nj SBj where Bj denotes the minibatch-size of\nthe stochastic gradient b∇¯ℓ in device/accumulation step j.\n4. IVON is Effective for Large Deep Networks\nWe show that IVON effectively trains large deep networks\nfrom scratch (Sec. 4.1) and enables many downstream ap-\nplications, such as, predictive uncertainty (Sec. 4.2), fine-\ntuning and model merging (Sec. 4.3), as well as predict-\ning generalization and finding influential training examples\n(Sec. 4.4). We perform ablation studies on computational\nefficiency (App. B.1) and the choice of Hessian estima-\n4\nVariational Learning is Effective for Large Deep Networks\n0\n25\n50\n75\n100\nTrain step (x1,000)\n2.4\n2.8\n3.2\nTrain loss\nIVON\nAdamW\n(a) Training loss on OpenWebText\n0\n5\n10\n15\n20\nTrain step (x1,000)\n3.0\n4.0\nTrain loss\nIVON (float32)\nIVON (bf16)\n(b) Low precision training\n1\n2\n4\n8\n16\n32\n64\n# MC Samples\n14.470\n14.485\n14.500\nValidation Perplexity\nPredictive Posterior\nMean\n(c) Predictive posterior\nFigure 2: When training GPT-2, IVON not only improves upon AdamW in terms of validation perplexity but also converges\nto matching or even better training loss than AdamW, as shown in (a). IVON also provides stable training when using\nlow-precision floating point numbers, as shown in (b) on the example of bf16. Finally, in (c) we see that using posterior\naveraging using IVON’s learned distribution we can further improve the validation perplexity on GPT-2.\ntor (App. B.2) In the following, we refer by IVON@mean\nto the prediction using m as the weights, whereas IVON\ndenotes a model average with 64 samples drawn from the\nposterior learned by IVON.\n4.1. Better Scalability and Generalization\nHere, we show how IVON scalably trains large models from\nscratch. First, we train LLMs with up to 773M parameters\nfrom scratch on ca. 50B tokens in Sec. 4.1.1. Then, we show\nimproved accuracy and uncertainty when training various\nimage classification models, such as ResNets with 26M pa-\nrameters at ImageNet-scale, in Sec. 4.1.2. Additional results\non smaller recurrent neural networks with 2M parameters\nare shown in App. D.1.\n4.1.1. PRETRAINING LANGUAGE MODELS\nPretraining transformer language models (Vaswani et al.,\n2017) with variational learning has been challenging and no\nlarge-scale result exists so far. We show that IVON can train\nlarge language models at scale. We train models following\nthe GPT-2 (Radford et al., 2019) architecture for 49.2 bil-\nlion tokens in total on the OpenWebText corpus (Gokaslan\n& Cohen, 2019). We use the same hyperparameters for\nAdamW as prior work (Liu et al., 2023) and optimize the hy-\nperparameters for IVON by grid search on a smaller model.\nWe pretrain models with 125M, 355M (“GPT-2-medium”),\nand 773M (“GPT-2-large”) parameters from scratch using\ngradient clipping to stabilize the training. Details and exact\nhyperparameters are given in App. C.1.\nValidation perplexities are reduced from 18.1 to 17.9,\nfrom 14.5 to 14.1 and from 13.0 to 12.6 for models with\n125M, 355M and 773M parameters, respectively, as shown\nin Fig. 1(a). IVON also converges to matching or even better\ntraining loss than AdamW, as shown in Fig. 2(a). In Fig. 2(b)\nwe show that IVON also provides stable training with bf16\nprecision, and in Fig. 2(c) we show that using the predictive\nposterior by sampling multiple models from IVON’s learned\ndistribution further improves performance when a sufficient\nnumber of samples is used. This demonstrates that varia-\ntional learning is effective for training large Transformers\nfrom scratch on large datasets.\n4.1.2. IMAGE CLASSIFICATION\nWe compare IVON to AdamW (Loshchilov & Hutter, 2017)\nand SGD for image classification on various models and\nbenchmarks. Table 1 shows that IVON improves upon both\nAdamW and the stronger SGD baseline in terms of both\naccuracy and uncertainty, here measured by negative log-\nlikelihood (NLL), expected calibration error (ECE), and\nBrier score. We also find that IVON does not overfit on\nsmaller tasks, unlike AdamW which tends to overfit on Tiny-\nImageNet and CIFAR-100. This holds on various datasets\nand models trained for 200 epochs, of which we show here:\n1) ResNet-50 with ≈ 25.6M parameters (He et al., 2016a) on\nImageNet-1k which has ≈ 1.2M images with 1000 classes;\n2) ResNet-18 with 11M parameters and PreResNet-110 with\n4M parameters on both TinyImageNet and CIFAR-100. We\nlist further details on the experiments in App. C.2 along\nwith more results using also DenseNet-121 and ResNet-20\non other datasets, such as CIFAR-10. There, IVON again\nimproves accuracy and uncertainty.\nWe hypothesize that these improvements are partly due to\nflat-minima seeking properties of variational learning. Meth-\nods aimed to find flat minima, such as sharpness-aware min-\nimization (SAM) (Foret et al., 2021), have recently gained\nin popularity to boost test-accuracy. M¨ollenhoff & Khan\n(2023) have shown that SAM optimizes a relaxation of the\nvariational loss in Eq. 1. Our results here indicate that simi-\nlar improvements can be obtained by direct optimization.\n5\nVariational Learning is Effective for Large Deep Networks\nSGD\nBBB\nMC-D\nSWAG\nIVON\n(a) CIFAR-10 & SVHN (OOD)\nSGD\nBBB\nMC-D\nSWAG\nIVON\n(b) CIFAR-10 & Flowers102 (OOD)\n2.5\n0.0\n2.5\n5.0\n7.5\n2\n1\n0\n1\n2\n(c) IVON in-between uncertainty\nFigure 3: In panel (a) and (b) we see that IVON’s histogram of predictive entropy has a high peak similar to SGD for\nin-domain data (red, CIFAR-10) but at the same time is spread out widely similar to the other Bayesian deep learning\nmethods for out-of-domain data (gray, SVHN (a) & Flowers102 (b)). The colors are shaded proportional to the height of the\npeak, that is, darker red or darker gray indicates a higher peak. In panel (c) we see that IVON handles in-between uncertainty\nwell, which has been shown to be challenging for variational methods by Foong et al. (2019).\nAcc. (%) ↑\nNLL ↓\nECE ↓\nBrier ↓\nAdamW\n90.04±0.27\n0.589±0.018\n0.074±0.002\n0.170±0.004\nSGD\n91.86±0.14\n0.288±0.015\n0.040±0.004\n0.126±0.004\nBBB\n91.09±0.16\n0.289±0.005\n0.053±0.001\n0.139±0.002\nMC-D\n91.85±0.17\n0.242±0.004 0.008±0.002 0.120±0.002\nSWAG\n92.45±0.23\n0.230±0.002\n0.024±0.002\n0.112±0.002\nIVON\n92.71±0.07 0.219±0.002 0.008±0.001 0.108±0.001\nDeep Ens.\n93.57±0.16\n0.198±0.003 0.014±0.001 0.096±0.001\nMulti-IVON 94.37±0.13 0.179±0.002 0.029±0.001 0.087±0.001\nTable 2: IVON’s predictive uncertainty is better than other\nbaselines for in-domain examples. Multi-IVON is a mixture-\nof-Gaussian ensemble which further improves the perfor-\nmance and is competitive with a deep ensemble.\n4.2. Posterior Averaging for Predictive Uncertainty\nVariational learning naturally allows for improved predic-\ntive uncertainties, because Monte-Carlo samples from the\nlearned posterior can be averaged to estimate the predic-\ntive posterior. Unlike other BDL methods, no postprocess-\ning or model architecture changes are required for this. In\nthe following, we compare IVON to Bayes-by-Backprop\n(BBB), MC Dropout, SWAG and deep ensembles (Laksh-\nminarayanan et al., 2017) in in-domain and out-of-domain\n(OOD) settings. We report common metrics from existing\nbenchmarks (Liang et al., 2018; Snoek et al., 2019). Further\ndetails on the experimental setup can be found in App. C.3.\n4.2.1. IN-DOMAIN COMPARISON\nTo evaluate in-domain uncertainty, we train and evaluate\nResNet-20 models (He et al., 2016a) on the smaller CIFAR-\n10 dataset for a fair comparison, because BBB is difficult to\napply successfully on larger datasets. Results are reported\nFPR@95% ↓ Det. Err. ↓ AUROC ↑ AUPR-In ↑ AUPR-Out ↑\nSVHN, see Fig. 3(a)\nSGD\n20.7±1.6\n18.8±0.9\n86.7±1.0\n81.8±1.4\n91.8±0.7\nBBB\n24.5±0.7\n17.8±0.3\n87.0±0.3\n83.4±0.4\n91.3±0.4\nMC-D\n20.7±1.3\n17.0±0.6\n88.0±0.8\n84.6±0.9\n92.1±0.7\nSWAG\n19.8±2.2\n16.6±1.0\n88.9±1.1\n85.3±1.2\n93.0±0.9\nIVON\n17.4±0.8\n16.6±0.5\n89.2±0.4\n85.2±0.6\n93.4±0.4\nFlowers102, see Fig. 3(b)\nSGD\n22.1±0.5\n20.7±0.4\n86.3±0.3\n92.1±0.2\n75.4±0.4\nBBB\n22.2±0.8\n19.5±0.7\n88.2±0.7\n93.1±0.5\n79.8±0.9\nMC-D\n20.3±0.8\n19.6±1.1\n87.8±0.9\n93.0±0.7\n78.4±1.1\nSWAG\n19.5±0.8\n18.1±0.5\n89.3±0.6\n93.9±0.4\n81.0±0.9\nIVON\n17.8±0.5\n18.1±0.5\n89.0±0.5\n93.8±0.3\n80.2±0.8\nTable 3: IVON improves OOD detection, here for ResNet-\n20/CIFAR-10 evaluated on SVHN and Flowers102.\nin Table 2. Overall, all BDL baselines except for BBB,\nwhich is known to under-perform, have significantly better\nuncertainty metrics than SGD. Among all non-ensemble ap-\nproaches, IVON stands out in both accuracy and uncertainty\nestimation.\nDeep ensembles made up of five models from different\nSGD runs, on the other hand, clearly improve over the non-\nensemble methods. This said, ensembling is a generic tech-\nnique that can be seen as a mixture with equal importance\nand IVON is easily amenable to such posterior forms (Lin\net al., 2019). Therefore, we evaluate a mixture-of-Gaussian\nposterior constructed from five independently-trained IVON\nmodels, referred to as Multi-IVON in Table 2. We find\nthat this can further improve upon deep ensembles. This\nconfirms that good in-domain uncertainty estimates can be\nobtained using variational learning with IVON.\n6\nVariational Learning is Effective for Large Deep Networks\n1\n8\n64\n512\nTest MC Samples\n91%\n92%\n93%\nAccuracy\nmean\n1\n8\n64\n512\nTest MC Samples\n0.2\n0.25\n0.3\nNLL\n1\n2\n4\n8\n16\n32\nTrain MC Samples\n92%\n93%\nAccuracy\n1\n2\n4\n8\n16\n32\nTrain MC Samples\n0.2\n0.25\n0.3\nNLL\nFigure 4: Using more MC samples during inference (top\nrow) or training (bottow row) can improve both accuracy\nand NLL, here plotted for ResNet-20 on CIFAR-10.\n4.2.2. OUT-OF-DOMAIN EXPERIMENTS\nNext, we consider the OOD case by reusing the CIFAR-10\nmodels on data from a different domain. While we would\nexpect the model to be certain for correct in-domain predic-\ntions, it should be uncertain for out-of-domain examples.\nThis would allow for distinguishing the CIFAR-10 data\nfrom OOD samples, for which we use the street view house\nnumber (SVHN) (Netzer et al., 2011) and the 102 Flowers\ndataset (Nilsback & Zisserman, 2008, Flowers102).\nTable 3 shows that IVON is consistently the best at distin-\nguishing OOD examples from SVHN and Flowers102 from\nin-domain CIFAR-10 data. These results are further illus-\ntrated by the predictive entropy plots in Figs. 3(a) and 3(b).\nIn these plots, IVON’s histogram has a similarly high peak\nas SGD for in-domain data (red), but is much more spread\nout than SGD for out-of-domain data (gray). While the other\nBayesian deep learning method’s histograms are also spread\nout for OOD data, they struggle to achieve a high peak for\nin-domain data. Overall, IVON’s histogram has the most\nclear separation between in-domain data and out-of-domain\ndata. As illustrated in Fig. 3(c), IVON’s predictive posterior\nalso gives good in-between uncertainty which has been chal-\nlenging for other variational methods (Foong et al., 2019).\nWe show further distribution shift experiments in App. D.2.\n4.2.3. MC SAMPLES FOR AVERAGING\nHere, we summarize results for using an increasing number\nof MC samples for prediction with ResNet-20 on CIFAR-\n10. We find consistent improvements when using more\nMNLI QNLI QQP RTE SST2 MRPC CoLA STS-B\nRoBERTa (125M params)\nAdamW\n87.7\n92.8\n90.9 80.9 94.8\n85.8\n63.6\n90.6\nIVON@mean\n87.8\n92.6\n90.8 80.6 95.0\n87.3\n63.3\n90.8\nDeBERTAv3 (440M params)\nAdamW\n91.3\n95.7\n93.1 91.0 96.5\n91.0\n74.8\n92.4\nIVON@mean\n91.6\n95.7\n93.0 91.7 96.9\n91.9\n75.1\n92.6\nAdamW†\n91.8\n96.0\n93.0 92.7 96.9\n92.2\n75.3\n93.0\nTable 4: In our experiments, IVON also gives improve-\nments over AdamW when finetuning DeBERTAv3large with\n440M params and RoBERTabase with 125M parameters on\nGLUE. Results on development set. † is state-of-the-art\ntaken from He et al. (2023).\nMC samples both during training and inference, but eventu-\nally improvements saturate and deliver diminishing returns.\nFig. 4 shows that using multiple samples during inference\nalso improves over using the learned mean, especially in\nterms of NLL, but at higher inference cost. Similarly, using\nmultiple samples during training improves both accuracy\nand uncertainty, as highlighted in Fig. 4.\n4.2.4. NEURIPS 2021 COMPETITION\nAn earlier version of IVON won first place in both tracks of\nthe NeurIPS 2021 competition on approximate inference in\nBayesian deep learning (Wilson et al., 2022). The methods\nwere evaluated by their difference in predictions to these of\na ’ground-truth’ posterior computed on hundreds of TPUs.\nThe winning solution used Multi-IVON (see also Table 2),\nthat is, a uniformly weighted mixture-of-Gaussian posterior\nconstructed from independent runs of IVON. We summarize\nthe results of the challenge and the differences of the earlier\nversion to Alg. 1 in App. D.3.\n4.3. Finetuning and Model Merging\nThe variance of IVON’s model posterior provides valuable\ninformation for adaptation, since it shows how far parame-\nters can be varied without a sharp loss increase. The natural-\ngradient descent update of IVON also preconditions the\nmean update with the posterior variance, encouraging adap-\ntation in directions without large loss increase. Furthermore,\nwe expect the variance to be useful for uncertainty-guided\nmodel merging (Daheim et al., 2024), because it can be used\nto scale models when merging. The following experiments\nconfirm this and show improvements for both applications.\n4.3.1. FINETUNING PRETRAINED LANGUAGE MODELS\nWe compare finetuning a large masked-language model De-\nBERTAv3 (He et al., 2023) using AdamW and IVON on\nGLUE (Wang et al., 2018), excluding WNLI following prior\n7\nVariational Learning is Effective for Large Deep Networks\nIMDB Yelp\nRT\nSST2 Amazon Avg. Overhead\nSG\n93.5\n97.0 89.7\n92.8\n96.6\n93.9\n100%\nIVON\n93.6\n96.9 89.8\n92.8\n96.7\n94.0\n0%\nTable 5: IVON’s weight uncertainty can be used for im-\nproved model merging without incurring any overhead, here\nmeasured by the share of one training epoch and compared\nto a squared gradients estimator (SG) that uses an additional\npass over the data after training.\nwork (Devlin et al., 2019). DeBERTAv3 has 440M parame-\nters and we finetune the full model using a publicly available\ncheckpoint that was initially trained with AdamW. Table 4\nshows that IVON can improve upon AdamW in our experi-\nments, both on classification and regression tasks (STS-B),\neven when evaluated at the learned mean. However, we also\ncould not fully replicate the AdamW results from He et al.\n(2023) which show higher scores. For RoBERTa (Liu et al.,\n2019), we also find that IVON can give improvements over\nAdamW on average. Further details on the experiment and\nhyperparameters are shown in App. C.4.\n4.3.2. MERGING MASKED-LANGUAGE MODELS\nWe replicate the experimental set-up from Daheim et al.\n(2024) and merge RoBERTa models finetuned using IVON\non IMDB (Maas et al., 2011), Amazon (Zhang et al., 2015),\nYelp (Zhang et al., 2015), RottenTomatoes (Pang & Lee,\n2005), and SST2 (Socher et al., 2013), according to the\nmethod outlined in App. C.5. We compare using IVON’s\nlearned variance estimate against a squared gradient (SG)\nestimator as scaling matrices for model merging. The SG\nestimator uses P\ni[∇ℓ(ˆyi, xi)]2 after training, where ˆyi is\nsampled from the model distribution. This can be seen as a\nLaplace approximation (Daheim et al., 2024).\nTable 5 shows that both estimates perform similarly. How-\never, our variational method does not have any overhead un-\nlike Laplace which requires an additional pass over training\ndata and therefore incurs significant overhead. We expect\neven better results for IVON when the model is pretrained\nwith it, because the variance is already estimated during\npretraining. We leave this for future work.\n4.4. Sensitivity Analysis and Predicting Generalization\nThe posterior can also be used to estimate a model’s sensi-\ntivity to perturbation in the training data, such as example\nremoval, without expensive retraining. Sensitivity estimates\ncan be used to find influential examples, clean datasets,\nand diagnose model training by estimating generalization\nperformance. While existing works often rely on Hessian\napproximations at a converged model (Koh & Liang, 2017)\nor even on full retraining (Feldman & Zhang, 2020), the\n0\n20\n40\n60\n80 100\nEpochs\n4\n3\n2\n1\nNLL\nTrue loss\nPredicted loss\n(a) IVON\n0\n20\n40\n60\n80 100\nEpochs\n4\n3\n2\n1\nNLL\nTrue loss\nPredicted loss\n(b) AdamW\nFigure 5: IVON predicts test loss much more faithfully\nthan AdamW, where especially at the end of training the\npredicted loss diverges heavily from the test loss. IVON\nalso faithfully predicts the bump in loss around epoch 40.\nvariational posterior can be used to estimate sensitivity al-\nready during training (Nickl et al., 2023), enabling model\ndiagnosis on-the-fly and avoiding costly post-hoc analysis.\nWe discuss two applications for diagnosing models: qualita-\ntive sensitivity analysis of training examples and estimation\nof generalization performance without a held-out validation\nset. We summarize the approach and describe hyperparame-\nters, models and datasets in App. C.6.\n4.4.1. PREDICTION OF MODEL GENERALIZATION\nSensitivity estimates from IVON can be used to approximate\na leave-one-out (LOO) cross-validation criterion without\nretraining. This allows us to train a model with all avail-\nable data and standard loss, while estimating generalization\nperformance during training with the LOO objective. The\nestimate can be helpful, for example, to detect over-fitting\nor for early-stopping without a separate validation set.\nIn Fig. 5 we find that IVON can be used to faithfully pre-\ndict the loss on unseen test data. The LOO objective is\nevaluated using sensitivities calculated from IVON during\ntraining. The heuristic estimate with sensitivities obtained\nfrom AdamW’s squared-gradients performs less well and\ndiverges at the end of training. In Fig. 5(a) we notice that\nIVON can even predict the bump in the test-loss around\nepoch 40. For the plot, we use a similar ImageNet train-\ning set-up as in Table 1 and evaluate the LOO criterion at\nregular intervals. We show further results using different\narchitectures on CIFAR-10 in App. D.4.\n4.4.2. TRAINING DATA SENSITIVITY ANALYSIS\nIVON’s posterior variance is useful to understand sensitiv-\nitiy to data. Possible applications are to clean the dataset\nfrom mislabeled or ambiguous training samples that are\ncharacterized by high sensitivity or for removing redundant,\nlow-sensitivity data to speed up the training.\n8\nVariational Learning is Effective for Large Deep Networks\nEpoch 5\nEpoch 50\nEpoch 100\nLowest\nHighest\nLowest\nHighest\nLowest\nHighest\nFigure 6: Sensitivity can also be used to directly analyze\nthe data samples. At different stages of training, we show\nthe data examples for the “great white shark” class on Ima-\ngeNet with lowest and highest sensitivities, as well as the\nhistogram of data sensitivity. As the training progress, the\nmodel becomes more sensitive to a few “unusual” data ex-\namples. This is especially apparent at the end of training,\nwhere only a few examples have high sensitivity.\nFig. 6 illustrates low- and high-sensitivity images at different\ntraining stages with the same sensitivities that were used\nin Fig. 5 to predict generalization. All samples have small\nsensitivities in the first epochs and, as training progresses,\nthe model becomes highly sensitive to only a small fraction\nof the examples. At the end of training the most sensitive\nexamples are highly unusual, for instance, showing the face\nof a woman rather than a shark.\n5. Discussion and Limitations\nWe show the effectiveness of variational learning for training\nlarge networks. Especially our results on GPT-2 and other\nLLMs are first of their kind and clearly demonstrate the\npotential that variational learning holds. We also discussed\nmany new use cases where we consistently find benefits by\nswitching to a variational approach. We expect our results\nto be useful for future work on showing the effectiveness of\nBayesian learning in general.\nAlthough we borrow practical tricks from deep learning, not\nall of them are equally useful for IVON, for example, we\nfind that IVON does not go well with batch normalization\nlayers (Ioffe & Szegedy, 2015). Future research should\nexplore this limitation and investigate the reasons behind the\neffectiveness of some practical tricks. Using MC samples\nin variational learning increases the computation cost and\nwe believe it is difficult to fix this problem. Deterministic\nversions of the variational objective can be used to fix this,\nfor example, those discussed by M¨ollenhoff & Khan (2023).\nThis is another future direction of research.\nIVON can be easily modified to learn flexible posterior\nforms (Lin et al., 2019). Our Multi-IVON method in this\npaper uses a simple mixture distribution, but we expect\nfurther improvements by using other types of mixtures and\nalso by learning the mixing distribution. We expect this\naspect of IVON to help researchers further investigate the\nbenefits of Bayesian principles to improve deep learning.\nAcknowledgements\nM.E. Khan, R. Yokota, P. Nickl, G.M. Marconi and\nT. M¨ollenhoff were supported by the Bayes duality project,\nJST CREST Grant Number JPMJCR2112. N. Daheim and\nI. Gurevych acknowledge the funding by the German Fed-\neral Ministry of Education and Research and the Hessian\nMinistry of Higher Education, Research, Science and the\nArts within their joint support of the National Research\nCenter for Applied Cybersecurity ATHENE. Y. Shen and\nD. Cremers acknowledge the support by the Munich Center\nfor Machine Learning (MCML) and the ERC Advanced\nGrant SIMULACRON. We thank Happy Buzaaba for first\nexperiments on finetuning transformers with IVON. We also\nthank Keigo Nishida, Falko Helm and Hovhannes Tamoyan\nfor feedback on a draft of this paper.\nReferences\nBlundell, C., Cornebise, J., Kavukcuoglu, K., and Wierstra,\nD. Weight uncertainty in neural networks. In Interna-\ntional Conference on Machine Learning (ICML), 2015.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Spe-\ncia, L. SemEval-2017 task 1: Semantic textual similarity\nmultilingual and crosslingual focused evaluation. In In-\nternational Workshop on Semantic Evaluation (SemEval-\n2017), 2017.\nCho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D.,\nBougares, F., Schwenk, H., and Bengio, Y. Learning\nphrase representations using RNN encoder-decoder for\nstatistical machine translation. In Conference on Empiri-\ncal Methods in Natural Language Processing (EMNLP),\n2014.\nCoker, B., Bruinsma, W. P., Burt, D. R., Pan, W., and Doshi-\nVelez, F. Wide mean-field Bayesian neural networks\nignore the data. In International Conference on Artificial\nIntelligence and Statistics (AISTATS), 2022.\n9\nVariational Learning is Effective for Large Deep Networks\nDaheim, N., M¨ollenhoff, T., Ponti, E. M., Gurevych, I., and\nKhan, M. E. Model merging by uncertainty-based gradi-\nent matching. In International Conference on Learning\nRepresentations (ICLR), 2024.\nDauphin, Y., De Vries, H., and Bengio, Y. Equilibrated adap-\ntive learning rates for non-convex optimization. Advances\nin Neural Information Processing Systems (NeurIPS),\n2015.\nDelaunoy, A. and Louppe, G. Sae: Sequential anchored\nensembles. arXiv:2201.00649, 2021.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. ImageNet: A large-scale hierarchical image database.\nIn IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2009.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. BERT:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Conference of the North Amer-\nican Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1 (Long\nand Short Papers), 2019.\nDolan, W. B. and Brockett, C. Automatically construct-\ning a corpus of sentential paraphrases. In International\nWorkshop on Paraphrasing (IWP2005), 2005.\nFeldman, V. and Zhang, C. What neural networks memorize\nand why: Discovering the long tail via influence esti-\nmation. In Advances in Neural Information Processing\nSystems (NeurIPS), 2020.\nFoong, A., Burt, D., Li, Y., and Turner, R. On the ex-\npressiveness of approximate inference in Bayesian neural\nnetworks. Advances in Neural Information Processing\nSystems, 33, 2020.\nFoong, A. Y., Li, Y., Hern´andez-Lobato, J. M., and Turner,\nR. E. ’in-between’uncertainty in bayesian neural net-\nworks. ICML Workshop on Uncertainty and Robustness\nin Deep Learning, 2019.\nForet, P., Kleiner, A., Mobahi, H., and Neyshabur, B.\nSharpness-aware minimization for efficiently improving\ngeneralization. In International Conference on Learning\nRepresentations (ICLR), 2021.\nFortuin, V., Garriga-Alonso, A., Wenzel, F., R¨atsch, G.,\nTurner, R., van der Wilk, M., and Aitchison, L. Bayesian\nneural network priors revisited. In International Confer-\nence on Learning Representations (ICLR), 2022.\nGal, Y. and Ghahramani, Z. Dropout as a Bayesian approx-\nimation: Representing model uncertainty in deep learn-\ning. In International Conference on Machine Learning\n(ICML), 2016.\nGokaslan, A. and Cohen, V.\nOpenWebText corpus,\n2019.\nURL http://Skylion007.github.io/\nOpenWebTextCorpus.\nGraves, A. Practical variational inference for neural net-\nworks. In Advances in Neural Information Processing\nSystems (NeurIPS), 2011.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition.\nIn IEEE Conference\non Computer Vision and Pattern Recognition (CVPR),\n2016a.\nHe, K., Zhang, X., Ren, S., and Sun, J. Identity mappings\nin deep residual networks. In European Conference on\nComputer Vision (ECCV), 2016b.\nHe, P., Gao, J., and Chen, W. DeBERTav3: Improving de-\nBERTa using ELECTRA-style pre-training with gradient-\ndisentangled embedding sharing. In International Con-\nference on Learning Representations (ICLR), 2023.\nHendrycks, D. and Dietterich, T. G. Benchmarking neural\nnetwork robustness to common corruptions and perturba-\ntions. In International Conference on Learning Represen-\ntations (ICLR), 2019.\nHendrycks, D. and Gimpel, K. Gaussian error linear units\n(GELUs). arXiv preprint arXiv:1606.08415, 2016.\nHern´andez-Lobato, J. M. and Adams, R. Probabilistic back-\npropagation for scalable learning of Bayesian neural net-\nworks. In International Conference on Machine Learning\n(ICML), 2015.\nHoffman, M. D., Blei, D. M., Wang, C., and Paisley, J.\nStochastic variational inference. J. Mach. Learn. Res.\n(JMLR), 14(5), 2013.\nHoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E.,\nCai, T., Rutherford, E., de las Casas, D., Hendricks, L. A.,\nWelbl, J., Clark, A., Hennigan, T., Noland, E., Millican,\nK., van den Driessche, G., Damoc, B., Guy, A., Osindero,\nS., Simonyan, K., Elsen, E., Vinyals, O., Rae, J. W.,\nand Sifre, L. An empirical analysis of compute-optimal\nlarge language model training. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\nHuang, G., Liu, Z., van der Maaten, L., and Weinberger,\nK. Q. Densely connected convolutional networks. In\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2017.\nIoffe, S. and Szegedy, C. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift.\nIn International Conference on Machine Learning, 2015.\n10\nVariational Learning is Effective for Large Deep Networks\nIzmailov, P., Vikram, S., Hoffman, M. D., and Wilson,\nA. G. What are Bayesian neural network posteriors really\nlike? In International Conference on Machine Learning\n(ICML), 2021.\nKhan, M. E. and Lin, W. Conjugate-computation varia-\ntional inference: Converting variational inference in non-\nconjugate models to inferences in conjugate models. In\nInternational Conference on Artificial Intelligence and\nStatistics (AISTATS), 2017.\nKhan, M. E. and Rue, H.\nThe Bayesian learning rule.\narXiv:2107.04562, 2021.\nKhan, M. E., Nielsen, D., Tangkaratt, V., Lin, W., Gal, Y.,\nand Srivastava, A. Fast and scalable Bayesian deep learn-\ning by weight-perturbation in Adam. In International\nConference on Machine Learning (ICML), 2018.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations (ICLR), 2015. arXiv:1412.6980.\nKingma, D. P., Salimans, T., and Welling, M.\nVaria-\ntional dropout and the local reparameterization trick.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2015.\nKoh, P. W. and Liang, P. Understanding black-box predic-\ntions via influence functions. In International Conference\non Machine Learning (ICML), 2017.\nKrizhevsky, A. Learning multiple layers of features from\ntiny images. Technical report, University of Toronto,\n2009.\nLakshminarayanan, B., Pritzel, A., and Blundell, C. Simple\nand scalable predictive uncertainty estimation using deep\nensembles. In Advances in Neural Information Process-\ning Systems (NeurIPS), 2017.\nLe, Y. and Yang, X. S. Tiny imagenet visual recognition\nchallenge. Technical report, Stanford University, 2015.\nLevesque, H., Davis, E., and Morgenstern, L. The winograd\nschema challenge. In International Conference on the\nPrinciples of Knowledge Representation and Reasoning,\n2012.\nLiang, S., Li, Y., and Srikant, R. Enhancing the reliability\nof out-of-distribution image detection in neural networks.\nIn ICLR, 2018.\nLin, W., Khan, M. E., and Schmidt, M.\nFast and sim-\nple natural-gradient variational inference with mixture\nof exponential-family approximations. In International\nConference on Machine Learning (ICML), 2019.\nLin, W., Schmidt, M., and Khan, M. E.\nHandling the\npositive-definite constraint in the Bayesian learning\nrule. In International Conference on Machine Learning\n(ICML), 2020.\nLiu, H., Li, Z., Hall, D., Liang, P., and Ma, T. Sophia: A\nscalable stochastic second-order optimizer for language\nmodel pre-training. arXiv:2305.14342, 2023.\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\nRoBERTa: A robustly optimized BERT pretraining ap-\nproach, 2019.\nLoshchilov, I. and Hutter, F. Decoupled weight decay regu-\nlarization. arXiv:1711.05101, 2017.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y.,\nand Potts, C. Learning word vectors for sentiment analy-\nsis. In Association for Computational Linguistics (ACL),\n2011.\nMacKay, D. J. C.\nA practical Bayesian framework for\nbackpropagation networks. Neural Comput., 4(3):448–\n472, 1992.\nMaddox, W. J., Izmailov, P., Garipov, T., Vetrov, D. P., and\nWilson, A. G. A simple baseline for Bayesian uncertainty\nin deep learning. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2019.\nM¨ollenhoff, T. and Khan, M. E. SAM as an optimal relax-\nation of Bayes. In International Conference on Learning\nRepresentations (ICLR), 2023.\nNetzer, Y., Wang, T., Coates, A., Bissacco, A., Wu, B., and\nNg, A. Y. Reading digits in natural images with unsu-\npervised feature learning. In NIPS Workshop on Deep\nLearning and Unsupervised Feature Learning, 2011.\nNickl, P., Xu, L., Tailor, D., M¨ollenhoff, T., and Khan,\nM. E. The memory perturbation equation: Understand-\ning model’s sensitivity to data. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2023.\nNilsback, M.-E. and Zisserman, A. Automated flower clas-\nsification over a large number of classes. In Indian Con-\nference on Computer Vision, Graphics and Image Pro-\ncessing, 2008.\nNoci, L., Roth, K., Bachmann, G., Nowozin, S., and Hof-\nmann, T.\nDisentangling the roles of curation, data-\naugmentation and the prior in the cold posterior effect.\nIn Advances in Neural Information Processing Systems\n(NeurIPS), 2021.\nOsawa, K., Swaroop, S., Jain, A., Eschenhagen, R., Turner,\nR. E., Yokota, R., and Khan, M. E. Practical deep learning\n11\nVariational Learning is Effective for Large Deep Networks\nwith Bayesian principles. Advances in Neural Informa-\ntion Processing Systems (NeurIPS), 2019.\nPang, B. and Lee, L. Seeing stars: Exploiting class relation-\nships for sentiment categorization with respect to rating\nscales.\nIn Association for Computational Linguistics\n(ACL), 2005.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D.,\nSutskever, I., et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 1(8):9, 2019.\nSato, M.-A. Online model selection based on the variational\nBayes. Neural computation, 13(7):1649–1681, 2001.\nSnoek, J., Ovadia, Y., Fertig, E., Lakshminarayanan, B.,\nNowozin, S., Sculley, D., Dillon, J. V., Ren, J., and Nado,\nZ. Can you trust your model’s uncertainty? evaluating\npredictive uncertainty under dataset shift. In Advances in\nNeural Information Processing Systems (NeurIPS), 2019.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep models for\nsemantic compositionality over a sentiment treebank. In\nConference on Empirical Methods in Natural Language\nProcessing (EMNLP), 2013.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: A simple way to prevent\nneural networks from overfitting. Journal of Machine\nLearning Research, 15(56):1929–1958, 2014.\nTrippe, B. and Turner, R.\nOverpruning in variational\nBayesian neural networks. In Advances in Approximate\nBayesian Inference, 2017.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. Advances in Neural Information\nProcessing Systems (NeurIPS), 2017.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and\nBowman, S. GLUE: A multi-task benchmark and analysis\nplatform for natural language understanding. In EMNLP\nWorkshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, 2018.\nWarstadt, A., Singh, A., and Bowman, S. R.\nNeu-\nral network acceptability judgments.\narXiv preprint\narXiv:1805.12471, 2018.\nWelling, M. and Teh, Y. W. Bayesian learning via stochastic\ngradient Langevin dynamics. In International Conference\non Machine Learning (ICML), 2011.\nWen, Y., Vicol, P., Ba, J., Tran, D., and Grosse, R. B. Flipout:\nEfficient pseudo-independent weight perturbations on\nmini-batches. In International Conference on Learning\nRepresentations (ICLR), 2018.\nWenzel, F., Roth, K., Veeling, B. S., Swiatkowski, J., Tran,\nL., Mandt, S., Snoek, J., Salimans, T., Jenatton, R., and\nNowozin, S. How good is the bayes posterior in deep\nneural networks really? In International Conference on\nMachine Learning (ICML), 2020.\nWilliams, A., Nangia, N., and Bowman, S.\nA broad-\ncoverage challenge corpus for sentence understanding\nthrough inference. In Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Papers),\n2018.\nWilson, A. G., Izmailov, P., Hoffman, M. D., Gal, Y., Li, Y.,\nPradier, M. F., Vikram, S., Foong, A., Lotfi, S., and Far-\nquhar, S. Evaluating approximate inference in Bayesian\ndeep learning. In Proceedings of the NeurIPS 2021 Com-\npetitions and Demonstrations Track, 2022.\nYao, Z., Gholami, A., Shen, S., Mustafa, M., Keutzer, K.,\nand Mahoney, M. W. AdaHessian: an adaptive second or-\nder optimizer for machine learning. In AAAI Conference\non Artificial Intelligence (AAAI), 2021.\nZhang, G., Sun, S., Duvenaud, D., and Grosse, R. Noisy\nnatural gradient as variational inference. In International\nConference on Machine Learning (ICML), 2018.\nZhang, X., Zhao, J., and LeCun, Y. Character-level Convo-\nlutional Networks for Text Classification. In Advances in\nNeural Information Processing Systems (NeurIPS), 2015.\n12\nVariational Learning is Effective for Large Deep Networks\nA. Practical Guideline for Choosing IVON Hyperparameters\nTo facilitate the usage of IVON, we provide here some practical guidelines for choosing hyperparameters and refer to their\nnotations from Algorithm 1.\nLearning rate schedule αt.\nFor ResNets, the initial learning rate of IVON can be set to the same value that works well\nfor SGD, or slightly larger. For Transformers, we have found larger learning rates to work well, such as 0.1 for finetuning\nRoBERTa (Liu et al., 2019), or 0.2 for pretraining GPT-2 (Radford et al., 2019) with 355M parameters. Typical learning rate\nschedules like linear decay or cosine annealing work well for IVON. We have found decaying the learning rate to 0 to work\nbest for pretraining GPT-2, better than decaying it to the initial learning rate divided by 10 as suggested by Hoffmann et al.\n(2022).\nEffective sample size λ.\nSetting this to the size of training dataset (λ = N) in Eq. (1) is a good starting point. This\nrecovers the standard evidence lower bound objective for variational learning. Setting it smaller is equivalent to increased\ntemperature and setting it higher to decreased temperature. In our experiments we mostly set λ ≈ N, except for finetuning\ntransformers on very small datasets where we notice larger λ can improve performance and stabilize the short training. As\nseen from line 8 in Alg. 1, the choice of λ directly influences the posterior variance and too small values may lead to a high\nvariance and unstable training whereas too large values may lead to a collapsed posterior that offers little benefits.\nWeight decay δ.\nFor ResNets, the weight decay of IVON can be set to the same values that work well for SGD or\nAdam. For Transformers, we have found smaller values, such as 10−5, which we use for finetuning, or 10−6, which we\nuse for pretraining, to work well for weight decay. Larger values are feasible when using a quadratic penalty biased to the\ninitialization of the model for finetuning.\nGradient momentum β1.\nSetting β1 = 0.9 tends to work well, similar to SGD or Adam. This plays a similar role as the\ngradient momentum in other optimizers so we expect the good settings to be similar.\nHessian momentum β2.\nThe Hessian momentum needs to be set rather close to one, for example, β2 = 1 − 10−5 worked\nwell in our experiments. The Hessian momentum in theory is given by β2 = 1 − λ−1Nρ, where ρ is the step-size of natural\ngradient descent. If β2 is set too small, for example, 0.999 or 0.9999 the training can sometimes become unstable.\nHessian initialization h0.\nAlong with the effective sample size λ, the Hessian initialization h0 controls the noise at\ninitialization. Typically values between 0.01 and 1 work well in practice but also smaller values like 0.001 have shown good\nresults. Large values of h0 correspond to more concentrated and deterministic initial posterior and can help stabilizing the\ntraining, but this can lead to poorer results. It can be helpful to monitor the statistics of the Hessian vector h during training,\nto see whether a reasonable covariance is being learned.\nBatch size, training epochs.\nTypical batch sizes and training epochs that work well for SGD and AdamW tend to also\nwork well for IVON. For example, our GPT-2 results in Fig. 1(a) use the same batch size and number of epochs for IVON\nand AdamW. This said, we observe that longer training and larger batch size seems to benefit IVON more than SGD,\npossibly because this would further improve the Hessian estimate.\nClip radius ξ.\nWhen training transformers, element-wise gradient clipping can stabilize the training. A clip-radius of\nξ = 10−3 worked well in practice. When picking a smaller clip-radius, one often requires a larger learning rate.\nB. Ablation Studies\nB.1. Computational Efficiency of IVON\nThe computational budget required by IVON is similar to standard deep learning optimizers. To validate its efficiency\nempirically, we measure the run time and peak GPU memory usage for image classification experiments on CIFAR-10 with\nResNet-20 (He et al., 2016a) with an identical setup except for the choice of optimizer. Table 6 shows that IVON has similar\ncomputational costs as SGD and AdamW. However, we find a slight overhead when training larger models like GPT-2 as\nshown in Fig. 1(a), potentially because of the additional sampling step and unoptimized implementation.\n13\nVariational Learning is Effective for Large Deep Networks\nRuntime (hrs)\nMemory (GB)\nAdamW SGD VOGN IVON\nAdamW SGD VOGN IVON\nResNet-20\n0.38\n0.38\n0.68\n0.38\n1.7\n1.7\n2.0\n1.7\nGPT-2 (125M)\n15.0\n-\n-\n18.5\n21.8\n-\n-\n23.2\nGPT-2 (355M)\n37.5\n-\n-\n44.7\n23.7\n-\n-\n27.7\nTable 6: Runtime and memory for CIFAR-10 classification results with ResNet-20 and pretraining GPT-2 on OpenWebText.\nIVON has a small overhead for larger models which might be due to the additional weight sampling and a not fully optimized\nimplementation.\nAcc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nMem ↓\nSG\n88.81±0.31\n0.464±0.020\n0.070±0.004\n0.180±0.006\n363MB\nGGN\n92.37±0.23\n0.226±0.005\n0.008±0.001\n0.111±0.003\n645MB\nReparam.\n92.64±0.13\n0.219±0.005\n0.009±0.002\n0.107±0.002\n363MB\nTable 7: IVON’s reparameterization-trick-based Hessian estimator has better accuracy and uncertainty than other Hessian\nestimators at low computational cost, here for ResNet-20 on CIFAR-10.\nB.2. Comparison of Hessian estimators\nIVON’s efficiency is enabled by estimating gh with the reparameterization-trick-based estimator in Eq. 3. Here, we\ncompare this estimator to the two squared-gradient estimators discussed in the previous section: 1) the Squared Gradient\n(SG) estimator which uses the square of mini-batch gradients bh ← bg2 used in Vprop and Vadam (Khan et al., 2018);\n2) the Gauss-Newton (GN) estimator which uses per-sample squared gradients, bh ←\n1\n|B|\nP\ni∈B\n\u0002\n∇ℓi(θ + σϵ)\n\u00032 used in\nVOGN (Osawa et al., 2019). One drawback of the GN estimator is that per-example gradients require significant overhead,\nsince the backpropagation process of typical deep learning frameworks only computes an averaged mini-batch gradient bg.\nTable 7 shows results for training ResNet-20 on CIFAR-10 with these estimators. We observe that the reparameterization\nestimator provides best performance. The squared gradient estimator is similarly efficient but underperforms, whereas\nGauss-Newton incurs significant overhead in GPU memory and time usage without large benefits in test performance.\nC. Experimental Details\nC.1. Pretraining GPT-2 Models on OpenWebText\nWe pretrain GPT-2 models (Radford et al., 2019) on OpenWebText (Gokaslan & Cohen, 2019) for multiple epochs and\naround 49.2B tokens in total using a batch size of 480 which is achieved by gradient accumulation. We train on 8 NVIDIA\nA100 GPUs with 40GB GPU memory each for up to three days. We use 2,000 warmup steps, 100,000 training steps in total,\nand evaluate every 1,000 steps on a held-out set. Each validation step is shown in Fig. 1(a). The learning rate is decayed\nto 0, which we have found to work better than 1/10-times the initial learning rate for both AdamW and IVON. This is\nrecommended in prior work (Hoffmann et al., 2022). For IVON, we use an initial learning rate of 0.3 for the 125M parameter\ncheckpoint, 0.2 for the 355M parameter checkpoint, and 0.15 for the 773M parameter checkpoint. Note, that we do not\nrescale by h0 and δ in this case, because element-wise clipping is used. We use β1 = 0.9, β2 = 1 − 10−5, h0 = 0.001 and\na weight decay factor of 10−6, as well as element-wise clipping of 10−3. These hyperparameters were found by grid search\non a smaller model and it is possible that better hyperparameter configurations exist. We train with a single MC sample.\nFor training GPT-2 with AdamW, we use an initial learning rate of 6 · 10−4, β1 = 0.9, β2 = 0.95 and a weight decay of\n0.1. This follows the hyerparameters used in prior works (Liu et al., 2023). We follow the implementation in https:\n//github.com/karpathy/nanoGPT/, which uses GeLU activations (Hendrycks & Gimpel, 2016) and does not use\ndropout (Srivastava et al., 2014) and biases during pretraining.\n14\nVariational Learning is Effective for Large Deep Networks\nAcc. ↑\nTop-5 Acc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nAdamW\n90.04±0.27\n99.62±0.03\n0.589±0.018\n0.074±0.002\n0.170±0.004\nAdaHessian\n91.46±0.06\n99.71±0.02\n0.477±0.018\n0.061±0.001\n0.144±0.001\nSGD\n91.86±0.14\n99.70±0.08\n0.288±0.015\n0.040±0.004\n0.126±0.003\nIVON@mean 92.53±0.04\n99.77±0.05\n0.256±0.005\n0.034±0.001\n0.115±0.001\nResNet-20\n(272k params)\nIVON\n92.71±0.07\n99.78±0.03\n0.219±0.002 0.008±0.001 0.108±0.001\nAdamW\n92.41±0.26\n99.72±0.04\n0.594±0.022\n0.062±0.002\n0.135±0.005\nAdaHessian\n92.95±0.87\n99.72±0.14\n0.514±0.028\n0.056±0.006\n0.124±0.014\nSGD\n92.54±0.30\n99.62±0.04\n0.328±0.008\n0.050±0.003\n0.123±0.003\nIVON@mean 93.31±0.31\n99.74±0.03\n0.282±0.014\n0.042±0.003\n0.110±0.004\nDenseNet-121\n(1M params)\nIVON\n93.53±0.26\n99.78±0.04\n0.200±0.007 0.009±0.001 0.096±0.003\nAdamW\n92.39±0.27\n99.69±0.05\n0.653±0.024\n0.064±0.003\n0.137±0.005\nAdaHessian\n93.76±0.25\n99.78±0.03\n0.431±0.021\n0.049±0.002\n0.109±0.004\nSGD\n93.70±0.15\n99.66±0.08\n0.298±0.010\n0.045±0.001\n0.107±0.002\nIVON@mean 93.99±0.08\n99.80±0.03\n0.259±0.008\n0.042±0.001\n0.100±0.002\nPreResNet-110\n(deep, 4M params)\nIVON\n94.02±0.14\n99.84±0.03\n0.180±0.003 0.010±0.001 0.087±0.001\nAdamW\n92.40±0.32\n99.69±0.05\n0.676±0.006\n0.064±0.003\n0.137±0.005\nAdaHessian\n88.66±1.51\n99.38±0.13\n0.569±0.037\n0.081±0.008\n0.190±0.023\nSGD\n94.03±0.14\n99.72±0.03\n0.282±0.009\n0.043±0.002\n0.101±0.003\nIVON@mean 94.17±0.08\n99.78±0.04\n0.305±0.007\n0.045±0.001\n0.102±0.002\nResNet-18\n(wide, 11M params)\nIVON\n94.32±0.13\n99.84±0.03\n0.175±0.002 0.010±0.001 0.084±0.001\nTable 8: IVON results on CIFAR-10 compared with various baseline optimizers using convolutional networks with different\nwidths and depths. IVON@mean denotes point estimate results evaluated at the mean of IVON posterior.\nAcc. ↑\nTop-5 Acc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nAdamW\n60.76±0.47\n86.81±0.48\n1.931±0.044\n0.202±0.004\n0.580±0.008\nAdaHessian\n64.19±0.28\n88.68±0.39\n1.612±0.033\n0.167±0.007\n0.521±0.004\nSGD\n67.23±0.35\n90.75±0.11\n1.173±0.021\n0.059±0.008\n0.441±0.005\nIVON@mean 67.87±0.55\n90.95±0.10\n1.168±0.012\n0.069±0.007\n0.438±0.005\nResNet-20\n(272k params)\nIVON\n68.28±0.50\n91.27±0.05\n1.113±0.010 0.018±0.003 0.425±0.005\nAdamW\n65.47±0.93\n88.74±0.80\n2.967±0.104\n0.264±0.007\n0.587±0.015\nAdaHessian\n71.02±0.57\n92.00±0.17\n2.379±0.038\n0.222±0.005\n0.494±0.010\nSGD\n70.74±0.49\n91.82±0.10\n1.230±0.012\n0.131±0.004\n0.427±0.006\nIVON@mean 72.67±0.43\n92.86±0.14\n1.118±0.017\n0.119±0.002\n0.397±0.005\nDenseNet-121\n(1M params)\nIVON\n73.68±0.37\n93.31±0.15\n0.940±0.012 0.022±0.002 0.361±0.004\nAdamW\n65.88±0.84\n88.34±0.56\n2.893±0.088\n0.258±0.006\n0.578±0.014\nAdaHessian\n72.43±0.36\n91.92±0.38\n1.844±0.044\n0.194±0.004\n0.452±0.008\nSGD\n74.19±0.11\n92.41±0.14\n1.204±0.012\n0.137±0.002\n0.393±0.004\nIVON@mean 75.23±0.23\n93.45±0.16\n1.149±0.010\n0.136±0.002\n0.380±0.003\nPreResNet-110\n(deep, 4M params)\nIVON\n75.81±0.18\n93.93±0.19\n0.884±0.007 0.030±0.003 0.336±0.001\nAdamW\n64.12±0.43\n86.85±0.51\n3.357±0.071\n0.278±0.005\n0.615±0.008\nAdaHessian\n56.42±6.22\n80.56±4.81\n2.503±0.261\n0.258±0.014\n0.666±0.071\nSGD\n74.46±0.17\n92.66±0.06\n1.083±0.007\n0.113±0.001\n0.376±0.001\nIVON@mean 74.51±0.24\n92.74±0.19\n1.284±0.013\n0.152±0.003\n0.399±0.002\nResNet-18\n(wide, 11M params)\nIVON\n75.14±0.34\n93.30±0.19\n0.912±0.009 0.021±0.003 0.344±0.003\nTable 9: IVON results on CIFAR-100 compared with various baseline optimizers using convolutional networks with different\nwidths and depths. IVON @mean denotes point estimate results evaluated at the mean of IVON posterior.\n15\nVariational Learning is Effective for Large Deep Networks\nAcc. ↑\nTop-5 Acc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nAdamW\n46.62±0.78\n72.71±0.75\n2.387±0.042\n0.121±0.004\n0.692±0.009\nAdaHessian\n50.06±0.53\n76.09±0.29\n2.120±0.016\n0.084±0.007\n0.642±0.004\nSGD\n51.08±0.22\n77.17±0.25\n1.989±0.007 0.020±0.003 0.622±0.002\nIVON@mean 50.71±0.38\n76.82±0.41\n2.014±0.017 0.020±0.006 0.629±0.005\nResNet-20\n(272k params)\nIVON\n50.85±0.42\n76.92±0.37\n2.017±0.016\n0.060±0.005\n0.632±0.004\nAdamW\n50.01±0.28\n74.76±0.32\n5.515±0.112\n0.385±0.003\n0.851±0.004\nAdaHessian\n43.66±10.76\n69.86±9.69\n3.142±0.320\n0.189±0.150\n0.772±0.044\nSGD\n56.57±1.00\n80.46±0.81\n1.913±0.056\n0.126±0.008\n0.585±0.012\nIVON@mean 58.47±0.10\n82.58±0.23\n1.675±0.008\n0.046±0.004 0.542±0.003\nDenseNet-121\n(1M params)\nIVON\n58.90±0.34\n82.69±0.35\n1.644±0.012 0.035±0.002 0.536±0.003\nAdamW\n50.65±0.0∗\n74.94±0.0∗\n4.487±0.0∗\n0.357±0.0∗\n0.812±0.0∗\nAdaHessian\n55.03±0.53\n78.49±0.34\n2.971±0.064\n0.272±0.005\n0.690±0.008\nSGD\n59.39±0.50\n81.34±0.30\n2.040±0.040\n0.176±0.006\n0.577±0.007\nIVON@mean 60.85±0.39\n83.89±0.14\n1.584±0.009\n0.053±0.002 0.514±0.003\nPreResNet-110\n(deep, 4M params)\nIVON\n61.25±0.48\n84.13±0.17\n1.550±0.009 0.049±0.002 0.511±0.003\nAdamW\n47.33±0.90\n71.54±0.95\n6.823±0.235\n0.421±0.008\n0.913±0.018\nAdaHessian\n51.80±0.29\n75.01±0.10\n3.416±0.028\n0.304±0.002\n0.748±0.005\nSGD\n61.39±0.18\n82.30±0.22\n1.811±0.010\n0.138±0.002\n0.536±0.002\nIVON@mean 62.41±0.15\n83.77±0.18\n1.776±0.018\n0.150±0.005\n0.532±0.002\nResNet-18\n(wide, 11M params)\nIVON\n62.68±0.16\n84.12±0.24\n1.528±0.010 0.019±0.004 0.491±0.001\nTable 10: IVON results on TinyImageNet compared with various baseline optimizers using convolutional networks with\ndifferent widths and depths. IVON @mean denotes point estimate results evaluated at the mean of IVON posterior. (∗)\nAdamW only converged for one of the five random seeds for PreResNet-110.\nC.2. Training with IVON for Image Classification\nWe train a ResNet-50 (≈ 25.6 million parameters) (He et al., 2016a) with filter response normalization on the ImageNet\ndataset (≈ 1.2 million examples with 1000 classes) (Deng et al., 2009). Training for 200 epochs takes around 30 hours\non 8 A100 GPUs for all methods. Our distributed implementation of IVON uses different random perturbations on each\naccelerator. IVON’s initial learning rate is 2.5, we set β1 = 0.9, β2 = 1 − 5 · 10−6, δ = 5 · 10−5, h0 = 0.05 and\nλ = N = 1281167. No clipping is used and we train with a single MC sample. SGD uses a learning rate of 0.5 with\nsame momentum β1 = 0.9 and weight-decay δ = 5 · 10−5. AdamW uses β1 = 0.9, β2 = 0.999, learning rate 0.001 and\nweight-decay 0.1. All methods anneal the learning rate to zero with a cosine learning rate schedule after a linear warmup\nphase over 5 epochs.\nHere we also include additional image classification results using also deeper DenseNet-121 (Huang et al., 2017) and\nResNet-20 in addition to ResNet-18 and PreResNet-110 (He et al., 2016b) on CIFAR-10 and the previously reported\nCIFAR-100 (Krizhevsky, 2009) and TinyImageNet (Le & Yang, 2015). The results are summarized in Tables 8, 9 and 10.\nWe also compare to AdaHessian (Yao et al., 2021). We find that IVON improves over other optimizers in terms of both\naccuracy and uncertainty, across all datasets and all metrics. Finally, IVON does not overfit on smaller datasets.\nFor the experiments on CIFAR and TinyImagenet in Tables 1 and 8 to 10, the hyperparameters of all methods were tuned\nonly for the ResNet-20 on CIFAR-10, and kept fixed across the other models and datasets. For SGD the learning rate α = 0.1\nwas the largest stable learning rate across all models and datasets and gave the best results. AdaHessian uses α = 0.05. It\nwas not stable across all datasets when using the same learning rate as SGD as recommended by Yao et al. (2021). AdamW\nuses learning rate α = 0.002, except for the PreResNet-110 on TinyImageNet, where we reran with α = 0.0005 to get it to\nconverge. We set β2 = 0.999 in AdamW. IVON uses α = 0.2, β2 = 1 − 10−5, λ = N and h0 = 0.5. All methods use\ngradient momentum β1 = 0.9. We ran all optimizers for 200 epochs with batch-size 50. The learning rate was warmed up\nfor 5 epochs using a linear schedule, and then decayed using a cosine learning rate annealing. The weight-decay is set to\nδ = 0.0002 for all algorithms, datasets and models.\n16\nVariational Learning is Effective for Large Deep Networks\nC.3. In-domain and OOD Comparison to Bayesian Deep Learning Methods\nWe train all ResNet-20 models with 200 epochs and batch size 50. Weight decay is set to 0.0002. Apart from SWAG, which\nrequires custom scheduling, all other methods use 5 warm-up epochs followed by a cosine annealing learning rate schedule\nthat decays to zero. We do 5 runs with different random seeds and report the average results and their standard deviations in\nthe tables.\nFor the uncertainty estimation metrics used in in-domain and distributional shift experiments, we follow the setup of Snoek\net al. (2019) and report three metrics: negative log-likelihood (NLL), expected calibration error (ECE), and Brier score.\nFor the OOD experiments we used the same metrics as Liang et al. (2018), i.e. False Positive Rate (FPR), the share of\nmisclassified OOD samples, at 95% TPR, detection error, which measures the probability of misclassifications for 95%\nTPR, Area Under the Receiver Operating Characteristic curve (AUROC), AUPR-in, and AUPR-out. Here, AUPR stands for\nArea under the Precision-Recall for the in-domain data (AUPR-in) or OOD data (AUPR-out), respectively.\nThe specific training hyperparameters for each method are:\n• SGD and IVON use the same setting as in Section C.2, except that SGD also uses learning rate 0.2 which is stable for\nResNet-20;\n• BBB uses learning rate 0.002. We set the same initial posterior as IVON and train BBB without using a cold posterior;\n• MC dropout uses learning rate 0.2 and a fixed dropout rate of 0.05;\n• For SWAG, we first do normal training with cosine annealing from lr 0.05 to 0.01 over 160 epochs, then do 40 SWAG\nepochs with constant learning rate 0.01 and maintain a rank 20 approximation of the SWAG posterior as is done\nin (Maddox et al., 2019).\nWe use 64 posterior samples for IVON, BBB, and SWAG. For MC dropout, we only draw 32 samples for all experiments as\nwe observe no improvement when drawing 64 samples.\nC.4. Finetuning on GLUE\nMNLI-m QNLI QQP RTE SST2\nMRPC\nCoLA\nSTS-B\nMetric\nAcc.\nAcc.\nAcc. Acc. Acc. Acc / F1 Spearman\nMCC\n#Train\n393k\n105k\n364k 2.5k\n67k\n3.7k\n8.5k\n7k\n#Validation\n9.8k\n5.5k\n40.4k 277\n872\n408\n1k\n1.5k\nTable 11: Dataset sizes of individual GLUE tasks used in this paper and the used evaluation metrics.\nGLUE (Wang et al., 2018) is a multi-task benchmark consisting of in total 9 diverse tasks which capture classification\nand regression problems. We use all tasks but WNLI (Levesque et al., 2012) following previous work (Devlin et al.,\n2019). Namely, we use: CoLA (Warstadt et al., 2018), MNLI (Williams et al., 2018), MRPC (Dolan & Brockett, 2005),\nQNLI (Wang et al., 2018), QQP, RTE, SST2 (Socher et al., 2013), and STS-B (Cer et al., 2017).\nFor IVON, we use the same hyperparameters for the two models used in our experiments: RoBERTa and DeBERTav3 shown\nin Sec. 4.3.1. We use an initial learning rate of 0.1 or 0.2 which is decayed to 0.0 using cosine decay. We set β1 = 0.9,\nβ2 = 1 − 10−5, h0 = 1.0, a weight decay factor of 10−5, and also use element-wise clipping of 10−3. Furthermore, we use\n500 warmup steps.\nFor RoBERTa with AdamW, we use the hyperparameters reported in (Liu et al., 2019, Table 10). Namely, we sweep learning\nrates over {10−5, 2 · 10−5, 3 · 10−5}. We use a weight decay of 0.1, β1 = 0.9, and β2 = 0.98.\nFor DeBERTAv3 with AdamW we use the hyperparameters as reported in (He et al., 2023, Table 11) but were unable\nto sweep all possible combinations that are listed due to the high computational demand. Therefore, we fix the number\nof warmup steps to 500 and the batch size to 32. Also, we do not use last layer dropout. We sweep learning rates over\n{5 · 10−6, 8 · 10−6, 9 · 10−6, 10−5}, use a weight decay of 0.1, β1 = 0.9, and β2 = 0.999.\nWe evaluate after each epoch and train for up to 10 epochs on every dataset but MRPC, where we allow 15 epochs. The\nbatch size is always set to 32 for both AdamW and IVON.\n17\nVariational Learning is Effective for Large Deep Networks\nC.5. Improved Model Merging with IVON\nWe use the uncertainty-guided model merging method introduced by Daheim et al. (2024). Given a pretrained model with\nparameters θ0 and T finetuned models θ1, . . . , θT , trained each by optimizing ¯ℓt(θt) on Dt initialized from θ0, the models\nare merged into a single model θ1:T by using the following equation:\nθ1:T = θ0 +\nT\nX\nt=1\n˜\nHt(θt − θ0).\nHere, ˜\nHt = (PT\nτ=0 ∇2¯ℓτ(θτ))−1(∇2¯ℓ0(θ0) + ∇2¯ℓt(θt)) is a ratio of Hessians that determines model scales and weighs\nthe contribution of each model to the merged model. A Hessian approximation is already calculated by IVON during\noptimization, because h (cf. Line 6 in Alg. 1) already keeps a running average of diagonal Hessians during the optimization.\nTherefore, we can use the approximation ∇2¯ℓτ(θτ) ≈ diag(hτ) which also reduces computations to elementwise ones\nand avoids large matrix inversions. For all model-merging experiments presented in the main paper we use the same\nhyperparameters that were used to finetune RoBERTa on GLUE as in App. C.4.\nC.6. Predicting Generalization and Understanding Models’ Sensitivity to Data\nWe use the memory-perturbation equation (MPE) framework from Nickl et al. (2023) for sensitivity analysis using IVON’s\nlearned posterior. As shown in that paper, one can approximate the change in model output f i when removing data example i\nby f i(θ\\i\nt ) − f i(θt) ≈ Viteit. Viteit is called the sensitivity of training example i at iteration t and it is composed of\nprediction variance Vit and prediction error eit. f i(θ\\i\nt ) denotes the model output for the i-th example at iteration t, when\nthe example is removed from the training set and the model is retrained without it.\nIn Sec. 4.4.1 in the main text, sensitivities Viteit are used to approximate a leave-one-out (LOO) cross-validation loss to\npredict generalization. It is given as LOO(θt) = PN\ni=1 ℓ(f i(θ\\i\nt )) ≈ PN\ni=1 ℓ(f i(θt) + Viteit). In Sec. 4.4.2 we analyze\nthe data according to a scalar sensitivity score, which is directly computed from the sensitivity Viteit via its ℓ1-norm.\nIn Sec. 4.4.1 and 4.4.2, the sensitivies Viteit are computed as follows. A matrix of prediction variances is given by\nVit = ∇f i(θt)⊤diag(σ2\nt)∇f i(θt), where ∇f i(θt) ∈ RP ×C is the Jacobian matrix of the neural-network with respect to\nits parameters. P is the number of parameters and C is the number of classes. For IVON we use the posterior variance\nσ2\nt = 1/λ(ht + δ). In the case of SGD and AdamW, we construct σ2\nt in ad-hoc ways. For SGD we use σ2\nt = 1/N(1 + δ).\nFor AdamW we use σ2\nt = 1/N(√ht + δ), where ht is the second moment vector that maintains a running-average of\nsquared gradients. We compute the residuals eit ∈ RC as eit = σ(f i(θt)) − yi, where σ(·) is the softmax function. For\nIVON, θ is set to the posterior mean m.\nFor all data sensitivity experiments in the main paper we used the following hyperparameters to train a ResNet-50 on\nImageNet for 100 epochs. IVON uses an initial learning rate of 3, β1 = 0.9, β2 = 1 − 10−6, h0 = 0.008, λ = N and a\nweight decay of δ = 5 · 10−5. h0 was selected on a grid of [0.008, 0.01, 0.05] to achieve a faithful estimate of generalization\nperformance while keeping a competitive test accuracy. AdamW uses a learning rate of 0.001, β1 = 0.9, β2 = 0.999 and\nweight decay 0.1. Both methods use 5 warmup epochs after which the learning rate is decayed to 0 using cosine decay. The\nmodel trained with IVON has an accuracy of 75%, whereas the AdamW model has 74.7% accuracy.\nD. Additional Results\nD.1. IVON with Recurrent Neural Networks\nWe train a simple model based on Gated Recurrent Units (Cho et al., 2014) on three text classification datasets (CoLA,\nIMDB and AG News). The model consists of an embedding layer, two GRUs and a fully connected layer, for a total of 2\nmillion parameters. We train the same model with SGD, AdamW and IVON. IVON results are evaluated both at the mean\nand at a Monte-Carlo approximation of the posterior using 64 samples. Results are reported in Table 12. IVON improves\nboth accuracy and uncertainty compared to the baselines. The chosen model can easily overfit the presented datasets,\nachieving close to 100% accuracy on the training set. Therefore, extra care is required when choosing the hyperparameters\nfor AdamW and, especially, SGD. However, we find it easier to tune IVON for satisfactory results both in terms of accuracy\nand uncertainty.\n18\nVariational Learning is Effective for Large Deep Networks\nAcc. ↑\nNLL ↓\nECE ↓\nBrier ↓\nAUROC ↑\nAdamW\n64.35±0.27\n0.666±0.026\n0.322±0.026\n0.658±0.048 0.579±0.019\nSGD\n67.65±0.92 0.631±0.011\n0.060±0.006\n0.448±0.006\n0.548±0.027\nIVON@mean 68.54±0.00\n0.623±0.03\n0.030±0.005 0.432±0.003 0.509±0.041\nCoLA\nIVON\n68.54±0.00\n0.623±0.03\n0.029±0.005 0.432±0.003 0.510±0.041\nAdamW\n84.66±0.46\n1.929±0.344\n0.136±0.007\n0.285±0.011\n0.725±0.01\nSGD\n85.06±0.38\n0.468±0.02\n0.065±0.009 0.233±0.031\n0.764±0.032\nIVON@mean 89.55±0.20\n0.428±0.02\n0.061±0.003 0.251±0.023 0.811±0.025\nIMDB\nIVON\n87.73±0.96\n0.568±0.119 0.065±0.010 0.199±0.018 0.751±0.031\nAdamW\n90.65±0.32\n0.985±0.046\n0.408±0.03\n0.171±0.006\n0.826±0.006\nSGD\n89.57±0.40\n0.386±0.009\n0.055±0.004\n0.167±0.005\n0.845±0.004\nIVON@mean 92.43±0.01 0.233±0.003 0.017±0.002 0.118±0.001 0.871±0.003\nAG News\nIVON\n92.46±0.01 0.231±0.002 0.014±0.003 0.117±0.001 0.871±0.002\nTable 12: IVON results on NLP classification datasets compared with to SGD and AdamW. IVON@mean denotes point\nestimate results evaluated at the mean of IVON posterior.\nD.2. Robustness to Distribution Shift\nHaving trained and evaluated various models on CIFAR-10 in the in-domain scenario, here we conduct distributional shift\nexperiments, where we use the previously trained networks to directly classify CIFAR-10 test set images corrupted with\nartificial perturbations. For this we use the CIFAR-10-C (Hendrycks & Dietterich, 2019) dataset which collects a range\nof common image distortions and artifacts, each with 5 severity levels. The results are grouped by severity level and\nsummarized in Fig. 7 on the next page.\nIn general, the performance of all models decrease with increasing severity, as the classification task is getting harder.\nWe observe that IVON keeps the best performance for low severity levels. For high severity levels, IVON is notably\noutperformed by SWAG. Despite this, IVON in general is still comparable to BDL baselines for high severity cases. And as\nan optimizer, it remains a better choice over the standard SGD training.\nMethod\nCIFAR-10\nMedMNIST\nUCI\nAgree ↑\nTVD ↓\nAgree ↑\nTVD ↓\nW2 ↓\nMulti-IVON\n78.7%\n0.198\n88.4%\n0.099\n0.094\nMulti-SWAG\n77.8%\n0.219\n89.0%\n0.098\n0.166\nSAE\n77.3%\n0.210\n87.5%\n0.107\n0.116\nTable 13: An earlier version of IVON won the NeurIPS 2021 competition on approximate inference in Bayesian deep\nlearning (Wilson et al., 2022). The second best method used a combination of SWAG and SGLD. Third place was a\nSequential Anchored Ensemble (SAE).\nD.3. NeurIPS 2021 Approximate Inference Competition\nAn earlier version of IVON won the first place2 in both the light and extended track of the NeurIPS 2021 Competition\non Approximate Inference in Bayesian Deep Learning (Wilson et al., 2022). The earlier version included an additional\nheuristic damping to bh in line 3 of Alg. 1 and the weight-decay was added in line 4 rather than in lines 5 and 6. We found\nthe damping term to be unneccesary when using a proper Hessian initialization h0 and momentum β2 and therefore removed\nit, making IVON easier to tune. The three highest scoring submissions to the competition are summarized in Table 13.\nFirst place is Multi-IVON (using the earlier version of IVON), which is a mixture-of-Gaussian ensemble (with uniform\nweights) as described in the experiments section on uncertainty estimation in the main paper. The second place solution\n(Multi-SWAG) uses multiple runs of SWAG to construct a mixture-of-Gaussian approximation (Izmailov et al., 2021)\n2For the official results, see the following link.\n19\nVariational Learning is Effective for Large Deep Networks\n0\n1\n2\n3\n4\n5\nSeverity\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAccuracy\nMAP\nBBB\nMC Drop.\nSWAG\nIVON\n0\n1\n2\n3\n4\n5\nSeverity\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nNLL\nMAP\nBBB\nMC Drop.\nSWAG\nIVON\n0\n1\n2\n3\n4\n5\nSeverity\n0.0\n0.1\n0.2\n0.3\n0.4\nECE\nMAP\nBBB\nMC Drop.\nSWAG\nIVON\n0\n1\n2\n3\n4\n5\nSeverity\n0.2\n0.4\n0.6\n0.8\n1.0\nBrier\nMAP\nBBB\nMC Drop.\nSWAG\nIVON\nFigure 7: Distributional shift results on CIFAR-10-C with various degree of severities. Severity 0 corresponds to the\nin-domain case.\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\nNLL\nTest NLL\nLOO\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\nNLL\nTest NLL\nLOO\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\n0\n50\n100\n150\n200\nEpochs\n0.0\n0.8\n1.6\n2.4\nFigure 8: We predict test NLL using LOO estimation during training of two models on CIFAR10. From top to bottom the\nmodels are: ResNet–18 and PreResNet–110. IVON (first column) allows us to faithfully predict generalization, while the\nheuristic LOO estimates with AdamW (second column) and SGD (third column) work less well.\n20\nVariational Learning is Effective for Large Deep Networks\nwith SGLD (Welling & Teh, 2011) as a base optimizer. Third place was obtained by a deep ensembling method called\nsequential anchored ensembles (SAE) (Delaunoy & Louppe, 2021). In Table 13, Agree denotes predictive agreement with\na ground-truth Bayesian posterior obtained by running Hamiltonian Monte-Carlo method on hundreds of TPUs. TVD\ndenotes the total variation distance and W2 the Wasserstein-2 distance between this ground-truth predictive posterior and the\napproximate posterior. We refer to Wilson et al. (2022) for more details.\nD.4. Predicting Generalization\nIn Fig. 8, we conduct additional experiments with ResNet-18 and PreResNet-10 on the CIFAR10 dataset. We estimate\ngeneralization performance during training using the LOO criterion described in App. C.6. The accuracy of IVON is similar\nto the SGD baseline. IVON however results in a more faithful estimate of the generalization performance in comparison to\nAdamW and SGD. We evaluate the sensitivity of the models to data perturbation as described in App. C.6 with the difference\nthat we compute the sensitivities as eivi, where vi are the diagonal elements of Vi and viei is an element-wise product.\nFor training the models, we use the same hyperparameters as in the image classification experiments. The exception is the\nHessian initaliziation h0. We do a grid search over the values [0.01, 0.05, 0.1, 0.5]. We select the value that results in a\nfaithful estimate of the generalization performance while keeping a competitive test accuracy. The Hessian initialization\nis set to h0 = 0.1 for both models. The test accuracies are 93.68% for ResNet–18 and 93.52% for PreResNet–110 with\npredictions at the mean of the variational posterior.\n21\n"
}