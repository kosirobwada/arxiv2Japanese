{
    "optim": "1\nGeometric Deep Learning for Computer-Aided\nDesign: A Survey\nNegar Heidari and Alexandros Iosifidis\nDepartment of Electrical and Computer Engineering, Aarhus University, Denmark\nEmails: {negar.heidari, ai}@ece.au.dk\nAbstract—Geometric Deep Learning techniques have become\na transformative force in the field of Computer-Aided Design\n(CAD), and have the potential to revolutionize how designers\nand engineers approach and enhance the design process. By\nharnessing the power of machine learning-based methods, CAD\ndesigners can optimize their workflows, save time and effort\nwhile making better informed decisions, and create designs that\nare both innovative and practical. The ability to process the\nCAD designs represented by geometric data and to analyze\ntheir encoded features enables the identification of similarities\namong diverse CAD models, the proposition of alternative designs\nand enhancements, and even the generation of novel design\nalternatives. This survey offers a comprehensive overview of\nlearning-based methods in computer-aided design across various\ncategories, including similarity analysis and retrieval, 2D and 3D\nCAD model synthesis, and CAD generation from point clouds.\nAdditionally, it provides a complete list of benchmark datasets\nand their characteristics, along with open-source codes that have\npropelled research in this domain. The final discussion delves\ninto the challenges prevalent in this field, followed by potential\nfuture research directions in this rapidly evolving field.\nIndex Terms—geometric deep learning, machine learning,\ngraph neural networks, computer aided design, automated CAD\ndesign, 2D-3D shape modeling\nI. INTRODUCTION\nDeep Learning has revolutionized data processing across\ndifferent domains spanning from grid-structured data in Eu-\nclidean spaces, such as image, video, text, to graph-structured\ndata with complex relationships. Geometric Deep Learning\n(GDL) encompasses a wide array of neural network architec-\ntures ranging from Convolutional Neural Networks (CNNs),\nGraph Neural Networks (GNNs), Recurrent Neural Networks\n(RNNs), to Transformer Networks, all of which encode a\ngeometric understanding of the data such as symmetry and\ninvariance as an inductive bias in their learning process [1].\nOver the past decade, GDL methods, including CNNs, GNNs,\nRNNs, and Transformer Networks, have made remarkable\nstrides in diverse tasks coming from different applications,\nincluding computer vision, natural language processing, com-\nputer graphics, and bioinformatics. However, the application of\nGDL methods on complex parametric Computer-Aided Design\n(CAD) data is rarely studied. Boundary Representation (B-\nRep), which is a fundamental data format for CAD models\nencoding a high level of parametric details in CAD models,\ncan be used to learn the intricate geometric features of\nsuch models. Although GDL methods have seen considerable\nsuccess in analyzing 3D shapes in mesh [2], [3], voxel [4]–\n[6], and point cloud [7], [8] data formats, extracting feature\nrepresentations directly from B-Rep data poses challenges.\nConverting B-Rep data to conventional formats like triangle\nmeshes is not only computationally expensive but also leads\nto information loss [9]. Hence, learning feature representations\ndirectly from B-Rep data becomes imperative, ensuring an\nefficient capture of the most representative geometric features\nin CAD models for subsequent analyses without the burden\nof extensive computation and memory usage.\nRecently, there has been a great research interest in lever-\naging GDL methods for learning the structure of CAD models\nand for facilitating the design process in different aspects.\nWhile conventional machine learning and deep learning ap-\nproaches have been explored for CAD classification and clus-\ntering tasks [10]–[12], these methods predominantly focus on\nlearning feature representations based on physical features like\nvolume, bounding box, area, density, mass, principal axes, or\ndirectly work on other data formats for 3D data, such as point\ncloud, without considering B-Rep. Accordingly, they often fall\nshort in capturing the concise geometric properties embedded\nin CAD models.\nAs GDL methods continue to evolve, they are expected to\nplay a pivotal role in shaping the future of CAD design across\nindustries. In recent years, more advanced GDL methods such\nas GNNs and (Graph) Transformer Networks have shown\ngreat potential in learning the complex geometric features\nembedded in CAD models, particularly from B-Rep data,\nirrespective of their physical features [9], [10], [13]–[16].\nThese learned feature representations serve various purposes,\nsuch as reconstructing CAD models [15]–[17], determining\njoints between CAD solids [18], [19], and autocompleting\nunfinished CAD models [20]–[23]. The overarching objective\nof these learning-based approaches is to elevate the level of\nautomation in the CAD design process. By alleviating the\nneed for experts to perform repetitive and time-consuming\ntasks, such as sketching and drafting, these methods have the\npotential to empower designers to focus on the more creative\nfacets of the design process. By analyzing historical design\ndata, extracting geometric features, and discerning valuable\ninsights like similarities between CAD models, these methods\ncan facilitate the reuse of CAD models in new products. This\nnot only saves time and resources by preventing redundant\ndesigns but also aids designers in customization by generating\ndesign alternatives based on specific parameters and objec-\ntives. Furthermore, there is a growing demand for CAD tools\nthat can reverse engineer models and generate diverse design\noptions based on concise parameters derived from preliminary\narXiv:2402.17695v1  [cs.CG]  27 Feb 2024\n2\nconcept sketches. GDL-based methodologies have made some\nprogress in meeting this demand [22], [24]. However, the\nintroduction of machine learning-based methods in this area\nis not without challenges. A major challenge is the scarcity\nof annotated CAD datasets in B-Rep format. Unlike other\ndata formats such as images, videos, or text, collecting CAD\nmodels is intricate and time-consuming, typically requiring the\nexpertise of skilled engineers. Moreover, these datasets often\nremain proprietary and inaccessible to the public. On the other\nhand, annotating CAD datasets, especially for mechanical\nobjects, demands substantial domain knowledge. Given that\ntraining deep learning methods without large-scale annotated\ndatasets is impractical, recent research efforts have also made\nvaluable contributions by providing benchmark CAD datasets.\nAs this field is still emerging, there is plenty of room for\nfuture research and development, and several challenges need\nto be explored and addressed. Our survey, to the best of\nour knowledge, stands as the first comprehensive review in\nthis rapidly evolving domain, which endeavors to provide a\nthorough exploration of recent advancements, challenges, and\ncontributions in this area. Figure 1 shows the structure of the\nsurvey and highlights the most representative methods in the\nfield reviewed in more detail. As we embark on this survey,\nour goal is to serve as a guiding resource for researchers\nand practitioners eager to navigate the dynamic intersection\nof GDL and CAD. The survey is tailored to provide:\n• A comprehensive review of the state-of-the-art GDL\nmethods employed in the analysis of CAD data. This\nencompasses diverse categories, including similarity anal-\nysis, classification, and retrieval, as well as segmentation,\nalong with the synthesis of 2D and 3D CAD models. Fur-\nthermore, the survey explores techniques for generating\nCAD models from alternative data representations, such\nas point clouds.\n• A detailed overview of benchmark CAD datasets crucial\nfor the advancement of research in this field, accompanied\nby open-source codes that have been instrumental in\npushing the boundaries of achievable outcomes.\n• An in-depth discussion about the challenges that persist\nin this domain followed by potential future research\ndirections by assessing the limitations of existing method-\nologies.\nII. BACKGROUND\nA. Computer Aided Design (CAD)\nComputer-Aided Design (CAD) is a manufacturing tech-\nnology that has revolutionized the way engineers, architects,\ndesigners, and other professionals create and visualize designs\n[35]. CAD process involves the use of specialized software\nto design, modify, analyze, and optimize 2D drawings and\n3D models of physical objects or systems digitally before\nconstructing them. Engineering drawing entails the use of\ngraphical symbols such as points, lines, curves, planes and\nshapes, and it essentially gives detailed description about any\ncomponent in a graphical form. There are several CAD soft-\nware options which are widely used in industry for different\npurposes.\n1) CAD Tools and Python APIs: To name some of the\nmost popular software options, AutoCAD [36] and Fusion 360\n[37] (a cloud-based software) offer 2D/3D design tools for a\nvariety of industries. SolidWorks [38] and CATIA [39] pro-\nvide parametric modeling and simulation for the mechanical,\naerospace, and automotive industries. OnShape [40] (a cloud-\nbased software) and Creo [41] offer 3D parametric design\nand simulation. TinkerCAD [42] is a free-of-charge and web-\nbased CAD design tool which is mostly used for beginners\nand educational purposes.\nFor CAD parsing and development, several Python APIs\nare available, providing tools and libraries to work with CAD\ndata, create 3D models, visualize CAD models, and to perform\nvarious related operations. To name some of the most popular\nPython APIs for CAD, PythonOCC [43] offers an open-source\n3D modeling and CAD library for Python. It is based on\nOpenCASCADE Technology (OCCT) [44] which is a power-\nful geometry kernel and modeling framework used for CAD.\nPythonOCC provides Python bindings to the OpenCASCADE\nC++ library, allowing developers to use Python for creating,\nmanipulating, visualizing and analyzing 3D geometry and\nCAD models. OCCWL [45] has been recently released as\na simple, lightweight Pythonic wrapper around PythonOCC.\nCADQuery [46], a Python library for scripting 3D CAD\nmodels, is also built on top of the OpenCASCADE geometry\nkernel.\nAnother powerful 3D geometric modeling kernel is Para-\nsolid [47], which provides a set of functions and methods\nfor creating, manipulating, and visualizing and analyzing 3D\nsolid models. However, unlike OpenCASCADE, Parasolid is\nnot open-source and software developers need to license it to\nintegrate its capabilities into CAD applications.\n2) Terminology in CAD Design: Boundary Representation\n(B-Rep), serves as a fundamental modeling technique utilized\nin CAD to represent intricate 3D models along with their\ngeometrical attributes. It facilitates accurate and consistent\ndesign, modification, manipulation, analysis and representation\nof 3D entities by characterizing surfaces, curves, points, and\ntopological relationships between them in 3D space. In Table\nI, we introduce the basic terminology of CAD design which is\nessential for understanding the concepts and methodologies in\nthis field. However, depending on the industries and different\ndesign software applications that are used for design, there\nmight exist more specialized terms and concepts that are\nnot covered here. For the sake of maintaining consistency\nthroughout this paper, we will employ this terminology for\nall methodologies and datasets discussed in the subsequent\nsections.\nB. Geometric Deep Learning (GDL)\nGeometric Deep Learning (GDL) has arisen as a special\nand fundamental branch of artificial intelligence (AI) that\nexpands deep learning approaches from regular Euclidean data\nto complex geometric structured data [1]. While traditional\ndeep learning methods have led to great advancements in\ndifferent applications by processing regular Euclidean data\nstructures, such as audio, text, images and videos, formed by\n3\nCAD analysis with GDL\nRepresentation Learning on CAD\nCAD Classification and Retrieval [9], [13]\nCAD Segmentation [9], [14], [25]\nCAD Assembly [18], [19]\nCAD Construction with Generative Deep Learning\nEngineering 2D Sketch Generation [20], [21], [23], [26], [27]\n3D CAD Generation\n3D CAD Generation from Sketch [22], [24], [28]\n3D CAD Command Generation [17], [29]–[31]\n3D CAD Generation with Direct B-rep synthesis [15], [16]\n3D CAD Generation from Point Cloud [32]–[34]\nFig. 1: Taxonomy and the structure of the most representative methods reviewed in this survey.\nregular 1D, 2D and 3D grids, GDL is tailored for processing\nand extracting intricate spatial and topological features from\nirregular structured data like 3D shapes, meshes, point clouds,\nand graphs. These irregular data structures processed by GDL\ncan be either Euclidean or non-Euclidean, depending on the\ncontext and the geometric properties they possess. GDL has\ngained significant research attention in the CAD domain for its\nability in learning complex geometric features and facilitating\ndesign process for engineers and designers.\nGraph neural networks (GNNs) [48], [49] are one of the\nmost popular types of GDL approaches which excel in pro-\ncessing graph-structured data. GNNs have made remarkable\nstrides in diverse tasks tasks coming from different applica-\ntions, such as computer vision [50]–[52], bio-chemical pattern\nrecognition [53], [54], and financial data analysis [55]. GNNs\noffer a specialized approach for modeling complex CAD\ngeometric structures encoded in B-Rep, as a graph which\nrepresents the topology and geometry of 3D shapes through\nnodes and edges. GNN models can effectively capture the\ntopological structure, connectivity, and attributes inherent in\ngraphs (B-Reps) and allow hierarchical analysis by considering\nlocal and global contexts, aligning with the complex structure\nof B-Rep models. GNNs are also able to extract and propagate\ngeometric features through the graph, capturing subtle aspects\nof shape and curvatures and leveraging data-driven insights\nfrom existing B-Rep models, facilitating tasks like solid and/or\nsketch segmentation [9], [14], [56], shape reconstruction and\ngeneration [16], [20], shape analysis and retrieval [9], [13].\nBy representing B-Rep as a graph and leveraging GNN\ncapabilities to capture its topological structure, connectivity,\nand geometry, designers and engineers can analyze, optimize,\nand create intricate 3D models with enhanced precision and\nefficiency.\nLet us present a more detailed explanation of how B-\nRep models are represented as graphs. In the context of\nB-Rep, graph nodes can represent points, lines, and faces\nwith associated attributes like geometric coordinates, line\nlengths, curvature, face area and surface normals. Edges in the\ngraph represent connections between nodes. For B-Rep, these\nconnections indicate constraints, like adjacency, tangency, or\ncoincidence, between primitives like points, lines, faces, etc.\nGNN methods embed each node’s features, creating node\nrepresentations that capture geometric attributes that aggregate\ninformation from neighboring nodes. In the B-Rep context,\nthis simulates the way geometric properties flow through\nadjacent points, lines, curves, and faces. Through feature\n4\nTerm\nDescription\nModel\n2D or 3D representation of a real-world object or system created within a CAD software.\nB-Rep\nBoundary Representation, a data format for geometrically describing objects by representing their topological components such\nas surfaces, edges, vertices, and the relationship between them.\nSketch\n2D drawing of an object, served as the basis for creating 3D models, made of lines, curves, and other basic geometric shapes.\nExtrude\nA CAD operation for expanding a 2D drawing along a specific dimension to create a 3D model.\nPrimitive\nA basic 2D/3D geometric shape which serves as backbone for more complex designs. For 2D sketches, primitives are points,\nlines, arcs, circles, ellipses and polygons. For 3D shapes primitives are cubes, spheres, cylinders, cones.\nConstraint\nGeometric relationship, such as coincident, tangent, perpendicular, between primitives in a 2D/3D design.\nPoint\nBasic geometric entity representing a specific location in space with 3D coordinates X, Y, Z.\nLine\nA straight path between two points in a 3D space.\nCircle\nA closed curve defined by its center and radius.\nArc\nA curved line, as a part of a circle or ellipse, defined by a center, start and end points.\nLoop\nA closed sequence of curves forming the boundary of a surface.\nProfile\nA closed loop made by a collection of curves joint together.\nFace\nA 2D surface bounded by a profile.\nEdge\nA piece of a curve bounded by two points, defining the boundaries of a face.\nShell\nA collection of connected faces with joint boundaries and points.\nBody/Part\nA 3D shape created by extruding a face.\nSolid\nA 3D shape that is used as a building block to design an object.\nComponent\nBuilding blocks of assemblies containing one or more bodies (parts).\nAssembly\nA collection of connected bodies (or parts) to create a larger object.\nJoint\nThe connection, defined by coordinate and axis, between bodies (parts) to make an assembly.\nTopology\nThe structure of points, curves, faces and other geometric components in a 3D model.\nWireframe\nVisual representation of 3D B-Rep models, showing the structure of the object with only lines and curves. Also known as\nskeletal representation of 3D objects.\nRendering\nThe process of visualizing CAD models to simulate their real-world appearance.\nSurface Normal\nA unit vector perpendicular to a surface at a specific point.\nTABLE I: Terminology in CAD design. A summary of the widely used terms through different research works and CAD\nplatforms.\nType\nDescription\nFormat\nSTEP\nOriginal parametric B-Rep format containing explicit description of the topology and geometry information of\nthe CAD models.\n.txt, .step, .smt\nSTL\nStandard Tessellation Language, a format describing 3D surfaces of an object with triangular facets (meshes).\n.stl\nObj\nA format describing 3D surface geometry using a triangular meshes.\n.obj\nStatistics\nStatistical infromation of CAD models.\n.yml, .txt\nFeatures\nA description of the properties of surfaces and curves with references to the corresponding vertices and faces\nof the discrete triangle mesh representation.\n.yml\nImage\nVisualization of objects produced by rendering process.\n.png\nPCD\nPoint Cloud Data, a collection of points in the 3D space representing the structure of a 3D model.\n.ASC, .PTX, .OBJ\nTABLE II: A summary of different file formats and their corresponding description. Every CAD dataset provides at least one\nof these data formats to be processed for different purposes.\naggregation, the method updates each node’s representation by\ncombining its own features with information of its neighbors.\nThis step captures the influence of neighboring geometry.\nThe core operation in GNNs is the graph convolution, which\nis combined with a nonlinear function applied on the fused\ninformation from neighboring nodes. In a multi-layer GNN,\nthe graph convolution operation captures multi-hop neighbor-\ning features allowing for hierarchical analysis. For B-Rep,\nthis operation helps to capture the hierarchical relationships\nbetween primitives. To obtain a global representation of the\nB-Rep model, GNNs can employ global pooling, summarizing\nthe entire graph information into a single feature vector. Figure\n2, 3 show an example of a 3D shape and a 2D sketch\nrepresented as graph, respectively.\nThe most popular GNN architectures, i.e., Graph Convolu-\ntional Networks (GCNs) [57]–[59] and Graph AutoEncoders\n(GAEs) [60], which are used for (semi-)supervised and unsu-\npervised learning on graphs, respectively, for different graph\nanalysis tasks such as node/graph classification and graph\nreconstruction/generation. As GNN based methods continue\nto evolve, they are expected to play a pivotal role in shaping\nthe future of CAD design across industries. In the following\nV1\nV2\nV3\nV4\nV6\nV5\nFig. 2: An example of a 3D solid represented as a graph, where\nthe solid primitives such as curves and surfaces are represented\nas graph nodes, and their adjacency relationship between the\nsolid primitives are represented as graph edges.\nsections, we delve into details of the current state-of-the-art\nmethods in this area and the challenges.\nIII. DATASETS\nLarge data collections play a pivotal role in enhancing\nperformance of deep learning models when applied in prob-\nlems coming from different applications. Collecting such large\n5\nArc\nCircle\nCircle\nLine\nCircle\nLine\nLine\nMirror\nCoincident\nCoincident\nCoincident\nCoincident\nTangent\nTangent\nFig. 3: An example of a simple 2D sketch represented as a\ngraph, where the sketch primitives such as curves (circle, arc,\nline, etc.) are modeled as graph nodes, and the constraints\nbetween these primitives are shown as graph edges. Sketch-\nGraphs [20] dataset contains such 2D sketches modeled as\ngraphs.\ndatasets in regular formats such as image, video, audio, text,\nand their distribution through different platforms like social\nmedia has greatly accelerated the progress of deep learning in\ncomputer vision and natural language processing. GDL offers\nadvantages for tasks like 3D shape analysis, shape reconstruc-\ntion, and building geometric feature descriptors. Nonetheless,\ncreating and annotating high-quality 3D geometric data needs\nsignificant level of domain knowledge, and engineering and\ndesign skills. Collecting such datasets is also challenging due\nto various factors such as concerns about proprietary rights and\nownership, and lack of consistency and compatibility among\ndata from available sources.\nTable III provides a summary of the existing datasets\nalong with their properties. The details about each dataset\nare provided in the dedicated sections for methodologies\nlinked to each dataset. Table II introduces different CAD data\nformats along with their corresponding description. Existing\ncommonly used 3D CAD datasets mostly provide mesh ge-\nometry for 3D shape segmentation, classification and retrieval\n[5], [61]–[64], human body mesh registration [65], 2D/3D\nshape alignment [66] and 3D scene categorization, semantic\nsegmentation and object detection [67], [68]. Engineering\nshape datasets such as ESB [69], MCB [70], AAD [71],\nFeatureNet [72], and CADNet [73] also provide annotated\ndata with mesh geometry for mechanical shape classification\nand segmentation. The primary limitation of these datasets is\ntheir lack of parametric and topological features of curves and\nsurfaces commonly referred to as boundary representation (B-\nRep). These B-Rep features are essential for conducting para-\nmetric CAD shape analysis. Recently, several geometric B-\nRep datasets of different sizes and properties were introduced\nto boost GDL progress on CAD design. FabWave [74] is a\ncollection of 5, 373 3D shapes annotated with 52 mechanical\npart classes, including gears and brackets, and Traceparts [75]\nis a small collection of 600 CAD models produced by different\ncompanies labeled into 6 classes (100 CAD models in each\nclass), including screws, nuts, and hinges, which can be used\nfor 3D shape classification. MFCAD [76] is a synthetic 3D\nsegmentation dataset containing 15, 488 shapes with annotated\nfaces of 16 classes, which can be used for parametric face\nsegmentation in CAD shapes.\nAdvancing across various tasks in GDL, and effective\ntraining of deep learning models, necessitate large parametric\nCAD data collections. The existing parametric CAD datasets\nwith B-Rep data are limited in size and insufficient to meet\nthese demands. To address this shortfall, three big datasets,\nABC [77], Fusion 360 Gallery [30], and SketchGraphs [20],\nwere introduced recently and provide valuable resources for re-\nsearch in this area. ABC [77] is the first large-scale, real-world,\nand hand designed dataset with over 1 million high-quality 3D\nCAD models covering a wide range of object types. Each CAD\nmodel consists of a set of precisely parameterized curves and\nsurfaces, offering accurate reference points for sharp geometric\nfeature representation, patch segmentation, analytic differential\nmeasurements, and the process of shape reconstruction. The\nCAD models in ABC dataset are compiled and collected via\nan openly accessible interface hosted by Onshape [40], and an\nopen-source geometry processing pipeline has been developed\nto process and prepare the CAD models in ABC dataset to be\nused by deep learning methods.1\nTraining machine learning models to facilitate CAD con-\nstruction and synthesis can significantly benefit designers\nby minimizing the time and effort required for the design\nprocess. An essential necessity for tasks related to CAD (re-\n)construction is to understand how a CAD model is designed\nand how to interpret the provided construction information\nin the dataset. The construction history information in ABC\ndataset can only be retrieved by querying the Onshape API in a\nproprietary format with limited documentation which makes it\nchallenging to develop CAD (re-)construction methods on that\ndataset. SketchGraphs dataset has been produced to fill this\ngap by providing a collection of 15 million human-designed\n2D sketches of real-world CAD models from Onshape API.\n2D sketches representing geometric primitives (like lines and\narcs) and constraints between them (like coincidence and\ntangency) can be seen as the basis of 3D shape parametric\nconstruction, and each 2D sketch is presented as a geo-\nmetric constraint graph where nodes denote 2D primitives\nand edges are geometric constraints between nodes (3). The\nSketchGraphs dataset can be used for various applications\nin automating design process such as auto-completing sketch\ndrawing by predicting sequences of sketch construction op-\nerations or interactively suggesting next steps to designer,\nand autoconstraint application where the method is predicting\na set of constraints between geometric primitives of sketch.\nOther potential applications are CAD inference from images\nwhere the method receives a noisy 2D drawing image and\ninfers its design steps to produce the corresponding parametric\nCAD model, and learning semantic representation encoded in\nsketches. An open-source Python pipeline for data processing\nand preparation for deep learning frameworks also comes\nalong the dataset.2 Similar to SketchGraphs, freehand 2D\nsketch datasets such as [78]–[80] also have been introduced\nto tackle this challenge by providing the sketch construction\nsequence.\n1https://deep-geometry.github.io/abc-dataset/\n2https://github.com/PrincetonLIPS/SketchGraphs\n6\nFusion 360 Gallery [30] has been introduced recently by\nAutodesk as the first human designed 3D CAD dataset and\nenvironment with 3D operation sequences for programmatic\nCAD construction. The fusion 360 Gallery dataset comes\nalong an open source Python environment named as Fusion\n360 Gym for processing and preparing the CAD operations\nfor machine learning methods. This dataset contains both 2D\nand 3D geometric CAD data which are produced by the\nusers of the Autodesk Fusion 360 CAD software and are\ncollected into the Autodesk Online Gallery. Several datasets\nfor different learning goals, such as CAD reconstruction [30],\nCAD segmentation [14], and CAD assembly [18], are created\nbased on a total of real-world 20, 000 designs in Fusion 360\nGallery.\nIV. CAD REPRESENTATION LEARNING\nStudying extensive data repositories and uncovering hidden\nfeatures in data for various tasks such as similarity analysis and\nshape retrieval has been a vibrant field of research in machine\nlearning and artificial intelligence. The significance of this\nconcept extends to CAD data as well. Machine learning-based\nsimilarity analysis of CAD models can effectively facilitate\nthe design process for designers by categorizing designs,\nand retrieving similar CAD models as design alternatives.\nIt has been shown that approximately 40% of new CAD\ndesigns could be constructed based on existing designs in\nthe CAD repository, and at least 75% of design processes\nleverage existing knowledge for designing a new CAD model\n[82]. Learning from CAD data and extracting features from\nextensive collections of geometric CAD shapes is important\nfor CAD retrieval and similarity analysis, and it involves em-\nploying various machine learning and deep learning methods\nfor extracting geometric attributes encoded in B-Rep data for\nassessing similarity between pairs of CAD models.\nThe first step in this process is to select a subset of geomet-\nrical, topological, functional and other properties of the CAD\nmodel, based on specific analysis goals, to be represented in a\nsuitable format, such as numerical vectors, matrices, tensors,\nor graphs, for the deep learning methods to process. Nev-\nertheless, representing B-Rep data is challenging due to the\ncoexistence of continuous non-Euclidean geometric features\nand discrete topological properties, making it difficult to fit\ninto regular structured formats such as tensors or fixed-length\nencodings. A key contribution of each state-of-the-art method\nin this topic is the introduction of a method for encoding or\ntokenizing B-Rep data into a format suitable for the adopted\ndeep learning architecture, tailored to the particular application\nthey are addressing. The deep learning methods receive these\nrepresentations as input and learn to classify, cluster, segment,\nor reconstruct CAD models, considering the annotations at\nhand. However, the application of machine learning and deep\nlearning methods to CAD models is very rare because of the\nscarcity of annotated CAD data available in the B-Rep format.\nIn contrast to the geometric CAD models like ShapeNet [62],\nmany parametric CAD datasets are not publicly released due\nto the proprietary nature of design data.\nWhile there have been recent releases of small annotated\ndatasets containing mechanical parts in B-Rep format for\nmachine learning research [75], the majority of large-scale\npublic databases, such as ABC, remain predominantly unla-\nbeled. Additionally, not only the process of manually anno-\ntating B-Rep data in a specialized format needs engineering\nexpertise, but it is also very time-consuming and costly, thus\nposing a significant limitation. Consequently, deep learning\napproaches that do not rely on external annotations, such as\nunsupervised and self-supervised learning, become particularly\ncrucial alongside traditional supervised learning approaches\nin such scenarios. Learning feature representations based on\nintrinsic data features, without external annotations or expert\nknowledge, proves highly advantageous in overcoming the\nscarcity of annotated CAD data. In this section, we introduce\nexisting research endeavors that employ GDL to extract feature\nrepresentations from CAD B-Reps. These methods operate in\neither a supervised, self-supervised, or unsupervised manner,\ncatering to various tasks such as classification, segmentation,\nsimilarity analysis and retrieval, and assembly prediction.\nA. CAD Classification and Retrieval\nDesigning methodologies for categorizing 3D components\nwithin B-Rep assemblies is significantly important for various\napplications, such as re-using similar CAD components in\ndifferent assemblies, shape recommendation and alternative\nsuggestion, and product complexity estimation, especially\nwhen 3D CAD models with substantially different geometries\nare within the same category and share similar topology. One\nof the first deep learning methods proposed to work directly on\nB-Rep data format of 3D CAD models is UV-Net [9]. UV-Net\nproposed a cohesive graph representation for B-Rep data by\nmodeling topology with an adjacency graph, and modeling\ngeometry in a regular grid format based on the U and V\nparameter domains of curves and surfaces. One of the main\ncontributions of this work is to extract crucial geometric and\ntopological features out of B-Rep data and make a grid data\nstructure out of complex B-Rep data to feed to deep learning\nmodels for different tasks such as B-Rep classification and\nretrieval. To generate grid-structured feature representations\nfrom B-Rep data, this approach transforms each 3D surface\ninto a regular 2D grid by performing surface sampling with\nfixed step sizes. In a similar fashion, it transforms each solid\ncurve into a 1D grid. The resulting 1D/2D grid mappings are\nreferred to as UV-grids. Each sampled point in a surface’s\n2D grid conveys three distinct values across 7 channels: a)\nThe 3D absolute point position, represented as xyz in the\nUV coordinate system, b) The 3D absolute surface normal,\nand c) The trimming mask with 1 and 0 denoting samples\nin the visible region and trimmed region, respectively. For the\nsampled points in a curve’s 1D grid, the encoding includes the\nabsolute point UV coordinates and, optionally, the unit tangent\nvector.\nAs illustrated in Figure 4, UV-Net model architecture is\ncomprised of both CNN and GCN layers to first extract fea-\ntures from 1D and 2D grids representing curves and surfaces,\nrespectively, and then to capture the topological structure of\nthe 3D shape encoded as a graph with hierarchical graph\nconvolution layers. The 1D curve and 2D surface UV-grids\n7\nDataset\n#Models\nB-Rep\nMesh\nSketch\n#Categories\nApplication\nShapeNet [62]\n3M+\n×\n✓\n×\n3,135\nClassification\nModelNet [5]\n12,311\n×\n✓\n×\n40\nClassification\nPartNet [64]\n26,671\n×\n✓\n×\n24\nClassification, Segmentation\nPrincetonSB [61]\n6,670\n×\n✓\n×\n92\nClassification\nAAD [71]\n180\n×\n✓\n×\n9\nClassification\nESB [69]\n867\n×\n✓\n×\n45\nClassification\nThingi10k [63]\n10,000\n×\n✓\n×\n2,011\nClassification\nFeatureNet [72]\n23,995\n×\n✓\n×\n24\nClassification\nMCB [70]\n58,696\n×\n✓\n×\n68\nClassification\nCADNet [73]\n3,317\n×\n✓\n×\n43\nClassification\nFABWave [74]\n5,373\n✓\n✓\n×\n52\nClassification\nTraceparts [75]\n600\n✓\n✓\n×\n6\nClassification\nSolidLetters [9]\n96,000\n✓\n×\n×\n26\nClassification\nMFCAD [76]\n15,488\n✓\n×\n×\n16\nSegmentation\nMFCAD++ [81]\n59,655\n✓\n×\n×\n25\nSegmentation\nFusion 360 Segmentation [14]\n35,680\n✓\n✓\n×\n8\nSegmentation\nCC3D-Ops\n37,000\n✓\n✓\n×\n-\nSegmentation\nFusion 360 Assembly [18]\n154,468\n✓\n✓\n×\n−\nJoint Prediction\nAutoMate [19]\n3M+\n✓\n✓\n×\n−\nJoint Prediction\nABC [77]\n1M+\n✓\n✓\n×\n−\nReconstruction\nFusion 360 Reconstruction [30]\n8,625\n✓\n✓\n×\n−\nReconstruction\nSketchGraphs [20]\n15M+\n×\n×\n✓\n−\nReconstruction\nCAD as Language [27]\n4.7M+\n×\n×\n✓\n−\nReconstruction\nSketch2CAD [24]\n50,000\n×\n×\n✓\n−\nReconstruction\nCAD2Sketch [28]\n6000\n×\n×\n✓\n−\nReconstruction\nDeepCAD [17]\n178,238\n✓\n×\n×\n−\nReconstruction\nPVar [15]\n120,000\n✓\n×\n×\n60\nReconstruction, Classification\nCADParser [16]\n40,000\n✓\n×\n×\n−\nReconstruction\nFree2CAD [22]\n82,000\n✓\n×\n✓\n−\nReconstruction\nTABLE III: Overview of the existing common object and mechanical CAD datasets with their properties. For each dataset, the\nnumber of CAD models, representation formats such as B-Reps, Mesh, Sketch, and different tasks they are annotated for, such\nas Segmentation, Classification and CAD Reconstruction, are reported. The first 4 rows of the table show the common object\ndatasets annotated for 3D shape classification. The remaining rows list mechanical object datasets, created for CAD analysis.\nare processed by 1D and 2D convolution and pooling layers\nwhile the weights of the convolution layers are shared among\nall curves and surfaces in a B-Rep to make them permutation-\ninvariant. The 64-dimensional feature vectors derived from\nsurfaces and curves by the convolution layers of the CNN\nserve as the node and edge features within a face-adjacency\ngraph G(V, E), where the set of graph nodes V represent the\nfaces (surfaces) in B-Rep and the set of edges E represents\nthe connection between faces. Subsequently, this graph is\nintroduced to a multi-layer GCN, where the graph convolution\nlayers propagate these features across the graph, enabling\nthe capturing of both local and global structures inherent\nin the shape. The UV-Net encoder is used as a backbone\nfor supervised and self-supervised learning on labeled and\nunlabeled datasets, respectively. For CAD classification, which\nis a supervised task, UV-Net encoder is followed by a 2-\nlayer classification network to map the learned features to\nclasses and the model is trained in an end-to-end manner\non 3 annotated datasets, SolidLetters [9], FabWave [74], and\nFeatureNet [72]. SolidLetters is currently the biggest synthetic\n3D B-Rep dataset with a great variation in both geometry\nand topology annotated for a classification task. It consists of\n96, 000 3D shapes which represent 26 English alphabet letters\n(a-z) with different fonts and dimensions.\nFor CAD retrieval, however, the premise is the absence of\nlabeled data, and thus, the UV-Net encoder needs to be trained\nin a self-supervised manner. This leads to the utilization of\ndeep learning models designed for self-supervised training,\nsuch as Graph Contrastive Learning (GCL) [83] or Graph\nAutoEncoder (GAE) [60]. For training the UV-Net encoder\nas a self-supervised model, GCL is leveraged to apply trans-\nformations on the face-adjacency graph and make positive\npairs for each B-Rep sample. These transformations can be\nperformed in various ways, such as randomly deleting nodes\nor edges with uniform probability or extracting a random node\nand its n-hop neighbors within a graph. Assuming that each\nB-Rep and its transformed verison are positive pairs, the UV-\nNet encoder extracts the shape embeddings of each pair as\n{hi, hj} and a 3-layer non-linear projection head with ReLU\nactivations transforms these embeddings into latent vectors\nzi and zj, respectively. For a batch comprising N B-Rep\nsamples, the latent vector for each sample, along with its\ncorresponding positive pair, is computed. The entire model\nis then trained in an end-to-end manner with the objective of\n8\nV1\nV2\nV3\nV4\nV6\nV5\nV1\nV2\nV3\nV4\nV6\nV5\n2D CNN\n1D CNN\n64D \nTrimming mask\nSurface normal\nPoint Coordinates\nGCN\nPooling\nGraph embedding\nNode embeddings\n128D\n128D\nSurface\nCurve\nU\nV\n64D \nFig. 4: Schematic representation of the UV-Net model architecture [9]. The model takes B-Rep data as input, generating grid-\nstructured features for surfaces and connecting curves. These UV-grid mappings are further processed through CNN and GCN\narchitectures to learn feature embeddings for the overall graph and its individual nodes.\nbringing the embedding of each sample closer to its positive\npair. Simultaneously, the model works to treat the remaining\n2(N − 1) B-Reps as negative examples, aiming to push them\nfurther away from the positive pair. Through this process, the\nmodel learns to capture the intrinsic features and patterns\nwithin the data without the need for labeled examples. It\nessentially leverages the relationships within the data itself,\ngenerated through transformations or augmentations, to learn\nmeaningful representations. This makes contrastive learning a\nself-supervised learning approach which is particularly useful\nin scenarios where obtaining labeled data is challenging or\nimpractical. For retrieving similar CAD models, the model\nis trained on an unlabeled dataset like ABC. Subsequently,\nembeddings for random samples from the test set are used as\nqueries, and their k-nearest neighbors are calculated within the\nUV-Net shape embedding space.\nLikewise, the method proposed in [13] utilizes geometry\nas a means of self-supervision, extending its application to\nfew-shot learning. Specifically, the method involves training\nan encoder-decoder structure to rasterize local CAD geometry,\ntaking CAD B-Reps as input and generating surface rasteri-\nzations as output. B-Reps are assembled piecewise through\nexplicitly defined surfaces with implicitly defined boundaries.\nAccordingly, the encoder of this approach adopts the hier-\narchical message passing architecture of SB-GCN proposed\nin [19] to effectively capture boundary features for encoding\nB-Rep faces. The decoder, in turn, reconstructs faces by\nsimultaneously decoding the explicit surface parameterization\nand the implicit surface boundary. The embeddings learned\nthrough self-supervised learning in this method serve as input\nfeatures for subsequent supervised learning tasks, including\nCAD classification on the FabWave dataset. Notably, with very\nlimited labeled data (ranging from tens to hundreds of exam-\nples), the method outperforms previous supervised approaches\nwhile leveraging smaller training sets. This underscores the\neffectiveness of the differentiable CAD rasterizer in learning\nintricate 3D geometric features.\nB. CAD Segmentation\nCAD segmentation entails decomposing a geometric solid\nmodel, represented in B-Rep format, into its individual com-\nponents including faces and edges. CAD segmentation finds\napplications in retrieving CAD parametric feature history,\nmachining feature recognition, Computer Aided Engineering\n(CAE) and Computer Aided Process Planning (CAPP). This\nis particularly intriguing as it facilitates the automation of\nseveral tedious manual tasks within CAD design and analysis,\nespecially when users need to repeatedly select groups of faces\nand/or edges according to the manufacturing process as input\nfor modeling or manufacturing operations [76], [84], [85].\nHowever, progress in CAD segmentation was hindered until\nvery recently due to the absence of advanced deep learning\nmethods and large annotated datasets. MFCAD [76] is the first\nsynthetic segmentation dataset consisting of 15, 488 3D CAD\nmodels with planar faces, each annotated with 16 types of\nmachining feature such as chamfer, triangular pocket, through\nhole, etc. Figure 5 shows some examples from this dataset.\nThe CAD segmentation task can be framed as a node classi-\nfication problem, with each 3D solid represented as a face adja-\ncency graph. The graph nodes, corresponding to B-Rep faces,\nare then classified into distinct machining feature classes.\nCADNet [81] is one of the first proposed methods in this\nregard which represents the B-Rep solid as a graph encoding\nface geometry and topology, and utilizes a hierarchical graph\nconvolutional network called Hierarchical CADNet to classify\nthe graph nodes (or solid faces) into different machining fea-\ntures. For evaluation, this method not only leveraged MFCAD\ndataset, but it also created and released the extended version\nof the dataset, MFCAD++, which consists of 59, 655 CAD\nmodels with 3 to 10 machining features including both planar\nand non-planar faces. The UV-Net method, as introduced in\nthe previous section, also addresses CAD segmentation in the\nsame fashion, training its encoder in a supervised manner on\nthe MFCAD dataset. However, translating the B-Rep into a\nface adjacency graph results in the loss of some information\n9\nrectangular_through_slot\ntriangular_through_slot\nrectangular_passage\ntriangular_passage\n6sides_passage\nrectangular_through_step\n2sides_through_step\nslanted_through_step\nrectangular_blind_step\ntriangular_blind_step\nrectangular_blind_slot\nrectangular_pocket\ntriangular_pocket\n6sides_pocket\nchamfer\nstock\nFig. 5: Examples from the MFCAD dataset for manufacturing\ndriven segmentation. Each solid face is labeled with 16 types\nof machining features [76].\nregarding the relative topological locations of nearby entities.\nMoreover, graph representations constructed based on UV\ncoordinates lack invariance to translation and rotation.\nBRepNet [14] is the first method specifically designed for\nB-Rep segmentation based on deep learning and, notably,\nit accomplishes this without introducing to the network any\ncoordinate information. It operates directly on B-Rep faces\nand edges, exploiting compact information derived from their\ntopological relationships for B-Rep segmentation. The mo-\ntivation behind the BRepNet approach stems from the idea\nseen in the convolutional operation in CNNs used for image\nprocessing. In this operation, the local features are aggregated\nby sliding a small window called a filter or kernel over the\ngrid data and performing element-wise multiplication between\nthe filter and the overlapping grid cells, then pooling the\nresults. This concept is extended to B-Reps, allowing the\nidentification of a collection of faces, edges, and coedges at\nprecisely defined locations relative to each coedge in the data\nstructure. Coedge is a doubly linked list of directed edges,\nrepresenting the neighboring structures of B-Rep entities. Each\ncoedge also retains information about its parent face and edge,\nits neighbor (mating) coedge, and also pointers to the next and\nprevious coedge in the loop around a face. Figure 6 illustrates\na topology example traversed by a sequence of walks from\na given coedge (in red) to some of its neighboring entities\nsuch as mating coedge, next and previous coedges, faces and\nedges. Information about geometric features around a coedge,\nincluding face and edge type, face area, edge convexity and\nlength, and coedge direction, is encoded as one-hot vectors\nand concatenated in a predetermined order forming feature\nmatrices Xf, Xe, Xc of face, edge and coedge, respectively.\nThese matrices are then passed into a neural network where\nconvolution operations are performed through matrix/vector\nmultiplication to recognize patterns around each coedge. Ad-\nditionally, the performance of BRepNet is evaluated on a\nsegmentation dataset from the Fusion 360 Gallery which is\nreleased alongside BRepNet method, as the first segmentation\ndataset comprising of real 3D designs. As shown in Table III,\nFace\nEdge\nCoedge\nCoedge\n1. Start\n2. Mate\n3. Next\n4. Mate\n5. Edge\nVertex\nFig. 6: (left): The topology of a solid can be defined as a set of\nface, edge, coedge, vertex entities. and it can be traveresed by\na sequence of walks. (right): The topology can be traveresed\nby a sequence of walks from the starting entitiy (in red). The\nwalks can be Edge, Face, Next, Previous and Mate. As an\nexample, here the starting entity is the red coedge, followed\nby Mate, Next, Mate, Edge walks [14].\nExtrudeSide\nExtrudeEnd\nCutEnd\nFillet\nChamfer\nRevolveSide\nRevolveEnd\nCutSide\nFig. 7: Examples from the Fusion 360 Gallery dataset anno-\ntated for construction-based segmentation. Each solid face is\nlabeled with operation used in its construction [14].\nthis dataset comprises 35, 680 3D shapes each annotated with\n8 types of modeling operations utilized to create faces in the\nrespective model. Figure 7 shows some examples from this\ndataset.\nThe self-supervised method proposed in [13], is also as-\nsessed on a segmentation task by initially pre-training the\nnetwork on a subset of 27, 450 parts from the Fusion 360\nGallery segmentation dataset. This pre-training is conducted\nin a self-supervised manner without utilizing face annotations.\nSubsequently, the pre-trained network undergoes fine-tuning in\na supervised manner, being exposed to only a limited number\nof annotated parts to demonstrate the method’s performance in\na few-shot setting on both Fusion 360 and MFCAD datasets.\nCADOps-Net [25] draws inspiration from CAD segmentation\nmethods like UV-Net [9] and BRepNet [14], which segment\nB-Reps into distinct faces based on their associated CAD\noperations, and proposes a neural network architecture that\ntakes the B-Rep data of a 3D shape as input and learns to\ndecompose it into various operation steps and their corre-\nsponding types. Besides, it introduces the CC3D-Ops dataset,\ncomprising 37, 000 CAD models annotated with per-face CAD\noperation types and construction steps 3.\nC. CAD Assembly\nThe physical objects we encounter all around us are pri-\nmarily intricate assemblies built up by CAD designers by\ndesigning and then aligning multiple smaller and simpler parts\n3https://cvi2.uni.lu/cc3d-ops/\n10\nthrough CAD software. The meticulous pairing of parts in\nCAD is a laborious manual process, consuming approximately\none-third of designers’ time [19]. It entails precise positioning\nof parts in relation to each other and specifying their relative\nmovement. Consequently, optimizing this process is crucial\nfor enhancing the efficiency of CAD systems. This issue has\nbeen explored in various studies using deep learning methods\nin recent years [86]–[93] to simplify the part assembling and\nopen avenues for various applications such as robotic assembly\n[94], CAD assembly synthesis [95], part motion prediction\n[96], robot design optimization [97] and similarity analysis in\nCAD assemblies [98]. However, all these approaches operate\nwith non-parametric data structures like meshes, point clouds,\nand voxel grids. Consequently, they leverage GDL methods\ntailored for these data structures, such as DGCNN [99],\nPCPNet [100], PointNet [8], and PointNet++ [8], to learn the\nsurface representations. These methods primarily adopt a top-\ndown approach to predict the absolute pose of a set of parts\nfor assembly in a global coordinate system. However, this\napproach lacks support for the parametric variations of parts\nfor modifying the assembly or modeling degrees of freedom,\nand it may also result in failures when parts cannot achieve\ncomplete alignment. Additionally, these methods heavily rely\non annotated datasets like PartNet [64], which exclusively\nprovides data in mesh format.\nA bottom-up approach to assembling pairs of parts, relying\non pairwise constraints and utilizing the joint or contact infor-\nmation available in B-Rep data, can address this issue without\nrequiring class annotations on datasets. However, current B-\nRep datasets like ABC [77] and Fusion 360 Gallery [30]\nlack pairing benchmarks for CAD assemblies, making them\nunsuitable for training assembly prediction models. AutoMate\n[19] and JoinABLe [18] are the only two works proposed\nrecently, employing a bottom-up learning approach for pairing\nparts locally to form assemblies. Additionally, they have\nreleased B-Rep datasets along with their methods, providing\npairing benchmarks for training models.\n1) AutoMate [19]: Automate is the first work in this area\nfocusing on CAD assembly by operating on parametric B-\nRep data format. Additionally, this work introduces the first\nlarge-scale dataset of CAD assemblies in B-Rep format with\nmating benchmarks, released to facilitate future research in\nthis field. AutoMate dataset is made by collecting publicly\navailable CAD designs from OnShape API, containing 92, 529\nunique assemblies with an average size of 12 mates each,\nand 541, 635 unique mates. Mating in this work means align-\ning two parts with pairwise constraints defined based on\nB-Rep topology. These pairwise constraints are referred to\nas mates or joints, and they dictate the relative pose and\ndegrees of freedom (DOF) of parts within an assembly. Two\nparts can be mated through various topological entities, such\nas faces, edges, and vertices. Therefore, it is essential to\nlearn the feature representation of multiple topological entities\nat different levels to address the complexities of the CAD\nassembly problem. In contrast to previous CAD represen-\ntation learning approaches, like BRepNet [14] and UV-Net\n[9], which construct a face adjacency graph to capture the\nhomogeneous structure of the B-Rep and focus on learning\nfeature representations for face entities, AutoMate [19] takes\na different approach. It aims to learn representations for faces,\nloops, edges, and vertices by capturing the heterogeneous B-\nRep structure. This is achieved through the introduction of\nthe Structured B-Rep Graph Convolution Network (SB-GCN)\narchitecture, a message-passing network designed for learning\nthe heterogeneous graph representation of the B-Rep.\nSB-GCN takes a heterogeneous graph as input, where faces\nF, edges E, vertices V, and loops L serve as graph nodes,\nand directed bipartite connection sets between them represent\ngraph edges. Specifically, the relations between B-Rep vertices\nand edges are denoted by V:E and its transpose E:V. Similarly,\nE:L and L:E represent relationships between B-Rep edges and\nloops, while L:F and F:L denote connections between faces\nand loops. Additionally, there are undirected relations (meta-\npath) between geometrically adjacent faces, expressed as F:F.\nEach node is associated with a parametric geometry function\nencoded as a one-hot feature vector. The network utilizes\nstructured convolutions to generate output feature vectors for\nall graph nodes. The adjacency structure of different node\ntypes is captured in an ordered hierarchy in different network\nlayers. The initial three layers capture relations in the B-Rep\nhierarchy in a bottom-up order: Vertex to Edge, Edge to Loop,\nand Loop to Face. Subsequently, the following k layers focus\non capturing meta-relations between faces. The final three\nlayers reverse the node relations: Face to Loop, Loop to Edge,\nand Edge to Vertex. The network makes predictions for mate\nlocation and type using two distinct output heads. The mate\nlocation prediction head assesses pairs of mating coordinate\nframes (MCFs) that are adjacent to the selected faces on the\ntwo mating parts. Meanwhile, the mate type prediction head\nanticipates how this pair of MCFs should be mated. This\nis done by classifying features into seven different mating\ncategories, i.e. fastened,revolute, planar, slider, cylindrical,\nparallel, ball, pin slot. The model is trained on 180, 102 mates\nfrom the AutoMate dataset, and is integrated as an extension\nto the Onshape CAD system. This integration assists designers\nby providing mate recommendations for pairs of parts during\nthe CAD design process.\n2) JoinABLe [18]: Joinable is another recent method that\nemploys a bottom-up approach to predict joints (or mates)\nbetween parts based on pairwise constraints. In contrast to\nAutoMate, which relies on user contact surface selections\non parts to rank and recommend multiple mating solutions,\nJoinABLe identifies joints between parts without being limited\nto predefined surfaces, doing so automatically and without\nrequiring any user assistance. The Fusion 360 Gallery As-\nsembly dataset is concurrently introduced and made available\nwith the JoinABLe method for the purpose of training and\nevaluating the JoinABLe model. This dataset comprises two\ninterconnected sets of data: Assembly and Joint. These datasets\nare collected from user-designed CAD models publicly ac-\ncessible in the Autodesk Online Gallery. The Assembly data\nencompasses 8, 251 assemblies, totaling 154, 468 individual\nparts, along with their corresponding contact surfaces, holes,\njoints, and the associated graph structure. The Joint data\nincludes 23, 029 distinct parts, incorporating 32, 148 joints\nbetween them. JoinABLe is trained using the joint information\n11\nFig. 8: An example showing how joints are defined between\nparts (solids) in Fusion 360 Assembly-Joint dataset. Give\na pair of parts, JoinABLe predicts the joint axis and pose\nbetween them according to the joint defined by the ground\ntruth [18].\nextracted from Fusion 360 Gallery Assembly-Joint dataset.\nThe Assembly Dataset [18] is a subset of designs in which\neach CAD model is made of multiple parts. In CAD design\ncontext, each assembly or 3D shape is a collection of parts\nthat joined together and a set of assemblies can make a CAD\ndesign or 3D object.\nTwo parts are aligned through their joint axes, each pos-\nsessing a specific direction. The joint axis on each part can be\ndefined on either a face or edge entity, featuring an origin\npoint and a directional vector. For instance, on a circular\nsurface entity, the origin point is the center of the circle\nand the direction vector is the normal vector. These joint\naxes information for pairs of parts are provided in Fusion\n360 Gallery Assembly-Joint dataset, which is a subset of\nFusion 360 Gallery Assembly dataset, as ground truth for\ntraining a joint prediction deep learning model. This dataset\ncontains 23,029 parts with 32, 148 joints between them. As\nshown in Figure 8, given two parts, JoinABLe is designed\nto predict the parametric joint between them, encompassing\njoint axis prediction (origin points on two parts and direction\nvectors) and joint pose prediction. To facilitate this, each\npart is modeled as a graph G(V, E) with B-Rep faces and\nedges serving as graph nodes, and their adjacency relationships\nfunctioning as graph edges. B-Rep faces are encoded as one-\nhot feature vectors, representing surface types like plane and\ncylinder, along with a flag indicating if the surface is reversed\nrelative to the face. Similarly, one-hot vectors for B-Rep edges\nencompass curve characteristics such as line and circle, curve\nlength, and a flag denoting whether the curve is reversed\nconcerning the edge. These one-hot vectors function as graph\nnode features.\nFor two parts represented by graphs G1 and G2 with N\nand M nodes, respectively, the joint graph Gj is constructed to\nillustrate connections between the two graphs. This connection\ninformation in Gj can be expressed as a binary matrix of\ndimensions N × M. In alignment with the ground truth\njoints provided in the Joint dataset, only one matrix element,\nrepresenting the joint between two entities across the two parts,\nshould be set to 1, while the others remain 0. The JoinABLe\nmodel aims to find that one positive element and it comprises\nthree main components: encoder, joint axis prediction, and\njoint pose prediction. The encoder follows a Siamese-style\narchitecture with two distinct MLP branches. One branch\nfocuses on learning node features that represent B-Rep faces,\nwhile the other concentrates on learning node features repre-\nsenting B-Rep edges. The learned embeddings for faces and\nedges in each part are subsequently concatenated to form the\nnode embeddings for the corresponding graph. These node\nembeddings are then input into a Graph Attention Network\nv2 (GATv2) [101] which captures the local neighborhood\nstructure within each graph through message passing.\nThe joint axis prediction is formulated as a link prediction\ntask, aiming to predict a connection between the graphs G1\nand G2 by linking two nodes. This involves edge convolution\non the joint graph Gj which illustrates the connections be-\ntween the two graphs and is updated with the node features\nlearned by the encoder in the previous step. Assuming that\nhv, hu represent the learned features for the two nodes v,\nu, from G1, G2, respectively, the edge convolution to learn\nthe connection between the two nodes in the two graphs is\nperformed as follows:\nhuv = Φ(hu ⊕ hv),\n(1)\nwhere Φ(·) is a 3-layer MLP applied to the concatenated\nfeatures of hu, hv. Following the edge convolution, a softmax\nfunction is applied to normalize the features and predict the\nmost probable link between the nodes in Gj. After predicting\nthe joint axes to align the two parts, the pose prediction\nhead employs a neurally guided search approach to iterate\nthrough the top-k joint axis predictions. As an supplementary\nwork, JoinABLe has also proposed assembling a multi-part\ndesign using only the individual parts and the sequence of\npart pairs derived from the Assembly dataset. However, in real-\nworld applications, this well-defined assembly sequence and\nthe corresponding assembly graph might not be available for\nthe network to use. Additionally, a misalignment in any of the\nassembly steps could result in an incorrect overall assembly.\nTherefore, it is suggested that for large and complex assem-\nblies, a combination of top-down and bottom-up approaches\nmight be more effective.\nIn the CAD Assembly problem two primary questions often\narise: how to select the pair of parts to joint (or mate), and how\nto assemble them [102]. The former question can be addressed\nby leveraging similarity analysis and retrieval methods to\nidentify suitable pairs of parts. AutoMate and JoinAble, on\nthe other hand concentrate on answering the latter question\nby learning the process of mating two parts in an assembly.\nV. CAD CONSTRUCTION WITH GENERATIVE DEEP\nLEARNING\nCAD construction with generative deep learning involves\nleveraging advanced GDL methods to automatically generate\nor assist in the creation of parametric CAD models. These\napproaches can support designers in various ways to streamline\nthe design process. This includes tasks like generating or auto-\ncompleting sketches, as well as generating CAD operations to\n12\nconstruct a 3D model based on the designed sketch. In this\nsection, we categorize these methods into five distinct groups:\n1) methods focusing on 2D sketches, aiming to automate the\nsketching process in a 2D space as an initial step before devel-\noping 3D CAD models, 2) methods targeting 3D CAD model\nreconstruction given the sketch of the model, 3) methods\ngenerating CAD construction sequences, specifically focused\non sketch and extrude operations, for CAD construction, 4)\nmethods which perform direct B-Rep synthesis to generate\n3D CAD models, and 5) methods generating 3D CAD models\nfrom Point Cloud data.\nA. Engineering 2D Sketch Generation for CAD\nEngineering 2D sketches form the basis 3D CAD design.\nThe 2D sketches composed of a collection of geometric\nprimitives, such as vertex, lines, arcs, and circles, with their\ncorresponding parameters (e.g. radius, length, and coordi-\nnates), and explicit imposed constraints between primitives\n(e.g. perpendicularity, orthogonality, coincidence, parallelism,\nsymmetry, and equality) determining their final configura-\ntion. These 2D sketches can then be extruded to make 3D\ndesigns. Synthesizing parametric 2D sketches and learning\ntheir encoded relational structure can save a lot of time and\neffort from designers when designing complex engineering\nsketches. However, leveraging deep learning approaches in this\nregard needs large-scale datasets of 2D engineering sketches.\nMost of the existing large-scale datasets provide hand-drawn\nsketches of common objects, such as furniture, cars, etc. The\nQuickDraw dataset [103] is collected from the Quick, Draw!\nonline game [104], and Sketchy dataset [79] is a collection\nof paired pixel-based natural images and their corresponding\nvector sketches. These sketch datasets are based on vector\nimages of sketches not their underlying parametric relational\ngeometry. For reasoning about parametric CAD sketches and\ninferring their design steps using deep learning models, a\nlarge-scale dataset of parametric CAD sketches is needed.\n1) SketchGraphs [20]: The first dataset introduced in this\nregard is SketchGraphs, which is a collection of 15 million\nreal-world 2D CAD sketches from Onshape platform [40],\neach of which represented as a geometric constraint graph\nwhere the nodes are the geometric primitives and edges\ndenote the designer-imposed geometric relationships between\nprimitives. An open-source data processing pipeline is also\nreleased along with the dataset to facilitate further research\non this area.4 SketchGraphs does not only provide the under-\nlying parametric geometry of the sketches, but it also pro-\nvides ground truth construction operations for both geometric\nprimitives and the constraints between them. Therefore, it\ncan be used for training deep learning models for different\ngenerative applications facilitating the design process. One of\nthese applications which could be used as an advanced feature\nin CAD software is to automatically build the parameteric\nCAD model given a hand-drawn sketch or a noisy scan of\nthe object. The SketchGraphs processing pipeline can be used\nto produce noisy renderings of the sketches. In this way,\na large-scale dataset or paired geometric sketches and their\n4https://github.com/PrincetonLIPS/SketchGraphs\nnoisy rendered images can be created to train a deep learning\nmodel for predicting the design steps of a sketch given its\nhand-drawn image. The 3D design of the model can then\nbe obtained by extruding the designed 2D sketch. In [20],\nan auto-regressive model is proposed which employs Sketch-\nGraphs dataset for two use cases: 1) Autoconstrain, which is\nconditional completion of an sketch by generating constraints\nbetween primitives given the unconstrained geometry, and 2)\nGenerative modeling, which is auto-completing a partially de-\nsigned sketch by generating construction operations for adding\nthe next primitives and constraints between them. Although\nmost of the CAD software have a built-in constraint solver to\nbe used in design process, these generative methods are useful\nwhen an unconstrained sketch is uploaded to the software as\na drawing scan designer needs to find the constraints between\nthe sketch primitives and/or complete the sketch. This problem\nis quite similar to program synthesis or induction in constraint\nprogramming. The SketchGraphs dataset and its generative\nmethods can be a good baseline for the future works in this\ndirection. In the following, the two generative use-cases are\ndescribed in more detail.\nLet us assume a sketch represented by a multi-hypergraph\nG = (V, E) where nodes V denote primitives and edges E\ndenote constraints between them. In this graph, each edge\nmight connect one or more nodes, and multiple edges might\nshare the same set of connected nodes. An edge with a single\nnode is indicated as a self-loop showing a single constraint\n(e.g. length) for a single primitive (e.g. line), and a hyper-edge\napplies on three or more nodes (e.g. mirror constraint applied\non two primitives while assigning one more primitive as an\naxis of symmetry). An example of a sketch graph is illustrated\nin Figure 3. Each constraint is identified by its type, and each\nprimitive is identified by its type and its parameters (different\nprimitives might have different number of parameters). For\nAutoconstrain task, all the graph nodes (primitives) are given,\nand the model is trained in a supervised manner to predict the\nsequence of graph edges considering the ground truth ordering\nof constraints in the dataset. This problem can be seen as\nan example of graph link prediction [105] which predicts the\ninduced relationships between the graph nodes. Starting from\nthe first construction step, the model first predicts which node\nshould be connected to the current node and then creates a link\n(edge) between these two neighboring nodes. Then it predicts\nthe type of this edge (constraint). For generative modeling task\nwhich auto-completes a partially completed sketch (graph) by\ngenerating new primitives (nodes) and constraints between\nthem (edges), the primitives are only represented by their type\n(the primitive parameters are ignored) and the constraints are\nrepresented by both their type and their numerical or categor-\nical parameters. As an example, if the constraint between two\nprimitives is distance, its parameter could be a scalar value\ndenoting the Euclidean distance between the two primitives.\nHowever, while this model predicts both type and parameters\nfor constraints, it only predicts the type of the primitives, not\ntheir parameters, and the initial coordinates of the primitives\nmight not fit into the sketch and constraints. Therefore, the\nfinal configuration of the primitive coordinates in the sketch\nneeds to be found by the CAD software’s built-in geometric\n13\nVertex\nLine\nArc\nCircle\nFig. 9: The illustration showcases a basic sketch composed\nof 12 vertices and 9 hyperedges of types line, circle and\narc entities. On the right side, a sequence of commands for\nrendering this sketch, following the grammar proposed by [21],\nis presented. Each line connects 2 vertices denoted by ∆, every\narc is defined by traversing 3 vertices, and circles pass through\n4 vertices. Notably, in each loop, the initiation point of each\nentity coincides with the termination point of the preceding\nentity in the sequence.\nconstraint solver. This issue is addressed in the next proposed\nwork for 2D sketch generation, CurveGen-TurtleGen [21],\nwhich is introduced in V-A2.\n2) CurveGen-TurtleGen [21]: As discussed previously in\nV-A1, the generative models in SketchGraphs rely on the\nbuilt-in sketch constraint solver in CAD software to set the\nfinal configuration of the produced sketch. This issue is\naddressed by two generative models, CurveGen and Turtle-\nGen, proposed in [21] which encode constraint information\nimplicitly in the geometric coordinates to be independent\nfrom the sketch constraint solver. In this work, the primitive\ntypes are simply limited to lines, curves, and circles, as they\nare the most common primitives used in 2D sketches, and\nit is considered that the constraints between the primitives\nin sketches should be defined in a way that the geometric\nprimitives can form closed profile loops. In this regard, this\nmethod proposed two different representations for engineering\nsketches: Sketch Hypergraph representation which is used by\nCurveGen, and Turtle Graphics representation which is used\nby TurtleGen. In the Sketch Hypergraph representation, the\nsketch is represented as a hypergraph G = (V, E), where a\nset of vertices V = {ν1, ν2, ..., νn} with their corresponding\n2D coordinates νi = (xi, yi) are encoded as graph nodes,\nwhile E denotes a set of hyperedges connecting two or more\nvertices to make different primitives. The primitive type is\ndefined by the cardinality of the hyperedge. For example, the\nprimitive line is made by two connected vertices, arc is made\nby three connected vertices and circle can be seen as a set\nof four connected vertices. Figure 9 shows an example of a\nsimple sketch consisting of 12 vertices and 9 hyperedges. This\nrepresentation is used by CurveGen, which is an autoregressive\nTransformer based on PolyGen [106]. PolyGen is a an autore-\ngressive generative method for generating 3D meshes using\nTransformer architectures which are able to capture long-range\ndependencies. The mesh vertices are modeled unconditionally\nby a Transformer, and the mesh faces are modeled conditioned\non the mesh vertices by a combination of Transformers and\npointer networks [107]. Similar to PolyGen, CurveGen also\ngenerates directly the sketch hypergraph representation by first\ngenerating the graph vertices V which are used to make curves,\nand then generating the graph hyperedges E conditioned on\ngenerated vertices as follows:\np(G) = p(E|V)p(V).\n(2)\nIn this way, the network predicts the precise coordinates of\neach primitive while the type of primitive is implicitly encoded\nin the hyperedges which group vertices together to make\ndifferent types of primitives. Therefore, this method is inde-\npendent of any constraint solver for finding the final primitives\nconfiguration. By having the precise coordinates of the sketch\ncurves, the constraints between them can be automatically\nobtained as a post-processing step. However, implicit inference\nof constraints makes editing the sketch in the software more\ndifficult. If the designer wants to change one of the constraints\nbetween the primitives, for example scale of a distance, this\nchange will not propagate through the whole sketch and\nprimitives, because the exact positioning of the primitives\nare somehow fixed. In the Turtle Graphics representation, the\nsketch is represented by a sequence of drawing commands,\npen-down, pen-draw, pen-up, which can be executed to form\nan engineering sketch in the hypergraph representation. The\nTurtleGen network is an autoregressive Transfromer model\ngenerating a sequence of drawing commands to iteratively\ndraw a series of closed loops forming the engineering sketch.\nIn this way, each sketch is represented as a sequence of loops,\nand each loop is made by a LoopStart command which lifts\nthe pen, displace it to the specified position, put it down,\nand starts drawing parametric curves specified by the Draw\ncommand. In Figure 9, a simple sketch with its corresponding\ncommand sequence is illustrated. The sketch encompasses 2\nloops and 9 Draw commands, including arc, line, and circle\ntypes. The initial loop starts at a position ∆ = (int, int) and\ncomprises 4 arcs. Each arc originates from the endpoint of\nthe preceding arc, passing through 2 additional vertices. This\nloop transitions into a sequence of 4 lines. The first line begins\nwhere the last arc ended, traversing one more vertex, and this\npattern repeats for the subsequent lines. The second loop is a\ncircle that begins at another position ∆ and passes through\n3 additional vertices. CurveGen and TurtleGen have been\nevaluated on the SketchGraphs dataset, demonstrating superior\nperformance compared to the generative models proposed in\nSketchGraphs. We refer the reader to [21] for further details\non models’ architecture, training and evaluation settings.\n3) SketchGen [26]: Concurrent to CurveGen-TurtleGen,\nSketchGen, which is an autoregressive generative method,\nproposed based on PolyGen [106] and pointer networks [107].\nUnlike PolyGen, SketchGen aims to capture the heterogeneity\nof the primitives and constraints in sketches, where each type\nof primitive and constraint might have a different number\nof parameters, each of different type, and therefore have\na representation of a different size. The selection of input\nsequence representation has a great impact on the perfor-\nmance of the Transformers, and transforming heterogeneous\n14\nconstraint graphs of sketches into an appropriate sequence\nof tokens is considerably challenging. One simple solution\nis padding all the primitives’ and constraints’ representations\nto make them of the same size. However, this technique\nis inefficient and inaccurate for complex sketches. Sketch-\nGen proposed a language with a simple syntax to describe\nheterogeneous constraint graphs effectively. The proposed\nlanguage for CAD sketches encodes the constraints and\nprimitives parameters using a formal grammar. The terminal\nsymbols for encoding the type and parameters of primi-\ntives are {Λ, Ω, τ, κ, x, y, u, v, a, b} and for constraints are\n{Λ, ν, λ, µ, Ω}. The start and end of a new primitive or\nconstraint sequence are marked with Λ and Ω, respectively.\nThe primitive type is denoted with τ, and ν shows the the\nconstraint type. κ, x, y, u, v, a, b show the specific parameters\nfor each primitive, such as coordinate and direction. λ, and µ\nare the specific constraint parameters indicating the primitive\nreference of the constraint and the part of the primitive\nit is targeting, respectively. This formal language, enables\ndistinguishing different primitive or constraint types along\nwith their respective parameters. For example, the sequence for\na line primitive is Λ, τ, κ, x, y, u, v, a, b, Ω, which starts with\nΛ, followed by the primitive type τ = line, the construction\nindicator κ, the coordinates of the starting point x and y, the\nline direction u and v, the line range a, b, and ends with Ω. The\nsequence for a parallelism constraint is Λ, ν, λ1, µ1, λ2, µ2, Ω\nwhich starts with Λ, followed by the constraint type ν =\nparallelism, the reference to the first primitive λ1, the part\nof the first primitive µ1, the reference to the second primitive\nλ2, the part of the second primitive µ2, and ends with Ω. In\nthis way, each token qi (a sequence of symbols) represents\neither a primitive or a constraint, and each sketch Q is\nrepresented as a sequence of tokens. Similar to PolyGen and\nCurveGen-TurtleGen, as stated in Eq. (2), the generative model\nof SketchGen is also decomposed into two parts by first\ngenerating the primitives p(P) and then generating constraints\nconditioned on primitives p(C|P)p(P) as follows:\np(S) = p(C|P)p(P).\n(3)\nTherefore, the generative network learns the distribution of\nconstraint sketches via two autoregressive Transformers, one\nfor primitive generation, and the other for conditional con-\nstraint generation. The sketch is parsed into a sequence\nof tokens, by initially iterating through all the primitives\nand expressing them using the language sequences explained\nabove. Subsequently, a similar process is applied to repre-\nsent all the constraints within the sketch. As illustrated in\nFigure 10, the input of the primitive generator network is a\nsequence of concatenated primitive tokens seperated by Λ,\nsuch as Λ, τ1, κ1, x1, y1, u1, v1, a1, b1, Λ, τ2, κ2, x2, y2, ..., Ω,\nand the input of the constraint generator is a sequence\nof concatenated constraint tokens seperated by Λ, like\nΛ, ν1, λ11, µ11, λ12, µ12, Λ, ν2, λ21, ....Ω.\nAll the primitive and constraint parameters are quantized\nfirst and then mapped by an embedding layer to make the\ninput feature vectors for the network. The positional infor-\nmation of the tokens in the sequence is also captured by\na positional encoding added to each embedding vector. The\nresulting sequences of tokens are then fed into the Transformer\narchitectures to generate primitives and constraints. The con-\nstraint generator network not only receives the embedded and\npositionally encoded tokens for constraints as input, but also it\nreceives the embedded and positionally encoded sequence of\ntokens that represents the primitives generated in the previous\nstep, to generate constraints conditioned on primitives. Similar\nto [20], this model is evaluated on SketchGraphs dataset for\nthe two tasks of constraint prediction given sketch primitives,\nand full sketch generation from scratch by generating both\nprimitives and constraints sequentially. The final generated\nsketch needs to be regularized by a constraint solver to\nremove the potential errors caused by quantizing the sketch\nparameters.\n4) CAD as Language [27]:\nConcurrent to CurveGen-\nTurtleGen and SketchGen, CAD as Language is another au-\ntoregressive Transformer method proposed for 2D sketch gen-\neration based on PolyGen [106]. Unlike CurveGen-TurtleGen\nthat predicts only the primitive parameters with implicit\nconstraints and independent of constraint solver, CAD as\nLanguage and SketchGen methods generate both primitives\nand constraints but are dependent to a built-in constraint\nsolver to obtain the final configuration of the sketch. However,\nSketchGen produces primivies and constrains via two separate\nTransformer networks while do not support arbitrary orderings\nof primitive and constraint tokens. In SketchGen, the sketch is\nparsed into a sequence of tokens, by initially iterating through\nall the primitives and then iterating through all the constraints.\nCAD as Language method, not only handles the arbitrary\norderings of primitives and constraints, but also generates\nboth primitives an constraint via one Transformer network.\nUnlike all the previous sketch generation methods which\nare evaluated on SketchGraphs dataset, CAD as Language\nevaluated its generative method on a new collection of over 4.7\nmillion sketches from the OnShape platform which avoids the\nproblem of data redundancy in SketchGraphs. This collected\ndataset and the corresponding processing pipeline are publicly\navailable on Github.5\nCAD as Language uses a method for describing structured\nobjects using Protocol Buffers (PB) [108], which demonstrates\nmore efficiency and flexibility for representing the precise\nstructure of complex objects than JSON format. In this format,\neach sketch is described as a PB message. Similar to other\nTransformer-based methods, the first and most important step\nin the processing pipeline is to parse sketches to a sequence\nof tokens. In this method, each sketch (or PB message) is\nrepresented as a sequence of triplets (di, ci, fi), where each\ntriplet with index i denotes a token. Each token (triplet)\nrepresents only one component (type or parameter) of a\nprimitive or constraint in a sketch, where di is a discrete value\ndenoting the type of object it is referring to, the type of entity\n(primitive, constraint, etc), ci is a continuous value denoting\nthe parameter value for the corresponding entity. At each time,\neither di or ci is active and gets a value, and the other one\nis set to zero. fi is a boolean flag specifying the end of a\nrepeated token (for example the end of an object containing\n5https://github.com/google-deepmind/deepmind-research/tree/master/cadl\n15\nPrimitive Generator\nConstraint Generator\nFig. 10: A simple illustration of the SketchGen generative approach, comprising two generative networks for primitive and\nconstraint generation. At each generation step, the network produces the next token based on both the input and the previously\ngenerated token. The start and end of a new primitive or constraint sequence are denoted by Λ and Ω, respectively. The figure\nshowcases the parameters of a line primitive and a parallelism constraint as an example. More details about the model structure\ncan be found in the original paper [26].\n(0, 0.0, False)\n(0, 0.0, False)\n(1, 0.0, False)\n(0, -0.8, False)\n(0, 2.6, False)\n(0, 1.1, False)\n(0, -3.7, False)\n(0, 0.0, False)\n(1, 0.0, False)\n(0, 0.0, False)\n(0, 1.1, False)\n(0, -3.7, False)\n(0, 0.0, True)\nLine\nPoint\nTriplet\nField\nobjects.kind\nentity.kind\nline.is_construction\npoint.is_construction\nentity.kind\nobjects.kind\nline.start.x\nline.start.y\nline.end.x\nline.end.y\npoint.x\npoint.y\nobjects.kind\nFig. 11: The description of a simple sketch, consisting of a line\nand one point on one of its ends, using the language structure\nproposed by [27]. The active element of each triplet (on left\nside) is specified in bold red color, and the corresponding field\nof the object for each triplet is shown on the right side.\nan entity). 11 shows an example of tokens specifying a line\nand a point primitive on one of its ends, respectively.\nAs it is shown in this example, the first triplet (objects.kind)\nis always associated with the type of the object the token\nreferring to. The values in the second triplet depends on the\ntype of object specified in the first triplet. As in this example,\nd1 = 0 shows that this sequence of tokens is about creating a\nprimitive (like a line), therefore the second triplet specifies\nthe type of the primitive (entity.kind) which is 0 for line,\nand 1 for point. The next triplets in the sequence specify\nthe specific parameters associated with the corresponding\nprimitive identified in the second triplet. For example, line\nprimitive is defined with a start and ending point, while the\npoint primitive, which is also repeated in the line primitive, is\ndefined with x, y coordinates.\nFor interpreting these triplets (tokens), CAD as Language\nmethod also proposed a custom interpreter which receives as\ninput a sequence of tokens, each representing a sketch com-\nponent (which can be an entity type, parameter or any other\ndesign step), and converts it into a valid PB message. This\ninterpreter is designed in a way to handle arbitrary orderings\nof tokens and make sure that all the token sequences can be\nconverted into a valid PB message (sketch). This interpreter\nguides the Transformer network through the sketch generation\nprocess. The Transformer network receives as input a sequence\nof tokens, and at each time step it outputs a raw value which\nis passed to the interpreter to infer the corresponding triplet\nfor that value. This triplet is a part of a PB message which\nmakes the final sketch. When the interpreter inferred the output\nvalue, it propagates its interpretation back to the Transformer\nto guide it through generating the next value. Therefore, the\nstructure this method proposed for parsing a sketch into tokens\nand interpreting them, enables it to generate every sketch com-\nponent (primitives and constraints) via only one Transformer\nnetwork, while handling different orderings of input tokens. A\nconditional variant of the proposed Transformer model is also\nexplored and evaluated in this method, where it is conditioned\non an input image of the sketch.\n5) VITRUVION [23]: This method is the latest autore-\ngressive generative model proposed for sketch generation.\nSimilar to SketchGen and CAD as Language, VITROVION\nalso generates both primitives and constraints autoregressively.\nHowever, this method is more similar to SketchGen in a sense\nthat primitives and constraints are generated independently\nthrough training two distinct Transformer networks. The major\ncontribution of this method compared to the previous ones,\nis conditioning the model on various contexts such as hand-\ndrawn sketches. This contribution is one step forward towards\nthe highly-sought feature in CAD softwares to reverse engineer\na mechanical part given a hand-drawing or a noisy scan of it.\nGenerating parametric primitives and constraints of a sketch\ngiven a hand-drawing or noisy image saves a lot of time and\neffort in designing process.\nSimilar to the previous methods, VITROVION is a general-\nization of PolyGen [106]. It generates propability distribution\nof sketches by first generating primitives P, and then generat-\ning constraints C conditioned on primitives. However, in this\nmethod, the primitive generation is optionally conditioned on\nan image follows:\np(P, C|I) = p(C|P)p(P|I),\n(4)\nwhere I is a context such as hand-drawn image of a sketch.\n16\nHowever, constraint modeling in this method only supports\nconstraints with one or two reference primitives, and not hy-\nperedges connecting more than two primitives as in CurveGen-\nTurtleGen method. The generative models are trained and\nevaluated on SketchGraphs dataset for autoconstraint, auto-\ncomplete, and conditional sketch synthesis tasks. As explained\nin V-A1, in autoconstraint application the network generates\nconstraints conditioning on a set of available primitives, but\nin autocomplete task, an incomplete sketch is completed by\ngenerating both primitives and constraints. In both cases,\nthe constraint generator network is conditioned on generated\nprimitives which can be imperfect. VITROVION increases\nthe robustness of the constraint generator network by con-\nditioning it on noise-injected primitives. The final primitive\nand constraint parameters are finally adjusted via a standard\nconstraint solver. For image-conditional sketch synthesis, the\nmodel infers primitives conditioning on a raster-image of a\nhand-drawn sketch. In this regard, an encoder network based\non Vision Transformer [109] architecture is used to obtain the\nembeddings of the image patches, and the primitive generator\nnetwork then cross-attends to these patch embeddings for\npredicting sketch primitives. This idea is based on a similar\nidea in PolyGen for image-conditioned mesh generation.\nThis method tokenizes a sketch by representing each prim-\nitive and constraint as a tuple of three tokens: value, ID,\nposition. The value token has two-folds, one indicating the\ntype of primitive or constraint and the other indicating the\nnumerical value of the associated parameter of that primitive or\nconstraint. ID token indicates the type of parameter specified\nby the value token, and position token indicates the ordered\nindex of the primitive or constraint to which this ID and value\ntokens belong to. The ordering of primitives are according\nto design steps indicated in SketchGraphs dataset, and the\nordering of constraints are according to the ordering of their\ncorresponding reference primitives.\nB. 3D CAD Generation from Sketch\n1) Sketch2CAD [24]: The first work proposed for sequen-\ntial 3D CAD modeling by interactive sketching in context is\nSketch2CAD. This work is a learning-based interactive model-\ning system that unifies CAD modeling and sketching by inter-\npreting the user’s input sketches as a sequence of parametric\n3D CAD operations. Given an existing incomplete 3D shape\nand input sketch strokes added on it by the user, Sketch2CAD\nfirst obtains the normal and depth maps of sketching local\ncontext which are introduced to the CAD operator classifica-\ntion and segmentation networks. Classification is done by a\nCNN network predicting the type of CAD operation needed\nfor creating the corresponding input sketch on the 3D shape. It\nreceives as input the concatenation of three maps, each of size\n256 × 256, i.e., the sketching map representing stroke pixels\nwith binary values, and normal and depth maps representing\nthe local context via rendering the shape through a specific\nviewpoint. It should be noted that only four types of CAD\noperations, which are the most widely used ones, are supported\nin this method, i.e., Extrude, Add/Subtract, Bevel, Sweep.\nAccording to the predicted operation type, the parameters of\nthe operator are regressed via specific segmentation networks.\nFor example, for the predicted Sweep operator, a SweepNet\nis trained to infer the corresponding parameters. Each of the\nfour segmentation networks have U-Net structure with one\nencoder and two decoders, one producing the probability map\nof base face for the sketch and the other one producing the\ncorresponding curve segmentation map. The segmentation is\nfollowed by an optimization process for fitting the operation\nparameters.\nThe parametric nature of the recognized operations provides\nstrong regularization to approximate input strokes and allows\nusers to refine the result by adjusting and regressing param-\neters. The modeling system also incorporates standard CAD\nmodeling features such as automatic snapping and autocom-\npletion. The output of the system is a series of CAD instruc-\ntions ready to be processed by downstream CAD tools. Since\nno dataset of paired sketches and 3D CAD modeling operation\nsequences exists, Sketch2CAD introduced a synthetic dataset\nof 40, 000 shapes for training and 10, 000 shapes for testing,\ncontaining step-by-step CAD operation sequences and their\ncorresponding sketch image renderings. The dataset, code, and\nvisual examples are publicly available.6\n2) Free2CAD [22]:\nOne of the main challenges of\nSketch2CAD is that it can only handle one CAD operation\nat a time, which means that it assumes that at each step,\nthe user drawing added to the shape corresponds to only\none CAD operation. It requires the user to draw the sketch\nsequentially and part-by-part so that it can be decomposed\ninto the meaningful CAD operations. Free2CAD is proposed to\naddress this restriction. This is the first sketch-based modeling\nmethod in which the user can input the complete drawing of a\ncomplex 3D shape without needing to have expert knowledge\nof how this sketch should be decomposed into the CAD\noperations or following a specific strategy for drawing the\nsketch and working with the system.\nFree2CAD is a sequence-to-sequence Transformer network\nwhich receives as input a sequence of user drawn strokes\ndepicting a 3D shape, processes and analyzes them to produce\na valid sequence of CAD operation commands, which may be\nexecuted to create the CAD model. The main contribution of\nthis method is the automatic grouping of the sketch strokes\nand the production of the parametric CAD operations for each\ngroup of strokes sequentially, conditioned on the groups that\nhave been reconstructed in the previous iterations. The method\nis comprised of two phases, namely stroke grouping phase and\noperation reconstruction phase. In the stroke grouping phase,\nfirst each sketch stroke is embedded as a token via a specially\ndesigned Transformer encoder network, then it is processed\nby the Transformer decoder network which produces group\nprobabilities for the input tokens. In this way, the sketch\nstrokes which might make a specific part of the shape are\ngrouped together. The most closely related work to the stroke\ngrouping phase is the SketchGNN [56] method which pro-\nposes a Graph Neural Network approach for freehand sketch\nsemantic segmentation. Next, in the operation reconstruction\nphase, the candidate groups are converted into geometric prim-\n6https://geometry.cs.ucl.ac.uk/projects/2020/sketch2cad/\n17\nitives with their corresponding parameters, conditioned on the\nexisting geometric context. This step is followed by geometric\nfitting and grouping correction before passing back the updated\ngroups as geometric context for the next iteration. At the end\nof this process, the desired CAD shape and the sequence of\nCAD commands are obtained. The method is also extended to\nhandle long stroke sequences making complex shapes using a\nsliding window scheme progressively outputting CAD models.\nSimilar to the Sketch2CAD [24], Free2CAD also provides\na large-scale synthetic dataset of 82, 000 paired CAD mod-\neling operation sequences and their corresponding rendered\nsketches which are segmented based their corresponding CAD\ncommands. The code and dataset of this method are publicly\navailable 7. The evaluation results of Free2CAD on both their\ngenerated dataset and on Fusion 360 dataset illustrate its\nhigh performance in processing different user drawings and\nproducing CAD commands which make desirable 3D shapes\nwhen executed by CAD tools.\n3) CAD2Sketch [28]: Unlike Sketch2CAD which tries to\nfacilitate the design process for non-expert users in an interac-\ntive modeling system, CAD2Sketch is designed to assist expert\nindustrial designers. As the name suggests, CAD2Sketch is a\nmethod dedicated to synthesizing concept sketches from CAD\nmodels. Concept sketching is a preliminary stage in CAD\nmodeling, wherein designers refine their mental conception\nof a 3D object from rough outlines to intricate details, often\nusing numerous construction lines. An example of a simple\nconcept sketch is depicted in 12, shown on the left side,\nalongside its refined version displayed on the right side.\nNotably, concept sketching mirrors the detailed steps in a\ndesigner’s mind, akin to the stages of CAD modeling, while\nsketches in the Sketch2CAD dataset typically present the final\nfree-hand sketch without these detailed auxiliary construction\nlines. CAD2Sketch introduced a large-scale synthetic dataset\nof concept sketches by proposing a method for converting\nCAD B-Rep data into concept sketches. CAD2Sketch es-\ntablishes a large-scale synthetic dataset of concept sketches\nby introducing an method to convert CAD B-Rep data into\nconcept sketches. The dataset is comparable to the OpenSketch\n[80] dataset which has 400 real concept sketches crafted\nby various expert designers. However, CAD2Sketch targets\nto bridge the gap between synthetic and real data, enabling\nthe training of neural networks on concept sketches. The\nCAD2Sketch method initially generates construction lines for\neach operation in a CAD sequence. To avoid overwhelming the\nsketch with too many lines, a subset of these lines is chosen\nby solving a binary optimization problem. Subsequently, the\nopacity and shape of each line are adjusted to achieve a visual\nresemblance to real concept sketches. The synthetic concept\nsketches produced by CAD2Sketch closely resemble their\ncorresponding real pairs in OpenSketch, so that even designers\ncan hardly distinguish between them [28]. CAD2Sketch also\ngenerates a substantial number of paired sketches and normal\nmaps, utilized for training a neural network to infer normal\nmaps from concept sketches. The dataset contains approx-\nimately 6, 000 paired concept sketches and normal maps.\n7https://geometry.cs.ucl.ac.uk/projects/2022/free2cad/\nThe trained neural network’s generalization to real shapes is\nevaluated on a test set of 108 CAD sequences from the ABC\ndataset, yielding promising results. However, it is worth noting\nthat this method is evaluated on a limited number of real\nsketches, given the relatively small size of the OpenSketch\ndataset. Additionally, the CAD2Sketch method, built on CAD\nsequences from existing large-scale CAD datasets, is limited\nto sequences composed of sketch and extrusion operations.\nC. 3D CAD Command Generation\nWhile\nprevious\napproaches\nhave\nintroduced\nsynthetic\ndatasets for 3D CAD reconstruction, the absence of a stan-\ndardized collection of human-designed 3D CAD models with\nretained CAD command sequences poses a limitation. Similar\nto the valuable contribution of the SketchGraphs dataset in\nthe area of 2D sketch synthesis, a curated dataset of human-\ndesigned 3D CAD models with preserved CAD command\nsequences would greatly benefit research and the development\nof practical methods for real-world applications. Fusion 360\nReconstruction dataset [30] fills this gap as the first human-\ndesigned dataset, featuring 8, 625 CAD models constructed\nusing a sequence of Sketch and Extrude CAD operations.\nAccompanying this dataset is an environment known as the\nFusion 360 Gym, capable of executing these CAD opera-\ntions. Each CAD model is represented as a Domain-Specific\nLanguage (DSL) program, a stateful language serving as a\nsimplified wrapper for the underlying Fusion 360 Python\nAPI. This language keeps track of the current geometry\nunder construction, updated iteratively through a sequence of\nsketch and extrude commands. The data and correponding\ncodes are publicly available 8. This dataset is benchmarked\nthrough the training and evaluation of a machine learning-\nbased approach featuring neurally guided search for program-\nmatic CAD reconstruction from a specified geometry. The\napproach begins by training a policy, which is instantiated as a\nMessage Passing Network (MPN) [110], [111] with an original\nencoding of state and action. This training is conducted\nthrough imitation learning, drawing insights from ground truth\nconstruction sequences. In the subsequent inference stage,\nthe method incorporates a search mechanism, utilizing the\nlearned neural policy to iteratively engage with the Fusion 360\nGym environment until a precise CAD program is discerned.\nHowever, a limitation of this method is its assumption that\nthe 2D geometry is given, with the method solely predicting\nthe sketches to be extruded and their extent at each iteration.\nSubsequently, this dataset has been utilized by generative\nmethods that tackle scenarios where the geometry is not\nprovided, and the entire 3D model needs to be synthesized\nfrom scratch using Sketch and Extrude operations.\nAs another effort to infer CAD modeling construction\nsequences through neural-guided search, a method in which\nthe B-Rep data of each CAD model is expressed in the form\nof a zone graph, where solid regions constitute the zones\nand surface patches (curves) form the edges is proposed in\n[29]. This representation is then introduced to a GCN to\nfacilitate feature learning and, subsequently, a search method is\n8https://github.com/AutodeskAILab/Fusion360GalleryDataset/blob/master/docs/reconstru\n18\nFig. 12: An illustration showcasing a conceptual sketch (left) designed for creating a 3D shape. Examples of these sketches\ncan be found in the CAD2Sketch dataset [28]. After refining the concept sketch, the final freehand sketch of the model is\nachieved (right). Refined sketches like these are available in the Sketch2CAD dataset [24].\nemployed to deduce a sequence of CAD operations that can\nfaithfully recreate this zone graph. However, none of these\ntwo aforementioned methods leverage generative methods for\nCAD operation generation. In this subsection, the most recent\ngenerative methods in this domain are introduced.\n1) DeepCAD [17]:\nThe first generative deep learning\nmodel proposed for creating 3D CAD commands is Deep-\nCAD. Given the sequential and irregular nature of CAD\noperations and B-Rep data, it is essential to choose a fixed\nset of the most commonly employed CAD operations and\norganize them into a unified structure for utilization by a\ngenerative neural network. Drawing inspiration from earlier\ngenerative techniques designed for 2D CAD analysis and\nsketch generation such as CurveGen-TurtleGen, SketchGen,\nCAD as Language, and VITROVION outlined in Section\nV-A, DeepCAD follows a similar approach by likening CAD\noperations to natural language. It introduces a generative\nTransformer network for autoencoding CAD operations. It is\nworth noting that DeepCAD diverges from previous generative\nmethods by adopting a feed-forward Transformer structure\ninstead of the autoregressive Transformer commonly used\nin such contexts. Additionally, DeepCAD has constructed\nand released a large-scale dataset featuring 178, 238 CAD\nmodels from OnShape repository generated through Sketch\nand Extrude operations, accompanied by their respective CAD\nconstruction sequences. This dataset surpasses the size of the\nFusion 360 Gallery dataset, which contained approximately\n8, 000 designs. The substantial increase in the number of\ndesigns within the DeepCAD dataset enhances its suitability\nfor training generative networks.\nIn the standardized structure suggested for CAD operations\nin DeepCAD, the CAD commands are explicitly detailed,\nproviding information on their type, parameters, and sequential\nindex. The Sketch commands encompass curves of types\nline, arc and circle along with their respective parameters.\nMeanwhile, Extrude commands signify extrusion operations of\ntypes one-sided, symmetric, two-sided and boolean operations\nof types new body, join, cut, or intersect. These operations\nare employed to integrate the modified shape with the pre-\nviously constructed form. A CAD model M is represented\nas a sequence of curve commands which build the sketch,\nintertwined with extrude commands. The total number of CAD\ncommands to represent each CAD model is fixed to 60 and\na padding approach with empty commands is used to fit the\nCAD models with shorter command sequence in this fixed-\nlength structure. Each command undergoes encoding into a 16-\ndimensional vector representing the whole set of parameters\nof all commands. In instances where specific parameters for a\ngiven command are not applicable, they are uniformly set to\n1. As illustrated in Figure 13, the autoencoder network takes a\nsequence of CAD commands as input, transforms them into a\nlatent space through Transformer encoder, and subsequently\ndecodes a latent vector to reconstruct a sequence of CAD\ncommands. The generated CAD commands can be imported\ninto CAD software for final user editing.\nThe performance of DeepCAD is evaluated on two tasks:\nCAD model autoencoding and random CAD model generation.\nOnce the autoencoder network is trained for reconstructing\nCAD commands, the latent-GAN technique [112] is employed\nto train a generator and a discriminator on the learned latent\nspace. The generator produces a latent vector z by receiving as\ninput a random vector sampled from a multivariate Gaussian\ndistribution. This latent vector can then be introduced to the\ntrained Transformer decoder to produce the CAD model com-\nmands. Their experiments also demonstrate that the pretrained\nmodel on the DeepCAD dataset exhibits good generalization\ncapabilities when applied to the Fusion 360 dataset. Notably,\nthese two datasets originate from distinct sources, namely\nOnShape and Autodesk Fusion 360.\nThe application of DeepCAD generative model is further\ndemonstrated in the conversion of 3D point cloud data into a\nCAD model. In this context, both the generative autoencoder\n19\nEncoder\nDecoder\nEmbedding\nCAD Command Sequence\nGenerated CAD Command\nSequence\nFeed Forward Embedding\nTransformer Encoder\nTransformer Decoder\nZ\nAverage Pooling\nLinear Embedding\nFig. 13: Schematic representation of the DeepCAD model architecture [17]. The model, depicted on the left, is a transformer\nautoencoder network trained in an unsupervised manner. It takes a sequence of CAD commands as input and reconstructs\nthem. The detailed structure of this autoencoder is shown on the right, where {C1, C2, ..., CN} tokens represent the input CAD\ncommand sequence, and {C1, C2, ..., CN} are the reconstructed commands.\nand the PointNet++ encoder [8] are trained concurrently to\nencode the CAD model into the same latent vector z from the\nCAD commands sequence and its corresponding point cloud\ndata, respectively. During inference, the pretrained PointNet++\n[8] encoder embeds the point cloud data into the latent vector\nz which is subsequently input to the generative decoder of\nthe pretrained DeepCAD autoencoder to produce the CAD\ncommands sequence.\n2) SkexGen [31]: Despite advancements achieved by Deep-\nCAD which excels in generating a diverse range of shapes, a\npersistent challenge remains in the limited user control over\nthe generated designs. The ability for users to exert influence\nover the output and tailor designs to meet specific requirements\nwould be a great advantage for the real world applications.\nIn response to this challenge, SkexGen proposed a novel au-\ntoregressive Transformer network with three separate encoders\ncapturing the topological, geometric and extrusion variations\nin the CAD command sequences separately. This approach\nenables more effective and distinct user control over the\ntopology and geometry of the model, facilitating exploration\nwithin a broader search space of related designs which in turn,\nresults in the generation of more realistic and diverse CAD\nmodels.\nInspired by CurveGen-TurtleGen [21] and DeepCAD [17]\nmethods, a CAD model in SkexGen is represented by a\nhierarchy of primitives with a sketch-and-extrude construction\nsequence. Within this hierarchy, a 3D model is composed\nof 3D solids, where each solid is defined as an extruded\nsketch. A sketch constitutes a collection of faces, and each face\nrepresents a 2D surface enclosed by a loop. A loop is formed\nby one or multiple curves, including lines, arcs or circles,\nand curve represents the fundamental level of the hierarchy.\nConsequently, CAD models are encoded using five types\nof tokens for input to the Transformer: 1) topology tokens,\ndenoting the curve type, 2) geometry tokens, specifying 2D\ncoordinates along the curves, 3) end of primitive tokens, 4) ex-\ntrusion tokens, indicating parameters of extrusion and Boolean\noperations, and 5) end of sequence tokens. The autoregressive\nTransformer network introduced by SkexGen is composed\nof two independent branches, each trained separately: 1) the\nSketch branch is composed of two distinct encoders dedi-\ncated to learning topological and geometrical variations in\nsketches. Additionally, a single decoder is employed, receiving\nconcatenated topology and geometry encoded codebooks as\ninput and predicting the sketch subsequence autoregressively.\n2) The “Extrude” branch comprises an encoder and decoder\n20\nspecifically designed to learn variations in extrusion and\nBoolean operations. Furthermore, an additional autoregressive\ndecoder is positioned at the end, tasked with learning an ef-\nfective combination of geometrical, topological, and extrusion\ncodebooks, thereby generating CAD construction sequences.\nAn additional autoregressive decoder on top learns an effec-\ntive combination of geometrical, topological and extrusion\ncodebooks as CAD construction sequences. This intricate\ntokenization and multifaceted network architecture allows for\na nuanced and comprehensive control over both topology\nand geometry, enabling the generation of CAD models by\neffectively capturing various design aspects.\nGiven capacity of SkexGen to independently encode and\ngenerate Sketch and Extrude command sequences, this ap-\nproach is versatile and can be applied to both 2D sketch\ngeneration and 3D CAD generation tasks. Evaluation results\nhighlight its proficiency in generating more intricate designs\ncompared to CurveGen-TurtleGen [21] and DeepCAD [17]\nmethods. Notably, SkexGen excels in supporting multi-step\nSketch and Extrude sequences, a capability lacking in Deep-\nCAD, which primarily produces single-step results.\nD. 3D CAD Generation with Direct B-Rep Synthesis\n1) SolidGen [15]: Creating 3D CAD designs through either\nthe generation of CAD commands or direct B-Rep synthesis\ncomes with a set of advantages and disadvantages. The genera-\ntive methods introduced in previous works, namely Fusion 360\nReconstruction, DeepCAD, and SkexGen, share a common\ngoal of producing 3D CAD models by generating sequences\nof 3D CAD operations or commands. These commands are\nthen processed by a solid modeling kernel in a CAD tool to\nrecover the final CAD design in B-Rep format. Generating\nCAD commands, as opposed to directly creating the B-Rep,\noffers several advantages. Converting CAD commands into\nB-Rep format is feasible, while the reverse process is more\nchallenging due to the potential ambiguity where different\ncommand sequences may result in the same B-Rep. Addition-\nally, CAD commands are more human-interpretable, enabling\nusers to edit designs for various applications by processing\nthe commands in a CAD tool. However, training models for\nsuch tasks necessitates large-scale CAD datasets that retain the\nhistory of CAD modeling operations. Consequently, datasets\nlike DeepCAD (comprising around 190,000 models) and Fu-\nsion 360 Reconstruction (with approximately 8,000 models)\nare exclusively constructed for this purpose. In contrast, most\nlarge-scale datasets in the field, such as ABC (with over 1\nmillion models), solely provide B-Rep data without a stored\nsequence of CAD modeling operations. While generating CAD\ncommands offers increased flexibility, interoperability, and\nuser control, an alternative strategy is to directly synthesize the\nB-Rep. This approach may prove beneficial when leveraging\nexisting large-scale datasets. Moreover, direct B-Rep synthesis\nallows for the support of more complex curves and surfaces in\ncreating 3D shapes. This stands in contrast to CAD command\ngenerative methods, which are constrained to CAD models\nconstructed using Sketch and Extrude commands with a lim-\nited list of supported curve types.\nSolidGen [15] introduces a method for direct B-Rep synthe-\nsis, eliminating the necessity for a history of CAD command\nsequences. The approach leverages pointer networks [107] and\nautoregressive Transformer networks to learn B-Rep topology\nand progressively predict vertices, edges, and faces individ-\nually. Running in parallel with SolidGen, the work by [113]\nis focused on 3D face identification within B-Rep data, given\na single 2D line drawing. Drawing inspiration from pointer\nnetworks, this approach also employs an autoregressive Trans-\nformer network to identify edge loops in the 2D line drawing,\nand predicts one co-edge index at a time, corresponding to the\nactual planar and cylindrical faces in the 3D design. However,\nSolidGen stands out as a more advantageous method. It goes\nbeyond edge loop identification and synthesizes the complete\nB-Rep data for the 3D shape. Additionally, it supports the\nrepresentation of all types of faces in the design, providing\na more comprehensive and versatile solution. SolidGen also\nintroduces the Indexed Boundary Representation (Indexed B-\nRep) to represent B-Reps as numeric arrays suitable for neural\nnetwork use. This indexed B-Rep organizes B-Rep vertices,\nedges, and faces in a clearly defined hierarchy to capture both\ngeometric and topological relations. Structurally, it consists of\nthree lists, denoted as V, E, F, representing Vertices, Edges,\nand Faces, respectively. In such hierarchical structure, edges\nE are denoted as list of indices referring to vertices V, and each\nface in F indicates an index list referring into edges E. The\nproposed autoregressive network progressively predicts the B-\nRep tokens through training three distinct Transformers for\ngenerating vertices, edges, and faces, respectively. Formally,\nits goal is to learn a joint distribution over B-Rep B:\np(B) = p(V, E, F),\n(5)\nfactorized as:\np(B) = p(F | E, V)p(E | V)p(V).\n(6)\nThis structure also allows for conditioning the distribution on\nan external context c, such as class labels, images, and voxels:\np(B) = p(F | E, V, c)p(E | V, c)p(V | c).\n(7)\nFollowing training, the generation of an Indexed B-Rep\ninvolves sampling vertices, edges conditioned on vertices, and\nfaces conditioned on both edges and vertices. Subsequently,\nthe obtained Indexed B-Rep can be transformed into the\nactual B-Rep through a post-processing step. For a more\ncomprehensive understanding of these processes, interested\nreaders are encouraged to refer to the original paper [15]. The\nefficacy of this method is assessed using a refined version of\nthe DeepCAD dataset. Additionally, SolidGen introduces the\nParametric Variations (PVar) dataset, purposefully designed\nfor evaluating the model’s performance in class-conditional\ngeneration tasks. This synthetic dataset comprises 120, 000\nCAD models distributed across 60 classes.\n2) CADParser [16]: In conjunction with SolidGen, CAD-\nParser has been introduced to predict the sequence of CAD\ncommands from a given B-Rep CAD model. Unlike previous\napproaches that often utilized synthetic CAD datasets or relied\non datasets like DeepCAD and Fusion 360 reconstruction,\n21\nwhich were restricted to CAD models created with only\ntwo operations, namely Sketch and Extrude, CADParser has\nintroduced a comprehensive dataset featuring 40, 000 CAD\nmodels. These models incorporate a broader range of CAD\noperations, including Sketch, Extrusion, Revolution, Fillet, and\nChamfer. This dataset provides a more diverse collection of\nCAD models constructed with five distinct types of CAD\noperations compared to previous datasets limited to only two\noperations. Each CAD model in this dataset is accompanied by\nboth B-Rep data and the corresponding construction command\nsequence.\nCADParser also introduces a deep neural network architec-\nture, referred to as the deep parser, designed to predict the\nCAD construction sequence for each B-Rep model. Drawing\ninspiration from UV-Net and BRepNet, both discussed in\nSec. IV as pioneering methods in representation learning\nfor B-Rep data, CADParser treats each CAD model as a\ngraph G = (V, E) where the nodes represent faces, edges,\nand coedges of the model, and E signifies the connections\nbetween the graph nodes. The BRepNet architecture serves\nas the graph encoder backbone, taking node features and the\nconstructed adjacency matrix as input, and extracting local\nand global features of the graph through graph convolutions\nand topological walks. Simultaneously, the sequence of CAD\ncommands S = (C1, C2, ..., Cn) that constructs the CAD\nmodel is encoded as feature vectors. These are combined\nwith the graph-encoded global and local features and fed\ninto a Transformer decoder to autoregressively predict the\nnext command sequence. Similar to previous methods, CAD\ncommands are tokenized by representing each command Ci\nas a tuple of command type ti and command parameter pi. ti\nis encoded as a 12-dimensional one-hot vector, representing\n12 different command types one at a time, and pi is a\n257-dimensional vector. This vector represents the quantized\n256-dimensional parameter vector of the command and a 1-\ndimensional index indicating whether this command is used\nfor the corresponding CAD model or not. The length of the\nCAD command sequence for all CAD models is fixed at 32\nfor simplicity, with unused commands for each CAD model\nindicated by an index set to −1. The Transformer decoder\nhas two separate output branches for predicting the CAD\ncommand type ti and parameter pi vectors, respectively. For\nmore details on the model architecture and training process,\nadditional information can be found in [16]. The contribution\nof this work represents a step forward in generating more di-\nverse CAD models by incorporating various CAD commands.\nE. 3D CAD Generation from Point Cloud\nAs highlighted in the overview of existing 3D CAD gen-\nerative models, the reverse engineering of CAD shapes has\nprimarily been explored through methods utilizing either CAD\nsketches, B-Rep data or sequences of CAD commands as\ninput. These approaches have made promising advances so\nfar in reconstructing and generating CAD models. However, an\nequally crucial aspect is generating CAD data from alternative\nraw geometric data modalities, such as point clouds. This\nconsideration also aligns with the future work outlined by the\nbarrel\nbase\nFig. 14: An example of Extrusion Cylinder. The 2D sketch is\na circle here (right) which is extruded to make the Extrusion\nCylinder (left) [32].\nDeepCAD method. The ability to seamlessly convert diverse\ndata representations into CAD models opens new avenues for\nreal-world applications, bridging the gap between different\ndata modalities and expanding the utility of CAD technol-\nogy. This becomes especially critical in scenarios where new\nvariations of a physical object are needed or when repairing\na machinery object without access to the corresponding CAD\nmodel. This can be particularly challenging in instances where\nthe object predates the digital era of manufacturing. In these\nscenarios, the process typically begins with scanning the object\nusing a 3D sensor, which generates a point cloud. Subse-\nquently, the acquired point cloud data needs to be decomposed\ninto a collection of geometric primitives, such as curves or\nsurfaces, to be interpreted by CAD software. The traditional\nthree-step procedure [114] involves converting the point cloud\ninto a mesh, explaining it through parametric surfaces to\ncreate a solid (B-Rep), and inferring a CAD program. Recent\nadvancements in fitting primitives to point clouds [115]–\n[117] have managed to bypass the initial step of converting\npoint clouds to meshes. Nevertheless, a notable limitation of\nthese methods is their reliance on a finite set of fixed and\ndisjoint primitives, which poses a challenge for convenient\nshape editing in subsequent steps. Addressing this, the recently\nproposed Point2Cyl [32] method frames the problem as an\nExtrusion Cylinder decomposition task, leveraging a neural\nnetwork to predict per-point extrusion instances, surface nor-\nmals, and base/barrel membership. These geometric proxies\ncan then be used to estimate the extrusion parameters through\ndifferentiable and closed-form formulations. In this method, an\nExtrusion Cylinder is considered a fundamental primitive that\nsignifies an extruded 2D sketch, characterized by parameters\nsuch as the extrusion axis, center, sketch, and sketch scale,\nwhich can be used to represent 3D CAD models. The terms\nbase and barrel are utilized to denote specific surfaces of\nan extrusion cylinder, representing the base/top plane and the\nside surface of the extrusion cylinder, respectively. Figure 14\nshows an example of Extrusion Cylinder which is obtained by\nextruding a circle as the base 2D sketch. Point2Cyl leverages\nPointNet++ [8] to learn point cloud feature embeddings, which\nare then passed into two distinct fully connected networks for\n22\npoint cloud segmentation into extrusion cylinder, base/barrel\nand surface normal prediction. The approach is evaluated on\nFusion Gallery and DeepCAD datasets, outperforming base-\nlines and showcasing its effectiveness in reconstruction and\nshape editing. The code of this method is publicly available.9\nNevertheless, it is important to note that this method is\nlimited in its ability to handle cases where the input data\nis not noisy or distorted. The task of reconstructing the\nsharp edges and surfaces in prismatic 3D shapes from noisy\npoint cloud data becomes challenging, given that point clouds\ninherently offer only an approximate representation of the 3D\nshape. This challenge is particularly pronounced when dealing\nwith point clouds acquired through low-cost scanners, where\nany distortions or irregularities present in the shape may be\naddressed by smoothing during the surface reconstruction pro-\ncess. Lambourne et al. [33] proposed a method concurrently\nwith Point2Cyl, addressing the reconstruction challenge of\nsharp prismatic shapes when provided with an approximate\nrounded (smoothed) point cloud. This approach introduces\na differentiable pipeline that reconstructs a target shape in\nterms of voxels while extracting geometric parameters. An\nautoencoder network is trained to process a signed distance\nfunction represented as a voxel grid, obtained through methods\nconverting dense point clouds into signed distance functions\n[118], [119]. The encoder produces an embedding vector, and\nthe decoders further decompose the shape into 2D profile\nimages and 1D envelope arrays. During inference, CAD data\nis generated by searching a repository of 2D constrained\nsketches, extruding them, and combine them via Boolean\noperations to construct the final CAD model. Evaluation on\nthe ABC dataset demonstrates the method’s ability to better\napproximate target shapes compared to DeepCAD method.\nConcurrent to the two previous works, ComplexGen [34]\nhas introduced a new approach, ComplexNet, for directly gen-\nerating B-Rep data from point clouds. This approach reframes\nthe reconstruction task as a holistic detection of geometric\nprimitives and their interconnections, encapsulated within a\nchain complex structure. ComplexNet, as a neural network\narchitecture, harnesses a sparse CNN for embedding point\ncloud features and a tri-path Transformer decoder to produce\nthree distinct groups of geometric primitives and their mutual\nrelationships defined as adjacency matrices. Subsequently, a\nglobal optimization step refines the predicted probabilistic\nstructure into a definite B-Rep chain complex, considering\nstructural validity constraints and geometric refinements. Ex-\ntensive experiments on the ABC dataset demonstrate the effec-\ntiveness of this approach in generating structurally complete\nand accurate CAD B-Rep models.\nVI. DISCUSSION AND FUTURE WORKS\nAlthough Geometric Deep Learning (GDL) methods have\nmade remarkable progress in analyzing CAD models and\nautomating the design process at different levels, several\nchallenges remain in this field. In this section, some of these\nchallenges are discussed and the potential future research\ndirections for tackling them are proposed.\n9point2cyl.github.io\nLimited Annotated B-Rep Data: Despite the release of\nseveral large-scale CAD datasets, which include B-Rep format\nalongside conventional 3D data formats in recent years, there\nremains a considerable demand for annotated datasets for\nsupervised learning-based approaches. The datasets annotated\nfor CAD model classification are still limited in size, lacking\nthe diversity and complexity needed for a comprehensive\nanalysis of CAD models. While there are large-scale annotated\ndatasets for common object CAD shapes, like ShapeNet [62],\nthese datasets are only available in mesh format. Consequently,\ntransferring knowledge from common object datasets to me-\nchanical datasets is only viable when using mesh or point\ncloud data, not B-Rep. On the other hand, annotating mechan-\nical CAD models is challenging and requires domain knowl-\nedge expertise to recognize the type, functionality, or other\nproperties of mechanical objects. Inconsistencies in expert\nannotations may arise due to different terminologies used for\nthese objects in different industries. Similar challenges exist\nfor datasets annotated for CAD segmentation into different\nfaces. As shown in Table III, there are a few annotated datasets\nin this regard with different types of annotation, such as how\ndifferent faces are manufactured or the CAD operation used\nto construct each face. These datasets lack shared classes in\ntheir surface types, making it challenging to transfer learned\nfeatures from one dataset to another using transfer learning or\ndomain generalization approaches. More extensive datasets,\nlike MFCAD++ extending the MFCAD dataset, are needed\nin this domain, with different annotation types. Collecting\nlarge-scale CAD datasets that provide various data formats,\nincluding mesh, point cloud, and B-Rep, featuring a diverse\nrange of CAD models with various complexities, and annotat-\ning them for different tasks, such as CAD classification and\nsegmentation, will significantly contribute to future research\nand development in this field.\nAnalysis on Complex CAD Assemblies: Existing works for\nanalyzing CAD assemblies are focusing on how to connect\nsimple parts (solids), predicting the joint coordinates and\ndirection of the connection. A future research direction could\nbe considering how to assemble multiple solids hierarchicaly\nto make a more complex CAD model, or how to connect\ntwo sub-assemblies, each containing multiple parts (solids),\nto make a bigger assembly. However, the existing datasets are\nnot annotated for such applications, since the joints between\ndifferent parts are not explicitly specified in the B-Rep data\nby default. The AutoMate and Fusion 360 Assembly-Joint\ndatasets are annotated specifically for their corresponding\nmethods [18], [19], respectively. Another open problem in\nanalyzing complex CAD assemblies is to segment each as-\nsembly into its building block in different levels (either sub-\nassembly or part (solid)). Learning how to replace a part in an\nassembly, while considering all its connections to other parts,\nits complexity, functionality, materials, etc, with another part\nin the repository would be a great advantage in automating\nCAD models customization process. Annotating datasets for\nthis goal is also necessary.\nRepresentation Learning on B-Rep Data: The initial and the\nmost important step in training deep learning models on B-Rep\ndata involves creating numerical feature representations in a\n23\nformat suitable for deep learning architectures. Many methods\nfor CAD classification and segmentation, particularly those\nbased on UV-Net, use UV-grid sampling. However, UV-grid\nfeatures lack permutation invariance. In other words, when\nthe CAD solid is represented as a face adjacency graph,\nthe arrangement of nodes in the graph is crucial for the\ndeep learning model to recognize the graph structure and\nits similarity to other graphs (or CAD solids), while two\ngraphs with different node arrangements might still be similar\ntogether. Hence, exploring various invariances of the B-Rep\ngraph and alternative methods of representing its data in a\nstructured format, while considering the relative orientation of\nthe solid faces and edges, holds promise for future research.\nUnsupervised and Self-Supervised Methods: Given the\nshortage of annotated data for supervised learning in CAD,\nthere is significant potential to enhance self-supervised and/or\nunsupervised methods, leveraging large-scale datasets like\nABC. For instance, UV-Net utilized Graph Contrastive Learn-\ning (GCL) for self-supervised learning, and [13] explored\nshape rasterization to get the self-supervision from data for\ntraining an autoencoder and use the decoder for other super-\nvised tasks. Investigating diverse forms of self-supervision in\nCAD data for training autoencoders or graph autoencoders\nwould be intriguing. Examining and assessing different graph\ntransformations in GCL approaches to understand the structure\nof B-Rep graph locally and globally, maintaining various\ninvariances, is a crucial avenue for future research. Addition-\nally, focusing on Variational Autoencoders for direct synthesis\non B-Rep data, generating meaningful new shapes from the\nsame distribution as existing shapes, would enhance similarity\nanalysis and CAD retrieval.\nCAD Generation and B-Rep Synthesis: The current genera-\ntive methods for CAD commands often focus on a limited set\nof operations, like Sketch and Extrude, restricting the complex-\nity and diversity of the resulting CAD models. Notably, there\nis no assurance in certain methods, such as DeepCAD [17],\nthat all generated CAD command sequences will yield topo-\nlogically valid CAD models, particularly in cases of complex\nmodels with long command sequences. Therefore, an avenue\nfor future exploration involves expanding generative methods\nto encompass a broader range of CAD operations, such as fillet\nand chamfer, allowing the generation of command sequences\nfor more complex CAD shapes. Additionally, advancements\nin methods like SolidGen [15] and CADParser [16], which\ndirectly focus on synthesizing B-Rep data without relying\non CAD command sequences, mark a promising direction.\nDespite these advancements, there is still room for creativity\nand improvement in this area. Another direction for future\nresearch involves extending generative methods to produce\nCAD construction operations or B-Rep synthesis from other\n3D data formats, like mesh, voxel and point cloud. Recent\nprogress in converting point clouds to CAD models, described\nin Section V-E, opens possibilities for transferring knowledge\nacross different data domains. Moreover, generating 3D CAD\nmodels from noisy scans or hand-drawn sketches is in high\ndemand in CAD tools and presents significant potential for\nimprovement. Although some methods have been introduced\nin this domain, the lack of a paired sketch and B-Rep dataset\nmakes training supervised learning methods currently infeasi-\nble. For example, the sketches of SketchGraphs and 3D models\nin ABC datasets are both collected from Onshape repository.\nHowever, the 2D and 3D models in these two datasets are\nnot paired. Collecting such a paired dataset would greatly aid\nfuture research.\nReproducibility: In this domain, a significant challenge lies\nin reproducing and comparing the experimental results of\ndifferent methodologies. The absence of large-scale anno-\ntated benchmark datasets leads each method proposed for\nmachine learning-based CAD analysis to either introduce a\nnew annotated CAD dataset tailored for the specific task\nor modify and annotate a portion of a large-scale dataset\nfor evaluation. Supervised methods trained on these smaller\ndatasets often exhibit high performance, leaving little room for\nimprovement. Moreover, since each method is benchmarked\non a dataset adapted for its specific task, comparing results\nbecomes complex. For instance, when analyzing the results\nof different methods evaluated on a subset of the Fusion\n360 segmentation dataset in [13], it is unclear which part\nof the dataset each method used for evaluation. Reproducing\nthe results of different methods and conducting performance\ncomparisons become especially challenging when researchers\ndo not release their preprocessed data or do not provide\nclear preprocessing instructions. Another hurdle in replicating\nresearch results is the reliance of codes on CAD kernels\nthat are not open-source. For instance, replicating the results\nof AutoMate [19] requires installing the Parasolid kernel,\nwhich is typically accessible only to industrial developers and\ncompanies. This limited availability makes it challenging for\nindependent academic researchers to utilize and build upon\nsuch research works. While codes dependent on the open-\nsource OpenCASCADE kernel are more accessible, many\nmethods in this field either use constraint solvers through CAD\ntools during training or are in a way reliant on a CAD tool\nkernel, necessitating access and licensing for at least one CAD\nsoftware. To address these issues, it is highly recommended\nthat researchers provide comprehensive documentation for\ntheir released codes, detailing the data preprocessing setup\nand offering sufficient information on the experimental setting\nand code dependencies. This transparency can significantly\nfacilitate future research efforts in the field.\nVII. CONCLUSION\nDeveloping and training Geometric Deep Learning (GDL)\nmodels to learn and reason about CAD designs holds the\npromise of revolutionizing design workflows, bringing in\ngreater efficiency. However, extending machine learning-based\nmethods to complex parametric data, like B-Rep, poses a\ncrucial and challenging task. This survey offers a comprehen-\nsive review of GDL methods tailored for CAD data analysis.\nIt presents detailed explanation, comparisons, and summaries\nwithin two primary categories: 1) CAD representation learn-\ning, encompassing supervised and self-supervised methods\ndesigned for CAD classification, segmentation, assembly, and\nretrieval, and 2) CAD generation, involving generative meth-\nods for 2D and 3D CAD construction. Additionally, bench-\nmark datasets and open-source codes are introduced. The\n24\nsurvey concludes by discussing the challenges inherent in this\nrapidly evolving field and proposes potential avenues for future\nresearch.\nACKNOWLEDGEMENT\nThe research leading to the results of this paper received\nfunding from the Thomas B. Thriges Foundation and the\nIndustriens Foundation as part of the AI Supported Modular\nDesign and Implementation project.\nREFERENCES\n[1] M. M. Bronstein, J. Bruna, T. Cohen, and P. Veliˇckovi´c, “Geometric\ndeep learning: Grids, groups, graphs, geodesics, and gauges,” arXiv\npreprint arXiv:2104.13478, 2021.\n[2] E. Kalogerakis, M. Averkiou, S. Maji, and S. Chaudhuri, “3d shape\nsegmentation with projective convolutional networks,” in IEEE confer-\nence on computer vision and pattern recognition, 2017.\n[3] Y. Feng, Y. Feng, H. You, X. Zhao, and Y. Gao, “Meshnet: Mesh neural\nnetwork for 3d shape representation,” in AAAI conference on artificial\nintelligence, 2019.\n[4] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural\nnetwork for real-time object recognition,” in IEEE International Con-\nference on Intelligent Robots and Systems (IROS), 2015.\n[5] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao,\n“3d shapenets: A deep representation for volumetric shapes,” in IEEE\nconference on computer vision and pattern recognition, 2015.\n[6] C. Wang, M. Cheng, F. Sohel, M. Bennamoun, and J. Li, “Normalnet:\nA voxel-based cnn for 3d object classification and retrieval,” Neuro-\ncomputing, vol. 323, pp. 139–147, 2019.\n[7] T. Le and Y. Duan, “Pointgrid: A deep network for 3d shape un-\nderstanding,” in IEEE conference on computer vision and pattern\nrecognition, 2018.\n[8] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on\npoint sets for 3d classification and segmentation,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2017.\n[9] P. K. Jayaraman, A. Sanghi, J. G. Lambourne, K. D. Willis, T. Davies,\nH. Shayani, and N. Morris, “Uv-net: Learning from boundary rep-\nresentations,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2021.\n[10] C. Krahe, A. Br¨aunche, A. Jacob, N. Stricker, and G. Lanza, “Deep\nlearning for automated product design,” CIRP Design Conference,\n2020.\n[11] C. Krahe, M. Marinov, T. Schmutz, Y. Hermann, M. Bonny, M. May,\nand G. Lanza, “Ai based geometric similarity search supporting com-\nponent reuse in engineering design,” CIRP Design Conference, 2022.\n[12] D. Machalica and M. Matyjewski, “Cad models clustering with ma-\nchine learning,” Archive of Mechanical Engineering, vol. 66, no. 2,\n2019.\n[13] B. T. Jones, M. Hu, M. Kodnongbua, V. G. Kim, and A. Schulz, “Self-\nsupervised representation learning for cad,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2023.\n[14] J. G. Lambourne, K. D. Willis, P. K. Jayaraman, A. Sanghi, P. Meltzer,\nand H. Shayani, “Brepnet: A topological message passing system for\nsolid models,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2021.\n[15] P. K. Jayaraman, J. G. Lambourne, N. Desai, K. D. D. Willis,\nA. Sanghi, and N. J. W. Morris, “Solidgen: An autoregressive model for\ndirect b-rep synthesis,” Transactions on Machine Learning Research,\n2023.\n[16] S. Zhou, T. Tang, and B. Zhou, “Cadparser: a learning approach of\nsequence modeling for b-rep cad,” in International Joint Conference\non Artificial Intelligence, 2023.\n[17] R. Wu, C. Xiao, and C. Zheng, “Deepcad: A deep generative network\nfor computer-aided design models,” in IEEE International Conference\non Computer Vision, 2021.\n[18] K. D. Willis, P. K. Jayaraman, H. Chu, Y. Tian, Y. Li, D. Grandi,\nA. Sanghi, L. Tran, J. G. Lambourne, A. Solar-Lezama et al., “Joinable:\nLearning bottom-up assembly of parametric cad joints,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2022.\n[19] B. Jones, D. Hildreth, D. Chen, I. Baran, V. G. Kim, and A. Schulz,\n“Automate: A dataset and learning approach for automatic mating of\ncad assemblies,” ACM Transactions on Graphics (TOG), vol. 40, no. 6,\npp. 1–18, 2021.\n[20] A. Seff, Y. Ovadia, W. Zhou, and R. P. Adams, “Sketchgraphs: A\nlarge-scale dataset for modeling relational geometry in computer-aided\ndesign,” arXiv preprint arXiv:2007.08506, 2020.\n[21] K. D. Willis, P. K. Jayaraman, J. G. Lambourne, H. Chu, and Y. Pu,\n“Engineering sketch generation for computer-aided design,” in IEEE\nConference on Computer Vision and Pattern Recognition, 2021.\n[22] C. Li, H. Pan, A. Bousseau, and N. J. Mitra, “Free2cad: Parsing free-\nhand drawings into cad commands,” ACM Transactions on Graphics\n(TOG), vol. 41, no. 4, pp. 1–16, 2022.\n[23] A. Seff, W. Zhou, N. Richardson, and R. P. Adams, “Vitruvion:\nA generative model of parametric CAD sketches,” in International\nConference on Learning Representations, ICLR, 2022.\n[24] C. Li, H. Pan, A. Bousseau, and N. J. Mitra, “Sketch2cad: Sequential\ncad modeling by sketching in context,” ACM Transactions on Graphics\n(TOG), vol. 39, no. 6, pp. 1–14, 2020.\n[25] E. Dupont, K. Cherenkova, A. Kacem, S. A. Ali, I. Arzhannikov,\nG. Gusev, and D. Aouada, “Cadops-net: Jointly learning cad opera-\ntion types and steps from boundary-representations,” in International\nConference on 3D Vision (3DV).\nIEEE, 2022.\n[26] W. Para, S. Bhat, P. Guerrero, T. Kelly, N. Mitra, L. J. Guibas, and\nP. Wonka, “Sketchgen: Generating constrained cad sketches,” Advances\nin Neural Information Processing Systems, vol. 34, pp. 5077–5088,\n2021.\n[27] Y. Ganin, S. Bartunov, Y. Li, E. Keller, and S. Saliceti, “Computer-\naided design as language,” Advances in Neural Information Processing\nSystems, vol. 34, pp. 5885–5897, 2021.\n[28] F. H¨ahnlein, C. Li, N. J. Mitra, and A. Bousseau, “Cad2sketch:\nGenerating concept sketches from cad sequences,” ACM Transactions\non Graphics (TOG), vol. 41, no. 6, pp. 1–18, 2022.\n[29] X. Xu, W. Peng, C.-Y. Cheng, K. D. Willis, and D. Ritchie, “Inferring\ncad modeling sequences using zone graphs,” in IEEE conference on\ncomputer vision and pattern recognition, 2021.\n[30] K. D. Willis, Y. Pu, J. Luo, H. Chu, T. Du, J. G. Lambourne,\nA. Solar-Lezama, and W. Matusik, “Fusion 360 gallery: A dataset and\nenvironment for programmatic cad construction from human design\nsequences,” ACM Transactions on Graphics (TOG), vol. 40, no. 4, pp.\n1–24, 2021.\n[31] X. Xu, K. D. D. Willis, J. G. Lambourne, C. Cheng, P. K. Jayara-\nman, and Y. Furukawa, “Skexgen: Autoregressive generation of CAD\nconstruction sequences with disentangled codebooks,” in International\nConference on Machine Learning, 2022.\n[32] M. A. Uy, Y.-Y. Chang, M. Sung, P. Goel, J. G. Lambourne, T. Birdal,\nand L. J. Guibas, “Point2cyl: Reverse engineering 3d objects from point\nclouds to extrusion cylinders,” in IEEE Conference on Computer Vision\nand Pattern Recognition, 2022.\n[33] J. G. Lambourne, K. Willis, P. K. Jayaraman, L. Zhang, A. Sanghi, and\nK. R. Malekshan, “Reconstructing editable prismatic cad from rounded\nvoxel models,” in SIGGRAPH Asia 2022 Conference Papers, 2022.\n[34] H. Guo, S. Liu, H. Pan, Y. Liu, X. Tong, and B. Guo, “Complexgen:\nCad reconstruction by b-rep chain complex generation,” ACM Trans-\nactions on Graphics (TOG), vol. 41, no. 4, pp. 1–18, 2022.\n[35] M. Groover and E. Zimmers, CAD/CAM: computer-aided design and\nmanufacturing.\nPearson Education, 1983.\n[36] Autodesk, “Autocad,” 1982. [Online]. Available: https://www.autodesk.\ncom/products/autocad/overview?term=1-YEAR&tab=subscription\n[37] ——, “Fusion 360,” 2013. [Online]. Available: https://www.autodesk.\ncom/products/fusion-360/overview?term=1-YEAR&tab=subscription\n[38] Dassault Syst`emes, “Solidworks,” 1995. [Online]. Available: https:\n//www.3ds.com/products/solidworks\n[39] ——,\n“Catia,”\n1977.\n[Online].\nAvailable:\nhttps://www.3ds.com/\nproducts-services/catia/\n[40] PTC, “Onshape,” 2015. [Online]. Available: https://www.onshape.com/\nen/\n[41] ——, “Creo,” 2011. [Online]. Available: https://www.ptc.com/en/\nproducts/creo\n[42] Kai\nBackman,\nMikko\nMononen,\n“Tinkercad,”\n2011.\n[Online].\nAvailable: https://www.tinkercad.com\n[43] Thomas Paviot, “pythonocc-core,” 2022. [Online]. Available: https:\n//doi.org/10.5281/zenodo.3605364\n[44] “Opencascade technology (occt),” 2002. [Online]. Available: https:\n//github.com/AutodeskAILab/occwl\n[45] Pradeep Kumar Jayaraman, Joseph Lambourne, “Occwl,” 2023.\n[Online]. Available: https://dev.opencascade.org\n[46] “Cadquery,” 2019. [Online]. Available: https://github.com/CadQuery/\ncadquery/tree/master\n[47] “Parasolid,” 1980. [Online]. Available: https://www.plm.automation.\nsiemens.com/global/en/products/plm-components/parasolid.html\n25\n[48] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,\n“The graph neural network model,” IEEE transactions on neural\nnetworks, vol. 20, no. 1, pp. 61–80, 2008.\n[49] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, “A\ncomprehensive survey on graph neural networks,” IEEE transactions\non neural networks and learning systems, vol. 32, no. 1, pp. 4–24,\n2020.\n[50] N. Heidari and A. Iosifidis, “Temporal attention-augmented graph\nconvolutional network for efficient skeleton-based human action recog-\nnition,” in International Conference on Pattern Recognition (ICPR).\nIEEE, 2021.\n[51] ——, “Progressive spatio-temporal graph convolutional network for\nskeleton-based human action recognition,” in IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP), 2021.\n[52] L. Hedegaard, N. Heidari, and A. Iosifidis, “Continual spatio-temporal\ngraph convolutional networks,” Pattern Recognition, vol. 140, p.\n109528, 2023.\n[53] E. Mansimov, O. Mahmood, S. Kang, and K. Cho, “Molecular geome-\ntry prediction using a deep generative graph neural network,” Scientific\nreports, vol. 9, no. 1, p. 20381, 2019.\n[54] Y. Wang, J. Wang, Z. Cao, and A. Barati Farimani, “Molecular\ncontrastive learning of representations via graph neural networks,”\nNature Machine Intelligence, vol. 4, no. 3, pp. 279–287, 2022.\n[55] K. Baltakys, M. Baltakien˙e, N. Heidari, A. Iosifidis, and J. Kanniainen,\n“Predicting the trading behavior of socially connected investors: Graph\nneural network approach with implications to market surveillance,”\nExpert Systems with Applications, vol. 228, p. 120285, 2023.\n[56] L. Yang, J. Zhuang, H. Fu, X. Wei, K. Zhou, and Y. Zheng, “Sketchgnn:\nSemantic sketch segmentation with graph neural networks,” ACM\nTransactions on Graphics (TOG), vol. 40, no. 3, pp. 1–13, 2021.\n[57] T. N. Kipf and M. Welling, “Semi-supervised classification with graph\nconvolutional networks,” in International Conference on Learning\nRepresentations, 2017.\n[58] N. Heidari and A. Iosifidis, “Progressive graph convolutional networks\nfor semi-supervised node classification,” IEEE Access, vol. 9, pp.\n81 957–81 968, 2021.\n[59] N. Heidari, L. Hedegaard, and A. Iosifidis, “Graph convolutional\nnetworks,” in Deep Learning for Robot Perception and Cognition.\nElsevier, 2022, pp. 71–99.\n[60] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning\ndeep generative models of graphs,” arXiv preprint arXiv:1803.03324,\n2018.\n[61] P. Shilane, P. Min, M. Kazhdan, and T. Funkhouser, “The princeton\nshape benchmark,” in Proceedings Shape Modeling Applications, 2004.\nIEEE, 2004, pp. 167–178.\n[62] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su et al., “Shapenet:\nAn\ninformation-rich\n3d\nmodel\nrepository,”\narXiv\npreprint\narXiv:1512.03012, 2015.\n[63] Q. Zhou and A. Jacobson, “Thingi10k: A dataset of 10,000 3d-printing\nmodels,” arXiv preprint arXiv:1605.04797, 2016.\n[64] K. Mo, S. Zhu, A. X. Chang, L. Yi, S. Tripathi, L. J. Guibas, and H. Su,\n“Partnet: A large-scale benchmark for fine-grained and hierarchical\npart-level 3d object understanding,” in IEEE Conference on Computer\nVision and Pattern Recognition, 2019.\n[65] F. Bogo, J. Romero, G. Pons-Moll, and M. J. Black, “Dynamic faust:\nRegistering human bodies in motion,” in IEEE conference on computer\nvision and pattern recognition, 2017, pp. 6233–6242.\n[66] M. Aubry, D. Maturana, A. A. Efros, B. C. Russell, and J. Sivic,\n“Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large\ndataset of cad models,” in IEEE conference on computer vision and\npattern recognition, 2014, pp. 3762–3769.\n[67] S. Song, S. P. Lichtenberg, and J. Xiao, “Sun rgb-d: A rgb-d scene\nunderstanding benchmark suite,” in IEEE conference on computer\nvision and pattern recognition, 2015.\n[68] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and\nM. Nießner, “Scannet: Richly-annotated 3d reconstructions of indoor\nscenes,” in IEEE conference on computer vision and pattern recogni-\ntion, 2017.\n[69] S. Jayanti, Y. Kalyanaraman, N. Iyer, and K. Ramani, “Developing\nan engineering shape benchmark for cad models,” Computer-Aided\nDesign, vol. 38, no. 9, pp. 939–953, 2006.\n[70] S. Kim, H.-g. Chi, X. Hu, Q. Huang, and K. Ramani, “A large-scale\nannotated mechanical components benchmark for classification and\nretrieval tasks with deep neural networks,” in European Conference\non Computer Vision.\nSpringer, 2020.\n[71] D. Bespalov, C. Y. Ip, W. C. Regli, and J. Shaffer, “Benchmarking cad\nsearch techniques,” in ACM symposium on Solid and physical modeling,\n2005.\n[72] Z. Zhang, P. Jaiswal, and R. Rai, “Featurenet: Machining feature\nrecognition based on 3d convolution neural network,” Computer-Aided\nDesign, vol. 101, pp. 12–22, 2018.\n[73] B. Manda, P. Bhaskare, and R. Muthuganapathy, “A convolutional\nneural network approach to the classification of engineering models,”\nIEEE Access, vol. 9, pp. 22 711–22 723, 2021.\n[74] A. Angrish, B. Craver, and B. Starly, ““fabsearch”: A 3d cad model-\nbased search engine for sourcing manufacturing services,” Journal of\nComputing and Information Science in Engineering, vol. 19, no. 4, p.\n041006, 2019.\n[75] L. Mandelli and S. Berretti, “Cad 3d model classification by graph\nneural networks: A new approach based on step format,” arXiv preprint\narXiv:2210.16815, 2022.\n[76] W. Cao, T. Robinson, Y. Hua, F. Boussuge, A. R. Colligan, and\nW. Pan, “Graph representation of 3d cad models for machining feature\nrecognition with deep learning,” in International Design Engineering\nTechnical Conferences and Computers and Information in Engineering\nConference, 2020.\n[77] S. Koch, A. Matveev, Z. Jiang, F. Williams, A. Artemov, E. Burnaev,\nM. Alexa, D. Zorin, and D. Panozzo, “Abc: A big cad model dataset\nfor geometric deep learning,” in IEEE Conference on Computer Vision\nand Pattern Recognition, 2019.\n[78] M. Eitz, J. Hays, and M. Alexa, “How do humans sketch objects?”\nACM Transactions on graphics (TOG), vol. 31, no. 4, pp. 1–10, 2012.\n[79] P. Sangkloy, N. Burnell, C. Ham, and J. Hays, “The sketchy database:\nlearning to retrieve badly drawn bunnies,” ACM Transactions on\nGraphics (TOG), vol. 35, no. 4, pp. 1–12, 2016.\n[80] Y. Gryaditskaya, M. Sypesteyn, J. W. Hoftijzer, S. C. Pont, F. Durand,\nand A. Bousseau, “Opensketch: a richly-annotated dataset of product\ndesign sketches.” ACM Transactions on Graphics (TOG), vol. 38, no. 6,\npp. 232–1, 2019.\n[81] A. R. Colligan, T. T. Robinson, D. C. Nolan, Y. Hua, and W. Cao,\n“Hierarchical cadnet: Learning from b-reps for machining feature\nrecognition,” Computer-Aided Design, vol. 147, p. 103226, 2022.\n[82] T. G. Gunn, “The mechanization of design and manufacturing,” Scien-\ntific American, vol. 247, no. 3, pp. 114–131, 1982.\n[83] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph con-\ntrastive learning with augmentations,” Advances in neural information\nprocessing systems, vol. 33, pp. 5812–5823, 2020.\n[84] M. Al-Wswasi, A. Ivanov, and H. Makatsoris, “A survey on smart\nautomated computer-aided process planning (acapp) techniques,” The\nInternational Journal of Advanced Manufacturing Technology, vol. 97,\npp. 809–832, 2018.\n[85] Y. Shi, Y. Zhang, K. Xia, and R. Harik, “A critical review of feature\nrecognition techniques,” Computer-Aided Design and Applications,\nvol. 17, no. 5, pp. 861–899, 2020.\n[86] G. Zhan, Q. Fan, K. Mo, L. Shao, B. Chen, L. J. Guibas, H. Dong et al.,\n“Generative 3d part assembly via dynamic graph learning,” Advances\nin Neural Information Processing Systems, vol. 33, pp. 6315–6326,\n2020.\n[87] H. Lin, M. Averkiou, E. Kalogerakis, B. Kovacs, S. Ranade, V. Kim,\nS. Chaudhuri, and K. Bala, “Learning material-aware local descriptors\nfor 3d shapes,” in International Conference on 3D Vision (3DV). IEEE,\n2018.\n[88] K. Mo, P. Guerrero, L. Yi, H. Su, P. Wonka, N. Mitra, and L. J. Guibas,\n“Structurenet: hierarchical graph networks for 3d shape generation,”\nACM Transactions on Graphics (TOG), vol. 38, no. 6, pp. 242:1–\n242:19, 2019.\n[89] R. K. Jones, T. Barton, X. Xu, K. Wang, E. Jiang, P. Guerrero, N. J.\nMitra, and D. Ritchie, “Shapeassembly: Learning to generate programs\nfor 3d shape structure synthesis,” ACM Transactions on Graphics\n(TOG), vol. 39, no. 6, pp. 1–20, 2020.\n[90] Z. Wu, X. Wang, D. Lin, D. Lischinski, D. Cohen-Or, and H. Huang,\n“Sagnet: Structure-aware generative network for 3d-shape modeling,”\nACM Transactions on Graphics (TOG), vol. 38, no. 4, pp. 1–14, 2019.\n[91] K. Yin, Z. Chen, S. Chaudhuri, M. Fisher, V. G. Kim, and H. Zhang,\n“Coalesce: Component assembly by learning to synthesize connec-\ntions,” in International Conference on 3D Vision (3DV).\nIEEE, 2020.\n[92] C. Zhu, K. Xu, S. Chaudhuri, R. Yi, and H. Zhang, “Scores: Shape\ncomposition with recursive substructure priors,” ACM Transactions on\nGraphics (TOG), vol. 37, no. 6, pp. 1–14, 2018.\n[93] A. N. Harish, R. Nagar, and S. Raman, “Rgl-net: A recurrent graph\nlearning framework for progressive part assembly,” in IEEE Winter\nConference on Applications of Computer Vision (WACV), 2022.\n26\n[94] Y. Lee, E. S. Hu, and J. J. Lim, “Ikea furniture assembly environment\nfor long-horizon complex manipulation tasks,” in IEEE International\nConference on Robotics and Automation (icra).\nIEEE, 2021.\n[95] M. Sung, H. Su, V. G. Kim, S. Chaudhuri, and L. Guibas, “Comple-\nmentme: Weakly-supervised component suggestions for 3d modeling,”\nACM Transactions on Graphics (TOG), vol. 36, no. 6, pp. 1–12, 2017.\n[96] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, and K. Xu,\n“Shape2motion: Joint analysis of motion parts and attributes from\n3d shapes,” in IEEE Conference on Computer Vision and Pattern\nRecognition, 2019.\n[97] A. Zhao, J. Xu, M. Konakovi´c-Lukovi´c, J. Hughes, A. Spielberg,\nD. Rus, and W. Matusik, “Robogrammar: graph grammar for terrain-\noptimized robot design,” ACM Transactions on Graphics (TOG),\nvol. 39, no. 6, pp. 1–16, 2020.\n[98] F. Boussuge, C. M. Tierney, T. T. Robinson, and C. G. Armstrong,\n“Application of tensor factorisation to analyze similarities in cad\nassembly models,” in International Meshing Roundtable and User\nForum, vol. 1, 2019.\n[99] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, “Dynamic graph cnn for learning on point clouds,” ACM\nTransactions on Graphics (tog), vol. 38, no. 5, pp. 1–12, 2019.\n[100] P. Guerrero, Y. Kleiman, M. Ovsjanikov, and N. J. Mitra, “Pcpnet\nlearning local shape properties from raw point clouds,” in Computer\ngraphics forum, vol. 37, no. 2. Wiley Online Library, 2018, pp. 75–85.\n[101] S. Brody, U. Alon, and E. Yahav, “How attentive are graph attention\nnetworks?” in International Conference on Learning Representations,\nICLR, 2022.\n[102] T. Funkhouser, M. Kazhdan, P. Shilane, P. Min, W. Kiefer, A. Tal,\nS. Rusinkiewicz, and D. Dobkin, “Modeling by example,” ACM Trans-\nactions on Graphics (TOG), vol. 23, no. 3, pp. 652–663, 2004.\n[103] D. Ha and D. Eck, “A neural representation of sketch drawings,” in\nInternational Conference on Learning Representations, ICLR, 2018.\n[104] J. Jongejan, H. Rowley, T. Kawashima, J. Kim, and N. Fox-Gieg, “The\nquick, draw!-ai experiment,” Mount View, CA, accessed Feb, vol. 17,\nno. 2018, p. 4, 2016.\n[105] M. Zhang and Y. Chen, “Link prediction based on graph neural\nnetworks,” Advances in neural information processing systems, vol. 31,\n2018.\n[106] C. Nash, Y. Ganin, S. A. Eslami, and P. Battaglia, “Polygen: An autore-\ngressive generative model of 3d meshes,” in International Conference\non Machine Learning.\nPMLR, 2020.\n[107] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” Advances\nin neural information processing systems, vol. 28, 2015.\n[108] K. Varda, “Protocol buffers: Google’s data interchange format,” Google\nOpen Source Blog, Available at least as early as Jul, vol. 72, p. 23,\n2008.\n[109] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words: Trans-\nformers for image recognition at scale,” in International Conference on\nLearning Representations, ICLR, 2021.\n[110] D. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell, T. Hirzel,\nA. Aspuru-Guzik, and R. P. Adams, “Convolutional networks on graphs\nfor learning molecular fingerprints,” Advances in Neural Information\nProcessing Systems, vol. 28, 2015.\n[111] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,\n“Neural message passing for quantum chemistry,” in International\nConference on Machine Learning.\nPMLR, 2017.\n[112] P. Achlioptas, O. Diamanti, I. Mitliagkas, and L. Guibas, “Learning\nrepresentations and generative models for 3d point clouds,” in Inter-\nnational conference on machine learning.\nPMLR, 2018.\n[113] K. Wang, J. Zheng, and Z. Zhou, “Neural face identification in a 2d\nwireframe projection of a manifold object,” in IEEE Conference on\nComputer Vision and Pattern Recognition, 2022.\n[114] P. Benk˝o and T. V´arady, “Segmentation methods for smooth point\nregions of conventional engineering objects,” Computer-Aided Design,\nvol. 36, no. 6, pp. 511–523, 2004.\n[115] T. Birdal, B. Busam, N. Navab, S. Ilic, and P. Sturm, “Generic primitive\ndetection in point clouds using novel minimal quadric fits,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 42,\nno. 6, pp. 1333–1347, 2019.\n[116] L. Li, M. Sung, A. Dubrovina, L. Yi, and L. J. Guibas, “Supervised\nfitting of geometric primitives to 3d point clouds,” in IEEE Conference\non Computer Vision and Pattern Recognition, 2019.\n[117] C. Sommer, Y. Sun, E. Bylow, and D. Cremers, “Primitect: Fast\ncontinuous hough voting for primitive detection,” in International\nConference on Robotics and Automation (ICRA).\nIEEE, 2020.\n[118] J. A. Bærentzen, “Robust generation of signed distance fields from\ntriangle meshes,” in International Workshop on Volume Graphics.\nIEEE, 2005.\n[119] M. Sanchez, O. Fryazinov, and A. Pasko, “Efficient evaluation of\ncontinuous signed distance to a polygonal mesh,” in Spring Conference\non Computer Graphics, 2012.\n"
}