{
    "optim": "TorchMD-Net 2.0: Fast Neural Network Potentials for Molecular Simulations Raul P. Pelaeza,† Guillem Simeona,† Raimondas Galvelis,†,‡ Antonio Mirarchi,† Peter Eastman,¶ Stefan Doerr,‡ Philipp Thölke,‡ Thomas E. Markland,¶ and Gianni De Fabritiis∗,†,‡,§ †Computational Science Laboratory, Universitat Pompeu Fabra, Barcelona Biomedical Research Park (PRBB), C Dr. Aiguader 88, 08003 Barcelona, Spain. ‡Acellera Labs, C Dr Trueta 183, 08005, Barcelona, Spain ¶Department of Chemistry, Stanford University, Stanford, CA 94305, USA §Institució Catalana de Recerca i Estudis Avançats (ICREA), Passeig Lluis Companys 23, 08010 Barcelona, Spain E-mail: g.defabritiis@gmail.com aR.P.P. and G.S. contributed equally to this work. Abstract Achieving a balance between computational speed, prediction accuracy, and universal appli- cability in molecular simulations has been a per- sistent challenge. This paper presents substan- tial advancements in the TorchMD-Net soft- ware, a pivotal step forward in the shift from conventional force fields to neural network- based potentials. The evolution of TorchMD- Net into a more comprehensive and versa- tile framework is highlighted, incorporating cutting-edge architectures such as TensorNet. This transformation is achieved through a mod- ular design approach, encouraging customized applications within the scientific community. The most notable enhancement is a signifi- cant improvement in computational efficiency, achieving a very remarkable acceleration in the computation of energy and forces for TensorNet models, with performance gains ranging from 2- fold to 10-fold over previous iterations. Other enhancements include highly optimized neigh- bor search algorithms that support periodic boundary conditions and the smooth integra- tion with existing molecular dynamics frame- works. Additionally, the updated version intro- duces the capability to integrate physical priors, further enriching its application spectrum and utility in research. The software is available at https://github.com/torchmd/torchmd-net. 1 Introduction Neural Network Potentials (NNPs)1–7 are emerging as a key approach in molecular sim- ulations, striving to optimize the balance be- tween computational efficiency, predictive ac- curacy, and generality. Some software frameworks to facilitate the use of neural network potentials have been developed, such as SchNetPack,8 TorchANI,9 DeePMD-Kit,10 and others. Among the first to appear, we released TorchMD-Net, initially designed for the Equivariant Transformer ar- chitecture11 and a graph network, a simpler invariant graph neural network tailored for neural network potentials for protein coarse- graining.12 Over time, TorchMD-Net has ex- panded its model architectures to include Ten- sorNet,13 an O(3)-equivariant message-passing 1 arXiv:2402.17660v1  [cs.LG]  27 Feb 2024 neural network utilizing rank-2 Cartesian ten- sor representations which achieved state-of-the- art accuracy on benchmark datasets. This evo- lution positions TorchMD-Net not just as a standalone tool, but as a versatile library for the development of NNPs. Efficiency has been at the forefront of re- cent enhancements to TorchMD-Net. Among the optimizations, CUDA graphs have been integrated, providing a performance boost es- pecially for smaller workloads. TorchMD-Net has also incorporated the latest versions of its key dependencies (mainly PyTorch14 and PyTorch Lightning15), with a notable addi- tion being the search for compatibility with the torch.compile submodule from PyTorch 2.0, a feature that compiles Just-In-Time (JIT) the modules into optimized kernels. While TorchMD-Net has introduced low precision modes (i.e. bfloat16) primarily as an ex- ploratory tool for researchers, high precision (float64) is also available for ensuring detailed correctness checks during prototyping. The new technical enhancements include the introduction of periodic boundary condi- tions, a CUDA-optimized neighbor list, and memory-efficient dataset loaders. The inclu- sion of TorchMD-Net in the conda-forge16 package repository and the release of the doc- umentation17 are steps taken to enhance its accessibility to researchers. Another feature is TorchMD-Net’s capacity to blend empirical physical knowledge into NNPs via priors. The integration of atom-wise and molecule-wise pri- ors, such as the Ziegler-Biersack-Littmark18 and Coulomb potentials, allows for a more nu- anced approach in simulations. TorchMD-Net emphasizes compatibility with leading molecular dynamics (MD) packages, especially with OpenMM.19 OpenMM, widely recognized in the computational chemistry field, can now interface directly with TorchMD- Net through the OpenMM-Torch20 plugin. This integration has been a collaborative ef- fort, with OpenMM-Torch being co-developed by the core teams of both OpenMM and TorchMD-Net. This ensures streamlined and effective utilization of TorchMD-Net models within OpenMM’s simulation framework. In the following sections we provide an overview of the TorchMD-Net framework. The manuscript is ordered as follows. In the Meth- ods section we go over the currently available NNP architectures. We continue in section Training with details about the different parts involved in the training and deployment of these architectures and how they are exposed in TorchMD-Net. Then, in section Optimiza- tion, we lay out the optimization strategies employed in this release. Finally, we present a series of validation and performance results in the Results section. TorchMD-Net is freely available with a per- missive licence (MIT) at https://github. com/torchmd/torchmd-net. 2 Methods We interpret a neural network potential as a machine learning model that takes as input a se- ries of atomic positions, denoted by R, embed- ding indices such as atomic numbers, Z, and op- tionally charges (which might be per-sample or per-atom), q, and outputs a per-sample scalar value and optionally its negative gradient with respect to the positions, typically interpreted as the potential energy and atomic forces, respec- tively. Note, however, that TorchMD-Net is not limited to this interpretation of the outputs, which are generally labelled as y and neg_dy respectively. Figure 1 provides a comprehensive overview of the TorchMD-Net architecture. The dia- gram’s left section illustrates the various com- ponents of the primary module, designated as TorchMD_Net, which constitutes, concep- tually and in the API itself, a NNP model. Each component within this object is modu- lar and customizable, allowing for the creation of diverse models. At the heart of the NNP is the representation_model. This part of the architecture takes the set of inputs stated above and outputs a series of per-atom fea- tures. These features are subsequently fed into an output_model. The purpose of this model is to further process these features into single atomic values, which typically will be aggre- 2 Figure 1: The main module in TorchMD-Net is called TorchMD_Net from the torchmdnet.models.model module. This class combines a given representation model (such as the Equivariant Transformer), an output model (such as the scalar output module) and a prior model (such as the Atomref prior), producing a module that takes as input a series of atoms features and outputs a scalar value (i.e energy per molecule) and when derivative = True, its negative gradient with respect to the input positions (i.e atomic forces). 3 gated and will represent the total potential en- ergy, though it can represent other per-sample or per-atom quantities as well, depending on the specifics of its design and (optional) aggre- gation scheme. Output models normally in- clude learnable parameters (e.g. a multilayer perceptron). Prior models can be employed to augment either the atom-level features or the aggregated per-molecule value with further physical insights. Furthermore, the framework integrates PyTorch’s Autograd for automatic differentiation, enabling the computation of the negative gradient of the per-molecule scalar pre- diction with respect to atomic positions. This is particularly relevant when interpreting the per-molecule value as the potential energy, as it yields the atomic forces in a way that en- sures, by construction, that the resulting force field is energy conserving. This modular logic allows for flexibility in the combination of representation models and out- put models. Therefore, by building a custom output module, researchers can make use of the representation models for other prediction tasks beyond potential energy and forces. 2.1 Available representation mod- els Current models in TorchMD-Net at the time of writing are message-passing neural net- works21,22 (MPNNs) which learn approxima- tions to the many-body potential energy func- tion. Atoms are identified with graph nodes embedded in 3D space, building edges between them after the definition of some cutoff radius. The neural network uses atomic and geometric information to learn expressive representations by propagating, aggregating, and transforming features from neighboring nodes found within the cutoff radius.23,24 In most current NNPs, af- ter several message passing steps, node features are used to predict per-atom scalar quantities which are identified with atomic contributions to the energy of the molecule. 2.1.1 New architecture: TensorNet TensorNet13 is an O(3)-equivariant model based on rank-2 Cartesian tensor representa- tions. Euclidean neural network potentials25–27 have been shown to achieve state-of-the-art performance and better data efficiency than previous models, relying on higher-rank equiv- ariant features which are irreducible represen- tations of the rotation group, in the form of spherical tensors. However, the computation of tensor products in these models can be compu- tationally demanding. In contrast, TensorNet exploits the use of Cartesian rank-2 tensors (3x3 matrices) which can be very efficiently decomposed into scalar, vector and rank-2 ten- sor features. Furthermore, Clebsch-Gordan tensor products are substituted by straight- forward and node-level 3x3 matrix products. Overall, these properties allow TensorNet to achieve state-of-the-art accuracy on common benchmark datasets with a reduced number of message-passing steps, learnable parameters, and computational cost. The prediction of up to rank-2 molecular properties that behave appropriately under geometric transformations such as reflections and rotations is also possible. 2.1.2 Equivariant Transformer The Equivariant Transformer11 (ET) is an equivariant neural network that uses both scalar and Cartesian vector representations. The distinctive feature of the ET in compari- son to other Cartesian vector models such as PaiNN28 or EGNN29 is the use of a distance- dependent dot product attention mechanism, which achieved state-of-the-art performance on benchmark datasets at the time of publica- tion. Furthermore, the analysis of attention weights allowed us to extract insights into the interaction of different atomic species for the prediction of molecular energies and forces. The model also exhibits a low computational cost for inference and training in comparison to some of the most used NNPs in the literature.30 As part of the current release, we removed a discontinuity at the cutoff radius. In the orig- inal description, vector features’ residual up- dates, as opposed to scalar features’ updates, 4 received contributions from the value pathway of the attention mechanism which were not be- ing properly weighted by the cosine cutoff func- tion envelope, which is reflected in Eq. 9 in the original paper.11 We fixed it by applying ϕ(dij), i.e., split(Vj ⊙ DV ij) → split(ϕ(dij)Vj ⊙ DV ij). To ensure backward compatibility, this modifica- tion is only applied when setting the new ET ar- gument vector_cutoff = True. The impact of this modification is evaluated in the results section. 2.1.3 Graph Network The graph network is an invariant model in- spired by both the SchNet31 and PhysNet32 ar- chitectures. The network was optimized to have satisfactory performance on coarse-grained pro- teins, allowing the building of NNPs that cor- rectly reproduce fast-folder protein free energy landscapes.12 In contrast to the ET and Ten- sorNet, the graph network only uses relative distances between atoms as geometrical infor- mation, which are invariant to translations, ro- tations, and reflections. The distances are used by the model to learn a set of continuous fil- ters that are applied to feature graph convolu- tions as in SchNet,31 progressively updating the initial atomic embeddings by means of residual connections. 2.2 Prior models Priors are additional physical terms that can be introduced for the prediction of potential energies. Some of these terms have been used in NNPs in the literature,33,34 sometimes even including learnable parameters. In TorchMD- Net, we provide some predefined priors, which can be optionally added to the neural network prediction: • Atomref: These are per-element atomic reference energies, which are usually pro- vided directly in the dataset. In this case, the neural network has to predict the re- maining contribution to the potential en- ergy, which can be regarded as the forma- tion energy of the molecule. There is also the option of making this prior learnable, in which case it is initialized with atomic reference energies, but these contributions are modified during training. • Coulomb: This prior corresponds to the usual Coulomb electrostatic interaction, scaled by a cosine switching function to reduce its effect at short distances. Us- ing this prior requires providing per-atom partial charges. • D2 dispersion: In this case, the prior cor- responds to the D2 dispersive correction used in DFT-D2.35 C6 coefficients and Van der Waals radii for elements are al- ready incorporated in the method. • ZBL potential: This prior implements the Ziegler-Biersack-Littmark (ZBL) po- tential for screened nuclear repulsion as described in Ref 18. It is an empirical po- tential effectively describing the repulsion between atoms at very short distances, and only atomic numbers need to be pro- vided. Note that forces are computed directly by autograd when adding the energy contribu- tions coming from the priors before the back- ward automatic differentiation step. Even though the previous terms are the currently predefined options in TorchMD-Net, all these priors are derived from a general BasePrior class, which easily allows researchers to imple- ment their own priors, following the modular logic behind the framework. 2.3 Training The right diagram in Figure 1 depicts the main training loop in TorchMD-Net. A Dataset pro- vides sample/output pairs for the NNP and is divided into training, validation and testing sets and batched by a Dataloader (as provided by the Pytorch Geometric library36). We make use of the PyTorch Lightning library’s15 trainer, which also allows multi-GPU training. Check- points are generated during training, contain- ing the current weights of the model, which can then be subsequently loaded for inference or fur- ther training. 5 2.3.1 Datasets Within TorchMD-Net, datasets can be ac- cessed through the YAML configuration file for use with the torchmd-train util- ity or programmatically via the Python API. Predefined datasets include SPICE,37 QM9,38 WaterBox,39 (r)MD1740,41 MD22,42 ANI1,43 ANI1x,44 ANI1ccx,44 ANI2x45 and the COMP646 evaluation dataset with all its subsets (ANIMD, DrugBank, GDB07to09, GDB10to13, Tripeptides and S66X8), offer- ing diverse training environments for molecular dynamics and quantum chemistry applications. These datasets serve as common benchmarks in the field of neural network potentials. However, on top of these, the framework allows the flexi- ble incorporation of user-generated datasets for customized applications. The Custom dataset functionality allows users to train models with molecular data encapsulated in simple NumPy file formats without writing a single line of code. By specifying paths to coordinate and embed- ding index (e.g. atomic numbers) and reference energy and force files, researchers can easily integrate their datasets into the training pro- cess. This capability ensures TorchMD-Net’s adaptability to a wider array of applications beyond its pre-packaged offerings. In addition, TorchMD-Net offers support for other popular dataset formats, such as HDF5. Special care is taken to ensure data is cached as much as possible, using techniques such as in memory datasets and memory mapped files. 2.3.2 Losses During training, a weighted sum of mean squared error (MSE) losses of energy and forces is used, weighting each of them according to user input. In validation, we provide both L1 and MSE losses separately for energies and forces, while for testing L1 losses alone are used. The framework allows to use an expo- nential moving average (EMA) to update the losses during the training and validation stages to smooth out progression of loss values. 2.4 Usage examples In the following sections we showcase code for some typical usecases of TorchMD-Net. While these snippets are generally self-contained the reader is pointed to the online documentation17 for further information. 2.4.1 Training code example The project introduces a command line tool, torchmd-train, designed as a code-free method for model training. This tool is set up through a YAML file, with several examples available in the TorchMD-Net GitHub reposi- tory for reference. However, we also offer an illustrative script here that outlines the pro- cess of training an existing model using the Python API. The LNNP class, found within the torchmdnet.module module, encapsulates the procedures for both the creation and training of a model. This class is inherited from Py- torch Lightning LightningModule, offering all the extensive customization available in it. The following is a succinct yet comprehensive exam- ple of how to utilize LNNP for training purposes: 1 from torchmdnet.data import DataModule 2 from torchmdnet.module import LNNP 3 from pytorch_lightning import Trainer 4 args = { 5 ’dataset ’: ’ANI1X ’, 6 ’model ’: ’tensornet ’, 7 ’num_epochs ’: 200, 8 ’embedding_dimension ’: 128, 9 ’num_layers ’: 2, 10 ’num_rbf ’: 32, 11 ’rbf_type ’: ’expnorm ’, 12 ’trainable_rbf ’: False , 13 ’activation ’: ’silu ’, 14 ’cutoff_lower ’: 0.0, 15 ’cutoff_upper ’: 5.0, 16 ’max_z ’: 100, 17 ’max_num_neighbors ’: 64, 18 ’derivative ’: True # So the model returns forces. 19 } 20 data = DataModule(args) 21 data. prepare_data () 22 data.setup(\"fit\") 23 lnnp = LNNP(args , 24 prior_model =None , 25 mean=data.mean , 26 std=data.std) 27 trainer = Trainer(max_epochs=args[’num_epochs ’]) 28 trainer.fit(lnnp , data) 29 model = LNNP. load_from_checkpoint (trainer. checkpoint_callback . best_model_path ) 30 trainer = pl.Trainer( inference_mode =False) 31 trainer.test(model , data) This example shows the minimal steps required to prepare data, initialize the LNNP class, train and test a model using PyTorch Lightning’s Trainer. The Trainer here is simplified for brevity; in practice, additional callbacks and logger configurations could be added. 6 2.4.2 Loading a Trained Model for In- ference After training a model, the next logical step is to use it for inference. TorchMD-Net offers a dedicated function, load_model, to facilitate this. Below is a concise example: 1 from torchmdnet.models.model import load_model 2 # Define the path to the saved model checkpoint 3 checkpoint_path = \"path/to/ saved_model_checkpoint .ckpt\" 4 # Load the model 5 loaded_model = load_model( checkpoint_path ) 6 # Prepare the input data (atomic numbers , positions , batch index , etc.) 7 # For demonstration , these are placeholders and should be replaced with actual data 8 input_data = { 9 ’z’: torch.Tensor ([...]) , 10 ’pos’: torch.Tensor ([...]) , 11 ’batch ’: torch.Tensor ([...]) 12 # ... other optional fields 13 } 14 # Perform inference 15 energy , forces = loaded_model (** input_data) 16 # Energy and forces are now available for further analysis or visualization In this example, checkpoint_path should point to the location where the trained model checkpoint is saved. The input_data dic- tionary should be populated with the actual atomic numbers, positions, and other required or optional fields. Finally, energy and forces are obtained from the loaded model and can be used as needed. 2.4.3 Integration with OpenMM It is possible to run TorchMD-Net neural net- work potentials as force fields in OpenMM19 to run molecular dynamics. The OpenMM- Torch20 package is leveraged for this. In- tegration consists of writing a wrapper class to accommodate for the unit requirements of OpenMM and to provide to the model any in- formation not proper to OpenMM (like the em- bedding indices). The following code showcases an example on how to add a TorchMD-Net NNP as an OpenMM Force. 1 from torchmdnet.models.model import load_model 2 from openmmtorch import TorchForce 3 from openmm import System 4 5 class Wrapper(torch.nn.Module): 6 7 def __init__(self , embeddings , model): 8 super(Wrapper , self).__init__ () 9 self.embeddings = embeddings 10 # OpenMM will compute the forces 11 # by backpropagating the energy , 12 # so we can load the model with derivative=False 13 self.model = load_model(model , derivative=False) 14 15 def forward(self , positions): 16 # OpenMM works with nanometer positions 17 # and kilojoule per mole energies 18 # Depending on the model , you might need 19 # to convert the units 20 positions = positions * 10.0 # nm -> A 21 energy = self.model(z=self.embeddings , pos=positions) [0] 22 return energy * 96.4916 # eV -> kJ/mol 23 24 # The embeddings used during training (e.g. atomic numbers) 25 # for each atom in the simulation. 26 z = torch.tensor ([1,1], torch.long) 27 model = torch.jit.script(Wrapper(z, \"model.ckpt\")) 28 # Create a TorchForce object from the model 29 torch_force = openmmtorch.TorchForce(model) 30 system = System () 31 # The TorchForce object can be used as a regular OpenMM Force 32 system.addForce(torch_force ) 33 # Set up the rest of the OpenMM simulation 34 # ... 2.5 Optimization techniques Typical neural network potential (NNP) algo- rithms implemented in PyTorch14 comprise a series of sequential operations such as multi- layer perceptrons and message passing opera- tions. As PyTorch operations translate into highly optimized CUDA kernel calls, the efficiency of modern GPUs often turns kernel launch- ing overhead into a performance bottleneck. CUDA graphs address this by consolidating multiple kernel calls into a single graph, drasti- cally reducing kernel launch overhead. How- ever, CUDA graphs impose stringent limita- tions. These include the need for static shapes in graphed code sections, which can lead to costly recompilations or memory inefficiencies, and the exclusion of operations requiring CPU- GPU synchronization. Conversely, developments in the compiler community47 , including technologies like Ope- nAI’s Triton48 and subsequently PyTorch en- hancements, are gradually diminishing the reliance on CUDA graphs by automatically changing the structure of the code in ever more profound ways (i.e kernel fusion49–51). These advancements, such as TorchDynamo intro- duced in PyTorch 2.0 through torch.compile, optimize code structure through Just-In-Time (JIT) compilation. Even with JIT, and in general transpilation- based techniques, CUDA graphs often provide the best out-of-the-box performance improve- ments and at the bare minimum, facilitate the optimizations introduced by the former. En- capsulating a piece of code within a CUDA graph, a process known as ’stream capture’, ne- cessitates adherence to several specific require- ments. This often demands substantial mod- 7 ifications to the code. Crucially, for code to be eligible for capture, it must avoid any CPU- GPU synchronization activities, including syn- chronization barriers and memory copies. Ad- ditionally, all arrays involved in the operations must possess static shapes and fixed memory addresses, precluding any dynamic memory al- locations during the process. The CUDA graph interface in PyTorch allevi- ates many challenges associated with adapting code for stream capture. It particularly excels in managing memory allocations within cap- tured environments automatically and trans- parently. However, challenges arise in spe- cific implementations, as exemplified by Ten- sorNet. The main issue in TensorNet is its neighbor list, which inherently varies in shape at each inference step due to the fluctuating number of neighbors. This variation affects the early stages of the architecture, resulting in TensorNet primarily operating on dynami- cally shaped tensors. To address this, we imple- mented a static shape mode that creates place- holder neighbor pairs up to a predetermined maximum. We then ensure the architecture dis- regards these placeholders’ contributions. Al- though this method increases the initial work- load, our empirical data indicates that the per- formance gains from capturing the entire net- work substantially outweigh this added over- head. In the following sections, we explore the im- pact of these optimizations on both inference and training performance. 2.5.1 Neighbor search and periodic boundary conditions Message-passing neural networks, such as the architectures currently supported in the frame- work, require a list of connections among nodes referred to as edges. This list is constructed by proximity after the definition of a cutoff radius (a neighbor list). TorchMD-Net offers a neigh- bor list construction engine specifically tailored for NNPs, exposing a naive brute-force O(N 2) algorithm that works best for small workloads and a cell list (a standard O(N) hash-and-sort strategy widely used in MD52,53) that performs better for large systems (see Figures 3 and 2). Effectively, this engine makes neighbor search a negligible part of the overall computation. Special measures are taken into account to ensure that the neighbor search is compatible with CUDA-graphs. For this matter, it is re- quired that the neighbor search works on a statically-shaped set of input/outputs, which poses a problem given that the number of neigh- bor pairs in the system is not known in advance and is bound to change from input to input. We solve this by requiring an upper bound for the number of pairs in the system and padding the outputs with a special value (−1) for un- used pairs. Furthermore, TorchMD-Net archi- tectures support rectangular and triclinic peri- odic boundary conditions. Contrary to usual MD workloads, it is com- mon to have batches of input samples in NNPs. This owns to the very nature of neural net- work training but also can benefit inference (for instance, allowing the possibility of running many simulations in parallel, like TorchMD54 does). Our neighbor list is able to handle ar- bitrary batch sizes while maintaining compati- bility with CUDA graphs. The current cell list implementation constructs a single cell list in- cluding atoms for all batches, excluding pairs of particles that pertain to different batches when finding neighbors in it. This makes it so that each particle has to perform a check against ev- ery other particle in the vicinity for all batches, which degrades performance with increasing batch size. We find this to be an acceptable compromise given that doing it this way facili- tates compatibility with CUDA graphs and we assume that with increasing number of particles (where the cell list excels) the typical batch size will decrease. Still, the particularities of the cell list implementation makes its performance spe- cially susceptible to the batch size, as evidenced by the variability observed in the cell list curves in figures 3 and 2. All data presented in this section was gath- ered in an RTX4090 NVIDIA GPU using CUDA 12. Each point is obtained by averaging 50 identical executions. Warmup executions are also performed before measuring. 8 0.5 2 8 32 128 512 1 2 3 4 5 6 Batch Size Time (ms) Cell Brute force Figure 2: Performance comparison of cell (solid line) and brute-force (dashed line) neighbor search strategies across different batch sizes for a random cloud of 32k particles with 64 neigh- bors per particle on average. The particles are split into a certain number of batches. 103 104 105 10−1 100 101 Number of particles Time (ms) Batch 1 Batch 2 Batch 64 Figure 3: Performance comparison of cell (solid line) and brute-force (dashed line) neighbor search strategies across different batch sizes for a random cloud of particles with 64 neighbors per particle on average. Cell list performance tends to degrade with increasing batch size, while the opposite is true for brute force. 2.5.2 Training Optimizing neural network training presents distinct challenges compared to inference op- timization. Primarily, the variable length of each training sample, exacerbated by batch- ing processes (where a varying number of sam- ples constitute a single network input), im- pedes optimizations dependent on static shapes (i.e. CUDA graphs). A potential solution involves integrating ’ghost molecules’, akin to strategies used in static neighbor list shap- ing, to standardize the atom count inputted to the network. However, this method increases memory consumption in an already memory- constrained environment and raises concerns about the backpropagation of losses for these non-existent atoms, which may lead to numer- ical instability. Moreover, training necessitates backpropaga- tion through the network. In our context, this involves a double backpropagation process when the loss function includes force calcula- tions. Currently, double backpropagation is inadequately supported by the PyTorch com- piler. A workaround is to manually implement the network’s initial backward pass (specifi- cally, the force computation). This adjust- ment enables Autograd to perform only a single backward pass during training, leveraging the PyTorch compiler’s capabilities. Nevertheless, challenges persist with the PyTorch compiler when managing dynamic input shapes. Given the current constraints, the current re- lease does not include any training-specific opti- mizations besides the improved dataloader sup- port as previously described. 3 Results 3.1 Validation In this subsection, we evaluate the impact of the architectural modifications introduced in the models on predictive accuracy. In the case of TensorNet the modifications targeted its com- putational performance alone, while for the ET one needs to consider the changes induced by vector_cutoff = True. 9 3.1.1 Accuracy with TensorNet Original test MAE presented in Ref. 13 for the QM9 U0 target quantity is 3.9(1) meV, while the latest optimized versions of the model (see Fig- ure 4) yield 3.8(2) meV, confirming that the ar- chitectural optimizations do not affect Tensor- Net’s prediction performance. The training loss was computed in this case as the MSE between predicted and true energies. This state-of-the- art performance is achieved with the largest model with 4.0 million trainable parameters, with specific architectural and training hyper- parameters being found in Table 5. We also provide in Table 1 the accuracy of smaller and shallower models on the same QM9 quantity (that is, using the same hyperparameters as in Table 5, except for embedding_dimension = 128 and num_layers = 0, 1, 2), while com- paring them to other NNPs. Overall, Ten- sorNet demonstrates very satisfactory perfor- mances, achieving close to state-of-the-art ac- curacy (< 5 meV MAE) with a very reduced number of parameters. 101 102 103 10−6 10−5 10−4 10−3 10−2 Epoch MSE Loss Training Validation Figure 4: Training and validation curves for TensorNet on the QM9 U0 benchmark, hyper- parameters are in Table 5. 3.1.2 Accuracy with the Equivariant Transformer As previously mentioned, we provide an im- plementation of the ET where it is modified by applying the cutoff function to the values’ Table 1: Mean absolute error in meV for dif- ferent models trained on QM9 target property U0. TensorNet 3L* uses an embedding dimen- sion of 256, while in other cases 128. For the ET, subscripts new and old correspond to the new and the original implementation, that is, with vector_cutoff = True and False, re- spectively. Model U0 MAE (meV) Cormorant55 22 SEGNN56 15 SchNet57 14 EGNN29 11 Equiformer58 6.6 DimeNet++59 6.3 SphereNet60 6.3 ETold 11 6.2 PaiNN28 5.9 Allegro26 4.7 MACE27 4.1 ETnew 5.7 TensorNet 0L 7.2 TensorNet 1L 4.7 TensorNet 2L 4.4 TensorNet 3L* 3.9 10 pathway of the attention mechanism to enforce a continuous energy landscape at the cutoff distance. Therefore, we checked to which ex- tent these changes, together with TorchMD- Net’s ones, affect the accuracy of the Equiv- ariant Transformer. We trained the model on the MD17 aspirin dataset (Figure 5) us- ing the hyperparameters defined for the orig- inal version of the ET (Table 3, with the ad- dition of vector_cutoff = True), giving fi- nal test MAEs of 0.139 kcal/mol and 0.232 kcal/mol/Åin energies and forces, respectively, compared to the original implementation which gave 0.123 kcal/mol and 0.253 kcal/mol/Å.11 Regarding QM9 U0, we reused the original hy- perparameters for the dataset found in Table 4 (again, adding vector_cutoff = True), and comparative results can be found in Table 1. 101 102 103 10−2 10−1 100 101 102 Epoch MSE Loss Training Validation Figure 5: Training and validation curves for the new implementation of the Equivariant Trans- former on the MD17 benchmark, hyperparam- eters are in Table 3. 3.2 Molecular Simulations We performed NVT molecular dynamics simu- lations employing TensorNet models trained on the ANI-2x dataset.45 A table detailing the hy- perparameters is provided for reference in Table 6. Note that we did not include any physical priors in these trainings nor in the subsequent simulations, i.e all forces in the system come from the model itself. Starting from the SPICE dataset,37 we selected the PubChem subset and utilized it to create a test set comprising four randomly chosen conformers. This test set aimed to evaluate the ability of the NNP to perform stable molecular dynamics (MD) sim- ulations on molecules not encountered during the training stage.61,62 The training dataset, as well as the PubChem subset, represent a broad diversification of molecules containing the el- ements H, C, N, O, S, F, Cl. To generate the input data, the SMILES and the coordi- nates of interest were used to build a molecule object using openff-toolkit,63 and the atomic numbers were used as embeddings. Using the more accurate TensorNet 2L model, a 50 ns trajectory with a time-step of 1 fs was gen- erated for each molecule using OpenMM’s19 LangevinMiddleIntegrator at 298.5K and a friction coefficient of 1 ps−1. We also used for one of the molecules a TensorNet 0L model with the same simulation settings to test its stabil- ity. A root mean square displacement (RMSD) analysis was performed for each trajectory tak- ing the starting conformation as reference, see Figure 6. The results highlight the model’s abil- ity to run stable MD simulations, even for the 0L case where the model’s receptive field and parameter count are substantially reduced. 3.3 Speed performance All results presented in this work were car- ried out using an NVIDIA RTX4090 GPU (driver version 525.147) with a 32 core Intel(R) Xeon(R) Silver 4110 CPU in Ubuntu 20.04. We used CUDA 12.0 with Pytorch version 2.1.0 from conda-forge. We provide all timings in million steps per day, which can be easily con- verted to nanoseconds per day. These units are more commonly used in molecular dynamics settings, and the conversion can be done by tak- ing the quantity in million steps per day times the timestep in femtoseconds. Therefore, for example, 1 million steps per day is equivalent to 1 ns/day for a timestep of 1 fs. To study the optimization strategies laid out in section 2.5 we show energy and forces in- ference performance for several equivalent im- plementations of TensorNet in Figure 7. Note 11 0 5 10 15 20 25 30 35 40 45 50 0.5 1 1.5 2 2.5 3 Time (ns) RMSD (Å) A-2L A-0L B-2L C-2L D-2L A: 135129529 B: 136963008 C: 160861289 D: 252660926 Figure 6: (Left) RMSD analysis for the trajectories of 4 molecules outside of the training set. Simulations are carried out with TensorNet 2L, using the parameters in table 6, with the exception of A-0L, in which a 0L TensorNet model is showcased. Presented data is plotted only every 1.28 ns for visualization clarity. (Right) Representation of the simulated molecules. Labels show the PubChem ID for each molecule. Table 2: TensorNet inference times in million steps per day for the \"Plain\" (P), \"Compile\" (C) and \"Graph\" (G) implementations and varying number of layers. Molecule (atoms) P 0L P 1L P 2L C 0L C 1L C 2L G 0L G 1L G 2L Alanine dipeptide (22) 19.86 10.29 8.50 40.19 28.70 21.23 172.80 84.71 56.47 Testosterone (49) 15.05 11.93 8.56 38.57 27.00 21.49 154.29 63.53 39.82 Chignolin (166) 19.77 11.88 7.90 36.77 24.90 21.39 77.14 26.02 15.57 DHFR (2489) 5.56 1.67 0.98 14.47 3.27 1.83 5.65 1.69 1.00 Factor IX (5807) 2.32 0.69 0.41 5.42 1.35 0.77 2.33 0.70 0.42 that in TorchMD-Net, running inference re- quires one backpropagation step to compute forces as the negative gradient of the energies with respect to the input positions, which are computed via Autograd. This step is also in- cluded in these benchmarks. We make sure not to include any warmup times in these bench- marks by running the models for 100 itera- tions before timing. We refer as \"Graph\" to an implementation that has been modified to ensure every CUDA graph requirement is met. For \"Compile\" the implementation is carefully tailored to look for the best performance in torch.compile in addition to the changes in- troduced for \"Graph\". Finally, \"Plain\" repre- sents the baseline implementation in PyTorch. 12 101 102 103 101 102 Million steps per day 0L Plain Compile Graphs 101 102 103 100 101 102 Million steps per day 1L Plain Compile Graphs 101 102 103 100 101 102 Number of particles Million steps per day 2L Plain Compile Graphs Figure 7: Comparison between TensorNet in- ference times (energy and forces) with 0, 1 and 2 layers, embedding dimension 128, 64 neigh- bors on average. All atoms are passed in a single batch. Plain represents the bare Ten- sorNet implementation; with Compile the mod- ule has been preprocessed with torch.compile with the \"max-autotune\" option; for Graphs the whole computation has been captured into a single CUDA graph. Although in principle the code received by the compiler is entirely capturable by a graph, it often decides to capture only some sections of it, introducing other kinds of optimizations in- stead. This is also made evident by the appear- ance of the same kind of \"plateau\" performance for smaller workloads in both Plain and Com- pile, which can be attributed to a bottleneck produced by kernel launch overhead. Still, the torch compiler is able to provide a speedup of a factor 2 to 3 for all workloads with respect to the original implementation. CUDA kernel overhead (and thus the perfor- mance gain of CUDA graphs) is expected to dominate for small workloads, where it is usual for the kernel launching time to be larger than the actual execution. Figure 7 indeed corrobo- rates this by showing speedups between 10 and 2 times for molecules with up to a few hundreds of atoms and for all numbers of interaction lay- ers (0, 1 and 2). Starting from workloads con- sisting of several hundreds of atoms, the perfor- mance of the Plain version is recovered. We also explore inference times for some molecules with varying number of atoms in Ta- ble 2. For these molecules, which can be found in the repository for speed benchmarking pur- poses, we measure time to compute the poten- tial energy and atomic forces of a single exam- ple using TensorNet with 0, 1 and 2 interaction layers. Again, we express this time in million steps per day. In all cases we use a cutoff of 4.5Å, an embedding dimension of 128, 32 radial basis functions and a maximum of 32 neighbors per particle. 4 Conclusions TorchMD-Net has significantly evolved in its re- cent iterations, becoming a comprehensive plat- form for neural network potentials (NNPs). It provides researchers with robust tools for both rapid prototyping of new models and executing production-level tasks. However, despite these advancements, NNPs still face substantial chal- lenges before they can fully replace traditional force fields in molecular dynamics simulations. Currently, while the necessary software infras- 13 tructure is largely in place, as evidenced by the first-class support for NNPs in popular pack- ages,19 issues such as memory requirements and computational performance remain significant concerns. The impact of memory limitations is antici- pated to diminish with ongoing hardware ad- vancements. Yet, enhancing computational performance to a level that is competitive with traditional methods necessitates more intricate strategies. This involves developing architec- tures and their implementations in a manner that leverages the full capabilities of GPU hard- ware. From a software development perspective, the compilation functionality within PyTorch is an evolving feature, still in its early stages. Its current development trajectory, which aims to minimize the necessary code modifications for effective utilization, suggests that future Py- Torch releases will likely bring performance enhancements. Continuous improvements in the relevant toolset, encompassing PyTorch, CUDA, Triton, and others, are gradually nar- rowing the performance gap between highly op- timized code and more straightforward imple- mentations. Acknowledgement We thank Prof. Jan Rezac for discovering the spurious discontinuity in the Equivari- ant Transformer. G. S. is financially sup- ported by Generalitat de Catalunya’s Agency for Management of University and Research Grants (AGAUR) PhD grant FI-2-00587. This project has received funding from the Euro- pean Union’s Horizon 2020 research and inno- vation programme under grant agreement No. 823712; and the project PID2020-116564GB- I00 has been funded by MCIN / AEI / 10.13039/501100011033; Research reported in this publication was supported by the National Institute of General Medical Sciences (NIGMS) of the National Institutes of Health under award number R01GM140090. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health. References (1) Behler, J.; Parrinello, M. Generalized Neural-Network Representation of High- Dimensional Potential-Energy Surfaces. Phys. Rev. Lett. 2007, 98, 146401. (2) Kocer, E.; Ko, T. W.; Behler, J. Neural Network Potentials: A Concise Overview of Methods. 2021. (3) Behler, J. Perspective: Machine learning potentials for atomistic simulations. The Journal of Chemical Physics 2016, 145, 170901. (4) Schütt, K. T.; Sauceda, H. E.; Kinder- mans, P.-J.; Tkatchenko, A.; Müller, K.- R. SchNet – A deep learning architecture for molecules and materials. The Journal of Chemical Physics 2018, 148, 241722. (5) Deringer, V. L.; Caro, M. A.; Csányi, G. Machine Learning Interatomic Potentials as Emerging Tools for Materials Science. Advanced Materials 2019, 31, 1902765. (6) Botu, V.; Batra, R.; Chapman, J.; Ram- prasad, R. Machine Learning Force Fields: Construction, Validation, and Outlook. The Journal of Physical Chemistry C 2017, 121, 511–522. (7) Ko, T. W.; Finkler, J. A.; Goedecker, S.; Behler, J. A fourth-generation high- dimensional neural network potential with accurate electrostatics including non-local charge transfer. Nature Communications 2021, 12, 398. (8) Schütt, K. T.; Hessmann, S. S. P.; Gebauer, N. W. A.; Lederer, J.; Gastegger, M. SchNetPack 2.0: A neural network toolbox for atomistic ma- chine learning. The Journal of Chemical Physics 2023, 158. (9) Gao, X.; Ramezanghorbani, F.; Isayev, O.; Smith, J. S.; Roitberg, A. E. TorchANI: 14 A Free and Open Source PyTorch-Based Deep Learning Implementation of the ANI Neural Network Potentials. Journal of Chemical Information and Modeling 2020, 60, 3408–3415. (10) Zeng, J. et al. DeePMD-kit v2: A soft- ware package for deep potential models. The Journal of Chemical Physics 2023, 159. (11) Thölke, P.; Fabritiis, G. D. Equivari- ant Transformers for Neural Network based Molecular Potentials. International Conference on Learning Representations. 2022. (12) Majewski, M.; Pérez, A.; Thölke, P.; Do- err, S.; Charron, N. E.; Giorgino, T.; Hu- sic, B. E.; Clementi, C.; Noé, F.; Fab- ritiis, G. D. Machine Learning Coarse- Grained Potentials of Protein Thermody- namics. 2022. (13) Simeon, G.; Fabritiis, G. D. TensorNet: Cartesian Tensor Representations for Ef- ficient Learning of Molecular Potentials. Thirty-seventh Conference on Neural In- formation Processing Systems. 2023. (14) Paszke, A. et al. Advances in Neural In- formation Processing Systems 32; Curran Associates, Inc., 2019; pp 8024–8035. (15) https://lightning.ai/ pytorch-lightning. (16) conda-forge community The conda-forge Project: Community-based Software Dis- tribution Built on the conda Package For- mat and Ecosystem. 2015; https://doi. org/10.5281/zenodo.4774216. (17) TorchMD-NET Documen- tation. https://torchmd- net.readthedocs.io/en/latest/, Accessed: 2024-02-13. (18) Biersack, J. P.; Ziegler, J. F. Ion Implan- tation Techniques; Springer Berlin Heidel- berg, 1982; p 122–156. (19) Eastman, P. et al. OpenMM 8: Molec- ular Dynamics Simulation with Machine Learning Potentials. The Journal of Phys- ical Chemistry B 2024, 128, 109–116, PMID: 38154096. (20) OpenMM-Torch. https://github.com/ openmm/openmm-torch. (21) Bronstein, M. M.; Bruna, J.; Cohen, T.; Veličković, P. Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges. 2021; https://arxiv.org/abs/ 2104.13478. (22) Gilmer, J.; Schoenholz, S. S.; Riley, P. F.; Vinyals, O.; Dahl, G. E. Neural Message Passing for Quantum Chemistry. 2017; https://arxiv.org/abs/1704.01212. (23) Joshi, C. K.; Bodnar, C.; Mathis, S. V.; Cohen, T.; Liò, P. On the Expressive Power of Geometric Graph Neural Net- works. 2023; https://arxiv.org/abs/ 2301.09308. (24) Duval, A.; Mathis, S. V.; Joshi, C. K.; Schmidt, V.; Miret, S.; Malliaros, F. D.; Cohen, T.; Lio, P.; Bengio, Y.; Bron- stein, M. A Hitchhiker’s Guide to Geomet- ric GNNs for 3D Atomic Systems. 2023; https://arxiv.org/abs/2312.07511. (25) Batzner, S.; Musaelian, A.; Sun, L.; Geiger, M.; Mailoa, J. P.; Kornbluth, M.; Molinari, N.; Smidt, T. E.; Kozinsky, B. E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials. Nature Communications 2022, 13. (26) Musaelian, A.; Batzner, S.; Johansson, A.; Sun, L.; Owen, C. J.; Kornbluth, M.; Kozinsky, B. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications 2023, 14. (27) Batatia, I.; Kovacs, D. P.; Simm, G. N. C.; Ortner, C.; Csanyi, G. MACE: Higher Order Equivariant Message Passing Neu- ral Networks for Fast and Accurate Force 15 Fields. Advances in Neural Information Processing Systems. 2022. (28) Schütt, K. T.; Unke, O. T.; Gastegger, M. Equivariant message passing for the pre- diction of tensorial properties and molec- ular spectra. 2021; https://arxiv.org/ abs/2102.03150. (29) Satorras, V. G.; Hoogeboom, E.; Welling, M. E(n) Equivariant Graph Neural Networks. 2021; https://arxiv.org/abs/2102.09844. (30) Bihani, V.; Pratiush, U.; Mannan, S.; Du, T.; Chen, Z.; Miret, S.; Mi- coulaut, M.; Smedskjaer, M. M.; Ranu, S.; Krishnan, N. M. A. EGraFFBench: Eval- uation of Equivariant Graph Neural Net- work Force Fields for Atomistic Simu- lations. 2023; https://arxiv.org/abs/ 2310.02428. (31) Schütt, K. T.; Arbabzadah, F.; Chmiela, S.; Müller, K. R.; Tkatchenko, A. Quantum-chemical insights from deep tensor neural net- works. Nature Communications 2017, 8, 13890. (32) Unke, O. T.; Meuwly, M. PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges. J Chem Theory Comput 2019, 15, 3678–3693. (33) Unke, O. T.; Meuwly, M. PhysNet: A Neural Network for Predicting Energies, Forces, Dipole Moments, and Partial Charges. Journal of Chemical Theory and Computation 2019, 15, 3678–3693. (34) Unke, O. T.; Chmiela, S.; Gastegger, M.; Schütt, K. T.; Sauceda, H. E.; Müller, K.- R. SpookyNet: Learning force fields with electronic degrees of freedom and nonlo- cal effects. Nature Communications 2021, 12. (35) Grimme, S. Semiempirical GGA-type density functional constructed with a long-range dispersion correction. Journal of Computational Chemistry 2006, 27, 1787–1799. (36) Fey, M.; Lenssen, J. E. Fast Graph Repre- sentation Learning with PyTorch Geomet- ric. CoRR 2019, abs/1903.02428. (37) Eastman, P.; Behara, P. K.; Dotson, D. L.; Galvelis, R.; Herr, J. E.; Horton, J. T.; Mao, Y.; Chodera, J. D.; Pritchard, B. P.; Wang, Y.; Fabritiis, G. D.; Mark- land, T. E. SPICE, A Dataset of Drug-like Molecules and Peptides for Training Ma- chine Learning Potentials. Scientific Data 2023, 10. (38) Ramakrishnan, R.; Dral, P. O.; Rupp, M.; von Lilienfeld, O. A. Quantum chem- istry structures and properties of 134 kilo molecules. Scientific Data 2014, 1. (39) Cheng, B.; Engel, E. A.; Behler, J.; Del- lago, C.; Ceriotti, M. Ab initio thermody- namics of liquid and solid water. Proceed- ings of the National Academy of Sciences 2019, 116, 1110–1115. (40) Chmiela, S.; Tkatchenko, A.; Sauceda, H. E.; Poltavsky, I.; Schütt, K. T.; Müller, K.-R. Machine learning of accurate energy-conserving molecular force fields. Science Advances 2017, 3. (41) https://figshare.com/articles/ dataset/Revised_MD17_dataset_ rMD17_/12672038. (42) Chmiela, S.; Vassilev-Galindo, V.; Unke, O. T.; Kabylda, A.; Sauceda, H. E.; Tkatchenko, A.; Müller, K.-R. Accurate global machine learning force fields for molecules with hundreds of atoms. Science Advances 2023, 9. (43) Smith, J. S.; Isayev, O.; Roitberg, A. E. ANI-1: an extensible neural network potential with DFT accuracy at force field computational cost. Chemical Sci- ence 2017, 8, 3192–3203. 16 (44) Smith, J. S.; Zubatyuk, R.; Nebgen, B.; Lubbers, N.; Barros, K.; Roitberg, A. E.; Isayev, O.; Tretiak, S. The ANI-1ccx and ANI-1x data sets, coupled-cluster and density functional theory properties for molecules. Scientific Data 2020, 7. (45) Devereux, C.; Smith, J. S.; Huddle- ston, K. K.; Barros, K.; Zubatyuk, R.; Isayev, O.; Roitberg, A. E. Extending the Applicability of the ANI Deep Learn- ing Molecular Potential to Sulfur and Halogens. Journal of Chemical Theory and Computation 2020, 16, 4192–4202, PMID: 32543858. (46) Smith, J. S.; Nebgen, B.; Lubbers, N.; Isayev, O.; Roitberg, A. E. Less is more: Sampling chemical space with ac- tive learning. The Journal of Chemical Physics 2018, 148, 241733. (47) Jia, Z.; Padon, O.; Thomas, J.; Warsza- wski, T.; Zaharia, M.; Aiken, A. TASO: Optimizing Deep Learning Computation with Automatic Generation of Graph Sub- stitutions. Proceedings of the 27th ACM Symposium on Operating Systems Princi- ples. New York, NY, USA, 2019; p 47–62. (48) Tillet, P.; Kung, H. T.; Cox, D. Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations. Pro- ceedings of the 3rd ACM SIGPLAN In- ternational Workshop on Machine Learn- ing and Programming Languages (MAPL 2019). Phoenix, Arizona, United States, 2019. (49) Appleyard, J.; Kociský, T.; Blunsom, P. Optimizing Performance of Recurrent Neural Networks on GPUs. CoRR 2016, abs/1604.01946. (50) Wang, G.; Lin, Y.; Yi, W. Kernel fu- sion: An effective method for better power efficiency on multithreaded GPU. 2010 IEEE/ACM Int’l Conference on Green Computing and Communications & Int’l Conference on Cyber, Physical and Social Computing. 2010; pp 344–350. (51) Filipovič, J.; Madzin, M.; Fousek, J.; Matyska, L. Optimizing CUDA code by kernel fusion: application on BLAS. The Journal of Supercomputing 2015, 71, 3934–3957. (52) Nguyen, H.; Corporation, N. GPU Gems 3; Lab Companion Series v. 3; Addison- Wesley, 2008. (53) Tang, Y.-H.; Karniadakis, G. E. Acceler- ating dissipative particle dynamics simula- tions on GPUs: Algorithms, numerics and applications. Computer Physics Commu- nications 2014, 185, 2809–2822. (54) Doerr, S.; Majewski, M.; Pérez, A.; Krämer, A.; Clementi, C.; Noe, F.; Giorgino, T.; Fabritiis, G. D. TorchMD: A Deep Learning Framework for Molecular Simulations. Journal of Chemical Theory and Computation 2021, 17, 2355–2363. (55) Anderson, B.; Hy, T.-S.; Kondor, R. Cor- morant: Covariant Molecular Neural Net- works. Proceedings of the 33rd Interna- tional Conference on Neural Information Processing Systems. Red Hook, NY, USA, 2019. (56) Brandstetter, J.; Hesselink, R.; van der Pol, E.; Bekkers, E. J.; Welling, M. Ge- ometric and Physical Quantities Improve E(3) Equivariant Message Passing. 2021; https://arxiv.org/abs/2110.02905. (57) Schütt, K. T.; Sauceda, H. E.; Kinder- mans, P.-J.; Tkatchenko, A.; Müller, K.- R. SchNet – A deep learning architecture for molecules and materials. The Journal of Chemical Physics 2018, 148, 241722. (58) Liao, Y.-L.; Smidt, T. Equiformer: Equiv- ariant Graph Attention Transformer for 3D Atomistic Graphs. The Eleventh In- ternational Conference on Learning Rep- resentations. 2023. (59) Gasteiger, J.; Giri, S.; Margraf, J. T.; Günnemann, S. Fast and Uncertainty- Aware Directional Message Passing for 17 Non-Equilibrium Molecules. 2020; https: //arxiv.org/abs/2011.14115. (60) Liu, Y.; Wang, L.; Liu, M.; Zhang, X.; Oztekin, B.; Ji, S. Spherical Message Pass- ing for 3D Graph Networks. 2021; https: //arxiv.org/abs/2102.05013. (61) Fu, X.; Wu, Z.; Wang, W.; Xie, T.; Keten, S.; Gomez-Bombarelli, R.; Jaakkola, T. S. Forces are not Enough: Benchmark and Critical Evaluation for Machine Learning Force Fields with Molecular Simulations. Transactions on Machine Learning Research 2023, Survey Certification. (62) Vita, J. A.; Schwalbe-Koda, D. Data effi- ciency and extrapolation trends in neural network interatomic potentials. Machine Learning: Science and Technology 2023, 4, 035031. (63) Wagner, J. et al. openforcefield/openff- toolkit: 0.14.5 Minor feature release. 2023; https://doi.org/10.5281/ zenodo.10103216. A Hyperparameters In the pursuit of transparency and reproducibil- ity, this appendix provides a detailed account of the hyperparameters employed in our computa- tional experiments. The tables contained herein present the specific values and settings used to achieve the results discussed in the main body of this paper. Readers and fellow researchers are encouraged to refer to these tables when at- tempting to replicate our results or when uti- lizing the torchmd-train utility for their own training purposes. Table 3: MD17 hyperparameters used for the ET training in Figure 5. Parameter Value activation silu attn_activation silu batch_size 8 cutoff_lower 0.0 cutoff_upper 5.0 derivative True distance_influence both early_stopping_patience 300 ema_alpha_neg_dy 1.0 ema_alpha_y 0.05 embedding_dimension 128 lr 1e-3 lr_factor 0.8 lr_min 1e-7 lr_patience 30 lr_warmup_steps 1000 neg_dy_weight 0.8 num_heads 8 num_layers 6 num_rbf 32 seed 1 train_size 950 val_size 50 vector_cutoff True y_weight 0.2 18 Table 4: QM9 U0 hyperparameters used to ob- tain the results for ETnew in Table 1. Parameter Value activation silu attn_activation silu batch_size 128 cutoff_lower 0.0 cutoff_upper 5.0 derivative False distance_influence both early_stopping_patience 150 ema_alpha_neg_dy 1.0 ema_alpha_y 1.0 embedding_dimension 256 lr 4e-4 lr_factor 0.8 lr_min 1e-7 lr_patience 15 lr_warmup_steps 10000 neg_dy_weight 0.0 num_heads 8 num_layers 8 num_rbf 64 remove_ref_energy true seed 1 train_size 110000 val_size 10000 vector_cutoff True y_weight 1.0 Table 5: QM9 U0 hyperparameters used for the trainings in Figure 4 to obtain the results for TensorNet 3L* in Table 1. Parameter Value activation silu batch_size 16 cutoff_lower 0.0 cutoff_upper 5.0 derivative False early_stopping_patience 150 embedding_dimension 256 equivariance_invariance_group O(3) gradient_clipping 40 lr 1e-4 lr_factor 0.8 lr_min 1e-7 lr_patience 15 lr_warmup_steps 1000 neg_dy_weight 0.0 num_layers 3 num_rbf 64 remove_ref_energy true seed 2 train_size 110000 val_size 10000 y_weight 1.0 19 Table 6: ANI2x hyperparameters used for Ten- sorNet training. The model was then used to run the stable molecular dynamics simulations in Figure 6. Parameter Value activation silu batch_size 256 cutoff_lower 0.0 cutoff_upper 5.0 derivative True early_stopping_patience 50 embedding_dimension 128 equivariance_invariance_group O(3) gradient_clipping 100 lr 1e-3 lr_factor 0.5 lr_min 1e-7 lr_patience 4 lr_warmup_steps 1000 neg_dy_weight 100 num_layers {0,2} num_rbf 32 seed 1 train_size 0.9 val_size 0.1 y_weight 1.0 20 "
}