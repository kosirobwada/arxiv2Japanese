{
    "optim": "TorchMD-Net 2.0: Fast Neural Network\nPotentials for Molecular Simulations\nRaul P. Pelaeza,† Guillem Simeona,† Raimondas Galvelis,†,‡ Antonio Mirarchi,†\nPeter Eastman,¶ Stefan Doerr,‡ Philipp Thölke,‡ Thomas E. Markland,¶ and\nGianni De Fabritiis∗,†,‡,§\n†Computational Science Laboratory, Universitat Pompeu Fabra, Barcelona Biomedical\nResearch Park (PRBB), C Dr. Aiguader 88, 08003 Barcelona, Spain.\n‡Acellera Labs, C Dr Trueta 183, 08005, Barcelona, Spain\n¶Department of Chemistry, Stanford University, Stanford, CA 94305, USA\n§Institució Catalana de Recerca i Estudis Avançats (ICREA), Passeig Lluis Companys 23,\n08010 Barcelona, Spain\nE-mail: g.defabritiis@gmail.com\naR.P.P. and G.S. contributed equally to this work.\nAbstract\nAchieving a balance between computational\nspeed, prediction accuracy, and universal appli-\ncability in molecular simulations has been a per-\nsistent challenge. This paper presents substan-\ntial advancements in the TorchMD-Net soft-\nware, a pivotal step forward in the shift from\nconventional force fields to neural network-\nbased potentials. The evolution of TorchMD-\nNet into a more comprehensive and versa-\ntile framework is highlighted, incorporating\ncutting-edge architectures such as TensorNet.\nThis transformation is achieved through a mod-\nular design approach, encouraging customized\napplications within the scientific community.\nThe most notable enhancement is a signifi-\ncant improvement in computational efficiency,\nachieving a very remarkable acceleration in the\ncomputation of energy and forces for TensorNet\nmodels, with performance gains ranging from 2-\nfold to 10-fold over previous iterations. Other\nenhancements include highly optimized neigh-\nbor search algorithms that support periodic\nboundary conditions and the smooth integra-\ntion with existing molecular dynamics frame-\nworks. Additionally, the updated version intro-\nduces the capability to integrate physical priors,\nfurther enriching its application spectrum and\nutility in research. The software is available at\nhttps://github.com/torchmd/torchmd-net.\n1\nIntroduction\nNeural\nNetwork\nPotentials\n(NNPs)1–7\nare\nemerging as a key approach in molecular sim-\nulations, striving to optimize the balance be-\ntween computational efficiency, predictive ac-\ncuracy, and generality.\nSome software frameworks to facilitate the\nuse of neural network potentials have been\ndeveloped, such as SchNetPack,8 TorchANI,9\nDeePMD-Kit,10 and others.\nAmong the first\nto appear, we released TorchMD-Net, initially\ndesigned for the Equivariant Transformer ar-\nchitecture11 and a graph network, a simpler\ninvariant graph neural network tailored for\nneural network potentials for protein coarse-\ngraining.12 Over time, TorchMD-Net has ex-\npanded its model architectures to include Ten-\nsorNet,13 an O(3)-equivariant message-passing\n1\narXiv:2402.17660v1  [cs.LG]  27 Feb 2024\nneural network utilizing rank-2 Cartesian ten-\nsor representations which achieved state-of-the-\nart accuracy on benchmark datasets. This evo-\nlution positions TorchMD-Net not just as a\nstandalone tool, but as a versatile library for\nthe development of NNPs.\nEfficiency has been at the forefront of re-\ncent enhancements to TorchMD-Net.\nAmong\nthe optimizations, CUDA graphs have been\nintegrated, providing a performance boost es-\npecially for smaller workloads. TorchMD-Net\nhas also incorporated the latest versions of\nits key dependencies (mainly PyTorch14 and\nPyTorch Lightning15), with a notable addi-\ntion being the search for compatibility with\nthe torch.compile submodule from PyTorch\n2.0, a feature that compiles Just-In-Time (JIT)\nthe modules into optimized kernels.\nWhile\nTorchMD-Net has introduced low precision\nmodes (i.e.\nbfloat16) primarily as an ex-\nploratory tool for researchers, high precision\n(float64) is also available for ensuring detailed\ncorrectness checks during prototyping.\nThe new technical enhancements include\nthe introduction of periodic boundary condi-\ntions, a CUDA-optimized neighbor list, and\nmemory-efficient dataset loaders.\nThe inclu-\nsion of TorchMD-Net in the conda-forge16\npackage repository and the release of the doc-\numentation17 are steps taken to enhance its\naccessibility to researchers.\nAnother feature\nis TorchMD-Net’s capacity to blend empirical\nphysical knowledge into NNPs via priors. The\nintegration of atom-wise and molecule-wise pri-\nors, such as the Ziegler-Biersack-Littmark18\nand Coulomb potentials, allows for a more nu-\nanced approach in simulations.\nTorchMD-Net emphasizes compatibility with\nleading molecular dynamics (MD) packages,\nespecially with OpenMM.19 OpenMM, widely\nrecognized in the computational chemistry\nfield, can now interface directly with TorchMD-\nNet through the OpenMM-Torch20\nplugin.\nThis integration has been a collaborative ef-\nfort, with OpenMM-Torch being co-developed\nby the core teams of both OpenMM and\nTorchMD-Net.\nThis ensures streamlined and\neffective utilization of TorchMD-Net models\nwithin OpenMM’s simulation framework.\nIn the following sections we provide an\noverview of the TorchMD-Net framework. The\nmanuscript is ordered as follows. In the Meth-\nods section we go over the currently available\nNNP architectures.\nWe continue in section\nTraining with details about the different parts\ninvolved in the training and deployment of\nthese architectures and how they are exposed\nin TorchMD-Net. Then, in section Optimiza-\ntion, we lay out the optimization strategies\nemployed in this release. Finally, we present a\nseries of validation and performance results in\nthe Results section.\nTorchMD-Net is freely available with a per-\nmissive licence (MIT) at https://github.\ncom/torchmd/torchmd-net.\n2\nMethods\nWe interpret a neural network potential as a\nmachine learning model that takes as input a se-\nries of atomic positions, denoted by R, embed-\nding indices such as atomic numbers, Z, and op-\ntionally charges (which might be per-sample or\nper-atom), q, and outputs a per-sample scalar\nvalue and optionally its negative gradient with\nrespect to the positions, typically interpreted as\nthe potential energy and atomic forces, respec-\ntively. Note, however, that TorchMD-Net is not\nlimited to this interpretation of the outputs,\nwhich are generally labelled as y and neg_dy\nrespectively.\nFigure 1 provides a comprehensive overview\nof the TorchMD-Net architecture.\nThe dia-\ngram’s left section illustrates the various com-\nponents of the primary module, designated\nas TorchMD_Net, which constitutes, concep-\ntually and in the API itself, a NNP model.\nEach component within this object is modu-\nlar and customizable, allowing for the creation\nof diverse models.\nAt the heart of the NNP\nis the representation_model.\nThis part of\nthe architecture takes the set of inputs stated\nabove and outputs a series of per-atom fea-\ntures. These features are subsequently fed into\nan output_model. The purpose of this model\nis to further process these features into single\natomic values, which typically will be aggre-\n2\nFigure\n1:\nThe\nmain\nmodule\nin\nTorchMD-Net\nis\ncalled\nTorchMD_Net\nfrom\nthe\ntorchmdnet.models.model module. This class combines a given representation model (such as\nthe Equivariant Transformer), an output model (such as the scalar output module) and a prior\nmodel (such as the Atomref prior), producing a module that takes as input a series of atoms\nfeatures and outputs a scalar value (i.e energy per molecule) and when derivative = True, its\nnegative gradient with respect to the input positions (i.e atomic forces).\n3\ngated and will represent the total potential en-\nergy, though it can represent other per-sample\nor per-atom quantities as well, depending on\nthe specifics of its design and (optional) aggre-\ngation scheme.\nOutput models normally in-\nclude learnable parameters (e.g.\na multilayer\nperceptron).\nPrior models can be employed\nto augment either the atom-level features or\nthe aggregated per-molecule value with further\nphysical insights. Furthermore, the framework\nintegrates PyTorch’s Autograd for automatic\ndifferentiation, enabling the computation of the\nnegative gradient of the per-molecule scalar pre-\ndiction with respect to atomic positions. This\nis particularly relevant when interpreting the\nper-molecule value as the potential energy, as\nit yields the atomic forces in a way that en-\nsures, by construction, that the resulting force\nfield is energy conserving.\nThis modular logic allows for flexibility in the\ncombination of representation models and out-\nput models. Therefore, by building a custom\noutput module, researchers can make use of the\nrepresentation models for other prediction tasks\nbeyond potential energy and forces.\n2.1\nAvailable representation mod-\nels\nCurrent models in TorchMD-Net at the time\nof writing are message-passing neural net-\nworks21,22 (MPNNs) which learn approxima-\ntions to the many-body potential energy func-\ntion.\nAtoms are identified with graph nodes\nembedded in 3D space, building edges between\nthem after the definition of some cutoff radius.\nThe neural network uses atomic and geometric\ninformation to learn expressive representations\nby propagating, aggregating, and transforming\nfeatures from neighboring nodes found within\nthe cutoff radius.23,24 In most current NNPs, af-\nter several message passing steps, node features\nare used to predict per-atom scalar quantities\nwhich are identified with atomic contributions\nto the energy of the molecule.\n2.1.1\nNew architecture: TensorNet\nTensorNet13\nis\nan\nO(3)-equivariant\nmodel\nbased on rank-2 Cartesian tensor representa-\ntions. Euclidean neural network potentials25–27\nhave been shown to achieve state-of-the-art\nperformance and better data efficiency than\nprevious models, relying on higher-rank equiv-\nariant features which are irreducible represen-\ntations of the rotation group, in the form of\nspherical tensors. However, the computation of\ntensor products in these models can be compu-\ntationally demanding. In contrast, TensorNet\nexploits the use of Cartesian rank-2 tensors\n(3x3 matrices) which can be very efficiently\ndecomposed into scalar, vector and rank-2 ten-\nsor features.\nFurthermore, Clebsch-Gordan\ntensor products are substituted by straight-\nforward and node-level 3x3 matrix products.\nOverall, these properties allow TensorNet to\nachieve state-of-the-art accuracy on common\nbenchmark datasets with a reduced number of\nmessage-passing steps, learnable parameters,\nand computational cost.\nThe prediction of\nup to rank-2 molecular properties that behave\nappropriately under geometric transformations\nsuch as reflections and rotations is also possible.\n2.1.2\nEquivariant Transformer\nThe Equivariant Transformer11 (ET) is an\nequivariant neural network that uses both\nscalar and Cartesian vector representations.\nThe distinctive feature of the ET in compari-\nson to other Cartesian vector models such as\nPaiNN28 or EGNN29 is the use of a distance-\ndependent dot product attention mechanism,\nwhich achieved state-of-the-art performance on\nbenchmark datasets at the time of publica-\ntion.\nFurthermore, the analysis of attention\nweights allowed us to extract insights into the\ninteraction of different atomic species for the\nprediction of molecular energies and forces.\nThe model also exhibits a low computational\ncost for inference and training in comparison to\nsome of the most used NNPs in the literature.30\nAs part of the current release, we removed a\ndiscontinuity at the cutoff radius. In the orig-\ninal description, vector features’ residual up-\ndates, as opposed to scalar features’ updates,\n4\nreceived contributions from the value pathway\nof the attention mechanism which were not be-\ning properly weighted by the cosine cutoff func-\ntion envelope, which is reflected in Eq. 9 in the\noriginal paper.11 We fixed it by applying ϕ(dij),\ni.e., split(Vj ⊙ DV\nij) → split(ϕ(dij)Vj ⊙ DV\nij). To\nensure backward compatibility, this modifica-\ntion is only applied when setting the new ET ar-\ngument vector_cutoff = True. The impact\nof this modification is evaluated in the results\nsection.\n2.1.3\nGraph Network\nThe graph network is an invariant model in-\nspired by both the SchNet31 and PhysNet32 ar-\nchitectures. The network was optimized to have\nsatisfactory performance on coarse-grained pro-\nteins, allowing the building of NNPs that cor-\nrectly reproduce fast-folder protein free energy\nlandscapes.12 In contrast to the ET and Ten-\nsorNet, the graph network only uses relative\ndistances between atoms as geometrical infor-\nmation, which are invariant to translations, ro-\ntations, and reflections. The distances are used\nby the model to learn a set of continuous fil-\nters that are applied to feature graph convolu-\ntions as in SchNet,31 progressively updating the\ninitial atomic embeddings by means of residual\nconnections.\n2.2\nPrior models\nPriors are additional physical terms that can\nbe introduced for the prediction of potential\nenergies. Some of these terms have been used\nin NNPs in the literature,33,34 sometimes even\nincluding learnable parameters. In TorchMD-\nNet, we provide some predefined priors, which\ncan be optionally added to the neural network\nprediction:\n• Atomref: These are per-element atomic\nreference energies, which are usually pro-\nvided directly in the dataset. In this case,\nthe neural network has to predict the re-\nmaining contribution to the potential en-\nergy, which can be regarded as the forma-\ntion energy of the molecule. There is also\nthe option of making this prior learnable,\nin which case it is initialized with atomic\nreference energies, but these contributions\nare modified during training.\n• Coulomb: This prior corresponds to the\nusual Coulomb electrostatic interaction,\nscaled by a cosine switching function to\nreduce its effect at short distances. Us-\ning this prior requires providing per-atom\npartial charges.\n• D2 dispersion: In this case, the prior cor-\nresponds to the D2 dispersive correction\nused in DFT-D2.35 C6 coefficients and\nVan der Waals radii for elements are al-\nready incorporated in the method.\n• ZBL potential:\nThis prior implements\nthe Ziegler-Biersack-Littmark (ZBL) po-\ntential for screened nuclear repulsion as\ndescribed in Ref 18. It is an empirical po-\ntential effectively describing the repulsion\nbetween atoms at very short distances,\nand only atomic numbers need to be pro-\nvided.\nNote that forces are computed directly by\nautograd when adding the energy contribu-\ntions coming from the priors before the back-\nward automatic differentiation step.\nEven\nthough the previous terms are the currently\npredefined options in TorchMD-Net, all these\npriors are derived from a general BasePrior\nclass, which easily allows researchers to imple-\nment their own priors, following the modular\nlogic behind the framework.\n2.3\nTraining\nThe right diagram in Figure 1 depicts the main\ntraining loop in TorchMD-Net. A Dataset pro-\nvides sample/output pairs for the NNP and is\ndivided into training, validation and testing sets\nand batched by a Dataloader (as provided by\nthe Pytorch Geometric library36). We make use\nof the PyTorch Lightning library’s15 trainer,\nwhich also allows multi-GPU training. Check-\npoints are generated during training, contain-\ning the current weights of the model, which can\nthen be subsequently loaded for inference or fur-\nther training.\n5\n2.3.1\nDatasets\nWithin TorchMD-Net, datasets can be ac-\ncessed\nthrough\nthe\nYAML\nconfiguration\nfile for use with the torchmd-train util-\nity\nor\nprogrammatically\nvia\nthe\nPython\nAPI. Predefined datasets include SPICE,37\nQM9,38 WaterBox,39 (r)MD1740,41 MD22,42\nANI1,43 ANI1x,44 ANI1ccx,44 ANI2x45 and\nthe COMP646 evaluation dataset with all its\nsubsets\n(ANIMD,\nDrugBank,\nGDB07to09,\nGDB10to13, Tripeptides and S66X8), offer-\ning diverse training environments for molecular\ndynamics and quantum chemistry applications.\nThese datasets serve as common benchmarks in\nthe field of neural network potentials. However,\non top of these, the framework allows the flexi-\nble incorporation of user-generated datasets for\ncustomized applications. The Custom dataset\nfunctionality allows users to train models with\nmolecular data encapsulated in simple NumPy\nfile formats without writing a single line of code.\nBy specifying paths to coordinate and embed-\nding index (e.g. atomic numbers) and reference\nenergy and force files, researchers can easily\nintegrate their datasets into the training pro-\ncess.\nThis capability ensures TorchMD-Net’s\nadaptability to a wider array of applications\nbeyond its pre-packaged offerings. In addition,\nTorchMD-Net offers support for other popular\ndataset formats, such as HDF5. Special care\nis taken to ensure data is cached as much as\npossible, using techniques such as in memory\ndatasets and memory mapped files.\n2.3.2\nLosses\nDuring training,\na weighted sum of mean\nsquared error (MSE) losses of energy and forces\nis used, weighting each of them according to\nuser input. In validation, we provide both L1\nand MSE losses separately for energies and\nforces, while for testing L1 losses alone are\nused.\nThe framework allows to use an expo-\nnential moving average (EMA) to update the\nlosses during the training and validation stages\nto smooth out progression of loss values.\n2.4\nUsage examples\nIn the following sections we showcase code for\nsome typical usecases of TorchMD-Net. While\nthese snippets are generally self-contained the\nreader is pointed to the online documentation17\nfor further information.\n2.4.1\nTraining code example\nThe\nproject\nintroduces\na\ncommand\nline\ntool, torchmd-train, designed as a code-free\nmethod for model training. This tool is set up\nthrough a YAML file, with several examples\navailable in the TorchMD-Net GitHub reposi-\ntory for reference.\nHowever, we also offer an\nillustrative script here that outlines the pro-\ncess of training an existing model using the\nPython API. The LNNP class, found within the\ntorchmdnet.module module, encapsulates the\nprocedures for both the creation and training\nof a model.\nThis class is inherited from Py-\ntorch Lightning LightningModule, offering all\nthe extensive customization available in it. The\nfollowing is a succinct yet comprehensive exam-\nple of how to utilize LNNP for training purposes:\n1\nfrom\ntorchmdnet.data\nimport\nDataModule\n2\nfrom\ntorchmdnet.module\nimport\nLNNP\n3\nfrom\npytorch_lightning\nimport\nTrainer\n4\nargs = {\n5\n’dataset ’: ’ANI1X ’,\n6\n’model ’: ’tensornet ’,\n7\n’num_epochs ’: 200,\n8\n’embedding_dimension ’: 128,\n9\n’num_layers ’: 2,\n10\n’num_rbf ’: 32,\n11\n’rbf_type ’: ’expnorm ’,\n12\n’trainable_rbf ’: False ,\n13\n’activation ’: ’silu ’,\n14\n’cutoff_lower ’: 0.0,\n15\n’cutoff_upper ’: 5.0,\n16\n’max_z ’: 100,\n17\n’max_num_neighbors ’: 64,\n18\n’derivative ’: True # So the\nmodel\nreturns\nforces.\n19\n}\n20\ndata = DataModule(args)\n21\ndata. prepare_data ()\n22\ndata.setup(\"fit\")\n23\nlnnp = LNNP(args ,\n24\nprior_model =None ,\n25\nmean=data.mean ,\n26\nstd=data.std)\n27\ntrainer = Trainer(max_epochs=args[’num_epochs ’])\n28\ntrainer.fit(lnnp , data)\n29\nmodel = LNNP. load_from_checkpoint (trainer. checkpoint_callback .\nbest_model_path )\n30\ntrainer = pl.Trainer( inference_mode =False)\n31\ntrainer.test(model , data)\nThis example shows the minimal steps required\nto prepare data, initialize the LNNP class, train\nand test a model using PyTorch Lightning’s\nTrainer.\nThe Trainer here is simplified for\nbrevity; in practice, additional callbacks and\nlogger configurations could be added.\n6\n2.4.2\nLoading a Trained Model for In-\nference\nAfter training a model, the next logical step\nis to use it for inference. TorchMD-Net offers\na dedicated function, load_model, to facilitate\nthis. Below is a concise example:\n1\nfrom\ntorchmdnet.models.model\nimport\nload_model\n2\n# Define\nthe\npath to the\nsaved\nmodel\ncheckpoint\n3\ncheckpoint_path = \"path/to/ saved_model_checkpoint .ckpt\"\n4\n# Load\nthe\nmodel\n5\nloaded_model = load_model( checkpoint_path )\n6\n# Prepare\nthe\ninput\ndata (atomic\nnumbers , positions , batch\nindex , etc.)\n7\n# For\ndemonstration , these\nare\nplaceholders\nand\nshould be\nreplaced\nwith\nactual\ndata\n8\ninput_data = {\n9\n’z’: torch.Tensor ([...]) ,\n10\n’pos’: torch.Tensor ([...]) ,\n11\n’batch ’: torch.Tensor ([...])\n12\n# ...\nother\noptional\nfields\n13\n}\n14\n# Perform\ninference\n15\nenergy , forces = loaded_model (** input_data)\n16\n# Energy\nand\nforces\nare now\navailable\nfor\nfurther\nanalysis\nor\nvisualization\nIn this example, checkpoint_path should\npoint to the location where the trained model\ncheckpoint is saved.\nThe input_data dic-\ntionary should be populated with the actual\natomic numbers, positions, and other required\nor optional fields. Finally, energy and forces\nare obtained from the loaded model and can be\nused as needed.\n2.4.3\nIntegration with OpenMM\nIt is possible to run TorchMD-Net neural net-\nwork potentials as force fields in OpenMM19\nto run molecular dynamics.\nThe OpenMM-\nTorch20 package is leveraged for this.\nIn-\ntegration consists of writing a wrapper class\nto accommodate for the unit requirements of\nOpenMM and to provide to the model any in-\nformation not proper to OpenMM (like the em-\nbedding indices). The following code showcases\nan example on how to add a TorchMD-Net\nNNP as an OpenMM Force.\n1\nfrom\ntorchmdnet.models.model\nimport\nload_model\n2\nfrom\nopenmmtorch\nimport\nTorchForce\n3\nfrom\nopenmm\nimport\nSystem\n4\n5\nclass\nWrapper(torch.nn.Module):\n6\n7\ndef\n__init__(self , embeddings , model):\n8\nsuper(Wrapper , self).__init__ ()\n9\nself.embeddings = embeddings\n10\n# OpenMM\nwill\ncompute\nthe\nforces\n11\n#\nby\nbackpropagating\nthe energy ,\n12\n# so we can\nload\nthe\nmodel\nwith\nderivative=False\n13\nself.model = load_model(model , derivative=False)\n14\n15\ndef\nforward(self , positions):\n16\n# OpenMM\nworks\nwith\nnanometer\npositions\n17\n#\nand\nkilojoule\nper\nmole\nenergies\n18\n# Depending\non the model , you\nmight\nneed\n19\n#\nto\nconvert\nthe\nunits\n20\npositions = positions * 10.0 # nm -> A\n21\nenergy = self.model(z=self.embeddings , pos=positions)\n[0]\n22\nreturn\nenergy * 96.4916 # eV -> kJ/mol\n23\n24\n# The\nembeddings\nused\nduring\ntraining (e.g. atomic\nnumbers)\n25\n#\nfor\neach\natom in the\nsimulation.\n26\nz = torch.tensor ([1,1],\ntorch.long)\n27\nmodel = torch.jit.script(Wrapper(z, \"model.ckpt\"))\n28\n# Create a TorchForce\nobject\nfrom\nthe\nmodel\n29\ntorch_force = openmmtorch.TorchForce(model)\n30\nsystem = System ()\n31\n# The\nTorchForce\nobject\ncan be used as a regular\nOpenMM\nForce\n32\nsystem.addForce(torch_force )\n33\n# Set up the\nrest of the\nOpenMM\nsimulation\n34\n# ...\n2.5\nOptimization techniques\nTypical neural network potential (NNP) algo-\nrithms implemented in PyTorch14 comprise a\nseries of sequential operations such as multi-\nlayer perceptrons and message passing opera-\ntions.\nAs PyTorch operations translate into highly\noptimized CUDA kernel calls, the efficiency\nof modern GPUs often turns kernel launch-\ning overhead into a performance bottleneck.\nCUDA graphs address this by consolidating\nmultiple kernel calls into a single graph, drasti-\ncally reducing kernel launch overhead.\nHow-\never, CUDA graphs impose stringent limita-\ntions. These include the need for static shapes\nin graphed code sections, which can lead to\ncostly recompilations or memory inefficiencies,\nand the exclusion of operations requiring CPU-\nGPU synchronization.\nConversely, developments in the compiler\ncommunity47 , including technologies like Ope-\nnAI’s Triton48 and subsequently PyTorch en-\nhancements,\nare gradually diminishing the\nreliance on CUDA graphs by automatically\nchanging the structure of the code in ever more\nprofound ways (i.e kernel fusion49–51).\nThese\nadvancements, such as TorchDynamo intro-\nduced in PyTorch 2.0 through torch.compile,\noptimize code structure through Just-In-Time\n(JIT) compilation.\nEven with JIT, and in general transpilation-\nbased techniques, CUDA graphs often provide\nthe best out-of-the-box performance improve-\nments and at the bare minimum, facilitate the\noptimizations introduced by the former.\nEn-\ncapsulating a piece of code within a CUDA\ngraph, a process known as ’stream capture’, ne-\ncessitates adherence to several specific require-\nments. This often demands substantial mod-\n7\nifications to the code.\nCrucially, for code to\nbe eligible for capture, it must avoid any CPU-\nGPU synchronization activities, including syn-\nchronization barriers and memory copies. Ad-\nditionally, all arrays involved in the operations\nmust possess static shapes and fixed memory\naddresses, precluding any dynamic memory al-\nlocations during the process.\nThe CUDA graph interface in PyTorch allevi-\nates many challenges associated with adapting\ncode for stream capture. It particularly excels\nin managing memory allocations within cap-\ntured environments automatically and trans-\nparently.\nHowever, challenges arise in spe-\ncific implementations, as exemplified by Ten-\nsorNet.\nThe main issue in TensorNet is its\nneighbor list, which inherently varies in shape\nat each inference step due to the fluctuating\nnumber of neighbors.\nThis variation affects\nthe early stages of the architecture, resulting\nin TensorNet primarily operating on dynami-\ncally shaped tensors. To address this, we imple-\nmented a static shape mode that creates place-\nholder neighbor pairs up to a predetermined\nmaximum. We then ensure the architecture dis-\nregards these placeholders’ contributions. Al-\nthough this method increases the initial work-\nload, our empirical data indicates that the per-\nformance gains from capturing the entire net-\nwork substantially outweigh this added over-\nhead.\nIn the following sections, we explore the im-\npact of these optimizations on both inference\nand training performance.\n2.5.1\nNeighbor\nsearch\nand\nperiodic\nboundary conditions\nMessage-passing neural networks, such as the\narchitectures currently supported in the frame-\nwork, require a list of connections among nodes\nreferred to as edges. This list is constructed by\nproximity after the definition of a cutoff radius\n(a neighbor list). TorchMD-Net offers a neigh-\nbor list construction engine specifically tailored\nfor NNPs, exposing a naive brute-force O(N 2)\nalgorithm that works best for small workloads\nand a cell list (a standard O(N) hash-and-sort\nstrategy widely used in MD52,53) that performs\nbetter for large systems (see Figures 3 and 2).\nEffectively, this engine makes neighbor search a\nnegligible part of the overall computation.\nSpecial measures are taken into account to\nensure that the neighbor search is compatible\nwith CUDA-graphs. For this matter, it is re-\nquired that the neighbor search works on a\nstatically-shaped set of input/outputs, which\nposes a problem given that the number of neigh-\nbor pairs in the system is not known in advance\nand is bound to change from input to input.\nWe solve this by requiring an upper bound for\nthe number of pairs in the system and padding\nthe outputs with a special value (−1) for un-\nused pairs. Furthermore, TorchMD-Net archi-\ntectures support rectangular and triclinic peri-\nodic boundary conditions.\nContrary to usual MD workloads, it is com-\nmon to have batches of input samples in NNPs.\nThis owns to the very nature of neural net-\nwork training but also can benefit inference\n(for instance, allowing the possibility of running\nmany simulations in parallel, like TorchMD54\ndoes). Our neighbor list is able to handle ar-\nbitrary batch sizes while maintaining compati-\nbility with CUDA graphs. The current cell list\nimplementation constructs a single cell list in-\ncluding atoms for all batches, excluding pairs of\nparticles that pertain to different batches when\nfinding neighbors in it. This makes it so that\neach particle has to perform a check against ev-\nery other particle in the vicinity for all batches,\nwhich degrades performance with increasing\nbatch size.\nWe find this to be an acceptable\ncompromise given that doing it this way facili-\ntates compatibility with CUDA graphs and we\nassume that with increasing number of particles\n(where the cell list excels) the typical batch size\nwill decrease. Still, the particularities of the cell\nlist implementation makes its performance spe-\ncially susceptible to the batch size, as evidenced\nby the variability observed in the cell list curves\nin figures 3 and 2.\nAll data presented in this section was gath-\nered in an RTX4090 NVIDIA GPU using\nCUDA 12. Each point is obtained by averaging\n50 identical executions.\nWarmup executions\nare also performed before measuring.\n8\n0.5\n2\n8\n32\n128\n512\n1\n2\n3\n4\n5\n6\nBatch Size\nTime (ms)\nCell\nBrute force\nFigure 2: Performance comparison of cell (solid\nline) and brute-force (dashed line) neighbor\nsearch strategies across different batch sizes for\na random cloud of 32k particles with 64 neigh-\nbors per particle on average. The particles are\nsplit into a certain number of batches.\n103\n104\n105\n10−1\n100\n101\nNumber of particles\nTime (ms)\nBatch 1\nBatch 2\nBatch 64\nFigure 3: Performance comparison of cell (solid\nline) and brute-force (dashed line) neighbor\nsearch strategies across different batch sizes for\na random cloud of particles with 64 neighbors\nper particle on average. Cell list performance\ntends to degrade with increasing batch size,\nwhile the opposite is true for brute force.\n2.5.2\nTraining\nOptimizing neural network training presents\ndistinct challenges compared to inference op-\ntimization.\nPrimarily, the variable length of\neach training sample, exacerbated by batch-\ning processes (where a varying number of sam-\nples constitute a single network input), im-\npedes optimizations dependent on static shapes\n(i.e.\nCUDA graphs).\nA potential solution\ninvolves integrating ’ghost molecules’, akin to\nstrategies used in static neighbor list shap-\ning, to standardize the atom count inputted to\nthe network. However, this method increases\nmemory consumption in an already memory-\nconstrained environment and raises concerns\nabout the backpropagation of losses for these\nnon-existent atoms, which may lead to numer-\nical instability.\nMoreover, training necessitates backpropaga-\ntion through the network.\nIn our context,\nthis involves a double backpropagation process\nwhen the loss function includes force calcula-\ntions.\nCurrently, double backpropagation is\ninadequately supported by the PyTorch com-\npiler. A workaround is to manually implement\nthe network’s initial backward pass (specifi-\ncally, the force computation).\nThis adjust-\nment enables Autograd to perform only a single\nbackward pass during training, leveraging the\nPyTorch compiler’s capabilities. Nevertheless,\nchallenges persist with the PyTorch compiler\nwhen managing dynamic input shapes.\nGiven the current constraints, the current re-\nlease does not include any training-specific opti-\nmizations besides the improved dataloader sup-\nport as previously described.\n3\nResults\n3.1\nValidation\nIn this subsection, we evaluate the impact of the\narchitectural modifications introduced in the\nmodels on predictive accuracy. In the case of\nTensorNet the modifications targeted its com-\nputational performance alone, while for the ET\none needs to consider the changes induced by\nvector_cutoff = True.\n9\n3.1.1\nAccuracy with TensorNet\nOriginal test MAE presented in Ref. 13 for the\nQM9 U0 target quantity is 3.9(1) meV, while the\nlatest optimized versions of the model (see Fig-\nure 4) yield 3.8(2) meV, confirming that the ar-\nchitectural optimizations do not affect Tensor-\nNet’s prediction performance. The training loss\nwas computed in this case as the MSE between\npredicted and true energies. This state-of-the-\nart performance is achieved with the largest\nmodel with 4.0 million trainable parameters,\nwith specific architectural and training hyper-\nparameters being found in Table 5.\nWe also\nprovide in Table 1 the accuracy of smaller and\nshallower models on the same QM9 quantity\n(that is, using the same hyperparameters as\nin Table 5, except for embedding_dimension =\n128 and num_layers = 0, 1, 2), while com-\nparing them to other NNPs.\nOverall, Ten-\nsorNet demonstrates very satisfactory perfor-\nmances, achieving close to state-of-the-art ac-\ncuracy (< 5 meV MAE) with a very reduced\nnumber of parameters.\n101\n102\n103\n10−6\n10−5\n10−4\n10−3\n10−2\nEpoch\nMSE Loss\nTraining\nValidation\nFigure 4: Training and validation curves for\nTensorNet on the QM9 U0 benchmark, hyper-\nparameters are in Table 5.\n3.1.2\nAccuracy\nwith\nthe\nEquivariant\nTransformer\nAs previously mentioned, we provide an im-\nplementation of the ET where it is modified\nby applying the cutoff function to the values’\nTable 1: Mean absolute error in meV for dif-\nferent models trained on QM9 target property\nU0. TensorNet 3L* uses an embedding dimen-\nsion of 256, while in other cases 128. For the\nET, subscripts new and old correspond to the\nnew and the original implementation, that is,\nwith vector_cutoff = True and False, re-\nspectively.\nModel\nU0 MAE (meV)\nCormorant55\n22\nSEGNN56\n15\nSchNet57\n14\nEGNN29\n11\nEquiformer58\n6.6\nDimeNet++59\n6.3\nSphereNet60\n6.3\nETold 11\n6.2\nPaiNN28\n5.9\nAllegro26\n4.7\nMACE27\n4.1\nETnew\n5.7\nTensorNet 0L\n7.2\nTensorNet 1L\n4.7\nTensorNet 2L\n4.4\nTensorNet 3L*\n3.9\n10\npathway of the attention mechanism to enforce\na continuous energy landscape at the cutoff\ndistance. Therefore, we checked to which ex-\ntent these changes, together with TorchMD-\nNet’s ones, affect the accuracy of the Equiv-\nariant Transformer.\nWe trained the model\non the MD17 aspirin dataset (Figure 5) us-\ning the hyperparameters defined for the orig-\ninal version of the ET (Table 3, with the ad-\ndition of vector_cutoff = True), giving fi-\nnal test MAEs of 0.139 kcal/mol and 0.232\nkcal/mol/Åin energies and forces, respectively,\ncompared to the original implementation which\ngave 0.123 kcal/mol and 0.253 kcal/mol/Å.11\nRegarding QM9 U0, we reused the original hy-\nperparameters for the dataset found in Table\n4 (again, adding vector_cutoff = True), and\ncomparative results can be found in Table 1.\n101\n102\n103\n10−2\n10−1\n100\n101\n102\nEpoch\nMSE Loss\nTraining\nValidation\nFigure 5: Training and validation curves for the\nnew implementation of the Equivariant Trans-\nformer on the MD17 benchmark, hyperparam-\neters are in Table 3.\n3.2\nMolecular Simulations\nWe performed NVT molecular dynamics simu-\nlations employing TensorNet models trained on\nthe ANI-2x dataset.45 A table detailing the hy-\nperparameters is provided for reference in Table\n6. Note that we did not include any physical\npriors in these trainings nor in the subsequent\nsimulations, i.e all forces in the system come\nfrom the model itself. Starting from the SPICE\ndataset,37 we selected the PubChem subset and\nutilized it to create a test set comprising four\nrandomly chosen conformers.\nThis test set\naimed to evaluate the ability of the NNP to\nperform stable molecular dynamics (MD) sim-\nulations on molecules not encountered during\nthe training stage.61,62 The training dataset, as\nwell as the PubChem subset, represent a broad\ndiversification of molecules containing the el-\nements H, C, N, O, S, F, Cl.\nTo generate\nthe input data, the SMILES and the coordi-\nnates of interest were used to build a molecule\nobject using openff-toolkit,63 and the atomic\nnumbers were used as embeddings. Using the\nmore accurate TensorNet 2L model, a 50 ns\ntrajectory with a time-step of 1 fs was gen-\nerated for each molecule using OpenMM’s19\nLangevinMiddleIntegrator at 298.5K and a\nfriction coefficient of 1 ps−1. We also used for\none of the molecules a TensorNet 0L model with\nthe same simulation settings to test its stabil-\nity. A root mean square displacement (RMSD)\nanalysis was performed for each trajectory tak-\ning the starting conformation as reference, see\nFigure 6. The results highlight the model’s abil-\nity to run stable MD simulations, even for the\n0L case where the model’s receptive field and\nparameter count are substantially reduced.\n3.3\nSpeed performance\nAll results presented in this work were car-\nried out using an NVIDIA RTX4090 GPU\n(driver version 525.147) with a 32 core Intel(R)\nXeon(R) Silver 4110 CPU in Ubuntu 20.04. We\nused CUDA 12.0 with Pytorch version 2.1.0\nfrom conda-forge.\nWe provide all timings in\nmillion steps per day, which can be easily con-\nverted to nanoseconds per day.\nThese units\nare more commonly used in molecular dynamics\nsettings, and the conversion can be done by tak-\ning the quantity in million steps per day times\nthe timestep in femtoseconds.\nTherefore, for\nexample, 1 million steps per day is equivalent\nto 1 ns/day for a timestep of 1 fs.\nTo study the optimization strategies laid out\nin section 2.5 we show energy and forces in-\nference performance for several equivalent im-\nplementations of TensorNet in Figure 7. Note\n11\n0\n5\n10\n15\n20\n25\n30\n35\n40\n45\n50\n0.5\n1\n1.5\n2\n2.5\n3\nTime (ns)\nRMSD (Å)\nA-2L\nA-0L\nB-2L\nC-2L\nD-2L\nA: 135129529\nB: 136963008\nC: 160861289\nD: 252660926\nFigure 6: (Left) RMSD analysis for the trajectories of 4 molecules outside of the training set.\nSimulations are carried out with TensorNet 2L, using the parameters in table 6, with the exception\nof A-0L, in which a 0L TensorNet model is showcased. Presented data is plotted only every 1.28\nns for visualization clarity. (Right) Representation of the simulated molecules. Labels show the\nPubChem ID for each molecule.\nTable 2: TensorNet inference times in million steps per day for the \"Plain\" (P), \"Compile\" (C)\nand \"Graph\" (G) implementations and varying number of layers.\nMolecule (atoms)\nP 0L\nP 1L\nP 2L\nC 0L\nC 1L\nC 2L\nG 0L\nG 1L\nG 2L\nAlanine dipeptide (22)\n19.86\n10.29\n8.50\n40.19\n28.70\n21.23\n172.80\n84.71\n56.47\nTestosterone (49)\n15.05\n11.93\n8.56\n38.57\n27.00\n21.49\n154.29\n63.53\n39.82\nChignolin (166)\n19.77\n11.88\n7.90\n36.77\n24.90\n21.39\n77.14\n26.02\n15.57\nDHFR (2489)\n5.56\n1.67\n0.98\n14.47\n3.27\n1.83\n5.65\n1.69\n1.00\nFactor IX (5807)\n2.32\n0.69\n0.41\n5.42\n1.35\n0.77\n2.33\n0.70\n0.42\nthat in TorchMD-Net, running inference re-\nquires one backpropagation step to compute\nforces as the negative gradient of the energies\nwith respect to the input positions, which are\ncomputed via Autograd. This step is also in-\ncluded in these benchmarks. We make sure not\nto include any warmup times in these bench-\nmarks by running the models for 100 itera-\ntions before timing.\nWe refer as \"Graph\" to\nan implementation that has been modified to\nensure every CUDA graph requirement is met.\nFor \"Compile\" the implementation is carefully\ntailored to look for the best performance in\ntorch.compile in addition to the changes in-\ntroduced for \"Graph\". Finally, \"Plain\" repre-\nsents the baseline implementation in PyTorch.\n12\n101\n102\n103\n101\n102\nMillion steps per day\n0L\nPlain\nCompile\nGraphs\n101\n102\n103\n100\n101\n102\nMillion steps per day\n1L\nPlain\nCompile\nGraphs\n101\n102\n103\n100\n101\n102\nNumber of particles\nMillion steps per day\n2L\nPlain\nCompile\nGraphs\nFigure 7: Comparison between TensorNet in-\nference times (energy and forces) with 0, 1 and\n2 layers, embedding dimension 128, 64 neigh-\nbors on average.\nAll atoms are passed in a\nsingle batch.\nPlain represents the bare Ten-\nsorNet implementation; with Compile the mod-\nule has been preprocessed with torch.compile\nwith the \"max-autotune\" option; for Graphs\nthe whole computation has been captured into\na single CUDA graph.\nAlthough in principle the code received by the\ncompiler is entirely capturable by a graph, it\noften decides to capture only some sections of\nit, introducing other kinds of optimizations in-\nstead. This is also made evident by the appear-\nance of the same kind of \"plateau\" performance\nfor smaller workloads in both Plain and Com-\npile, which can be attributed to a bottleneck\nproduced by kernel launch overhead. Still, the\ntorch compiler is able to provide a speedup of\na factor 2 to 3 for all workloads with respect to\nthe original implementation.\nCUDA kernel overhead (and thus the perfor-\nmance gain of CUDA graphs) is expected to\ndominate for small workloads, where it is usual\nfor the kernel launching time to be larger than\nthe actual execution. Figure 7 indeed corrobo-\nrates this by showing speedups between 10 and\n2 times for molecules with up to a few hundreds\nof atoms and for all numbers of interaction lay-\ners (0, 1 and 2). Starting from workloads con-\nsisting of several hundreds of atoms, the perfor-\nmance of the Plain version is recovered.\nWe also explore inference times for some\nmolecules with varying number of atoms in Ta-\nble 2. For these molecules, which can be found\nin the repository for speed benchmarking pur-\nposes, we measure time to compute the poten-\ntial energy and atomic forces of a single exam-\nple using TensorNet with 0, 1 and 2 interaction\nlayers. Again, we express this time in million\nsteps per day. In all cases we use a cutoff of\n4.5Å, an embedding dimension of 128, 32 radial\nbasis functions and a maximum of 32 neighbors\nper particle.\n4\nConclusions\nTorchMD-Net has significantly evolved in its re-\ncent iterations, becoming a comprehensive plat-\nform for neural network potentials (NNPs). It\nprovides researchers with robust tools for both\nrapid prototyping of new models and executing\nproduction-level tasks. However, despite these\nadvancements, NNPs still face substantial chal-\nlenges before they can fully replace traditional\nforce fields in molecular dynamics simulations.\nCurrently, while the necessary software infras-\n13\ntructure is largely in place, as evidenced by the\nfirst-class support for NNPs in popular pack-\nages,19 issues such as memory requirements and\ncomputational performance remain significant\nconcerns.\nThe impact of memory limitations is antici-\npated to diminish with ongoing hardware ad-\nvancements.\nYet, enhancing computational\nperformance to a level that is competitive with\ntraditional methods necessitates more intricate\nstrategies.\nThis involves developing architec-\ntures and their implementations in a manner\nthat leverages the full capabilities of GPU hard-\nware.\nFrom a software development perspective, the\ncompilation functionality within PyTorch is an\nevolving feature, still in its early stages.\nIts\ncurrent development trajectory, which aims to\nminimize the necessary code modifications for\neffective utilization, suggests that future Py-\nTorch releases will likely bring performance\nenhancements.\nContinuous improvements in\nthe relevant toolset, encompassing PyTorch,\nCUDA, Triton, and others, are gradually nar-\nrowing the performance gap between highly op-\ntimized code and more straightforward imple-\nmentations.\nAcknowledgement\nWe thank Prof.\nJan Rezac for discovering\nthe spurious discontinuity in the Equivari-\nant Transformer.\nG. S. is financially sup-\nported by Generalitat de Catalunya’s Agency\nfor Management of University and Research\nGrants (AGAUR) PhD grant FI-2-00587. This\nproject has received funding from the Euro-\npean Union’s Horizon 2020 research and inno-\nvation programme under grant agreement No.\n823712; and the project PID2020-116564GB-\nI00 has been funded by MCIN / AEI /\n10.13039/501100011033; Research reported in\nthis publication was supported by the National\nInstitute of General Medical Sciences (NIGMS)\nof the National Institutes of Health under award\nnumber R01GM140090. The content is solely\nthe responsibility of the authors and does not\nnecessarily represent the official views of the\nNational Institutes of Health.\nReferences\n(1) Behler, J.;\nParrinello, M. Generalized\nNeural-Network Representation of High-\nDimensional Potential-Energy Surfaces.\nPhys. Rev. Lett. 2007, 98, 146401.\n(2) Kocer, E.; Ko, T. W.; Behler, J. Neural\nNetwork Potentials: A Concise Overview\nof Methods. 2021.\n(3) Behler, J. Perspective: Machine learning\npotentials for atomistic simulations. The\nJournal of Chemical Physics 2016, 145,\n170901.\n(4) Schütt, K. T.; Sauceda, H. E.; Kinder-\nmans, P.-J.; Tkatchenko, A.; Müller, K.-\nR. SchNet – A deep learning architecture\nfor molecules and materials. The Journal\nof Chemical Physics 2018, 148, 241722.\n(5) Deringer, V. L.; Caro, M. A.; Csányi, G.\nMachine Learning Interatomic Potentials\nas Emerging Tools for Materials Science.\nAdvanced Materials 2019, 31, 1902765.\n(6) Botu, V.; Batra, R.; Chapman, J.; Ram-\nprasad, R. Machine Learning Force Fields:\nConstruction, Validation, and Outlook.\nThe Journal of Physical Chemistry C\n2017, 121, 511–522.\n(7) Ko, T. W.; Finkler, J. A.; Goedecker, S.;\nBehler,\nJ.\nA\nfourth-generation\nhigh-\ndimensional neural network potential with\naccurate electrostatics including non-local\ncharge transfer. Nature Communications\n2021, 12, 398.\n(8) Schütt,\nK.\nT.;\nHessmann,\nS.\nS.\nP.;\nGebauer,\nN.\nW.\nA.;\nLederer,\nJ.;\nGastegger,\nM.\nSchNetPack\n2.0:\nA\nneural network toolbox for atomistic ma-\nchine learning. The Journal of Chemical\nPhysics 2023, 158.\n(9) Gao, X.; Ramezanghorbani, F.; Isayev, O.;\nSmith, J. S.; Roitberg, A. E. TorchANI:\n14\nA Free and Open Source PyTorch-Based\nDeep Learning Implementation of the\nANI Neural Network Potentials. Journal\nof Chemical Information and Modeling\n2020, 60, 3408–3415.\n(10) Zeng, J. et al. DeePMD-kit v2: A soft-\nware package for deep potential models.\nThe Journal of Chemical Physics 2023,\n159.\n(11) Thölke, P.;\nFabritiis, G. D. Equivari-\nant\nTransformers\nfor\nNeural\nNetwork\nbased Molecular Potentials. International\nConference on Learning Representations.\n2022.\n(12) Majewski, M.; Pérez, A.; Thölke, P.; Do-\nerr, S.; Charron, N. E.; Giorgino, T.; Hu-\nsic, B. E.; Clementi, C.; Noé, F.; Fab-\nritiis, G. D. Machine Learning Coarse-\nGrained Potentials of Protein Thermody-\nnamics. 2022.\n(13) Simeon, G.; Fabritiis, G. D. TensorNet:\nCartesian Tensor Representations for Ef-\nficient Learning of Molecular Potentials.\nThirty-seventh Conference on Neural In-\nformation Processing Systems. 2023.\n(14) Paszke, A. et al. Advances in Neural In-\nformation Processing Systems 32; Curran\nAssociates, Inc., 2019; pp 8024–8035.\n(15) https://lightning.ai/\npytorch-lightning.\n(16) conda-forge community The conda-forge\nProject: Community-based Software Dis-\ntribution Built on the conda Package For-\nmat and Ecosystem. 2015; https://doi.\norg/10.5281/zenodo.4774216.\n(17) TorchMD-NET\nDocumen-\ntation.\nhttps://torchmd-\nnet.readthedocs.io/en/latest/,\nAccessed:\n2024-02-13.\n(18) Biersack, J. P.; Ziegler, J. F. Ion Implan-\ntation Techniques; Springer Berlin Heidel-\nberg, 1982; p 122–156.\n(19) Eastman, P. et al. OpenMM 8:\nMolec-\nular Dynamics Simulation with Machine\nLearning Potentials. The Journal of Phys-\nical Chemistry B 2024, 128, 109–116,\nPMID: 38154096.\n(20) OpenMM-Torch.\nhttps://github.com/\nopenmm/openmm-torch.\n(21) Bronstein, M. M.; Bruna, J.; Cohen, T.;\nVeličković, P. Geometric Deep Learning:\nGrids, Groups, Graphs, Geodesics, and\nGauges. 2021; https://arxiv.org/abs/\n2104.13478.\n(22) Gilmer, J.; Schoenholz, S. S.; Riley, P. F.;\nVinyals, O.; Dahl, G. E. Neural Message\nPassing for Quantum Chemistry. 2017;\nhttps://arxiv.org/abs/1704.01212.\n(23) Joshi, C. K.; Bodnar, C.; Mathis, S. V.;\nCohen, T.; Liò, P. On the Expressive\nPower of Geometric Graph Neural Net-\nworks. 2023;\nhttps://arxiv.org/abs/\n2301.09308.\n(24) Duval, A.; Mathis, S. V.; Joshi, C. K.;\nSchmidt, V.; Miret, S.; Malliaros, F. D.;\nCohen, T.; Lio, P.; Bengio, Y.; Bron-\nstein, M. A Hitchhiker’s Guide to Geomet-\nric GNNs for 3D Atomic Systems. 2023;\nhttps://arxiv.org/abs/2312.07511.\n(25) Batzner, S.;\nMusaelian, A.;\nSun, L.;\nGeiger, M.; Mailoa, J. P.; Kornbluth, M.;\nMolinari, N.; Smidt, T. E.; Kozinsky, B.\nE(3)-equivariant graph neural networks\nfor data-efficient and accurate interatomic\npotentials. Nature Communications 2022,\n13.\n(26) Musaelian, A.; Batzner, S.; Johansson, A.;\nSun, L.; Owen, C. J.; Kornbluth, M.;\nKozinsky, B. Learning local equivariant\nrepresentations for large-scale atomistic\ndynamics. Nature Communications 2023,\n14.\n(27) Batatia, I.; Kovacs, D. P.; Simm, G. N. C.;\nOrtner, C.; Csanyi, G. MACE: Higher\nOrder Equivariant Message Passing Neu-\nral Networks for Fast and Accurate Force\n15\nFields. Advances in Neural Information\nProcessing Systems. 2022.\n(28) Schütt, K. T.; Unke, O. T.; Gastegger, M.\nEquivariant message passing for the pre-\ndiction of tensorial properties and molec-\nular spectra. 2021; https://arxiv.org/\nabs/2102.03150.\n(29) Satorras,\nV.\nG.;\nHoogeboom,\nE.;\nWelling,\nM.\nE(n)\nEquivariant\nGraph\nNeural\nNetworks.\n2021;\nhttps://arxiv.org/abs/2102.09844.\n(30) Bihani, V.; Pratiush, U.; Mannan, S.;\nDu,\nT.;\nChen,\nZ.;\nMiret,\nS.;\nMi-\ncoulaut, M.; Smedskjaer, M. M.; Ranu, S.;\nKrishnan, N. M. A. EGraFFBench: Eval-\nuation of Equivariant Graph Neural Net-\nwork Force Fields for Atomistic Simu-\nlations. 2023; https://arxiv.org/abs/\n2310.02428.\n(31) Schütt,\nK.\nT.;\nArbabzadah,\nF.;\nChmiela,\nS.;\nMüller,\nK.\nR.;\nTkatchenko,\nA.\nQuantum-chemical\ninsights from deep tensor neural net-\nworks. Nature Communications 2017, 8,\n13890.\n(32) Unke, O. T.; Meuwly, M. PhysNet:\nA\nNeural Network for Predicting Energies,\nForces,\nDipole Moments,\nand Partial\nCharges. J Chem Theory Comput 2019,\n15, 3678–3693.\n(33) Unke, O. T.; Meuwly, M. PhysNet:\nA\nNeural Network for Predicting Energies,\nForces,\nDipole Moments,\nand Partial\nCharges. Journal of Chemical Theory and\nComputation 2019, 15, 3678–3693.\n(34) Unke, O. T.; Chmiela, S.; Gastegger, M.;\nSchütt, K. T.; Sauceda, H. E.; Müller, K.-\nR. SpookyNet: Learning force fields with\nelectronic degrees of freedom and nonlo-\ncal effects. Nature Communications 2021,\n12.\n(35) Grimme,\nS.\nSemiempirical\nGGA-type\ndensity functional constructed with a\nlong-range dispersion correction. Journal\nof Computational Chemistry 2006, 27,\n1787–1799.\n(36) Fey, M.; Lenssen, J. E. Fast Graph Repre-\nsentation Learning with PyTorch Geomet-\nric. CoRR 2019, abs/1903.02428.\n(37) Eastman, P.; Behara, P. K.; Dotson, D. L.;\nGalvelis, R.; Herr, J. E.; Horton, J. T.;\nMao, Y.; Chodera, J. D.; Pritchard, B. P.;\nWang,\nY.;\nFabritiis,\nG.\nD.;\nMark-\nland, T. E. SPICE, A Dataset of Drug-like\nMolecules and Peptides for Training Ma-\nchine Learning Potentials. Scientific Data\n2023, 10.\n(38) Ramakrishnan, R.; Dral, P. O.; Rupp, M.;\nvon Lilienfeld, O. A. Quantum chem-\nistry structures and properties of 134 kilo\nmolecules. Scientific Data 2014, 1.\n(39) Cheng, B.; Engel, E. A.; Behler, J.; Del-\nlago, C.; Ceriotti, M. Ab initio thermody-\nnamics of liquid and solid water. Proceed-\nings of the National Academy of Sciences\n2019, 116, 1110–1115.\n(40) Chmiela,\nS.;\nTkatchenko,\nA.;\nSauceda,\nH.\nE.;\nPoltavsky,\nI.;\nSchütt, K. T.;\nMüller, K.-R. Machine\nlearning\nof\naccurate\nenergy-conserving\nmolecular force fields. Science Advances\n2017, 3.\n(41) https://figshare.com/articles/\ndataset/Revised_MD17_dataset_\nrMD17_/12672038.\n(42) Chmiela,\nS.;\nVassilev-Galindo,\nV.;\nUnke, O. T.; Kabylda, A.; Sauceda, H. E.;\nTkatchenko, A.; Müller, K.-R. Accurate\nglobal\nmachine\nlearning\nforce\nfields\nfor molecules with hundreds of atoms.\nScience Advances 2023, 9.\n(43) Smith, J. S.; Isayev, O.; Roitberg, A. E.\nANI-1:\nan extensible neural network\npotential with DFT accuracy at force\nfield computational cost. Chemical Sci-\nence 2017, 8, 3192–3203.\n16\n(44) Smith, J. S.; Zubatyuk, R.; Nebgen, B.;\nLubbers, N.; Barros, K.; Roitberg, A. E.;\nIsayev, O.; Tretiak, S. The ANI-1ccx and\nANI-1x data sets, coupled-cluster and\ndensity functional theory properties for\nmolecules. Scientific Data 2020, 7.\n(45) Devereux, C.;\nSmith, J. S.;\nHuddle-\nston, K. K.; Barros, K.; Zubatyuk, R.;\nIsayev, O.; Roitberg, A. E. Extending\nthe Applicability of the ANI Deep Learn-\ning Molecular Potential to Sulfur and\nHalogens. Journal of Chemical Theory\nand Computation 2020, 16, 4192–4202,\nPMID: 32543858.\n(46) Smith, J. S.; Nebgen, B.; Lubbers, N.;\nIsayev,\nO.;\nRoitberg,\nA.\nE.\nLess\nis\nmore: Sampling chemical space with ac-\ntive learning. The Journal of Chemical\nPhysics 2018, 148, 241733.\n(47) Jia, Z.; Padon, O.; Thomas, J.; Warsza-\nwski, T.; Zaharia, M.; Aiken, A. TASO:\nOptimizing Deep Learning Computation\nwith Automatic Generation of Graph Sub-\nstitutions. Proceedings of the 27th ACM\nSymposium on Operating Systems Princi-\nples. New York, NY, USA, 2019; p 47–62.\n(48) Tillet, P.; Kung, H. T.; Cox, D. Triton: An\nIntermediate Language and Compiler for\nTiled Neural Network Computations. Pro-\nceedings of the 3rd ACM SIGPLAN In-\nternational Workshop on Machine Learn-\ning and Programming Languages (MAPL\n2019). Phoenix, Arizona, United States,\n2019.\n(49) Appleyard, J.; Kociský, T.; Blunsom, P.\nOptimizing\nPerformance\nof\nRecurrent\nNeural Networks on GPUs. CoRR 2016,\nabs/1604.01946.\n(50) Wang, G.; Lin, Y.; Yi, W. Kernel fu-\nsion: An effective method for better power\nefficiency on multithreaded GPU. 2010\nIEEE/ACM Int’l Conference on Green\nComputing and Communications & Int’l\nConference on Cyber, Physical and Social\nComputing. 2010; pp 344–350.\n(51) Filipovič, J.;\nMadzin, M.;\nFousek, J.;\nMatyska, L. Optimizing CUDA code by\nkernel fusion: application on BLAS. The\nJournal of Supercomputing\n2015,\n71,\n3934–3957.\n(52) Nguyen, H.; Corporation, N. GPU Gems\n3; Lab Companion Series v. 3; Addison-\nWesley, 2008.\n(53) Tang, Y.-H.; Karniadakis, G. E. Acceler-\nating dissipative particle dynamics simula-\ntions on GPUs: Algorithms, numerics and\napplications. Computer Physics Commu-\nnications 2014, 185, 2809–2822.\n(54) Doerr, S.;\nMajewski, M.;\nPérez, A.;\nKrämer,\nA.;\nClementi,\nC.;\nNoe,\nF.;\nGiorgino, T.; Fabritiis, G. D. TorchMD: A\nDeep Learning Framework for Molecular\nSimulations. Journal of Chemical Theory\nand Computation 2021, 17, 2355–2363.\n(55) Anderson, B.; Hy, T.-S.; Kondor, R. Cor-\nmorant: Covariant Molecular Neural Net-\nworks. Proceedings of the 33rd Interna-\ntional Conference on Neural Information\nProcessing Systems. Red Hook, NY, USA,\n2019.\n(56) Brandstetter, J.; Hesselink, R.; van der\nPol, E.; Bekkers, E. J.; Welling, M. Ge-\nometric and Physical Quantities Improve\nE(3) Equivariant Message Passing. 2021;\nhttps://arxiv.org/abs/2110.02905.\n(57) Schütt, K. T.; Sauceda, H. E.; Kinder-\nmans, P.-J.; Tkatchenko, A.; Müller, K.-\nR. SchNet – A deep learning architecture\nfor molecules and materials. The Journal\nof Chemical Physics 2018, 148, 241722.\n(58) Liao, Y.-L.; Smidt, T. Equiformer: Equiv-\nariant Graph Attention Transformer for\n3D Atomistic Graphs. The Eleventh In-\nternational Conference on Learning Rep-\nresentations. 2023.\n(59) Gasteiger, J.; Giri, S.; Margraf, J. T.;\nGünnemann, S. Fast and Uncertainty-\nAware Directional Message Passing for\n17\nNon-Equilibrium Molecules. 2020; https:\n//arxiv.org/abs/2011.14115.\n(60) Liu, Y.; Wang, L.; Liu, M.; Zhang, X.;\nOztekin, B.; Ji, S. Spherical Message Pass-\ning for 3D Graph Networks. 2021; https:\n//arxiv.org/abs/2102.05013.\n(61) Fu, X.;\nWu, Z.;\nWang, W.;\nXie, T.;\nKeten,\nS.;\nGomez-Bombarelli,\nR.;\nJaakkola, T. S. Forces are not Enough:\nBenchmark and Critical Evaluation for\nMachine\nLearning\nForce\nFields\nwith\nMolecular Simulations. Transactions on\nMachine Learning Research 2023, Survey\nCertification.\n(62) Vita, J. A.; Schwalbe-Koda, D. Data effi-\nciency and extrapolation trends in neural\nnetwork interatomic potentials. Machine\nLearning: Science and Technology 2023,\n4, 035031.\n(63) Wagner, J. et al. openforcefield/openff-\ntoolkit:\n0.14.5 Minor feature release.\n2023;\nhttps://doi.org/10.5281/\nzenodo.10103216.\nA\nHyperparameters\nIn the pursuit of transparency and reproducibil-\nity, this appendix provides a detailed account of\nthe hyperparameters employed in our computa-\ntional experiments. The tables contained herein\npresent the specific values and settings used to\nachieve the results discussed in the main body\nof this paper. Readers and fellow researchers\nare encouraged to refer to these tables when at-\ntempting to replicate our results or when uti-\nlizing the torchmd-train utility for their own\ntraining purposes.\nTable 3: MD17 hyperparameters used for the\nET training in Figure 5.\nParameter\nValue\nactivation\nsilu\nattn_activation\nsilu\nbatch_size\n8\ncutoff_lower\n0.0\ncutoff_upper\n5.0\nderivative\nTrue\ndistance_influence\nboth\nearly_stopping_patience\n300\nema_alpha_neg_dy\n1.0\nema_alpha_y\n0.05\nembedding_dimension\n128\nlr\n1e-3\nlr_factor\n0.8\nlr_min\n1e-7\nlr_patience\n30\nlr_warmup_steps\n1000\nneg_dy_weight\n0.8\nnum_heads\n8\nnum_layers\n6\nnum_rbf\n32\nseed\n1\ntrain_size\n950\nval_size\n50\nvector_cutoff\nTrue\ny_weight\n0.2\n18\nTable 4: QM9 U0 hyperparameters used to ob-\ntain the results for ETnew in Table 1.\nParameter\nValue\nactivation\nsilu\nattn_activation\nsilu\nbatch_size\n128\ncutoff_lower\n0.0\ncutoff_upper\n5.0\nderivative\nFalse\ndistance_influence\nboth\nearly_stopping_patience\n150\nema_alpha_neg_dy\n1.0\nema_alpha_y\n1.0\nembedding_dimension\n256\nlr\n4e-4\nlr_factor\n0.8\nlr_min\n1e-7\nlr_patience\n15\nlr_warmup_steps\n10000\nneg_dy_weight\n0.0\nnum_heads\n8\nnum_layers\n8\nnum_rbf\n64\nremove_ref_energy\ntrue\nseed\n1\ntrain_size\n110000\nval_size\n10000\nvector_cutoff\nTrue\ny_weight\n1.0\nTable 5: QM9 U0 hyperparameters used for the\ntrainings in Figure 4 to obtain the results for\nTensorNet 3L* in Table 1.\nParameter\nValue\nactivation\nsilu\nbatch_size\n16\ncutoff_lower\n0.0\ncutoff_upper\n5.0\nderivative\nFalse\nearly_stopping_patience\n150\nembedding_dimension\n256\nequivariance_invariance_group\nO(3)\ngradient_clipping\n40\nlr\n1e-4\nlr_factor\n0.8\nlr_min\n1e-7\nlr_patience\n15\nlr_warmup_steps\n1000\nneg_dy_weight\n0.0\nnum_layers\n3\nnum_rbf\n64\nremove_ref_energy\ntrue\nseed\n2\ntrain_size\n110000\nval_size\n10000\ny_weight\n1.0\n19\nTable 6: ANI2x hyperparameters used for Ten-\nsorNet training. The model was then used to\nrun the stable molecular dynamics simulations\nin Figure 6.\nParameter\nValue\nactivation\nsilu\nbatch_size\n256\ncutoff_lower\n0.0\ncutoff_upper\n5.0\nderivative\nTrue\nearly_stopping_patience\n50\nembedding_dimension\n128\nequivariance_invariance_group\nO(3)\ngradient_clipping\n100\nlr\n1e-3\nlr_factor\n0.5\nlr_min\n1e-7\nlr_patience\n4\nlr_warmup_steps\n1000\nneg_dy_weight\n100\nnum_layers\n{0,2}\nnum_rbf\n32\nseed\n1\ntrain_size\n0.9\nval_size\n0.1\ny_weight\n1.0\n20\n"
}