{
    "optim": "Seeing and Hearing: Open-domain Visual-Audio Generation with Diffusion\nLatent Aligners\nYazhou Xing1*\nYingqing He1∗\nZeyue Tian1∗\nXintao Wang2\nQifeng Chen1\n1HKUST\n2ARC Lab, Tencent PCG\nAbstract\nVideo and audio content creation serves as the core tech-\nnique for the movie industry and professional users. Re-\ncently, existing diffusion-based methods tackle video and\naudio generation separately, which hinders the technique\ntransfer from academia to industry. In this work, we aim\nat filling the gap, with a carefully designed optimization-\nbased framework for cross-visual-audio and joint-visual-\naudio generation. We observe the powerful generation abil-\nity of off-the-shelf video or audio generation models. Thus,\ninstead of training the giant models from scratch, we pro-\npose to bridge the existing strong models with a shared la-\ntent representation space. Specifically, we propose a mul-\ntimodality latent aligner with the pre-trained ImageBind\nmodel. Our latent aligner shares a similar core as the clas-\nsifier guidance that guides the diffusion denoising process\nduring inference time.\nThrough carefully designed opti-\nmization strategy and loss functions, we show the superior\nperformance of our method on joint video-audio genera-\ntion, visual-steered audio generation, and audio-steered vi-\nsual generation tasks. The project website can be found\nat https://yzxing87.github.io/Seeing-and-Hearing/.\n1. Introduction\nRecently, AI-generated content has made significant ad-\nvances in creating diverse and high-realistic images [4, 9,\n22, 32, 34], videos [4, 7, 15, 19, 20, 22, 38], or sound [25,\n28–30, 44], based on the input descriptions from users.\nHowever, existing works primarily concentrate on generat-\ning content within a single modality, disregarding the mul-\ntimodal nature of the real world. Consequently, the gen-\nerated videos lack accompanying audio, and the generated\naudio lacks synchronized visual effects. This research gap\nrestricts users from creating content with greater impact,\nsuch as producing films that necessitate the simultaneous\ncreation of both visual and audio modalities. In this work,\nwe study the visual-audio generation task for crafting both\n*equal contribution\nvideo and audio content.\nOne potential solution to this problem is to generate vi-\nsual and audio content in two stages. For example, users\ncan first generate the video based on the input text prompt\nutilizing existing text-to-video (T2V) models [7, 18]. Then,\na video-to-audio (V2A) model can be employed to gener-\nate aligned audio. Alternatively, a combination of text-to-\naudio (T2A) and audio-to-video (A2V) models can be used\nto generate paired visual-audio content. However, existing\nV2A and A2V generation methods [26, 46] either have lim-\nited capability to specific downstream domains or exhibit\npoor generation performance. Moreover, the task of joint\nvideo-audio generation (Joint-VA) has received limited at-\ntention, and existing work [36] shows limited generation\nperformance even within a small domain and also lacks se-\nmantic control.\nIn this work, we propose a new generation paradigm\nfor open-domain visual-audio generation. We observe that:\n(1) There are well-trained single-modality text-conditioned\ngeneration models that demonstrate excellent performance.\nLeveraging these pre-trained models can avoid expensive\ntraining for synthesizing each modality. (2) We have no-\nticed that the pre-trained model ImageBind [17] possesses\nremarkable capability in establishing effective connections\nbetween different data modalities within a shared semantic\nspace. Our objective is to explore how we can leverage Im-\nageBind as a bridge to connect and integrate various modal-\nities effectively.\nLeveraging these observations, we propose to utilize Im-\nageBind as an aligner in the diffusion latent space of differ-\nent modalities. During the generation of one modality, we\ninput the noisy latent and the guided condition of another\nmodality to our aligner to produce a guidance signal that in-\nfluences the generation process. By gradually injecting the\nguidance into the denoising process, we bridge the gener-\nated content closer to the input condition in the ImageBind\nembedding space. For Joint-VA generation, we make the\nguidance bidirectional to impact the generation processes\nof both modalities.\nWith our design, we successfully bridge the pre-trained\nsingle-modality generation models into an organic system\n1\narXiv:2402.17723v1  [cs.CV]  27 Feb 2024\n“A giant \nwave \ncrashes \ndown in \nthe \nocean”\nPretrained \nDiffusion \nModel\nImageBind\nFigure 1. Overview. Our approach is versatile and can tackle four tasks: joint video-audio generation (Joint-VA), video-to-audio (V2A),\naudio-to-video (A2V), and image-to-audio (I2A). By leveraging a multimodal binder, e.g., pretrained ImageBind, we establish a connection\nbetween isolated generative models that are designed for generating a single modality. This enables us to achieve both bidirectional\nconditional and joint video/audio generation.\nand achieve a versatile and flexible visual-audio generation.\nIn addition, our approach does not require training on large-\nscale datasets, making our approach very resource-friendly.\nBesides the generality and low cost of our approach, we\nvalidate our performance on four tasks and show the supe-\nriority over baseline approaches.\nIn summary, our key contributions are as follows:\n• We propose a novel paradigm that bridges pre-trained\ndiffusion models of single modality together to achieve\naudio-visual generation.\n• We introduce diffusion latent aligner to gradually align\ndiffusion latent of visual and audio modalities in a multi-\nmodal embedding space.\n• We conduct extensive experiments on four tasks including\nV2A, I2A, A2V, and Joint-VA, demonstrating the superi-\nority and generality of our approach.\n• To the best of our knowledge, we present the first work\nfor text-guided joint video-audio generation.\n2. Related Work\n2.1. Conditional Audio Generation\nAudio generation is an emerging field that focuses on mod-\neling the creation of diverse audio content. This includes\ntasks such as generating audio conditioned on various in-\nputs like text [11, 16, 24, 25, 28, 44], images [37], and\nvideos [12, 26, 31, 39].\nIn the field of text-to-audio research, AudioGen [28]\nproposes an auto-regressive generative model that oper-\nates on discrete audio representations, DiffSound [44] uti-\nlizes non-autoregressive token-decoder to address the limi-\ntations of unidirectional generation in auto-regressive mod-\nels. While some other works like Make-An-Audio [25],\nAudioLDM [29], employ latent diffusion methods for au-\ndio generation.\nSome recent studies, such as Make-an-\nAudio2 [24], AudioLDM2 [30], TANGO [16], have lever-\naged Large Language Models (LLMs) to enhance the per-\nformance of audio generation models.\nResearch focusing on audio generation that is condi-\ntioned on images and videos, exemplified by works like\nIm2Wav [37] and SpecVQGAN [26], has also captured sig-\nnificant interest within the scholarly community. Utilizing\nthe semantics of a pre-trained CLIP model for visual repre-\nsentation (Contrastive Language–Image Pre-training) [33],\nIm2Wav [37] first crafts a foundational audio representation\nvia a language model, then employs an additional language\nmodel to upsample these audio tokens into high-fidelity\nsound samples. SpecVQGAN [26] utilizes a transformer\nto generate new spectrograms from a pre-trained codebook\nbased on input video features. It then reconstructs the wave-\nform from these spectrograms using a pre-trained vocoder.\n2.2. Conditional Visual Generation\nThe task of text-to-image generation has seen significant\ndevelopment and achievements in recent years [2, 35, 40].\nThis progress has sparked interest in a new research do-\nmain focusing on audio-to-image generation. In 2019, [42]\nproposed a method to generate images from audio record-\nings, employing Generative Adversarial Networks (GANs).\n[47] focused narrowly on generating images of MNIST dig-\nits using audio inputs and did not extend to image gen-\neration from general audio sounds.\nIn contrast, the ap-\nproach by\n[42], was capable of generating images from\na broader range of audio signals. Wav2CLIP [43] adopts\na CLIP-inspired approach to learn joint representations for\naudio-image pairs, which can subsequently facilitate im-\nage generation using VQ-GAN [13].\nText-to-video has\nalso achieved remarkable progress recently [1, 4, 7, 15, 19,\n2\n22, 23, 25, 48, 49] empowered by video diffusion mod-\nels [23]. The mainstream idea is to incorporate temporal\nmodeling modules in the U-Net architecture to learn the\ntemporal dynamics [1, 19, 23, 38, 49] in the video pixel\nspace [22, 23] or in the latent space [4, 19]. In this work, we\nleverage the open-source latent-based text-to-video model\nas our base model for the video generation counterpart.\nThere’re also some Audio-to-video works that have been\ndone, such as Sound2sight [5], TATS [14], and Tempoto-\nkens [45]. While [5] focuses on extending videos in a way\nthat aligns with the audio, Tempotokens [45] takes a dif-\nferent approach by exclusively generating videos from the\naudio input. TATS [14] introduced a technique for creating\nvideos synchronized with audio, but despite its remarkable\naspects, the variety in the videos it produces is significantly\nconstrained.\n2.3. Multimodal Joint Generation\nSome research has already begun exploring the area of Mul-\ntimodal Joint Generation [36, 50]. MM-Diffusion [36] in-\ntroduces the first framework for simultaneous audio-video\ngeneration, designed to synergistically enhance both visual\nand auditory experiences cohesively and engagingly. How-\never, it’s unconditional and can only generate results in the\ntraining set domain, which would limit generation diversity.\nMovieFactory [50] employs ChatGPT to elaborately expand\nuser-input text into detailed sequential scripts for generating\nmovies, which are then vividly actualized both visually and\nacoustically through vision generation and audio retrieval\ntechniques. However, a notable constraint of MovieFactory\nlies in its reliance on audio retrieval, limiting its capacity to\ngenerate audio that is more intricately tailored to the spe-\ncific scenes.\n3. Method\n3.1. Preliminaries\n3.1.1\nLatent diffusion models\nWe adopt latent-based diffusion models (LDM) for our gen-\neration model. The diffusion process follows the standard\nformulation in DDPM [21] that consists of a forward diffu-\nsion and a backward denoising process. Given a data sam-\nple x ∼ p(x), an autoencoder consisting an encoder E and\na decoder D first project the x into latent z via z = E(x).\nThen, the diffusion and denoising process are conducted\nin the latent space.\nOnce the denoising is completed at\ntimestep 0, the sample will be decoded via x = D( ˜z0). The\nforward diffusion is a fixed Markov process of T timesteps\nthat yields latent variables zt based on the latent variable at\nprevious timestep zt−1 via\nq(zt|zt−1) = N(zt;\np\n1 − βtzt−1, βtI),\n(1)\nwhere βt is a predefined variance at each step t. Finally, the\nclean data z0 becomes zT , which is indistinguishable from\na Gaussian noise. The zt can be directly derived from z0 in\na closed form:\nq(zt|z0) = N(zt; √¯αtz0, (1 − ¯αt)I),\n(2)\nwhere ¯αt = Qt\ni=1 αi, and αt = 1 − βt. Leveraging the\nreparameterization trick, the zt can be computed via\nzt = √¯αtz0 + (1 − ¯αt)ϵ,\n(3)\nwhere ϵ is a random Gaussian noise. The backward de-\nnoising process leverages a trained denoiser θ to obtain less\nnoisy data zt−1 from the noisy input zt at each timestep:\npθ (zt−1 | zt) = N (zt−1; µθ (zt, t, p) , Σθ (zt, t, p)) .\n(4)\nHere µθ and Σθ are determined through a denoiser net-\nwork ϵθ (zt, t, p), where p represents input prompt. The\ntraining objective of θ is a noise estimation loss, formulated\nas\nmin\nθ\nEt,zt,ϵ ∥ϵ − ϵθ (zt, t, p)∥2\n2 .\n(5)\n3.1.2\nClassifier guidance\nClassifier guidance [10] is a conditional generation mech-\nanism that leverages the unconditional diffusion model to\ngenerate samples with the desired category. Given an un-\nconditional diffusion model pθ(zt|zt+1), in order to condi-\ntion it on a class label y, it can be approximated via\npθ,ϕ(zt|zt+1, y) = Zpθ(zt|zt+1)pϕ(y|zt, t),\n(6)\nwhere Z is a constant coefficient for normalization, ϕ is a\ntrained time-aware noisy classifier for the approximation of\nlabel distribution of each sample of zt. The guidance from\nthe classifier ϕ is the gradient of zt with respect to y and is\napplied to the original zt predicted from ϵθ:\nˆϵ(zt) = ϵθ(zt) −\np\n1 − ˆαt▽zt log pϕ(y|zt).\n(7)\n3.1.3\nLinking multiple modalities\nWe aim to force the generated samples in different modali-\nties to become closer in a joint semantic space. To achieve\nthis goal, we choose ImageBind [17] as the aligner since\nit learns an effective joint embedding space for multiple\nmodalities. ImageBind learns a joint semantic embedding\nspace that binds multiple different modalities including im-\nage, text, video, audio, depth, and thermal. Given a pair\nof data with different modalities (M1, M2), e.g., (video, au-\ndio), the encoder of the corresponding modality Ei takes the\ndata as input and predicts its embedding ei. The ImageBind\n3\n𝑧!\n\"#\n𝑧!$#\n\"#\n𝑧%\n\"#\n…\n𝑧%\n\"#\n𝑧!$#\n\"#\n𝑧!\n\"#\n𝑧&\n\"#\n𝑧&\n\"'\n𝑧&\n\"'\n𝑧&\n\"'\n…\n𝑧!\n\"'\n𝑧!$#\n\"'\n𝑧%\n\"'\n…\nDenoising\nE!\"\nE!#\n“prompt”\n(𝑝𝑟𝑒𝑑)\n(𝑐𝑜𝑛𝑑)\n×\nalignment\nalignment\nalignment\nℒ\n𝑧$%\"\n&\"\n𝑧$\n&\"\n𝑧'\n&\"\n𝑧'\n&#\nForward computation\nBackpropagation\n𝑖𝑓 𝑡! = 0\nImageBind\nFigure 2. The proposed diffusion latent aligner. During the denoising process of generating one specific modality (visual/audio), we\nadopt the condition information (audio/video) to guide the denoising process. By leveraging the pretrained ImageBind model, we calculate\nthe distance of the generative latent zM1\nt\nwith the condition zM2\n0\nin the shared embedding space of ImageBind. Then we backpropagate\nthe distance value to obtain the gradient of zM1\nt\nwith respect to the distance.\nis trained with a contrastive learning objective formulated as\nfollows:\nLM1,M2 = − log\nexp(q⊺\ni ki/τ)\nexp(q⊺\ni ki/τ) + P\nj̸=i exp(q⊺\ni kj/τ),\n(8)\nwhere τ is a temperature factor to control the smoothness\nof the Softmax distribution, and j represents the negative\nsample, which is the data from another pair. By project-\ning samples of different modalities into embeddings in a\nshared space, minimizing the distance of the embeddings\nfrom the same data pair, and maximizing the distance of\nthe embeddings from different data pairs, the ImageBind\nmodel achieves semantic alignment capability and thus can\nbe served as a desired tool for multimodal alignment.\n3.2. Diffusion Latent Aligner\n3.2.1\nProblem formulation\nConsider two modalities M1, M2, where M2 is the condi-\ntional modality and M1 is the generative modality. Given a\nlatent diffusion model (LDM) θ that produces data of M1,\nour objective is to leverage the information from the con-\ndition xM2 ∼ p(xM2) to steer the generation process to\na desired content, i.e., aligned the intermediate generative\ncontent with the input condition. To achieve this goal, we\ndevise a diffusion latent aligner that guides the intermediate\nnoisy latent towards a target direction to the content that the\ncondition depicted during the denoising process. Formally,\ngiven a sequence of latent variables zt, zt−1, ..., z0 from an\nLDM, the diffusion latent aligner A takes the corresponding\nlatent zt at arbitrary timestep t alongside the guided condi-\ntion xM2, and produce a modified latent ˆzt which has better\nalignment with the condition.\nˆzM1\nt\n= A(zM1\nt\n, xM2).\n(9)\nFor joint visual-audio generation, the aligner should simul-\ntaneously obtain information from the two modalities and\nprovide guidance signals to these latents:\n(ˆzM1\nt\n, ˆzM2\nt\n) = A(zM1\nt\n, zM2\nt\n).\n(10)\nAfter the sequential denoising process, the goal of our\naligner is to minimize the F(D(zM1\n0\n), xM2), for unidirec-\ntional guidance, and F(D(zM1\n0\n), D(zM2\n0\n)) for synchronized\nbidirectional guidance, where F indicates a distance func-\ntion to measure the degree of alignment between samples\nwith two modalities. The updatable parameters in this pro-\ncess can be latent variables, embedding vectors, or neural\nnetwork parameters.\n3.2.2\nMultimodal guidance\nTo design such a latent aligner stated in Section 3.2.1, we\npropose a training-free solution that leverages the great ca-\npability of a multimodal model trained for representation\nlearning, i.e., ImageBind [17] to provide rational guidance\non the denoising process. Specifically, given latent variables\nzt at each timestep t, the predicted z0 can be derived from\nzt and the predicted noise ˆϵ via\n˜z0 = G(zt) =\n1\n√¯αt\nzt −\nr\n1 − ¯αt\n¯αt\nˆϵ,\n(11)\nwhere ˆϵ = ϵθ(zt, t).\nWith such a clean prediction, we\ncan leverage the external models that are trained on normal\ndata without retraining them on noisy data like the classifier\nguidance is needed. We feed the z0 and the guiding con-\ndition to the ImageBind model to compute their distance in\n4\nAlgorithm 1 Multimodal guidance for joint-VA generation\nRequire: Learning rate λ1, λ2, optimization steps N,\nwarmup steps K, prompt p\n1: y = EMB(p)\n2: for t = T to 0 do\n3:\nzv\nt ←− DENOISE(zv\nt+1, y)\n4:\nza\nt ←− DENOISE(za\nt+1, y)\n5:\nif t < K then\n6:\nfor n = 0 to N do\n7:\n˜zv\n0 =\n1\n√\n¯αv\nt (zv\nt − √1 − ¯αv\nt ϵv\nt )\n8:\n˜za\n0 =\n1\n√\n¯αa\nt (zv\nt − √1 − ¯αa\nt ϵa\nt )\n9:\nea, ev, ep = IMAGEBIND(za\n0, zv\n0, P)\n10:\nLjoint-va = F(ev, ep) + F(ev, ea) + F(ea, ep)\n11:\nˆzv\nt = zv\nt − λ1∇zv\nt L\n12:\nˆza\nt = za\nt − λ1∇za\nt L\n13:\nˆy = y − λ2∇yL\n14:\nend for\n15:\nend if\n16: end for\n17: return zv\n0, za\n0\nthe ImageBind embedding space. The obtained distance can\nthen serve as a penalty, which can be used to backpropagate\nthe computation graph and obtain a gradient on the latent\nvariable zt:\nL(˜z0, xM2) = 1 − F(EM1(˜z0), EM2(xM2)).\n(12)\nThen we update the zt via\nˆzt = zt − λ1∇ztL(D(˜z0), xM2),\n(13)\nwhere λ1 serves as the learning rate of each optimization\nstep. In this way, we alter the sampling trajectory at each\ntimestep through our multimodal guidance signal to achieve\nboth audio-to-visual and visual-to-audio. This procedure\nonly costs a small amount of extra sampling time, without\nany additional datasets and expensive network training.\n3.2.3\nDual/Triangle loss function\nWe observed that audio often lacks enough semantic infor-\nmation such as some audio is pure background music, while\nthe paired video contains rich semantic information such as\nmultiple objects and environment sound. Using this type of\ncondition to guide visual generation is not enough and may\nprovide useless guidance information. To solve this, we in-\ncorporate another modality, e,g., text, to provide a compre-\nhensive measurement as\nLa2v = F(ev, ea) + F(ev, ep).\n(14)\nThe ev, ea and ep are the corresponding embeddings in the\nmultimodal space of ImageBind. The F represents the dis-\ntance function between two embedding vectors which is one\nminus cosine similarity between them. Similarly, the loss\nfor V2A can be written as\nLv2a = F(ea, ev) + F(ea, ep).\n(15)\nFor visual-audio joint generation, the loss turns into a trian-\ngle:\nLjoint-va = F(ev, ep) + F(ev, ea) + F(ea, ep).\n(16)\nThe text can be input by the user to provide a user-guided\ninteractive system or can be extracted via audio captioning\nmodels. As stated before, the audio tends to present in-\ncomplete semantic information. Thus, the extracted caption\nshould be worse than that. However, we empirically find\nthat our approach helps to correct these semantic errors, and\nimproves the semantic alignment.\n3.2.4\nGuided prompt tuning\nUsing the aforementioned multimodal latent guidance, we\nsuccessfully achieved good generation quality and better\ncontent alignment on visual-to-audio generation. However,\nwe observed that when applying this approach to audio-\nto-visual generation, the guidance has a neglectable effect.\nMeanwhile, when leveraging the audio to generate corre-\nsponding audios, the generated video becomes less tempo-\nral consistent due to the gradient of each frame having no\nensure of temporal coherence. Therefore, to overcome this\nissue, we further propose guided prompt tuning by optimiz-\ning the input text embedding vector of the generative model,\nwhich is formulated as\nˆy = y − λ2∇yL.\n(17)\nThe λ2 indicates the learning rate for the prompt embed-\nding. Specifically, we detach the prompt text embedding at\nthe beginning of predicting the noise and retain a compu-\ntational graph from the text embedding to the calculation\nof multimodal loss. Then we backpropagate the computa-\ntional graph to obtain the gradient of the prompt embedding\nw.r.t. the multimodal loss. The updated embedding is shared\nacross all timesteps to provide consistent semantic guidance\ninformation.\n4. Experiments\n4.1. Experimental Setup\nDataset We utilize the VGGSound dataset [6] and Land-\nscape dataset [36] for evaluation on video-to-audio, audio-\nto-video, and audio-video joint generation task. Since our\nmethod is an optimization-based solution, there is no need\nto utilize the entire dataset for evaluation. Instead, we ran-\ndomly sample 3k video-audio pairs from the VGGSound\n5\nTask\nMethod\nMetric\nV2A\nKL↓\nISc↑\nFD↓\nFAD↓\nSpecVQGAN [26]\n3.290\n5.108\n37.269\n7.736\nOurs-vanilla\n3.203\n5.625\n40.457\n6.850\nOurs\n2.619\n5.831\n32.920\n7.316\nI2A\nKL↓\nISc↑\nFD↓\nFAD↓\nIm2Wav [37]\n2.612\n7.055\n19.627\n7.576\nOurs-vanilla\n3.115\n4.986\n33.049\n7.364\nOurs\n2.691\n6.149\n20.958\n6.869\nA2V\nFVD↓\nKVD↓\nAV-align↑\n-\nTempoToken [46]\n1866.285\n389.096\n0.423\n-\nOurs-vanilla\n417.398\n36.262\n0.518\n-\nOurs\n402.385\n34.764\n0.522\n-\nJoint VA Generation\nFVD↓\nKVD↓\nFAD↓\nLandscape: MM [36]\n1141.009\n135.368\n7.752\n-\nLandscape: MM [36] + Ours\n1174.856\n135.422\n6.463\n-\nAV-alignbind↑\nVT-alignbind ↑\nAT-alignbind ↑\nAV-align ↑\nOpen-domain: MM[36]\nN/A\nN/A\nN/A\nN/A\nOpen-domain: Ours-vanilla\n0.074\n0.322\n0.081\n0.226\nOpen-domain: Ours\n0.096\n0.324\n0.138\n0.283\nTable 1. Quantitative comparison with baselines on four tasks.\nInput video\nSpecVQGAN\nOurs\nFigure 3. Compared with baseline on the video-to-audio generation task. SpecVQGAN fails to generate realistic and aligned audio with\nthe input video. Our method can produce aligned audio with the input video rhythm.\ndataset for video-to-audio generation, 3k pairs for audio-to-\nvideo generation, and 3k pairs for image-to-audio genera-\ntion respectively. We extract the key frame from each video\nfor the image-to-audio generation task. We also randomly\nsample 200 video-audio pairs from the Landscape dataset\nfor video-audio joint generation.\nImplementation details We utilize the pretrained Audi-\noLDM [29] for video-to-audio and image-to-audio gener-\nation, the AnimateDiff [18] for audio-to-video generation.\nWe use both the pre-trained AudioLDM and AnimateDiff\nfor the joint audio-video generation. We set the denoising\nstep to 30 for video-to-audio generation, 25 for audio-to-\nvideo generation, and 25 for audio-video joint generation,\nrespectively. We use the learning rate 0.1 for guiding the\nAudioLDM denoising and 0.01 for guiding the Animate-\nDiff denoising, which applies to all the tasks. We fixed\nthe random seed of the optimization process for fair com-\nparisons. All the experiments are conducted on NVIDIA\nGeforce RTX 3090 GPUs.\n4.2. Baselines\nVideo-to-Audio We choose SpecVQGAN [26] as the base-\nline of Video-to-Audio generation task. We used the pre-\ntrained model, which was trained using ResNet-50 with 5\nfeatures on VGGSound [26] as our inference model and\ncompared our method with SpecVQGAN on 3k VGGSound\nsample datasets.\nImage-to-Audio We choose Im2Wav as the baseline of the\nImage-to-Audio generation task and used the pre-trained\nmodel provided by the authors [37], test on 3k Paprika\nstyle transferred VGGSound samples transferred by Ani-\nmeGANv2 [8].\nAudio-to-Video We choose TempoTokens as the baseline\nof the Audio-to-Video generation task and used the pre-\n6\nA bear with \nsunglasses \nmaking \nsmoothies in \na kitchen.\nA bicycle on \ntop of a boat\nA man is \nriding a horse \nin sunset\nOurs\nOurs-vanilla\nFigure 4. Compared with baseline on the joint video-and-audio generation task. Our method can produce better text-aligned visual content\nthan the vanilla model. Besides, our generated audio is also of better quality and better alignment with the generated videos.\nTempoToken\nInput audio\nOurs-Vanilla\nOurs\nFigure 5. Compared with baseline on the audio-to-video task. Given the input audio, the generated videos by TempoToken are not aligned\nwith the input audio and the generation with poor visual quality. Our method can produce visually much better and semantically aligned\ncontent with the input condition.\ntrained model provided by the authors [46], test on 3k VG-\nGSound samples.\nJoint video and audio generation As MM-Diffusion [36]\nis the state-of-the-art of unconditional video and audio joint\ngeneration task, We choose it as the baseline of uncondi-\ntional video and audio joint generation task in the limit\nLandscape domain with 200 Landscape samples using the\nmodel pre-trained on Landscape datasets. On the open do-\nmain, we compare our Ours-with-guidance model with the\nOurs-vanilla model, as, to the best of our knowledge, there\nis no established baseline for this task.\nOurs-Vanilla We design several vanilla models of our tasks\nwith the combination of existing tools. For the video-to-\naudio task, we extract the key frame [27] and use a pre-\n7\nOurs\nOurs-\nVanilla\nInput\nVideo\nFigure 6. Compared with our vanilla model on the video-to-audio\ngeneration task. Our method can significantly reduce the back-\nground and irrelevant sound and thus achieve better audio quality,\nwhich is also reflected in Tab. 1.\ntrained image caption model [3] to obtain the caption for the\nvideo. We then use the extracted caption to generate audio\nwith the AudioLDM model. For the audio-to-video task, we\nuse an audio caption model and feed the extracted caption to\nthe AnimateDiff to generate the videos for the input audio.\nFor the joint audio and video generation task, we directly\ntake the test prompt as input to the AudioLDM model and\nAnimateDiff model to compose the joint generation results.\n4.3. Visual-to-Audio Generation\nVisual-to-audio generation includes video-to-audio gener-\nation and image-to-audio generation tasks. The image-to-\naudio generation requires audio-visual alignment from the\nsemantic level, whereas temporal alignment is additionally\nneeded for video-to-audio generation. Moreover, the gen-\nerated audio also needs to be high-fidelity.\nTo quantita-\ntively evaluate our performance on these aspects, we utilize\nthe MKL metric [26] for audio-video relevance, Inception\nscore (ISc), Frechet distance (FD), and Frechet audio dis-\ntance (FAD) for audio fidelity evaluation. From Tab. 1, we\ncan see that even though our method is training-free, we\ncan still outperform the baseline which requires large-scale\ntraining on audio-video pairs.\nWhen compared with the\ntext-to-audio baseline, we could see that our method con-\nsistently improves the audio-video relevance and the audio\ngeneration quality. When compared with our vanilla base-\nline, we find our method can significantly improve the audio\nquality, especially by reducing irrelevant sound and back-\nground noise, as shown in Fig. 6.\n4.4. Audio-to-Video Generation\nAudio-to-video generation requires the generated videos\nto be high-quality, as well as semantically and temporally\naligned with the input audio.\nTo quantitatively evaluate\nthe visual quality of the generated videos, we adopt the\nFrechet Video Distance (FVD) and Kernel Video Distance\n(KVD) [41] as the metrics. We also use the audio-video\nalignment (AV-align) [46] metric to measure the alignment\nof the generated video and the input audio. We show our\nquantitative results in Tab. 1. We observe that our training-\nfree method can outperform the training-based baseline in\nterms of both semantic alignment and video quality. Be-\nOurs\nOurs-\nVanilla\nInput\nVideo\nFigure 7. We visualize the effect of our guided prompt tuning.\nThe automatic caption generated is “frozen 2 - screenshot”, which\nfails to capture the meaningful visual content, and thus, the text-\nto-audio method fails to produce meaningful sounds. Our prompt\ntuning can inspect the visual information to complement the se-\nmantic information to generate meaningful sounds.\nsides, compared with the text-to-video method, our method\ncan achieve better audio-video alignment while maintain-\ning a comparable visual quality performance. We show our\nqualitative results in Fig. 5. We observe that TempoToken\nstruggles with visual quality and audio-visual alignment,\nand thus the generated videos are not relevant to the input\naudio and the generated quality is relatively poor. Although\nthe text-to-video method can achieve good performance on\nthe visual quality of the generated videos, it struggles to\naccurately align with the input audio content. Our training-\nfree method, utilizing a shared audio-visual representation\nspace, can achieve a good tradeoff between visual quality\nand audio-visual alignment.\n4.5. Joint Video and Audio Generation\nThe practical joint video and audio generation task should\ntake the text as the input, produce high-fidelity videos\nand audio, maintain the audio-video alignment, and main-\ntain the text-audio and text-video relevance.\nSpecifi-\ncally, we adopt the FVD for video quality, FAD for au-\ndio quality, AV-align [46] for audio-video relevance, TA-\nalign for text-audio alignment, and the TV-align for text-\nvideo alignment. Our quantitative evaluation is shown in\nTab. 1. Our latent aligner can be plugged into existing un-\nconditional audio-video joint generation framework MM-\nDiffusion [36]. The results show that compared with the\noriginal MM-Diffusion, our latent aligner can boost the au-\ndio generation quality when maintaining the video gener-\nation performance.\nWe also verify our method of text-\nconditioned joint video and audio generation. We bridge the\nvideo diffusion model AnimateDiff [18] and audio diffusion\nmodel AudioLDM [29] with our diffusion latent aligner. We\nrandomly collect 100 prompts from the web to condition our\ngeneration. Compared with separate text-to-video and text-\nto-audio models, our aligner can improve text-video align-\nment, text-audio alignment, and video-audio alignment. We\nshow the qualitative comparison in Fig. 4. More qualitative\nresults can be found in the Supplementary.\n8\n4.6. Limitations\nOur performance is limited by the generation capability of\nthe adopted foundation generation models, i.e., AudioLDM\nand AnimateDiff. For example, for our A2V and Joint VA\ntasks built on AnimateDiff, the visual quality, complex con-\ncept composition, and complex motion could be improved\nin the future. Notably, the flexibility of our method allows\nfor adopting more powerful generative models in the future\nto further improve the performance.\n5. Conclusion\nWe propose an optimization-based method for the open-\ndomain audio and visual generation task.\nOur method\ncan enable video-to-audio generation, audio-to-video gen-\neration, video-audio joint generation, image-to-audio gen-\neration, and audio-to-image generation tasks.\nInstead of\ntraining giant models from scratch, we utilize the a shared\nmultimodality embedding space provided by ImageBind to\nbridge the pre-trained visual generation and audio genera-\ntion diffusion models. Through extensive experiments on\nseveral evaluation datasets, we show the advantages of our\nmethod, especially in terms of improving the audio genera-\ntion fidelity and audio-visual alignment.\nAcknowlegement\nThis project was supported by the Na-\ntional Key R&D Program of China under grant number\n2022ZD0161501.\nReferences\n[1] Jie An, Songyang Zhang, Harry Yang, Sonal Gupta, Jia-Bin\nHuang, Jiebo Luo, and Xi Yin. Latent-shift: Latent diffu-\nsion with temporal shift for efficient text-to-video genera-\ntion. arXiv preprint arXiv:2304.08477, 2023. 2, 3\n[2] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta,\nYaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried,\nand Xi Yin. Spatext: Spatio-textual representation for con-\ntrollable image generation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18370–18380, 2023. 2\n[3] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xi-\naodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang,\nBinyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Day-\niheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin\nMa, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi\nTan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei\nWang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang,\nHao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen\nYu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan\nZhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren\nZhou, Xiaohuan Zhou, and Tianhang Zhu. Qwen technical\nreport. arXiv preprint arXiv:2309.16609, 2023. 8\n[4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n22563–22575, 2023. 1, 2, 3\n[5] Moitreya Chatterjee and Anoop Cherian. Sound2sight: Gen-\nerating visual dynamics from sound and context. In Com-\nputer Vision–ECCV 2020: 16th European Conference, Glas-\ngow, UK, August 23–28, 2020, Proceedings, Part XXVII 16,\npages 701–719. Springer, 2020. 3\n[6] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew\nZisserman. Vggsound: A large-scale audio-visual dataset.\nIn ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages\n721–725. IEEE, 2020. 5\n[7] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang,\nXiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu,\nQifeng Chen, Xintao Wang, Chao Weng, and Ying Shan.\nVideocrafter1: Open diffusion models for high-quality video\ngeneration, 2023. 1, 2\n[8] Xin Chen.\nAnimeganv2.\nhttps://github.com/\nTachibanaYoshino/AnimeGANv2/, 2022. 6\n[9] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang\nWang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xi-\naofang Wang, Abhimanyu Dubey, et al.\nEmu: Enhanc-\ning image generation models using photogenic needles in a\nhaystack. arXiv preprint arXiv:2309.15807, 2023. 1\n[10] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 3\n[11] Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhat-\ntacharya,\nSantiago Pascual,\nJoan Serr`a,\nTaylor Berg-\nKirkpatrick, and Julian McAuley. Clipsonic: Text-to-audio\nsynthesis with unlabeled videos and pretrained language-\nvision models. arXiv preprint arXiv:2306.09635, 2023. 2\n[12] Yuexi Du, Ziyang Chen, Justin Salamon, Bryan Russell, and\nAndrew Owens. Conditional generation of audio from video\nvia foley analogies. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n2426–2436, 2023. 2\n[13] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming\ntransformers for high-resolution image synthesis.\nIn Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 12873–12883, 2021. 2\n[14] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan\nPang, David Jacobs, Jia-Bin Huang, and Devi Parikh.\nLong video generation with time-agnostic vqgan and time-\nsensitive transformer. In European Conference on Computer\nVision, pages 102–118. Springer, 2022. 3\n[15] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, An-\ndrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang,\nMing-Yu Liu, and Yogesh Balaji. Preserve your own cor-\nrelation: A noise prior for video diffusion models. arXiv\npreprint arXiv:2305.10474, 2023. 1, 2\n[16] Deepanway Ghosal, Navonil Majumder, Ambuj Mehrish,\nand Soujanya Poria.\nText-to-audio generation using\ninstruction-tuned llm and latent diffusion model.\narXiv\npreprint arXiv:2304.13731, 2023. 2\n9\n[17] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat\nSingh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan\nMisra. Imagebind: One embedding space to bind them all.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 15180–15190, 2023.\n1, 3, 4\n[18] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu\nQiao, Dahua Lin, and Bo Dai. Animatediff: Animate your\npersonalized text-to-image diffusion models without specific\ntuning. arXiv preprint arXiv:2307.04725, 2023. 1, 6, 8\n[19] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and\nQifeng Chen. Latent video diffusion models for high-fidelity\nvideo generation with arbitrary lengths.\narXiv preprint\narXiv:2211.13221, 2022. 1, 2, 3\n[20] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun,\nYuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao\nWeng, Ying Shan, et al.\nAnimate-a-story:\nStorytelling\nwith retrieval-augmented video generation. arXiv preprint\narXiv:2307.06940, 2023. 1\n[21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 3\n[22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 1, 3\n[23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv preprint arXiv:2204.03458, 2022. 3\n[24] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang,\nZhenhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma,\nand Zhou Zhao. Make-an-audio 2: Temporal-enhanced text-\nto-audio generation. arXiv preprint arXiv:2305.18474, 2023.\n2\n[25] Rongjie Huang, Jiawei Huang, Dongchao Yang, Yi Ren,\nLuping Liu, Mingze Li, Zhenhui Ye, Jinglin Liu, Xiang\nYin, and Zhou Zhao. Make-an-audio: Text-to-audio genera-\ntion with prompt-enhanced diffusion models. arXiv preprint\narXiv:2301.12661, 2023. 1, 2, 3\n[26] Vladimir Iashin and Esa Rahtu.\nTaming visually guided\nsound generation. arXiv preprint arXiv:2110.08791, 2021.\n1, 2, 6, 8\n[27] KeplerLab. Tool for automating common video key-frame\nextraction, video compression and image auto-crop/image-\nresize tasks.\nhttps://github.com/keplerlab/\nkatna, 2021. 7\n[28] Felix Kreuk, Gabriel Synnaeve, Adam Polyak, Uriel Singer,\nAlexandre D´efossez, Jade Copet, Devi Parikh, Yaniv Taig-\nman, and Yossi Adi. Audiogen: Textually guided audio gen-\neration. arXiv preprint arXiv:2209.15352, 2022. 1, 2\n[29] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley. Audi-\noldm: Text-to-audio generation with latent diffusion models.\narXiv preprint arXiv:2301.12503, 2023. 2, 6, 8\n[30] Haohe Liu, Qiao Tian, Yi Yuan, Xubo Liu, Xinhao Mei, Qi-\nuqiang Kong, Yuping Wang, Wenwu Wang, Yuxuan Wang,\nand Mark D Plumbley. Audioldm 2: Learning holistic audio\ngeneration with self-supervised pretraining. arXiv preprint\narXiv:2308.05734, 2023. 1, 2\n[31] Simian Luo, Chuanhao Yan, Chenxu Hu, and Hang Zhao.\nDiff-foley: Synchronized video-to-audio synthesis with la-\ntent diffusion models.\narXiv preprint arXiv:2306.17203,\n2023. 2\n[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 1\n[33] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2\n[34] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gener-\nation with clip latents. arXiv preprint arXiv:2204.06125, 1\n(2):3, 2022. 1\n[35] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR 2022,\nNew Orleans, LA, USA, June 18-24, 2022, pages 10674–\n10685. IEEE, 2022. 2\n[36] Ludan Ruan, Yiyang Ma, Huan Yang, Huiguo He, Bei Liu,\nJianlong Fu, Nicholas Jing Yuan, Qin Jin, and Baining\nGuo. Mm-diffusion: Learning multi-modal diffusion mod-\nels for joint audio and video generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 10219–10228, 2023. 1, 3, 5, 6, 7, 8\n[37] Roy Sheffer and Yossi Adi.\nI hear your true colors: Im-\nage guided audio generation. In ICASSP 2023-2023 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 1–5. IEEE, 2023. 2, 6\n[38] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 1, 3\n[39] Kun Su, Kaizhi Qian, Eli Shlizerman, Antonio Torralba,\nand Chuang Gan. Physics-driven diffusion models for im-\npact sound synthesis from videos.\nIn Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9749–9759, 2023. 2\n[40] Ming Tao, Bing-Kun Bao, Hao Tang, and Changsheng Xu.\nGalip: Generative adversarial clips for text-to-image synthe-\nsis. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 14214–14223,\n2023. 2\n[41] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 8\n10\n[42] Chia-Hung Wan, Shun-Po Chuang, and Hung-Yi Lee. To-\nwards audio to scene image synthesis using generative ad-\nversarial network.\nIn ICASSP 2019-2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 496–500. IEEE, 2019. 2\n[43] Ho-Hsiang Wu, Prem Seetharaman, Kundan Kumar, and\nJuan Pablo Bello. Wav2clip: Learning robust audio repre-\nsentations from clip. In ICASSP 2022-2022 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 4563–4567. IEEE, 2022. 2\n[44] Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao\nWeng, Yuexian Zou, and Dong Yu.\nDiffsound: Discrete\ndiffusion model for text-to-sound generation.\nIEEE/ACM\nTransactions on Audio, Speech, and Language Processing,\n2023. 1, 2\n[45] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz,\nand Yossi Adi.\nDiverse and aligned audio-to-video gen-\neration via text-to-video model adaptation. arXiv preprint\narXiv:2309.16429, 2023. 3\n[46] Guy Yariv, Itai Gat, Sagie Benaim, Lior Wolf, Idan Schwartz,\nand Yossi Adi.\nDiverse and aligned audio-to-video gen-\neration via text-to-video model adaptation. arXiv preprint\narXiv:2309.16429, 2023. 1, 6, 7, 8\n[47] Maciej ˙Zelaszczyk and Jacek Ma´ndziuk.\nAudio-to-image\ncross-modal generation. In 2022 International Joint Confer-\nence on Neural Networks (IJCNN), pages 1–8. IEEE, 2022.\n2\n[48] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu,\nRui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and\nMike Zheng Shou. Show-1: Marrying pixel and latent dif-\nfusion models for text-to-video generation. arXiv preprint\narXiv:2309.15818, 2023. 3\n[49] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,\nYizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video\ngeneration with latent diffusion models.\narXiv preprint\narXiv:2211.11018, 2022. 3\n[50] Junchen Zhu, Huan Yang, Huiguo He, Wenjing Wang, Zixi\nTuo, Wen-Huang Cheng, Lianli Gao, Jingkuan Song, and\nJianlong Fu. Moviefactory: Automatic movie creation from\ntext using large generative models for language and images.\narXiv preprint arXiv:2306.07257, 2023. 3\n11\n"
}