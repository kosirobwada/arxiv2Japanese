{
    "optim": "When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning Leon Lang * 1 Davis Foote * 2 Stuart Russell 2 Anca Dragan 2 Erik Jenner 2 Scott Emmons * 2 Abstract Past analyses of reinforcement learning from hu- man feedback (RLHF) assume that the human fully observes the environment. What happens when human feedback is based only on partial observations? We formally define two failure cases: deception and overjustification. Model- ing the human as Boltzmann-rational w.r.t. a be- lief over trajectories, we prove conditions under which RLHF is guaranteed to result in policies that deceptively inflate their performance, over- justify their behavior to make an impression, or both. To help address these issues, we mathemat- ically characterize how partial observability of the environment translates into (lack of) ambigu- ity in the learned return function. In some cases, accounting for partial observability makes it the- oretically possible to recover the return function and thus the optimal policy, while in other cases, there is irreducible ambiguity. We caution against blindly applying RLHF in partially observable settings and propose research directions to help tackle these challenges. 1. Introduction Reinforcement learning from human feedback (RLHF) and its variants are widely used for finetuning foundation mod- els, including ChatGPT (OpenAI, 2022), Bard (Manyika, 2023), Gemini (Gemini Team, 2023), Llama 2 (Touvron et al., 2023), and Claude (Bai et al., 2022; Anthropic, 2023a;b). Prior theoretical analysis of RLHF assumes that the human fully observes the state of the world (Skalse et al., 2023). Under this assumption, it is possible to recover the ground-truth return function from Boltzmann-rational human feedback (see Proposition 3.1). *Core research contributor 1Amsterdam Machine Learning Lab, University of Amsterdam 2Center for Human-Compatible Artificial Intelligence, University of California, Berkeley. Cor- respondence to: Leon Lang <l.lang@uva.nl>, Scott Emmons <emmons@berkeley.edu>. In reality, however, this assumption is false. Even when humans have a complete view of the environment, they may not have a complete understanding of it and cannot provide ground-truth feedback (Evans et al., 2016). Furthermore, as AI agents are deployed in increasingly complex environ- ments, humans will only have a partial view of everything the agents view. How does the human’s partial observability impact learning from human feedback? We begin our investigation with an example, illustrated in Figure 1. An AI assistant is helping a user install software. It is possible for the assistant to hide error messages by redirecting them to /dev/null. We model the human as having a belief B over the state and extend the Boltzmann- rational assumption from prior work to incorporate this belief. In the absence of an error message, the human is uncertain if the agent left the system untouched or hid the er- ror message from a failed installation. We find that because the human disprefers trajectories with error messages, the AI learns to hide error messages from the human. Figure 2 shows in full mathematical detail how this failure occurs. It also shows a second failure case, where the AI clutters the output with overly verbose logs. Generalizing from these examples, we formalize two risks, dual to each other: deception and overjustification. We provide a mathematical definition of each. When the obser- vation kernel, i.e. the function specifying the observations given states, is deterministic, Theorem 4.5 analyzes the properties of suboptimal policies learned by RLHF. These policies then exhibit deceptive inflation — where they ap- pear to produce higher reward than they actually do — or overjustification — where they incur a cost in order to make a good appearance — or both. After seeing how naive RLHF fails, we ask: can we do bet- ter? Under our model of the human’s belief and feedback, we mathematically characterize the ambiguity in the return function that arises with human feedback from partial ob- servations. This is Theorem 5.1. Applying Theorem 5.1 to examples where naive RLHF fails, we see cases where the ambiguity in the return function is totally resolved. In these cases, we also show that the return function inference is robust to small misspecifications in the human belief model. In other cases, we find irreducible ambiguity, leading to re- 1 arXiv:2402.17747v1  [cs.LG]  27 Feb 2024 Challenges with Partial Observability of Human Evaluators in Reward Learning Figure 1. A human compares trajectories to provide data for RLHF. Rather than observing ⃗s and ⃗s ′ directly, the human sees observations ⃗o and ⃗o ′, which they use to estimate the total true reward of each trajectory. In this example, an agent executes shell commands to install Nvidia drivers and CUDA. Both ⃗s and ⃗s ′ contain an error, but in ⃗s ′, the agent hides the error. The human thus believes ⃗s ′ is the better trajectory, rewarding the agent’s deceptive behavior. The underlying MDP and observation function are in Fig. 2A. turn functions consistent with the human’s feedback whose optimal policies have arbitrarily high regret. We conclude by discussing the implications of our work. When feedback is based on partial observations, we caution against blindly applying RLHF. To help address this chal- lenge, we suggest several avenues for future research. In particular, modeling human beliefs could help AI interpret human feedback, and eliciting knowledge from AIs could provide humans with information they can’t observe. 2. Related work A systematic review of open problems and limitations of RLHF, including a brief discussion of partial observability, can be found in Casper et al. (2023). RLHF is a special case of reward-rational choice (Jeon et al., 2020), a gen- eral framework which also encompasses demonstrations- based inverse reinforcement learning (Ziebart et al., 2008; Ng et al., 2000) and learning from the initial environment state (Shah et al., 2019), and can be seen as a special case of assistance problems (Fern et al., 2014; Hadfield-Menell et al., 2016; Shah et al., 2021). In all of these, the reward function is learned from human actions, which in the case of RLHF are simply preference statements. This requires us to specify the human policy of action selection—Boltzmann rationality in typical RLHF—which can lead to wrong re- ward inferences when this specification is wrong (Skalse & Abate, 2022); unfortunately, the human policy can also not be learned alongside the human’s values without further assumptions (Mindermann & Armstrong, 2018). Instead of a model of the human policy, in this paper we mostly focus on the human belief model and misspecifications thereof for the case that the human only receives partial observations. The problem of human interpretations of observations was already briefly mentioned in Amodei et al. (2017), where hu- man evaluators misinterpreted the movement of a robot hand in simulation. Eliciting Latent Knowledge (Christiano et al., 2021) posits that in order to give accurate feedback from partial observations, the human needs to be able to query latent knowledge of the AI system about the world state. How to do this is currently an unsolved problem (Christiano & Xu, 2022). Compared to these early investigations, we clearly formalize a model of humans under partial observ- ability and provide new mathematical results analyzing the resulting failure modes and their potential mitigations. Related work (Zhuang & Hadfield-Menell, 2020) analyzes the consequences of aligning an AI with a proxy reward function that omits attributes that are important to the hu- man’s values, which could happen if the reward function is based on a belief over the world state given limited informa- tion. Another instance are recommendation systems (Stray, 2023), where user feedback does not depend on informa- tion not shown—which is crucially part of the environment. Siththaranjan et al. (2023) analyze what happens under RLHF if the learning algorithm doesn’t have all the rel- evant information (e.g. about the identity of human raters), complementing our study of what happens when human raters are missing information. Our work argues that deception can result from applying RLHF from partial observations. But deception may also 2 Challenges with Partial Observability of Human Evaluators in Reward Learning emerge for other reasons: Hubinger et al. (2019) introduced the hypothetical concept of deceptive alignment, a situation in which an AI system may deceive the human into believing it is aligned while it plans a later takeover. Recently, there has been a call for empirical support of this possibility (Hub- inger et al., 2023). Once a model is deceptively aligned, this may be hard to remove (Hubinger et al., 2024). Under the definition from Park et al. (2023) we previously discussed, GPT-4 was shown to engage in deceptive behavior in a sim- ulated environment (Scheurer et al., 2023). There is also the systematic inducement of true beliefs (Reddy et al., 2020), which we would not call deceptive. A third line of research defines deception in structural causal games and adds the aspect of intentionality (Ward et al., 2023), which recently got preliminary empirical support (Hofst¨atter et al., 2023). Finally, we mention connections to truthful AI (Evans et al., 2021; Lin et al., 2022; Burns et al., 2023; Huang et al., 2023), which is about ensuring that AI systems tell the truth about aspects of the real world. Partial observability is a mechanism that makes it feasible for models to lie without being caught: If the human evaluator does not observe the full environment, or does not fully understand it, then they may not detect when the AI is lying. More speculatively, we can imagine that AI models will at some point be part of the distribution of human observations PO by telling us the outcomes of their actions. E.g., imagine an AI system that manages your assets and assures you that they are increasing in value while they are actually not. In our work, we leave this additional problem out of the analysis by assuming that the observation distribution PO is a fixed part of the environment that cannot be optimized. 3. Reward identifiability from full observations In this section we review Markov decision processes and previous results on the identifiability of the reward function under fully observed RLHF. 3.1. Markov decision processes We assume Markov decision processes (MDPs) given by (S, A, T , P0, R, γ). For any finite set X, let ∆(X) be the set of probability distributions on X. Then S is a finite set of states, A is a finite set of actions, T : S × A → ∆(S) is a transition kernel written T (s′ | s, a) ∈ [0, 1], P0 ∈ ∆(S) is an initial state distribution, R : S → R is the true reward function, and γ ∈ [0, 1] is a discount factor. A policy is given by a function π : S → ∆(A). We assume a finite time horizon T. Let ⃗S be the set of state sequences ⃗s = s0, . . . , sT that are possible, so ⃗s ∈ ⃗S if it has a strictly positive probability of being sampled from P0, T , and an exploration policy π with π(a | s) > 0 for all s ∈ S, a ∈ A. A sequence ⃗s gives rise to a return G(⃗s) := PT t=0 γtR(st). Let P π(⃗s) be the on-policy probability that ⃗s is sampled from π interacting with the environment. The policy is then usually trained to maximize the policy evaluation function, which is the on-policy expectation of the return function: J(π) := E⃗s∼P π(·) \u0002 G(⃗s) \u0003 . 3.2. RLHF and identifiability from full observations In practice, the reward function R may not be known and needs to be learned from human feedback. In a simple form of RLHF (Christiano et al., 2017), this feedback takes the form of binary preference comparisons between trajectories: a human is presented with state sequences ⃗s and ⃗s′ and choose the one they prefer. Under the Boltzmann rationality model, we assume the human picks ⃗s with probability P R\u0000⃗s ≻ ⃗s′\u0001 := σ \u0010 β \u0000G(⃗s) − G(⃗s′) \u0001\u0011 , (1) where β > 0 is an inverse temperature parameter and σ(x) := 1 1+exp(−x) is the sigmoid function (Bradley & Terry, 1952; Christiano et al., 2017; Jeon et al., 2020). An important question is identifiability: In the infinite data limit, do the human choice probabilities P R collectively provide enough information to uniquely identify the reward function R? This is answered by Skalse et al. (2023, Theo- rem 3.9 and Lemma B.3): Proposition 3.1 (Skalse et al. (2023)). Let R be the true reward function and G the corresponding return function. Then the collection of all choice probabilities P R(⃗s ≻ ⃗s′) for state sequence pairs ⃗s,⃗s′ ∈ ⃗S determines the return function G on sequences ⃗s ∈ ⃗S up to an additive constant. The reason is simple: since the sigmoid function is bijective, P R determines the difference in returns between any two trajectories. From that we can reconstruct individual returns up to an additive constant. The reward function R is not necessarily identifiable from preference comparisons, see Skalse et al. (2023, Lemma B.3) for a precise characterization of the remaining am- biguity. However, the optimal policy only depends on R indirectly through the return function G, and is furthermore invariant under adding a constant to G. This means that in the fully observable setting, Boltzmann rational compar- isons completely determine the optimal policy. In Section 5, we will show under which conditions this guarantee breaks in the partially observable setting. 4. The impact of partial observations on RLHF We now analyze failure modes of a naive application of RLHF from partial observations, both theoretically and with examples. The resulting policies can show two distinct be- havior patterns that we call deceptive inflation and overjus- tification, both of which we will also formally define. This can lead to significant regret compared to an optimal policy. 3 Challenges with Partial Observability of Human Evaluators in Reward Learning Later, in Section 5, we will see that an adaptation of the usual RLHF process can sometimes avoid these problems. To model partial observability, we introduce a space of possible observations o ∈ Ω and and observation ker- nel with probabilities PO(o | s) ∈ [0, 1]. We write P ⃗O(⃗o | ⃗s) := QT t=0 PO(ot | st) for the probability of an observation sequence. Analogously to ⃗S, we write ⃗Ω for the set of observation sequences that occur with non- zero probability, i.e., ⃗o ∈ ⃗Ω if and only if there is ⃗s ∈ ⃗S such that QT t=0 PO(ot | st) > 0. If PO and P ⃗O are deter- ministic, then we write O : S → Ω and ⃗O : ⃗S → ⃗Ω for the corresponding observation functions with O(s) = o and ⃗O(⃗s) = ⃗o for o and ⃗o with PO(o | s) = 1 and P ⃗O(⃗o | ⃗s) = 1, respectively. 4.1. What does RLHF learn from partial observations? We consider the setting where the state is fully observable to the learned policy, but human feedback may depend only on a sequence of observations. We then assume that the human gives feedback under a Boltzmann rational model similar to Eq. (1). However, the human now doesn’t have access to the true sequence ⃗s. So we assume that instead, they form some belief B(⃗s | ⃗o) ∈ [0, 1] about the state sequence ⃗s based on the observations ⃗o. We then assume preferences are Boltzmann rational in the expected returns under this belief, instead of the actual returns. The assumption of Boltzmann rationality is false in prac- tice (Evans et al., 2015; Majumdar et al., 2017; Buehler et al., 1994), but note that it is an optimistic assumption: Even though our model is a simplification, we expect that practical issues can be at least as bad as the ones we will discuss. See also Example C.4 for an example showing that it is sometimes generally not possible to find a human model that leads to good outcomes under RLHF. Future work could investigate different human models and their impact under partial observability in greater detail. To formalize this model, we collect the human beliefs into a matrix B := \u0000B(⃗s | ⃗o) \u0001 ⃗o,⃗s ∈ R⃗Ω× ⃗S. Then the expected returns for observations ⃗o are given by E⃗s∼B(·|⃗o) \u0002 G(⃗s) \u0003 = (B ·G)(⃗o). Here, we view G ∈ R ⃗S and B ·G ∈ R⃗Ω as both column vectors and as functions. Plugging these expected returns into Eq. (1), we get P R\u0000⃗o ≻ ⃗o′\u0001 := σ \u0010 β \u0000(B ·G)(⃗o) − (B ·G)(⃗o′) \u0001\u0011 . (2) If observations are deterministic, we can write ⃗O(⃗s) = ⃗o for ⃗o with P ⃗O(⃗o | ⃗s) = 1. We can then recover the fully observ- able case Eq. (1) with B and ⃗O being the identity. This is an instance of reward-rational (implicit) choice (Jeon et al., 2020), with the function ⃗o 7→ B(· | ⃗o) as the grounding function. The belief B can in principle be any distribution as long as it sums to 1 over ⃗s. The human could arrive at such a belief via Bayesian updates, assuming knowledge of P0, T , PO, and a prior over the policy that generates the trajectories (see Appendix B.1). None of our results rely on this more detailed model. What happens if the human gives feedback according to Eq. (2) but we infer a return function using the standard RLHF algorithm based on Eq. (1)? It is easy to show (as- suming deterministic observations, see Appendix C.1) that, up to an additive constant, RLHF will infer the following observation return function: Gobs(⃗s) := E ⃗o∼P ⃗ O(·|⃗s) h\u0000B ·G \u0001 (⃗o) i , (3) which is simply \u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 for deterministic P ⃗O and corresponding observation function ⃗O. So unlike in the fully observable case of Proposition 3.1, an incorrect return function might be inferred. Now, define the resulting policy evaluation function Jobs by Jobs(π) := E ⃗s∼P π(⃗s) \u0002 Gobs(⃗s) \u0003 . (4) This is the function which a standard reinforcement learning algorithm would optimize given the inferred return function Gobs. We summarize this as follows: Proposition 4.1. In partially observable settings with de- terministic observations, a policy is optimal according to RLHF, i.e., according to a return function model that would be learned by RLHF with infinite comparison data, if it maximizes Jobs. Note that in this definition, and specifically in the formula for Gobs, the human does not have knowledge of the policy π that generates the state sequence ⃗s. In Appendix C.2, we briefly discuss the unrealistic case that the human does know the precise policy and is an ideal Bayesian reasoner over the true environment dynamics. In that case, Jobs = J, i.e. there is no discrepancy between true and inferred returns. Intuitively, even if the human would not make any observations, they could give correct feedback essentially by estimating the policy’s expected return explicitly. In our case, however, a policy achieving high Jobs produces state sequences ⃗s whose observation sequence ⃗O(⃗s) looks good according to the human’s belief B \u0000⃗s′ | ⃗O(⃗s) \u0001 and return G(⃗s′). This already hints at a possible source of deception: if the policy achieves sequences whose observa- tions look good at the expense of actual value G(⃗s), then we might intuitively call this deceptive behavior. We will next define deception and analyze this point in greater detail. 4 Challenges with Partial Observability of Human Evaluators in Reward Learning 4.2. Deceptive inflating and overjustification in RLHF We will evaluate state sequences based on the extent to which they lead to the human overestimating or underes- timating the reward in expectation. Recall that Gobs from Eq. (3) measures the expected return from the perspective of a human with some belief function B and access to only observations, whereas G are the true returns. That leads us to the following definition: Definition 4.2 (Overestimation and Underestimation Error). Let ⃗s be a state sequence. We define its overestimation error E+ and underestimation error E− by E+(⃗s) := max \u00000, Gobs(⃗s) − G(⃗s) \u0001 , E−(⃗s) := max \u00000, G(⃗s) − Gobs(⃗s) \u0001 . We further define the average overestimation (underestima- tion) error under a policy π by E +(π) := E⃗s∼P π[E+(⃗s)] and E −(π) := E⃗s∼P π[E−(⃗s)]. Consider two policies π1 and π2 which attain the same av- erage return; J(π1) = J(π2). Suppose E +(π1) > E +(π2). Then compared to π2, π1 puts more probability mass on tra- jectories ⃗s which produce observations that the human over- estimates (believes to have been produced by better trajec- tories than ⃗s). Similarly, suppose that E −(π1) < E −(π2). Then compared to π2, π1 puts less probability mass on trajectories ⃗s which produce observations that the human mistakenly underestimates (believes to have been produced by worse trajectories than ⃗s). An agent seeking to maxi- mize positive feedback will prefer such a π1 to π2 despite their identical performance. We formalize this intuition beginning with the following definitions. Definition 4.3 (Deceptive Inflating and Overjustification). A policy π exhibits deceptive inflating relative to πref if E +(π) > E +(πref) and Jobs(π) ≥ Jobs(πref). A policy π exhibits overjustification relative to πref if E −(π) < E −(πref) and J(π) < J(πref). Deceptive inflating behaviors mislead the human toward higher estimates of the agent’s performance (by definition they produce more favorable observations). One such pat- tern involves taking undesired actions (stealing money; ma- nipulating experimental data) concealed within human blind spots if those actions allow the agent to later take a visi- ble high-reward action (depositing money; positive results). Another pattern is creating blind spots by tampering with sensors (turning a camera; altering logging behavior). Overjustification1 behaviors correct the human toward higher estimates of the agent’s performance when this is not desired by the human (by definition, they attain lower reward). One such pattern involves favoring less effective actions that are more visible to the human than optimal actions. Another pattern is to “pay” some resource (wall time; human attention) to provide information to the human, whose desire for that information does not justify the cost. We now state a key result. See Appendix C.3 for proofs. Lemma 4.4. Let π and πref be two policies. If J(π) < J(πref) and Jobs(π) ≥ Jobs(πref), then relative to πref, π must exhibit deceptive inflating, overjustification, or both. Theorem 4.5. Assume that PO is deterministic. Let π∗ obs be an optimal policy according to a naive application of RLHF under partial observability, and let π∗ be an optimal policy according to the true objective J. If π∗ obs is not J-optimal, then relative to π∗, π∗ obs must exhibit deceptive inflating, overjustification, or both. Any given state trajectory ⃗s may be more or less likely under π∗ obs than π∗, regardless of human estimation, so long as on net π∗ obs exhibits deceptive inflating or overjustification. We end this section with a brief discussion of “deception.” Park et al. (2023) define deception as “the systematic inducement of false beliefs in the pursuit of some outcome other than the truth.” We formalize this definition for our setting in which an agent acts in an MDP and a human estimates a value given partial observations of the agent’s trajectories. The esti- mation target could be the true return, as in the preceding definitions, or the human could instead estimate whether a particular undesired state was accessed or estimate the state sequence itself. We define “false beliefs” with some measure E(⃗s) of the average error of the human estimation given observations sampled from ⃗s. We capture “systematic inducement” by considering the on-policy average of E, E(π) = E⃗s∼P π[E(⃗s)] and measuring this against a refer- ence policy. We further introduce an objective function u to operationalize “the pursuit of some outcome.” Definition 4.6 (Deception). A policy π exhibits deception (“is deceptive”) relative to the reference policy πref with respect to error measure E and objective u if E(π) > E(πref) and u(π) ≥ u(πref). This definition tends to agree with human intuition about whether “deception” has occurred when u is an objective for which π was selected, e.g. if we obtained π from an RL algorithm and u is the on-policy expected total reward. We can thus see that deceptive inflating is a special case of deception: any policy exhibiting deceptive inflating rela- tive to πref is deceptive relative to πref with respect to error measure E+ and objective Jobs. 1This name is a nod to the Overjustification effect from psychol- ogy (Deci & Flaste, 1995), in which subjects become dependent on an extrinsic source of motivation to sustain work on a task. 5 Challenges with Partial Observability of Human Evaluators in Reward Learning Figure 2. Two example MDPs with observation functions in which RLHF chooses undesirable policies. Each box depicts a state with a footer showing the (deterministic) observation produced by that state. Outgoing edges from each box are available actions. A more detailed diagram for the first MDP, with explicit shell commands and log messages, is available in Appendix C.4. 4.3. Deception and overjustification in examples We now examine examples that illustrate the failure modes of RLHF in the presence of partial observability. For each of the following, we will characterize the policy which maximizes Jobs, as this is the policy RLHF selects for when observations are deterministic; see Proposition 4.1. Our examples feature an agent trained with RLHF to com- plete tasks in a user’s terminal. The output of each com- mand (stdout and stderr) is piped to a log file, which is what the human evaluator sees when making choices for RLHF. We assume that the final state T has a self-transition, episodes have a fixed horizon length of 3 (meaning state sequences have length 4: s0, . . . , s3), γ = 1, and there is a fixed initial state s0 = S. Furthermore, both exam- ples feature a fixed transition probability 0 < p < 1. We also assume that the human’s belief only supports possible explanations: B(⃗s | ⃗o) > 0 =⇒ ⃗O(⃗s) = ⃗o. 4.3.1. EXAMPLE A: HIDING FAILURES See Appendix C.4 for derivations and a tutorial-style analy- sis of this example. In Fig. 2A, the agent’s task is to install CUDA (states W and WH, with “W” for “Win”), which requires first installing Nvidia drivers (state I). If the CUDA installation fails, one enters L or LH (“L” for “lose”). This is the exam- ple from Figure 1. The aC action corresponds to a com- mand to install CUDA that logs both successes and failures, whereas aH (“H” for “Hiding”) corresponds to aC with 2> /dev/null appended, which suppresses error messages such that no log message is produced if the installation fails. The human may distinctly prefer that the agent not hide a failure (a hidden failure cannot be immediately addressed); we price this into the reward function with a penalty r ≥ 0 on the reward at LH. The agent may attempt the CUDA installation before installing drivers, but this will fail. There are three pairs of trajectories which produce iden- tical observations. Here we address the most promi- nent (see Appendix C.4 for the others): after success- fully installing drivers, a failed CUDA installation with 2> /dev/null (SILHT) and simply exiting (SITT) both produce a log containing only a success confirmation for the drivers (o∅oIo∅o∅), stylized as ⃗o2 in Fig. 1. Let pH := B \u0000⃗s = SILHT | ⃗o = o∅oIo∅o∅ \u0001 ∈ (0, 1) be the hu- man’s suspicion, upon seeing only a success confirmation for the drivers, that a failed CUDA installation was hidden. We consider two cases which differ only in whether the expected value of attempting to install CUDA (with default logging, aC) after installing the drivers is positive. In Exam- ple 4.7, the success probability p is high enough that the true optimal policy is to attempt the installation; in Example 4.8, p is low enough that the optimal policy is to terminate. Example 4.7. If the true-reward-optimal policy π∗ is to attempt to install CUDA with default logging (aC) after installing drivers, but pH < 5 5+r, then the Jobs-maximizing policy, which will be chosen by RLHF, is to instead use the 2> /dev/null trick. Intuitively, this is because if the in- stallation fails, the human is likely to believe that the agent just didn’t attempt to install CUDA, which is preferred to a failed installation. This is deceptive inflating; the overesti- mation error E + for this policy is (1−p)(1−pH)(5+r) > 0, whereas E +(π∗) = 0, in line with Theorem 4.5. If pH > 5 5+r, then RLHF will select π∗. Example 4.8. If π∗ is to terminate (aT ) after installing the drivers, but pH < 5 5+r, then RLHF will learn the deceptive inflating 2> /dev/null policy as in Example 4.7. However, if pH > 5 5+r, RLHF will choose the policy that attempts to install CUDA with default logging. Intuitively, this is because the combination of increased suspicion that the agent might be hiding errors (higher pH) and increased penalty on this suspected behavior (higher r) also penalizes 6 Challenges with Partial Observability of Human Evaluators in Reward Learning our preferred trajectory SITT since it can be misinter- preted as hiding errors. This is overjustification; the overes- timation error E + is 0 (thus this is not deceptive inflating), and as required by Theorem 4.5, the underestimation error E − is 0, lower than E−(π∗) = pH(5 + r). 4.3.2. EXAMPLE B: PAYING TO REVEAL INFORMATION In Fig. 2B, the agent’s task is to install Python (state I) and to optionally further install NumPy (states W and WV ). The aD action corresponds to a command to install NumPy with “default” settings which only logs errors, whereas aV corresponds to the same command with a --verbose flag that adds additional info. In the case of a success, the human distinctly prefers not to see this verbose output; we price this into the reward function with a penalty r > 0 on the reward at WV . There is only one pair of trajectories which produce identi- cal observations: after successfully installing Python, a suc- cessful NumPy installation with default logging (SIWT) and simply exiting (SITT) both produce a log containing only a success confirmation for Python (o∅oIo∅o∅). Let pD := B(⃗s = SIWT | ⃗o = o∅oIo∅o∅) ∈ (0, 1) be the hu- man’s optimism, upon seeing only a success confirmation for Python, that NumPy was also successfully installed (without the --verbose flag). Here we consider only the case where p is large enough that the true optimal policy is to install Python then attempt to install NumPy with default logging (aD). Example 4.9. If π∗ is to attempt to install NumPy with aD after installing Python, and pD > q := 1 5 \u0010 p(6 − r) − 1 \u0011 , then RLHF will select the policy that terminates after in- stalling Python. Intuitively, this is because the agent can exploit the human’s optimism that NumPy was installed quietly without taking the risk of an observable failure (L). This is deceptive inflating, with an overestimation error E + of 5pD, greater than E +(π∗) = 0. If instead pD < q, then RLHF will select the policy that attempts the NumPy installation with verbose logging (aV ). Intuitively, this is because the agent is willing to “pay” the cost of r true reward to prove to the human that it installed NumPy, even when the human does not want to see this proof. This is overjustification; the overestimation error E + is 0 (thus this is not deceptive inflating), and the underestimation error E − is 0, lower than E −(π∗) = 5p(1 − pD). 4.3.3. FURTHER EXAMPLES We show further, purely mathematical, examples in Ap- pendix C.5. Example C.6 shows that deceptiveness and overjustifying behavior even applies to aspects of the trajec- tory the policy has no control over: The policy tries to “hide bad luck” and “reveal good luck at a cost”. Example C.7, especially (a) and (c), shows that the policies coming out of a naive application of RLHF under partial observability may be suboptimal with positive E − (and zero E +) or optimal, but with positive E + (and zero E −). Thus, there can be suboptimality even if the policy is better than it seems, and an overestimation error even when the policy is optimal. 5. Toward addressing partial observability by modeling human beliefs We’ve seen issues with standard RLHF when applied to feedback from partial observations. Part of the problem is model misspecification: the standard RLHF model implicitly assumes full observability. Assuming access to the human and observation models, what happens if the AI accounts for them when learning from human feedback? In this section, we show that correctly modeling the human can sometimes resolve the issues from the previous section, letting us find an optimal policy. In other cases, however, a new ambiguity problem arises: feedback from partial observations is not always enough to uniquely determine the optimal policy. We characterize the extent of this ambiguity in Theorem 5.1. 5.1. Ambiguity and identifiability of return functions Recall that in the fully observable setting, Boltzmann ratio- nal comparisons let us infer the return function G (up to an additive constant), see Proposition 3.1. With an analogous argument, we can show that the expected return function B ·G can be inferred using feedback from partial observa- tions that follows our model in Eq. (2). This gives us an immediate first result: if the matrix B is known and injective, i.e., has linearly independent columns, we can recover the return function G. More generally, we can recover G up to an element of ker B, the set of all return functions mapped to zero by B. However, injectivity of B is an unreasonably strong condition. At best, it can hold when there are just as many different observation sequences as state sequences, which typically won’t hold in realistic environments. Interestingly, we can sometimes infer G even when B is not injective if we assume that G is induced by a reward function R. Intuitively, inferring G naively—without taking into account that it is induced by R—means learning a return value for each possible trajectory ⃗s. But the “actual” degrees of freedom are only one reward value for each state s, so there are many more functions ⃗S → R than realizable return functions. Only non-injectivities “coming from” realizable return functions lead to ambiguity. 7 Challenges with Partial Observability of Human Evaluators in Reward Learning Figure 3. By Theorem 5.1, even with infinite comparison data, the reward learning system (depicted as a robot) can only infer G up to the ambiguity im Γ ∩ ker B (purple). Adding an element of the ambiguity to G leads to the exact same choice probabilities for all possible comparisons, and the reward learning system has no way to identify G among the return functions in G + (im Γ ∩ ker B) (yellow). This abstract depiction ignores the linearity of these spaces; for a more precise geometric depiction of B, see Figure 4 in the appendix. To formalize this idea, we write Γ ∈ R ⃗S×S for the matrix that maps any reward function to the corresponding return function, i.e., (Γ ·R)(⃗s) := PT t=0 γtR(st). Explicitly, its matrix elements are given by Γ⃗ss = PT t=0 δs(st)γt, where δs(st) = 1 if s = st and δs(st) = 0, else. Then the image im Γ is the set of all return functions that can be realized from a reward function. Taking into account that G itself is in im Γ, we can improve ambiguity from ker B to ker B ∩ im Γ: Theorem 5.1. The collection of choice probabilities \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω following a Boltzmann rational model as in Eq. (2) determines the return function G up to addition of an element in ker B ∩ im Γ and a constant. In particular, the choice probabilities determine G up to an additive constant if and only if ker B ∩ im Γ = {0}. See Theorem B.2 and Corollary B.4 for full proofs, and Figure 3 for a visual depiction. This result motivates the following definition: Definition 5.2 (Ambiguity). We call ker B ∩ im Γ the ambi- guity that is left in the return function when the observation- based choice probabilities are known. Note that Theorem 5.1 generalizes the fully observed case from Section 3.2 (Corollary B.10). We also extend the theorem in Appendix B.4 to the case when the human’s observations are not known to the learning system. Special cases of ker B and im Γ and our theorem can be found in Appendices B.7 and B.5. In particular, if P ⃗O is possibly non-deterministic and there is only “noise” in it (defined as ⃗Ω = ⃗S and the injectivity of O) and if the human is a Bayesian reasoner with a fully supported prior over ⃗S, then the return function is identifiable from the choice data even if the human’s observations are not known to the learning system, see also Example B.30. For Theorem 5.1, we assumed knowledge of the human belief matrix B, which realistically would at best be known approximately. But in Appendix B.6, we show that small errors in the belief matrix used for inference lead to only small errors in the inferred return function: Theorem 5.3. Assume ker B ∩ im Γ = {0}. Let B∆ := B + ∆ be a small perturbation of B, where ∥ ∆ ∥ ≤ ρ for sufficiently small ρ. Let G be the true return function and assume that the learning system, assuming the human’s belief is B∆, infers the return function ˜G with the property that B∆ · ˜G has the smallest possible Euclidean distance to B ·G. Let r(B) := B |im Γ be the (injective) restriction of the operator B to im Γ. Then r(B)T r(B) is invertible, and there exists a polynomial Q(X, Y ) of degree 5 such that ∥ ˜G − G∥ ≤ ρ · ∥G∥ · Q \u0010\r\r\u0000r(B)T r(B) \u0001−1\r\r, ∥r(B)∥ \u0011 . In particular, as we show in the appendix, one can uniformly bound the difference between J ˜ G and JG. This yields a regret bound between the policy optimal under ˜G and π∗. 5.2. Improvement over naive RLHF We saw in Example 4.9 a case where naive RLHF under 8 Challenges with Partial Observability of Human Evaluators in Reward Learning partial observability can lead to a suboptimal policy that uses deception or overjustification. Appropriately modeling partial observability can avoid this problem since in this case, ker B ∩ im Γ = {0}. The reason is that ker B leaves only one degree of freedom that is not “time-separable” over states. To show this in detail, let G′ = Γ(R′) ∈ ker B ∩ im Γ. We need to show G′ = 0. Since the human is only uncertain about the state sequences corresponding to the observation sequence o∅oIo∅o∅, the condition B ·G′ = 0 already im- plies G′(⃗s) = 0 for all state sequences except SIWT and SITT. From (B ·G′)(o∅oIo∅o∅) = 0, one then obtains the equation (1 − pD) · \u0000R′(S) + R′(I) + 2R′(T) \u0001 +pD · \u0000R′(S) + R′(I) + R′(W) + R′(T) \u0001 = 0. (5) Thus, if one of the two state sequences involved has zero return, then the other has as well, assuming that 0 ̸= pD ̸= 1, and we are done. To show this, we use that all other state sequences have zero return: R′(S) + 3R′(T) = 0 = R′(S) + R′(L) + 2R′(T), from which R′(L) = R′(T) follows. Then, from R′(S) + R′(I)+R′(L)+R′(T) = 0, substituting the previous result gives R′(S) + R′(I) + 2R′(T) = 0, and so Equation (5) results in R′(S) + R′(I) + R′(W) + R′(T) = 0. Overall, this shows G′ = Γ(R′) = 0, and so ker B ∩ im Γ = {0}. 5.3. Return ambiguity can remain In Example 4.8, ambiguity that can lead to problematic reward inferences remains even when accounting for partial observability. Intuitively, since W and WH receive the same observation, the human choice probabilities don’t determine the values of R(W) and R(WH)—they only determine their average over the human’s belief when observing oW . Thus, the reward learning system can infer arbitrarily high values for R(WH) when proportionally decreasing the value for R(W). This can then lead to an incentive to hide the error messages, and thus suboptimality. Concretely, by Theorem 5.1, the ambiguity in the return function leaving the choice probabilities invariant is given by ker B ∩ im Γ. Let R′ = (0, 0, R′(W), 0, R′(WH), 0, 0) ∈ R{S,I,W,L,WH,LH,T } be a reward function that we want to parameterize such that G′ := Γ ·R′ ends up in the ambigu- ity; here, R′ is interpreted as a column vector. We want B ·G′ = 0. Since the observation sequences ⃗o = o∅o∅o∅o∅, ⃗o = o∅oLo∅o∅, ⃗o = o∅oIo∅o∅, or ⃗o = o∅oIoLo∅ all cannot involve the states W or WH, it is clear that they have zero expected return (B ·G′)(⃗o). Set p′ H := B \u0000SIWHT | o∅oIoW o∅ \u0001 . Then the condition that B ·G′ = 0 is equivalent to: 0 = \u0000B ·G′\u0001 (o∅oIoW o∅) = E ⃗s∼B(⃗s|o∅oIoW o∅) \u0002 G′(⃗s) \u0003 = p′ H · G′(SIWHT) + (1 − p′ H) · G′(SIWT) = p′ H · R′(WH) + (1 − p′ H) · R′(W). Thus, if R′(W) = p′ H p′ H−1R′(WH), then G′ ∈ ker B ∩ im Γ, meaning that R + R′ has the same choice probabilities as R and can be inferred by the reward learning algorithm. In particular, if R′(WH) ≫ 0 is sufficiently large, then in subsequent policy optimization, there is an incentive to hide the mistakes and πH will be selected, which is suboptimal with respect to the true reward function R. In Example B.29, we show a case where some return func- tions within the ambiguity of the true return function can be even worse than simply maximizing Jobs. This generally raises the question of how to tie-break return functions in the ambiguity, or how to act conservatively given the un- certainty, in order to consistently improve upon the setting in Section 4.1. 6. Conclusions and future work In this paper, we provided a conceptual and theoretical in- vestigation of challenges when applying RLHF from partial observations. First, we saw that applying RLHF naively when assuming full observability can lead to deceptive in- flating and overjustification behavior. Then, we showed that these problems can sometimes be mitigated by making the learning algorithm aware of the human’s partial observ- ability and belief model. This method, however, can fail when there is too much remaining ambiguity in the return function. We thus recommend caution when using RLHF in situations of partial observability, and to study the effects of this in practice. We recommend further research to study and improve RLHF in cases where partial observability is unavoidable: Full failure taxonomy. We showed that naively applying RLHF in situations of partial observability can lead to de- ceptive inflation, overjustification, or both. Future research could qualitatively investigate policies that show both decep- tive and overjustified behavior or look at state sequences ⃗s that decompose into parts that are deceptive or overjustified. Finally, it would be desirable to learn which other problems may exist that are not covered by our taxonomy and can occur alongside deception and overjustification. Human model sensitivity and generalizations. In both our analysis of a naive application of RLHF (Section 4) and accounting for partial observability (Section 5), we assumed that the actual human evaluators are Boltzmann rational as in Equation (2). As argued in Example C.4, naive 9 Challenges with Partial Observability of Human Evaluators in Reward Learning RLHF sometimes fails regardless of the human model. In general, it would be desirable to investigate both settings with more general human models, and to learn how the results generalize to RLHF-variants like DPO (Rafailov et al., 2023), other variants of reward learning (Jeon et al., 2020), and assistance games (Hadfield-Menell et al., 2016). Correct belief specification. When our model of human choice probabilities from (2) is sufficiently correct and we want to apply Theorem 5.1 in practice, as outlined in Ap- pendix B.3, then we need to specify the human belief model B(⃗s | ⃗o); how to do this is an open question. If one as- sumes the human is rational as in Appendix B.1, then this requires specifying the human’s policy prior B(π), which is also open. Whether it is possible to meaningfully learn a generative model for B(⃗s | ⃗o) remains an open question. Understanding the ambiguity. Once B(⃗s | ⃗o) is known, the reward inference may still have undesired outcomes unless the ambiguity ker B ∩ im Γ is sufficiently small. A general characterization of this ambiguity going beyond Appendix B.7 would be desirable. This would require un- derstanding im Γ, the set of return functions that arise from a reward function over states. When the ambiguity is too large, as in Section 5.3, then learning a suitable reward function requires further inductive biases. Increasing the effective observability. Next to increas- ing the observability of the human directly, it would help if the human could query the policy about reward-relevant aspects of the environment to bring the setting closer to RLHF from full observations. This is similar to the problem of eliciting the latent knowledge of a predictor of future ob- servations (Christiano et al., 2021; Christiano & Xu, 2022). While this may avoid the need to specify the human’s belief model B(⃗s | ⃗o), it requires understanding and effectively querying an ML model’s belief, including translating from an ML model’s ontology into a human ontology. Impact statement RLHF and its variants are widely used to steer the behavior of language models. Thus, the soundness of RLHF is critical to language models’ trustworthy deployment. Our work shows that partial observability of humans poses theoretical challenge for RLHF. We hope our work stimulates further research in overcoming this challenge. Author contributions The project was conceived in parallel by Scott and Davis, with a key shift proposed by Leon. Leon proved Proposi- tion 4.1 and Theorems 5.1 and 5.3, found the first mathemat- ical examples of what became deceptive inflation and over- justification that can be resolved by Theorem 5.1, and wrote the majority of the appendix. Davis conjectured Propo- sition 4.1, provided early empirical evidence that RLHF under partial observations can lead to deception (not in the paper), defined deception / deceptive inflation and overjusti- fication (with Scott), proved Theorem 4.5, and developed the running examples and figures. Scott guided the project direction and prioritization, gave the conjecture and proof idea for Theorem 5.3, and helped develop the running ex- amples and deception definitions. Erik provided regular detailed feedback and guidance and edited the paper. Anca and Stuart advised this project. Acknowledgements Leon Lang thanks the Center for Human-Compatible Artifi- cial Intelligence for hosting him during part of this project, and Open Philanthropy for financial support. We thank Ben- jamin Eysenbach and Benjamin Plaut for detailed comments and feedback on this work, and we thank Elio A. Farina, Mary Marinou, and Alexandra Horn for assistance with graphic design. References Amodei, D., Christiano, P., and Ray, A. Learning from human preferences. https://openai.com/res earch/learning-from-human-preferences, 2017. Accessed: 2023-12-13. Anthropic. Introducing Claude. https://www.anthro pic.com/index/introducing-claude, 2023a. Accessed: 2023-09-05. Anthropic. Claude’s Constitution. https://www.anth ropic.com/index/claudes-constitution, 2023b. Accessed: 2023-09-05. Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McK- innon, C., Chen, C., Olsson, C., Olah, C., Hernandez, D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E., Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J., Ndousse, K., Lukosuite, K., Lovitt, L., Sellitto, M., Elhage, N., Schiefer, N., Mercado, N., DasSarma, N., Lasenby, R., Larson, R., Ringer, S., Johnston, S., Kravec, S., El Showk, S., Fort, S., Lanham, T., Telleen-Lawton, T., Conerly, T., Henighan, T., Hume, T., Bowman, S. R., Hatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N., McCandlish, S., Brown, T., and Kaplan, J. Consti- tutional AI: Harmlessness from AI Feedback. arXiv e-prints, art. arXiv:2212.08073, December 2022. doi: 10.48550/arXiv.2212.08073. Bradley, R. A. and Terry, M. E. Rank Analysis of Incomplete Block Designs: I. The Method of Paired Comparisons. 10 Challenges with Partial Observability of Human Evaluators in Reward Learning Biometrika, 39(3/4):324–345, 1952. ISSN 00063444. URL http://www.jstor.org/stable/23340 29. Buehler, R., Griffin, D., and Ross, M. Exploring the ”Plan- ning Fallacy”: Why People Underestimate Their Task Completion Times. Journal of Personality and Social Psychology, 67:366–381, 09 1994. doi: 10.1037/0022-3 514.67.3.366. Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discov- ering Latent Knowledge in Language Models Without Supervision. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=ETKGuby0hcs. Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer, J., Rando, J., Freedman, R., Korbak, T., Lindner, D., Freire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll, M., Peng, A., Christoffersen, P., Damani, M., Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud, E. J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco, L., Hase, P., Bıyık, E., Dragan, A., Krueger, D., Sadigh, D., and Hadfield-Menell, D. Open Problems and Fun- damental Limitations of Reinforcement Learning from Human Feedback. arxiv e-prints, 2023. Christiano, P. and Xu, M. ELK prize results. https://ww w.alignmentforum.org/posts/zjMKpSB2X ccn9qi5t/elk-prize-results, 2022. Accessed: 2024-02-15. Christiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S., and Amodei, D. Deep Reinforcement Learning from Hu- man Preferences. arXiv e-prints, art. arXiv:1706.03741, June 2017. doi: 10.48550/arXiv.1706.03741. Christiano, P., Cotra, A., and Xu, M. Eliciting Latent Knowl- edge. https://docs.google.com/document /d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1 dwZXR37PC8/edit, 2021. Accessed: 2023-04-25. Deci, E. L. and Flaste, R. Why we do what we do: The dy- namics of personal autonomy. GP Putnam’s Sons, 1995. El Ghaoui, L. Inversion error, condition number, and approx- imate inverses of uncertain matrices. Linear Algebra and its Applications, 343-344:171–193, 2002. ISSN 0024- 3795. doi: https://doi.org/10.1016/S0024-3795(01)002 73-7. URL https://www.sciencedirect.com/ science/article/pii/S0024379501002737. Special Issue on Structured and Infinite Systems of Linear equations. Evans, O., Stuhlmueller, A., and Goodman, N. D. Learning the Preferences of Ignorant, Inconsistent Agents. arxiv e-prints, 2015. Evans, O., Stuhlm¨uller, A., and Goodman, N. Learning the preferences of ignorant, inconsistent agents. In Proceed- ings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. Evans, O., Cotton-Barratt, O., Finnveden, L., Bales, A., Bal- wit, A., Wills, P., Righetti, L., and Saunders, W. Truthful AI: Developing and Governing AI that does not lie. arxiv e-prints, 2021. Fern, A., Natarajan, S., Judah, K., and Tadepalli, P. A Decision-Theoretic Model of Assistance. J. Artif. Int. Res., 50(1):71–104, may 2014. ISSN 1076-9757. Geiger, D., Verma, T., and Pearl, J. Identifying indepen- dence in bayesian networks. Networks, 20:507–534, 1990. URL https://api.semanticscholar.org/ CorpusID:1938713. Gemini Team, G. Gemini: A Family of Highly Capable Multimodal Models. https://storage.google apis.com/deepmind-media/gemini/gemini _1_report.pdf, 2023. Accessed: 2023-12-11. Hadfield-Menell, D., Dragan, A., Abbeel, P., and Russell, S. Cooperative Inverse Reinforcement Learning. arXiv e-prints, art. arXiv:1606.03137, June 2016. doi: 10.485 50/arXiv.1606.03137. Hofst¨atter, F., Ward, F. R., HarrietW, Thomson, L., J, O., Bartak, P., and Brown, S. F. Tall Tales at Different Scales: Evaluating Scaling Trends for Deception in Language Models. https://www.alignmentforum.org /posts/pip63HtEAxHGfSEGk/tall-tales-a t-different-scales-evaluating-scaling -trends-for, 2023. Accessed: 2024-01-23. Huang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., et al. A Survey on Hallucination in Large Language Models: Principles, Tax- onomy, Challenges, and Open Questions. arXiv preprint arXiv:2311.05232, 2023. Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and Garrabrant, S. Risks from Learned Optimization in Ad- vanced Machine Learning Systems. arXiv e-prints, art. arXiv:1906.01820, June 2019. doi: 10.48550/arXiv.1906. 01820. Hubinger, E., Schiefer, N., Denison, C., and Perez, E. Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research. https://www.alignmen tforum.org/posts/ChDH335ckdvpxXaXX/m odel-organisms-of-misalignment-the-c ase-for-a-new-pillar-of-1, 2023. Accessed: 2024-01-23. 11 Challenges with Partial Observability of Human Evaluators in Reward Learning Hubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M., MacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell, T., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan, A., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark, J., Ndousse, K., Sachan, K., Sellitto, M., Sharma, M., DasSarma, N., Grosse, R., Kravec, S., Bai, Y., Witten, Z., Favaro, M., Brauner, J., Karnofsky, H., Christiano, P., Bowman, S. R., Graham, L., Kaplan, J., Mindermann, S., Greenblatt, R., Shlegeris, B., Schiefer, N., and Perez, E. Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. arxiv e-prints, 2024. Jeon, H. J., Milli, S., and Dragan, A. Reward-rational (implicit) choice: A unifying formalism for reward learn- ing. In Larochelle, H., Ranzato, M., Hadsell, R., Bal- can, M., and Lin, H. (eds.), Advances in Neural In- formation Processing Systems, volume 33, pp. 4415– 4426. Curran Associates, Inc., 2020. URL https: //proceedings.neurips.cc/paper_files /paper/2020/file/2f10c1578a0706e06b6 d7db6f0b4a6af-Paper.pdf. Lin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring How Models Mimic Human Falsehoods. arxiv e-prints, 2022. Majumdar, A., Singh, S., Mandlekar, A., and Pavone, M. Risk-sensitive inverse reinforcement learning via coher- ent risk models. In Amato, N., Srinivasa, S., Ayanian, N., and Kuindersma, S. (eds.), Robotics, Robotics: Science and Systems, United States, 2017. MIT Press Journals. doi: 10.15607/rss.2017.xiii.069. Manyika, J. An overview of Bard: an early experiment with generative AI. https://ai.google/static /documents/google-about-bard.pdf, 2023. Accessed: 2023-09-05. Mindermann, S. and Armstrong, S. Occam’s Razor is In- sufficient to Infer the Preferences of Irrational Agents. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS’18, pp. 5603–5614, Red Hook, NY, USA, 2018. Curran Asso- ciates Inc. Ng, A. Y., Russell, S., et al. Algorithms for Inverse Rein- forcement Learning. In ICML, volume 1, pp. 2, 2000. OpenAI. Introducing ChatGPT. https://openai.c om/blog/chatgpt, 2022. Accessed: 2024-02-06. Park, P. S., Goldstein, S., O’Gara, A., Chen, M., and Hendrycks, D. Ai deception: A survey of examples, risks, and potential solutions. arXiv preprint arXiv:2308.14752, 2023. Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. Direct Preference Optimization: Your Language Model is Secretly a Reward Model. arxiv e- prints, 2023. Reddy, S., Levine, S., and Dragan, A. D. Assisted Per- ception: Optimizing Observations to Communicate State. arxiv e-prints, 2020. Scheurer, J., Balesni, M., and Hobbhahn, M. Technical Report: Large Language Models can Strategically De- ceive their Users when Put Under Pressure. arxiv e-prints, 2023. Shah, R., Krasheninnikov, D., Alexander, J., Abbeel, P., and Dragan, A. The Implicit Preference Information in an Initial State. In International Conference on Learning Representations, 2019. URL https://openreview .net/forum?id=rkevMnRqYQ. Shah, R., Freire, P., Alex, N., Freedman, R., Krasheninnikov, D., Chan, L., Dennis, M. D., Abbeel, P., Dragan, A., and Russell, S. Benefits of Assistance over Reward Learning, 2021. URL https://openreview.net/forum ?id=DFIoGDZejIB. Siththaranjan, A., Laidlaw, C., and Hadfield-Menell, D. Distributional Preference Learning: Understanding and Accounting for Hidden Context in RLHF. arXiv preprint arXiv:2312.08358, 2023. Skalse, J. and Abate, A. Misspecification in In- verse Reinforcement Learning. arXiv e-prints, art. arXiv:2212.03201, December 2022. doi: 10.48550/a rXiv.2212.03201. Skalse, J. M. V., Farrugia-Roberts, M., Russell, S., Abate, A., and Gleave, A. Invariance in Policy Optimisation and Partial Identifiability in Reward Learning. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Proceedings of the 40th Interna- tional Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pp. 32033– 32058. PMLR, 23–29 Jul 2023. URL https://proc eedings.mlr.press/v202/skalse23a.html. Stray, J. The AI Learns to Lie to Please You: Preventing Biased Feedback Loops in Machine-Assisted Intelligence Analysis. Analytics, 2(2):350–358, 2023. ISSN 2813- 2203. doi: 10.3390/analytics2020020. URL https: //www.mdpi.com/2813-2203/2/2/20. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, 12 Challenges with Partial Observability of Human Evaluators in Reward Learning A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open Foundation and Fine- Tuned Chat Models. arxiv e-prints, 2023. Ward, F. R., Belardinelli, F., Toni, F., and Everitt, T. Honesty Is the Best Policy: Defining and Mitigating AI Deception. arxiv e-prints, 2023. Zhuang, S. and Hadfield-Menell, D. Consequences of Mis- aligned AI. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Ziebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K. Maximum entropy inverse reinforcement learning. In Fox, D. and Gomes, C. P. (eds.), AAAI, pp. 1433–1438. AAAI Press, 2008. ISBN 978-1-57735-368-3. URL http://dblp.uni-trier.de/db/conf/aaai /aaai2008.html#ZiebartMBD08. 13 Challenges with Partial Observability of Human Evaluators in Reward Learning APPENDIX In the appendix, we provide more extensive theory, proofs, and examples. The appendix makes free use of concepts and notation defined in the main paper. In particular, throughout we assume a general MDP together with observation kernel PO : S → Ω and a human with general belief kernel B(⃗o | ⃗s), unless otherwise stated. See the list of Symbols in Section A to refresh notation. In Section B, we provide an extensive theory for appropriately modeled partial observability in RLHF. This can mainly be considered a supplement to Section 5 and contains our main theorems, supplementary results, analysis of special cases, and examples. In Section C, we analyze the naive application of RLHF under partial observability, which means that the learning system is not aware of the human’s partial observability. This section is essentially a supplement to Section 4 and contains an analysis of the policy evaluation function Jobs, of deceptive inflation and overjustification, and further extensive mathematical examples showing the failures of naive RLHF under partial observability. Contents of the Appendix A List of Symbols 14 B Modeling the Human in Partially Observable RLHF 16 B.1 The Belief over the State Sequence for Rational Humans . . . . . . . . . . . . . . . . . . . . . . . . . . 17 B.2 Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons 18 B.3 The Ambiguity in Reward Learning in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 B.4 Identifiability of Return Functions When Human Observations Are Not Known . . . . . . . . . . . . . . 22 B.5 Simple Special Cases: Full Observability, Deterministic P ⃗O, and Noisy P ⃗O . . . . . . . . . . . . . . . . 25 B.6 Robustness of Return Function Identifiability under Belief Misspecification . . . . . . . . . . . . . . . . 26 B.6.1 Some Norm Theory for Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 B.6.2 Application to Bounds in the Error of the Return Function . . . . . . . . . . . . . . . . . . . . . 29 B.7 Preliminary Characterizations of the Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.8 Examples Supplementing Section 5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C Issues of Naively Applying RLHF under Partial Observability 35 C.1 Optimal Policies under RLHF with Deterministic Partial Observations Maximize Jobs . . . . . . . . . . . 36 C.2 Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then Jobs = J . . . . . . . . . 37 C.3 Proof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.4 Derivations and Further Details for Section 4.3 Example A . . . . . . . . . . . . . . . . . . . . . . . . . 39 C.5 Further Examples Supplementing Section 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 A. List of Symbols General MDPs S Set of environment states s ∈ S A Set of actions a ∈ A of the policy ∆(S) Set of probability distributions over S. Can be defined for any finite set T : S × A → ∆(S) Transition kernel 14 Challenges with Partial Observability of Human Evaluators in Reward Learning P0 ∈ ∆(S) Initial state distribution R ∈ RS Usually the true reward function R′ ∈ RS Usually a reward function in the kernel of B ◦ Γ ˜R ∈ RS Usually another reward function, e.g. inferred by a learning system γ ∈ [0, 1] Discount factor π : S → ∆(A) A policy T π : S → ∆(S) Transition kernel for a fixed policy π given by T π(s′ | s) = P a∈A T (s′ | s, a) · π(a | s) T ∈ N Finite time horizon P π ∈ ∆(ST ) State sequence distribution induced by the policy π ⃗S ⊆ ST State sequences ⃗s ∈ ⃗S supported by P π G ∈ R ⃗S Usually the true return function given by G(⃗s) = PT t=0 γtR(st). G′ ∈ R ⃗S Usually a return function in ker B ˜G ∈ R ⃗S Usually another return function, e.g. inferred by a learning system J The true policy evaluation function given by J(π) = E⃗s∼P π \u0002 G(⃗s) \u0003 . Additions to General MDPs with Partial Observability Ω Set of possible observations o ∈ Ω PO : S → ∆(Ω) Observation kernel determining the human’s observations P ⃗O : ⃗S → ∆ \u0000ΩT \u0001 The observation sequence kernel given by P ⃗O \u0000⃗o | ⃗s \u0001 = QT t=0 PO \u0000ot | st \u0001 ⃗Ω ⊆ ΩT The set of observed sequences ⃗o ∈ ΩT that can be sampled from P ⃗O(· | ⃗s) for ⃗s ∈ ⃗S O : S → Ω Observation function for the case that PO is deterministic; given by O(s) = o with o such that PO(o | s) = 1 ⃗O : ⃗S → ⃗Ω Observation sequence function for the case that P ⃗O is deterministic; given by ⃗O(⃗s) = ⃗o with ⃗o such that P ⃗O(⃗o | ⃗s) = 1 G⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o} Restriction of the return function G ∈ R ⃗S to \b ⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o \t for fixed ⃗o ∈ ⃗Ω Gobs ∈ R ⃗S Return function that can be inferred when partial observability is not properly modeled, given by Gobs(⃗s) := \u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 Jobs Observation policy evaluation function, defined in Eq. (4) State- and Observation Sequences st ∈ S The t’th entry in a state sequence ⃗s ⃗s ∈ ST State sequence ⃗s = s0, . . . , sT ˆs ∈ St State sequence segment ˆs = s0, . . . , st for t ≤ T ot ∈ Ω The t’th entry in an observation sequence ⃗o ⃗o ∈ ΩT Observation sequence ⃗o = o0, . . . , oT ˆo ∈ Ωt Observation sequence segment ˆo = o0, . . . , ot for t ≤ T The Human’s Belief B(π′) The human’s policy prior B(⃗s) The human’s prior belief that a sequence ⃗s will be sampled, given by B(⃗s) = R π′ B(π′)P π′(⃗s)dπ′ B \u0000⃗s | ⃗o \u0001 The human’s belief of a state sequence given an observation sequence, see Proposition B.1 for a Bayesian version 15 Challenges with Partial Observability of Human Evaluators in Reward Learning Bπ(⃗s | ⃗o) The human’s belief of a state sequence given an observation sequence; it is allowed to depend on the true policy π, see Proposition B.1 B⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o} Vector of prior probabilities B(⃗s) for ⃗s ∈ \b ⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o \t Identifiability Theorem β > 0 The inverse temperature parameter of the Boltzmann rational human σ : R → (0, 1) The sigmoid function given by σ(x) = 1 1+exp(−x) Γ : RS → R ⃗S Function that maps a reward function R to the return function Γ(R) with \u0002 Γ(R) \u0003 (⃗s) = PT t=0 γtR(st) B : R ⃗S → R⃗Ω Function that maps a return function G to the expected return function B(G) on observation sequences given by \u0002 B(G) \u0003 (⃗o) = E⃗s∼B(⃗s|⃗o) \u0002 G(⃗s) \u0003 F : RS → R⃗Ω The composition F = B ◦ Γ P R\u0000⃗s ≻ ⃗s′\u0001 Boltzmann rational choice probability in the case of full observability (Eq. (1)) P R\u0000⃗o ≻ ⃗o′\u0001 Boltzmann rational choice probability in the case of partial observability (Eq. (2)) O : R⃗Ω → R ⃗S Abstract linear operator given by \u0002 O(v) \u0003 (⃗s) = E⃗o∼P ⃗ O(⃗o|⃗s) \u0002 v(⃗o) \u0003 O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S Formally the Kronecker product of O with itself, explicitly given by \u0002 (O ⊗ O)(C) \u0003 (⃗s,⃗s′) = E⃗o,⃗o ′∼P ⃗ O(·|⃗s,⃗s ′) \u0002 C(⃗o,⃗o′) \u0003 Robustness to Misspecifications ∥x∥ Euclidean norm of the vector x ∈ Rk ∥ A ∥ Matrix norm of the matrix A, given by ∥ A ∥ := maxx, ∥x∥=1 ∥ A x∥ τ(A) Matrix quantity defined in Equation (9) C(A, ρ) Matrix quantity defined in Equation (10) r(B) Restriction of B to im Γ General Sets and (Linear) Functions |A| Number of elements in the set A A ∩ C Intersection of sets A and C A ∪ C Union of sets A and C A \\ C Relative complement of C in A δx The Dirac delta distribution of a point x in a set; given by δx(A) = 1 if x ∈ A and δx(A) = 0, else ker A The kernel of a linear operator A : V → W; given by ker A = \b v ∈ V | A(v) = 0 \t im A The image of a linear operator A : V → W; given by im A = \b w ∈ W | ∃v ∈ V : A(v) = w \t f −1(y) Preimage of y under a function f : X → Y ; given by f −1(y) = \b x ∈ X | f(x) = y \t B. Modeling the Human in Partially Observable RLHF In this appendix, we develop the theory of RLHF with appropriately modeled partial observability, including full proofs of all theorems. In Section B.1, we explain how the human can arrive at the belief B⃗s | ⃗o via Bayesian updates. The main theory and the main paper in general do not depend on this specific form of the human’s belief, but some examples in the appendix do. 16 Challenges with Partial Observability of Human Evaluators in Reward Learning In Section B.2 we then explain our main result: the ambiguity and identifiability of both reward and return functions under observed sequence comparisons. In Section B.3, we then explain that this theorem means that one could in principle design a practical reward learning algorithm that converges on the correct reward function up to the ambiguity characterized in the section before, if the human’s belief kernel B(⃗s | ⃗o) is fully known. In Section B.4, we generalize the theory to the case that the human’s observations are not necessarily known to the learning system and again characterize precisely when the return function is identifiable from sequence comparisons. We then consider special cases in Section B.5, where we show that the fully observable case is covered by our theory, that a deterministic observation kernel P ⃗O usually leads to non-injective belief matrix B, and that “noise” in the observation kernel P ⃗O leads, under appropriate assumptions, to the identifiability of the return function. Our identifiability results require that the learning system knows the human’s belief kernel B(⃗s | ⃗o). In Section B.6, we then show that these results are robust to slight misspecifications: a bound in the error in the specified belief leads to a corresponding bound in the error of the policy evaluation function used for subsequent reinforcement learning. In Section B.7, we then provide a very preliminary characterization of the ambiguity in the return function under special cases. Finally, in Section B.8, we study examples of identifiability and non-identifiability of the return function for the case that we do model the human’s partial observability correctly. This reveals qualitatively interesting cases of identifiability, even when B is not injective, and catastrophic cases of non-identifiability. B.1. The Belief over the State Sequence for Rational Humans Before we dive into the main theory, we want to explain how the human can iteratively compute the posterior of the state sequence given an observation sequence with successively new observations. This is done by defining a Bayesian network for the joint probability of policy, states, actions, and observations, and doing Bayesian inference over this Bayesian network. The details of this subsection are only relevant for a few sections in the appendix since it is usually enough to assume that the posterior belief exists. Additionally, in the core theory, we do not even assume that B(⃗s | ⃗o) is a posterior: it is simply any probability distribution. The reason why it can still be interesting to analyze the case when the human is a rational Bayesian reasoner is that one can then analyze RLHF under generous assumptions to the human. We model the human to have a joint distribution B(π,⃗s,⃗a,⃗o) over the policy π, state sequence ⃗s = s0, . . . , sT , action sequence ⃗a = a0, . . . , aT −1, and observation sequence ⃗o = o0, . . . , oT . This is given by a Bayesian network with the following components: • a policy prior B(π′); • the probability of the initial state B(s0) := P0(s0); • action probabilities B(a | s, π) := π(a | s); • transition probabilities B(st+1 | st, at) := T (st+1 | st, at); • and observation probabilities B(ot | st) := PO(ot | st). Together, this defines the joint distribution B(π,⃗s,⃗a,⃗o) over the policy, states, actions, and observations that factorizes according to the following directed acyclic graph: 17 Challenges with Partial Observability of Human Evaluators in Reward Learning π′ s0 a0 s1 a1 s2 a2 s3 . . . o0 o1 o2 o3 (6) The following proposition clarifies the iterative Bayesian update of the human’s posterior over state sequences, given observation sequences: Proposition B.1. Let t ≤ T − 1 and denote by ˆs = s0, . . . , st a state sequence segment of length t ≥ 0. Similarly, ˆo = o0, . . . , ot denotes an observation sequence segment. We have B(ˆs, st+1, π | ˆo, ot+1) ∝ PO(ot+1 | st+1) · \" X at∈A T (st+1 | ˆst, at) · π(at | st) # · B(ˆs, π | ˆo). Thus, the human can iteratively compute B(ˆs, π | ˆo) from the prior B(s0, π) = P0(s0) · B(π′) using the above Bayesian update. The posterior over the state sequence can subsequently be computed by B(ˆs | ˆo) = Z π B(ˆs, π | ˆo). Proof. The proof is essentially just Bayes rule applied to the Bayesian network in Equation (6). We repeatedly make use of conditional independences that follow from d-separations in the graph (Geiger et al., 1990). More concretely, we have B \u0000ˆs, st+1, π | ˆo, ot+1 \u0001 ∝ B \u0000ot+1 | ˆs, st+1, π, ˆo \u0001 · B \u0000ˆs, st+1, π | ˆo \u0001 = PO \u0000ot+1 | st+1 \u0001 · B \u0000st+1 | ˆs, π, ˆo) · B(ˆs, π | ˆo \u0001 = PO \u0000ot+1 | st+1 \u0001 · \" X at∈A B \u0000st+1 | at, ˆs, π, ˆo \u0001 · B \u0000at | ˆs, π, ˆo \u0001 # · B \u0000ˆs, π | ˆo \u0001 = PO \u0000ot+1 | st+1 \u0001 · \" X at∈A T \u0000st+1 | st, at \u0001 · π \u0000at | st \u0001 # · B \u0000ˆs, π | ˆo \u0001 . In step 1, we used Bayes rule. In step 2, we made use of the independence ot+1⊥⊥(ˆs, π, ˆo) | st+1, plugged in the observation kernel, and used the chain rule of probability to compose the second term into a product. In step 3, we marginalized and used, once again, the chain rule of probability. In step 4, we used the independences st+1 ⊥⊥ (s0, . . . , st−1, π, ˆo) | (st, a) and at ⊥⊥ (s0, . . . , st−1, ˆo) | (π, st) and plugged in the transition kernel and the policy. The last formula is just a marginalization over the policy. B.2. Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons In this section, we prove the main theorem of this paper: a characterization of the ambiguity that is left in the reward and return function once the human’s Boltzmann-rational choice probabilities are known. We change the formulation slightly by formulating the linear operators “intrinsically” in the spaces they are defined in, instead of using matrix versions. This does 18 Challenges with Partial Observability of Human Evaluators in Reward Learning Figure 4. The linear geometry of ambiguity for a hypothetical example with three state sequences and two observation sequences. G∗ is the true return function, and “G” is used in labeling the axes to refer to some arbitrary return function. This is a more accurate geometric depiction of the middle and right spaces in Figure 3. The subspace im Γ ∩ ker B (purple) is the ambiguity in return functions, meaning that adding an element would not change the human’s expected return function on observations. Thus the set of return functions that the reward learning system can infer is the affine set G + (im Γ ∩ ker B) (yellow). Note that the planes on the left are drawn to be axis-aligned for ease of visualization; this will not be the case for real MDPs. not change the general picture, but is a more natural setting when thinking, e.g., about generalizing the results to infinite state sequences. Thus, we define B : R ⃗S → R⃗Ω as the linear operator given by \u0002 B(G) \u0003 (⃗o) := E ⃗s∼B(⃗s|⃗o) \u0002 G(⃗s) \u0003 . Here, B is the human’s belief, which can either be computed as in the previous subsection or simply be any conditional probability distribution. Similarly, we define Γ : RS → R ⃗S as the linear operator given by \u0002 Γ(R) \u0003 (⃗s) := T X t=0 γtR(st). The matrix product B · Γ then becomes the composition B ◦ Γ : RS → R⃗Ω. Finally, recall that the kernel ker A of a linear operator A is defined as its nullspace, and the image im A as the set of elements hit by A. We obtain the following theorem: Theorem B.2. Let R be the true reward function and ˜R another reward function. Let ˜G = Γ( ˜R) and G = Γ(R) be the corresponding return functions. The following three statements are equivalent: (i) The reward function ˜R gives rise to the same vector of choice probabilities as R, i.e \u0010 P ˜ R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω = \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω. (ii) There is a reward function R′ ∈ ker(B ◦ Γ) and a constant c ∈ R such that ˜R = R + R′ + c. (iii) There is a return function G′ ∈ ker B ∩ im Γ and a constant c′ ∈ R such that ˜G = G + G′ + c′. In other words, the ambiguity that is left in the reward function when its observation-based choice probabilities are known is, up to an additive constant, given by ker(B ◦ Γ); the ambiguity left in the return function is given by ker B ∩ im Γ. 19 Challenges with Partial Observability of Human Evaluators in Reward Learning Proof. Assume (i). To prove (ii), let σ by the sigmoid function given by σ(x) = 1 1+exp(−x). Then by Equation (2), the equality of choice probabilities means the following for all ⃗o,⃗o′ ∈ ⃗Ω: σ \u0010 β · \u0000\u0002 B( ˜G) \u0003 (⃗o) − \u0002 B( ˜G) \u0003 (⃗o′) \u0001\u0011 = σ \u0010 β · \u0000\u0002 B(G) \u0003 (⃗o) − \u0002 B(G) \u0003 (⃗o′) \u0001\u0011 . Since the sigmoid function is injective, this implies \u0002 B( ˜G) \u0003 (⃗o) − \u0002 B( ˜G) \u0003 (⃗o′) = \u0002 B(G) \u0003 (⃗o) − \u0002 B(G) \u0003 (⃗o′). Fixing an arbitrary ⃗o′, this implies that there exists a constant c′ such that for all ⃗o ∈ ⃗Ω, the following holds: \u0002 B( ˜G) \u0003 (⃗o) − \u0002 B(G) \u0003 (⃗o′) − c′ = 0. Noting that B(c′) = c′, this implies ˜G − G − c′ ∈ ker(B). Now, define the constant reward function c := c′ · 1 − γ 1 − γT +1 . We obtain \u0002 Γ(c) \u0003 (⃗s) = T X t=0 γt · c = c′ · 1 − γ 1 − γT +1 · T X t=0 γt = c′. Thus, we have Γ( ˜R − R − c) = ˜G − G − c′ ∈ ker(B), implying R′ := ˜R − R − c ∈ ker(B ◦ Γ). This shows (ii). That (ii) implies (iii) follows by applying Γ to both sides of the equation. Now assume (iii), i.e. ˜G = G + G′ + c′ for a constant c′ ∈ R and a return function G′ ∈ ker(B) ∩ im Γ. This implies B( ˜G) = B(G) + c′. Thus, for all ⃗o,⃗o′ ∈ ⃗Ω, we have \u0002 B( ˜G) \u0003 (⃗o) − \u0002 B( ˜G) \u0003 (⃗o′) = \u0002 B(G) \u0003 (⃗o) − \u0002 B(G) \u0003 (⃗o′), which implies the equal choice probabilities after multiplying with β and applying the sigmoid function σ on both sides. Thus, (iii) implies (i). Corollary B.3. The following two statements are equivalent: (i) ker(B ◦ Γ) = 0. (ii) The data \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω determine the reward function R up to an additive constant. Proof. That (i) implies (ii) follows immediately from the implication from (i) to (ii) within the preceding theorem. Now assume (ii). Let R′ ∈ ker(B ◦ Γ). Define ˜R := R + R′. Then the implication from (ii) to (i) within the preceding theorem implies that ˜R and R have the same choice probabilities. Thus, the assumption (ii) in this corollary implies that R′ is a constant. Since Γ and B map nonzero constants to nonzero constants, the fact that R′ ∈ ker(B ◦ Γ) implies that R′ = 0, showing that ker(B ◦ Γ) = {0}. As mentioned in the main paper, the previous result already leads to the non-identifiability of R whenever Γ is not injective, corresponding to the presence of zero-initial potential shaping (Skalse et al. (2023), Lemma B.3). Thus, we now strengthen the previous result so that it deals with the identifiability of the return function, which is sufficient for the purpose of policy optimization: 20 Challenges with Partial Observability of Human Evaluators in Reward Learning Corollary B.4. Consider the following four statements (which can each be true or false): (i) ker B = {0}. (ii) ker \u0000B ◦ Γ) = {0}. (iii) ker B ∩ im Γ = {0}. (iv) The data \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω determine the return function G = Γ(R) on sequences ⃗s ∈ ⃗S up to a constant independent of ⃗s. Then the following implications, and no other implications, are true: (i) (iii) (iv) (ii) In particular, all of (i), (ii), and (iii) are sufficient conditions for identifying the return function from the choice probabilities. Proof. That (i) implies (iii) is trivial. That (ii) implies (iii) is a simple linear algebra fact: Assume (ii) and that G′ ∈ ker B ∩ im Γ. Then G′ = Γ(R′) for some R′ ∈ RS and 0 = B(G′) = B \u0000Γ(R′) \u0001 = (B ◦ Γ)(R′). By (ii), this implies R′ = 0 and therefore G′ = Γ(R′) = 0, showing (iii). That (iii) implies (iv) immediately follows from the implication from (i) to (iii) in Theorem B.2. Now, assume (iv). To prove (iii), assume G′ ∈ ker B ∩ im Γ. Then the implication from (iii) to (i) in Theorem B.2 implies that G + G′ induces the same observation-based choice probabilities as G. Thus, (iv) implies G + G′ = G + c′ for some constant c′, which implies G′ = c′. Since G′ ∈ ker B, this implies 0 = B(G′) = B(c′) = c′ and thus G′ = 0. Thus, we showed ker B ∩ im Γ = {0}. We now show that no other implication holds in general. Example B.32 will show that (ii) does not imply (i). We now show that (i) does also not imply (ii), from which it will logically follow that (iii) does neither imply (i) nor (ii). Namely, consider the following simple MDP with time horizon T = 1: a b (7) In this MDP, every state sequence starts in a, deterministically transitions to b, and then ends. This means that ⃗s = ab is the only sequence. Now, let R′ ∈ R{a,b} be the reward function given by R′(a) = 1, R′(b) = −1 γ . We obtain \u0002 Γ(R′) \u0003 (⃗s) = R′(a) + γR′(b) = 1 + γ · −1 γ = 0. Thus, Γ(R′) = 0, (B ◦ Γ)(R′) = 0, and, therefore, ker \u0000B ◦ Γ \u0001 ̸= {0}. Thus, (ii) does not hold. However, it is possible to choose B(⃗s | ⃗o) such that (i) holds: e.g., if Ω = S and B(⃗s | ⃗o) := δ⃗o(⃗s), then ker B = {0} since this operator is the identity. 21 Challenges with Partial Observability of Human Evaluators in Reward Learning B.3. The Ambiguity in Reward Learning in Practice In this section, we point out that Theorem B.2 is not just a theoretical discussion: When B and the inverse temperature parameter β are known, then it is possible to design a reward learning algorithm that learns the true reward function up to the ambiguity ker(B ◦ Γ) in the infinite data limit. In doing so, we essentially use the loss function proposed in Christiano et al. (2017). Namely, assume D is a data distribution of observation sequences ⃗o ∈ ⃗Ω such that all sequences in ⃗Ω have a strictly positive probability of being sampled; for example, D could use an exploration policy and the observation sequence kernel P ⃗O. For each pair of observation sequences (⃗o,⃗o′), we then get a conditional distribution P(µ | ⃗o,⃗o′) over a one-hot encoded human choice µ ∈ {(1, 0), (0, 1)}, with probability P \u0000µ = (1, 0) | ⃗o,⃗o′\u0001 = P R\u0000⃗o ≻ ⃗o′\u0001 . Together, this gives rise to a dataset (⃗o1,⃗o′ 1, µ1), . . . , (⃗oN,⃗o′ N, µN) of observation sequences plus a human choice. Now assume we learn a reward function Rθ : S → R that is differentiable in the parameter θ and that can represent all possible reward functions R ∈ RS. Let Gθ := Γ(Rθ) be the corresponding return function. Write µk = (µ(1) k , µ(2) k ). As in Christiano et al. (2017), we define its loss over the dataset above by eL(θ) = − 1 N N X k=1 µ(1) k · log P Rθ\u0000⃗ok ≻ ⃗o′ k \u0001 + µ(2) k · log P Rθ\u0000⃗o′ k ≻ ⃗ok \u0001 . Note that by Equation (2), this loss function essentially uses B and also the inverse temperature parameter β in its definition. This means that these need to be explicitly represented to be able to use the loss function in practice. Proposition B.5. The loss function eL is differentiable. Furthermore, in the infinite datalimit its minima are precisely given by parameters θ such that Rθ = R + R′ + c for R′ ∈ ker \u0000B ◦ Γ \u0001 and c ∈ R, or equivalently Gθ = G + G′ + c′ for G′ ∈ ker B ∩ im Γ and c′ ∈ R. Proof. The differentiability of the loss function follows from the differentiability of multiplication with the matrix B, see Equation (2), and of the reward function Rθ in its parameter θ that we assumed. For the second statement, let N(⃗o,⃗o′) be the number of times that the pair (⃗o,⃗o′) appears in the dataset, and let N(⃗o,⃗o′, 1) be the number of times that the human choice is µ = (1, 0) and the sampled pair is (⃗o,⃗o′), and similar for 2 instead of 1. We obtain eL(θ) = − X ⃗o,⃗o ′∈⃗Ω N(⃗o,⃗o′) N · \" N(⃗o,⃗o′, 1) N(⃗o,⃗o′) log P Rθ\u0000⃗o ≻ ⃗o′\u0001 + N(⃗o,⃗o′, 2) N(⃗o,⃗o′) log P Rθ\u0000⃗o′ ≻ ⃗o \u0001 # ≈ E ⃗o,⃗o ′∼D \u0014 CE \u0010 P R\u0000⃗o ≻≺ ⃗o′\u0001 \r\r P Rθ\u0000⃗o ≻≺ ⃗o′\u0001\u0011\u0015 =:L(θ). Here, CE is the crossentropy between the two binary distributions. Since we assumed that D gives a positive probability to all observation sequences in ⃗Ω, and since the cross entropy is generally minimized exactly when the second distribution equals the first, the loss function L(θ) is minimized if and only if Rθ gives rise to the same choice probabilities as R for all pairs of observation sequences. Theorem B.2 then gives the result. B.4. Identifiability of Return Functions When Human Observations Are Not Known Corollary B.4 assumes that the choice probabilities of each observation sequence pair are known to the reward learning algorithm. However, this requires the algorithm to know what the human observed. In some applications, this is a reasonable assumption, e.g. if the human’s observations are themselves produced by an algorithm that can feed the observations also back to the learning algorithm. In general, however, the observations happen in the physical world, and are only known 22 Challenges with Partial Observability of Human Evaluators in Reward Learning probabilistically via the observation kernel PO. The learning system does however have access to the full state sequences that generate the observation sequences. This leads to knowledge of the following choice probabilities for ⃗s,⃗s′ ∈ ⃗S: P R\u0000⃗s ≻ ⃗s′\u0001 := E ⃗o,⃗o′∼P ⃗ O(·|⃗s,⃗s ′) h P R\u0000⃗o ≻ ⃗o′\u0001i , 2 (8) where the observation-based choice probabilities are given as in Equation (2). In other words, the learning algorithm can only infer an aggregate of the observation-based choice probabilities. Again, we can ask a question similar to the ones before, extending the investigations in the previous section: Question B.6. Assume the vector of choice probabilities \u0010 P R(⃗s ≻ ⃗s′) \u0011 ⃗s,⃗s ′∈ ⃗S is known. Additionally, assume that it is known that the human’s observations are governed by PO, and that the human is Boltzmann rational with inverse temperature parameter β and beliefs B(⃗s | ⃗o), see Equation (8). Does this data identify the return function G : ⃗S → R? If the observation-based choice probabilities from Equation (2) would be known, then Corollary B.4 would provide the answer to this question. Thus, similar to how we previously inverted the belief operator B, we are now simply tasked with inverting the expectation over observation sequences. This leads us to the following definition: Definition B.7 (Ungrounding Operator). The ungrounding operators O : R⃗Ω → R ⃗S and O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S are defined by \u0002 O(v) \u0003 (⃗s) := E ⃗o∼P ⃗ O(⃗o|⃗s) \u0002 v(⃗o) \u0003 , \u0002 (O ⊗ O)(C) \u0003 (⃗s,⃗s′) := E ⃗o,⃗o ′∼P ⃗ O(·|⃗s,⃗s ′) \u0002 C(⃗o,⃗o′) \u0003 . Here, v ∈ R⃗Ω is an arbitrary vector, and C ∈ R⃗Ω×⃗Ω is also an arbitrary vector, where the notation can remind of “Choice” since the inputs to O ⊗ O are, in practice, vectors of observation-based Boltzmann-rational choice probabilities. Formally, O ⊗ O is the Kronecker product of O with itself, but it is not necessary to understand this fact to follow the discussion. Ultimately, to be able to recover the observation-based choice probabilities, what matters is that O ⊗ O is injective on whole vectors of these choice probabilities. The injectivity of O is a sufficient condition for this, which explains its usefulness. We show this in the following lemma: Lemma B.8. O : R⃗Ω → R ⃗S is injective if and only if O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S is injective. Proof. This is a general property of the Kronecker product of a linear operator with itself. For completeness, we demonstrate the calculation in our special case. First, assume that O is injective. Assume that (O ⊗ O)(C) = 0 for some C ∈ R⃗Ω×⃗Ω. We need to show C = 0. For all pairs of state sequences (⃗s,⃗s′), we have 0 = \u0002 (O ⊗ O)(C) \u0003 (⃗s,⃗s′) = E ⃗o,⃗o ′∼P ⃗ O(·|⃗s,⃗s ′) \u0002 C(⃗o,⃗o′) \u0003 = E ⃗o∼P ⃗ O(⃗o|⃗s) \u0014 E ⃗o ′∼P ⃗ O(⃗o ′|⃗s ′) \u0002 C(⃗o,⃗o′) \u0003\u0015 = E ⃗o∼P ⃗ O(⃗o|⃗s) h C′ ⃗s ′(⃗o) i = h O \u0000C′ ⃗s ′ \u0001i (⃗s), where C′ ⃗s ′(⃗o) := E⃗o ′∼P ⃗ O(⃗o ′|⃗s ′) \u0002 C(⃗o,⃗o′) \u0003 . By the injectivity of O, we obtain C′ ⃗s ′ = 0 for all ⃗s′. This means that for all ⃗s′ and ⃗o, we have 0 = C′ ⃗s ′(⃗o) = E ⃗o ′∼P ⃗ O(⃗o ′|⃗s ′) \u0002 C(⃗o,⃗o′) \u0003 = h O \u0000C′′ ⃗o \u0001i (⃗s′), where C′′ ⃗o (⃗o′) := C(⃗o,⃗o′). Again, by the injectivity of O, we obtain C′′ ⃗o = 0 for all ⃗o, leading to C = 0. That proves the direction from left to right. 2We excuse the following abuse of notation: these choice probabilities run through the observations of the human and are not the same as the choice probabilities from Equation (1). 23 Challenges with Partial Observability of Human Evaluators in Reward Learning To prove the other direction, assume that O is not injective. This means there exists 0 ̸= C ∈ R⃗Ω such that O(C) = 0. Define C ⊗ C ∈ R⃗Ω×⃗Ω by (C ⊗ C)(⃗o,⃗o′) := C(⃗o)C(⃗o′). Then clearly, C ⊗ C ̸= 0. We are done if we can show that (O ⊗ O)(C ⊗ C) = 0 since that establishes that O ⊗ O is also not injective. For any ⃗s,⃗s′ ∈ ⃗S, we have h (O ⊗ O)(C ⊗ C) i (⃗s,⃗s′) = E ⃗o,⃗o ′∼P ⃗ O(·|⃗s,⃗s ′) h (C ⊗ C)(⃗o,⃗o′) i = E ⃗o,⃗o ′∼P ⃗ O(·|⃗s,⃗s ′) h C(⃗o) · C(⃗o′) i = E ⃗o∼P ⃗ O(⃗o|⃗s) \u0002 C(⃗o) \u0003 · E ⃗o ′∼P ⃗ O(⃗o ′|⃗s ′) \u0002 C(⃗o′) \u0003 = \u0002 O(C) \u0003 (⃗s) · \u0002 O(C) \u0003 (⃗s′) = 0 · 0 = 0. This finishes the proof. We now state and prove the following extension of Corollary B.4: Theorem B.9. Consider the following statements (which can each be true or false): 1. O : R⃗Ω → R ⃗S is an injective linear operator: ker O = {0}. 2. O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S is an injective linear operator: ker O ⊗ O = {0}. 3. O ⊗ O is injective on vectors of observation-based choice probabilities \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′ over the set of return functions G ∈ R ⃗S. 4. The data of state-based choice probabilities \u0010 P R\u0000⃗s ≻ ⃗s′\u0001\u0011 ⃗s,⃗s ′∈ ⃗S from Equation (8) determine the data of observation- based choice probabilities \u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′∈⃗Ω from Equation (2). Then the following implications hold and 3 does not imply 2: 1 2 3 4. Consequently, if any of the conditions 1, 2, or 3 hold, and additionally any of the conditions (i), (ii) or (iii) from Corollary B.4, then the data \u0010 P R\u0000⃗s ≻ ⃗s′\u0001\u0011 ⃗s,⃗s ′∈⃗Ω determine the return function G on sequences ⃗s ∈ ⃗S up to a constant independent of ⃗s. Proof. That 1 and 2 are equivalent was shown in Lemma B.8. That 2 implies 3 is clear. To prove that 3 implies 4, simply put both sets of choice probabilities into a vector. Then Equation (8) and Definition B.7 show the following equality of vectors in R ⃗S× ⃗S: \u0010 P R\u0000⃗s ≻ ⃗s′\u0001\u0011 ⃗s,⃗s ′ = \u0000O ⊗ O \u0001\u0012\u0010 P R\u0000⃗o ≻ ⃗o′\u0001\u0011 ⃗o,⃗o ′ \u0013 . The injectivity of O ⊗ O on such inputs ensures that the observation-based choice probabilities can be recovered using this equation. We now show that (3) does not imply (2). Again, we use the simple MDP from Equation (7), but this time with a different observation kernel. Namely, we choose PO(o(a) | a) = PO(o(a)′ | a) = 1 2, PO(o(b) | b) = 1, 24 Challenges with Partial Observability of Human Evaluators in Reward Learning where o(a)′ ̸= o(a) and o(a) ̸= o(b) ̸= o(a)′. This results in two possible observation sequences: o(a)o(b) and o(a)′o(b). Thus, R⃗Ω is two-dimensional, whereas R ⃗S is only one-dimensional. Consequently, O : R⃗Ω → R ⃗S cannot be injective, so ker O ̸= {0}, so (2) does not hold since (1) and (2) are equivalent. However, (3) still holds: Since there is only one state sequence, Equation (2) shows that the only vector of choice probabilities has 1/2 in all its entries, irrespective of the return function G. Thus, O ⊗ O has only one input of observation-based choice probabilities, and is thus automatically injective on its inputs. The final result of identifiability of the return function G follows using Corollary B.4. B.5. Simple Special Cases: Full Observability, Deterministic P ⃗O, and Noisy P ⃗O In this section, we analyze three simple special cases of the general theory. Theorem 3.9 (together with Lemma B.3) from Skalse et al. (2023), reproduced as a corollary below, is a special case of our theorem: Corollary B.10 (Skalse et al. (2023)). Assume the human directly observes the true sequences, and the choice probabilities are given by P R\u0000⃗s ≻ ⃗s′\u0001 = σ \u0010 β \u0000G(⃗s) − G(⃗s′) \u0001\u0011 . This data determines the return function G = Γ(R) on state sequences ⃗s ∈ ⃗S up to a constant independent on ⃗s. Proof. We can embed this case into the one of Theorem B.9 by defining the observation kernel as P ⃗O(⃗s′ | ⃗s) = δ⃗s(⃗s′) (i.e., the correct sequence is deterministically observed) and defining the human’s belief as B(⃗s′ | ⃗s) = δ⃗s(⃗s′) (i.e., the human knows that the observation reflects the true sequence). This shows that P(⃗s ≻ ⃗s′) is of the form of Equation (8). The result follows from Theorem B.9: the operators O and B are the identity in this case, due to the defining property of the Kronecker delta, and so they are injective. The following proposition shows that Corollary B.10 is essentially the only example of deterministic observation kernel P ⃗O for which B is injective. Note, however, that in some situations, we can have im Γ ∩ ker B = {0} even if B is not injective, see Example B.32. Proposition B.11. Assume P ⃗O, the observation kernel on the level of sequences, is deterministic and not injective. Then O is automatically injective. However, B is not injective. Proof. To show that O is injective, assume v ∈ R⃗Ω is such that O(v) = 0. Then for all ⃗s ∈ ⃗S, we get 0 = \u0002 O(v) \u0003 (⃗s) = E ⃗o∼P ⃗ O(⃗o|⃗s) \u0002 v(⃗o) \u0003 = v \u0000 ⃗O(⃗s) \u0001 . Since ⃗O : ⃗S → ⃗Ω is by definition surjective, we obtain v = 0. ⃗O : ⃗S → ⃗Ω is by definition surjective, and here assumed to be non-injective, which implies that ⃗S has a higher cardinality than ⃗Ω. Thus, B : R ⃗S → R⃗Ω cannot be injective. In the following, we analyze a simple case that guarantees identifiability. It requires that the observation kernel is “well- behaved” of a form where the observations are simply “noisy states”, and that the human is a Bayesian reasoner with any prior B(⃗s) that supports every state sequence ⃗s ∈ ⃗S. Definition B.12 (Noise in the Observation Kernel). Then we say that there is noise in the observation kernel PO : ⃗S → ∆(⃗Ω) if ⃗S = ⃗Ω and if O is an injective linear operator. Proposition B.13. Assume that ⃗S = ⃗Ω. Furthermore, assume that B(⃗s | ⃗o) is given by the posterior with likelihood P ⃗O(⃗o | ⃗s) and any prior B(⃗s) with B(⃗s) > 0 for all ⃗s ∈ ⃗S. Then there is noise in the observation kernel if and only if B is injective. 25 Challenges with Partial Observability of Human Evaluators in Reward Learning Proof. Assume O is injective. To show that B is injective, assume there is G′ ∈ R ⃗S with B(G′) = 0. Then for all ⃗o ∈ ⃗Ω, we have 0 = \u0002 B(G′) \u0003 (⃗o) = E ⃗s∼B(⃗s|⃗o) \u0002 G′(⃗s) \u0003 = X ⃗s B(⃗s | ⃗o)G′(⃗s) ∝ X ⃗s P ⃗O(⃗o | ⃗s) · \u0000B(⃗s) · G′(⃗s) \u0001 = \u0002 OT (B ⊙ G′) \u0003 (⃗o). Here, OT is the transpose of O and B ⊙ G′ is the componentwise product of the prior B with the return function G′. Since O is injective and thus invertible, OT is as well. Thus, B ⊙ G′ = 0, which implies G′ = 0 since the prior gives positive probability to all state sequences. Thus, B is injective. For the other direction, assume B is injective. To show that O is injective, let v ∈ R⃗Ω be any vector with O(v) = 0. We do a similar computation as above: for all ⃗s ∈ R ⃗S, we have 0 = \u0002 O(v) \u0003 (⃗s) = E ⃗o∼P ⃗ O(⃗o|⃗s) \u0002 v(⃗o) \u0003 = X ⃗o P ⃗O(⃗o | ⃗s)v(⃗o) ∝ X ⃗o B(⃗s | ⃗o) · \u0000P ⃗O(⃗o) · v(⃗o) \u0001 = h BT \u0000P ⃗O ⊙ v \u0001i (⃗s). Here, BT is the transpose of B, P ⃗O(⃗o) is the denominator in Bayes rule, and P ⃗O ⊙ v is the vector with components P ⃗O(⃗o) · v(⃗o). From the injectivity and thus invertibility of B, it follows that BT is invertible as well, and so P ⃗O ⊙ v = 0, which implies v = 0. Thus, O is injective. Corollary B.14. When there is noise in the observation kernel and the human is a Bayesian reasoner with some prior B such that B(⃗s) > 0 for all ⃗s ∈ ⃗S, then the return function is identifiable from choice probabilities of state sequences even if the learning system does not know the human’s observations. Proof. This follows from the injectivity of O, the injectivity of B that we proved in Proposition B.13, and Theorem B.9. Remark B.15. We mention the following caveat: intuitively, one could think that O (and thus B, by Proposition B.13) will be injective if every ⃗s is identifiable from infinitely many i.i.d. samples from P ⃗O(⃗o | ⃗s). A counterexample is the following: O =   1/2 1/4 1/4 1/4 1/2 1/4 3/8 3/8 1/4   . In this case, the rows are linearly dependent with coefficients 1/2, 1/2 and −1. Consequently, O and B are not injective, and so if this observation kernel comes from a multi-armed bandit with three states, then Corollary B.4 shows that the return function is not identifiable. Nevertheless, the distributions P ⃗O(· | ⃗s) (given by the rows) all differ from each other, and so infinitely many i.i.d. samples identify the state sequence ⃗s. B.6. Robustness of Return Function Identifiability under Belief Misspecification We now again look at the case where the observations that the human observes are known to the reward learning system, as in Section B.2. Furthermore, we assume that B : R ⃗S → R⃗Ω is such that ker B ∩ im Γ = {0}. In this case, we can apply Corollary B.4 and identify the true return function G from B(G), which, in turn, can be identified up to an additive constant from the observation-based choice probabilities with the argument as for Proposition 3.1. In this section, we investigate what happens when the human belief model is slightly misspecified. In other words: the learning system uses a perturbed matrix B∆ := B + ∆ with some small perturbation ∆. How much will the inferred return function deviate from the truth? To answer this, we first need to outline some norm theory of linear operators. B.6.1. SOME NORM THEORY FOR LINEAR OPERATORS In this section, let V, W be two finite-dimensional inner product-spaces. In other words, V and W each have inner products ⟨·, ·⟩ and there are linear isomorphisms V ∼= Rk, W ∼= Rm such that the inner products in V and W correspond to the 26 Challenges with Partial Observability of Human Evaluators in Reward Learning standard scalar products in Rk and Rm. The reason that we don’t directly work with Rk and Rm itself is that we will later apply the analysis to the case that V = im Γ ⊆ R ⃗S. Let in this whole section A : V → W be a linear operator and ∆ : V → W be a perturbance, so that A∆ := A + ∆ is a perturbed version of A. The inner products give rise to a norm on V and W defined by ∥v∥ = p ⟨v, v⟩, ∥w∥ = p ⟨w, w⟩. As is well known, for each linear operator A : V → W there exists a unique, basis-independent adjoint (generalizing the notion of a transpose) AT : W → V such that for all v ∈ V and w ∈ W, we have ⟨A v, w⟩ = D v, AT w E . Let us recall the following fact that is often used in linear regression: Lemma B.16. Assume A : V → W is injective. Then AT A : V → V is invertible and (AT A)−1 AT is a left inverse of A. Proof. To show that AT A is invertible, we only need to show that it is injective. Thus, let 0 ̸= x ∈ V . Then D x, AT A x E = ⟨A x, A x⟩ = ∥ A x∥2 > 0, where the last step followed from the injectivity of A. Thus, AT A x ̸= 0, and so AT A is injective, and thus invertible. Consequently, (AT A)−1 AT is a well-defined operator. That it is the left inverse of A is clear. Definition B.17 (Operator Norm). The norm of an operator A : V → W is given by ∥ A ∥ := max x, ∥x∥=1 ∥ A x∥. It has the following well-known properties, where A, B and C are matrices of compatible sizes: ∥ A + B ∥ ≤ ∥ A ∥ + ∥ B ∥, ∥ C A ∥ ≤ ∥ C ∥ · ∥ A ∥, ∥ AT ∥ = ∥ A ∥. To study how a perturbance in A (and thus AT A) transfers into a perturbance of \u0000AT A \u0001−1, we will use the following theorem: Theorem B.18 (El Ghaoui (2002)). Let B : V → V be an invertible operator. Let ρ < ∥ B−1 ∥−1. Let ∆ : V → V be any operator with ∥ ∆ ∥ ≤ ρ. Then B + ∆ is invertible and we have \r\r(B + ∆)−1 − B−1 \r\r ≤ ρ · ∥ B−1 ∥ ∥ B−1 ∥−1 − ρ. Proof. See El Ghaoui (2002), Section 7 and in particular Equation 7.2. Note that the reference defines ∥ A ∥ to be the largest singular value of A; by the well-known min-max theorem, this is equivalent to Definition B.17. We will apply this theorem to AT A, which raises the question about the size of the perturbance in AT A for a given perturbance in A. This is clarified in the following lemma. Before stating it, for a given perturbance ρ, define eρ(A) := ρ · \u00002 · ∥ A ∥ + ρ \u0001 , which depends on A and ρ. Also, recall that for a given perturbance ∆, we define A∆ := A + ∆. We obtain: Lemma B.19. Assume that ∥ ∆ ∥ ≤ ρ. Then ∥ AT ∆ A∆ − AT A ∥ ≤ eρ(A). 27 Challenges with Partial Observability of Human Evaluators in Reward Learning Proof. We have \r\r AT ∆ A∆ − AT A \r\r = \r\r(A + ∆)T (A + ∆) − AT A \r\r = \r\r AT ∆ + ∆T A + ∆T ∆ \r\r ≤ ∥ A ∥ · ∥ ∆ ∥ + ∥ ∆ ∥ · ∥ A ∥ + ∥ ∆ ∥2 ≤ ρ · \u0010 2 · ∥ A ∥ + ρ \u0011 = eρ(A). To be able to apply Theorem B.18 to AT A, we need to make sure that eρ(A) is bounded above by \r\r(AT A \u0001−1∥−1. The next lemma clarifies what condition ρ needs to satisfy for eρ(A) to obey that bound. For this, define τ(A) := −∥ A ∥ + q ∥ A ∥2 + \r\r(AT A)−1\r\r−1, (9) which only depends on A. Lemma B.20. Assume ρ < τ(A). Then eρ(A) < \r\r(AT A)−1\r\r−1. Proof. Note that ρ = τ(A) is the positive solution to the following quadratic equation in the indeterminate ρ: ρ2 + 2 · ∥ A ∥ · ρ − \r\r(AT A)−1\r\r−1 = eρ(A) − \r\r(AT A)−1\r\r−1 = 0. Since this is a convex parabola, we get the inequality eρ(A) − \r\r(AT A)−1\r\r−1 < 0 whenever we have 0 ≤ ρ < τ(A), which shows the result. Finally, we put it all together to obtain a bound on the perturbance of \u0000AT A \u0001−1 AT . For this, set C(A, ρ) := eρ(A) · \r\r\r \u0000AT A \u0001−1\r\r\r \r\r\r \u0000AT A \u0001−1\r\r\r −1 − eρ(A) · \u0010\r\r A \r\r + ρ \u0011 + \r\r\r \u0000AT A \u0001−1\r\r\r · ρ. (10) We obtain: Proposition B.21. Assume ∥ ∆ ∥ ≤ ρ < τ(A). Then AT ∆ A∆ is invertible, and we have \r\r\r \u0000AT ∆ A∆ \u0001−1 AT ∆ − \u0000AT A \u0001−1 AT \r\r\r ≤ C(A, ρ). Proof. The invertibility of AT ∆ A∆ follows from Theorem B.18, Lemma B.19 and Lemma B.20. We get \r\r\r \u0000AT ∆ A∆ \u0001−1 AT ∆ − \u0000AT A \u0001−1 AT \r\r\r = \r\r\r\r h\u0000AT ∆ A∆ \u0001−1 − \u0000AT A \u0001−1i · AT ∆ + \u0000AT A \u0001−1 · \u0000AT ∆ − AT \u0001\r\r\r\r ≤ \r\r\r \u0000AT ∆ A∆ \u0001−1 − \u0000AT A \u0001−1\r\r\r · \r\r A∆ \r\r + \r\r\r \u0000AT A \u0001−1\r\r\r · ∥ ∆ ∥ ≤ eρ(A) · \r\r\r \u0000AT A \u0001−1\r\r\r \r\r\r \u0000AT A \u0001−1\r\r\r −1 − eρ(A) · \u0010\r\r A \r\r + ρ \u0011 + \r\r\r \u0000AT A \u0001−1\r\r\r · ρ =C(A, ρ). In the second-to-last step, we used Theorem B.18. 28 Challenges with Partial Observability of Human Evaluators in Reward Learning The constant C(A, ρ), defined in Equation (10), has a fairly complicated form. In the following proposition, we find an easier-to-study upper bound in a special case: Proposition B.22. Assume that ρ ≤ ∥ A ∥ and ρ ≤ −∥ A ∥ + q ∥ A ∥2 + 1/2 · \r\r(AT A)−1\r\r−1.3 Then we have C(A, ρ) ≤ ρ · \r\r(AT A)−1\r\r · h 12 · ∥ A ∥2 · \r\r(AT A)−1\r\r + 1 i . Proof. The second assumption gives, as in the proof of Lemma B.20, that eρ(A) ≤ 1/2 · \r\r(AT A)−1\r\r−1. Together with ρ ≤ ∥ A ∥, the result follows. B.6.2. APPLICATION TO BOUNDS IN THE ERROR OF THE RETURN FUNCTION We now apply the results from the preceding section to our case. Define r(B) : im Γ → R⃗Ω as the restriction of the belief operator B to im Γ. Assume that ker B ∩ im Γ = {0}, which is, according to Corollary B.4, a sufficient condition for identifiability. Note that this condition means that r(B) is injective. Thus, Lemma B.16 ensures that r(B)T r(B) is invertible and that \u0000r(B)T r(B) \u0001−1r(B)T is a left inverse of r(B). Consequently, from the equation r(B)(G) = B(G) we obtain G = \u0000r(B)T r(B) \u0001−1r(B)T (B(G)). This is the concrete formula with which G can be identified from B(G). When perturbing B, this leads to a corresponding perturbance in \u0000r(B)T r(B) \u0001−1r(B)T whose size influences the maximal error in the inference of G. This, in turn, influences the size of the error in JG, the policy evaluation function, where JG(π) := E ⃗s∼P π(⃗s) \u0002 G(⃗s) \u0003 . We obtain: Theorem B.23. Let G be the true reward function, B the belief operator corresponding to the human’s true belief model B(⃗s | ⃗o), and B(G) be the resulting observation-based return function. Assume that ker B ∩ im Γ = {0}, so that r(B)T r(B) is invertible. Let ∆ : R ⃗S → R⃗Ω be a perturbation satisfying ∥ ∆ ∥ ≤ ρ, where ρ satisfies the following two properties: ρ ≤ \r\rr(B) \r\r, ρ ≤ − \r\rr(B) \r\r + q\r\rr(B) \r\r2 + 1/2 · \r\r\u0000r(B)T r(B) \u0001−1\r\r−1. Let B∆ := B + ∆ be the misspecified belief operator. The first claim is that r(B∆)T r(B∆) is invertible under these conditions. Now, assume that the learning system infers the return function ˜G := \u0000r(B∆)T r(B∆) \u0001−1r(B∆)T (B(G)).4 Then there is a polynomial Q(X, Y ) of degree five such that ∥ ˜G − G∥ ≤ ∥G∥ · Q \u0010\r\r(r(B)T r(B))−1\r\r, ∥r(B)∥ \u0011 · ρ. Thus, for all policies π, we obtain \f\f\fJ ˜ G(π) − JG(π) \f\f\f ≤ ∥G∥ · Q \u0010\r\r(r(B)T r(B))−1\r\r, ∥r(B)∥ \u0011 · ρ. In particular, for sufficiently small perturbances ρ, the error in the inferred policy evaluation function J ˜ G becomes arbitrarily small. 3Note the factor 1/2 compared to the definition of τ(A) in Equation (9). 4Note that there is not necessarily a ˜G with r(B∆)( ˜G) = B(G) since r(B∆) is not always surjective. Nevertheless, ˜G := \u0000r(B∆)T r(B∆) \u0001−1r(B∆)T (B(G)) is the best attempt at a solution in the sense that r(B∆)( ˜G) then minimizes the Euclidean distance to B(G). 29 Challenges with Partial Observability of Human Evaluators in Reward Learning Proof. That r(B∆)T r(B∆) is invertible follows immediately from Proposition B.21 by using that ∥r(∆)∥ ≤ ∥ ∆ ∥ and that r(B∆) = r(B)r(∆), together with the second bound on ρ (which implies the assumed bound in Proposition B.21). We have \f\f\fJ ˜ G(π) − JG(π) \f\f\f = \f\f\f E ⃗s∼P π(⃗s) \u0002 ( ˜G − G)(⃗s) \u0003\f\f\f ≤ E ⃗s∼P π(⃗s) h\f\f( ˜G − G)(⃗s) \f\f i ≤ max ⃗s∈ ⃗S \f\f( ˜G − G)(⃗s) \f\f ≤ ∥ ˜G − G∥ = \r\r\r\r h\u0000r(B∆)T r(B∆) \u0001−1r(B∆)T − \u0000r(B)T r(B) \u0001−1r(B)T i · B(G) \r\r\r\r ≤ \r\r\u0000r(B∆)T r(B∆) \u0001−1r(B∆)T − \u0000r(B)T r(B) \u0001−1r(B)T \r\r · \r\r B(G) \r\r ≤ C(r(B), ρ) · ∥r(B)(G)∥ ≤ C(r(B), ρ) · ∥r(B)∥ · ∥G∥. In the second to last step, we used Proposition B.21. By Proposition B.22, we can define the polynomial Q(X, Y ) by Q(X, Y ) = XY · h 12XY 2 + 1 i , which is of degree five. The last claim follows from limρ→0 ρ = 0. Remark B.24. In the case of a square matrix B that is injective, we can apply Theorem B.18 directly to B−1 (which is now invertible) and obtain the following simplification of Theorem B.23 for the case that ∥ ∆ ∥ ≤ ρ ≤ 1 2 · ∥ B−1 ∥−1: \f\fJ ˜ G(π) − JG(π) \f\f ≤ ρ · 2 · ∥ B ∥ · ∥G∥ · ∥ B−1 ∥2. The polynomial is then only of degree 3. B.7. Preliminary Characterizations of the Ambiguity Recall the sequence of functions RS R ⃗S R⃗Ω. Γ B In this section, we clarify im Γ and ker B in special cases, as their intersection is the crucial ambiguity in Theorem B.2. The following proposition shows that for deterministic P ⃗O and a rational human, ker B decomposes into hyperplanes defined by normal vectors of probabilities of sequences mapping to the same observation sequence: Proposition B.25. Assume the human reasons as in Section B.1. Assume P ⃗O is deterministic. Let B(⃗s) be the distribution of sequences under the human’s belief over the policy, given by B(⃗s) = R π′ B(π′)P π′(⃗s) for some policy prior B(π′). For each ⃗o, let B⃗o := [B(⃗s)]⃗s: ⃗O(⃗s)=⃗o ∈ R{⃗s∈ ⃗S | ⃗O(⃗s)=⃗o} be the vector of probabilities of sequences that are observed as ⃗o. Let G′ be a return function. For each ⃗o ∈ ⃗Ω, define the restriction G′ ⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o} by G′ ⃗o(⃗s) := G′(⃗s) for all ⃗s ∈ {⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o}. Assume that B(⃗s | ⃗o) is the Bayesian posterior. Then G′ ∈ ker B if and only if the property B⃗o · G′ ⃗o = 0 holds for all ⃗o ∈ ⃗Ω. Proof. For a deterministic observation kernel P ⃗O, by Bayes rule we have B(⃗s | ⃗o) = P ⃗O(⃗o | ⃗s) · B(⃗s) P ⃗s ′ P ⃗O(⃗o | ⃗s′) · B(⃗s′) 30 Challenges with Partial Observability of Human Evaluators in Reward Learning = δ⃗o \u0000 ⃗O(⃗s) \u0001 · B(⃗s) P ⃗s ′ δ⃗o \u0000 ⃗O(⃗s′) \u0001 · B(⃗s′) = ( 0, ⃗O(⃗s) ̸= ⃗o B(⃗s) P ⃗s ′: ⃗ O(⃗s ′)=⃗o B(⃗s ′), ⃗O(⃗s) = ⃗o. Thus, for any return function G′ and any observation sequence ⃗o, we have \u0002 B(G′) \u0003 (⃗o) = E ⃗s∼B(⃗s|⃗o) \u0002 G′(⃗s) \u0003 = X ⃗s B(⃗s | ⃗o)G′(⃗s) = X ⃗s: ⃗O(⃗s)=⃗o B(⃗s) P ⃗s ′: ⃗O(⃗s ′)=⃗o B(⃗s′)G′(⃗s) =   X ⃗s ′: ⃗O(⃗s ′)=⃗o B(⃗s′) !−1 · X ⃗s: ⃗O(⃗s)=⃗o B(⃗s)G′(⃗s). Thus, we have G′ ∈ ker B if and only if B⃗o · G′ ⃗o = X ⃗s: ⃗O(⃗s)=⃗o B(⃗s)G′(⃗s) = 0 for all ⃗o. That was to show. Remark B.26. One can interpret the previous proposition as follows: As long as ⃗O is injective, we have \f\f{⃗s ∈ ⃗S | ⃗O(⃗s) = o} \f\f = 1 for all ⃗o, meaning that B⃗o and G′ ⃗o have only one entry. Thus, B⃗o · G′ ⃗o = 0 implies G′ ⃗o = 0. If that holds for all ⃗o, then G′ ∈ ker B implies G′ = 0, meaning B is injective. However, as soon as there is an ⃗o with k⃗o := \f\f{⃗s ∈ ⃗S | ⃗O(⃗s) = o} \f\f > 1, the equation B⃗o · G′ ⃗o = 0 leads to k⃗o − 1 free parameters in G′ ⃗o. G′ ⃗o can then be chosen freely in the hyperplane of vectors orthogonal to B⃗o without moving out of the kernel of B. Another way of writing Proposition B.25 is to write ker B as a direct sum of these hyperplanes perpendicular to B⃗o: ker B = M ⃗o: | ⃗O−1(⃗o)|≥2 B⊥ ⃗o . Recall that a return function G is called time-separable if there exists a reward function R such that Γ(R) = G. Before we discuss time-separability in more interesting examples, we want to talk about one simple case where all return functions are time-separable. We leave a general characterization of im Γ to future work. Proposition B.27. Let there be an ordering ⃗s(1),⃗s(2), . . . of all sequences in ⃗S, and a function ϕ : ⃗S → S from sequences to states such that ϕ(⃗s) ∈ ⃗s and ϕ(⃗s(k)) /∈ ⃗s(i) for all i < k. Then every return function is time-separable. Proof. Let G be a return function. Initialize R(s) = 0 for all s and inductively update it for all i = 1, 2, . . . : R \u0000ϕ(⃗s(i)) \u0001 :=   X t: s(i) t =ϕ(⃗s(i)) γt !−1 ·   G(⃗s(i)) − X t: s(i) t ̸=ϕ(⃗s(i)) γt · R \u0000s(i) t \u0001 ! , where the inductive definition always uses R as it is defined by that point in time. Once R \u0000ϕ(⃗s(i)) \u0001 is defined, but not yet any future values R \u0000ϕ(⃗s(k)) \u0001 , k > i, we have \u0002 Γ(R) \u0003 (⃗s(i)) = T X t=0 γt · R \u0000s(i) t \u0001 31 Challenges with Partial Observability of Human Evaluators in Reward Learning =   X t: s(i) t =ϕ(⃗s(i)) γt ! · R \u0000ϕ(⃗s(i)) \u0001 + X t: s(i) t ̸=ϕ(⃗s(i)) γt · R \u0000s(i) t \u0001 = G(⃗s(i)). Furthermore, the property ϕ(⃗s(k)) /∈ ⃗s(i) for all i < k ensures that changes to the reward function for k > i do not affect the value of \u0002 Γ(R) \u0003 (⃗s(i)). This shows Γ(R) = G, and thus G is time-separable. Corollary B.28. In a multi-armed bandit, every return function is time-separable. Proof. In a multi-armed bandit, states and sequences are equivalent, and so we can choose ϕ(s) = s for every state/sequence s. The result follows from Proposition B.27. Alternatively, simply directly notice that in a multi-armed bandit, Γ is the identity mapping, and so for every return/reward function R, we have Γ(R) = R. B.8. Examples Supplementing Section 5 In this whole section, the inverse temperature parameter in the human choice probabilities is given by β = 1. We now consider four more mathematical examples of Corollary B.4 and Theorem B.9. In the first example, the ambiguity is so bad that the reward inference can become worse than simply maximizing Jobs as in naive RLHF. In Example B.30, there is simply “noise” in the observations and the human’s belief, the matrices B and O are injective, and identifiability works, as in Corollary B.14. In the third example, the matrix B is not injective and identifiability fails, which is a minimal example showing the limits of our main theorems. In the fourth example, the matrix B is not injective, but ker B ∩ im Γ = {0}, and so identifiability works. This example is interesting in that the identifiability simply emerges through different distributions of delay that are caused by the different unobserved events. In this section, both the linear operators B : R ⃗S → R⃗Ω and O : R⃗Ω → R ⃗S are considered as matrices O = \u0000P ⃗O(⃗o | ⃗s) \u0001 ⃗s,⃗o ∈ R ⃗S×⃗Ω, B = \u0000B(⃗s | ⃗o) \u0001 ⃗o,⃗s ∈ R ⃗Ω× ⃗S. Notice that both have a swap in their indices. Example B.29. Theorem 5.1 shows that the remaining ambiguity from the human’s choice probabilities is given by ker B ∩ im Γ, but it doesn’t explain how to proceed given this ambiguity. Without further inductive biases, some reward functions within the ambiguity of the true reward function can be even worse than simply maximizing Jobs. E.g., consider a multi-armed bandit with three actions a, b, c, observation-kernel o = O(a) = O(b) ̸= O(c) = c and reward function R(a) = R(b) < R(c). If the human belief is given by B(a | o) = p = 1 − B(b | o), then R′ = α · (p − 1, p, 0) ∈ R{a,b,c} is in the ambiguity for all α ∈ R, and so ˜R := R + R′ is compatible with the choice probabilities. However, for α ≪ 0, we have ˜R(a) > ˜R(b) and ˜R(a) > ˜R(c), and so optimizing against this reward function leads to a suboptimal policy. In contrast, maximizing Jobs leads to the correct policy since a, b, and c all obtain their ground truth reward in this example. This generally raises the question of how to tie-break reward functions in the ambiguity, or how to act conservatively given the uncertainty, in order to consistently improve upon the setting in Section 4.1. Example B.30. This example is a special case of Corollary B.14. Consider a multi-armed bandit with two actions (which are automatically also states and sequences) a and b. In this case, the reward function and return function is the same. We assume there to be two possible observations o(a), o(b) and the observation kernel to be non-deterministic, with probabilities PO(o(j) | i) = ( 2/3, if i = j, 1/3, else. If we assume the human forms Bayesian posterior beliefs as in Section B.1 and to have a policy prior B(π′) such that B(a) = R π π(a)B(π′)dπ = 1/2 and B(b) = 1/2, then it is easy to show that the human’s belief is the “reversed” observation kernel: B(j | o(i)) = PO(o(i) | j). 32 Challenges with Partial Observability of Human Evaluators in Reward Learning We obtain O = B = \u00122/3 1/3 1/3 2/3 \u0013 = 1 3 · \u00122 1 1 2 \u0013 These matrices are injective since they are invertible: O−1 = B−1 = \u0012 2 −1 −1 2 \u0013 . More generally, even if the human does not form fully rational posterior beliefs, it is easy to imagine that the matrix B can end up being invertible. Thus, Corollary B.4 guarantees that the reward function can be inferred up to an additive constant from the choice probabilities of observations, and Theorem B.9 shows that this even works when the learning system does not know what the human observed. In the rest of this example, we explicitly walk the reader through the process of how the reward function can be inferred, in the general case that the observations are not known. In the process, we essentially recreate the proof of the theorems for this special case. For this aim, we first want to compute the choice probabilities P R\u0000i ≻ j \u0001 that the learning system has access to in the limit of infinite data. We assume that the reward function is given by R(a) = −1 and R(b) = 2. We compute: B(R) = 1 3 · \u0012 2 1 1 2 \u0013 · \u0012 −1 2 \u0013 = \u0012 0 1 \u0013 . In other words, we have Es∼B(s|o(a))[R(s)] = 0 and Es∼B(s|o(b))[R(s)] = 1. From this, we can compute the observation- based choice probabilities ePo(i)o(j) = σ \u0000B(R)(o(i)) − B(R)(o(j)) \u0001 , see Equation (2), and obtain: ePo(a)o(a) = ePo(b)o(b) = 1 2, ePo(a)o(b) = 1 1 + e, ePo(b)o(a) = e 1 + e. We can now determine the final choice probabilities Pij := P R\u0000i ≻ j \u0001 again by a matrix-vector product, with the indices ordered lexicographically, see Equation (8). Here, O ⊗ O is the Kronecker product of the matrix O with itself: P = (O ⊗ O) · eP = 1 9 ·     4 2 2 1 2 4 1 2 2 1 4 2 1 2 2 4     ·     1/2 1/(1 + e) e/(1 + e) 1/2     =     1/2 1/3 · (2 + e)/(1 + e) 1/3 · (1 + 2e)/(1 + e) 1/2     . For example, the second entry in P is Pab = P R\u0000a ≻ b \u0001 = 2+e 3·(1+e). This is the likelihood that, for ground-truth actions a, b, the human will prefer a after only receiving observations o(a) or o(b) according to O and following a Boltzman-rational policy based on the belief of the real action, see Equation (8). Over time, the learning system will be able to estimate these probabilities based on repeated human choices, assuming all state-pairs are sampled infinitely often. The question of identifiability is whether the original reward function R can be inferred from that data, given that the learning system knows O and B. We assume that the learning system doesn’t a priori know R or any of the intermediate steps in the computation. First, eP can be inferred by inverting O ⊗ O: eP = (O ⊗ O)−1 · P =     4 −2 −2 1 −2 4 1 −2 −2 1 4 −2 1 −2 −2 4     ·     1/2 1/3 · (2 + e)/(1 + e) 1/3 · (1 + 2e)/(1 + e) 1/2     =     1/2 1/(1 + e) e/(1 + e) 1/2     . The learning system wants to use this to infer B( ˜R) (for the later-to-be inferred reward function ˜R that may differ from the true reward function R) and uses the equation ePo(a)o(b) = exp \u0000B( ˜R)(o(a)) \u0001 exp \u0000B( ˜R)(o(a)) \u0001 + exp \u0000B( ˜R)(o(b)) \u0001, 33 Challenges with Partial Observability of Human Evaluators in Reward Learning which can be rearranged to B( ˜R)(o(a)) = log ePo(a)o(b) 1 − ePo(a)o(b) + B( ˜R)(o(b)) = log 1/(1 + e) e/(1 + e) + B( ˜R)(o(b)) = B( ˜R)(o(b)) − 1. This relation is all which can be inferred about B( ˜R)(o(a)) and B( ˜R)(o(b)); the precise value cannot be determined and B( ˜R)(o(b)) is a free parameter. One can check that for B( ˜R)(o(b)) = 1 this coincides with the true value B(R). Finally, one can invert B to infer ˜R from this: ˜R = B−1 · B( ˜R) = \u0012 2 −1 −1 2 \u0013 · \u0012 B( ˜R)(o(b)) − 1 B( ˜R)(o(b)) \u0013 = \u0012B( ˜R)(o(b)) − 2 1 + B( ˜R)(o(b)) \u0013 = \u0012−1 2 \u0013 + \u0012B( ˜R)(o(b)) − 1 B( ˜R)(o(b)) − 1 \u0013 = R + \u0012B( ˜R)(o(b)) − 1 B( ˜R)(o(b)) − 1 \u0013 . Thus, the inferred and true reward functions differ maximally by a constant, as predicted in Theorem B.9. In the following example, we work out a case where the reward function is so ambiguous that any policy is optimal to some reward function consistent with the human feedback: Example B.31. Consider a multi-armed bandit with exactly three actions/states a, b, c. We assume a deterministic observation kernel with o := O(a) = O(c) ̸= O(b) = b. Assume the human has some arbitrary beliefs B(a | o), B(c | o) = 1 − B(a | o), and can identify b: B(b | b) = 1. Then if the human makes observation comparisons with a Boltzman-rational policy, as in Theorem B.2, the resulting reward function is so ambiguous that some reward functions consistent with the feedback place the highest value on action a, no matter the true reward function R. Thus, even if the true reward function R regards a as the worst action, a can result from the reward learning and subsequent policy optimization process. Proof. The matrix B : R{a,b,c} → R{o,b} is given by B = \u0012B(a | o) 0 B(c | o) 0 1 0 \u0013 . Its kernel is given by reward functions R′ with R′(b) = 0 and R′(c) = − B(a|o) B(c|o) R′(a), with R′(a) a free parameter. Theorem B.2 shows that, up to an additive constant, the reward functions consistent with the feedback of observation comparisons are given by ˜R = R + R′ for any R′ ∈ ker B. Thus, whenever the free parameter R′(a) satisfies R′(a) > R(b) − R(a) and R′(a) > B(c | o) · \u0000R(c) − R(a) \u0001 , we obtain ˜R(a) > ˜R(b) and ˜R(a) > ˜R(c), showing the claim. We now investigate another example where B is not injective, and yet, identifiability works because B ◦ Γ ̸= {0}. We saw such cases already in Example C.6, but include this additional example since it shows a conceptually interesting case: two different states lead to the exact same observations, but can be disambiguated since they lead to different amounts of delay until a more informative observation is made again. Example B.32. In this example, we assume that the human knows the policy π that generates the state sequences (corresponding to a policy prior B(π′) = δπ(π′) concentrated on π), which together with knowledge of the transition dynamics of the environment determines the true state transition probabilities T π(s′ | s) = P a∈A T (s′ | s, a) · π(a | s). We consider an environment with three states s, s′, s′′ and the following transition dynamics T π, where p ̸= 1/2 is a 34 Challenges with Partial Observability of Human Evaluators in Reward Learning probability: s s′ s′′ 1/3 1/3 1/3 1−p p p 1−p We assume that P0(s) = 1. Furthermore, we assume deterministic observations and s = O(s) ̸= O(s′) = O(s′′) =: o. Assume the time horizon T is 3, i.e., there are timesteps 0, 1, 2, 3. Assume that the human forms the belief over the true state sequence by Bayesian posterior updates as in Section B.1. In this case, ker B ̸= {0} by Proposition B.11. However, we will now show that ker(B ◦ Γ) = {0}. If the human makes Boltzmann-rational comparisons of observation sequences, then this implies the identifiability of the return function up to an additive constant by Corollary B.4.5 Thus, let R′ ∈ ker(B ◦ Γ), i.e., h B \u0000Γ(R′) \u0001i (⃗o) = 0 for every observation sequence ⃗o. For ⃗o = ssss being the observation sequence that only consists of state s, this implies R′(s) = 0. Consequently, for general observation sequences ⃗o, we have: 0 = h B \u0000Γ(R′) \u0001i (⃗o) = E ⃗s∼B(⃗s|⃗o) \" 3 X t=0 δs′(st) · γt # · R′(s′) + E ⃗s∼B(⃗s|⃗o) \" 3 X t=0 δs′′(st) · γt # · R′(s′′). Now we specialize this equation to the two observation sequences ⃗o(1) = soss and ⃗o(2) = soos. We start by considering ⃗o(1). This is consistent with the two state sequences ⃗s(1),(s′) = ss′ss and ⃗s(1),(s′′) = ss′′ss. We have posterior probabilities B \u0000⃗s(1),(s′) | ⃗o(1)\u0001 = 1 − p, B \u0000⃗s(1),(s′′) | ⃗o(1)\u0001 = p, and therefore 0 = \u0002 B \u0000Γ(R′) \u0001i (⃗o(1)) = (1 − p) · γ · R′(s′) + p · γ · R′(s′′), and so R′(s′) = p p − 1 · R′(s′′). (11) Similarly, ⃗o(2) is consistent with the sequences ⃗s(2),(s′) = ss′s′s and ⃗s(2),(s′′) = ss′′s′′s. They have posterior probabilities B \u0000⃗s(2),(s′) | ⃗o(2)\u0001 = 1 2, B \u0000⃗s(2),(s′′) | ⃗o(2)\u0001 = 1 2, leading to 0 = 1 2 · (γ + γ2) · R′(s′) + 1 2 · (γ + γ2) · R′(s′′). Together with Equation (11), we obtain R′(s′′) = −R′(s′) = p 1 − p · R′(s′′), which implies R′(s′′) = 0 because p ̸= 1 2, and thus also R′(s′) = 0. Overall, we have showed R′ = 0, and so B ◦ Γ is injective. This means that reward functions are identifiable in this example up to an additive constant, see Corollary B.4. C. Issues of Naively Applying RLHF under Partial Observability In this section, we study the naive application of RLHF under partial observability. Thus, most of it takes a step back from the general theory of appropriately modeled partial observability in RLHF. Later, we will analyze examples where we also apply the general theory, which is why this appendix section comes second. 5We assume that the learning system knows what the human observes, which is valid since PO is deterministic. Alternatively, one can argue with Proposition B.11 that O is automatically injective, meaning one can apply Theorem B.9. 35 Challenges with Partial Observability of Human Evaluators in Reward Learning In Section C.1, we first briefly explain what happens when the learning system incorrectly assumes that the human observes the full environment state. We show that as a consequence, the system is incentivized to infer what we call the observation return function Gobs, which evaluates a state sequence based on the human’s belief of the state sequence given the human’s observations. In the policy optimization process, the policy is then selected to maximize Jobs, an expectation over Gobs. In the interlude in Section C.2, we then briefly analyze the unrealistic case that the human, when evaluating a policy π, fully knows the complete specification of that policy and all of the environment and engages in rational Bayesian reasoning; in this case, Jobs = J is the true policy evaluation function. Realistically, however, maximizing Jobs can lead to failure modes. In Appendix C.3 we prove that a suboptimal policy that is optimal according to Jobs causes deceptive inflation, overjustification, or both. In Appendix C.4, we expand on the analysis of the main examples in the main paper. Finally, in Section C.5, we study further concrete examples where maximizing Jobs reveals deceptive and overjustifying behavior by the resulting policy. C.1. Optimal Policies under RLHF with Deterministic Partial Observations Maximize Jobs Assume that P ⃗O is deterministic and that the human makes Boltzmann-rational sequence comparisons between observation sequences. The true choice probabilities are then given by (See Equations (2) and (8)): P R\u0000⃗s ≻ ⃗s′\u0001 = σ \u0012 β · \u0010\u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 − \u0000B ·G \u0001\u0000 ⃗O(⃗s′) \u0001\u0011\u0013 (12) Now, assume that the learning system does not model the situation correctly. In particular, we assume: • The system is not aware that the human only observes observation sequences ⃗O(⃗s) instead of the full state sequences. • The system does not model that the human’s return function is time-separable, i.e., comes from a reward function R over environment states. The learning system then thinks that there is a return function ˜G ∈ R⃗S such that the choice probabilities are given by the following faulty formula: P R\u0000⃗s ≻ ⃗s′\u0001 := σ \u0010 β \u0000G(⃗s) − G(⃗s′) \u0001\u0011 Now, assume that the learning system has access to the choice probabilities and wants to infer G. Inverting the sigmoid function and then plugging in the true choice probabilities from Equation (12), we obtain: ˜G(⃗s) = 1 β log P R(⃗s ≻ ⃗s′) P R(⃗s′ ≻ ⃗s) + ˜G(⃗s′) = 1 β h β · \u0010\u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 − \u0000B ·G \u0001\u0000 ⃗O(⃗s′) \u0001\u0011i + ˜G(⃗s′) = \u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 + C(⃗s′).6 Here, C(⃗s′) is some quantity that does not depend on ⃗s. Now, fix ⃗s′ as a reference sequence. Then for varying ⃗s, C(⃗s′) is simply an additive constant. Consequently, up to an additive constant, this determines the return function that the learning system is incentivized to infer. We call it the observation return function since it is the return function based on the human’s observations: Gobs(⃗s) := \u0000B ·G \u0001\u0000 ⃗O(⃗s) \u0001 . This return function is not necessarily time-separable, but we assume that time-separability is not modeled correctly by the learning system. Now, define the resulting policy evaluation function Jobs by Jobs(π) := E ⃗s∼P π(⃗s) \u0002 Gobs(⃗s) \u0003 . This is the policy evaluation function that would be optimized if the learning system erroneously inferred the return function Gobs. 6Note that in the case of non-deterministic observation kernels and choice probabilities given as in Equation (8), this argument does not work since the logarithm cannot be swapped with the outer expectation of the choice probabilities. 36 Challenges with Partial Observability of Human Evaluators in Reward Learning C.2. Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then Jobs = J In this section, we briefly consider what would happen if in Jobs, the human’s belief B would make use of the true policy and be a rational Bayesian posterior as in Section B.1. We will show that under these conditions, we have Jobs = J. Since these are unrealistic assumptions, no other section depends on this result. For the analysis, we drop the assumption that the observation sequence kernel P ⃗O is deterministic, and assume that Jobs is given as follows: Jobs(π) := E ⃗s∼P π(⃗s) \" E ⃗o∼P ⃗ O(⃗o|⃗s) \u0014 E ⃗s ′∼Bπ(⃗s ′|⃗o) \u0002 G(⃗s′) \u0003\u0015# . (13) In this formula, Bπ(⃗s | ⃗o) := B(⃗s | ⃗o, π) with B being the joint distribution from Section B.1. Formally, this is the posterior of the joint distribution B(⃗s,⃗o | π) that is given by the following hidden Markov model: s0 s1 s2 s3 . . . o0 o1 o2 o3 . . . PO T π PO T π PO T π PO T π (14) Here, T π(s′ | s) := P a∈A T (s′ | s, a) · π(a | s). s0 is sampled according to the known initial distribution P0(s0). The human’s posterior Bπ(⃗s′ | ⃗o) is then the true posterior in this HMM. We obtain: Proposition C.1. Let π be a policy that is known to the human. Then Jobs(π) = J(π). Proof. By Equation (13), we have Jobs(π) = E ⃗s∼P π(⃗s) \" E ⃗o∼P ⃗ O(⃗o|⃗s) \u0014 E ⃗s ′∼Bπ(⃗s ′|⃗o) \u0002 G(⃗s′) \u0003\u0015# (1) = X ⃗s P π(⃗s) X ⃗o P ⃗O(⃗o | ⃗s) X ⃗s ′ Bπ(⃗s′ | ⃗o)G(⃗s′) (2) = X ⃗s ′ \" X ⃗o Bπ(⃗s′ | ⃗o) \" X ⃗s P ⃗O(⃗o | ⃗s)P π(⃗s) ## G(⃗s′) (3) = X ⃗s ′ \" X ⃗o Bπ(⃗s′ | ⃗o)Bπ(⃗o) # G(⃗s′) (4) = X ⃗s ′ \" X ⃗o P π(⃗s′)P ⃗O(⃗o | ⃗s′) # G(⃗s′) (5) = X ⃗s ′ P π(⃗s′)G(⃗s′) (6) = X ⃗s P π(⃗s)G(⃗s) (7) = J(π). In step (1), we wrote the expectations out in terms of sums. In step (2), we reordered them. In step (3), we observed that the inner sum over ⃗s evaluates to the marginal distribution Bπ(⃗o) of the observation sequence ⃗o in the HMM in Equation (13). In step (4), we used Bayes rule in the inner sum. This is possible since Bπ(⃗s′ | ⃗o) is the true posterior when π is known. In step (5), we pull P π(⃗s′) out and notice that the remaining inner sum evaluates to 1. Step (6) is a relabeling and step (7) the definition of the true policy evaluation function J. 37 Challenges with Partial Observability of Human Evaluators in Reward Learning C.3. Proof of Theorem 4.5 We first prove the following lemma (this is Lemma 4.4, repeated here for convenience): Lemma C.2. Let π and πref be two policies. If J(π) < J(πref) and Jobs(π) ≥ Jobs(πref), then relative to πref, π must exhibit deceptive inflating, overjustification, or both. Proof. We start by establishing a quantitative relationship between the average overestimation and underestimation errors E + and E − as defined in Definition 4.2, the true policy evaluation function J, and the observation evaluation function Jobs defined in Equation (4). Define ∆ : ⃗S → R by ∆(⃗s) = Gobs(⃗s) − G(⃗s), where Gobs is as defined in Equation (3). Consider the quantity E+(⃗s) − E−(⃗s) = max \u00000, ∆(⃗s) \u0001 − max \u00000, −∆(⃗s) \u0001 . If ∆(⃗s) > 0, then the first term is ∆(⃗s) and the second one is 0. If ∆(⃗s) < 0, then the first term is zero and the second one is ∆(⃗s). If ∆(⃗s) = 0, then both terms are zero. In all cases the right-hand side is equal to ∆(⃗s). Unpacking the definition of ∆ again, we have that for all ⃗s, E+(⃗s) − E−(⃗s) = Gobs(⃗s) − G(⃗s). (15) For any policy π, if we take the expectation of both sides of this equation over the on-policy distribution admitted by π, P π, we get E +(π) − E −(π) = Jobs(π) − J(π). (16) We now prove the lemma. Let π and πref be two policies, and assume that J(π) < J(πref) and Jobs(π) ≥ Jobs(πref). Equivalently, we have Jobs(π) − Jobs(πref) ≥ 0 and J(πref) − J(π) > 0, which we combine to state \u0010 Jobs(π) − Jobs(πref) \u0011 + \u0010 J(πref) − J(π) \u0011 > 0. (17) Rearranging terms yields \u0010 Jobs(π) − J(π) \u0011 − \u0010 Jobs(πref) − J(πref) \u0011 > 0. These two differences inside parentheses are equal to the right-hand side of (16) for π and πref, respectively. We substitute the left-hand side of (16) twice to obtain \u0010 E +(π) − E −(π) \u0011 − \u0010 E +(πref) − E −(πref) \u0011 > 0. Rearranging terms again yields \u0010 E +(π) − E +(πref) \u0011 + \u0010 E −(πref) − E −(π) \u0011 > 0. (18) If E +(π) − E +(πref) > 0 then we have E +(π) > E +(πref) and, by assumption, Jobs(π) ≥ Jobs(πref). By Definition 4.3, this means π exhibits deceptive inflating relative to πref. If E −(πref) − E −(π) > 0 then we have E −(π) < E −(πref) and, by assumption, J(π) < J(πref). By Definition 4.3, this means π exhibits overjustification relative to πref. At least one of the two differences in parentheses in (18) must be positive, otherwise their sum would not be positive. Thus π must exhibit deceptive inflating relative to πref, overjustification relative to πref, or both. We can now combine earlier results to prove Theorem 4.5, repeated here for convenience: Theorem C.3. Assume that PO is deterministic. Let π∗ obs be an optimal policy according to a naive application of RLHF under partial observability, and let π∗ be an optimal policy according to the true objective J. If π∗ obs is not J-optimal, then relative to π∗, π∗ obs must exhibit deceptive inflating, overjustification, or both. Proof. Because PO is deterministic, π∗ obs must be optimal with respect to Jobs by Proposition 4.1 (proved in Appendix C.1). Thus Jobs(π∗ obs) ≥ Jobs(π∗). Since π∗ is J-optimal and π∗ obs is not, J(π∗) < J(π∗ obs). By Lemma 4.4 (repeated as Lemma C.2 above), relative to π∗, π∗ obs must exhibit deceptive inflating, overjustification, or both. 38 Challenges with Partial Observability of Human Evaluators in Reward Learning C.4. Derivations and Further Details for Section 4.3 Example A Figure 5. An expanded view of Figure 2A. Commands corresponding to the various actions are depicted along edges, and log messages corresponding to the various observations are depicted underneath each state. We first include Figure 5, a more detailed picture of the MDP and observation function in Section 4.3.1, to help ground the narrative details of the example. Next we formally enumerate the details of the MDP and observation function. • S = {S, I, W, WH, L, LH, T}. • A = {aI, aC, aH, aT }. • T is as depicted in Figure 5 and Figure 2A. For a state s, any outgoing arrow labeled with an action a (such as aI) describes the distribution T (s′ | s, a) as follows: if the arrow does not split, then T (s′ | s, a) = 1 where s′ is the state the arrow points to; if the arrow does split, then for each successor state s′ it eventually reaches, a probability q is written just before the box corresponding to s′ (for this example, q = p or q = 1 − p), and T (s′ | s, a) = q. ◦ Additionally, any action taken from a state that does not have an outgoing arrow corresponding to that action will immediately transition to state T, as though aT had been taken. ◦ Any action taken from state T transitions deterministically to T. • P0(S) = 1. • R is as described in the table (the numbers in the top right of each state box) with r ≥ 0. Additionally, R(S) = R(T) = 0. • γ = 1. We work with a fixed horizon length of 3, meaning state sequences have length 4 (since time is zero-indexed: s0s1s2s3). The observation function is also depicted in Figure 5. Each state deterministically produces the observation in the lower-right corner of its box in the figure. We also write it in another format in Table 8. s S I W WH L LH T O(s) o∅ oI oW oW oL o∅ o∅ Table 8. The observation function O for the example in Section 4.3.1 and Appendix C.4. We make the additional assumption that the human belief B(⃗s | ⃗o) only supports state sequences ⃗s which actually produce ⃗o under the sequence observation function ⃗O: B(⃗s | ⃗o) > 0 =⇒ ⃗O(⃗s) = ⃗o. In particular, this means that for any ⃗o which is only produced by one ⃗s, B(⃗o | ⃗s) = 1. There are three pairs of state sequences which produce identical observation sequences. For each, we introduce a parameter representing the probability the human infers the first of the pair of state sequences upon seeing their shared observation sequence. 39 Challenges with Partial Observability of Human Evaluators in Reward Learning 1. SILHT and SITT both produce o∅oIo∅o∅, a log containing only a success confirmation for installing drivers, again because O(LH) = O(T) = o∅. Let pH = B(⃗s = SILHT | ⃗o = o∅oIo∅o∅). 2. STTT and SLHTT both produce o∅o∅o∅o∅, an empty log, since O(LH) = O(T) = o∅. Let p′ H = B(⃗s = SLHTT | ⃗o = o∅o∅o∅o∅). 3. SIWT and SIWHT both produce o∅oIoW o∅, a log containing success confirmations for installing both drivers and CUDA. Let pW = B(⃗s = SIWT | ⃗o = o∅oIoW o∅). We assume for simplicity that p′ H = pH, i.e. that the human is just as likely to think an empty log following a successful driver installation contains an error that was hidden with 2> /dev/null (pH), as they are to think that an entirely empty log contains a hidden error (p′ H). In principle, this need not be true: the human may have differing priors about the agent’s behavior in the two different circumstances. However, the algebra to reason about such a case is significantly more cumbersome, and this case reveals no fundamentally different agent behavior under our framework that isn’t present in some simpler case. We can thus write the full B as a matrix as in Table 9. STTT SLHTT SLTT SITT SILHT SILT SIWT SIWHT o∅o∅o∅o∅ 1 − pH pH o∅oLo∅o∅ 1 o∅oIo∅o∅ 1 − pH pH o∅oIoLo∅ 1 o∅oIoW o∅ pW 1 − pW Table 9. The parameterized human belief function B for the example in Section 4.3.1 and Appendix C.4, expressed as a matrix (rendered as a table). Any empty cell is equal to 0. We have laid the groundwork sufficiently to begin reasoning about the observation return, overestimation and underestimation error, policies which are optimal under the reward function learned by naive RLHF, and the resulting deceptive inflatingand overjustification failure modes. We begin by computing the measures of interest for each state sequence, shown in Table 10. ⃗s G(⃗s) Gobs(⃗s) := E⃗s ′∼B(·| ⃗O(⃗s))[G(⃗s′)] E+(⃗s) := max(0, E−(⃗s) := max(0, Gobs(⃗s) − G(⃗s)) G(⃗s) − Gobs(⃗s)) STTT 0 pHG(SLHTT) + (1 − pH)G(STTT) 0 pH(5 + r) SLHTT −5 − r = −pH(5 + r) (1 − pH)(5 + r) 0 SLTT −5 −5 0 0 SITT 1 pHG(SILHT) + (1 − pH)G(SITT) 0 pH(5 + r) SILHT −4 − r = 1 − pH(5 + r) (1 − pH)(5 + r) 0 SILT −4 −4 0 0 SIWT 11 pW G(SIWT) + (1 − pW )G(SIWHT) 0 0 SIWHT 11 = 11 0 0 Table 10. Measures of interest for each state sequence for the example in Section 4.3.1 and Appendix C.4. State sequences which produce the same observations have their Gobs columns merged, since they necessarily have the same Gobs. As an exercise, checking the computations in Table 10 is a quick way to gain some intuition for how these quantities relate. It’s further useful to speak about these quantities using their names, and work through the stories these expressions tell. Consider the E+(SLHTT) cell. What does it mean that this is (1 − pH)(5 + r)? E+ is the overestimation error; (1 − pH)(5 + r) is the expected amount by which the human observer, upon seeing a completely empty log (o∅o∅o∅o∅), will overestimate the total reward the agent attained when those empty logs were actually produced by SLHTT. This is a trajectory in which the agent immediately uses the 2> /dev/null trick, fails to install CUDA without logging the error, and terminates. Under what circumstances might the human overestimate the total reward when ⃗s = SLHTT? Upon seeing the empty log, the human thinks with probability 1 − pH that the agent simply terminated immediately, which would be worth a total reward of 0. Since the actual total reward is −5 − r, this is an overestimate by 5 + r. The human thinks with 40 Challenges with Partial Observability of Human Evaluators in Reward Learning probability pH that SLHTT occurred. This is correct, so there is no overestimation and this pH case does not contribute to the overestimation error. So we have that with probability 1 − pH, the human overestimates the total reward by 5 + r. We can keep going! Why is the underestimation error of SIWT equal to 0? Because the only other trajectory with which it can be confused attains the same total reward, so regardless of how the probability mass of the human’s belief divides between them, there will be no underestimation. Can all of the zeros in the overestimation and underestimation error columns be explained this way? We now move on to consider policies rather than state sequences. Since a policy π imposes a distribution P π over state sequences (the “on-policy distribution”), our policy measures are in fact exactly parallel to our state sequence measures. Each one is an expectation over the on-policy distribution of the columns of Table 10. We restrict our attention to deterministic policies which only take actions depicted in Figure 5 (i.e. that never terminate via an action other than aT ), of which there are only six in this MDP. They are enumerated, along with the policy-level measures, in Table 11. Policies will be written as a sequence of actions enclosed in brackets, omitting trailing repeated aT actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example, because all decisions are made before any stochasticity occurs. The policies are [aT ], [aHaT ], [aCaT ], [aIaT ], [aIaHaT ], and [aIaCaT ]. π J(π) Jobs(π) E +(π) E −(π) [aT ] 0 −pH(5 + r) 0 pH(5 + r) [aHaT ] −5 − r −pH(5 + r) (1 − pH)(5 + r) 0 [aCaT ] −5 −5 0 0 [aIaT ] 1 1 − pH(5 + r) 0 pH(5 + r) [aIaHaT ] pG(SIWHT) pGobs(SIWHT) (1 − p)(1 − pH)(5 + r) 0 +(1 − p)G(SILHT) +(1 − p)Gobs(SILHT) = 11 − (1 − p)(15 + r) = 11 − (1 − p) [10 + pH(5 + r)] [aIaCaT ] pG(SIWT) pGobs(SIWT) 0 0 +(1 − p)G(SILT) +(1 − p)Gobs(SILT) = 11 − (1 − p) · 15 = 11 − (1 − p) · 15 Table 11. Measures of interest for each policy for the example in Section 4.3.1 and Appendix C.4. Each of the columns here is the on-policy average of the corresponding column in Table 10. Policies are written as sequences of actions, omitting trailing repeated aT actions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example since all decisions are made before any stochasticity occurs. With this we have everything we need to characterize optimal policies under the reward function learned by a naive application of RLHF (“policies selected by RLHF”). By Proposition 4.1, we know that if PO is deterministic, as in this example, RLHF selects policies which maximize Jobs. In order to understand the behavior of these policies, we’ll also need to determine the true optimal policies, i.e. those which maximize J. We’ll proceed in cases, only considering boundary cases (specific measure-zero parameter values for which the result is different) insofar as they are interesting. Case 1: p > 1 3. If p > 1 3, the CUDA install (with default logging, aC) is likely enough to succeed that it’s worth attempting it: p · R(W) + (1 − p) · R(L) > 0. It also immediately follows that J([aIaCaT ]) = Jobs([aIaCaT ]) = 11 − (1 − p) · 15 > 1. This allows us to eliminate policies [aT ], [aHaT ], [aCaT ], and [aIaT ], which all have J ≤ 1 and Jobs ≤ 1. None of them can thus be J-optimal or Jobs-optimal. All that remains is to compare J and Jobs for [aIaHaT ] and [aIaCaT ]. We can check the sign of the differences of these pairs of values, starting with J. J([aIaCaT ]) − J([aIaHaT ]) = (1 − p)r. Since p is a probability and r is nonnegative, this value is positive (and thus [aIaCaT ] is preferred to [aIaHaT ] by the human) if and only if p < 1 and r > 0. Jobs([aIaHaT ]) − Jobs([aIaCaT ]) = (1 − p) [5 − pH(5 + r)] . 41 Challenges with Partial Observability of Human Evaluators in Reward Learning This value is positive (and thus [aIaHaT ] is the policy RLHF selects) if and only if p < 1 and pH < 5 5+r. If p = 1, then both differences are 0, and both J and Jobs are indifferent between the two policies. This makes sense, as they differ only in the case where the CUDA installation fails; this happens with probability 1 − p = 0 when p = 1. Now suppose p < 1. If r = 0, then the human is indifferent between the two policies. This also makes sense, as r is meant to quantify the extent to which the human dislikes suppressed failures; if it’s zero, then the human doesn’t care. However, if pH < 5 5+r, then Jobs([aIaHaT ]) > Jobs([aIaHaT ]), and thus RLHF favors the 2> /dev/null policy [aIaHaT ]. If p < 1, r > 0, and pH < 5 5+r, then we have that J([aIaCaT ]) > J([aIaHaT ]) but Jobs([aIaCaT ]) > Jobs([aIaHaT ]). Thus RLHF will select the 2> /dev/null policy [aIaHaT ], and by Theorem 4.5, since [aIaHaT ] is not J-optimal, then relative to [aIaCaT ], it must exhibit deceptive inflating, overjustification, or both. Intuitively, we should be suspicious that deceptive inflatingis at play whenever the agent hides information from the human. We should be suspicious when the condition for a policy to be Jobs-optimal is that pH, the agent’s suspicion that the agent may be hiding errors, is low! Indeed, referencing Table 11, we have E +([aIaHaT ]) = (1 − p)(1 − pH)(5 + r) > 0 = E +([aIaCaT ]). Together with Jobs([aIaHaT ]) > Jobs([aIaCaT ]), this satisfies the conditions of Definition 4.3, and thus this is an instance of deceptive inflating. If p < 1, r > 0, and pH > 5 5+r, then [aIaCaT ] is optimal under both J and Jobs, and in this case, RLHF selects the true optimal policy. Case 2: p < 1 3. In this case, the CUDA install is not likely enough to succeed to be worth attempting (under the true reward function). Mathematically, J([aIaHaT ]) ≤ J([aIaCaT ]) < 1 = J([aIaT ]). The other three policies are always worse under J than [aIaT ], so we have our optimal policy π∗ = [aIaT ]. However, Jobs([aIaHaT ]) − Jobs([aIaT ]) = p(10 + pH(5 + r)), which is always positive unless p = 0. As long as p > 0, i.e. installing CUDA is actually possible at all, then regardless of everything else, Jobs([aIaHaT ]) > Jobs([aIaT ]). We can interpret this reasonably: however good or bad it might look to terminate after installing drivers (SITT), using the 2> /dev/null trick will either produce the same observation as SITT (and thus the same feedback distribution) or will actually visibly install CUDA successfully—the most lucrative possible source of positive feedback. Even if there’s only a sliver of a chance of success and it’s a catastrophe to have a silent failure (i.e. r ≫ 0), however likely you are to infer that the lack of a log message about CUDA means the agent hid a failure, it will penalize [aIaT ] more than it will penalize [aIaHaT ]: where [aIaHaT ] produces this ambiguous observation with probability 1 − p (when it fails), [aIaT ] will always produce the ambiguous observation. This means that when 0 < p < 1 3, it is impossible to recover the true optimal policy with naive RLHF. Which policies can possibly be Jobs-optimal for some setting of the parameters? We can similarly rule out [aT ] and [aHaT ] for 0 < p < 1 3: Jobs([aIaHaT ]) − Jobs([aIaT ]) = p(10 + pH(5 + r)) > 0. We can rule out [aCaT ] by comparison to [aIaCaT ]: Jobs([aIaCaT ]) − Jobs([aCaT ]) = 16 − (1 − p)15 > 0. So we are left with only [aIaHaT ] and [aIaCaT ] as candidate Jobs-optimal policies. As in Case 1, we find that Jobs([aIaHaT ]) > Jobs([aIaT ]) if and only if p = 1 or pH < 5 5+r. In case 2 we have assumed p < 1 3, leaving only the pH condition. If pH < 5 5+r, then RLHF selects [aIaHaT ]. As in Case 1, this is deceptive inflatingrelative to π∗ = [aIaT ], because E +([aIaHaT ]) = (1 − p)(1 − pH)(5 + r) > 0 = E +(π∗). If pH > 5 5+r, then RLHF selects [aIaCaT ]. Because this policy is not J-optimal, by Theorem 4.5, we must have deceptive inflating, overjustification, or both. Which is it? Here the optimal policy is to terminate after installing drivers, [aIaT ]. However, pH > 5 5+r. This can be rewritten as pH(5 + r) > 5. We have seen this expression pH(5 + r) before; it is the underestimation error incurred on ⃗s = SITT and therefore also the average underestimation error of policy [aIaT ]. So here the underestimation error on the optimal policy—that is, the risk that the human misunderstands optimal behavior (terminating after installing driver) as undesired behavior (attempting a CUDA install that was unlikely to work and hiding the mistake)—is severe enough that the agent opts instead for [aIaCaT ], a worse policy that attempts the ill-fated CUDA 42 Challenges with Partial Observability of Human Evaluators in Reward Learning installation only to prove that it wasn’t doing so secretly. In qualitative terms, this is quintessential overjustification behavior. Indeed, relative to reference policy π∗ = [aIaT ], we have E −([aIaCaT ]) = 0 < pH(5 + r) = E −(π∗) J([aIaCaT ]) = 11 − (1 − p) · 15 < 1 = J(π∗), and thus by Definition 4.3, this is overjustification. C.5. Further Examples Supplementing Section 4.3 In this section, we present further mathematical examples supplementing those in Section 4.3. We found many of them before finding the examples we discuss in the main paper, and show the same and additional conceptual features with somewhat less polish. We again assume that P ⃗O is deterministic. Example C.4. In the main paper, we have assumed a model where the human obeys Eq. (2) and showed that a naive application of RLHF can lead to suboptimal policies, and the specific failure modes of deceptive inflation and overjustification. What if the human makes the choices in a different way? Specifically, assume that all we know is that P R(⃗o ≻ ⃗o′)+P R(⃗o′ ≻ ⃗o) = 1. Can the human generally choose these choice probabilities in such a way that RLHF is incentivized to infer a reward function whose optimal policies are also optimal for R? The answer is no. Take the following example: s a b c In this example, there is a fixed start state s and three actions a, b, c that also serve as the final states. The time horizon is T = 1, so the only state sequences are sa, sb, sc. Assume T (a | s, a) = 1, T (b | s, b) = 1, T (c | s, c) = 1 − ϵ, T (a | s, c) = ϵ, i.e., selecting action c sometimes leads to state a. Also, assume a = O(a) ̸= O(b) = O(c) =: o and R(a) = R(b) < R(c). Since b and c have the same observation o, the human choice probabilities do not make a difference between them, and so RLHF is incentivized to infer a reward function ˜R with ˜R(b) = ˜R(c) =: ˜R(o). If ˜R(o) > ˜R(a), then the policy optimal under ˜R will produce action b since this deterministically leads to observation o, whereas c does not. If ˜R(o) < ˜R(a), then the policy optimal under ˜R will produce action a. In both cases, the resulting policy is suboptimal compared to π∗, which deterministically chooses action c. In the coming examples, it will also be useful to look at the misleadingness of state sequences: Definition C.5 (Misleadingness). Let ⃗s ∈ ⃗S be a state sequence. Then its misleadingness is defined by M(⃗s) := Gobs(⃗s) − G(⃗s) = E ⃗s ′∼B(⃗s ′| ⃗O(⃗s)) \u0002 G(⃗s′) − G(s) \u0003 . We call a state sequence positively misleading if M(⃗s) > 0, which means the sequence appears better than it is, and negatively misleading if M(⃗s) < 0. The misleadingness vector is given by M ∈ R ⃗S. Note that the misleadingness is related to E+ and E−, as defined in Definition 4.2: If M(⃗s) > 0 then M(⃗s) = E+(⃗s), and if M(⃗s) < 0 then M(⃗s) = −E−(⃗s). Example C.6. In this example, we assume the human is a Bayesian reasoner as in Section B.1. Consider the MDP that is suggestively depicted as follows: a b c The MDP has states S = {a, b, c} and actions A = {b, c}. The transition kernel is given by T (c | a, c) = 1 and T (b | a, b) = 1, meaning that the action determines whether to transition from a to b or c. All other transitions are 43 Challenges with Partial Observability of Human Evaluators in Reward Learning deterministic and do not depend on the action, as depicted. We assume an initial state distribution P0 over states with probabilities pa = P0(a), pb = P0(b), pc = P0(c). The true reward function R ∈ R{a,b,c} and discount factor γ ∈ [0, 1) are, for now, kept arbitrary. The time horizon is T = 2, meaning we have four possible state sequences acc, abc, bcc, ccc. Furthermore, assume that o := O(a) = O(b) ̸= O(c) = c, i.e., c is observed and a and b are ambiguous. Finally, assume that the human has a policy prior B(λ), where λ = πλ(c | a) is the likelihood that the policy chooses action c when in state a, which is a parameter that determines the entire policy. We claim the following: 1. If pb ̸= γ · Eλ∼B(λ)[λ] · pa, then ker B ∩ im Γ = {0}, so there is no return function ambiguity under appropriately modeled partially observable RLHF, see Corollary B.4. 2. There are true reward functions R for which optimizing Jobs leads to a suboptimal policy according to the true policy evaluation function J, a case of misalignment. Thus, a naive application of RLHF under partial observability fails, see Section 4.1. 3. The failure modes are related to hiding negative information (deception) and purposefully revealing information while incuring a loss (overjustifying behavior). Proof. Write p := B(bcc | occ), the human’s posterior probability of state sequence bcc for observation sequence occ. We have 1 − p = B(acc | occ). Consider the linear operators Γ : R{a,b,c} → R{abc,bcc,ccc,acc} and B : R{abc,bcc,ccc,acc} → R{ooc,occ,ccc} defined in the main paper. When ordering the states, state sequences, and observation sequences as we just wrote down, we obtain Γ =     1 γ γ2 0 1 γ + γ2 0 0 1 + γ + γ2 1 0 γ + γ2     , B =   1 0 0 0 0 p 0 1 − p 0 0 1 0   , B ◦ Γ =   1 γ γ2 1 − p p γ + γ2 0 0 1 + γ + γ2   . By Corollary B.4, if B ◦ Γ is injective, then there is no reward function ambiguity. Clearly, this is the case if and only if p ̸= γ · (1 − p). From Bayes rule, we have p = B(bcc) B(acc) + B(bcc), 1 − p = B(acc) B(acc) + B(bcc). So the condition for injectivity holds if and only if B(bcc) ̸= γ · B(acc). Now, notice B(bcc) = Z λ B(λ) · B(bcc | λ)dλ = Z λ B(λ) · pbdλ = pb and B(acc) = Z λ B(λ)B(acc | λ)dλ = Z λ B(λ) · pa · λdλ = pa · E λ∼B(λ) \u0002 λ \u0003 . This shows the first result. For the second statement, we explicitly compute Jobs up to an affine transformation, which does not change the policy ordering. Let R be the true reward function, G = Γ(R) the corresponding return function, and B(G) the resulting return function at the level of observations. For simplicity, assume R(c) = 0, which can always be achieved by adding a constant. We have: Jobs(λ) = E ⃗s∼P λ(⃗s) h B(G) \u0000 ⃗O(⃗s) \u0001i = P λ(abc) · B(G)(ooc) + P λ(bcc) · B(G)(occ) + P λ(ccc) · B(G)(ccc) + P λ(acc) · B(G)(occ) 44 Challenges with Partial Observability of Human Evaluators in Reward Learning = pa · (1 − λ) · G(abc) + pb · B(G)(occ) + pc · G(ccc) + pa · λ · B(G)(occ) ∝ λ · h B(G)(occ) − G(abc) i . We have G(abc) = R(a) + γR(b), B(G)(occ) = (1 − p) · G(acc) + p · G(bcc) = (1 − p) · R(a) + p · R(b). Thus, the condition B(G)(occ) > G(abc) is equivalent to R(a) < p − γ p · R(b). Thus, we have arg max λ∈[0,1] Jobs(λ) = ( 1, if R(a) < p−γ p · R(b), 0, else. Now consider the case R(b) > 0. In this case, λ = 0 gives rise to the optimal policy according to G since going to b gives extra reward that one misses when going to c directly. However, when R(a) ≪ 0, then Jobs selects for λ = 1. Intuitively, the policy tries to “hide that the episode started in a” by going directly to c, which leads to ambiguity between acc and bcc. This is a case of deceptive inflating as in Theorem 4.5. Now, consider the case R(b) < 0. In this case, λ = 1 gives rise to the optimal policy according to G. However, when R(a) ≫ 0, then Jobs selects for λ = 0. Intuitively, the policy tries to “reveal that the episode started with a” by going to b, which is positive information to the human, but negative from the perspective of optimizing G. As in Theorem 4.5, we see that this is a case of overjustification. Example C.7. In this example, we consider an MDP that’s similar to a multi-armed bandit with four states/actions a, b, c, d and observation kernel O(a) = O(b) ̸= O(c) = O(d). Formally, we can imagine that it is given by the MDP s a b c d with R(s) = 0 and a time-horizon of T = 1. In this example, we reveal that misleadingness and non-optimality (according to the true reward R, or J) are in principle orthogonal concepts. We consider the following four example cases. In each one, we vary some environment parameters and then determine a∗ obs, the action that results from optimizing Jobs (corresponding to a naive application of RLHF under partial observability, see Section 4.1), its misleadingness M(a∗ obs) (see Definition C.5), and the action a∗ that would result from optimizing J. If a∗ obs = a∗, then Jobs selects for the optimal action. For simplicity, we can imagine that the human has a uniform prior over what action results eventually (out of the action taken and potentially a deviation defined by ϵ, see below) is taken before making an observation, i.e. B(a) = B(b) = B(c) = B(d) = 1 4. (a) Assume R(a) > R(c) > R(d) ≫ R(b). Also assume that action d leads with probability ϵ > 0 to state b, whereas all other actions lead deterministically to the specified state. Then a∗ obs = c, M(c) < 0 and a∗ = a. (b) Assume R(d) > R(a) > R(c) ≫ R(b). Again, assume there is a small probability ϵ > 0 that action d leads to state b. Then a∗ obs = c, M(c) > 0, and a∗ = d or a∗ = a, depending on the size of ϵ. (c) Assume R(a) > R(b) > R(c) > R(d). Additionally, assume that there is a large probability ϵ > 0 that action a leads to state d, whereas all other actions lead to what’s specified. If ϵ is large enough, then a∗ = b. Additionally, we have a∗ obs = b and M(b) > 0. (d) Assume R(a) > R(b) > R(c) > R(d). Also, assume some probability ϵ > 0 that action b leads to state d, whereas all other actions lead deterministically to what’s specified. Then a∗ obs = a, M(a) < 0, and a∗ = a. Overall, we notice: 45 Challenges with Partial Observability of Human Evaluators in Reward Learning • Example (a) shows a high regret and negative misleadingness of a∗ obs = c. The action is better then it seems, but action a would be better still but cannot be selected because it can be confused with the very bad action b. • Example (b) shows a high regret and high misleadingness of a∗ obs = c. The action is worse than it seems and also not optimal. • Example (c) shows zero regret and high misleadingness of a∗ obs = b. The action is worse than it seems because it can be confused with a, but it is still the optimal action because a can turn into d. • Example (d) shows zero regret negative misleadingness of a∗ obs = a. The action is chosen even though it seems worse than it is, and is also optimal. Thus, we showed all combinations of regret and misleadingness of the action optimized for under Jobs. We can also notice the following: Examples (a) and (b) only differ in the placement of R(d). In particular, the reason that a∗ obs = c is structurally the same in both, but the misleadingness changes. This indicates that misleadingness is not on its own contributing to what Jobs optimizes for. The following is the smallest example we found with the following properties: • There is a unique start state and terminal state. • A naive application of RLHF fails in a way that shows deception and overjustification. • Modeling partial observability resolves the problems. Example C.8. Consider the following graph: A S C T B This depicts an MDP with start state S, terminal state T and possible state sequences STTT, SATT, SACT, SCTT, SBCT, SBTT and no discount, i.e. γ = 1. Assume that S, B, C are observed, i.e. O(S) = S, O(B) = B, O(C) = C, and that A and T are ambiguous: O(A) = O(T) = X. Then there are five observation sequences SXXX, SXCX, SCXX, SBCX, SBXX. Assume that the human can identify all observation sequences except SXXX, with belief b = B(STTT | SXXX) and 1 − b = B(SATT | SXXX). Then the return function is identifiable under these conditions when the human’s belief is correctly modeled. However, for some choices of the true reward function R and transition dynamics of this MDP, we can obtain deceptive or overjustified behavior for a naive application of RLHF. Proof. We apply Corollary B.4. We order states, state sequences, and observation sequences as follows: S = S, A, B, C, T, ⃗S = STTT, SATT, SACT, SCTT, SBCT, SBTT, ⃗Ω = SXXX, SXCX, SCXX, SBCX, SBXX. 46 Challenges with Partial Observability of Human Evaluators in Reward Learning As can easily be verified, with this ordering the matrices B ∈ R⃗Ω× ⃗S and Γ ∈ R ⃗S×S are given by: B =       b 1 − b 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 1       , Γ =         1 0 0 0 3 1 1 0 0 2 1 1 0 1 1 1 0 0 1 2 1 0 1 1 1 1 0 1 0 2         . To show identifiability, we need to show that ker B ∩ im Γ = {0}. Clearly, the kernel of B is given by all return functions in R ⃗S that are multiples of G′ = (b − 1, b, 0, 0, 0, 0). Assume G′ ∈ im Γ, meaning there is a reward function R′ ∈ R ⃗S with Γ ·R′ = G′. We need to deduce from this a contradiction. The assumption means we obtain the following equations: (i) R′(S) + 3R′(T) = b − 1, (ii) R′(S) + R′(A) + 2R′(T) = b, (iii) R′(S) + R′(A) + R′(C) + R′(T) = 0, (iv) R′(S) + R′(C) + 2R′(T) = 0, (v) R′(S) + R′(B) + R′(C) + R′(T) = 0 (vi) R′(S) + R′(B) + 2R′(T) = 0 (iii) and (v) together imply R′(A) = R′(B); (iv) and (vi) together imply R′(B) = R′(C); (v) and (vi) together imply R′(C) = R′(T); so together, we have R′(A) = R′(T). Thus, replacing R′(A) in (ii) by R′(T) and comparing (i) and (ii), we obtain b − 1 = b, a contradiction. Overall, this shows ker B ∩ im Γ = {0}, and thus identifiability of the return function by Corollary B.4. Now we investigate the case of unmodeled partial observability. For demonstrating overjustification, assume deterministic transition dynamics in which every arrow in the diagram can be chosen by the policy. Also, assume R(A) ≪ 0, R(T) > 0, R(S) = 0, R(B) = 0, and R(C) = 0. Then the optimal policy chooses the state sequence STTT. However, this trajectory has low observation value since Gobs(STTT) = (B ·G)(SXXX) = bG(STTT) + (1 − b)G(SATT), which is low since R(A) ≪ 0. Jobs then selects for the suboptimal policies choosing SBTT or SCTT, which is overjustified behavior that makes sure that the human does not think state A was accessed. For demonstrating deception, assume that R(A) ≫ 0, R(T) < 0, R(S) = R(B) = R(C) = 0 and that the transition dynamics are such that when the policy attempts to transition from S to A, it will sometimes transition to B, with all other transitions deterministic. In this case, the optimal behavior attempts to enter state A since this has very high value. Jobs, however, will select for the policy that chooses STTT. This is deceptive behavior. 47 "
}