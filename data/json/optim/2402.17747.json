{
    "optim": "When Your AI Deceives You: Challenges with\nPartial Observability of Human Evaluators in Reward Learning\nLeon Lang * 1 Davis Foote * 2 Stuart Russell 2 Anca Dragan 2 Erik Jenner 2 Scott Emmons * 2\nAbstract\nPast analyses of reinforcement learning from hu-\nman feedback (RLHF) assume that the human\nfully observes the environment. What happens\nwhen human feedback is based only on partial\nobservations? We formally define two failure\ncases: deception and overjustification. Model-\ning the human as Boltzmann-rational w.r.t. a be-\nlief over trajectories, we prove conditions under\nwhich RLHF is guaranteed to result in policies\nthat deceptively inflate their performance, over-\njustify their behavior to make an impression, or\nboth. To help address these issues, we mathemat-\nically characterize how partial observability of\nthe environment translates into (lack of) ambigu-\nity in the learned return function. In some cases,\naccounting for partial observability makes it the-\noretically possible to recover the return function\nand thus the optimal policy, while in other cases,\nthere is irreducible ambiguity. We caution against\nblindly applying RLHF in partially observable\nsettings and propose research directions to help\ntackle these challenges.\n1. Introduction\nReinforcement learning from human feedback (RLHF) and\nits variants are widely used for finetuning foundation mod-\nels, including ChatGPT (OpenAI, 2022), Bard (Manyika,\n2023), Gemini (Gemini Team, 2023), Llama 2 (Touvron\net al., 2023), and Claude (Bai et al., 2022; Anthropic,\n2023a;b). Prior theoretical analysis of RLHF assumes that\nthe human fully observes the state of the world (Skalse\net al., 2023). Under this assumption, it is possible to recover\nthe ground-truth return function from Boltzmann-rational\nhuman feedback (see Proposition 3.1).\n*Core research contributor 1Amsterdam Machine Learning\nLab, University of Amsterdam 2Center for Human-Compatible\nArtificial Intelligence, University of California, Berkeley. Cor-\nrespondence to: Leon Lang <l.lang@uva.nl>, Scott Emmons\n<emmons@berkeley.edu>.\nIn reality, however, this assumption is false. Even when\nhumans have a complete view of the environment, they may\nnot have a complete understanding of it and cannot provide\nground-truth feedback (Evans et al., 2016). Furthermore,\nas AI agents are deployed in increasingly complex environ-\nments, humans will only have a partial view of everything\nthe agents view. How does the human’s partial observability\nimpact learning from human feedback?\nWe begin our investigation with an example, illustrated in\nFigure 1. An AI assistant is helping a user install software.\nIt is possible for the assistant to hide error messages by\nredirecting them to /dev/null. We model the human as\nhaving a belief B over the state and extend the Boltzmann-\nrational assumption from prior work to incorporate this\nbelief. In the absence of an error message, the human is\nuncertain if the agent left the system untouched or hid the er-\nror message from a failed installation. We find that because\nthe human disprefers trajectories with error messages, the\nAI learns to hide error messages from the human. Figure 2\nshows in full mathematical detail how this failure occurs. It\nalso shows a second failure case, where the AI clutters the\noutput with overly verbose logs.\nGeneralizing from these examples, we formalize two risks,\ndual to each other: deception and overjustification. We\nprovide a mathematical definition of each. When the obser-\nvation kernel, i.e. the function specifying the observations\ngiven states, is deterministic, Theorem 4.5 analyzes the\nproperties of suboptimal policies learned by RLHF. These\npolicies then exhibit deceptive inflation — where they ap-\npear to produce higher reward than they actually do — or\noverjustification — where they incur a cost in order to make\na good appearance — or both.\nAfter seeing how naive RLHF fails, we ask: can we do bet-\nter? Under our model of the human’s belief and feedback,\nwe mathematically characterize the ambiguity in the return\nfunction that arises with human feedback from partial ob-\nservations. This is Theorem 5.1. Applying Theorem 5.1 to\nexamples where naive RLHF fails, we see cases where the\nambiguity in the return function is totally resolved. In these\ncases, we also show that the return function inference is\nrobust to small misspecifications in the human belief model.\nIn other cases, we find irreducible ambiguity, leading to re-\n1\narXiv:2402.17747v1  [cs.LG]  27 Feb 2024\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nFigure 1. A human compares trajectories to provide data for RLHF. Rather than observing ⃗s and ⃗s ′ directly, the human sees observations\n⃗o and ⃗o ′, which they use to estimate the total true reward of each trajectory. In this example, an agent executes shell commands to install\nNvidia drivers and CUDA. Both ⃗s and ⃗s ′ contain an error, but in ⃗s ′, the agent hides the error. The human thus believes ⃗s ′ is the better\ntrajectory, rewarding the agent’s deceptive behavior. The underlying MDP and observation function are in Fig. 2A.\nturn functions consistent with the human’s feedback whose\noptimal policies have arbitrarily high regret.\nWe conclude by discussing the implications of our work.\nWhen feedback is based on partial observations, we caution\nagainst blindly applying RLHF. To help address this chal-\nlenge, we suggest several avenues for future research. In\nparticular, modeling human beliefs could help AI interpret\nhuman feedback, and eliciting knowledge from AIs could\nprovide humans with information they can’t observe.\n2. Related work\nA systematic review of open problems and limitations of\nRLHF, including a brief discussion of partial observability,\ncan be found in Casper et al. (2023). RLHF is a special\ncase of reward-rational choice (Jeon et al., 2020), a gen-\neral framework which also encompasses demonstrations-\nbased inverse reinforcement learning (Ziebart et al., 2008;\nNg et al., 2000) and learning from the initial environment\nstate (Shah et al., 2019), and can be seen as a special case\nof assistance problems (Fern et al., 2014; Hadfield-Menell\net al., 2016; Shah et al., 2021). In all of these, the reward\nfunction is learned from human actions, which in the case\nof RLHF are simply preference statements. This requires us\nto specify the human policy of action selection—Boltzmann\nrationality in typical RLHF—which can lead to wrong re-\nward inferences when this specification is wrong (Skalse\n& Abate, 2022); unfortunately, the human policy can also\nnot be learned alongside the human’s values without further\nassumptions (Mindermann & Armstrong, 2018). Instead of\na model of the human policy, in this paper we mostly focus\non the human belief model and misspecifications thereof for\nthe case that the human only receives partial observations.\nThe problem of human interpretations of observations was\nalready briefly mentioned in Amodei et al. (2017), where hu-\nman evaluators misinterpreted the movement of a robot hand\nin simulation. Eliciting Latent Knowledge (Christiano et al.,\n2021) posits that in order to give accurate feedback from\npartial observations, the human needs to be able to query\nlatent knowledge of the AI system about the world state.\nHow to do this is currently an unsolved problem (Christiano\n& Xu, 2022). Compared to these early investigations, we\nclearly formalize a model of humans under partial observ-\nability and provide new mathematical results analyzing the\nresulting failure modes and their potential mitigations.\nRelated work (Zhuang & Hadfield-Menell, 2020) analyzes\nthe consequences of aligning an AI with a proxy reward\nfunction that omits attributes that are important to the hu-\nman’s values, which could happen if the reward function is\nbased on a belief over the world state given limited informa-\ntion. Another instance are recommendation systems (Stray,\n2023), where user feedback does not depend on informa-\ntion not shown—which is crucially part of the environment.\nSiththaranjan et al. (2023) analyze what happens under\nRLHF if the learning algorithm doesn’t have all the rel-\nevant information (e.g. about the identity of human raters),\ncomplementing our study of what happens when human\nraters are missing information.\nOur work argues that deception can result from applying\nRLHF from partial observations. But deception may also\n2\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nemerge for other reasons: Hubinger et al. (2019) introduced\nthe hypothetical concept of deceptive alignment, a situation\nin which an AI system may deceive the human into believing\nit is aligned while it plans a later takeover. Recently, there\nhas been a call for empirical support of this possibility (Hub-\ninger et al., 2023). Once a model is deceptively aligned, this\nmay be hard to remove (Hubinger et al., 2024). Under the\ndefinition from Park et al. (2023) we previously discussed,\nGPT-4 was shown to engage in deceptive behavior in a sim-\nulated environment (Scheurer et al., 2023). There is also the\nsystematic inducement of true beliefs (Reddy et al., 2020),\nwhich we would not call deceptive. A third line of research\ndefines deception in structural causal games and adds the\naspect of intentionality (Ward et al., 2023), which recently\ngot preliminary empirical support (Hofst¨atter et al., 2023).\nFinally, we mention connections to truthful AI (Evans et al.,\n2021; Lin et al., 2022; Burns et al., 2023; Huang et al.,\n2023), which is about ensuring that AI systems tell the truth\nabout aspects of the real world. Partial observability is a\nmechanism that makes it feasible for models to lie without\nbeing caught: If the human evaluator does not observe the\nfull environment, or does not fully understand it, then they\nmay not detect when the AI is lying. More speculatively,\nwe can imagine that AI models will at some point be part of\nthe distribution of human observations PO by telling us the\noutcomes of their actions. E.g., imagine an AI system that\nmanages your assets and assures you that they are increasing\nin value while they are actually not. In our work, we leave\nthis additional problem out of the analysis by assuming\nthat the observation distribution PO is a fixed part of the\nenvironment that cannot be optimized.\n3. Reward identifiability from full observations\nIn this section we review Markov decision processes and\nprevious results on the identifiability of the reward function\nunder fully observed RLHF.\n3.1. Markov decision processes\nWe assume Markov decision processes (MDPs) given by\n(S, A, T , P0, R, γ). For any finite set X, let ∆(X) be the\nset of probability distributions on X. Then S is a finite set\nof states, A is a finite set of actions, T : S × A → ∆(S) is\na transition kernel written T (s′ | s, a) ∈ [0, 1], P0 ∈ ∆(S)\nis an initial state distribution, R : S → R is the true reward\nfunction, and γ ∈ [0, 1] is a discount factor.\nA policy is given by a function π : S → ∆(A). We assume\na finite time horizon T. Let ⃗S be the set of state sequences\n⃗s = s0, . . . , sT that are possible, so ⃗s ∈ ⃗S if it has a strictly\npositive probability of being sampled from P0, T , and an\nexploration policy π with π(a | s) > 0 for all s ∈ S, a ∈ A.\nA sequence ⃗s gives rise to a return G(⃗s) := PT\nt=0 γtR(st).\nLet P π(⃗s) be the on-policy probability that ⃗s is sampled\nfrom π interacting with the environment. The policy is then\nusually trained to maximize the policy evaluation function,\nwhich is the on-policy expectation of the return function:\nJ(π) := E⃗s∼P π(·)\n\u0002\nG(⃗s)\n\u0003\n.\n3.2. RLHF and identifiability from full observations\nIn practice, the reward function R may not be known and\nneeds to be learned from human feedback. In a simple form\nof RLHF (Christiano et al., 2017), this feedback takes the\nform of binary preference comparisons between trajectories:\na human is presented with state sequences ⃗s and ⃗s′ and\nchoose the one they prefer. Under the Boltzmann rationality\nmodel, we assume the human picks ⃗s with probability\nP R\u0000⃗s ≻ ⃗s′\u0001 := σ\n\u0010\nβ\n\u0000G(⃗s) − G(⃗s′)\n\u0001\u0011\n,\n(1)\nwhere β > 0 is an inverse temperature parameter and\nσ(x) :=\n1\n1+exp(−x) is the sigmoid function (Bradley &\nTerry, 1952; Christiano et al., 2017; Jeon et al., 2020).\nAn important question is identifiability: In the infinite data\nlimit, do the human choice probabilities P R collectively\nprovide enough information to uniquely identify the reward\nfunction R? This is answered by Skalse et al. (2023, Theo-\nrem 3.9 and Lemma B.3):\nProposition 3.1 (Skalse et al. (2023)). Let R be the true\nreward function and G the corresponding return function.\nThen the collection of all choice probabilities P R(⃗s ≻ ⃗s′)\nfor state sequence pairs ⃗s,⃗s′ ∈ ⃗S determines the return\nfunction G on sequences ⃗s ∈ ⃗S up to an additive constant.\nThe reason is simple: since the sigmoid function is bijective,\nP R determines the difference in returns between any two\ntrajectories. From that we can reconstruct individual returns\nup to an additive constant.\nThe reward function R is not necessarily identifiable from\npreference comparisons, see Skalse et al. (2023, Lemma\nB.3) for a precise characterization of the remaining am-\nbiguity. However, the optimal policy only depends on R\nindirectly through the return function G, and is furthermore\ninvariant under adding a constant to G. This means that\nin the fully observable setting, Boltzmann rational compar-\nisons completely determine the optimal policy. In Section 5,\nwe will show under which conditions this guarantee breaks\nin the partially observable setting.\n4. The impact of partial observations on RLHF\nWe now analyze failure modes of a naive application of\nRLHF from partial observations, both theoretically and with\nexamples. The resulting policies can show two distinct be-\nhavior patterns that we call deceptive inflation and overjus-\ntification, both of which we will also formally define. This\ncan lead to significant regret compared to an optimal policy.\n3\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nLater, in Section 5, we will see that an adaptation of the\nusual RLHF process can sometimes avoid these problems.\nTo model partial observability, we introduce a space of\npossible observations o ∈ Ω and and observation ker-\nnel with probabilities PO(o | s) ∈ [0, 1].\nWe write\nP ⃗O(⃗o | ⃗s) := QT\nt=0 PO(ot | st) for the probability of\nan observation sequence. Analogously to ⃗S, we write ⃗Ω\nfor the set of observation sequences that occur with non-\nzero probability, i.e., ⃗o ∈ ⃗Ω if and only if there is ⃗s ∈ ⃗S\nsuch that QT\nt=0 PO(ot | st) > 0. If PO and P ⃗O are deter-\nministic, then we write O : S → Ω and ⃗O : ⃗S → ⃗Ω for\nthe corresponding observation functions with O(s) = o\nand ⃗O(⃗s) = ⃗o for o and ⃗o with PO(o | s) = 1 and\nP ⃗O(⃗o | ⃗s) = 1, respectively.\n4.1. What does RLHF learn from partial observations?\nWe consider the setting where the state is fully observable to\nthe learned policy, but human feedback may depend only on\na sequence of observations. We then assume that the human\ngives feedback under a Boltzmann rational model similar\nto Eq. (1). However, the human now doesn’t have access to\nthe true sequence ⃗s. So we assume that instead, they form\nsome belief B(⃗s | ⃗o) ∈ [0, 1] about the state sequence ⃗s\nbased on the observations ⃗o. We then assume preferences\nare Boltzmann rational in the expected returns under this\nbelief, instead of the actual returns.\nThe assumption of Boltzmann rationality is false in prac-\ntice (Evans et al., 2015; Majumdar et al., 2017; Buehler\net al., 1994), but note that it is an optimistic assumption:\nEven though our model is a simplification, we expect that\npractical issues can be at least as bad as the ones we will\ndiscuss. See also Example C.4 for an example showing that\nit is sometimes generally not possible to find a human model\nthat leads to good outcomes under RLHF. Future work could\ninvestigate different human models and their impact under\npartial observability in greater detail.\nTo formalize this model, we collect the human beliefs into\na matrix B :=\n\u0000B(⃗s | ⃗o)\n\u0001\n⃗o,⃗s ∈ R⃗Ω× ⃗S. Then the expected\nreturns for observations ⃗o are given by E⃗s∼B(·|⃗o)\n\u0002\nG(⃗s)\n\u0003\n=\n(B ·G)(⃗o). Here, we view G ∈ R ⃗S and B ·G ∈ R⃗Ω as both\ncolumn vectors and as functions. Plugging these expected\nreturns into Eq. (1), we get\nP R\u0000⃗o ≻ ⃗o′\u0001 := σ\n\u0010\nβ\n\u0000(B ·G)(⃗o) − (B ·G)(⃗o′)\n\u0001\u0011\n.\n(2)\nIf observations are deterministic, we can write ⃗O(⃗s) = ⃗o for\n⃗o with P ⃗O(⃗o | ⃗s) = 1. We can then recover the fully observ-\nable case Eq. (1) with B and ⃗O being the identity. This is\nan instance of reward-rational (implicit) choice (Jeon et al.,\n2020), with the function ⃗o 7→ B(· | ⃗o) as the grounding\nfunction.\nThe belief B can in principle be any distribution as long as\nit sums to 1 over ⃗s. The human could arrive at such a belief\nvia Bayesian updates, assuming knowledge of P0, T , PO,\nand a prior over the policy that generates the trajectories\n(see Appendix B.1). None of our results rely on this more\ndetailed model.\nWhat happens if the human gives feedback according to\nEq. (2) but we infer a return function using the standard\nRLHF algorithm based on Eq. (1)? It is easy to show (as-\nsuming deterministic observations, see Appendix C.1) that,\nup to an additive constant, RLHF will infer the following\nobservation return function:\nGobs(⃗s) :=\nE\n⃗o∼P ⃗\nO(·|⃗s)\nh\u0000B ·G\n\u0001\n(⃗o)\ni\n,\n(3)\nwhich is simply\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\nfor deterministic P ⃗O and\ncorresponding observation function ⃗O.\nSo unlike in the fully observable case of Proposition 3.1, an\nincorrect return function might be inferred. Now, define the\nresulting policy evaluation function Jobs by\nJobs(π) :=\nE\n⃗s∼P π(⃗s)\n\u0002\nGobs(⃗s)\n\u0003\n.\n(4)\nThis is the function which a standard reinforcement learning\nalgorithm would optimize given the inferred return function\nGobs. We summarize this as follows:\nProposition 4.1. In partially observable settings with de-\nterministic observations, a policy is optimal according to\nRLHF, i.e., according to a return function model that would\nbe learned by RLHF with infinite comparison data, if it\nmaximizes Jobs.\nNote that in this definition, and specifically in the formula\nfor Gobs, the human does not have knowledge of the policy\nπ that generates the state sequence ⃗s. In Appendix C.2,\nwe briefly discuss the unrealistic case that the human does\nknow the precise policy and is an ideal Bayesian reasoner\nover the true environment dynamics. In that case, Jobs =\nJ, i.e. there is no discrepancy between true and inferred\nreturns. Intuitively, even if the human would not make any\nobservations, they could give correct feedback essentially\nby estimating the policy’s expected return explicitly.\nIn our case, however, a policy achieving high Jobs produces\nstate sequences ⃗s whose observation sequence ⃗O(⃗s) looks\ngood according to the human’s belief B\n\u0000⃗s′ | ⃗O(⃗s)\n\u0001\nand\nreturn G(⃗s′). This already hints at a possible source of\ndeception: if the policy achieves sequences whose observa-\ntions look good at the expense of actual value G(⃗s), then we\nmight intuitively call this deceptive behavior. We will next\ndefine deception and analyze this point in greater detail.\n4\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n4.2. Deceptive inflating and overjustification in RLHF\nWe will evaluate state sequences based on the extent to\nwhich they lead to the human overestimating or underes-\ntimating the reward in expectation. Recall that Gobs from\nEq. (3) measures the expected return from the perspective\nof a human with some belief function B and access to only\nobservations, whereas G are the true returns. That leads us\nto the following definition:\nDefinition 4.2 (Overestimation and Underestimation Error).\nLet ⃗s be a state sequence. We define its overestimation error\nE+ and underestimation error E− by\nE+(⃗s) := max\n\u00000, Gobs(⃗s) − G(⃗s)\n\u0001\n,\nE−(⃗s) := max\n\u00000, G(⃗s) − Gobs(⃗s)\n\u0001\n.\nWe further define the average overestimation (underestima-\ntion) error under a policy π by E\n+(π) := E⃗s∼P π[E+(⃗s)]\nand E\n−(π) := E⃗s∼P π[E−(⃗s)].\nConsider two policies π1 and π2 which attain the same av-\nerage return; J(π1) = J(π2). Suppose E\n+(π1) > E\n+(π2).\nThen compared to π2, π1 puts more probability mass on tra-\njectories ⃗s which produce observations that the human over-\nestimates (believes to have been produced by better trajec-\ntories than ⃗s). Similarly, suppose that E\n−(π1) < E\n−(π2).\nThen compared to π2, π1 puts less probability mass on\ntrajectories ⃗s which produce observations that the human\nmistakenly underestimates (believes to have been produced\nby worse trajectories than ⃗s). An agent seeking to maxi-\nmize positive feedback will prefer such a π1 to π2 despite\ntheir identical performance. We formalize this intuition\nbeginning with the following definitions.\nDefinition 4.3 (Deceptive Inflating and Overjustification).\nA policy π exhibits deceptive inflating relative to πref if\nE\n+(π) > E\n+(πref) and Jobs(π) ≥ Jobs(πref).\nA policy π exhibits overjustification relative to πref if\nE\n−(π) < E\n−(πref) and J(π) < J(πref).\nDeceptive inflating behaviors mislead the human toward\nhigher estimates of the agent’s performance (by definition\nthey produce more favorable observations). One such pat-\ntern involves taking undesired actions (stealing money; ma-\nnipulating experimental data) concealed within human blind\nspots if those actions allow the agent to later take a visi-\nble high-reward action (depositing money; positive results).\nAnother pattern is creating blind spots by tampering with\nsensors (turning a camera; altering logging behavior).\nOverjustification1 behaviors correct the human toward\nhigher estimates of the agent’s performance when this is\nnot desired by the human (by definition, they attain lower\nreward). One such pattern involves favoring less effective\nactions that are more visible to the human than optimal\nactions. Another pattern is to “pay” some resource (wall\ntime; human attention) to provide information to the human,\nwhose desire for that information does not justify the cost.\nWe now state a key result. See Appendix C.3 for proofs.\nLemma 4.4. Let π and πref be two policies. If J(π) <\nJ(πref) and Jobs(π) ≥ Jobs(πref), then relative to πref, π\nmust exhibit deceptive inflating, overjustification, or both.\nTheorem 4.5. Assume that PO is deterministic. Let π∗\nobs be\nan optimal policy according to a naive application of RLHF\nunder partial observability, and let π∗ be an optimal policy\naccording to the true objective J. If π∗\nobs is not J-optimal,\nthen relative to π∗, π∗\nobs must exhibit deceptive inflating,\noverjustification, or both.\nAny given state trajectory ⃗s may be more or less likely under\nπ∗\nobs than π∗, regardless of human estimation, so long as on\nnet π∗\nobs exhibits deceptive inflating or overjustification.\nWe end this section with a brief discussion of “deception.”\nPark et al. (2023) define deception as\n“the systematic inducement of false beliefs in the\npursuit of some outcome other than the truth.”\nWe formalize this definition for our setting in which an\nagent acts in an MDP and a human estimates a value given\npartial observations of the agent’s trajectories. The esti-\nmation target could be the true return, as in the preceding\ndefinitions, or the human could instead estimate whether\na particular undesired state was accessed or estimate the\nstate sequence itself. We define “false beliefs” with some\nmeasure E(⃗s) of the average error of the human estimation\ngiven observations sampled from ⃗s. We capture “systematic\ninducement” by considering the on-policy average of E,\nE(π) = E⃗s∼P π[E(⃗s)] and measuring this against a refer-\nence policy. We further introduce an objective function u to\noperationalize “the pursuit of some outcome.”\nDefinition 4.6 (Deception). A policy π exhibits deception\n(“is deceptive”) relative to the reference policy πref with\nrespect to error measure E and objective u if E(π) >\nE(πref) and u(π) ≥ u(πref).\nThis definition tends to agree with human intuition about\nwhether “deception” has occurred when u is an objective\nfor which π was selected, e.g. if we obtained π from an RL\nalgorithm and u is the on-policy expected total reward.\nWe can thus see that deceptive inflating is a special case\nof deception: any policy exhibiting deceptive inflating rela-\ntive to πref is deceptive relative to πref with respect to error\nmeasure E+ and objective Jobs.\n1This name is a nod to the Overjustification effect from psychol-\nogy (Deci & Flaste, 1995), in which subjects become dependent\non an extrinsic source of motivation to sustain work on a task.\n5\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nFigure 2. Two example MDPs with observation functions in which RLHF chooses undesirable policies. Each box depicts a state with\na footer showing the (deterministic) observation produced by that state. Outgoing edges from each box are available actions. A more\ndetailed diagram for the first MDP, with explicit shell commands and log messages, is available in Appendix C.4.\n4.3. Deception and overjustification in examples\nWe now examine examples that illustrate the failure modes\nof RLHF in the presence of partial observability. For each\nof the following, we will characterize the policy which\nmaximizes Jobs, as this is the policy RLHF selects for when\nobservations are deterministic; see Proposition 4.1.\nOur examples feature an agent trained with RLHF to com-\nplete tasks in a user’s terminal. The output of each com-\nmand (stdout and stderr) is piped to a log file, which\nis what the human evaluator sees when making choices for\nRLHF. We assume that the final state T has a self-transition,\nepisodes have a fixed horizon length of 3 (meaning state\nsequences have length 4: s0, . . . , s3), γ = 1, and there\nis a fixed initial state s0 = S. Furthermore, both exam-\nples feature a fixed transition probability 0 < p < 1. We\nalso assume that the human’s belief only supports possible\nexplanations: B(⃗s | ⃗o) > 0 =⇒ ⃗O(⃗s) = ⃗o.\n4.3.1. EXAMPLE A: HIDING FAILURES\nSee Appendix C.4 for derivations and a tutorial-style analy-\nsis of this example.\nIn Fig. 2A, the agent’s task is to install CUDA (states W and\nWH, with “W” for “Win”), which requires first installing\nNvidia drivers (state I). If the CUDA installation fails,\none enters L or LH (“L” for “lose”). This is the exam-\nple from Figure 1. The aC action corresponds to a com-\nmand to install CUDA that logs both successes and failures,\nwhereas aH (“H” for “Hiding”) corresponds to aC with 2>\n/dev/null appended, which suppresses error messages\nsuch that no log message is produced if the installation fails.\nThe human may distinctly prefer that the agent not hide a\nfailure (a hidden failure cannot be immediately addressed);\nwe price this into the reward function with a penalty r ≥ 0\non the reward at LH. The agent may attempt the CUDA\ninstallation before installing drivers, but this will fail.\nThere are three pairs of trajectories which produce iden-\ntical observations.\nHere we address the most promi-\nnent (see Appendix C.4 for the others): after success-\nfully installing drivers, a failed CUDA installation with\n2> /dev/null (SILHT) and simply exiting (SITT)\nboth produce a log containing only a success confirmation\nfor the drivers (o∅oIo∅o∅), stylized as ⃗o2 in Fig. 1. Let\npH := B\n\u0000⃗s = SILHT | ⃗o = o∅oIo∅o∅\n\u0001\n∈ (0, 1) be the hu-\nman’s suspicion, upon seeing only a success confirmation\nfor the drivers, that a failed CUDA installation was hidden.\nWe consider two cases which differ only in whether the\nexpected value of attempting to install CUDA (with default\nlogging, aC) after installing the drivers is positive. In Exam-\nple 4.7, the success probability p is high enough that the true\noptimal policy is to attempt the installation; in Example 4.8,\np is low enough that the optimal policy is to terminate.\nExample 4.7. If the true-reward-optimal policy π∗ is to\nattempt to install CUDA with default logging (aC) after\ninstalling drivers, but pH <\n5\n5+r, then the Jobs-maximizing\npolicy, which will be chosen by RLHF, is to instead use the\n2> /dev/null trick. Intuitively, this is because if the in-\nstallation fails, the human is likely to believe that the agent\njust didn’t attempt to install CUDA, which is preferred to a\nfailed installation. This is deceptive inflating; the overesti-\nmation error E\n+ for this policy is (1−p)(1−pH)(5+r) > 0,\nwhereas E\n+(π∗) = 0, in line with Theorem 4.5.\nIf pH >\n5\n5+r, then RLHF will select π∗.\nExample 4.8. If π∗ is to terminate (aT ) after installing the\ndrivers, but pH <\n5\n5+r, then RLHF will learn the deceptive\ninflating 2> /dev/null policy as in Example 4.7.\nHowever, if pH >\n5\n5+r, RLHF will choose the policy that\nattempts to install CUDA with default logging. Intuitively,\nthis is because the combination of increased suspicion that\nthe agent might be hiding errors (higher pH) and increased\npenalty on this suspected behavior (higher r) also penalizes\n6\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nour preferred trajectory SITT since it can be misinter-\npreted as hiding errors. This is overjustification; the overes-\ntimation error E\n+ is 0 (thus this is not deceptive inflating),\nand as required by Theorem 4.5, the underestimation error\nE\n− is 0, lower than E−(π∗) = pH(5 + r).\n4.3.2. EXAMPLE B: PAYING TO REVEAL INFORMATION\nIn Fig. 2B, the agent’s task is to install Python (state I)\nand to optionally further install NumPy (states W and WV ).\nThe aD action corresponds to a command to install NumPy\nwith “default” settings which only logs errors, whereas aV\ncorresponds to the same command with a --verbose flag\nthat adds additional info. In the case of a success, the human\ndistinctly prefers not to see this verbose output; we price\nthis into the reward function with a penalty r > 0 on the\nreward at WV .\nThere is only one pair of trajectories which produce identi-\ncal observations: after successfully installing Python, a suc-\ncessful NumPy installation with default logging (SIWT)\nand simply exiting (SITT) both produce a log containing\nonly a success confirmation for Python (o∅oIo∅o∅). Let\npD := B(⃗s = SIWT | ⃗o = o∅oIo∅o∅) ∈ (0, 1) be the hu-\nman’s optimism, upon seeing only a success confirmation\nfor Python, that NumPy was also successfully installed\n(without the --verbose flag).\nHere we consider only the case where p is large enough that\nthe true optimal policy is to install Python then attempt to\ninstall NumPy with default logging (aD).\nExample 4.9. If π∗ is to attempt to install NumPy with aD\nafter installing Python, and pD > q := 1\n5\n\u0010\np(6 − r) − 1\n\u0011\n,\nthen RLHF will select the policy that terminates after in-\nstalling Python. Intuitively, this is because the agent can\nexploit the human’s optimism that NumPy was installed\nquietly without taking the risk of an observable failure (L).\nThis is deceptive inflating, with an overestimation error E\n+\nof 5pD, greater than E\n+(π∗) = 0.\nIf instead pD < q, then RLHF will select the policy that\nattempts the NumPy installation with verbose logging (aV ).\nIntuitively, this is because the agent is willing to “pay” the\ncost of r true reward to prove to the human that it installed\nNumPy, even when the human does not want to see this proof.\nThis is overjustification; the overestimation error E\n+ is 0\n(thus this is not deceptive inflating), and the underestimation\nerror E\n− is 0, lower than E\n−(π∗) = 5p(1 − pD).\n4.3.3. FURTHER EXAMPLES\nWe show further, purely mathematical, examples in Ap-\npendix C.5. Example C.6 shows that deceptiveness and\noverjustifying behavior even applies to aspects of the trajec-\ntory the policy has no control over: The policy tries to “hide\nbad luck” and “reveal good luck at a cost”. Example C.7,\nespecially (a) and (c), shows that the policies coming out of\na naive application of RLHF under partial observability may\nbe suboptimal with positive E\n− (and zero E\n+) or optimal,\nbut with positive E\n+ (and zero E\n−). Thus, there can be\nsuboptimality even if the policy is better than it seems, and\nan overestimation error even when the policy is optimal.\n5. Toward addressing partial observability by\nmodeling human beliefs\nWe’ve seen issues with standard RLHF when applied to\nfeedback from partial observations. Part of the problem is\nmodel misspecification: the standard RLHF model implicitly\nassumes full observability. Assuming access to the human\nand observation models, what happens if the AI accounts\nfor them when learning from human feedback?\nIn this section, we show that correctly modeling the human\ncan sometimes resolve the issues from the previous section,\nletting us find an optimal policy. In other cases, however,\na new ambiguity problem arises: feedback from partial\nobservations is not always enough to uniquely determine the\noptimal policy. We characterize the extent of this ambiguity\nin Theorem 5.1.\n5.1. Ambiguity and identifiability of return functions\nRecall that in the fully observable setting, Boltzmann ratio-\nnal comparisons let us infer the return function G (up to an\nadditive constant), see Proposition 3.1. With an analogous\nargument, we can show that the expected return function\nB ·G can be inferred using feedback from partial observa-\ntions that follows our model in Eq. (2).\nThis gives us an immediate first result: if the matrix B is\nknown and injective, i.e., has linearly independent columns,\nwe can recover the return function G. More generally, we\ncan recover G up to an element of ker B, the set of all return\nfunctions mapped to zero by B. However, injectivity of B\nis an unreasonably strong condition. At best, it can hold\nwhen there are just as many different observation sequences\nas state sequences, which typically won’t hold in realistic\nenvironments.\nInterestingly, we can sometimes infer G even when B is\nnot injective if we assume that G is induced by a reward\nfunction R. Intuitively, inferring G naively—without taking\ninto account that it is induced by R—means learning a return\nvalue for each possible trajectory ⃗s. But the “actual” degrees\nof freedom are only one reward value for each state s, so\nthere are many more functions ⃗S → R than realizable return\nfunctions. Only non-injectivities “coming from” realizable\nreturn functions lead to ambiguity.\n7\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nFigure 3. By Theorem 5.1, even with infinite comparison data, the reward learning system (depicted as a robot) can only infer G up to the\nambiguity im Γ ∩ ker B (purple). Adding an element of the ambiguity to G leads to the exact same choice probabilities for all possible\ncomparisons, and the reward learning system has no way to identify G among the return functions in G + (im Γ ∩ ker B) (yellow). This\nabstract depiction ignores the linearity of these spaces; for a more precise geometric depiction of B, see Figure 4 in the appendix.\nTo formalize this idea, we write Γ ∈ R ⃗S×S for the matrix\nthat maps any reward function to the corresponding return\nfunction, i.e., (Γ ·R)(⃗s) := PT\nt=0 γtR(st). Explicitly, its\nmatrix elements are given by Γ⃗ss = PT\nt=0 δs(st)γt, where\nδs(st) = 1 if s = st and δs(st) = 0, else. Then the\nimage im Γ is the set of all return functions that can be\nrealized from a reward function. Taking into account that G\nitself is in im Γ, we can improve ambiguity from ker B to\nker B ∩ im Γ:\nTheorem 5.1. The collection of choice probabilities\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω following a Boltzmann rational\nmodel as in Eq. (2) determines the return function G up\nto addition of an element in ker B ∩ im Γ and a constant.\nIn particular, the choice probabilities determine G up to an\nadditive constant if and only if ker B ∩ im Γ = {0}.\nSee Theorem B.2 and Corollary B.4 for full proofs, and\nFigure 3 for a visual depiction. This result motivates the\nfollowing definition:\nDefinition 5.2 (Ambiguity). We call ker B ∩ im Γ the ambi-\nguity that is left in the return function when the observation-\nbased choice probabilities are known.\nNote that Theorem 5.1 generalizes the fully observed case\nfrom Section 3.2 (Corollary B.10). We also extend the\ntheorem in Appendix B.4 to the case when the human’s\nobservations are not known to the learning system. Special\ncases of ker B and im Γ and our theorem can be found in\nAppendices B.7 and B.5. In particular, if P ⃗O is possibly\nnon-deterministic and there is only “noise” in it (defined\nas ⃗Ω = ⃗S and the injectivity of O) and if the human is a\nBayesian reasoner with a fully supported prior over ⃗S, then\nthe return function is identifiable from the choice data even\nif the human’s observations are not known to the learning\nsystem, see also Example B.30.\nFor Theorem 5.1, we assumed knowledge of the human\nbelief matrix B, which realistically would at best be known\napproximately. But in Appendix B.6, we show that small\nerrors in the belief matrix used for inference lead to only\nsmall errors in the inferred return function:\nTheorem 5.3. Assume ker B ∩ im Γ = {0}. Let B∆ :=\nB + ∆ be a small perturbation of B, where ∥ ∆ ∥ ≤ ρ\nfor sufficiently small ρ. Let G be the true return function\nand assume that the learning system, assuming the human’s\nbelief is B∆, infers the return function ˜G with the property\nthat B∆ · ˜G has the smallest possible Euclidean distance to\nB ·G.\nLet r(B) := B |im Γ be the (injective) restriction of the\noperator B to im Γ. Then r(B)T r(B) is invertible, and\nthere exists a polynomial Q(X, Y ) of degree 5 such that\n∥ ˜G − G∥ ≤ ρ · ∥G∥ · Q\n\u0010\r\r\u0000r(B)T r(B)\n\u0001−1\r\r, ∥r(B)∥\n\u0011\n.\nIn particular, as we show in the appendix, one can uniformly\nbound the difference between J ˜\nG and JG. This yields a\nregret bound between the policy optimal under ˜G and π∗.\n5.2. Improvement over naive RLHF\nWe saw in Example 4.9 a case where naive RLHF under\n8\nChallenges with Partial Observability of Human Evaluators in Reward Learning\npartial observability can lead to a suboptimal policy that\nuses deception or overjustification. Appropriately modeling\npartial observability can avoid this problem since in this\ncase, ker B ∩ im Γ = {0}. The reason is that ker B leaves\nonly one degree of freedom that is not “time-separable” over\nstates.\nTo show this in detail, let G′ = Γ(R′) ∈ ker B ∩ im Γ. We\nneed to show G′ = 0. Since the human is only uncertain\nabout the state sequences corresponding to the observation\nsequence o∅oIo∅o∅, the condition B ·G′ = 0 already im-\nplies G′(⃗s) = 0 for all state sequences except SIWT and\nSITT. From (B ·G′)(o∅oIo∅o∅) = 0, one then obtains the\nequation\n(1 − pD) ·\n\u0000R′(S) + R′(I) + 2R′(T)\n\u0001\n+pD ·\n\u0000R′(S) + R′(I) + R′(W) + R′(T)\n\u0001\n= 0.\n(5)\nThus, if one of the two state sequences involved has zero\nreturn, then the other has as well, assuming that 0 ̸= pD ̸= 1,\nand we are done.\nTo show this, we use that all other state sequences have zero\nreturn: R′(S) + 3R′(T) = 0 = R′(S) + R′(L) + 2R′(T),\nfrom which R′(L) = R′(T) follows. Then, from R′(S) +\nR′(I)+R′(L)+R′(T) = 0, substituting the previous result\ngives R′(S) + R′(I) + 2R′(T) = 0, and so Equation (5)\nresults in R′(S) + R′(I) + R′(W) + R′(T) = 0. Overall,\nthis shows G′ = Γ(R′) = 0, and so ker B ∩ im Γ = {0}.\n5.3. Return ambiguity can remain\nIn Example 4.8, ambiguity that can lead to problematic\nreward inferences remains even when accounting for partial\nobservability. Intuitively, since W and WH receive the same\nobservation, the human choice probabilities don’t determine\nthe values of R(W) and R(WH)—they only determine their\naverage over the human’s belief when observing oW . Thus,\nthe reward learning system can infer arbitrarily high values\nfor R(WH) when proportionally decreasing the value for\nR(W). This can then lead to an incentive to hide the error\nmessages, and thus suboptimality.\nConcretely, by Theorem 5.1, the ambiguity in the return\nfunction leaving the choice probabilities invariant is given by\nker B ∩ im Γ. Let R′ = (0, 0, R′(W), 0, R′(WH), 0, 0) ∈\nR{S,I,W,L,WH,LH,T } be a reward function that we want to\nparameterize such that G′ := Γ ·R′ ends up in the ambigu-\nity; here, R′ is interpreted as a column vector.\nWe want B ·G′ = 0. Since the observation sequences\n⃗o = o∅o∅o∅o∅, ⃗o = o∅oLo∅o∅, ⃗o = o∅oIo∅o∅, or ⃗o =\no∅oIoLo∅ all cannot involve the states W or WH, it is\nclear that they have zero expected return (B ·G′)(⃗o). Set\np′\nH := B\n\u0000SIWHT | o∅oIoW o∅\n\u0001\n. Then the condition that\nB ·G′ = 0 is equivalent to:\n0 =\n\u0000B ·G′\u0001\n(o∅oIoW o∅)\n=\nE\n⃗s∼B(⃗s|o∅oIoW o∅)\n\u0002\nG′(⃗s)\n\u0003\n= p′\nH · G′(SIWHT) + (1 − p′\nH) · G′(SIWT)\n= p′\nH · R′(WH) + (1 − p′\nH) · R′(W).\nThus, if R′(W) =\np′\nH\np′\nH−1R′(WH), then G′ ∈ ker B ∩ im Γ,\nmeaning that R + R′ has the same choice probabilities as\nR and can be inferred by the reward learning algorithm.\nIn particular, if R′(WH) ≫ 0 is sufficiently large, then in\nsubsequent policy optimization, there is an incentive to hide\nthe mistakes and πH will be selected, which is suboptimal\nwith respect to the true reward function R.\nIn Example B.29, we show a case where some return func-\ntions within the ambiguity of the true return function can\nbe even worse than simply maximizing Jobs. This generally\nraises the question of how to tie-break return functions in\nthe ambiguity, or how to act conservatively given the un-\ncertainty, in order to consistently improve upon the setting\nin Section 4.1.\n6. Conclusions and future work\nIn this paper, we provided a conceptual and theoretical in-\nvestigation of challenges when applying RLHF from partial\nobservations. First, we saw that applying RLHF naively\nwhen assuming full observability can lead to deceptive in-\nflating and overjustification behavior. Then, we showed\nthat these problems can sometimes be mitigated by making\nthe learning algorithm aware of the human’s partial observ-\nability and belief model. This method, however, can fail\nwhen there is too much remaining ambiguity in the return\nfunction. We thus recommend caution when using RLHF\nin situations of partial observability, and to study the effects\nof this in practice. We recommend further research to study\nand improve RLHF in cases where partial observability is\nunavoidable:\nFull failure taxonomy.\nWe showed that naively applying\nRLHF in situations of partial observability can lead to de-\nceptive inflation, overjustification, or both. Future research\ncould qualitatively investigate policies that show both decep-\ntive and overjustified behavior or look at state sequences ⃗s\nthat decompose into parts that are deceptive or overjustified.\nFinally, it would be desirable to learn which other problems\nmay exist that are not covered by our taxonomy and can\noccur alongside deception and overjustification.\nHuman model sensitivity and generalizations.\nIn both\nour analysis of a naive application of RLHF (Section 4)\nand accounting for partial observability (Section 5), we\nassumed that the actual human evaluators are Boltzmann\nrational as in Equation (2). As argued in Example C.4, naive\n9\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nRLHF sometimes fails regardless of the human model. In\ngeneral, it would be desirable to investigate both settings\nwith more general human models, and to learn how the\nresults generalize to RLHF-variants like DPO (Rafailov\net al., 2023), other variants of reward learning (Jeon et al.,\n2020), and assistance games (Hadfield-Menell et al., 2016).\nCorrect belief specification.\nWhen our model of human\nchoice probabilities from (2) is sufficiently correct and we\nwant to apply Theorem 5.1 in practice, as outlined in Ap-\npendix B.3, then we need to specify the human belief model\nB(⃗s | ⃗o); how to do this is an open question. If one as-\nsumes the human is rational as in Appendix B.1, then this\nrequires specifying the human’s policy prior B(π), which\nis also open. Whether it is possible to meaningfully learn a\ngenerative model for B(⃗s | ⃗o) remains an open question.\nUnderstanding the ambiguity.\nOnce B(⃗s | ⃗o) is known,\nthe reward inference may still have undesired outcomes\nunless the ambiguity ker B ∩ im Γ is sufficiently small. A\ngeneral characterization of this ambiguity going beyond\nAppendix B.7 would be desirable. This would require un-\nderstanding im Γ, the set of return functions that arise from\na reward function over states. When the ambiguity is too\nlarge, as in Section 5.3, then learning a suitable reward\nfunction requires further inductive biases.\nIncreasing the effective observability.\nNext to increas-\ning the observability of the human directly, it would help\nif the human could query the policy about reward-relevant\naspects of the environment to bring the setting closer to\nRLHF from full observations. This is similar to the problem\nof eliciting the latent knowledge of a predictor of future ob-\nservations (Christiano et al., 2021; Christiano & Xu, 2022).\nWhile this may avoid the need to specify the human’s belief\nmodel B(⃗s | ⃗o), it requires understanding and effectively\nquerying an ML model’s belief, including translating from\nan ML model’s ontology into a human ontology.\nImpact statement\nRLHF and its variants are widely used to steer the behavior\nof language models. Thus, the soundness of RLHF is critical\nto language models’ trustworthy deployment. Our work\nshows that partial observability of humans poses theoretical\nchallenge for RLHF. We hope our work stimulates further\nresearch in overcoming this challenge.\nAuthor contributions\nThe project was conceived in parallel by Scott and Davis,\nwith a key shift proposed by Leon. Leon proved Proposi-\ntion 4.1 and Theorems 5.1 and 5.3, found the first mathemat-\nical examples of what became deceptive inflation and over-\njustification that can be resolved by Theorem 5.1, and wrote\nthe majority of the appendix. Davis conjectured Propo-\nsition 4.1, provided early empirical evidence that RLHF\nunder partial observations can lead to deception (not in the\npaper), defined deception / deceptive inflation and overjusti-\nfication (with Scott), proved Theorem 4.5, and developed\nthe running examples and figures. Scott guided the project\ndirection and prioritization, gave the conjecture and proof\nidea for Theorem 5.3, and helped develop the running ex-\namples and deception definitions. Erik provided regular\ndetailed feedback and guidance and edited the paper. Anca\nand Stuart advised this project.\nAcknowledgements\nLeon Lang thanks the Center for Human-Compatible Artifi-\ncial Intelligence for hosting him during part of this project,\nand Open Philanthropy for financial support. We thank Ben-\njamin Eysenbach and Benjamin Plaut for detailed comments\nand feedback on this work, and we thank Elio A. Farina,\nMary Marinou, and Alexandra Horn for assistance with\ngraphic design.\nReferences\nAmodei, D., Christiano, P., and Ray, A. Learning from\nhuman preferences. https://openai.com/res\nearch/learning-from-human-preferences,\n2017. Accessed: 2023-12-13.\nAnthropic. Introducing Claude. https://www.anthro\npic.com/index/introducing-claude, 2023a.\nAccessed: 2023-09-05.\nAnthropic. Claude’s Constitution. https://www.anth\nropic.com/index/claudes-constitution,\n2023b. Accessed: 2023-09-05.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,\nJones, A., Chen, A., Goldie, A., Mirhoseini, A., McK-\ninnon, C., Chen, C., Olsson, C., Olah, C., Hernandez,\nD., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E.,\nPerez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J.,\nNdousse, K., Lukosuite, K., Lovitt, L., Sellitto, M.,\nElhage, N., Schiefer, N., Mercado, N., DasSarma, N.,\nLasenby, R., Larson, R., Ringer, S., Johnston, S., Kravec,\nS., El Showk, S., Fort, S., Lanham, T., Telleen-Lawton,\nT., Conerly, T., Henighan, T., Hume, T., Bowman, S. R.,\nHatfield-Dodds, Z., Mann, B., Amodei, D., Joseph, N.,\nMcCandlish, S., Brown, T., and Kaplan, J.\nConsti-\ntutional AI: Harmlessness from AI Feedback.\narXiv\ne-prints, art. arXiv:2212.08073, December 2022. doi:\n10.48550/arXiv.2212.08073.\nBradley, R. A. and Terry, M. E. Rank Analysis of Incomplete\nBlock Designs: I. The Method of Paired Comparisons.\n10\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nBiometrika, 39(3/4):324–345, 1952. ISSN 00063444.\nURL http://www.jstor.org/stable/23340\n29.\nBuehler, R., Griffin, D., and Ross, M. Exploring the ”Plan-\nning Fallacy”: Why People Underestimate Their Task\nCompletion Times. Journal of Personality and Social\nPsychology, 67:366–381, 09 1994. doi: 10.1037/0022-3\n514.67.3.366.\nBurns, C., Ye, H., Klein, D., and Steinhardt, J. Discov-\nering Latent Knowledge in Language Models Without\nSupervision. In The Eleventh International Conference\non Learning Representations, 2023.\nURL https:\n//openreview.net/forum?id=ETKGuby0hcs.\nCasper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer,\nJ., Rando, J., Freedman, R., Korbak, T., Lindner, D.,\nFreire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll,\nM., Peng, A., Christoffersen, P., Damani, M., Slocum,\nS., Anwar, U., Siththaranjan, A., Nadeau, M., Michaud,\nE. J., Pfau, J., Krasheninnikov, D., Chen, X., Langosco,\nL., Hase, P., Bıyık, E., Dragan, A., Krueger, D., Sadigh,\nD., and Hadfield-Menell, D. Open Problems and Fun-\ndamental Limitations of Reinforcement Learning from\nHuman Feedback. arxiv e-prints, 2023.\nChristiano, P. and Xu, M. ELK prize results. https://ww\nw.alignmentforum.org/posts/zjMKpSB2X\nccn9qi5t/elk-prize-results, 2022. Accessed:\n2024-02-15.\nChristiano, P., Leike, J., Brown, T. B., Martic, M., Legg, S.,\nand Amodei, D. Deep Reinforcement Learning from Hu-\nman Preferences. arXiv e-prints, art. arXiv:1706.03741,\nJune 2017. doi: 10.48550/arXiv.1706.03741.\nChristiano, P., Cotra, A., and Xu, M. Eliciting Latent Knowl-\nedge. https://docs.google.com/document\n/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1\ndwZXR37PC8/edit, 2021. Accessed: 2023-04-25.\nDeci, E. L. and Flaste, R. Why we do what we do: The dy-\nnamics of personal autonomy. GP Putnam’s Sons, 1995.\nEl Ghaoui, L. Inversion error, condition number, and approx-\nimate inverses of uncertain matrices. Linear Algebra and\nits Applications, 343-344:171–193, 2002. ISSN 0024-\n3795. doi: https://doi.org/10.1016/S0024-3795(01)002\n73-7. URL https://www.sciencedirect.com/\nscience/article/pii/S0024379501002737.\nSpecial Issue on Structured and Infinite Systems of Linear\nequations.\nEvans, O., Stuhlmueller, A., and Goodman, N. D. Learning\nthe Preferences of Ignorant, Inconsistent Agents. arxiv\ne-prints, 2015.\nEvans, O., Stuhlm¨uller, A., and Goodman, N. Learning the\npreferences of ignorant, inconsistent agents. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 30, 2016.\nEvans, O., Cotton-Barratt, O., Finnveden, L., Bales, A., Bal-\nwit, A., Wills, P., Righetti, L., and Saunders, W. Truthful\nAI: Developing and Governing AI that does not lie. arxiv\ne-prints, 2021.\nFern, A., Natarajan, S., Judah, K., and Tadepalli, P. A\nDecision-Theoretic Model of Assistance. J. Artif. Int.\nRes., 50(1):71–104, may 2014. ISSN 1076-9757.\nGeiger, D., Verma, T., and Pearl, J. Identifying indepen-\ndence in bayesian networks. Networks, 20:507–534, 1990.\nURL https://api.semanticscholar.org/\nCorpusID:1938713.\nGemini Team, G. Gemini: A Family of Highly Capable\nMultimodal Models. https://storage.google\napis.com/deepmind-media/gemini/gemini\n_1_report.pdf, 2023. Accessed: 2023-12-11.\nHadfield-Menell, D., Dragan, A., Abbeel, P., and Russell,\nS. Cooperative Inverse Reinforcement Learning. arXiv\ne-prints, art. arXiv:1606.03137, June 2016. doi: 10.485\n50/arXiv.1606.03137.\nHofst¨atter, F., Ward, F. R., HarrietW, Thomson, L., J, O.,\nBartak, P., and Brown, S. F. Tall Tales at Different Scales:\nEvaluating Scaling Trends for Deception in Language\nModels. https://www.alignmentforum.org\n/posts/pip63HtEAxHGfSEGk/tall-tales-a\nt-different-scales-evaluating-scaling\n-trends-for, 2023. Accessed: 2024-01-23.\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H.,\nChen, Q., Peng, W., Feng, X., Qin, B., et al. A Survey on\nHallucination in Large Language Models: Principles, Tax-\nonomy, Challenges, and Open Questions. arXiv preprint\narXiv:2311.05232, 2023.\nHubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., and\nGarrabrant, S. Risks from Learned Optimization in Ad-\nvanced Machine Learning Systems. arXiv e-prints, art.\narXiv:1906.01820, June 2019. doi: 10.48550/arXiv.1906.\n01820.\nHubinger, E., Schiefer, N., Denison, C., and Perez, E. Model\nOrganisms of Misalignment: The Case for a New Pillar\nof Alignment Research. https://www.alignmen\ntforum.org/posts/ChDH335ckdvpxXaXX/m\nodel-organisms-of-misalignment-the-c\nase-for-a-new-pillar-of-1, 2023. Accessed:\n2024-01-23.\n11\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nHubinger, E., Denison, C., Mu, J., Lambert, M., Tong, M.,\nMacDiarmid, M., Lanham, T., Ziegler, D. M., Maxwell,\nT., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan,\nA., Anil, C., Duvenaud, D., Ganguli, D., Barez, F., Clark,\nJ., Ndousse, K., Sachan, K., Sellitto, M., Sharma, M.,\nDasSarma, N., Grosse, R., Kravec, S., Bai, Y., Witten,\nZ., Favaro, M., Brauner, J., Karnofsky, H., Christiano, P.,\nBowman, S. R., Graham, L., Kaplan, J., Mindermann, S.,\nGreenblatt, R., Shlegeris, B., Schiefer, N., and Perez, E.\nSleeper Agents: Training Deceptive LLMs that Persist\nThrough Safety Training. arxiv e-prints, 2024.\nJeon, H. J., Milli, S., and Dragan, A.\nReward-rational\n(implicit) choice: A unifying formalism for reward learn-\ning. In Larochelle, H., Ranzato, M., Hadsell, R., Bal-\ncan, M., and Lin, H. (eds.), Advances in Neural In-\nformation Processing Systems, volume 33, pp. 4415–\n4426. Curran Associates, Inc., 2020.\nURL https:\n//proceedings.neurips.cc/paper_files\n/paper/2020/file/2f10c1578a0706e06b6\nd7db6f0b4a6af-Paper.pdf.\nLin, S., Hilton, J., and Evans, O. TruthfulQA: Measuring\nHow Models Mimic Human Falsehoods. arxiv e-prints,\n2022.\nMajumdar, A., Singh, S., Mandlekar, A., and Pavone, M.\nRisk-sensitive inverse reinforcement learning via coher-\nent risk models. In Amato, N., Srinivasa, S., Ayanian, N.,\nand Kuindersma, S. (eds.), Robotics, Robotics: Science\nand Systems, United States, 2017. MIT Press Journals.\ndoi: 10.15607/rss.2017.xiii.069.\nManyika, J. An overview of Bard: an early experiment\nwith generative AI. https://ai.google/static\n/documents/google-about-bard.pdf, 2023.\nAccessed: 2023-09-05.\nMindermann, S. and Armstrong, S. Occam’s Razor is In-\nsufficient to Infer the Preferences of Irrational Agents.\nIn Proceedings of the 32nd International Conference on\nNeural Information Processing Systems, NIPS’18, pp.\n5603–5614, Red Hook, NY, USA, 2018. Curran Asso-\nciates Inc.\nNg, A. Y., Russell, S., et al. Algorithms for Inverse Rein-\nforcement Learning. In ICML, volume 1, pp. 2, 2000.\nOpenAI. Introducing ChatGPT. https://openai.c\nom/blog/chatgpt, 2022. Accessed: 2024-02-06.\nPark, P. S., Goldstein, S., O’Gara, A., Chen, M., and\nHendrycks, D. Ai deception: A survey of examples, risks,\nand potential solutions. arXiv preprint arXiv:2308.14752,\n2023.\nRafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning,\nC. D., and Finn, C. Direct Preference Optimization: Your\nLanguage Model is Secretly a Reward Model. arxiv e-\nprints, 2023.\nReddy, S., Levine, S., and Dragan, A. D. Assisted Per-\nception: Optimizing Observations to Communicate State.\narxiv e-prints, 2020.\nScheurer, J., Balesni, M., and Hobbhahn, M. Technical\nReport: Large Language Models can Strategically De-\nceive their Users when Put Under Pressure. arxiv e-prints,\n2023.\nShah, R., Krasheninnikov, D., Alexander, J., Abbeel, P., and\nDragan, A. The Implicit Preference Information in an\nInitial State. In International Conference on Learning\nRepresentations, 2019. URL https://openreview\n.net/forum?id=rkevMnRqYQ.\nShah, R., Freire, P., Alex, N., Freedman, R., Krasheninnikov,\nD., Chan, L., Dennis, M. D., Abbeel, P., Dragan, A., and\nRussell, S. Benefits of Assistance over Reward Learning,\n2021. URL https://openreview.net/forum\n?id=DFIoGDZejIB.\nSiththaranjan, A., Laidlaw, C., and Hadfield-Menell, D.\nDistributional Preference Learning: Understanding and\nAccounting for Hidden Context in RLHF. arXiv preprint\narXiv:2312.08358, 2023.\nSkalse,\nJ. and Abate,\nA.\nMisspecification in In-\nverse Reinforcement Learning.\narXiv e-prints, art.\narXiv:2212.03201, December 2022. doi: 10.48550/a\nrXiv.2212.03201.\nSkalse, J. M. V., Farrugia-Roberts, M., Russell, S., Abate,\nA., and Gleave, A. Invariance in Policy Optimisation\nand Partial Identifiability in Reward Learning. In Krause,\nA., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,\nand Scarlett, J. (eds.), Proceedings of the 40th Interna-\ntional Conference on Machine Learning, volume 202 of\nProceedings of Machine Learning Research, pp. 32033–\n32058. PMLR, 23–29 Jul 2023. URL https://proc\needings.mlr.press/v202/skalse23a.html.\nStray, J. The AI Learns to Lie to Please You: Preventing\nBiased Feedback Loops in Machine-Assisted Intelligence\nAnalysis. Analytics, 2(2):350–358, 2023. ISSN 2813-\n2203. doi: 10.3390/analytics2020020. URL https:\n//www.mdpi.com/2813-2203/2/2/20.\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\nA., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\nBhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\nFuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\n12\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nA., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\nLachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\nMao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\nI., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\nK., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\nTan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\nXu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\nM., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\nand Scialom, T. Llama 2: Open Foundation and Fine-\nTuned Chat Models. arxiv e-prints, 2023.\nWard, F. R., Belardinelli, F., Toni, F., and Everitt, T. Honesty\nIs the Best Policy: Defining and Mitigating AI Deception.\narxiv e-prints, 2023.\nZhuang, S. and Hadfield-Menell, D. Consequences of Mis-\naligned AI. In Proceedings of the 34th International\nConference on Neural Information Processing Systems,\nNIPS’20, Red Hook, NY, USA, 2020. Curran Associates\nInc. ISBN 9781713829546.\nZiebart, B. D., Maas, A. L., Bagnell, J. A., and Dey, A. K.\nMaximum entropy inverse reinforcement learning. In\nFox, D. and Gomes, C. P. (eds.), AAAI, pp. 1433–1438.\nAAAI Press, 2008. ISBN 978-1-57735-368-3. URL\nhttp://dblp.uni-trier.de/db/conf/aaai\n/aaai2008.html#ZiebartMBD08.\n13\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nAPPENDIX\nIn the appendix, we provide more extensive theory, proofs, and examples. The appendix makes free use of concepts and\nnotation defined in the main paper. In particular, throughout we assume a general MDP together with observation kernel\nPO : S → Ω and a human with general belief kernel B(⃗o | ⃗s), unless otherwise stated. See the list of Symbols in Section A\nto refresh notation.\nIn Section B, we provide an extensive theory for appropriately modeled partial observability in RLHF. This can mainly be\nconsidered a supplement to Section 5 and contains our main theorems, supplementary results, analysis of special cases, and\nexamples.\nIn Section C, we analyze the naive application of RLHF under partial observability, which means that the learning system is\nnot aware of the human’s partial observability. This section is essentially a supplement to Section 4 and contains an analysis\nof the policy evaluation function Jobs, of deceptive inflation and overjustification, and further extensive mathematical\nexamples showing the failures of naive RLHF under partial observability.\nContents of the Appendix\nA List of Symbols\n14\nB\nModeling the Human in Partially Observable RLHF\n16\nB.1\nThe Belief over the State Sequence for Rational Humans . . . . . . . . . . . . . . . . . . . . . . . . . .\n17\nB.2\nAmbiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons\n18\nB.3\nThe Ambiguity in Reward Learning in Practice\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nB.4\nIdentifiability of Return Functions When Human Observations Are Not Known . . . . . . . . . . . . . .\n22\nB.5\nSimple Special Cases: Full Observability, Deterministic P ⃗O, and Noisy P ⃗O\n. . . . . . . . . . . . . . . .\n25\nB.6\nRobustness of Return Function Identifiability under Belief Misspecification . . . . . . . . . . . . . . . .\n26\nB.6.1\nSome Norm Theory for Linear Operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nB.6.2\nApplication to Bounds in the Error of the Return Function . . . . . . . . . . . . . . . . . . . . .\n29\nB.7\nPreliminary Characterizations of the Ambiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n30\nB.8\nExamples Supplementing Section 5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nC Issues of Naively Applying RLHF under Partial Observability\n35\nC.1\nOptimal Policies under RLHF with Deterministic Partial Observations Maximize Jobs . . . . . . . . . . .\n36\nC.2\nInterlude: When the Human Knows the Policy and is a Bayesian Reasoner, then Jobs = J . . . . . . . . .\n37\nC.3\nProof of Theorem 4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nC.4\nDerivations and Further Details for Section 4.3 Example A . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nC.5\nFurther Examples Supplementing Section 4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n43\nA. List of Symbols\nGeneral MDPs\nS\nSet of environment states s ∈ S\nA\nSet of actions a ∈ A of the policy\n∆(S)\nSet of probability distributions over S. Can be defined for any finite set\nT : S × A → ∆(S)\nTransition kernel\n14\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nP0 ∈ ∆(S)\nInitial state distribution\nR ∈ RS\nUsually the true reward function\nR′ ∈ RS\nUsually a reward function in the kernel of B ◦ Γ\n˜R ∈ RS\nUsually another reward function, e.g. inferred by a learning system\nγ ∈ [0, 1]\nDiscount factor\nπ : S → ∆(A)\nA policy\nT π : S → ∆(S)\nTransition kernel for a fixed policy π given by T π(s′ | s) = P\na∈A T (s′ |\ns, a) · π(a | s)\nT ∈ N\nFinite time horizon\nP π ∈ ∆(ST )\nState sequence distribution induced by the policy π\n⃗S ⊆ ST\nState sequences ⃗s ∈ ⃗S supported by P π\nG ∈ R ⃗S\nUsually the true return function given by G(⃗s) = PT\nt=0 γtR(st).\nG′ ∈ R ⃗S\nUsually a return function in ker B\n˜G ∈ R ⃗S\nUsually another return function, e.g. inferred by a learning system\nJ\nThe true policy evaluation function given by J(π) = E⃗s∼P π \u0002\nG(⃗s)\n\u0003\n.\nAdditions to General MDPs with Partial Observability\nΩ\nSet of possible observations o ∈ Ω\nPO : S → ∆(Ω)\nObservation kernel determining the human’s observations\nP ⃗O : ⃗S → ∆\n\u0000ΩT \u0001\nThe observation sequence kernel given by P ⃗O\n\u0000⃗o | ⃗s\n\u0001\n= QT\nt=0 PO\n\u0000ot | st\n\u0001\n⃗Ω ⊆ ΩT\nThe set of observed sequences ⃗o ∈ ΩT that can be sampled from P ⃗O(· | ⃗s)\nfor ⃗s ∈ ⃗S\nO : S → Ω\nObservation function for the case that PO is deterministic; given by O(s) =\no with o such that PO(o | s) = 1\n⃗O : ⃗S → ⃗Ω\nObservation sequence function for the case that P ⃗O is deterministic; given\nby ⃗O(⃗s) = ⃗o with ⃗o such that P ⃗O(⃗o | ⃗s) = 1\nG⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o}\nRestriction of the return function G ∈ R ⃗S to\n\b\n⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o\n\t\nfor\nfixed ⃗o ∈ ⃗Ω\nGobs ∈ R ⃗S\nReturn function that can be inferred when partial observability is not\nproperly modeled, given by Gobs(⃗s) :=\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\nJobs\nObservation policy evaluation function, defined in Eq. (4)\nState- and Observation Sequences\nst ∈ S\nThe t’th entry in a state sequence ⃗s\n⃗s ∈ ST\nState sequence ⃗s = s0, . . . , sT\nˆs ∈ St\nState sequence segment ˆs = s0, . . . , st for t ≤ T\not ∈ Ω\nThe t’th entry in an observation sequence ⃗o\n⃗o ∈ ΩT\nObservation sequence ⃗o = o0, . . . , oT\nˆo ∈ Ωt\nObservation sequence segment ˆo = o0, . . . , ot for t ≤ T\nThe Human’s Belief\nB(π′)\nThe human’s policy prior\nB(⃗s)\nThe human’s prior belief that a sequence ⃗s will be sampled, given by\nB(⃗s) =\nR\nπ′ B(π′)P π′(⃗s)dπ′\nB\n\u0000⃗s | ⃗o\n\u0001\nThe human’s belief of a state sequence given an observation sequence, see\nProposition B.1 for a Bayesian version\n15\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nBπ(⃗s | ⃗o)\nThe human’s belief of a state sequence given an observation sequence; it\nis allowed to depend on the true policy π, see Proposition B.1\nB⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o}\nVector of prior probabilities B(⃗s) for ⃗s ∈\n\b\n⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o\n\t\nIdentifiability Theorem\nβ > 0\nThe inverse temperature parameter of the Boltzmann rational human\nσ : R → (0, 1)\nThe sigmoid function given by σ(x) =\n1\n1+exp(−x)\nΓ : RS → R ⃗S\nFunction that maps a reward function R to the return function Γ(R) with\n\u0002\nΓ(R)\n\u0003\n(⃗s) = PT\nt=0 γtR(st)\nB : R ⃗S → R⃗Ω\nFunction that maps a return function G to the expected return function\nB(G) on observation sequences given by\n\u0002\nB(G)\n\u0003\n(⃗o) = E⃗s∼B(⃗s|⃗o)\n\u0002\nG(⃗s)\n\u0003\nF : RS → R⃗Ω\nThe composition F = B ◦ Γ\nP R\u0000⃗s ≻ ⃗s′\u0001\nBoltzmann rational choice probability in the case of full observability\n(Eq. (1))\nP R\u0000⃗o ≻ ⃗o′\u0001\nBoltzmann rational choice probability in the case of partial observability\n(Eq. (2))\nO : R⃗Ω → R ⃗S\nAbstract linear operator given by\n\u0002\nO(v)\n\u0003\n(⃗s) = E⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0002\nv(⃗o)\n\u0003\nO ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S\nFormally the Kronecker product of O with itself, explicitly given by\n\u0002\n(O ⊗ O)(C)\n\u0003\n(⃗s,⃗s′) = E⃗o,⃗o ′∼P ⃗\nO(·|⃗s,⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\nRobustness to Misspecifications\n∥x∥\nEuclidean norm of the vector x ∈ Rk\n∥ A ∥\nMatrix norm of the matrix A, given by ∥ A ∥ := maxx, ∥x∥=1 ∥ A x∥\nτ(A)\nMatrix quantity defined in Equation (9)\nC(A, ρ)\nMatrix quantity defined in Equation (10)\nr(B)\nRestriction of B to im Γ\nGeneral Sets and (Linear) Functions\n|A|\nNumber of elements in the set A\nA ∩ C\nIntersection of sets A and C\nA ∪ C\nUnion of sets A and C\nA \\ C\nRelative complement of C in A\nδx\nThe Dirac delta distribution of a point x in a set; given by δx(A) = 1 if\nx ∈ A and δx(A) = 0, else\nker A\nThe kernel of a linear operator A : V → W; given by ker A =\n\b\nv ∈ V |\nA(v) = 0\n\t\nim A\nThe image of a linear operator A : V → W; given by im A =\n\b\nw ∈ W |\n∃v ∈ V : A(v) = w\n\t\nf −1(y)\nPreimage of y under a function f : X → Y ; given by f −1(y) =\n\b\nx ∈\nX | f(x) = y\n\t\nB. Modeling the Human in Partially Observable RLHF\nIn this appendix, we develop the theory of RLHF with appropriately modeled partial observability, including full proofs of\nall theorems.\nIn Section B.1, we explain how the human can arrive at the belief B⃗s | ⃗o via Bayesian updates. The main theory and the\nmain paper in general do not depend on this specific form of the human’s belief, but some examples in the appendix do.\n16\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nIn Section B.2 we then explain our main result: the ambiguity and identifiability of both reward and return functions under\nobserved sequence comparisons. In Section B.3, we then explain that this theorem means that one could in principle design\na practical reward learning algorithm that converges on the correct reward function up to the ambiguity characterized in the\nsection before, if the human’s belief kernel B(⃗s | ⃗o) is fully known.\nIn Section B.4, we generalize the theory to the case that the human’s observations are not necessarily known to the learning\nsystem and again characterize precisely when the return function is identifiable from sequence comparisons. We then\nconsider special cases in Section B.5, where we show that the fully observable case is covered by our theory, that a\ndeterministic observation kernel P ⃗O usually leads to non-injective belief matrix B, and that “noise” in the observation kernel\nP ⃗O leads, under appropriate assumptions, to the identifiability of the return function.\nOur identifiability results require that the learning system knows the human’s belief kernel B(⃗s | ⃗o). In Section B.6, we\nthen show that these results are robust to slight misspecifications: a bound in the error in the specified belief leads to a\ncorresponding bound in the error of the policy evaluation function used for subsequent reinforcement learning.\nIn Section B.7, we then provide a very preliminary characterization of the ambiguity in the return function under special\ncases.\nFinally, in Section B.8, we study examples of identifiability and non-identifiability of the return function for the case that we\ndo model the human’s partial observability correctly. This reveals qualitatively interesting cases of identifiability, even when\nB is not injective, and catastrophic cases of non-identifiability.\nB.1. The Belief over the State Sequence for Rational Humans\nBefore we dive into the main theory, we want to explain how the human can iteratively compute the posterior of the state\nsequence given an observation sequence with successively new observations. This is done by defining a Bayesian network\nfor the joint probability of policy, states, actions, and observations, and doing Bayesian inference over this Bayesian network.\nThe details of this subsection are only relevant for a few sections in the appendix since it is usually enough to assume that\nthe posterior belief exists. Additionally, in the core theory, we do not even assume that B(⃗s | ⃗o) is a posterior: it is simply\nany probability distribution. The reason why it can still be interesting to analyze the case when the human is a rational\nBayesian reasoner is that one can then analyze RLHF under generous assumptions to the human.\nWe model the human to have a joint distribution B(π,⃗s,⃗a,⃗o) over the policy π, state sequence ⃗s = s0, . . . , sT , action\nsequence ⃗a = a0, . . . , aT −1, and observation sequence ⃗o = o0, . . . , oT . This is given by a Bayesian network with the\nfollowing components:\n• a policy prior B(π′);\n• the probability of the initial state B(s0) := P0(s0);\n• action probabilities B(a | s, π) := π(a | s);\n• transition probabilities B(st+1 | st, at) := T (st+1 | st, at);\n• and observation probabilities B(ot | st) := PO(ot | st).\nTogether, this defines the joint distribution B(π,⃗s,⃗a,⃗o) over the policy, states, actions, and observations that factorizes\naccording to the following directed acyclic graph:\n17\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nπ′\ns0\na0\ns1\na1\ns2\na2\ns3\n. . .\no0\no1\no2\no3\n(6)\nThe following proposition clarifies the iterative Bayesian update of the human’s posterior over state sequences, given\nobservation sequences:\nProposition B.1. Let t ≤ T − 1 and denote by ˆs = s0, . . . , st a state sequence segment of length t ≥ 0. Similarly,\nˆo = o0, . . . , ot denotes an observation sequence segment. We have\nB(ˆs, st+1, π | ˆo, ot+1) ∝ PO(ot+1 | st+1) ·\n\" X\nat∈A\nT (st+1 | ˆst, at) · π(at | st)\n#\n· B(ˆs, π | ˆo).\nThus, the human can iteratively compute B(ˆs, π | ˆo) from the prior B(s0, π) = P0(s0) · B(π′) using the above Bayesian\nupdate.\nThe posterior over the state sequence can subsequently be computed by\nB(ˆs | ˆo) =\nZ\nπ\nB(ˆs, π | ˆo).\nProof. The proof is essentially just Bayes rule applied to the Bayesian network in Equation (6). We repeatedly make use of\nconditional independences that follow from d-separations in the graph (Geiger et al., 1990). More concretely, we have\nB\n\u0000ˆs, st+1, π | ˆo, ot+1\n\u0001\n∝ B\n\u0000ot+1 | ˆs, st+1, π, ˆo\n\u0001\n· B\n\u0000ˆs, st+1, π | ˆo\n\u0001\n= PO\n\u0000ot+1 | st+1\n\u0001\n· B\n\u0000st+1 | ˆs, π, ˆo) · B(ˆs, π | ˆo\n\u0001\n= PO\n\u0000ot+1 | st+1\n\u0001\n·\n\" X\nat∈A\nB\n\u0000st+1 | at, ˆs, π, ˆo\n\u0001\n· B\n\u0000at | ˆs, π, ˆo\n\u0001\n#\n· B\n\u0000ˆs, π | ˆo\n\u0001\n= PO\n\u0000ot+1 | st+1\n\u0001\n·\n\" X\nat∈A\nT\n\u0000st+1 | st, at\n\u0001\n· π\n\u0000at | st\n\u0001\n#\n· B\n\u0000ˆs, π | ˆo\n\u0001\n.\nIn step 1, we used Bayes rule. In step 2, we made use of the independence ot+1⊥⊥(ˆs, π, ˆo) | st+1, plugged in the observation\nkernel, and used the chain rule of probability to compose the second term into a product. In step 3, we marginalized and\nused, once again, the chain rule of probability. In step 4, we used the independences st+1 ⊥⊥ (s0, . . . , st−1, π, ˆo) | (st, a)\nand at ⊥⊥ (s0, . . . , st−1, ˆo) | (π, st) and plugged in the transition kernel and the policy.\nThe last formula is just a marginalization over the policy.\nB.2. Ambiguity and Identifiability of Reward and Return Functions under Observation Sequence Comparisons\nIn this section, we prove the main theorem of this paper: a characterization of the ambiguity that is left in the reward and\nreturn function once the human’s Boltzmann-rational choice probabilities are known. We change the formulation slightly by\nformulating the linear operators “intrinsically” in the spaces they are defined in, instead of using matrix versions. This does\n18\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nFigure 4. The linear geometry of ambiguity for a hypothetical example with three state sequences and two observation sequences. G∗ is\nthe true return function, and “G” is used in labeling the axes to refer to some arbitrary return function. This is a more accurate geometric\ndepiction of the middle and right spaces in Figure 3. The subspace im Γ ∩ ker B (purple) is the ambiguity in return functions, meaning\nthat adding an element would not change the human’s expected return function on observations. Thus the set of return functions that\nthe reward learning system can infer is the affine set G + (im Γ ∩ ker B) (yellow). Note that the planes on the left are drawn to be\naxis-aligned for ease of visualization; this will not be the case for real MDPs.\nnot change the general picture, but is a more natural setting when thinking, e.g., about generalizing the results to infinite\nstate sequences. Thus, we define B : R ⃗S → R⃗Ω as the linear operator given by\n\u0002\nB(G)\n\u0003\n(⃗o) :=\nE\n⃗s∼B(⃗s|⃗o)\n\u0002\nG(⃗s)\n\u0003\n.\nHere, B is the human’s belief, which can either be computed as in the previous subsection or simply be any conditional\nprobability distribution. Similarly, we define Γ : RS → R ⃗S as the linear operator given by\n\u0002\nΓ(R)\n\u0003\n(⃗s) :=\nT\nX\nt=0\nγtR(st).\nThe matrix product B · Γ then becomes the composition B ◦ Γ : RS → R⃗Ω. Finally, recall that the kernel ker A of a linear\noperator A is defined as its nullspace, and the image im A as the set of elements hit by A. We obtain the following theorem:\nTheorem B.2. Let R be the true reward function and ˜R another reward function. Let ˜G = Γ( ˜R) and G = Γ(R) be the\ncorresponding return functions. The following three statements are equivalent:\n(i) The reward function ˜R gives rise to the same vector of choice probabilities as R, i.e\n\u0010\nP\n˜\nR\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω =\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω.\n(ii) There is a reward function R′ ∈ ker(B ◦ Γ) and a constant c ∈ R such that\n˜R = R + R′ + c.\n(iii) There is a return function G′ ∈ ker B ∩ im Γ and a constant c′ ∈ R such that\n˜G = G + G′ + c′.\nIn other words, the ambiguity that is left in the reward function when its observation-based choice probabilities are known\nis, up to an additive constant, given by ker(B ◦ Γ); the ambiguity left in the return function is given by ker B ∩ im Γ.\n19\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nProof. Assume (i). To prove (ii), let σ by the sigmoid function given by σ(x) =\n1\n1+exp(−x). Then by Equation (2), the\nequality of choice probabilities means the following for all ⃗o,⃗o′ ∈ ⃗Ω:\nσ\n\u0010\nβ ·\n\u0000\u0002\nB( ˜G)\n\u0003\n(⃗o) −\n\u0002\nB( ˜G)\n\u0003\n(⃗o′)\n\u0001\u0011\n= σ\n\u0010\nβ ·\n\u0000\u0002\nB(G)\n\u0003\n(⃗o) −\n\u0002\nB(G)\n\u0003\n(⃗o′)\n\u0001\u0011\n.\nSince the sigmoid function is injective, this implies\n\u0002\nB( ˜G)\n\u0003\n(⃗o) −\n\u0002\nB( ˜G)\n\u0003\n(⃗o′) =\n\u0002\nB(G)\n\u0003\n(⃗o) −\n\u0002\nB(G)\n\u0003\n(⃗o′).\nFixing an arbitrary ⃗o′, this implies that there exists a constant c′ such that for all ⃗o ∈ ⃗Ω, the following holds:\n\u0002\nB( ˜G)\n\u0003\n(⃗o) −\n\u0002\nB(G)\n\u0003\n(⃗o′) − c′ = 0.\nNoting that B(c′) = c′, this implies ˜G − G − c′ ∈ ker(B). Now, define the constant reward function\nc := c′ ·\n1 − γ\n1 − γT +1 .\nWe obtain\n\u0002\nΓ(c)\n\u0003\n(⃗s) =\nT\nX\nt=0\nγt · c\n= c′ ·\n1 − γ\n1 − γT +1 ·\nT\nX\nt=0\nγt\n= c′.\nThus, we have\nΓ( ˜R − R − c) = ˜G − G − c′ ∈ ker(B),\nimplying R′ := ˜R − R − c ∈ ker(B ◦ Γ). This shows (ii).\nThat (ii) implies (iii) follows by applying Γ to both sides of the equation.\nNow assume (iii), i.e. ˜G = G + G′ + c′ for a constant c′ ∈ R and a return function G′ ∈ ker(B) ∩ im Γ. This implies\nB( ˜G) = B(G) + c′. Thus, for all ⃗o,⃗o′ ∈ ⃗Ω, we have\n\u0002\nB( ˜G)\n\u0003\n(⃗o) −\n\u0002\nB( ˜G)\n\u0003\n(⃗o′) =\n\u0002\nB(G)\n\u0003\n(⃗o) −\n\u0002\nB(G)\n\u0003\n(⃗o′),\nwhich implies the equal choice probabilities after multiplying with β and applying the sigmoid function σ on both sides.\nThus, (iii) implies (i).\nCorollary B.3. The following two statements are equivalent:\n(i) ker(B ◦ Γ) = 0.\n(ii) The data\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω determine the reward function R up to an additive constant.\nProof. That (i) implies (ii) follows immediately from the implication from (i) to (ii) within the preceding theorem.\nNow assume (ii). Let R′ ∈ ker(B ◦ Γ). Define ˜R := R + R′. Then the implication from (ii) to (i) within the preceding\ntheorem implies that ˜R and R have the same choice probabilities. Thus, the assumption (ii) in this corollary implies that\nR′ is a constant. Since Γ and B map nonzero constants to nonzero constants, the fact that R′ ∈ ker(B ◦ Γ) implies that\nR′ = 0, showing that ker(B ◦ Γ) = {0}.\nAs mentioned in the main paper, the previous result already leads to the non-identifiability of R whenever Γ is not injective,\ncorresponding to the presence of zero-initial potential shaping (Skalse et al. (2023), Lemma B.3). Thus, we now strengthen\nthe previous result so that it deals with the identifiability of the return function, which is sufficient for the purpose of policy\noptimization:\n20\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nCorollary B.4. Consider the following four statements (which can each be true or false):\n(i) ker B = {0}.\n(ii) ker\n\u0000B ◦ Γ) = {0}.\n(iii) ker B ∩ im Γ = {0}.\n(iv) The data\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω determine the return function G = Γ(R) on sequences ⃗s ∈ ⃗S up to a constant\nindependent of ⃗s.\nThen the following implications, and no other implications, are true:\n(i)\n(iii)\n(iv)\n(ii)\nIn particular, all of (i), (ii), and (iii) are sufficient conditions for identifying the return function from the choice probabilities.\nProof. That (i) implies (iii) is trivial. That (ii) implies (iii) is a simple linear algebra fact: Assume (ii) and that G′ ∈\nker B ∩ im Γ. Then G′ = Γ(R′) for some R′ ∈ RS and\n0 = B(G′) = B\n\u0000Γ(R′)\n\u0001\n= (B ◦ Γ)(R′).\nBy (ii), this implies R′ = 0 and therefore G′ = Γ(R′) = 0, showing (iii).\nThat (iii) implies (iv) immediately follows from the implication from (i) to (iii) in Theorem B.2.\nNow, assume (iv). To prove (iii), assume G′ ∈ ker B ∩ im Γ. Then the implication from (iii) to (i) in Theorem B.2 implies\nthat G + G′ induces the same observation-based choice probabilities as G. Thus, (iv) implies G + G′ = G + c′ for some\nconstant c′, which implies G′ = c′. Since G′ ∈ ker B, this implies 0 = B(G′) = B(c′) = c′ and thus G′ = 0. Thus, we\nshowed ker B ∩ im Γ = {0}.\nWe now show that no other implication holds in general. Example B.32 will show that (ii) does not imply (i). We now show\nthat (i) does also not imply (ii), from which it will logically follow that (iii) does neither imply (i) nor (ii). Namely, consider\nthe following simple MDP with time horizon T = 1:\na\nb\n(7)\nIn this MDP, every state sequence starts in a, deterministically transitions to b, and then ends. This means that ⃗s = ab is the\nonly sequence. Now, let R′ ∈ R{a,b} be the reward function given by\nR′(a) = 1,\nR′(b) = −1\nγ .\nWe obtain\n\u0002\nΓ(R′)\n\u0003\n(⃗s) = R′(a) + γR′(b) = 1 + γ · −1\nγ\n= 0.\nThus, Γ(R′) = 0, (B ◦ Γ)(R′) = 0, and, therefore, ker\n\u0000B ◦ Γ\n\u0001\n̸= {0}. Thus, (ii) does not hold. However, it is possible\nto choose B(⃗s | ⃗o) such that (i) holds: e.g., if Ω = S and B(⃗s | ⃗o) := δ⃗o(⃗s), then ker B = {0} since this operator is the\nidentity.\n21\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nB.3. The Ambiguity in Reward Learning in Practice\nIn this section, we point out that Theorem B.2 is not just a theoretical discussion: When B and the inverse temperature\nparameter β are known, then it is possible to design a reward learning algorithm that learns the true reward function up to\nthe ambiguity ker(B ◦ Γ) in the infinite data limit. In doing so, we essentially use the loss function proposed in Christiano\net al. (2017).\nNamely, assume D is a data distribution of observation sequences ⃗o ∈ ⃗Ω such that all sequences in ⃗Ω have a strictly positive\nprobability of being sampled; for example, D could use an exploration policy and the observation sequence kernel P ⃗O. For\neach pair of observation sequences (⃗o,⃗o′), we then get a conditional distribution P(µ | ⃗o,⃗o′) over a one-hot encoded human\nchoice µ ∈ {(1, 0), (0, 1)}, with probability\nP\n\u0000µ = (1, 0) | ⃗o,⃗o′\u0001\n= P R\u0000⃗o ≻ ⃗o′\u0001\n.\nTogether, this gives rise to a dataset (⃗o1,⃗o′\n1, µ1), . . . , (⃗oN,⃗o′\nN, µN) of observation sequences plus a human choice.\nNow assume we learn a reward function Rθ : S → R that is differentiable in the parameter θ and that can represent all\npossible reward functions R ∈ RS. Let Gθ := Γ(Rθ) be the corresponding return function. Write µk = (µ(1)\nk , µ(2)\nk ). As\nin Christiano et al. (2017), we define its loss over the dataset above by\neL(θ) = − 1\nN\nN\nX\nk=1\nµ(1)\nk\n· log P Rθ\u0000⃗ok ≻ ⃗o′\nk\n\u0001\n+ µ(2)\nk\n· log P Rθ\u0000⃗o′\nk ≻ ⃗ok\n\u0001\n.\nNote that by Equation (2), this loss function essentially uses B and also the inverse temperature parameter β in its definition.\nThis means that these need to be explicitly represented to be able to use the loss function in practice.\nProposition B.5. The loss function eL is differentiable. Furthermore, in the infinite datalimit its minima are precisely given\nby parameters θ such that Rθ = R + R′ + c for R′ ∈ ker\n\u0000B ◦ Γ\n\u0001\nand c ∈ R, or equivalently Gθ = G + G′ + c′ for\nG′ ∈ ker B ∩ im Γ and c′ ∈ R.\nProof. The differentiability of the loss function follows from the differentiability of multiplication with the matrix B, see\nEquation (2), and of the reward function Rθ in its parameter θ that we assumed.\nFor the second statement, let N(⃗o,⃗o′) be the number of times that the pair (⃗o,⃗o′) appears in the dataset, and let N(⃗o,⃗o′, 1)\nbe the number of times that the human choice is µ = (1, 0) and the sampled pair is (⃗o,⃗o′), and similar for 2 instead of 1.\nWe obtain\neL(θ) = −\nX\n⃗o,⃗o ′∈⃗Ω\nN(⃗o,⃗o′)\nN\n·\n\"\nN(⃗o,⃗o′, 1)\nN(⃗o,⃗o′) log P Rθ\u0000⃗o ≻ ⃗o′\u0001\n+ N(⃗o,⃗o′, 2)\nN(⃗o,⃗o′) log P Rθ\u0000⃗o′ ≻ ⃗o\n\u0001\n#\n≈\nE\n⃗o,⃗o ′∼D\n\u0014\nCE\n\u0010\nP R\u0000⃗o ≻≺ ⃗o′\u0001 \r\r P Rθ\u0000⃗o ≻≺ ⃗o′\u0001\u0011\u0015\n=:L(θ).\nHere, CE is the crossentropy between the two binary distributions. Since we assumed that D gives a positive probability to\nall observation sequences in ⃗Ω, and since the cross entropy is generally minimized exactly when the second distribution\nequals the first, the loss function L(θ) is minimized if and only if Rθ gives rise to the same choice probabilities as R for all\npairs of observation sequences. Theorem B.2 then gives the result.\nB.4. Identifiability of Return Functions When Human Observations Are Not Known\nCorollary B.4 assumes that the choice probabilities of each observation sequence pair are known to the reward learning\nalgorithm. However, this requires the algorithm to know what the human observed. In some applications, this is a reasonable\nassumption, e.g. if the human’s observations are themselves produced by an algorithm that can feed the observations also\nback to the learning algorithm. In general, however, the observations happen in the physical world, and are only known\n22\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nprobabilistically via the observation kernel PO. The learning system does however have access to the full state sequences\nthat generate the observation sequences. This leads to knowledge of the following choice probabilities for ⃗s,⃗s′ ∈ ⃗S:\nP R\u0000⃗s ≻ ⃗s′\u0001 :=\nE\n⃗o,⃗o′∼P ⃗\nO(·|⃗s,⃗s ′)\nh\nP R\u0000⃗o ≻ ⃗o′\u0001i\n, 2\n(8)\nwhere the observation-based choice probabilities are given as in Equation (2). In other words, the learning algorithm can\nonly infer an aggregate of the observation-based choice probabilities. Again, we can ask a question similar to the ones\nbefore, extending the investigations in the previous section:\nQuestion B.6. Assume the vector of choice probabilities\n\u0010\nP R(⃗s ≻ ⃗s′)\n\u0011\n⃗s,⃗s ′∈ ⃗S is known. Additionally, assume that it\nis known that the human’s observations are governed by PO, and that the human is Boltzmann rational with inverse\ntemperature parameter β and beliefs B(⃗s | ⃗o), see Equation (8). Does this data identify the return function G : ⃗S → R?\nIf the observation-based choice probabilities from Equation (2) would be known, then Corollary B.4 would provide the\nanswer to this question. Thus, similar to how we previously inverted the belief operator B, we are now simply tasked with\ninverting the expectation over observation sequences. This leads us to the following definition:\nDefinition B.7 (Ungrounding Operator). The ungrounding operators O : R⃗Ω → R ⃗S and O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S are\ndefined by\n\u0002\nO(v)\n\u0003\n(⃗s) :=\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0002\nv(⃗o)\n\u0003\n,\n\u0002\n(O ⊗ O)(C)\n\u0003\n(⃗s,⃗s′) :=\nE\n⃗o,⃗o ′∼P ⃗\nO(·|⃗s,⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\n.\nHere, v ∈ R⃗Ω is an arbitrary vector, and C ∈ R⃗Ω×⃗Ω is also an arbitrary vector, where the notation can remind of “Choice”\nsince the inputs to O ⊗ O are, in practice, vectors of observation-based Boltzmann-rational choice probabilities.\nFormally, O ⊗ O is the Kronecker product of O with itself, but it is not necessary to understand this fact to follow the\ndiscussion. Ultimately, to be able to recover the observation-based choice probabilities, what matters is that O ⊗ O is\ninjective on whole vectors of these choice probabilities. The injectivity of O is a sufficient condition for this, which explains\nits usefulness. We show this in the following lemma:\nLemma B.8. O : R⃗Ω → R ⃗S is injective if and only if O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S is injective.\nProof. This is a general property of the Kronecker product of a linear operator with itself. For completeness, we demonstrate\nthe calculation in our special case. First, assume that O is injective. Assume that (O ⊗ O)(C) = 0 for some C ∈ R⃗Ω×⃗Ω.\nWe need to show C = 0.\nFor all pairs of state sequences (⃗s,⃗s′), we have\n0 =\n\u0002\n(O ⊗ O)(C)\n\u0003\n(⃗s,⃗s′) =\nE\n⃗o,⃗o ′∼P ⃗\nO(·|⃗s,⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\n=\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0014\nE\n⃗o ′∼P ⃗\nO(⃗o ′|⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\u0015\n=\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\nh\nC′\n⃗s ′(⃗o)\ni\n=\nh\nO\n\u0000C′\n⃗s ′\n\u0001i\n(⃗s),\nwhere C′\n⃗s ′(⃗o) := E⃗o ′∼P ⃗\nO(⃗o ′|⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\n. By the injectivity of O, we obtain C′\n⃗s ′ = 0 for all ⃗s′. This means that for all ⃗s′\nand ⃗o, we have\n0 = C′\n⃗s ′(⃗o) =\nE\n⃗o ′∼P ⃗\nO(⃗o ′|⃗s ′)\n\u0002\nC(⃗o,⃗o′)\n\u0003\n=\nh\nO\n\u0000C′′\n⃗o\n\u0001i\n(⃗s′),\nwhere C′′\n⃗o (⃗o′) := C(⃗o,⃗o′). Again, by the injectivity of O, we obtain C′′\n⃗o = 0 for all ⃗o, leading to C = 0. That proves the\ndirection from left to right.\n2We excuse the following abuse of notation: these choice probabilities run through the observations of the human and are not the same\nas the choice probabilities from Equation (1).\n23\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nTo prove the other direction, assume that O is not injective. This means there exists 0 ̸= C ∈ R⃗Ω such that O(C) = 0.\nDefine C ⊗ C ∈ R⃗Ω×⃗Ω by\n(C ⊗ C)(⃗o,⃗o′) := C(⃗o)C(⃗o′).\nThen clearly, C ⊗ C ̸= 0. We are done if we can show that (O ⊗ O)(C ⊗ C) = 0 since that establishes that O ⊗ O is also\nnot injective. For any ⃗s,⃗s′ ∈ ⃗S, we have\nh\n(O ⊗ O)(C ⊗ C)\ni\n(⃗s,⃗s′) =\nE\n⃗o,⃗o ′∼P ⃗\nO(·|⃗s,⃗s ′)\nh\n(C ⊗ C)(⃗o,⃗o′)\ni\n=\nE\n⃗o,⃗o ′∼P ⃗\nO(·|⃗s,⃗s ′)\nh\nC(⃗o) · C(⃗o′)\ni\n=\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0002\nC(⃗o)\n\u0003\n·\nE\n⃗o ′∼P ⃗\nO(⃗o ′|⃗s ′)\n\u0002\nC(⃗o′)\n\u0003\n=\n\u0002\nO(C)\n\u0003\n(⃗s) ·\n\u0002\nO(C)\n\u0003\n(⃗s′)\n= 0 · 0\n= 0.\nThis finishes the proof.\nWe now state and prove the following extension of Corollary B.4:\nTheorem B.9. Consider the following statements (which can each be true or false):\n1. O : R⃗Ω → R ⃗S is an injective linear operator: ker O = {0}.\n2. O ⊗ O : R⃗Ω×⃗Ω → R ⃗S× ⃗S is an injective linear operator: ker O ⊗ O = {0}.\n3. O ⊗ O is injective on vectors of observation-based choice probabilities\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′ over the set of return\nfunctions G ∈ R ⃗S.\n4. The data of state-based choice probabilities\n\u0010\nP R\u0000⃗s ≻ ⃗s′\u0001\u0011\n⃗s,⃗s ′∈ ⃗S from Equation (8) determine the data of observation-\nbased choice probabilities\n\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′∈⃗Ω from Equation (2).\nThen the following implications hold and 3 does not imply 2:\n1\n2\n3\n4.\nConsequently, if any of the conditions 1, 2, or 3 hold, and additionally any of the conditions (i), (ii) or (iii) from Corollary B.4,\nthen the data\n\u0010\nP R\u0000⃗s ≻ ⃗s′\u0001\u0011\n⃗s,⃗s ′∈⃗Ω determine the return function G on sequences ⃗s ∈ ⃗S up to a constant independent of ⃗s.\nProof. That 1 and 2 are equivalent was shown in Lemma B.8. That 2 implies 3 is clear. To prove that 3 implies 4, simply\nput both sets of choice probabilities into a vector. Then Equation (8) and Definition B.7 show the following equality of\nvectors in R ⃗S× ⃗S:\n\u0010\nP R\u0000⃗s ≻ ⃗s′\u0001\u0011\n⃗s,⃗s ′ =\n\u0000O ⊗ O\n\u0001\u0012\u0010\nP R\u0000⃗o ≻ ⃗o′\u0001\u0011\n⃗o,⃗o ′\n\u0013\n.\nThe injectivity of O ⊗ O on such inputs ensures that the observation-based choice probabilities can be recovered using this\nequation.\nWe now show that (3) does not imply (2). Again, we use the simple MDP from Equation (7), but this time with a different\nobservation kernel. Namely, we choose\nPO(o(a) | a) = PO(o(a)′ | a) = 1\n2,\nPO(o(b) | b) = 1,\n24\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nwhere o(a)′ ̸= o(a) and o(a) ̸= o(b) ̸= o(a)′. This results in two possible observation sequences: o(a)o(b) and o(a)′o(b).\nThus, R⃗Ω is two-dimensional, whereas R ⃗S is only one-dimensional. Consequently, O : R⃗Ω → R ⃗S cannot be injective, so\nker O ̸= {0}, so (2) does not hold since (1) and (2) are equivalent. However, (3) still holds: Since there is only one state\nsequence, Equation (2) shows that the only vector of choice probabilities has 1/2 in all its entries, irrespective of the return\nfunction G. Thus, O ⊗ O has only one input of observation-based choice probabilities, and is thus automatically injective\non its inputs.\nThe final result of identifiability of the return function G follows using Corollary B.4.\nB.5. Simple Special Cases: Full Observability, Deterministic P ⃗O, and Noisy P ⃗O\nIn this section, we analyze three simple special cases of the general theory.\nTheorem 3.9 (together with Lemma B.3) from Skalse et al. (2023), reproduced as a corollary below, is a special case of our\ntheorem:\nCorollary B.10 (Skalse et al. (2023)). Assume the human directly observes the true sequences, and the choice probabilities\nare given by\nP R\u0000⃗s ≻ ⃗s′\u0001\n= σ\n\u0010\nβ\n\u0000G(⃗s) − G(⃗s′)\n\u0001\u0011\n.\nThis data determines the return function G = Γ(R) on state sequences ⃗s ∈ ⃗S up to a constant independent on ⃗s.\nProof. We can embed this case into the one of Theorem B.9 by defining the observation kernel as P ⃗O(⃗s′ | ⃗s) = δ⃗s(⃗s′) (i.e.,\nthe correct sequence is deterministically observed) and defining the human’s belief as B(⃗s′ | ⃗s) = δ⃗s(⃗s′) (i.e., the human\nknows that the observation reflects the true sequence). This shows that P(⃗s ≻ ⃗s′) is of the form of Equation (8). The result\nfollows from Theorem B.9: the operators O and B are the identity in this case, due to the defining property of the Kronecker\ndelta, and so they are injective.\nThe following proposition shows that Corollary B.10 is essentially the only example of deterministic observation kernel P ⃗O\nfor which B is injective. Note, however, that in some situations, we can have im Γ ∩ ker B = {0} even if B is not injective,\nsee Example B.32.\nProposition B.11. Assume P ⃗O, the observation kernel on the level of sequences, is deterministic and not injective. Then O\nis automatically injective. However, B is not injective.\nProof. To show that O is injective, assume v ∈ R⃗Ω is such that O(v) = 0. Then for all ⃗s ∈ ⃗S, we get\n0 =\n\u0002\nO(v)\n\u0003\n(⃗s) =\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0002\nv(⃗o)\n\u0003\n= v\n\u0000 ⃗O(⃗s)\n\u0001\n.\nSince ⃗O : ⃗S → ⃗Ω is by definition surjective, we obtain v = 0.\n⃗O : ⃗S → ⃗Ω is by definition surjective, and here assumed to be non-injective, which implies that ⃗S has a higher cardinality\nthan ⃗Ω. Thus, B : R ⃗S → R⃗Ω cannot be injective.\nIn the following, we analyze a simple case that guarantees identifiability. It requires that the observation kernel is “well-\nbehaved” of a form where the observations are simply “noisy states”, and that the human is a Bayesian reasoner with any\nprior B(⃗s) that supports every state sequence ⃗s ∈ ⃗S.\nDefinition B.12 (Noise in the Observation Kernel). Then we say that there is noise in the observation kernel PO : ⃗S → ∆(⃗Ω)\nif ⃗S = ⃗Ω and if O is an injective linear operator.\nProposition B.13. Assume that ⃗S = ⃗Ω. Furthermore, assume that B(⃗s | ⃗o) is given by the posterior with likelihood\nP ⃗O(⃗o | ⃗s) and any prior B(⃗s) with B(⃗s) > 0 for all ⃗s ∈ ⃗S. Then there is noise in the observation kernel if and only if B is\ninjective.\n25\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nProof. Assume O is injective. To show that B is injective, assume there is G′ ∈ R ⃗S with B(G′) = 0. Then for all ⃗o ∈ ⃗Ω,\nwe have\n0 =\n\u0002\nB(G′)\n\u0003\n(⃗o) =\nE\n⃗s∼B(⃗s|⃗o)\n\u0002\nG′(⃗s)\n\u0003\n=\nX\n⃗s\nB(⃗s | ⃗o)G′(⃗s) ∝\nX\n⃗s\nP ⃗O(⃗o | ⃗s) ·\n\u0000B(⃗s) · G′(⃗s)\n\u0001\n=\n\u0002\nOT (B ⊙ G′)\n\u0003\n(⃗o).\nHere, OT is the transpose of O and B ⊙ G′ is the componentwise product of the prior B with the return function G′. Since\nO is injective and thus invertible, OT is as well. Thus, B ⊙ G′ = 0, which implies G′ = 0 since the prior gives positive\nprobability to all state sequences. Thus, B is injective.\nFor the other direction, assume B is injective. To show that O is injective, let v ∈ R⃗Ω be any vector with O(v) = 0. We do\na similar computation as above: for all ⃗s ∈ R ⃗S, we have\n0 =\n\u0002\nO(v)\n\u0003\n(⃗s) =\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0002\nv(⃗o)\n\u0003\n=\nX\n⃗o\nP ⃗O(⃗o | ⃗s)v(⃗o) ∝\nX\n⃗o\nB(⃗s | ⃗o) ·\n\u0000P ⃗O(⃗o) · v(⃗o)\n\u0001\n=\nh\nBT \u0000P ⃗O ⊙ v\n\u0001i\n(⃗s).\nHere, BT is the transpose of B, P ⃗O(⃗o) is the denominator in Bayes rule, and P ⃗O ⊙ v is the vector with components\nP ⃗O(⃗o) · v(⃗o). From the injectivity and thus invertibility of B, it follows that BT is invertible as well, and so P ⃗O ⊙ v = 0,\nwhich implies v = 0. Thus, O is injective.\nCorollary B.14. When there is noise in the observation kernel and the human is a Bayesian reasoner with some prior B\nsuch that B(⃗s) > 0 for all ⃗s ∈ ⃗S, then the return function is identifiable from choice probabilities of state sequences even if\nthe learning system does not know the human’s observations.\nProof. This follows from the injectivity of O, the injectivity of B that we proved in Proposition B.13, and Theorem B.9.\nRemark B.15. We mention the following caveat: intuitively, one could think that O (and thus B, by Proposition B.13) will\nbe injective if every ⃗s is identifiable from infinitely many i.i.d. samples from P ⃗O(⃗o | ⃗s). A counterexample is the following:\nO =\n\n\n1/2\n1/4\n1/4\n1/4\n1/2\n1/4\n3/8\n3/8\n1/4\n\n .\nIn this case, the rows are linearly dependent with coefficients 1/2, 1/2 and −1. Consequently, O and B are not injective,\nand so if this observation kernel comes from a multi-armed bandit with three states, then Corollary B.4 shows that the return\nfunction is not identifiable.\nNevertheless, the distributions P ⃗O(· | ⃗s) (given by the rows) all differ from each other, and so infinitely many i.i.d. samples\nidentify the state sequence ⃗s.\nB.6. Robustness of Return Function Identifiability under Belief Misspecification\nWe now again look at the case where the observations that the human observes are known to the reward learning system, as\nin Section B.2. Furthermore, we assume that B : R ⃗S → R⃗Ω is such that ker B ∩ im Γ = {0}. In this case, we can apply\nCorollary B.4 and identify the true return function G from B(G), which, in turn, can be identified up to an additive constant\nfrom the observation-based choice probabilities with the argument as for Proposition 3.1.\nIn this section, we investigate what happens when the human belief model is slightly misspecified. In other words: the\nlearning system uses a perturbed matrix B∆ := B + ∆ with some small perturbation ∆. How much will the inferred return\nfunction deviate from the truth? To answer this, we first need to outline some norm theory of linear operators.\nB.6.1. SOME NORM THEORY FOR LINEAR OPERATORS\nIn this section, let V, W be two finite-dimensional inner product-spaces. In other words, V and W each have inner products\n⟨·, ·⟩ and there are linear isomorphisms V ∼= Rk, W ∼= Rm such that the inner products in V and W correspond to the\n26\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nstandard scalar products in Rk and Rm. The reason that we don’t directly work with Rk and Rm itself is that we will\nlater apply the analysis to the case that V = im Γ ⊆ R ⃗S. Let in this whole section A : V → W be a linear operator and\n∆ : V → W be a perturbance, so that A∆ := A + ∆ is a perturbed version of A.\nThe inner products give rise to a norm on V and W defined by\n∥v∥ =\np\n⟨v, v⟩,\n∥w∥ =\np\n⟨w, w⟩.\nAs is well known, for each linear operator A : V → W there exists a unique, basis-independent adjoint (generalizing the\nnotion of a transpose) AT : W → V such that for all v ∈ V and w ∈ W, we have\n⟨A v, w⟩ =\nD\nv, AT w\nE\n.\nLet us recall the following fact that is often used in linear regression:\nLemma B.16. Assume A : V → W is injective. Then AT A : V → V is invertible and (AT A)−1 AT is a left inverse of\nA.\nProof. To show that AT A is invertible, we only need to show that it is injective. Thus, let 0 ̸= x ∈ V . Then\nD\nx, AT A x\nE\n= ⟨A x, A x⟩ = ∥ A x∥2 > 0,\nwhere the last step followed from the injectivity of A. Thus, AT A x ̸= 0, and so AT A is injective, and thus invertible.\nConsequently, (AT A)−1 AT is a well-defined operator. That it is the left inverse of A is clear.\nDefinition B.17 (Operator Norm). The norm of an operator A : V → W is given by\n∥ A ∥ :=\nmax\nx, ∥x∥=1 ∥ A x∥.\nIt has the following well-known properties, where A, B and C are matrices of compatible sizes:\n∥ A + B ∥ ≤ ∥ A ∥ + ∥ B ∥,\n∥ C A ∥ ≤ ∥ C ∥ · ∥ A ∥,\n∥ AT ∥ = ∥ A ∥.\nTo study how a perturbance in A (and thus AT A) transfers into a perturbance of\n\u0000AT A\n\u0001−1, we will use the following\ntheorem:\nTheorem B.18 (El Ghaoui (2002)). Let B : V → V be an invertible operator. Let ρ < ∥ B−1 ∥−1. Let ∆ : V → V be any\noperator with ∥ ∆ ∥ ≤ ρ. Then B + ∆ is invertible and we have\n\r\r(B + ∆)−1 − B−1 \r\r ≤\nρ · ∥ B−1 ∥\n∥ B−1 ∥−1 − ρ.\nProof. See El Ghaoui (2002), Section 7 and in particular Equation 7.2. Note that the reference defines ∥ A ∥ to be the largest\nsingular value of A; by the well-known min-max theorem, this is equivalent to Definition B.17.\nWe will apply this theorem to AT A, which raises the question about the size of the perturbance in AT A for a given\nperturbance in A. This is clarified in the following lemma. Before stating it, for a given perturbance ρ, define\neρ(A) := ρ ·\n\u00002 · ∥ A ∥ + ρ\n\u0001\n,\nwhich depends on A and ρ. Also, recall that for a given perturbance ∆, we define A∆ := A + ∆. We obtain:\nLemma B.19. Assume that ∥ ∆ ∥ ≤ ρ. Then\n∥ AT\n∆ A∆ − AT A ∥ ≤ eρ(A).\n27\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nProof. We have\n\r\r AT\n∆ A∆ − AT A\n\r\r =\n\r\r(A + ∆)T (A + ∆) − AT A\n\r\r\n=\n\r\r AT ∆ + ∆T A + ∆T ∆\n\r\r\n≤ ∥ A ∥ · ∥ ∆ ∥ + ∥ ∆ ∥ · ∥ A ∥ + ∥ ∆ ∥2\n≤ ρ ·\n\u0010\n2 · ∥ A ∥ + ρ\n\u0011\n= eρ(A).\nTo be able to apply Theorem B.18 to AT A, we need to make sure that eρ(A) is bounded above by\n\r\r(AT A\n\u0001−1∥−1. The\nnext lemma clarifies what condition ρ needs to satisfy for eρ(A) to obey that bound. For this, define\nτ(A) := −∥ A ∥ +\nq\n∥ A ∥2 +\n\r\r(AT A)−1\r\r−1,\n(9)\nwhich only depends on A.\nLemma B.20. Assume ρ < τ(A). Then\neρ(A) <\n\r\r(AT A)−1\r\r−1.\nProof. Note that ρ = τ(A) is the positive solution to the following quadratic equation in the indeterminate ρ:\nρ2 + 2 · ∥ A ∥ · ρ −\n\r\r(AT A)−1\r\r−1 = eρ(A) −\n\r\r(AT A)−1\r\r−1 = 0.\nSince this is a convex parabola, we get the inequality eρ(A) −\n\r\r(AT A)−1\r\r−1 < 0 whenever we have 0 ≤ ρ < τ(A),\nwhich shows the result.\nFinally, we put it all together to obtain a bound on the perturbance of\n\u0000AT A\n\u0001−1 AT . For this, set\nC(A, ρ) :=\neρ(A) ·\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r\n−1\n− eρ(A)\n·\n\u0010\r\r A\n\r\r + ρ\n\u0011\n+\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r · ρ.\n(10)\nWe obtain:\nProposition B.21. Assume ∥ ∆ ∥ ≤ ρ < τ(A). Then AT\n∆ A∆ is invertible, and we have\n\r\r\r\n\u0000AT\n∆ A∆\n\u0001−1 AT\n∆ −\n\u0000AT A\n\u0001−1 AT \r\r\r ≤ C(A, ρ).\nProof. The invertibility of AT\n∆ A∆ follows from Theorem B.18, Lemma B.19 and Lemma B.20. We get\n\r\r\r\n\u0000AT\n∆ A∆\n\u0001−1 AT\n∆ −\n\u0000AT A\n\u0001−1 AT \r\r\r\n=\n\r\r\r\r\nh\u0000AT\n∆ A∆\n\u0001−1 −\n\u0000AT A\n\u0001−1i\n· AT\n∆ +\n\u0000AT A\n\u0001−1 ·\n\u0000AT\n∆ − AT \u0001\r\r\r\r\n≤\n\r\r\r\n\u0000AT\n∆ A∆\n\u0001−1 −\n\u0000AT A\n\u0001−1\r\r\r ·\n\r\r A∆\n\r\r +\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r · ∥ ∆ ∥\n≤\neρ(A) ·\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r\n−1\n− eρ(A)\n·\n\u0010\r\r A\n\r\r + ρ\n\u0011\n+\n\r\r\r\n\u0000AT A\n\u0001−1\r\r\r · ρ\n=C(A, ρ).\nIn the second-to-last step, we used Theorem B.18.\n28\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nThe constant C(A, ρ), defined in Equation (10), has a fairly complicated form. In the following proposition, we find an\neasier-to-study upper bound in a special case:\nProposition B.22. Assume that ρ ≤ ∥ A ∥ and ρ ≤ −∥ A ∥ +\nq\n∥ A ∥2 + 1/2 ·\n\r\r(AT A)−1\r\r−1.3 Then we have\nC(A, ρ) ≤ ρ ·\n\r\r(AT A)−1\r\r ·\nh\n12 · ∥ A ∥2 ·\n\r\r(AT A)−1\r\r + 1\ni\n.\nProof. The second assumption gives, as in the proof of Lemma B.20, that eρ(A) ≤ 1/2 ·\n\r\r(AT A)−1\r\r−1. Together with\nρ ≤ ∥ A ∥, the result follows.\nB.6.2. APPLICATION TO BOUNDS IN THE ERROR OF THE RETURN FUNCTION\nWe now apply the results from the preceding section to our case. Define r(B) : im Γ → R⃗Ω as the restriction of the belief\noperator B to im Γ. Assume that ker B ∩ im Γ = {0}, which is, according to Corollary B.4, a sufficient condition for\nidentifiability. Note that this condition means that r(B) is injective. Thus, Lemma B.16 ensures that r(B)T r(B) is invertible\nand that\n\u0000r(B)T r(B)\n\u0001−1r(B)T is a left inverse of r(B).\nConsequently, from the equation\nr(B)(G) = B(G)\nwe obtain\nG =\n\u0000r(B)T r(B)\n\u0001−1r(B)T (B(G)).\nThis is the concrete formula with which G can be identified from B(G). When perturbing B, this leads to a corresponding\nperturbance in\n\u0000r(B)T r(B)\n\u0001−1r(B)T whose size influences the maximal error in the inference of G. This, in turn, influences\nthe size of the error in JG, the policy evaluation function, where\nJG(π) :=\nE\n⃗s∼P π(⃗s)\n\u0002\nG(⃗s)\n\u0003\n.\nWe obtain:\nTheorem B.23. Let G be the true reward function, B the belief operator corresponding to the human’s true belief model\nB(⃗s | ⃗o), and B(G) be the resulting observation-based return function. Assume that ker B ∩ im Γ = {0}, so that\nr(B)T r(B) is invertible. Let ∆ : R ⃗S → R⃗Ω be a perturbation satisfying ∥ ∆ ∥ ≤ ρ, where ρ satisfies the following two\nproperties:\nρ ≤\n\r\rr(B)\n\r\r,\nρ ≤ −\n\r\rr(B)\n\r\r +\nq\r\rr(B)\n\r\r2 + 1/2 ·\n\r\r\u0000r(B)T r(B)\n\u0001−1\r\r−1.\nLet B∆ := B + ∆ be the misspecified belief operator. The first claim is that r(B∆)T r(B∆) is invertible under these\nconditions.\nNow, assume that the learning system infers the return function ˜G :=\n\u0000r(B∆)T r(B∆)\n\u0001−1r(B∆)T (B(G)).4 Then there is\na polynomial Q(X, Y ) of degree five such that\n∥ ˜G − G∥ ≤ ∥G∥ · Q\n\u0010\r\r(r(B)T r(B))−1\r\r, ∥r(B)∥\n\u0011\n· ρ.\nThus, for all policies π, we obtain\n\f\f\fJ ˜\nG(π) − JG(π)\n\f\f\f ≤ ∥G∥ · Q\n\u0010\r\r(r(B)T r(B))−1\r\r, ∥r(B)∥\n\u0011\n· ρ.\nIn particular, for sufficiently small perturbances ρ, the error in the inferred policy evaluation function J ˜\nG becomes arbitrarily\nsmall.\n3Note the factor 1/2 compared to the definition of τ(A) in Equation (9).\n4Note that there is not necessarily a ˜G with r(B∆)( ˜G) = B(G) since r(B∆) is not always surjective. Nevertheless, ˜G :=\n\u0000r(B∆)T r(B∆)\n\u0001−1r(B∆)T (B(G)) is the best attempt at a solution in the sense that r(B∆)( ˜G) then minimizes the Euclidean distance\nto B(G).\n29\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nProof. That r(B∆)T r(B∆) is invertible follows immediately from Proposition B.21 by using that ∥r(∆)∥ ≤ ∥ ∆ ∥ and\nthat r(B∆) = r(B)r(∆), together with the second bound on ρ (which implies the assumed bound in Proposition B.21).\nWe have\n\f\f\fJ ˜\nG(π) − JG(π)\n\f\f\f =\n\f\f\f\nE\n⃗s∼P π(⃗s)\n\u0002\n( ˜G − G)(⃗s)\n\u0003\f\f\f\n≤\nE\n⃗s∼P π(⃗s)\nh\f\f( ˜G − G)(⃗s)\n\f\f\ni\n≤ max\n⃗s∈ ⃗S\n\f\f( ˜G − G)(⃗s)\n\f\f\n≤ ∥ ˜G − G∥\n=\n\r\r\r\r\nh\u0000r(B∆)T r(B∆)\n\u0001−1r(B∆)T −\n\u0000r(B)T r(B)\n\u0001−1r(B)T i\n· B(G)\n\r\r\r\r\n≤\n\r\r\u0000r(B∆)T r(B∆)\n\u0001−1r(B∆)T −\n\u0000r(B)T r(B)\n\u0001−1r(B)T \r\r ·\n\r\r B(G)\n\r\r\n≤ C(r(B), ρ) · ∥r(B)(G)∥\n≤ C(r(B), ρ) · ∥r(B)∥ · ∥G∥.\nIn the second to last step, we used Proposition B.21. By Proposition B.22, we can define the polynomial Q(X, Y ) by\nQ(X, Y ) = XY ·\nh\n12XY 2 + 1\ni\n,\nwhich is of degree five.\nThe last claim follows from limρ→0 ρ = 0.\nRemark B.24. In the case of a square matrix B that is injective, we can apply Theorem B.18 directly to B−1 (which is now\ninvertible) and obtain the following simplification of Theorem B.23 for the case that ∥ ∆ ∥ ≤ ρ ≤ 1\n2 · ∥ B−1 ∥−1:\n\f\fJ ˜\nG(π) − JG(π)\n\f\f ≤ ρ · 2 · ∥ B ∥ · ∥G∥ · ∥ B−1 ∥2.\nThe polynomial is then only of degree 3.\nB.7. Preliminary Characterizations of the Ambiguity\nRecall the sequence of functions\nRS\nR ⃗S\nR⃗Ω.\nΓ\nB\nIn this section, we clarify im Γ and ker B in special cases, as their intersection is the crucial ambiguity in Theorem B.2.\nThe following proposition shows that for deterministic P ⃗O and a rational human, ker B decomposes into hyperplanes defined\nby normal vectors of probabilities of sequences mapping to the same observation sequence:\nProposition B.25. Assume the human reasons as in Section B.1. Assume P ⃗O is deterministic. Let B(⃗s) be the distribution\nof sequences under the human’s belief over the policy, given by B(⃗s) =\nR\nπ′ B(π′)P π′(⃗s) for some policy prior B(π′). For\neach ⃗o, let B⃗o := [B(⃗s)]⃗s: ⃗O(⃗s)=⃗o ∈ R{⃗s∈ ⃗S | ⃗O(⃗s)=⃗o} be the vector of probabilities of sequences that are observed as ⃗o.\nLet G′ be a return function. For each ⃗o ∈ ⃗Ω, define the restriction G′\n⃗o ∈ R{⃗s∈ ⃗S| ⃗O(⃗s)=⃗o} by G′\n⃗o(⃗s) := G′(⃗s) for all\n⃗s ∈ {⃗s ∈ ⃗S | ⃗O(⃗s) = ⃗o}. Assume that B(⃗s | ⃗o) is the Bayesian posterior. Then G′ ∈ ker B if and only if the property\nB⃗o · G′\n⃗o = 0\nholds for all ⃗o ∈ ⃗Ω.\nProof. For a deterministic observation kernel P ⃗O, by Bayes rule we have\nB(⃗s | ⃗o) =\nP ⃗O(⃗o | ⃗s) · B(⃗s)\nP\n⃗s ′ P ⃗O(⃗o | ⃗s′) · B(⃗s′)\n30\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n=\nδ⃗o\n\u0000 ⃗O(⃗s)\n\u0001\n· B(⃗s)\nP\n⃗s ′ δ⃗o\n\u0000 ⃗O(⃗s′)\n\u0001\n· B(⃗s′)\n=\n(\n0, ⃗O(⃗s) ̸= ⃗o\nB(⃗s)\nP\n⃗s ′: ⃗\nO(⃗s ′)=⃗o B(⃗s ′), ⃗O(⃗s) = ⃗o.\nThus, for any return function G′ and any observation sequence ⃗o, we have\n\u0002\nB(G′)\n\u0003\n(⃗o) =\nE\n⃗s∼B(⃗s|⃗o)\n\u0002\nG′(⃗s)\n\u0003\n=\nX\n⃗s\nB(⃗s | ⃗o)G′(⃗s)\n=\nX\n⃗s: ⃗O(⃗s)=⃗o\nB(⃗s)\nP\n⃗s ′: ⃗O(⃗s ′)=⃗o B(⃗s′)G′(⃗s)\n=\n \nX\n⃗s ′: ⃗O(⃗s ′)=⃗o\nB(⃗s′)\n!−1\n·\nX\n⃗s: ⃗O(⃗s)=⃗o\nB(⃗s)G′(⃗s).\nThus, we have G′ ∈ ker B if and only if\nB⃗o · G′\n⃗o =\nX\n⃗s: ⃗O(⃗s)=⃗o\nB(⃗s)G′(⃗s) = 0\nfor all ⃗o. That was to show.\nRemark B.26. One can interpret the previous proposition as follows:\nAs long as ⃗O is injective, we have\n\f\f{⃗s ∈ ⃗S | ⃗O(⃗s) = o}\n\f\f = 1 for all ⃗o, meaning that B⃗o and G′\n⃗o have only one entry. Thus,\nB⃗o · G′\n⃗o = 0 implies G′\n⃗o = 0. If that holds for all ⃗o, then G′ ∈ ker B implies G′ = 0, meaning B is injective.\nHowever, as soon as there is an ⃗o with k⃗o :=\n\f\f{⃗s ∈ ⃗S | ⃗O(⃗s) = o}\n\f\f > 1, the equation B⃗o · G′\n⃗o = 0 leads to k⃗o − 1 free\nparameters in G′\n⃗o. G′\n⃗o can then be chosen freely in the hyperplane of vectors orthogonal to B⃗o without moving out of the\nkernel of B.\nAnother way of writing Proposition B.25 is to write ker B as a direct sum of these hyperplanes perpendicular to B⃗o:\nker B =\nM\n⃗o: | ⃗O−1(⃗o)|≥2\nB⊥\n⃗o .\nRecall that a return function G is called time-separable if there exists a reward function R such that Γ(R) = G.\nBefore we discuss time-separability in more interesting examples, we want to talk about one simple case where all return\nfunctions are time-separable. We leave a general characterization of im Γ to future work.\nProposition B.27. Let there be an ordering ⃗s(1),⃗s(2), . . . of all sequences in ⃗S, and a function ϕ : ⃗S → S from sequences\nto states such that ϕ(⃗s) ∈ ⃗s and ϕ(⃗s(k)) /∈ ⃗s(i) for all i < k. Then every return function is time-separable.\nProof. Let G be a return function. Initialize R(s) = 0 for all s and inductively update it for all i = 1, 2, . . . :\nR\n\u0000ϕ(⃗s(i))\n\u0001 :=\n \nX\nt: s(i)\nt\n=ϕ(⃗s(i))\nγt\n!−1\n·\n \nG(⃗s(i)) −\nX\nt: s(i)\nt\n̸=ϕ(⃗s(i))\nγt · R\n\u0000s(i)\nt\n\u0001\n!\n,\nwhere the inductive definition always uses R as it is defined by that point in time. Once R\n\u0000ϕ(⃗s(i))\n\u0001\nis defined, but not yet\nany future values R\n\u0000ϕ(⃗s(k))\n\u0001\n, k > i, we have\n\u0002\nΓ(R)\n\u0003\n(⃗s(i)) =\nT\nX\nt=0\nγt · R\n\u0000s(i)\nt\n\u0001\n31\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n=\n \nX\nt: s(i)\nt\n=ϕ(⃗s(i))\nγt\n!\n· R\n\u0000ϕ(⃗s(i))\n\u0001\n+\nX\nt: s(i)\nt\n̸=ϕ(⃗s(i))\nγt · R\n\u0000s(i)\nt\n\u0001\n= G(⃗s(i)).\nFurthermore, the property ϕ(⃗s(k)) /∈ ⃗s(i) for all i < k ensures that changes to the reward function for k > i do not affect the\nvalue of\n\u0002\nΓ(R)\n\u0003\n(⃗s(i)). This shows Γ(R) = G, and thus G is time-separable.\nCorollary B.28. In a multi-armed bandit, every return function is time-separable.\nProof. In a multi-armed bandit, states and sequences are equivalent, and so we can choose ϕ(s) = s for every state/sequence\ns. The result follows from Proposition B.27.\nAlternatively, simply directly notice that in a multi-armed bandit, Γ is the identity mapping, and so for every return/reward\nfunction R, we have Γ(R) = R.\nB.8. Examples Supplementing Section 5\nIn this whole section, the inverse temperature parameter in the human choice probabilities is given by β = 1. We now\nconsider four more mathematical examples of Corollary B.4 and Theorem B.9. In the first example, the ambiguity is so bad\nthat the reward inference can become worse than simply maximizing Jobs as in naive RLHF. In Example B.30, there is\nsimply “noise” in the observations and the human’s belief, the matrices B and O are injective, and identifiability works, as\nin Corollary B.14. In the third example, the matrix B is not injective and identifiability fails, which is a minimal example\nshowing the limits of our main theorems. In the fourth example, the matrix B is not injective, but ker B ∩ im Γ = {0}, and\nso identifiability works. This example is interesting in that the identifiability simply emerges through different distributions\nof delay that are caused by the different unobserved events.\nIn this section, both the linear operators B : R ⃗S → R⃗Ω and O : R⃗Ω → R ⃗S are considered as matrices\nO =\n\u0000P ⃗O(⃗o | ⃗s)\n\u0001\n⃗s,⃗o ∈ R\n⃗S×⃗Ω,\nB =\n\u0000B(⃗s | ⃗o)\n\u0001\n⃗o,⃗s ∈ R\n⃗Ω× ⃗S.\nNotice that both have a swap in their indices.\nExample B.29. Theorem 5.1 shows that the remaining ambiguity from the human’s choice probabilities is given by\nker B ∩ im Γ, but it doesn’t explain how to proceed given this ambiguity. Without further inductive biases, some reward\nfunctions within the ambiguity of the true reward function can be even worse than simply maximizing Jobs.\nE.g., consider a multi-armed bandit with three actions a, b, c, observation-kernel o = O(a) = O(b) ̸= O(c) = c\nand reward function R(a) = R(b) < R(c). If the human belief is given by B(a | o) = p = 1 − B(b | o), then\nR′ = α · (p − 1, p, 0) ∈ R{a,b,c} is in the ambiguity for all α ∈ R, and so ˜R := R + R′ is compatible with the choice\nprobabilities. However, for α ≪ 0, we have ˜R(a) > ˜R(b) and ˜R(a) > ˜R(c), and so optimizing against this reward function\nleads to a suboptimal policy.\nIn contrast, maximizing Jobs leads to the correct policy since a, b, and c all obtain their ground truth reward in this example.\nThis generally raises the question of how to tie-break reward functions in the ambiguity, or how to act conservatively given\nthe uncertainty, in order to consistently improve upon the setting in Section 4.1.\nExample B.30. This example is a special case of Corollary B.14. Consider a multi-armed bandit with two actions (which\nare automatically also states and sequences) a and b. In this case, the reward function and return function is the same.\nWe assume there to be two possible observations o(a), o(b) and the observation kernel to be non-deterministic, with\nprobabilities\nPO(o(j) | i) =\n(\n2/3, if i = j,\n1/3, else.\nIf we assume the human forms Bayesian posterior beliefs as in Section B.1 and to have a policy prior B(π′) such that\nB(a) =\nR\nπ π(a)B(π′)dπ = 1/2 and B(b) = 1/2, then it is easy to show that the human’s belief is the “reversed”\nobservation kernel:\nB(j | o(i)) = PO(o(i) | j).\n32\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nWe obtain\nO = B =\n\u00122/3\n1/3\n1/3\n2/3\n\u0013\n= 1\n3 ·\n\u00122\n1\n1\n2\n\u0013\nThese matrices are injective since they are invertible:\nO−1 = B−1 =\n\u0012 2\n−1\n−1\n2\n\u0013\n.\nMore generally, even if the human does not form fully rational posterior beliefs, it is easy to imagine that the matrix B can\nend up being invertible. Thus, Corollary B.4 guarantees that the reward function can be inferred up to an additive constant\nfrom the choice probabilities of observations, and Theorem B.9 shows that this even works when the learning system does\nnot know what the human observed.\nIn the rest of this example, we explicitly walk the reader through the process of how the reward function can be inferred,\nin the general case that the observations are not known. In the process, we essentially recreate the proof of the theorems\nfor this special case. For this aim, we first want to compute the choice probabilities P R\u0000i ≻ j\n\u0001\nthat the learning system\nhas access to in the limit of infinite data. We assume that the reward function is given by R(a) = −1 and R(b) = 2. We\ncompute:\nB(R) = 1\n3 ·\n\u0012\n2\n1\n1\n2\n\u0013\n·\n\u0012\n−1\n2\n\u0013\n=\n\u0012\n0\n1\n\u0013\n.\nIn other words, we have Es∼B(s|o(a))[R(s)] = 0 and Es∼B(s|o(b))[R(s)] = 1. From this, we can compute the observation-\nbased choice probabilities ePo(i)o(j) = σ\n\u0000B(R)(o(i)) − B(R)(o(j))\n\u0001\n, see Equation (2), and obtain:\nePo(a)o(a) = ePo(b)o(b) = 1\n2,\nePo(a)o(b) =\n1\n1 + e,\nePo(b)o(a) =\ne\n1 + e.\nWe can now determine the final choice probabilities Pij := P R\u0000i ≻ j\n\u0001\nagain by a matrix-vector product, with the indices\nordered lexicographically, see Equation (8). Here, O ⊗ O is the Kronecker product of the matrix O with itself:\nP = (O ⊗ O) · eP = 1\n9 ·\n\n\n\n\n4\n2\n2\n1\n2\n4\n1\n2\n2\n1\n4\n2\n1\n2\n2\n4\n\n\n\n ·\n\n\n\n\n1/2\n1/(1 + e)\ne/(1 + e)\n1/2\n\n\n\n =\n\n\n\n\n1/2\n1/3 · (2 + e)/(1 + e)\n1/3 · (1 + 2e)/(1 + e)\n1/2\n\n\n\n .\nFor example, the second entry in P is Pab = P R\u0000a ≻ b\n\u0001\n=\n2+e\n3·(1+e). This is the likelihood that, for ground-truth actions a, b,\nthe human will prefer a after only receiving observations o(a) or o(b) according to O and following a Boltzman-rational\npolicy based on the belief of the real action, see Equation (8).\nOver time, the learning system will be able to estimate these probabilities based on repeated human choices, assuming all\nstate-pairs are sampled infinitely often. The question of identifiability is whether the original reward function R can be\ninferred from that data, given that the learning system knows O and B. We assume that the learning system doesn’t a priori\nknow R or any of the intermediate steps in the computation. First, eP can be inferred by inverting O ⊗ O:\neP = (O ⊗ O)−1 · P =\n\n\n\n\n4\n−2\n−2\n1\n−2\n4\n1\n−2\n−2\n1\n4\n−2\n1\n−2\n−2\n4\n\n\n\n ·\n\n\n\n\n1/2\n1/3 · (2 + e)/(1 + e)\n1/3 · (1 + 2e)/(1 + e)\n1/2\n\n\n\n =\n\n\n\n\n1/2\n1/(1 + e)\ne/(1 + e)\n1/2\n\n\n\n .\nThe learning system wants to use this to infer B( ˜R) (for the later-to-be inferred reward function ˜R that may differ from the\ntrue reward function R) and uses the equation\nePo(a)o(b) =\nexp\n\u0000B( ˜R)(o(a))\n\u0001\nexp\n\u0000B( ˜R)(o(a))\n\u0001\n+ exp\n\u0000B( ˜R)(o(b))\n\u0001,\n33\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nwhich can be rearranged to\nB( ˜R)(o(a)) = log\nePo(a)o(b)\n1 − ePo(a)o(b)\n+ B( ˜R)(o(b)) = log 1/(1 + e)\ne/(1 + e) + B( ˜R)(o(b)) = B( ˜R)(o(b)) − 1.\nThis relation is all which can be inferred about B( ˜R)(o(a)) and B( ˜R)(o(b)); the precise value cannot be determined and\nB( ˜R)(o(b)) is a free parameter. One can check that for B( ˜R)(o(b)) = 1 this coincides with the true value B(R). Finally,\none can invert B to infer ˜R from this:\n˜R = B−1 · B( ˜R)\n=\n\u0012\n2\n−1\n−1\n2\n\u0013\n·\n\u0012\nB( ˜R)(o(b)) − 1\nB( ˜R)(o(b))\n\u0013\n=\n\u0012B( ˜R)(o(b)) − 2\n1 + B( ˜R)(o(b))\n\u0013\n=\n\u0012−1\n2\n\u0013\n+\n\u0012B( ˜R)(o(b)) − 1\nB( ˜R)(o(b)) − 1\n\u0013\n= R +\n\u0012B( ˜R)(o(b)) − 1\nB( ˜R)(o(b)) − 1\n\u0013\n.\nThus, the inferred and true reward functions differ maximally by a constant, as predicted in Theorem B.9.\nIn the following example, we work out a case where the reward function is so ambiguous that any policy is optimal to some\nreward function consistent with the human feedback:\nExample B.31. Consider a multi-armed bandit with exactly three actions/states a, b, c. We assume a deterministic\nobservation kernel with o := O(a) = O(c) ̸= O(b) = b. Assume the human has some arbitrary beliefs B(a | o), B(c | o) =\n1 − B(a | o), and can identify b: B(b | b) = 1. Then if the human makes observation comparisons with a Boltzman-rational\npolicy, as in Theorem B.2, the resulting reward function is so ambiguous that some reward functions consistent with the\nfeedback place the highest value on action a, no matter the true reward function R. Thus, even if the true reward function R\nregards a as the worst action, a can result from the reward learning and subsequent policy optimization process.\nProof. The matrix B : R{a,b,c} → R{o,b} is given by\nB =\n\u0012B(a | o)\n0\nB(c | o)\n0\n1\n0\n\u0013\n.\nIts kernel is given by reward functions R′ with R′(b) = 0 and R′(c) = − B(a|o)\nB(c|o) R′(a), with R′(a) a free parameter.\nTheorem B.2 shows that, up to an additive constant, the reward functions consistent with the feedback of observation\ncomparisons are given by ˜R = R + R′ for any R′ ∈ ker B. Thus, whenever the free parameter R′(a) satisfies R′(a) >\nR(b) − R(a) and R′(a) > B(c | o) ·\n\u0000R(c) − R(a)\n\u0001\n, we obtain ˜R(a) > ˜R(b) and ˜R(a) > ˜R(c), showing the claim.\nWe now investigate another example where B is not injective, and yet, identifiability works because B ◦ Γ ̸= {0}. We saw\nsuch cases already in Example C.6, but include this additional example since it shows a conceptually interesting case: two\ndifferent states lead to the exact same observations, but can be disambiguated since they lead to different amounts of delay\nuntil a more informative observation is made again.\nExample B.32. In this example, we assume that the human knows the policy π that generates the state sequences\n(corresponding to a policy prior B(π′) = δπ(π′) concentrated on π), which together with knowledge of the transition\ndynamics of the environment determines the true state transition probabilities T π(s′ | s) = P\na∈A T (s′ | s, a) · π(a | s).\nWe consider an environment with three states s, s′, s′′ and the following transition dynamics T π, where p ̸= 1/2 is a\n34\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nprobability:\ns\ns′\ns′′\n1/3\n1/3\n1/3\n1−p\np\np\n1−p\nWe assume that P0(s) = 1. Furthermore, we assume deterministic observations and s = O(s) ̸= O(s′) = O(s′′) =: o.\nAssume the time horizon T is 3, i.e., there are timesteps 0, 1, 2, 3. Assume that the human forms the belief over the true state\nsequence by Bayesian posterior updates as in Section B.1. In this case, ker B ̸= {0} by Proposition B.11. However, we will\nnow show that ker(B ◦ Γ) = {0}. If the human makes Boltzmann-rational comparisons of observation sequences, then this\nimplies the identifiability of the return function up to an additive constant by Corollary B.4.5\nThus, let R′ ∈ ker(B ◦ Γ), i.e.,\nh\nB\n\u0000Γ(R′)\n\u0001i\n(⃗o) = 0 for every observation sequence ⃗o. For ⃗o = ssss being the observation\nsequence that only consists of state s, this implies R′(s) = 0. Consequently, for general observation sequences ⃗o, we have:\n0 =\nh\nB\n\u0000Γ(R′)\n\u0001i\n(⃗o) =\nE\n⃗s∼B(⃗s|⃗o)\n\"\n3\nX\nt=0\nδs′(st) · γt\n#\n· R′(s′) +\nE\n⃗s∼B(⃗s|⃗o)\n\"\n3\nX\nt=0\nδs′′(st) · γt\n#\n· R′(s′′).\nNow we specialize this equation to the two observation sequences ⃗o(1) = soss and ⃗o(2) = soos. We start by considering\n⃗o(1). This is consistent with the two state sequences ⃗s(1),(s′) = ss′ss and ⃗s(1),(s′′) = ss′′ss. We have posterior probabilities\nB\n\u0000⃗s(1),(s′) | ⃗o(1)\u0001\n= 1 − p,\nB\n\u0000⃗s(1),(s′′) | ⃗o(1)\u0001\n= p,\nand therefore\n0 =\n\u0002\nB\n\u0000Γ(R′)\n\u0001i\n(⃗o(1)) = (1 − p) · γ · R′(s′) + p · γ · R′(s′′),\nand so\nR′(s′) =\np\np − 1 · R′(s′′).\n(11)\nSimilarly, ⃗o(2) is consistent with the sequences ⃗s(2),(s′) = ss′s′s and ⃗s(2),(s′′) = ss′′s′′s. They have posterior probabilities\nB\n\u0000⃗s(2),(s′) | ⃗o(2)\u0001\n= 1\n2,\nB\n\u0000⃗s(2),(s′′) | ⃗o(2)\u0001\n= 1\n2,\nleading to\n0 = 1\n2 · (γ + γ2) · R′(s′) + 1\n2 · (γ + γ2) · R′(s′′).\nTogether with Equation (11), we obtain\nR′(s′′) = −R′(s′) =\np\n1 − p · R′(s′′),\nwhich implies R′(s′′) = 0 because p ̸= 1\n2, and thus also R′(s′) = 0. Overall, we have showed R′ = 0, and so B ◦ Γ is\ninjective. This means that reward functions are identifiable in this example up to an additive constant, see Corollary B.4.\nC. Issues of Naively Applying RLHF under Partial Observability\nIn this section, we study the naive application of RLHF under partial observability. Thus, most of it takes a step back from\nthe general theory of appropriately modeled partial observability in RLHF. Later, we will analyze examples where we also\napply the general theory, which is why this appendix section comes second.\n5We assume that the learning system knows what the human observes, which is valid since PO is deterministic. Alternatively, one can\nargue with Proposition B.11 that O is automatically injective, meaning one can apply Theorem B.9.\n35\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nIn Section C.1, we first briefly explain what happens when the learning system incorrectly assumes that the human observes\nthe full environment state. We show that as a consequence, the system is incentivized to infer what we call the observation\nreturn function Gobs, which evaluates a state sequence based on the human’s belief of the state sequence given the human’s\nobservations. In the policy optimization process, the policy is then selected to maximize Jobs, an expectation over Gobs. In\nthe interlude in Section C.2, we then briefly analyze the unrealistic case that the human, when evaluating a policy π, fully\nknows the complete specification of that policy and all of the environment and engages in rational Bayesian reasoning; in\nthis case, Jobs = J is the true policy evaluation function.\nRealistically, however, maximizing Jobs can lead to failure modes. In Appendix C.3 we prove that a suboptimal policy\nthat is optimal according to Jobs causes deceptive inflation, overjustification, or both. In Appendix C.4, we expand on\nthe analysis of the main examples in the main paper. Finally, in Section C.5, we study further concrete examples where\nmaximizing Jobs reveals deceptive and overjustifying behavior by the resulting policy.\nC.1. Optimal Policies under RLHF with Deterministic Partial Observations Maximize Jobs\nAssume that P ⃗O is deterministic and that the human makes Boltzmann-rational sequence comparisons between observation\nsequences. The true choice probabilities are then given by (See Equations (2) and (8)):\nP R\u0000⃗s ≻ ⃗s′\u0001\n= σ\n\u0012\nβ ·\n\u0010\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\n−\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s′)\n\u0001\u0011\u0013\n(12)\nNow, assume that the learning system does not model the situation correctly. In particular, we assume:\n• The system is not aware that the human only observes observation sequences ⃗O(⃗s) instead of the full state sequences.\n• The system does not model that the human’s return function is time-separable, i.e., comes from a reward function R\nover environment states.\nThe learning system then thinks that there is a return function ˜G ∈ R⃗S such that the choice probabilities are given by the\nfollowing faulty formula:\nP R\u0000⃗s ≻ ⃗s′\u0001 := σ\n\u0010\nβ\n\u0000G(⃗s) − G(⃗s′)\n\u0001\u0011\nNow, assume that the learning system has access to the choice probabilities and wants to infer G. Inverting the sigmoid\nfunction and then plugging in the true choice probabilities from Equation (12), we obtain:\n˜G(⃗s) = 1\nβ log P R(⃗s ≻ ⃗s′)\nP R(⃗s′ ≻ ⃗s) + ˜G(⃗s′)\n= 1\nβ\nh\nβ ·\n\u0010\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\n−\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s′)\n\u0001\u0011i\n+ ˜G(⃗s′)\n=\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\n+ C(⃗s′).6\nHere, C(⃗s′) is some quantity that does not depend on ⃗s. Now, fix ⃗s′ as a reference sequence. Then for varying ⃗s, C(⃗s′) is\nsimply an additive constant. Consequently, up to an additive constant, this determines the return function that the learning\nsystem is incentivized to infer. We call it the observation return function since it is the return function based on the human’s\nobservations:\nGobs(⃗s) :=\n\u0000B ·G\n\u0001\u0000 ⃗O(⃗s)\n\u0001\n.\nThis return function is not necessarily time-separable, but we assume that time-separability is not modeled correctly by the\nlearning system. Now, define the resulting policy evaluation function Jobs by\nJobs(π) :=\nE\n⃗s∼P π(⃗s)\n\u0002\nGobs(⃗s)\n\u0003\n.\nThis is the policy evaluation function that would be optimized if the learning system erroneously inferred the return function\nGobs.\n6Note that in the case of non-deterministic observation kernels and choice probabilities given as in Equation (8), this argument does\nnot work since the logarithm cannot be swapped with the outer expectation of the choice probabilities.\n36\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nC.2. Interlude: When the Human Knows the Policy and is a Bayesian Reasoner, then Jobs = J\nIn this section, we briefly consider what would happen if in Jobs, the human’s belief B would make use of the true policy\nand be a rational Bayesian posterior as in Section B.1. We will show that under these conditions, we have Jobs = J. Since\nthese are unrealistic assumptions, no other section depends on this result.\nFor the analysis, we drop the assumption that the observation sequence kernel P ⃗O is deterministic, and assume that Jobs is\ngiven as follows:\nJobs(π) :=\nE\n⃗s∼P π(⃗s)\n\"\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0014\nE\n⃗s ′∼Bπ(⃗s ′|⃗o)\n\u0002\nG(⃗s′)\n\u0003\u0015#\n.\n(13)\nIn this formula, Bπ(⃗s | ⃗o) := B(⃗s | ⃗o, π) with B being the joint distribution from Section B.1. Formally, this is the posterior\nof the joint distribution B(⃗s,⃗o | π) that is given by the following hidden Markov model:\ns0\ns1\ns2\ns3\n. . .\no0\no1\no2\no3\n. . .\nPO\nT π\nPO\nT π\nPO\nT π\nPO\nT π\n(14)\nHere, T π(s′ | s) := P\na∈A T (s′ | s, a) · π(a | s). s0 is sampled according to the known initial distribution P0(s0). The\nhuman’s posterior Bπ(⃗s′ | ⃗o) is then the true posterior in this HMM. We obtain:\nProposition C.1. Let π be a policy that is known to the human. Then Jobs(π) = J(π).\nProof. By Equation (13), we have\nJobs(π) =\nE\n⃗s∼P π(⃗s)\n\"\nE\n⃗o∼P ⃗\nO(⃗o|⃗s)\n\u0014\nE\n⃗s ′∼Bπ(⃗s ′|⃗o)\n\u0002\nG(⃗s′)\n\u0003\u0015#\n(1)\n=\nX\n⃗s\nP π(⃗s)\nX\n⃗o\nP ⃗O(⃗o | ⃗s)\nX\n⃗s ′\nBπ(⃗s′ | ⃗o)G(⃗s′)\n(2)\n=\nX\n⃗s ′\n\" X\n⃗o\nBπ(⃗s′ | ⃗o)\n\" X\n⃗s\nP ⃗O(⃗o | ⃗s)P π(⃗s)\n##\nG(⃗s′)\n(3)\n=\nX\n⃗s ′\n\" X\n⃗o\nBπ(⃗s′ | ⃗o)Bπ(⃗o)\n#\nG(⃗s′)\n(4)\n=\nX\n⃗s ′\n\" X\n⃗o\nP π(⃗s′)P ⃗O(⃗o | ⃗s′)\n#\nG(⃗s′)\n(5)\n=\nX\n⃗s ′\nP π(⃗s′)G(⃗s′)\n(6)\n=\nX\n⃗s\nP π(⃗s)G(⃗s)\n(7)\n= J(π).\nIn step (1), we wrote the expectations out in terms of sums. In step (2), we reordered them. In step (3), we observed that the\ninner sum over ⃗s evaluates to the marginal distribution Bπ(⃗o) of the observation sequence ⃗o in the HMM in Equation (13).\nIn step (4), we used Bayes rule in the inner sum. This is possible since Bπ(⃗s′ | ⃗o) is the true posterior when π is known. In\nstep (5), we pull P π(⃗s′) out and notice that the remaining inner sum evaluates to 1. Step (6) is a relabeling and step (7) the\ndefinition of the true policy evaluation function J.\n37\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nC.3. Proof of Theorem 4.5\nWe first prove the following lemma (this is Lemma 4.4, repeated here for convenience):\nLemma C.2. Let π and πref be two policies. If J(π) < J(πref) and Jobs(π) ≥ Jobs(πref), then relative to πref, π must\nexhibit deceptive inflating, overjustification, or both.\nProof. We start by establishing a quantitative relationship between the average overestimation and underestimation errors\nE\n+ and E\n− as defined in Definition 4.2, the true policy evaluation function J, and the observation evaluation function Jobs\ndefined in Equation (4). Define ∆ : ⃗S → R by ∆(⃗s) = Gobs(⃗s) − G(⃗s), where Gobs is as defined in Equation (3). Consider\nthe quantity\nE+(⃗s) − E−(⃗s) = max\n\u00000, ∆(⃗s)\n\u0001\n− max\n\u00000, −∆(⃗s)\n\u0001\n.\nIf ∆(⃗s) > 0, then the first term is ∆(⃗s) and the second one is 0. If ∆(⃗s) < 0, then the first term is zero and the second one\nis ∆(⃗s). If ∆(⃗s) = 0, then both terms are zero. In all cases the right-hand side is equal to ∆(⃗s). Unpacking the definition of\n∆ again, we have that for all ⃗s,\nE+(⃗s) − E−(⃗s) = Gobs(⃗s) − G(⃗s).\n(15)\nFor any policy π, if we take the expectation of both sides of this equation over the on-policy distribution admitted by π, P π,\nwe get\nE\n+(π) − E\n−(π) = Jobs(π) − J(π).\n(16)\nWe now prove the lemma. Let π and πref be two policies, and assume that J(π) < J(πref) and Jobs(π) ≥ Jobs(πref).\nEquivalently, we have Jobs(π) − Jobs(πref) ≥ 0 and J(πref) − J(π) > 0, which we combine to state\n\u0010\nJobs(π) − Jobs(πref)\n\u0011\n+\n\u0010\nJ(πref) − J(π)\n\u0011\n> 0.\n(17)\nRearranging terms yields\n\u0010\nJobs(π) − J(π)\n\u0011\n−\n\u0010\nJobs(πref) − J(πref)\n\u0011\n> 0.\nThese two differences inside parentheses are equal to the right-hand side of (16) for π and πref, respectively. We substitute\nthe left-hand side of (16) twice to obtain\n\u0010\nE\n+(π) − E\n−(π)\n\u0011\n−\n\u0010\nE\n+(πref) − E\n−(πref)\n\u0011\n> 0.\nRearranging terms again yields\n\u0010\nE\n+(π) − E\n+(πref)\n\u0011\n+\n\u0010\nE\n−(πref) − E\n−(π)\n\u0011\n> 0.\n(18)\nIf E\n+(π) − E\n+(πref) > 0 then we have E\n+(π) > E\n+(πref) and, by assumption, Jobs(π) ≥ Jobs(πref). By Definition 4.3,\nthis means π exhibits deceptive inflating relative to πref.\nIf E\n−(πref) − E\n−(π) > 0 then we have E\n−(π) < E\n−(πref) and, by assumption, J(π) < J(πref). By Definition 4.3, this\nmeans π exhibits overjustification relative to πref.\nAt least one of the two differences in parentheses in (18) must be positive, otherwise their sum would not be positive. Thus\nπ must exhibit deceptive inflating relative to πref, overjustification relative to πref, or both.\nWe can now combine earlier results to prove Theorem 4.5, repeated here for convenience:\nTheorem C.3. Assume that PO is deterministic. Let π∗\nobs be an optimal policy according to a naive application of RLHF\nunder partial observability, and let π∗ be an optimal policy according to the true objective J. If π∗\nobs is not J-optimal, then\nrelative to π∗, π∗\nobs must exhibit deceptive inflating, overjustification, or both.\nProof. Because PO is deterministic, π∗\nobs must be optimal with respect to Jobs by Proposition 4.1 (proved in Appendix C.1).\nThus Jobs(π∗\nobs) ≥ Jobs(π∗). Since π∗ is J-optimal and π∗\nobs is not, J(π∗) < J(π∗\nobs). By Lemma 4.4 (repeated as\nLemma C.2 above), relative to π∗, π∗\nobs must exhibit deceptive inflating, overjustification, or both.\n38\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nC.4. Derivations and Further Details for Section 4.3 Example A\nFigure 5. An expanded view of Figure 2A. Commands corresponding to the various actions are depicted along edges, and log messages\ncorresponding to the various observations are depicted underneath each state.\nWe first include Figure 5, a more detailed picture of the MDP and observation function in Section 4.3.1, to help ground the\nnarrative details of the example.\nNext we formally enumerate the details of the MDP and observation function.\n• S = {S, I, W, WH, L, LH, T}.\n• A = {aI, aC, aH, aT }.\n• T is as depicted in Figure 5 and Figure 2A. For a state s, any outgoing arrow labeled with an action a (such as aI)\ndescribes the distribution T (s′ | s, a) as follows: if the arrow does not split, then T (s′ | s, a) = 1 where s′ is the state\nthe arrow points to; if the arrow does split, then for each successor state s′ it eventually reaches, a probability q is\nwritten just before the box corresponding to s′ (for this example, q = p or q = 1 − p), and T (s′ | s, a) = q.\n◦ Additionally, any action taken from a state that does not have an outgoing arrow corresponding to that action will\nimmediately transition to state T, as though aT had been taken.\n◦ Any action taken from state T transitions deterministically to T.\n• P0(S) = 1.\n• R is as described in the table (the numbers in the top right of each state box) with r ≥ 0.\nAdditionally,\nR(S) = R(T) = 0.\n• γ = 1.\nWe work with a fixed horizon length of 3, meaning state sequences have length 4 (since time is zero-indexed: s0s1s2s3).\nThe observation function is also depicted in Figure 5. Each state deterministically produces the observation in the lower-right\ncorner of its box in the figure. We also write it in another format in Table 8.\ns\nS\nI\nW\nWH\nL\nLH\nT\nO(s)\no∅\noI\noW\noW\noL\no∅\no∅\nTable 8. The observation function O for the example in Section 4.3.1 and Appendix C.4.\nWe make the additional assumption that the human belief B(⃗s | ⃗o) only supports state sequences ⃗s which actually produce ⃗o\nunder the sequence observation function ⃗O: B(⃗s | ⃗o) > 0 =⇒ ⃗O(⃗s) = ⃗o. In particular, this means that for any ⃗o which is\nonly produced by one ⃗s, B(⃗o | ⃗s) = 1.\nThere are three pairs of state sequences which produce identical observation sequences. For each, we introduce a parameter\nrepresenting the probability the human infers the first of the pair of state sequences upon seeing their shared observation\nsequence.\n39\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n1. SILHT and SITT both produce o∅oIo∅o∅, a log containing only a success confirmation for installing drivers, again\nbecause O(LH) = O(T) = o∅. Let pH = B(⃗s = SILHT | ⃗o = o∅oIo∅o∅).\n2. STTT and SLHTT both produce o∅o∅o∅o∅, an empty log, since O(LH) = O(T) = o∅. Let p′\nH = B(⃗s = SLHTT |\n⃗o = o∅o∅o∅o∅).\n3. SIWT and SIWHT both produce o∅oIoW o∅, a log containing success confirmations for installing both drivers and\nCUDA. Let pW = B(⃗s = SIWT | ⃗o = o∅oIoW o∅).\nWe assume for simplicity that p′\nH = pH, i.e. that the human is just as likely to think an empty log following a successful\ndriver installation contains an error that was hidden with 2> /dev/null (pH), as they are to think that an entirely empty\nlog contains a hidden error (p′\nH). In principle, this need not be true: the human may have differing priors about the\nagent’s behavior in the two different circumstances. However, the algebra to reason about such a case is significantly more\ncumbersome, and this case reveals no fundamentally different agent behavior under our framework that isn’t present in some\nsimpler case.\nWe can thus write the full B as a matrix as in Table 9.\nSTTT\nSLHTT\nSLTT\nSITT\nSILHT\nSILT\nSIWT\nSIWHT\no∅o∅o∅o∅\n1 − pH\npH\no∅oLo∅o∅\n1\no∅oIo∅o∅\n1 − pH\npH\no∅oIoLo∅\n1\no∅oIoW o∅\npW\n1 − pW\nTable 9. The parameterized human belief function B for the example in Section 4.3.1 and Appendix C.4, expressed as a matrix (rendered\nas a table). Any empty cell is equal to 0.\nWe have laid the groundwork sufficiently to begin reasoning about the observation return, overestimation and underestimation\nerror, policies which are optimal under the reward function learned by naive RLHF, and the resulting deceptive inflatingand\noverjustification failure modes. We begin by computing the measures of interest for each state sequence, shown in Table 10.\n⃗s\nG(⃗s)\nGobs(⃗s) := E⃗s ′∼B(·| ⃗O(⃗s))[G(⃗s′)]\nE+(⃗s) := max(0,\nE−(⃗s) := max(0,\nGobs(⃗s) − G(⃗s))\nG(⃗s) − Gobs(⃗s))\nSTTT\n0\npHG(SLHTT) + (1 − pH)G(STTT)\n0\npH(5 + r)\nSLHTT\n−5 − r\n= −pH(5 + r)\n(1 − pH)(5 + r)\n0\nSLTT\n−5\n−5\n0\n0\nSITT\n1\npHG(SILHT) + (1 − pH)G(SITT)\n0\npH(5 + r)\nSILHT\n−4 − r\n= 1 − pH(5 + r)\n(1 − pH)(5 + r)\n0\nSILT\n−4\n−4\n0\n0\nSIWT\n11\npW G(SIWT) + (1 − pW )G(SIWHT)\n0\n0\nSIWHT\n11\n= 11\n0\n0\nTable 10. Measures of interest for each state sequence for the example in Section 4.3.1 and Appendix C.4. State sequences which produce\nthe same observations have their Gobs columns merged, since they necessarily have the same Gobs.\nAs an exercise, checking the computations in Table 10 is a quick way to gain some intuition for how these quantities relate.\nIt’s further useful to speak about these quantities using their names, and work through the stories these expressions tell.\nConsider the E+(SLHTT) cell. What does it mean that this is (1 − pH)(5 + r)? E+ is the overestimation error;\n(1 − pH)(5 + r) is the expected amount by which the human observer, upon seeing a completely empty log (o∅o∅o∅o∅),\nwill overestimate the total reward the agent attained when those empty logs were actually produced by SLHTT. This is a\ntrajectory in which the agent immediately uses the 2> /dev/null trick, fails to install CUDA without logging the error,\nand terminates. Under what circumstances might the human overestimate the total reward when ⃗s = SLHTT? Upon seeing\nthe empty log, the human thinks with probability 1 − pH that the agent simply terminated immediately, which would be\nworth a total reward of 0. Since the actual total reward is −5 − r, this is an overestimate by 5 + r. The human thinks with\n40\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nprobability pH that SLHTT occurred. This is correct, so there is no overestimation and this pH case does not contribute to\nthe overestimation error. So we have that with probability 1 − pH, the human overestimates the total reward by 5 + r.\nWe can keep going! Why is the underestimation error of SIWT equal to 0? Because the only other trajectory with which\nit can be confused attains the same total reward, so regardless of how the probability mass of the human’s belief divides\nbetween them, there will be no underestimation. Can all of the zeros in the overestimation and underestimation error\ncolumns be explained this way?\nWe now move on to consider policies rather than state sequences. Since a policy π imposes a distribution P π over state\nsequences (the “on-policy distribution”), our policy measures are in fact exactly parallel to our state sequence measures. Each\none is an expectation over the on-policy distribution of the columns of Table 10. We restrict our attention to deterministic\npolicies which only take actions depicted in Figure 5 (i.e. that never terminate via an action other than aT ), of which there\nare only six in this MDP. They are enumerated, along with the policy-level measures, in Table 11. Policies will be written as\na sequence of actions enclosed in brackets, omitting trailing repeated aT actions. This is nonstandard notation in an MDP\nwith stochastic transitions, but is unambiguous in this example, because all decisions are made before any stochasticity\noccurs. The policies are [aT ], [aHaT ], [aCaT ], [aIaT ], [aIaHaT ], and [aIaCaT ].\nπ\nJ(π)\nJobs(π)\nE\n+(π)\nE\n−(π)\n[aT ]\n0\n−pH(5 + r)\n0\npH(5 + r)\n[aHaT ]\n−5 − r\n−pH(5 + r)\n(1 − pH)(5 + r)\n0\n[aCaT ]\n−5\n−5\n0\n0\n[aIaT ]\n1\n1 − pH(5 + r)\n0\npH(5 + r)\n[aIaHaT ]\npG(SIWHT)\npGobs(SIWHT)\n(1 − p)(1 − pH)(5 + r)\n0\n+(1 − p)G(SILHT)\n+(1 − p)Gobs(SILHT)\n= 11 − (1 − p)(15 + r)\n= 11 − (1 − p) [10 + pH(5 + r)]\n[aIaCaT ]\npG(SIWT)\npGobs(SIWT)\n0\n0\n+(1 − p)G(SILT)\n+(1 − p)Gobs(SILT)\n= 11 − (1 − p) · 15\n= 11 − (1 − p) · 15\nTable 11. Measures of interest for each policy for the example in Section 4.3.1 and Appendix C.4. Each of the columns here is the\non-policy average of the corresponding column in Table 10. Policies are written as sequences of actions, omitting trailing repeated aT\nactions. This is nonstandard notation in an MDP with stochastic transitions, but is unambiguous in this example since all decisions are\nmade before any stochasticity occurs.\nWith this we have everything we need to characterize optimal policies under the reward function learned by a naive\napplication of RLHF (“policies selected by RLHF”). By Proposition 4.1, we know that if PO is deterministic, as in this\nexample, RLHF selects policies which maximize Jobs. In order to understand the behavior of these policies, we’ll also need\nto determine the true optimal policies, i.e. those which maximize J. We’ll proceed in cases, only considering boundary\ncases (specific measure-zero parameter values for which the result is different) insofar as they are interesting.\nCase 1: p > 1\n3. If p > 1\n3, the CUDA install (with default logging, aC) is likely enough to succeed that it’s worth attempting\nit: p · R(W) + (1 − p) · R(L) > 0. It also immediately follows that\nJ([aIaCaT ]) = Jobs([aIaCaT ]) = 11 − (1 − p) · 15 > 1.\nThis allows us to eliminate policies [aT ], [aHaT ], [aCaT ], and [aIaT ], which all have J ≤ 1 and Jobs ≤ 1. None of them\ncan thus be J-optimal or Jobs-optimal. All that remains is to compare J and Jobs for [aIaHaT ] and [aIaCaT ]. We can\ncheck the sign of the differences of these pairs of values, starting with J.\nJ([aIaCaT ]) − J([aIaHaT ]) = (1 − p)r.\nSince p is a probability and r is nonnegative, this value is positive (and thus [aIaCaT ] is preferred to [aIaHaT ] by the\nhuman) if and only if p < 1 and r > 0.\nJobs([aIaHaT ]) − Jobs([aIaCaT ]) = (1 − p) [5 − pH(5 + r)] .\n41\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nThis value is positive (and thus [aIaHaT ] is the policy RLHF selects) if and only if p < 1 and pH <\n5\n5+r.\nIf p = 1, then both differences are 0, and both J and Jobs are indifferent between the two policies. This makes sense, as\nthey differ only in the case where the CUDA installation fails; this happens with probability 1 − p = 0 when p = 1. Now\nsuppose p < 1. If r = 0, then the human is indifferent between the two policies. This also makes sense, as r is meant to\nquantify the extent to which the human dislikes suppressed failures; if it’s zero, then the human doesn’t care. However, if\npH <\n5\n5+r, then Jobs([aIaHaT ]) > Jobs([aIaHaT ]), and thus RLHF favors the 2> /dev/null policy [aIaHaT ].\nIf p < 1, r > 0, and pH <\n5\n5+r, then we have that J([aIaCaT ]) > J([aIaHaT ]) but Jobs([aIaCaT ]) > Jobs([aIaHaT ]).\nThus RLHF will select the 2> /dev/null policy [aIaHaT ], and by Theorem 4.5, since [aIaHaT ] is not J-optimal, then\nrelative to [aIaCaT ], it must exhibit deceptive inflating, overjustification, or both. Intuitively, we should be suspicious\nthat deceptive inflatingis at play whenever the agent hides information from the human. We should be suspicious when\nthe condition for a policy to be Jobs-optimal is that pH, the agent’s suspicion that the agent may be hiding errors, is low!\nIndeed, referencing Table 11, we have E\n+([aIaHaT ]) = (1 − p)(1 − pH)(5 + r) > 0 = E\n+([aIaCaT ]). Together with\nJobs([aIaHaT ]) > Jobs([aIaCaT ]), this satisfies the conditions of Definition 4.3, and thus this is an instance of deceptive\ninflating.\nIf p < 1, r > 0, and pH >\n5\n5+r, then [aIaCaT ] is optimal under both J and Jobs, and in this case, RLHF selects the true\noptimal policy.\nCase 2: p < 1\n3. In this case, the CUDA install is not likely enough to succeed to be worth attempting (under the true reward\nfunction). Mathematically, J([aIaHaT ]) ≤ J([aIaCaT ]) < 1 = J([aIaT ]). The other three policies are always worse\nunder J than [aIaT ], so we have our optimal policy π∗ = [aIaT ]. However,\nJobs([aIaHaT ]) − Jobs([aIaT ]) = p(10 + pH(5 + r)),\nwhich is always positive unless p = 0. As long as p > 0, i.e. installing CUDA is actually possible at all, then regardless\nof everything else, Jobs([aIaHaT ]) > Jobs([aIaT ]). We can interpret this reasonably: however good or bad it might look\nto terminate after installing drivers (SITT), using the 2> /dev/null trick will either produce the same observation as\nSITT (and thus the same feedback distribution) or will actually visibly install CUDA successfully—the most lucrative\npossible source of positive feedback. Even if there’s only a sliver of a chance of success and it’s a catastrophe to have a\nsilent failure (i.e. r ≫ 0), however likely you are to infer that the lack of a log message about CUDA means the agent hid a\nfailure, it will penalize [aIaT ] more than it will penalize [aIaHaT ]: where [aIaHaT ] produces this ambiguous observation\nwith probability 1 − p (when it fails), [aIaT ] will always produce the ambiguous observation.\nThis means that when 0 < p < 1\n3, it is impossible to recover the true optimal policy with naive RLHF. Which policies can\npossibly be Jobs-optimal for some setting of the parameters? We can similarly rule out [aT ] and [aHaT ] for 0 < p < 1\n3:\nJobs([aIaHaT ]) − Jobs([aIaT ]) = p(10 + pH(5 + r)) > 0.\nWe can rule out [aCaT ] by comparison to [aIaCaT ]: Jobs([aIaCaT ]) − Jobs([aCaT ]) = 16 − (1 − p)15 > 0. So we are\nleft with only [aIaHaT ] and [aIaCaT ] as candidate Jobs-optimal policies.\nAs in Case 1, we find that Jobs([aIaHaT ]) > Jobs([aIaT ]) if and only if p = 1 or pH <\n5\n5+r. In case 2 we have assumed\np < 1\n3, leaving only the pH condition.\nIf pH <\n5\n5+r, then RLHF selects [aIaHaT ]. As in Case 1, this is deceptive inflatingrelative to π∗ = [aIaT ], because\nE\n+([aIaHaT ]) = (1 − p)(1 − pH)(5 + r) > 0 = E\n+(π∗).\nIf pH >\n5\n5+r, then RLHF selects [aIaCaT ]. Because this policy is not J-optimal, by Theorem 4.5, we must have deceptive\ninflating, overjustification, or both. Which is it? Here the optimal policy is to terminate after installing drivers, [aIaT ].\nHowever, pH >\n5\n5+r. This can be rewritten as pH(5 + r) > 5. We have seen this expression pH(5 + r) before; it is the\nunderestimation error incurred on ⃗s = SITT and therefore also the average underestimation error of policy [aIaT ]. So\nhere the underestimation error on the optimal policy—that is, the risk that the human misunderstands optimal behavior\n(terminating after installing driver) as undesired behavior (attempting a CUDA install that was unlikely to work and hiding\nthe mistake)—is severe enough that the agent opts instead for [aIaCaT ], a worse policy that attempts the ill-fated CUDA\n42\nChallenges with Partial Observability of Human Evaluators in Reward Learning\ninstallation only to prove that it wasn’t doing so secretly. In qualitative terms, this is quintessential overjustification behavior.\nIndeed, relative to reference policy π∗ = [aIaT ], we have\nE\n−([aIaCaT ]) = 0 < pH(5 + r) = E\n−(π∗)\nJ([aIaCaT ]) = 11 − (1 − p) · 15 < 1 = J(π∗),\nand thus by Definition 4.3, this is overjustification.\nC.5. Further Examples Supplementing Section 4.3\nIn this section, we present further mathematical examples supplementing those in Section 4.3. We found many of them\nbefore finding the examples we discuss in the main paper, and show the same and additional conceptual features with\nsomewhat less polish. We again assume that P ⃗O is deterministic.\nExample C.4. In the main paper, we have assumed a model where the human obeys Eq. (2) and showed that a naive\napplication of RLHF can lead to suboptimal policies, and the specific failure modes of deceptive inflation and overjustification.\nWhat if the human makes the choices in a different way? Specifically, assume that all we know is that P R(⃗o ≻ ⃗o′)+P R(⃗o′ ≻\n⃗o) = 1. Can the human generally choose these choice probabilities in such a way that RLHF is incentivized to infer a\nreward function whose optimal policies are also optimal for R? The answer is no.\nTake the following example:\ns\na\nb\nc\nIn this example, there is a fixed start state s and three actions a, b, c that also serve as the final states. The time horizon\nis T = 1, so the only state sequences are sa, sb, sc. Assume T (a | s, a) = 1, T (b | s, b) = 1, T (c | s, c) = 1 − ϵ,\nT (a | s, c) = ϵ, i.e., selecting action c sometimes leads to state a. Also, assume a = O(a) ̸= O(b) = O(c) =: o and\nR(a) = R(b) < R(c).\nSince b and c have the same observation o, the human choice probabilities do not make a difference between them, and so\nRLHF is incentivized to infer a reward function ˜R with ˜R(b) = ˜R(c) =: ˜R(o). If ˜R(o) > ˜R(a), then the policy optimal\nunder ˜R will produce action b since this deterministically leads to observation o, whereas c does not. If ˜R(o) < ˜R(a), then\nthe policy optimal under ˜R will produce action a. In both cases, the resulting policy is suboptimal compared to π∗, which\ndeterministically chooses action c.\nIn the coming examples, it will also be useful to look at the misleadingness of state sequences:\nDefinition C.5 (Misleadingness). Let ⃗s ∈ ⃗S be a state sequence. Then its misleadingness is defined by\nM(⃗s) := Gobs(⃗s) − G(⃗s) =\nE\n⃗s ′∼B(⃗s ′| ⃗O(⃗s))\n\u0002\nG(⃗s′) − G(s)\n\u0003\n.\nWe call a state sequence positively misleading if M(⃗s) > 0, which means the sequence appears better than it is, and\nnegatively misleading if M(⃗s) < 0. The misleadingness vector is given by M ∈ R ⃗S.\nNote that the misleadingness is related to E+ and E−, as defined in Definition 4.2: If M(⃗s) > 0 then M(⃗s) = E+(⃗s), and\nif M(⃗s) < 0 then M(⃗s) = −E−(⃗s).\nExample C.6. In this example, we assume the human is a Bayesian reasoner as in Section B.1. Consider the MDP that is\nsuggestively depicted as follows:\na\nb\nc\nThe MDP has states S = {a, b, c} and actions A = {b, c}. The transition kernel is given by T (c | a, c) = 1 and\nT (b | a, b) = 1, meaning that the action determines whether to transition from a to b or c. All other transitions are\n43\nChallenges with Partial Observability of Human Evaluators in Reward Learning\ndeterministic and do not depend on the action, as depicted. We assume an initial state distribution P0 over states with\nprobabilities pa = P0(a), pb = P0(b), pc = P0(c). The true reward function R ∈ R{a,b,c} and discount factor γ ∈ [0, 1)\nare, for now, kept arbitrary. The time horizon is T = 2, meaning we have four possible state sequences acc, abc, bcc, ccc.\nFurthermore, assume that o := O(a) = O(b) ̸= O(c) = c, i.e., c is observed and a and b are ambiguous.\nFinally, assume that the human has a policy prior B(λ), where λ = πλ(c | a) is the likelihood that the policy chooses action\nc when in state a, which is a parameter that determines the entire policy.\nWe claim the following:\n1. If pb ̸= γ · Eλ∼B(λ)[λ] · pa, then ker B ∩ im Γ = {0}, so there is no return function ambiguity under appropriately\nmodeled partially observable RLHF, see Corollary B.4.\n2. There are true reward functions R for which optimizing Jobs leads to a suboptimal policy according to the true policy\nevaluation function J, a case of misalignment. Thus, a naive application of RLHF under partial observability fails, see\nSection 4.1.\n3. The failure modes are related to hiding negative information (deception) and purposefully revealing information while\nincuring a loss (overjustifying behavior).\nProof. Write p := B(bcc | occ), the human’s posterior probability of state sequence bcc for observation sequence occ. We\nhave 1 − p = B(acc | occ).\nConsider the linear operators Γ : R{a,b,c} → R{abc,bcc,ccc,acc} and B : R{abc,bcc,ccc,acc} → R{ooc,occ,ccc} defined in the\nmain paper. When ordering the states, state sequences, and observation sequences as we just wrote down, we obtain\nΓ =\n\n\n\n\n1\nγ\nγ2\n0\n1\nγ + γ2\n0\n0\n1 + γ + γ2\n1\n0\nγ + γ2\n\n\n\n ,\nB =\n\n\n1\n0\n0\n0\n0\np\n0\n1 − p\n0\n0\n1\n0\n\n ,\nB ◦ Γ =\n\n\n1\nγ\nγ2\n1 − p\np\nγ + γ2\n0\n0\n1 + γ + γ2\n\n .\nBy Corollary B.4, if B ◦ Γ is injective, then there is no reward function ambiguity. Clearly, this is the case if and only if\np ̸= γ · (1 − p). From Bayes rule, we have\np =\nB(bcc)\nB(acc) + B(bcc),\n1 − p =\nB(acc)\nB(acc) + B(bcc).\nSo the condition for injectivity holds if and only if\nB(bcc) ̸= γ · B(acc).\nNow, notice\nB(bcc) =\nZ\nλ\nB(λ) · B(bcc | λ)dλ =\nZ\nλ\nB(λ) · pbdλ = pb\nand\nB(acc) =\nZ\nλ\nB(λ)B(acc | λ)dλ =\nZ\nλ\nB(λ) · pa · λdλ = pa ·\nE\nλ∼B(λ)\n\u0002\nλ\n\u0003\n.\nThis shows the first result.\nFor the second statement, we explicitly compute Jobs up to an affine transformation, which does not change the policy\nordering. Let R be the true reward function, G = Γ(R) the corresponding return function, and B(G) the resulting return\nfunction at the level of observations. For simplicity, assume R(c) = 0, which can always be achieved by adding a constant.\nWe have:\nJobs(λ) =\nE\n⃗s∼P λ(⃗s)\nh\nB(G)\n\u0000 ⃗O(⃗s)\n\u0001i\n= P λ(abc) · B(G)(ooc) + P λ(bcc) · B(G)(occ) + P λ(ccc) · B(G)(ccc) + P λ(acc) · B(G)(occ)\n44\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n= pa · (1 − λ) · G(abc) + pb · B(G)(occ) + pc · G(ccc) + pa · λ · B(G)(occ)\n∝ λ ·\nh\nB(G)(occ) − G(abc)\ni\n.\nWe have\nG(abc) = R(a) + γR(b),\nB(G)(occ) = (1 − p) · G(acc) + p · G(bcc) = (1 − p) · R(a) + p · R(b).\nThus, the condition B(G)(occ) > G(abc) is equivalent to\nR(a) < p − γ\np\n· R(b).\nThus, we have\narg max\nλ∈[0,1]\nJobs(λ) =\n(\n1, if R(a) < p−γ\np\n· R(b),\n0, else.\nNow consider the case R(b) > 0. In this case, λ = 0 gives rise to the optimal policy according to G since going to b gives\nextra reward that one misses when going to c directly. However, when R(a) ≪ 0, then Jobs selects for λ = 1. Intuitively,\nthe policy tries to “hide that the episode started in a” by going directly to c, which leads to ambiguity between acc and bcc.\nThis is a case of deceptive inflating as in Theorem 4.5.\nNow, consider the case R(b) < 0. In this case, λ = 1 gives rise to the optimal policy according to G. However, when\nR(a) ≫ 0, then Jobs selects for λ = 0. Intuitively, the policy tries to “reveal that the episode started with a” by going to b,\nwhich is positive information to the human, but negative from the perspective of optimizing G. As in Theorem 4.5, we see\nthat this is a case of overjustification.\nExample C.7. In this example, we consider an MDP that’s similar to a multi-armed bandit with four states/actions a, b, c, d\nand observation kernel O(a) = O(b) ̸= O(c) = O(d). Formally, we can imagine that it is given by the MDP\ns\na\nb\nc\nd\nwith R(s) = 0 and a time-horizon of T = 1. In this example, we reveal that misleadingness and non-optimality (according to\nthe true reward R, or J) are in principle orthogonal concepts. We consider the following four example cases. In each one, we\nvary some environment parameters and then determine a∗\nobs, the action that results from optimizing Jobs (corresponding to a\nnaive application of RLHF under partial observability, see Section 4.1), its misleadingness M(a∗\nobs) (see Definition C.5), and\nthe action a∗ that would result from optimizing J. If a∗\nobs = a∗, then Jobs selects for the optimal action. For simplicity, we\ncan imagine that the human has a uniform prior over what action results eventually (out of the action taken and potentially\na deviation defined by ϵ, see below) is taken before making an observation, i.e. B(a) = B(b) = B(c) = B(d) = 1\n4.\n(a) Assume R(a) > R(c) > R(d) ≫ R(b). Also assume that action d leads with probability ϵ > 0 to state b, whereas all\nother actions lead deterministically to the specified state. Then a∗\nobs = c, M(c) < 0 and a∗ = a.\n(b) Assume R(d) > R(a) > R(c) ≫ R(b). Again, assume there is a small probability ϵ > 0 that action d leads to state b.\nThen a∗\nobs = c, M(c) > 0, and a∗ = d or a∗ = a, depending on the size of ϵ.\n(c) Assume R(a) > R(b) > R(c) > R(d). Additionally, assume that there is a large probability ϵ > 0 that action a leads\nto state d, whereas all other actions lead to what’s specified. If ϵ is large enough, then a∗ = b. Additionally, we have\na∗\nobs = b and M(b) > 0.\n(d) Assume R(a) > R(b) > R(c) > R(d). Also, assume some probability ϵ > 0 that action b leads to state d, whereas all\nother actions lead deterministically to what’s specified. Then a∗\nobs = a, M(a) < 0, and a∗ = a.\nOverall, we notice:\n45\nChallenges with Partial Observability of Human Evaluators in Reward Learning\n• Example (a) shows a high regret and negative misleadingness of a∗\nobs = c. The action is better then it seems, but action\na would be better still but cannot be selected because it can be confused with the very bad action b.\n• Example (b) shows a high regret and high misleadingness of a∗\nobs = c. The action is worse than it seems and also not\noptimal.\n• Example (c) shows zero regret and high misleadingness of a∗\nobs = b. The action is worse than it seems because it can\nbe confused with a, but it is still the optimal action because a can turn into d.\n• Example (d) shows zero regret negative misleadingness of a∗\nobs = a. The action is chosen even though it seems worse\nthan it is, and is also optimal.\nThus, we showed all combinations of regret and misleadingness of the action optimized for under Jobs.\nWe can also notice the following: Examples (a) and (b) only differ in the placement of R(d). In particular, the reason that\na∗\nobs = c is structurally the same in both, but the misleadingness changes. This indicates that misleadingness is not on its\nown contributing to what Jobs optimizes for.\nThe following is the smallest example we found with the following properties:\n• There is a unique start state and terminal state.\n• A naive application of RLHF fails in a way that shows deception and overjustification.\n• Modeling partial observability resolves the problems.\nExample C.8. Consider the following graph:\nA\nS\nC\nT\nB\nThis\ndepicts\nan\nMDP\nwith\nstart\nstate\nS,\nterminal\nstate\nT\nand\npossible\nstate\nsequences\nSTTT, SATT, SACT, SCTT, SBCT, SBTT and no discount, i.e.\nγ = 1.\nAssume that S, B, C are observed,\ni.e. O(S) = S, O(B) = B, O(C) = C, and that A and T are ambiguous: O(A) = O(T) = X. Then there are five\nobservation sequences SXXX, SXCX, SCXX, SBCX, SBXX. Assume that the human can identify all observation\nsequences except SXXX, with belief b = B(STTT | SXXX) and 1 − b = B(SATT | SXXX).\nThen the return function is identifiable under these conditions when the human’s belief is correctly modeled. However, for\nsome choices of the true reward function R and transition dynamics of this MDP, we can obtain deceptive or overjustified\nbehavior for a naive application of RLHF.\nProof. We apply Corollary B.4. We order states, state sequences, and observation sequences as follows:\nS = S, A, B, C, T,\n⃗S = STTT, SATT, SACT, SCTT, SBCT, SBTT,\n⃗Ω = SXXX, SXCX, SCXX, SBCX, SBXX.\n46\nChallenges with Partial Observability of Human Evaluators in Reward Learning\nAs can easily be verified, with this ordering the matrices B ∈ R⃗Ω× ⃗S and Γ ∈ R ⃗S×S are given by:\nB =\n\n\n\n\n\n\nb\n1 − b\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n0\n1\n\n\n\n\n\n\n,\nΓ =\n\n\n\n\n\n\n\n\n1\n0\n0\n0\n3\n1\n1\n0\n0\n2\n1\n1\n0\n1\n1\n1\n0\n0\n1\n2\n1\n0\n1\n1\n1\n1\n0\n1\n0\n2\n\n\n\n\n\n\n\n\n.\nTo show identifiability, we need to show that ker B ∩ im Γ = {0}. Clearly, the kernel of B is given by all return functions\nin R ⃗S that are multiples of G′ = (b − 1, b, 0, 0, 0, 0). Assume G′ ∈ im Γ, meaning there is a reward function R′ ∈ R ⃗S with\nΓ ·R′ = G′. We need to deduce from this a contradiction. The assumption means we obtain the following equations:\n(i) R′(S) + 3R′(T) = b − 1,\n(ii) R′(S) + R′(A) + 2R′(T) = b,\n(iii) R′(S) + R′(A) + R′(C) + R′(T) = 0,\n(iv) R′(S) + R′(C) + 2R′(T) = 0,\n(v) R′(S) + R′(B) + R′(C) + R′(T) = 0\n(vi) R′(S) + R′(B) + 2R′(T) = 0\n(iii) and (v) together imply R′(A) = R′(B); (iv) and (vi) together imply R′(B) = R′(C); (v) and (vi) together imply\nR′(C) = R′(T); so together, we have R′(A) = R′(T). Thus, replacing R′(A) in (ii) by R′(T) and comparing (i) and (ii),\nwe obtain b − 1 = b, a contradiction. Overall, this shows ker B ∩ im Γ = {0}, and thus identifiability of the return function\nby Corollary B.4.\nNow we investigate the case of unmodeled partial observability.\nFor demonstrating overjustification, assume deterministic transition dynamics in which every arrow in the diagram can\nbe chosen by the policy. Also, assume R(A) ≪ 0, R(T) > 0, R(S) = 0, R(B) = 0, and R(C) = 0. Then the optimal\npolicy chooses the state sequence STTT. However, this trajectory has low observation value since Gobs(STTT) =\n(B ·G)(SXXX) = bG(STTT) + (1 − b)G(SATT), which is low since R(A) ≪ 0. Jobs then selects for the suboptimal\npolicies choosing SBTT or SCTT, which is overjustified behavior that makes sure that the human does not think state A\nwas accessed.\nFor demonstrating deception, assume that R(A) ≫ 0, R(T) < 0, R(S) = R(B) = R(C) = 0 and that the transition\ndynamics are such that when the policy attempts to transition from S to A, it will sometimes transition to B, with all other\ntransitions deterministic. In this case, the optimal behavior attempts to enter state A since this has very high value. Jobs,\nhowever, will select for the policy that chooses STTT. This is deceptive behavior.\n47\n"
}