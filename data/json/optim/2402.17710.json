{
    "optim": "Understanding Neural Network Binarization with\nForward and Backward Proximal Quantizers\nYiwei Lu∗\nSchool of Computer Science\nUniversity of Waterloo\nVector Institute\nyiwei.lu@uwaterloo.ca\nYaoliang Yu\nSchool of Computer Science\nUniversity of Waterloo\nVector Institute\nyaoliang.yu@uwaterloo.ca\nXinlin Li\nHuawei Noah’s Ark Lab\nxinlin.li1@huawei.com\nVahid Partovi Nia\nHuawei Noah’s Ark Lab\nvahid.partovinia@huawei.com\nAbstract\nIn neural network binarization, BinaryConnect (BC) and its variants are consid-\nered the standard. These methods apply the sign function in their forward pass\nand their respective gradients are backpropagated to update the weights. How-\never, the derivative of the sign function is zero whenever defined, which con-\nsequently freezes training. Therefore, implementations of BC (e.g., BNN) usu-\nally replace the derivative of sign in the backward computation with identity or\nother approximate gradient alternatives. Although such practice works well em-\npirically, it is largely a heuristic or “training trick.” We aim at shedding some\nlight on these training tricks from the optimization perspective. Building from\nexisting theory on ProxConnect (PC, a generalization of BC), we (1) equip PC\nwith different forward-backward quantizers and obtain ProxConnect++ (PC++)\nthat includes existing binarization techniques as special cases; (2) derive a prin-\ncipled way to synthesize forward-backward quantizers with automatic theoretical\nguarantees; (3) illustrate our theory by proposing an enhanced binarization algo-\nrithm BNN++; (4) conduct image classification experiments on CNNs and vision\ntransformers, and empirically verify that BNN++ generally achieves competitive\nresults on binarizing these models.\n1\nIntroduction\nThe recent success of numerous applications in machine learning is largely fueled by training big\nmodels with billions of parameters, e.g., GPTs in large language models [7, 8], on extremely large\ndatasets. However, as such models continue to scale up, end-to-end training or even fine-tuning\nbecomes prohibitively expensive, due to the heavy amount of computation, memory and storage\nrequired. Moreover, even after successful training, deploying these models on resource-limited\ndevices or environments that require real-time inference still poses significant challenges.\nA common way to tackle the above problems is through model compression, such as pruning [44, 47,\n51], reusing attention [6], weight sharing [57], structured factorization [49], and network quantiza-\ntion [16, 30, 32, 38]. Among them, network quantization (i.e., replacing full-precision weights with\nlower-precision ones) is a popular approach. In this work we focus on an extreme case of network\nquantization: binarization, i.e., constraining a subset of the weights to be only binary (i.e., ±1), with\n∗Work done during an internship at Huawei Noah’s Ark Lab.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2402.17710v1  [cs.LG]  27 Feb 2024\nthe benefit of much reduced memory and storage cost, as well as inference time through simpler\nand faster matrix-vector multiplications, which is one of the main computationally expensive steps\nin transformers and the recently advanced vision transformers [14, 34, 52].\nFor neural network binarization, BinaryConnect [BC, 11] is considered the de facto standard. BC\napplies the sign function to binarize the weights in the forward pass, and evaluates the gradient at\nthe binarized weights using the Straight Through Estimator [STE, 4]2. This widely adopted training\ntrick has been formally justified from an optimization perspective: Dockhorn et al. [13], among\nothers, identify BC as a nonconvex counterpart of dual averaging, which itself is a special case of\nthe generalized conditional gradient algorithm. Dockhorn et al. [13] further propose ProxConnect\n(PC) as an extension of BC, by allowing arbitrary proximal quantizers (with sign being a special\ncase) in the forward pass.\nHowever, practical implementations [e.g., 2, 12, 22] usually apply an approximate gradient of the\nsign function on top of STE. For example, Hubara et al. [22] employ the hard tanh function as\nan approximator of sign. Thus, in the backward pass, the derivative of sign is approximated by\nthe indicator function 1[−1,1], the derivative of hard tanh. Later, Darabi et al. [12] consider the\nsign-Swish function as a more accurate and flexible approximation in the backward pass (but still\nemploys the sign in the forward pass).\nDespite their excellent performance in practice, approximate gradient approaches cannot be readily\nunderstood in the PC framework of Dockhorn et al. [13], which does not equip any quantization\nin the backward pass. Thus, the main goal of this work is to further generalize PC and improve\nour understanding of approximate gradient approaches. Specifically, we introduce PC++ that comes\nwith a pair of forward-backward proximal quantizers, and we show that most of the existing approx-\nimate gradient approaches are special cases of our proximal quantizers, and hence offering a formal\njustification of their empirical success from an optimization perspective. Moreover, inspired by our\ntheoretical findings, we propose a novel binarization algorithm BNN++ that improves BNN+ [12]\non both theoretical convergence properties and empirical performances. Notably, our work provides\ndirect guidance on designing new forward-backward proximal quantizers in the PC++ family, with\nimmediate theoretical guarantees while enabling streamlined implementation and comparison of a\nwide family of existing quantization algorithms.\nEmpirically, we benchmark existing PC++ algorithms (including the new BNN++) on image clas-\nsification tasks on CNNs and vision transformers. Specifically, we perform weight (and activation)\nbinarization on various datasets and models. Moreover, we explore the fully binarized scenario,\nwhere the dot-product accumulators are also quantized to 8-bit integers. In general, we observe that\nBNN++ is very competitive against existing approaches on most tasks, and achieves 30x reduction\nin memory and storage with a modest 5-10% accuracy drop compared to full precision training.\nWe summarize our main contributions in more detail:\n• We generalize ProxConnect with forward-backward quantizers and introduce ProxConnect++\n(PC++) that includes existing binarization techniques as special cases.\n• We derive a principled way to synthesize forward-backward quantizers with theoretical guaran-\ntees. Moreover, we design a new BNN++ variant to illustrate our theoretical findings.\n• We empirically compare different choices of forward-backward quantizers on image classification\nbenchmarks, and confirm that BNN++ is competitive against existing alternatives.\n2\nBackground\nIn neural network quantization, we aim at minimizing the usual (nonconvex) objective function ℓ(w)\nwith discrete weights w:\nmin\nw∈Q ℓ(w),\n(1)\nwhere Q ⊆ Rd is a discrete, nonconvex quantization set such as Q = {±1}d. The acquired discrete\nweights w ∈ Q are compared directly with continuous full precision weights, which we denote\n2Note that we refer to STE as its original definition by Bengio et al. [4] for binarizing weights, and other\nvariants of STE (e.g., in BNN) as approximate gradient.\n2\nas w∗ for clarity. While our work easily extends to most discrete set Q, we focus on Q = {±1}d\nsince this binary setting remains most challenging and leads to the most significant savings. Existing\nbinarization schemes can be largely divided into the following two categories.\nPost-Training Binarization (PTB):\nwe can formulate post-training binarization schemes as the\nfollowing standard forward and backward pass:\nwt = PQ(w∗\nt ),\nw∗\nt+1 = w∗\nt − ηt e∇ℓ(w∗\nt ),\nwhere PQ is the projector that binarizes the continuous weights w∗ deterministically (e.g., the sign\nfunction) or stochastically3, and e∇ℓ(w∗\nt ) denotes a sample (sub)gradient of ℓ at w∗\nt . We point out\nthat PTB is merely a post-processing step, i.e., the binarized weights wt do not affect the update\nof the continuous weights w∗\nt , which are obtained through normal training. As a result, there is no\nguarantee that the acquired discrete weights wt is a good solution (either global or local) to eq. (1).\nBinarization-Aware Training (BAT):\nwe then recall the more difficult binarization-aware train-\ning scheme BinaryConnect (BC), first initialized by Courbariaux et al. [11]:\nwt = PQ(w∗\nt ),\nw∗\nt+1 = w∗\nt − ηt e∇ℓ(wt),\n(2)\nwhere we spot that the gradient is evaluated at the binarized weights wt but used to update the\ncontinuous weights w∗\nt . This approach is also known as Straight Through Estimator [STE, 4]. Note\nthat it is also possible to update the binarized weights instead, effectively performing the proximal\ngradient algorithm to solve (1), as shown by Bai et al. [2]:\nwt = PQ(w∗\nt ),\nw∗\nt+1 = wt − ηt e∇ℓ(wt).\nThis method is known as ProxQuant, and will serve as a baseline in our experiments.\n2.1\nProxConnect\nDockhorn et al. [13] proposed ProxConnect (PC) as a broad generalization of BinaryConnect in (2):\nwt = Pµt\nr (w∗\nt ),\nw∗\nt+1 = w∗\nt − ηt e∇ℓ(wt),\n(3)\nwhere µt := 1 + Pt−1\nτ=1 ητ, ηt > 0 is the step size, and Pµt\nr\nis the proximal quantizer:\nPµ\nr (w) := argmin\nz\n1\n2µ∥w − z∥2\n2 + r(z), and\nMµ\nr (w) := min\nz\n1\n2µ∥w − z∥2\n2 + r(z).\nIn particular, when the regularizer r = ιQ (the indicator function of Q), Pµt\nr\n= PQ (for any µt) and\nwe recover BC in (2). Dockhorn et al. [13] showed that the PC update (3) amounts to applying the\ngeneralized conditional gradient algorithm to a smoothened dual of the regularized problem:\nmin\nw\n[ℓ(w) + r(w)] ≈ min\nw∗ ℓ∗(−w∗) + Mµ\nr∗(w∗),\nwhere f ∗(w∗) := maxw⟨w, w∗⟩ − f(w) is the Fenchel conjugate of f. The theory behind PC thus\nformally justifies STE from an optimization perspective. We provide a number of examples of the\nproximal quantizer Pµt\nr\nin Appendix A.\nAnother natural cousin of PC is the reversed PC (rPC):\nwt = Pµt\nr (w∗\nt ),\nw∗\nt+1 = wt − ηt e∇ℓ(w∗\nt ),\nwhich is able to exploit the rich landscape of the loss by evaluating the gradient at the continuous\nweights w∗\nt . Thus, we also include it as a baseline in our experiments.\nWe further discuss other related works in Appendix B.\n3We only consider deterministic binarization in this paper.\n3\n3\nMethodology\nOne popular heuristic to explain BC is through the following reformulation of problem (1):\nmin\nw∗ ℓ\n\u0000PQ(w∗)\n\u0001\n.\nApplying (stochastic) “gradient” to update the continuous weights we obtain:\nw∗\nt+1 = w∗\nt − ηt · P′\nQ(w∗\nt ) · e∇ℓ(PQ(w∗\nt )).\nUnfortunately, the derivative of the projector PQ is 0 everywhere except at the origin, where the\nderivative actually does not exist. BC [11], see (2), simply “pretended” that P′\nQ = I. Later works\npropose to replace the troublesome P′\nQ by the derivative of functions that approximate PQ, e.g.,\nthe hard tanh in BNN [22] and the sign-Swish in BNN+ [12]. Despite their empirical success,\nit is not clear what is the underlying optimization problem or if it is possible to also replace the\nprojector inside e∇ℓ, i.e., allowing the algorithm to evaluate gradients at continuous weights, a clear\nadvantage demonstrated by Bai et al. [2] and Dockhorn et al. [13]. Moreover, the theory established\nin PC, through a connection to the generalized conditional gradient algorithm, does not apply to\nthese modifications yet, which is a gap that we aim to fill in this section.\n3.1\nProxConnect++\nTo address the above-mentioned issues, we propose to study the following regularized problem:\nmin\nw∗ ℓ(T(w∗)) + r(w∗),\n(4)\nas a relaxation of the (equivalent) reformulation of (1):\nmin\nw∗ ℓ(PQ(w∗)) + ιQ(w∗).\nIn other words, T : Rd → Rd is some transformation that approximates PQ and the regularizer\nr : Rd → R approximates the indicator function ιQ. Directly applying ProxConnect in (3) we\nobtain4:\nwt = Pµt\nr (w∗\nt ), w∗\nt+1 = w∗\nt − ηtT′(wt) · e∇ℓ\n\u0000T(wt)\n\u0001\n.\n(5)\nIntroducing the forward and backward proximal quantizers:\nFµ\nr := T ◦ Pµ\nr ,\nBµ\nr := T′ ◦ Pµ\nr ,\n(6)\nwe can rewrite the update in (5) simply as:\nw∗\nt+1 = w∗\nt − ηt · Bµt\nr (w∗\nt ) · e∇ℓ\n\u0000Fµt\nr (w∗\nt )\n\u0001\n.\n(7)\nIt is clear that the original ProxConnect corresponds to the special choice\nFµ\nr = Pµ\nr ,\nBµ\nr ≡ I.\nOf course, one may now follow the recipe in (6) to design new forward-backward quantizers. We call\nthis general formulation in (7) ProxConnect++ (PC++), which covers a broad family of algorithms.\nConversely, the complete characterization of proximal quantizers in Dockhorn et al. [13] allows us\nalso to reverse engineer T and r from manually designed forward and backward quantizers. As we\nwill see, most existing forward-backward quantizers turn out to be special cases of our proximal\nquantizers, and thus their empirical success can be justified from an optimization perspective. In-\ndeed, for simplicity, let us restrict all quantizers to univariate ones that apply component-wise. Then,\nthe following result is proven in Appendix C.\nCorollary 1. A pair of forward-backward quantizers (F, B) admits the decomposition in (6) (for\nsome smoothing parameter µ and regularizer r) iff both F and B are functions of P(w) :=\nR w\n−∞\n1\nB(ω) dF(ω), which is proximal (i.e., monotone, compact-valued and with a closed graph).\n4We assume throughout that T, and any function whose derivative we use, are locally Lipschitz so that their\ngeneralized derivative is always defined, see Rockafellar and Wets [50].\n4\nan−1\ninput to layer n\nw∗\nn\nfp-weights\nFµ\nr\nForward Quantizer\nwn\nbn-weights\nan\noutput of layer n\n∂an\n∂wn\nBµ\nr\nBackward Quantizer\nevaluate gradient\n×\nback-prop\nupdate\n−1\n1\n−1\n1\nFP (forward)\n−1\n1\n−1\n1\nFP (backward)\n−1\n1\n−1\n1\nPC (forward)\n−1\n1\n−1\n1\nPC (backward)\n−1\n1\n−1\n1\nBNN (forward)\n−1\n1\n−1\n1\nBNN (backward)\n−1\n1\n−1\n1\nBNN+ (forward)\n−1\n1\n−1\n1\nBNN+ (backward)\n−1\n1\n−1\n1\nBNN++ (forward)\n−1\n1\n−1\n1\nBNN++ (backward)\nFigure 1: Forward and backward pass for ProxConnect++ algorithms (red/blue arrows indicate the\nforward/backward pass), where fp denotes full precision, bn denotes binary and back-prop denotes\nbackpropagation.\nImportantly, with forward-backward proximal quantizers, the convergence results established by\nDockhorn et al. [13] for PC directly carries over to PC++ (see Appendix C for details). Let us\nfurther illustrate the convenience of Corollary 1 by some examples.\nExample 1 (BNN). Hubara et al. [22] proposed BNN with the choice\nF = sign\nand\nB = 1[−1,1],\nwhich satisfies the decomposition in (6). Indeed, let\nT(w) = min{1, max{−1, w}},\n(8)\nPµ\nr (w) =\n(\n1\nµw + sign(w)(1 − 1\nµ),\nif |w| > 1\nsign(w),\nif |w| ≤ 1 .\n(9)\nSince B is constant over [−1, 1], applying Corollary 1 we deduce that the proximal quantizer Pµ\nr , if\nexists, must coincide with F over the support of B. Applying monotonicity of Pµ\nr we may complete\nthe reverse engineering by making the choice over |w| > 1 as indicated above. We can easily verify\nthe decomposition in (6):\nF = sign = T ◦ Pµ\nr , B = 1[−1,1] = T′ ◦ Pµ\nr .\nThus, BNN is exactly BinaryConnect applied to the transformed problem in (4), where the transfor-\nmation T is the so-called hard tanh in (8) while the regularizer r is determined (implicitly) by the\nproximal quantizer Pµ\nr in (9).\nTo our best knowledge, this is the first time the (regularized) objective function that BNN aims to\noptimize has been identified. The convergence properties of BNN hence follow from the general\nresult of Dockhorn et al. [13] on ProxConnect, see Appendix C.\n5\nTable 1: Variants of ProxConnect++.\nForward Quantizer\nBackward Quantizer\nAlgorithm\nidentity\nidentity\nFP\nPQ\nidentity\nBC\nLϱ\nρ\nidentity\nPC\nPQ\n1[−1,1]\nBNN\nPQ\n∇SS\nBNN+\nSS\n∇SS\nBNN++\nExample 2 (BNN+). Darabi et al. [12] adopted the derivative of the sign-Swish (SS) function as a\nbackward quantizer while retaining the sign function as the forward quantizer:\nB(w) = ∇SS(w) := µ[1 − µw\n2 tanh( µw\n2 )] tanh′( µw\n2 ), F = sign,\nwhere µ is a hyperparameter that controls how well SS approximates the sign. Applying Corollary 1\nwe find that the derivative of SS (as backward) coupled with the sign (as forward) do not admit the\ndecomposition in (6), for any regularizer r. Thus, we are not able to find the (regularized) objective\nfunction (if it exists) underlying BNN+.\nWe conclude that BNN+ cannot be justified under the framework of PC++. However, it is possible to\ndesign a variant of BNN+ that does belong to the PC++ family and hence enjoys the accompanying\ntheoretical properties:\nExample 3 (BNN++). We propose that a simple fix of BNN+ would be to replace its sign forward\nquantizer with the sign-Swish (SS) function:\nF(w) = SS(w) := µw\n2 tanh′( µw\n2 ) + tanh( µw\n2 ),\nwhich is simply the primitive of B. In this case, the algorithm simply reduces to PC++ applied on (4)\nwith r = 0 (and hence essentially stochastic gradient descent). Of course, we could also compose\nwith a proximal quantizer to arrive at the pair (F ◦ Pµ\nr , B ◦ Pµ\nr ), which effectively reduces to PC++\napplied on the regularized objective in (4) with a nontrivial r. We call this variant BNN++.\nWe will demonstrate in the next section that BNN++ is more desirable than BNN+ empirically.\nMore generally, we have the following result on designing new forward-backward quantizers:\nCorollary 2. If the forward quantizer is continuously differentiable (with bounded support), then\none can simply choose the backward quantizer as the derivative of the forward quantizer.\nThis follows from Corollary 1 since P(w) ≡ w is clearly proximal. Note that the BNN example does\nnot follow from Corollary 2. In Appendix F, we provide additional examples of forward-backward\nquantizers based on existing methods, and we show that Corollary 2 consistently improves previous\npractices.\nIn summary: (1) ProxConnect++ enables us to design forward-backward quantizers with infinite\nmany choices of T and r, (2) it also allows us to reverse engineer T and r from existing forward-\nbackward quantizers, which helps us to better understand existing practices, (3) with our theoretical\ntool, we design a new BNN++ algorithm, which enjoys immediate convergence properties. Figure 1\nvisualizes ProxConnect++ with a variety of forward-backward quantizers.\n4\nExperiments\nIn this section, we perform extensive experiments to benchmark PC++ on CNN backbone models\nand the recently advanced vision transformer architectures in three settings: (a) binarizing weights\nonly (BW); (b) binarizing weights and activations (BWA), where we simply apply a similar forward-\nbackward proximal quantizer to the activations; and (c) binarizing weights, activations, with 8-bit\ndot-product accumulators (BWAA) [43].\n6\nTable 2: Binarizing weights (BW), binarizing weights and activation (BWA) and binarizing weights,\nactivation, with 8-bit accumulators (BWAA) on CNN backbones. We consider the fine-tuning (FT)\npipeline and the end-to-end (E2E) pipeline. We compare five variants of ProxConnect++ (BC, PC,\nBNN, BNN+, and BNN++) with FP, PQ, and rPC in terms of test accuracy. For the end-to-end\npipeline, we omit the results for BWAA due to training divergence and report the mean of five runs\nwith different random seeds.\nDataset\nPipeline\nTask\nFP\nPQ\nrPC\nProxConnect++\nBC\nPC\nBNN\nBNN+\nBNN++\nCIFAR-10\nFT\nBW\n92.01%\n89.94%\n89.98%\n90.31%\n90.31%\n90.35%\n90.27%\n90.40%\nBWA\n92.01%\n88.79%\n83.55%\n89.39%\n89.95%\n90.01%\n89.99%\n90.22%\nBWAA\n92.01%\n85.39%\n81.10%\n89.11%\n89.21%\n89.32%\n89.55%\n90.01%\nE2E\nBW\n92.01%\n81.59%\n81.82%\n87.51%\n88.05%\n89.92%\n89.39%\n90.03%\nBWA\n92.01%\n81.51%\n81.60%\n86.99%\n87.26%\n89.15%\n89.02%\n89.91%\nImageNet-1K\nFT\nBW\n78.87%\n66.77%\n69.22%\n71.35%\n71.29%\n71.41%\n70.22%\n72.33%\nBWA\n78.87%\n56.21%\n58.19%\n65.99%\n65.61%\n66.02%\n65.22%\n68.03%\nBWAA\n78.87%\n53.29%\n55.28%\n58.18%\n59.21%\n59.77%\n59.10%\n63.02%\nE2E\nBW\n78.87%\n63.23%\n66.39%\n67.45%\n67.51%\n67.49%\n66.99%\n68.11%\nBWA\n78.87%\n61.19%\n64.17%\n65.42%\n65.31%\n65.29%\n65.98%\n66.08%\n4.1\nExperimental settings\nDatasets: We perform image classification on CIFAR-10/100 datasets [27] and ImageNet-1K\ndataset [28]. Additional details on our experimental setting can be found in Appendix D.\nBackbone architectures: (1) CNNs: we evaluate CIFAR-10 classification using ResNet20 [18], and\nImageNet-1K with ResNet-50 [18]. We consider both fine-tuning and end-to-end training; (2) Vision\ntransformers: we further evaluate our algorithm on two popular vision transformer models: ViT [14]\nand DeiT [52]. For ViT, we consider ViT-B model and fine-tuning task across all models5. For DeiT,\nwe consider DeiT-B, DeiT-S, and DeiT-T, which consist of 12, 6, 3 building blocks and 768, 384 and\n192 embedding dimensions, respectively; we consider fine-tuning task on ImageNet-1K pre-trained\nmodel for CIFAR datasets and end-to-end training on ImageNet-1K dataset.\nBaselines: For ProxConnect++, we consider the 6 variants in Table 1. With different choices of the\nforward quantizer Fµ\nr and the backward quantizer Bµ\nr , we include the full precision (FP) baseline and\n5 binarization methods: BinaryConnect (BC) [11], ProxConnect (PC) [13], Binary Neural Network\n(BNN) [22], the original BNN+ [12], and the modified BNN++ with Fµ\nr = SS. Note that we linearly\nincrease µ in BNN++ to achieve full binarization in the end. We also compare ProxConnect++ with\nthe ProxQuant and reverseProxConnect baselines.\nHyperparameters: We apply the same training hyperparameters and fine-tune/end-to-end training\nfor 100/300 epochs across all models. For binarization methods: (1) PQ (ProxQuant): similar to\nBai et al. [2], we apply the LinearQuantizer (LQ), see (10) in Appendix A, with initial ρ0 = 0.01\nand linearly increase to ρT = 10; (2) rPC (reverseProxConnect): we use the same LQ for rPC; (3)\nProxConnect++: for PC, we apply the same LQ; for BNN+, we choose µ = 5 (no need to increase\nµ as the forward quantizer is sign); for BNN++, we choose µ0 = 5 and linearly increase to µT = 30\nto achieve binarization at the final step.\nAcross all the experiments with random initialization, we report the mean of three runs with different\nrandom seeds. Furthermore, we provide the complete results with error bars in Appendix G.\n4.2\nCNN as backbone\nWe first compare PC++ against baseline methods on various tasks employing CNNs:\n(1) Binarizing weights only (BW), where we simply binarize the weights and keep the other com-\nponents (i.e., activations and accumulations) in full precision.\n5Note that we use pre-trained models provided by Dosovitskiy et al. [14] on the ImageNet-21K/ImageNet-\n1K for fine-tuning ViT-B model on the ImageNet-1K/CIFAR datasets, respectively.\n7\nTable 3: Our results on binarizing vision transformers (binarizing weights only). We compare five\nvariants of ProxConnect++ (BC, PC, BNN, BNN+, and BNN++) with FP, PQ, and rPC. End-to-end\ntraining tasks are marked as bold (i.e., ImageNet-1K for DeiT-T/S/B), where the results are the mean\nof five runs with different random seeds.\nModel\nDataset\nFP\nPQ\nrPC\nProxConnect++\nBC\nPC\nBNN\nBNN+\nBNN++\nViT-B\nCIFAR-10\n98.13%\n85.06%\n86.22%\n87.97%\n90.12%\n89.08%\n88.12%\n90.24%\nCIFAR-100\n87.14%\n72.07%\n73.52%\n76.35%\n78.13%\n77.23%\n77.10%\n79.22%\nImageNet-1K\n77.91%\n57.65%\n55.33%\n63.24%\n66.33%\n65.31%\n63.55%\n66.33%\nDeiT-T\nCIFAR-10\n94.86%\n82.76%\n82.25%\n83.10%\n85.15%\n86.12%\n85.91%\n86.41%\nCIFAR-100\n72.37%\n54.55%\n55.66%\n59.65%\n60.15%\n60.06%\n59.77%\n60.33%\nImageNet-1K\n72.20%\n61.23%\n60.35%\n63.22%\n66.15%\n65.00%\n66.67%\n67.34%\nDeiT-S\nCIFAR-10\n95.10%\n81.67%\n80.23%\n84.85%\n85.13%\n85.09%\n85.16%\n86.19%\nCIFAR-100\n73.19%\n45.55%\n46.66%\n60.12%\n61.59%\n60.55%\n60.17%\n62.98%\nImageNet-1K\n79.91%\n69.87%\n68.74%\n73.16%\n73.51%\n73.77%\n73.23%\n73.53%\nDeiT-B\nCIFAR-10\n98.72%\n85.22%\n86.35%\n88.95%\n90.53%\n90.21%\n89.03%\n90.67%\nCIFAR-100\n86.65%\n72.11%\n73.40%\n75.40%\n78.55%\n76.22%\n76.51%\n78.30%\nImageNet-1K\n81.81%\n72.54%\n70.11%\n76.55%\n76.61%\n75.60%\n76.63%\n76.74%\nTable 4: Results on binarizing vision transformers (BW, BWA, and BWAA) on DeiT-T. We compare\n5 variants of ProxConnect++ (BC, PC, BNN, BNN+, and BNN++) with FP, PQ, and rPC. End-to-\nend training tasks are marked as bold (i.e., ImageNet-1K), where we omit the results for BWAA due\nto training divergence and the reported results are the mean of five runs with different random seeds.\nDataset\nTask\nFP\nPQ\nrPC\nProxConnect++\nBC\nPC\nBNN\nBNN+\nBNN++\nCIFAR-10\nBW\n94.85%\n82.76%\n82.25%\n83.10%\n85.15%\n86.12%\n85.91%\n86.41%\nBWA\n94.85%\n82.56%\n82.02%\n82.89%\n85.01%\n85.99%\n85.66%\n86.12%\nBWAA\n94.85%\n81.34%\n80.97%\n82.08%\n84.31%\n84.87%\n84.72%\n85.31%\nCIFAR-100\nBW\n72.37%\n54.55%\n55.66%\n59.65%\n60.15%\n60.06%\n59.77%\n60.33%\nBWA\n72.37%\n53.77%\n54.98%\n59.21%\n59.71%\n59.66%\n59.12%\n59.85%\nBWAA\n72.37%\n52.15%\n54.36%\n58.15%\n59.01%\n58.72%\n58.15%\n59.06%\nImageNet-1K\nBW\n72.20%\n61.23%\n60.35%\n63.23%\n66.15%\n65.00%\n66.67%\n67.34%\nBWA\n72.20%\n60.01%\n58.77%\n62.13%\n65.29%\n63.75%\n65.29%\n65.65%\n(2) Binarizing weights and activations (BWA), while keeping accumulation in full precision. Sim-\nilar to the weights, we apply the same forward-backward proximal quantizer to binarize activa-\ntions.\n(3) Binarizing weights, activations, with 8-bit accumulators (BWAA). BWAA is more desirable in\ncertain cases where the network bandwidth is narrow, e.g., in homomorphic encryption. To\nachieve BWAA, in addition to quantizing the weights and activations, we follow the implemen-\ntation of WrapNet [43] and quantize the accumulation of each layer with an additional cyclic\nfunction. In practice, we find that with 1-bit weights and activations, the lowest bits we can\nsuccessfully employ to quantize accumulation is 8, while any smaller choice would raise a high\noverflow rate and cause the network to diverge. Moreover, BWAA highly relies on a good ini-\ntialization and cannot be successfully trained end-to-end in our evaluation (and hence omitted).\nNote that for the fine-tuning pipeline, we initialize the model with their corresponding pre-trained\nfull precision weights. For the end-to-end pipeline, we utilize random initialization. We report our\nresults in Table 2 and observe: (1) the PC family outperforms baseline methods (i.e., PQ and rPC),\nand achieves competitive performance on both small and larger scale datasets; (2) BNN++ performs\nconsistently better and is more desirable among the five variants of PC++, especially on BWA and\nBWAA tasks. Its advantage over BNN+ further validates our theoretical guidance.\n4.3\nVision transformer as backbone\nNext, we perform similar experiments on the three tasks on vision transformers.\n8\nTable 5: Ablation study on the effect of the scaling factor, normalization, pre-training, and knowl-\nedge distillation. Experiments are performed on CIFAR-10 with ViT-B.\nMethod\nScaling\nNormalization\nPre-train\nKD\nAccuracy\nPC\n✗\n✗\n✗\n✗\n0.10%\n✓\n✗\n✗\n✗\n12.81%\n✓\n✓\n✗\n✗\n66.51%\n✓\n✓\n✓\n✗\n88.53%\n✓\n✓\n✓\n✓\n90.13%\nBNN++\n✗\n✗\n✗\n✗\n1.50%\n✓\n✗\n✗\n✗\n23.55%\n✓\n✓\n✗\n✗\n77.22%\n✓\n✓\n✓\n✗\n89.05%\n✓\n✓\n✓\n✓\n90.22%\nDeiT-T DeiT-S DeiT-B ViT-B\n50\n100\nFP\nBNN++\nPTB\nFigure 2: Comparison between Full\nPrecision (FP) model, BNN++, and\nPost-training Binarization (PTB) on\nthe fine-tuning task on CIFAR-10.\nImplementation on vision transformers: While network\nbinarization is popular for CNNs, its application for vision\ntransformers is still rare6. Here we apply four protocols for\nimplementation:\n(1) We keep the mean sn of full precision weights w∗\nn for\neach layer n as a scaling factor (can be thus absorbed into\nFµt\nr ) for the binary weights wn. Such an approach keeps the\nrange of w∗\nn during binarization and significantly reduces\ntraining difficulty without additional computation.\n(2) For binarized vision transformer models, LayerNorm is\nimportant to avoid gradient explosion. Thus, we add one\nmore LayerNorm layer at the end of each attention block.\n(3) When fine-tuning a pre-trained model (full precision),\nthe binarized vision transformer usually suffers from a bad\ninitialization. Thus, a few epochs of pre-training on the binarized vision transformer is extremely\nhelpful and can make fine-tuning much more efficient and effective.\n(4) We apply the knowledge distillation technique in BiBERT [45] to boost the performance. We\nuse full precision pre-trained models as the teacher model.\nMain Results: We report the main results of binarizing vision transformers in Table 3 (BW) and\nTable 4 (BW, BWA, BWAA), where we compare ProxConnect++ algorithms with the FP, PQ, and\nrPC baselines on fine-tuning and end-to-end training tasks. We observe that: (1) ProxConnect++\nvariants generally outperform PQ and rPC and are able to binarize vision transformers with less\nthan 10% accuracy degradation on the BW task. In particular, for end-to-end training, the best\nperforming ProxConnect++ algorithms achieve ≈ 5% accuracy drop; (2) Among the five variants,\nwe confirm BNN++ is also generally better overall for vision transformers. This provides evidence\nthat our Corollary 1 allows practitioners to easily design many and choose the one that performs best\nempirically; (3) With a clear underlying optimization objective, BNN++ again outperforms BNN+\nacross all tasks, which empirically verifies our theoretical findings on vision transformers; (4) In\ngeneral, we find that weight binarization achieves about 30x reduction in memory footprint, e.g.,\nfrom 450 MB to 15 MB for ViT-B.\nAblation Studies: We provide further ablation studies to gain more insights and verify our binariza-\ntion protocols for vision transformers.\nPost-training Binarization: in Figure 2, we verify the difference between PTB (post-training bina-\nrization) and BAT (binarization-aware training) on the fine-tuning task on CIFAR-10 across different\nmodels. Note that we use BNN++ as a demonstration of BAT. We observe that without optimization\nduring fine-tuning, the PTB approach fails in general, thus confirming the importance of considering\nBAT for vision transformers.\nEffect of binarizing protocols: here we show the effect of the four binarizing protocols mentioned\nat the beginning, including scaling the binarized weights using the mean of full precision weights\n6Notably, He et al. [19] also consider binarizing vision transformers, which we compare our implementation\ndetails and experimental results against in Appendix E.\n9\nFigure 3: Results of binarizing different components (blocks) of ViT-B architecture on CIFAR-10.\nWarmer color indicates significant accuracy degradation after binarization.\n(scaling), adding additional LayerNorm layers (normalization), BAT on the full precision pre-trained\nmodels (pre-train) and knowledge distillation. We report the results in Table 5 and confirm that each\nprotocol is essential to binarize vision transformers successfully.\nWhich block should one binarize: lastly, we visualize the sensitivity of each building block to bina-\nrization in vision transformers (i.e., ViT-B) on CIFAR-10 in Figure 3. We observe that binarizing\nblocks near the head and the tail of the architecture causes a significant accuracy drop.\n5\nConclusion\nIn this work we study the popular approximate gradient approach in neural network binarization. By\ngeneralizing ProxConnect and proposing PC++, we provide a principled way to understand forward-\nbackward quantizers and cover most existing binarization techniques as special cases. Furthermore,\nPC++ enables us to easily design the desired quantizers (e.g., the new BNN++) with automatic\ntheoretical guarantees. We apply PC++ to CNNs and vision transformers and compare its variants\nin extensive experiments. We confirm empirically that PC++ overall achieves competitive results,\nwhereas BNN++ is generally more desirable.\nBroader impacts and limitations\nWe anticipate our work to further enable training and deploying advanced machine learning models\nto resource limited devices and environments, and help reducing energy consumption and carbon\nfootprint at large. We do not foresee any direct negative societal impact. One limitation we hope\nto address in the future is to build a theoretical framework that will allow practitioners to quickly\nevaluate different forward-backward quantizers for a variety of applications.\nAcknowledgments and Disclosure of Funding\nWe thank the reviewers and the area chair for thoughtful comments that have improved our final\ndraft. We thank Arash Ardakani, Ali Mosleh and Marzieh Tahaei for their early participation in this\nproject. YY gratefully acknowledges NSERC and CIFAR for funding support.\n10\nReferences\n[1]\nM. Alizadeh, A. Behboodi, M. van Baalen, C. Louizos, T. Blankevoort, and M. Welling.\n“Gradient L1 Regularization for Quantization Robustness”. arXiv e-prints. 2020.\n[2]\nY. Bai, Y.-X. Wang, and E. Liberty. “ProxQuant: Quantized Neural Networks via Proximal\nOperators”. In: International Conference on Learning Representations. 2018.\n[3]\nR. Banner, Y. Nahshan, and D. Soudry. “Post training 4-bit quantization of convolutional\nnetworks for rapid-deployment”. In: Advances in Neural Information Processing Systems.\n2019, pp. 7948–7956.\n[4]\nY. Bengio, N. Léonard, and A. Courville. “Estimating or propagating gradients through\nstochastic neurons for conditional computation”. arXiv preprint arXiv:1308.3432. 2013.\n[5]\nY. Bhalgat, J. Lee, M. Nagel, T. Blankevoort, and N. Kwak. “LSQ+: Improving low-bit quanti-\nzation through learnable offsets and better initialization”. In: IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) Workshops. 2020, pp. 2978–2985.\n[6]\nS. Bhojanapalli, A. Chakrabarti, A. Veit, M. Lukasik, H. Jain, F. Liu, Y.-W. Chang, and\nS. Kumar. “Leveraging redundancy in attention with Reuse Transformers”. arXiv preprint\narXiv:2110.06821. 2021.\n[7]\nS. Biderman et al. “Pythia: A suite for analyzing large language models across training and\nscaling”. arXiv preprint arXiv:2304.01373 (2023).\n[8]\nT. Brown et al. “Language models are few-shot learners”. Advances in neural information\nprocessing systems, vol. 33 (2020), pp. 1877–1901.\n[9]\nY. Cai, Z. Yao, Z. Dong, A. Gholami, M. W. Mahoney, and K. Keutzer. “Zeroq: A novel zero\nshot quantization framework”. In: Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition. 2020, pp. 13169–13178.\n[10]\nJ. Choi, Z. Wang, S. Venkataramani, P. I.-J. Chuang, V. Srinivasan, and K. Gopalakrish-\nnan. “Pact: Parameterized clipping activation for quantized neural networks”. arXiv preprint\narXiv:1805.06085. 2018.\n[11]\nM. Courbariaux, Y. Bengio, and J.-P. David. “Binaryconnect: Training deep neural networks\nwith binary weights during propagations”. In: Advances in Neural Information Processing\nSystems. 2015.\n[12]\nS. Darabi, M. Belbahri, M. Courbariaux, and V. P. Nia. “Regularized binary network train-\ning”. In: NeurIPS Workshop on Energy Efficient Machine Learning and Cognitive Computing.\n2019.\n[13]\nT. Dockhorn, Y. Yu, E. Sari, M. Zolnouri, and V. Partovi Nia. “Demystifying and Generalizing\nBinaryConnect”. In: Advances in Neural Information Processing Systems. 2021, pp. 13202–\n13216.\n[14]\nA. Dosovitskiy et al. “An image is worth 16x16 words: Transformers for image recognition\nat scale”. In: International Conference on Learning Representations. 2021.\n[15]\nS. K. Esser, J. L. McKinstry, D. Bablani, R. Appuswamy, and D. S. Modha. “Learned step\nsize quantization”. arXiv preprint arXiv:1902.08153. 2019.\n[16]\nA. Ghaffari, M. S. Tahaei, M. Tayaranian, M. Asgharian, and V. P. Nia. “Is Integer Arith-\nmetic Enough for Deep Learning Training?” In: Advances in Neural Information Processing\nSystems. 2022.\n[17]\nS. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. “Deep Learning with Lim-\nited Numerical Precision”. In: Proceedings of the 32nd International Conference on Machine\nLearning (ICML). 2015, pp. 1737–1746.\n11\n[18]\nK. He, X. Zhang, S. Ren, and J. Sun. “Deep residual learning for image recognition”. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. 2016, pp. 770–\n778.\n[19]\nY. He, Z. Lou, L. Zhang, W. Wu, B. Zhuang, and H. Zhou. “BiViT: Extremely Compressed\nBinary Vision Transformer”. arXiv preprint arXiv:2211.07091. 2022.\n[20]\nK. Helwegen, J. Widdicombe, L. Geiger, Z. Liu, K.-T. Cheng, and R. Nusselder. “Latent\nweights do not exist: Rethinking binarized neural network optimization”. Advances in neural\ninformation processing systems, vol. 32 (2019).\n[21]\nZ. Hou and S.-Y. Kung. “Multi-Dimensional Model Compression of Vision Transformer”.\narXiv preprint arXiv:2201.00043. 2021.\n[22]\nI. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. “Binarized neural net-\nworks”. In: Advances in Neural Information Processing Systems. 2016.\n[23]\nB. Jacob, S. Kligys, B. Chen, M. Zhu, M. Tang, A. Howard, H. Adam, and D. Kalenichenko.\n“Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Infer-\nence”. In: IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2018.\n[24]\nS. R. Jain, A. Gural, M. Wu, and C. H. Dick. “Trained quantization thresholds for accurate and\nefficient fixed-point inference of deep neural networks”. arXiv preprint arXiv:1903.08066.\n2019.\n[25]\nS. Jung, C. Son, S. Lee, J. Son, J.-J. Han, Y. Kwak, S. J. Hwang, and C. Choi. “Learning to\nquantize deep networks by optimizing quantization intervals with task loss”. In: Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition. 2019, pp. 4350–4359.\n[26]\nH. Kim, J. Park, C. Lee, and J.-J. Kim. “Improving accuracy of binary neural networks using\nunbalanced activation distribution”. In: Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition. 2021, pp. 7862–7871.\n[27]\nA. Krizhevsky. “Learning multiple layers of features from tiny images”. Tech. rep. University\nof Toronto, 2009.\n[28]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. “Imagenet classification with deep convo-\nlutional neural networks”. In: Advances in Neural Information Processing Systems. 2012,\npp. 1097–1105.\n[29]\nY. Li, R. Gong, X. Tan, Y. Yang, P. Hu, Q. Zhang, F. Yu, W. Wang, and S. Gu. “BRECQ:\nPushing the Limit of Post-Training Quantization by Block Reconstruction”. In: International\nConference on Learning Representations. 2021.\n[30]\nZ. Li, T. Yang, P. Wang, and J. Cheng. “Q-ViT: Fully Differentiable Quantization for Vision\nTransformer”. arXiv preprint arXiv:2201.07703. 2022.\n[31]\nM. Lin, R. Ji, Z. Xu, B. Zhang, Y. Wang, Y. Wu, F. Huang, and C.-W. Lin. “Rotated bi-\nnary neural network”. Advances in neural information processing systems, vol. 33 (2020),\npp. 7474–7485.\n[32]\nY. Lin, T. Zhang, P. Sun, Z. Li, and S. Zhou. “FQ-ViT: Fully Quantized Vision Transformer\nwithout Retraining”. arXiv preprint arXiv:2111.13824. 2021.\n[33]\nC. Liu, P. Chen, B. Zhuang, C. Shen, B. Zhang, and W. Ding. “SA-BNN: State-aware binary\nneural network”. In: Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35.\n3. 2021, pp. 2091–2099.\n[34]\nZ. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. “Swin transformer:\nHierarchical vision transformer using shifted windows”. In: Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision. 2021.\n12\n[35]\nZ. Liu, Z. Shen, S. Li, K. Helwegen, D. Huang, and K.-T. Cheng. “How do adam and training\nstrategies help bnns optimization”. In: International conference on machine learning. PMLR.\n2021, pp. 6936–6946.\n[36]\nZ. Liu, Z. Shen, M. Savvides, and K.-T. Cheng. “Reactnet: Towards precise binary neural\nnetwork with generalized activation functions”. In: Computer Vision–ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XIV 16. Springer.\n2020, pp. 143–159.\n[37]\nZ. Liu, B. Wu, W. Luo, X. Yang, W. Liu, and K.-T. Cheng. “Bi-real net: Enhancing the\nperformance of 1-bit cnns with improved representational capability and advanced training\nalgorithm”. In: Proceedings of the European conference on computer vision (ECCV). 2018,\npp. 722–737.\n[38]\nZ. Liu, Y. Wang, K. Han, W. Zhang, S. Ma, and W. Gao. “Post-training quantization for vision\ntransformer”. Advances in Neural Information Processing Systems (2021), pp. 28092–28103.\n[39]\nC. Louizos, M. Reisser, T. Blankevoort, E. Gavves, and M. Welling. “Relaxed Quantization\nfor Discretized Neural Networks”. In: International Conference on Learning Representations\n(ICLR). 2019.\n[40]\nB. Martinez, J. Yang, A. Bulat, and G. Tzimiropoulos. “Training binary neural networks\nwith real-to-binary convolutions”. In: International Conference on Learning Representations.\n2019.\n[41]\nM. Nagel, R. A. Amjad, M. Van Baalen, C. Louizos, and T. Blankevoort. “Up or down?\nAdaptive rounding for post-training quantization”. In: International Conference on Machine\nLearning. 2020, pp. 7197–7206.\n[42]\nM. Nagel, M. v. Baalen, T. Blankevoort, and M. Welling. “Data-free quantization through\nweight equalization and bias correction”. In: Proceedings of the IEEE International Confer-\nence on Computer Vision. 2019, pp. 1325–1334.\n[43]\nR. Ni, H.-m. Chu, O. Castañeda Fernández, P.-y. Chiang, C. Studer, and T. Goldstein. “Wrap-\nnet: Neural net inference with ultra-low-precision arithmetic”. In: International Conference\non Learning Representations ICLR 2021. OpenReview. 2021.\n[44]\nB. Pan, R. Panda, Y. Jiang, Z. Wang, R. Feris, and A. Oliva. “IA-RED2: Interpretability-\nAware Redundancy Reduction for Vision Transformers”. In: Advances in Neural Information\nProcessing Systems. 2021, pp. 24898–24911.\n[45]\nH. Qin, Y. Ding, M. Zhang, Q. Yan, A. Liu, Q. Dang, Z. Liu, and X. Liu. “Bibert: Accurate\nfully binarized bert”. arXiv preprint arXiv:2203.06390. 2022.\n[46]\nH. Qin, R. Gong, X. Liu, M. Shen, Z. Wei, F. Yu, and J. Song. “Forward and backward\ninformation retention for accurate binary neural networks”. In: Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition. 2020, pp. 2250–2259.\n[47]\nY. Rao, W. Zhao, B. Liu, J. Lu, J. Zhou, and C.-J. Hsieh. “Dynamicvit: Efficient vision trans-\nformers with dynamic token sparsification”. In: Advances in Neural Information Processing\nSystems. 2021, pp. 13937–13949.\n[48]\nM. Rastegari, V. Ordonez, J. Redmon, and A. Farhadi. “Xnor-net: Imagenet classification\nusing binary convolutional neural networks”. In: European conference on computer vision.\nSpringer. 2016, pp. 525–542.\n[49]\nH. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai. “Combiner:\nFull attention transformer with sparse computation cost”. Advances in Neural Information\nProcessing Systems (2021), pp. 22470–22482.\n[50]\nR. T. Rockafellar and R. J.-B. Wets. “Variational Analysis”. Springer, 1998.\n13\n[51]\nM. S. Ryoo, A. Piergiovanni, A. Arnab, M. Dehghani, and A. Angelova. “TokenLearner:\nWhat Can 8 Learned Tokens Do for Images and Videos?” arXiv preprint arXiv:2106.11297.\n2021.\n[52]\nH. Touvron, M. Cord, M. Douze, F. Massa, A. Sablayrolles, and H. Jégou. “Training data-\nefficient image transformers & distillation through attention”. In: International Conference\non Machine Learning. 2021.\n[53]\nZ. Tu, X. Chen, P. Ren, and Y. Wang. “Adabin: Improving binary neural networks with adap-\ntive binary sets”. In: European conference on computer vision. Springer. 2022, pp. 379–395.\n[54]\nP. Wang, Q. Chen, X. He, and J. Cheng. “Towards accurate post-training network quantiza-\ntion via bit-split and stitching”. In: International Conference on Machine Learning. 2020,\npp. 9847–9856.\n[55]\nS. Xu, Y. Li, T. Ma, B. Zeng, B. Zhang, P. Gao, and J. Lu. “TerViT: An Efficient Ternary\nVision Transformer”. arXiv preprint arXiv:2201.08050. 2022.\n[56]\nZ. Xu, M. Lin, J. Liu, J. Chen, L. Shao, Y. Gao, Y. Tian, and R. Ji. “Recu: Reviving the\ndead weights in binary neural networks”. In: Proceedings of the IEEE/CVF international\nconference on computer vision. 2021, pp. 5198–5208.\n[57]\nJ. Zhang, H. Peng, K. Wu, M. Liu, B. Xiao, J. Fu, and L. Yuan. “MiniViT: Compressing Vision\nTransformers with Weight Multiplexing”. In: IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 2022, pp. 12145–12154.\n[58]\nX. Zhao, Y. Wang, X. Cai, C. Liu, and L. Zhang. “Linear symmetric quantization of neu-\nral networks for low-precision integer hardware”. In: International Conference on Learning\nRepresentations. 2019.\n14\nAppendix for Understanding Neural Network Binarization with\nForward and Backward Proximal Quantizers\nA\nMore on Proximal Quantizers\nDockhorn et al. [13] gave a complete characterization of the proximal quantizer Pr: a (multi-valued)\nmapping P is a proximal quantizer (of some underlying regularizer r) iff it is monotone, compact-\nvalued and with a closed graph. We now give a few examples to illustrate the ubiquity of proximal\nquantizers, as well as the generality of PC:\n• Identity function: apparently, choosing Pµt\nr\nas the identity function recovers the full preci-\nsion training.\n• Pµt\nr\n= PQ: as Q = {±1}, this choice recovers exactly BC in (2).\n• Pµt\nr\n= Lϱ\nρ: This is the general piecewise linear quantizer designed by Dockhorn et al. [13].\nRecall that Q = {qk}2\nk=1, where q1 = −1, q2 = +1, such that p2 = 0 is the middle point.\nBy introducing two parameters ρ, ϱ ≥ 0, we can define two shifts:\nhorizontal:q−\n1 = q1, q+\n1 = p2 ∧ (q1 + ρ)\nq−\n2 = p2 ∨ (q2 − ρ), q+\n2 = q2\nvertical:\np−\n2 = q1 ∨ (p2−ϱ), p+\n2 = q2 ∧ (p2+ϱ).\nThen, we define Lϱ\nρ as the piece-wise linear map (that simply connects the points by straight\nlines):\nLϱ\nρ(w∗)=\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nq1,\nif q−\n1 ≤ w∗ ≤ q+\n1\nq1 + (w∗ − q+\n1 )\np−\n2 −q1\np2−q+\n1\n,\nif q+\n1 ≤ w∗ < p2\np+\n2 + (w∗−p2)\nq2−p+\n2\nq−\n2 −p2\nif p2 < w∗ ≤ q−\n2\nq2,\nif q−\n2 ≤ w∗ ≤ q+\n2\n.\n(10)\nFor the middle points, Lϱ\nρ(w∗) can be regarded as the intermediate state between the identity\nfunction and PQ such that, where Lϱ\nρ(w∗) may take any value within the two limits. Note\nthat ρ controls the discretization vicinity, such that in practice, ρ is linearly increased over\ntime to fulfill binary weights in the end. We visualize examples of Lϱ\nρ(w∗) in Figure 4.\n−1\n1\n−1\n1\np−\n2\np+\n2\nw∗\nLϱ\nρ(w∗)\n(a) ρ = 0, ϱ = 0.2.\n−1\n1\n−1\n1\np−\n2\np+\n2\nw∗\nLϱ\nρ(w∗)\n(b) ρ = ϱ = 0.2.\n−1\n1\n−1\n1\nw∗\nLϱ\nρ(w∗)\n(c) ρ = 0.2, ϱ = 0.\nFigure 4: Different instantiations of the proximal map Lϱ\nρ in (10) for Q = {−1, 1}.\nB\nRelated works\nVision Transformer.\nIn computer vision, vision transformers have become one of the most pop-\nular backbone architectures. Dosovitskiy et al. [14] is the first to modify the transformer model to\nenable images as input, namely the ViT model. Specifically, Dosovitskiy et al. [14] translates an\nimage to a sequence of flattened image patches as input, and applies a self-attention mechanism\nto retrieve patch-wise information in the feature representation. Touvron et al. [52] further equips\nViT with knowledge distillation and proposes DeiT that generalizes well on smaller models and\ndatasets. Liu et al. [34] further proposes Swin as a hierarchical vision transformer that computes\nrepresentation with shifted windows.\n15\nIn vision transformers, the main computation overhead is the multi-head attention layer, whose cost\nis quadratic with the length of the image patches. As a result, such models are in general expensive to\ntrain. To reduce the computational cost, different compression techniques have been explored. For\ninstance, Pan et al. [44] performs dynamic pruning for less important patches; Bhojanapalli et al.\n[6] reuses attention scores computed for one layer in multiple building blocks; Hou and Kung [21]\napplies multi-dimensional model compression. In this paper, we focus on an alternative approach,\nnamely network quantization.\nNetwork Quantization.\nWe consider two possible scenarios of network quantization:\n(1) Post-training Quantization: We first discuss the easier post-training quantization methods. Such\napproaches usually quantize the full-precision pre-trained model and directly apply it for inference.\nPost-training quantization is widely used in CNNs [1, 3, 9, 29, 41, 42, 54]. Liu et al. [38] is the first\nto explore PTQ for vision transformers. It optimizes the quantization intervals and considers ranking\ninformation in the loss function. However, it only considers quantization to a 6-bit model without\nsevere performance degradation. For lower-bit quantization, it is essential to leverage training.\n(2) Quantization-Aware Training (QAT): different from post-training quantization, quantization-\naware training leverage quantization during pre-training or fine-tuning. Thus it can be formulated as\nan optimization problem for learning the optimal quantized weights [5, 10, 15, 17, 23–25, 39, 58].\nCompared with PTQ, QAT can obtain less accuracy drop in low-bit quantization compared to the\nfull-precision model. Li et al. [30] and Xu et al. [55] demonstrate that QAT requires a unique design\nto quantize vision transformers and it is possible to perform quantization to 3 bit without severe\nperformance degradation. [19] further performs binarization with softmax-aware binarization and\ninformation preservation.\nBinarization Techniques:\n(1) Here we summarize existing binarization approaches that can be\neither justified or improved by PC++: Some existing implementations [31, 37, 46] set the forward\nquantizer as sign and design the backward quantizer in an ad hoc fashion (based on graphic approx-\nimation of the sign function), e.g., Liu et al. [37] applies a piece-wise polynomial approximation;\nLin et al. [31] improves [37] with a dynamic polynomial approximation; Qin et al. [46] designs a\ndynamic error decay estimator based on the tanh function. These methods cannot be justified with\nPC++, but could be improved by designing new forward quantizers using our Corollary 2. We dis-\ncuss these new variants in Appendix F and compare them with our methods. Other implementations\n[36, 53] applies shift transformation on both forward and backward quantizers, which belongs to the\nPC++ family.\n(2) Architecture design that can be further integrated into our PC++ as future work: Xu et al. [56]\ndesigns a rectified clamp unit to address \"dead weights\"; Rastegari et al. [48] applies the absolute\nmean of weights and activations; Martinez et al. [40] uses real-to-binary attention matching and\ndata-driven channel re-scaling; Kim et al. [26] proposes a shifted activation function.\n(3) Optimization refinement, again could be integrated into our framework as future work: Liu\net al. [33] utilizes state-aware gradient state; Liu et al. [35] provides a weight decay scheme; and\nHelwegen et al. [20] proposes Bop, a new optimizer for BNNs.\nC\nAdditional Theoretical Results\nCorollary 1. A pair of forward-backward quantizers (F, B) admits the decomposition in (6) (for\nsome smoothing parameter µ and regularizer r) iff both F and B are functions of P(w) :=\nR w\n−∞\n1\nB(ω) dF(ω), which is proximal (i.e., monotone, compact-valued and with a closed graph).\nProof. We first recall the decomposition in (6):\nFµ\nr := T ◦ Pµ\nr ,\nBµ\nr := T′ ◦ Pµ\nr .\n(19)\nSuppose first that (F, B) satisfies the above decomposition. Clearly, both F and B are functions of\nP = Pµ\nr . Moreover,\nF′(ω)\nB(ω) = Pµ\nr\n′(ω) · T′ ◦ Pµ\nr\nT′ ◦ Pµ\nr\n= Pµ\nr\n′(ω)\n16\nand thus\nZ w\n−∞\n1\nB(ω) dF(ω) = Pµ\nr (w) − Pµ\nr (−∞),\nwhich is clearly proximal.\nConversely, let P(w) :=\nR w\n−∞\n1\nB(ω) dF(ω) be proximal. Taking (generalized) derivative we obtain\nP′(ω) = F′(ω)\nB(ω) .\nSince B is a function of P, say B = T′ ◦ P, performing integration we obtain\nF = T ◦ P,\nup to some immaterial constant (that can be absorbed into T). Thus, (F, B) satisfies the decomposi-\ntion (6).\nThe following convergence guarantee for PC++ follows directly from the results in Dockhorn et al.\n[13]:\nTheorem 1. Fix any w, the iterates in (7) satisfy:\nt\nX\nτ=s\nητ[⟨wτ − w, e∇ℓ(Twτ)⟩ + r(wτ) − r(w)] ≤ ∆s−1(w) − ∆t(w) +\nt\nX\nτ=s\n∆τ(wτ),\n(11)\nwhere ∆τ(w) := rτ(w) − rτ(wτ+1) −\n\nw − wτ+1, w∗\nτ+1\n\u000b\nis the Bregman divergence induced by\nthe (possibly nonconvex) function rτ(w) := µτ+1·r(w)+ 1\n2∥w∥2\n2. (Recall that µt := 1+Pt−1\nτ=1 ητ.)\nThe summand on the left-hand side of (11) is related to the duality gap, which is a natural measure\nof stationarity for the nonconvex problem (4). Indeed, it reduces to the familiar ones when convexity\nis present:\nTheorem 2. For convex ℓ ◦ T and any w, the iterates in (7) satisfy:\nmin\nτ=s,...,t E[f(wτ)−f(w)] ≤\n1\nPt\nτ=s ητ · E\n\u0002\n∆s−1(w)−∆t(w)+\nXt\nτ=s ∆τ(wτ)\n\u0003\n.\n(12)\nIf r is also convex, then\nmin\nτ=s,...,t E[f(wτ)−f(w)] ≤\n1\nPt\nτ=s ητ · E\n\u0002\n∆s−1(w)+\nXt\nτ=s\nη2\nτ\n2 ∥e∇ℓ(wτ)∥2\n2\n\u0003\n,\n(13)\nand\nE\n\u0002\nf( ¯wt)−f(w)\n\u0003\n≤\n1\nPt\nτ=s ητ · E\n\u0002\n∆s−1(w)+\nXt\nτ=s\nη2\nτ\n2 ∥e∇ℓ(wτ)∥2\n2\n\u0003\n,\n(14)\nwhere wt =\nPt\nτ=s ητ wτ\nPt\nτ=s ητ , and f := ℓ ◦ T + r is the regularized and transformed objective.\nThe right-hand sides of (13) and (14) diminish iff ηt → 0 and P\nt ηt = ∞ (assuming boundedness of\nthe stochastic gradient). We note some trade-off in choosing the step size ητ: both the numerator and\ndenominator of the right-hand sides of (13) and (14) are increasing w.r.t. ητ. The same conclusion\ncan be drawn for (12) and (11), where ∆τ also depends on ητ (through the accumulated magnitude\nof w∗\nτ+1).\nD\nAdditional Experimental Settings\nHardware and package:\nAll experiments were run on a GPU cluster with NVIDIA V100 GPUs.\nThe platform we use is PyTorch. Specifically, we apply ViT and DeiT models implemented in\nPytorch Image Models (timm) 7.\n7https://timm.fast.ai/\n17\n−1\n1\n−1\n1\nBi-Real(forward)\n−1\n1\n−1\n1\nBi-Real(backward)\n−1\n1\n−1\n1\nR-BNN (forward)\n−1\n1\n−1\n1\nR-BNN (backward)\n−1\n1\n−1\n1\nPoly+ (forward)\n−1\n1\n−1\n1\nPoly+(backward)\n−1\n1\n−1\n1\nEDE (forward)\n−1\n1\n−1\n1\nEDE (backward)\n−1\n1\n−1\n1\nEDE+ (forward)\n−1\n1\n−1\n1\nEDE+ (backward)\n−1\n1\n−1\n1\nReActNet (forward)\n−1\n1\n−1\n1\nReActNet (backward)\nFigure 5: Forward and backward pass for 6 additional ProxConnect++ algorithms.\nTable 6: Results for additional ProxConnect++ algorithms on binarizing vision transformers (bina-\nrizing weights only), where the results are the mean of five runs with different random seeds.\nModel\nDataset\nFP\nProxConnect++\nBi-Real\nR-BNN\nPoly+\nEDE\nEDE+\nReAct\nBNN++\nDeiT-T\nCIFAR-10\n94.85%\n84.11%\n84.54%\n85.31%\n84.99%\n85.57%\n85.35%\n86.41%\nCIFAR-100\n72.37%\n59.01%\n59.02%\n60.00%\n59.32%\n60.04%\n60.11%\n60.33%\nImageNet-1K\n72.20%\n64.55%\n64.59%\n64.97%\n64.28%\n65.01%\n65.37%\n67.34%\nDeiT-S\nCIFAR-10\n95.09%\n85.01%\n86.07%\n84.99%\n85.37%\n85.33%\n85.91%\n86.19%\nCIFAR-100\n73.19%\n59.66%\n59.75%\n60.14%\n60.09%\n61.17%\n61.09%\n62.98%\nImageNet-1K\n79.90%\n70.51%\n70.87%\n71.36%\n70.66%\n72.99%\n72.53%\n73.53%\nPre-trained models:\nIn this work, we applied pre-trained full precision models for fine-tuning\ntasks. Here we specify the links to the models we used (note that we choose patch size equal to 16\nacross all models):\n• ViT-B\n(ImageNet-1K):\nhttps://storage.googleapis.com/vit_models/\naugreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.\n0--imagenet2012-steps_20k-lr_0.01-res_224.npz;\n• ViT-B (ImageNet-21K): https://storage.googleapis.com/vit_models/augreg/\nB_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz;\n• DeiT-T\n(ImageNet-1K):\nhttps://dl.fbaipublicfiles.com/deit/deit_tiny_\npatch16_224-a1311bcf.pth;\n• DeiT-S\n(ImageNet-1K):\nhttps://dl.fbaipublicfiles.com/deit/deit_small_\npatch16_224-cd65a155.pth;\n• DeiT-B\n(ImageNet-1K):\nhttps://dl.fbaipublicfiles.com/deit/deit_base_\npatch16_224-b5f2ef4d.pth.\nE\nComparison with BiViT\nHe et al. [19] propose BiViT, which considers the same binarization task on vision transformers\n(specifically, Swin-T and Nest-T). He et al. [19] follow a different implementation with softmax-\naware binarization and information preservation. To fairly compare with this work, we follow the\nsame setting and run PC++ on Swin-T and NesT-T on ImageNet-1K. We observe that BNN++\nachieves 71.3% Top-1 accuracy (BiViT:70.8%) and 69.3% Top-1 accuracy (BiViT:68.7%) respec-\ntively on Swin-T and NesT-T. Note that BiViT simply applies BNN as the main algorithm and may\nbe further improved with PC++ algorithms.\nF\nAdditional forward-backward quantizers\nIn this section, we summarize additional forward-backward quantizers, which can be either im-\nproved or justified with our PC++ framework. Specifically, we find that:\n18\nTable 7: Error bar for binarizing weights (BW), binarizing weights and activation (BWA) and bina-\nrizing weights, activation, with 8-bit accumulators (BWAA) on CNN backbones. We consider the\nend-to-end (E2E) pipeline. We compare five variants of ProxConnect++ (BC, PC, BNN, BNN+, and\nBNN++) with FP, PQ, and rPC. For the end-to-end pipeline, we report the mean of five runs with\ndifferent random seeds.\nDataset\nTask\nFP\nPQ\nrPC\nProxConnect++\nBC\nPC\nBNN\nBNN+\nBNN++\nCIFAR-10\nBW\n92.01%\n81.59%\n81.82%\n87.51%\n88.05%\n89.92%\n89.39%\n90.03%\n±0.19\n±0.11\n±0.16\n±0.07\n±0.05\n±0.11\n±0.13\n±0.06\nBWA\n92.01%\n81.51%\n81.60%\n86.99%\n87.26%\n89.15%\n89.02%\n89.91%\n±0.13\n±0.16\n±0.09\n±0.11\n±0.23\n±0.08\n±0.16\n±0.09\nImageNet-1K\nBW\n78.87%\n63.23%\n66.39%\n67.45%\n67.51%\n67.49%\n66.99%\n68.11%\n±0.06\n±0.11\n±0.22\n±0.04\n±0.09\n±0.12\n±0.26\n±0.02\nBWA\n78.87%\n61.19%\n64.17%\n65.42%\n65.31%\n65.29%\n65.98%\n66.08%\n±0.18\n±0.22\n±0.19\n±0.22\n±0.17\n±0.21\n±0.15\n±0.13\nTable 8: Error bar on binarizing vision transformers (BW and BWA) on ImageNet-1K. We consider\nthe end-to-end (E2E) pipeline. We compare five variants of ProxConnect++ (BC, PC, BNN, BNN+,\nand BNN++) with FP, PQ, and rPC. The results are the mean of five runs with different random\nseeds.\nModel\nTask\nFP\nPQ\nrPC\nProxConnect++\nBC\nPC\nBNN\nBNN+\nBNN++\nDeiT-T\nBW\n72.20%\n61.23%\n60.35%\n63.22%\n66.15%\n65.00%\n66.67%\n67.34%\n±0.11\n±0.07\n±0.19\n±0.21\n±0.11\n±0.15\n±0.09\n±0.07\nBWA\n72.20%\n60.01%\n58.77%\n62.13%\n65.29%\n63.75%\n65.29%\n65.65%\n±0.13\n±0.12\n±0.08\n±0.06\n±0.19\n±0.18\n±0.06\n±0.03\nDeiT-S\nBW\n79.91%\n69.87%\n68.74%\n73.16%\n73.51%\n73.77%\n73.23%\n73.53%\n±0.21\n±0.26\n±0.16\n±0.19\n±0.22\n±0.08\n±0.11\n±0.13\nDeiT-B\nBW\n81.81%\n72.54%\n70.11%\n76.55%\n76.61%\n75.60%\n76.63%\n76.74%\n±0.17\n±0.15\n±0.23\n±0.07\n±0.24\n±0.17\n±0.13\n±0.07\n• Bi-Real Net [37]/R-BNN [31]: F(w) = sign, B(w) = ∇F(w), where F(w) is a piewise\npolynomial function. We simply choose F = F(w) and arrive at our legitimate variant\nPoly+. Note that we gradually increase the coefficient of F(w) such that we ensure full\nbinarization at the end of the training phase.\n• EDE in IR-Net [46]: F(w) = sign, B(w) = ∇g(w) = kt(1 − tanh2(tw)), where k and t\nare control variables varying during the training process, such that g(w) ≈ sign at the end\nof training. Again, we choose F = g(w) and arrive at our new legitimate variant EDE+.\n• ReActNet [36] can be well justified and is a special case of PC++.\nWe visualize these forward-backward quantizers and our new variants in Figure 5. Moreover, we\nperform experiments on vision transformers to examine the performance of additional quantizers and\ntheir modified variants. We report the results in Table 6 and observe that (1)Our new proposed Poly+\nand EDE+ always outperform the original algorithms and further confirm that our PC++ framework\nmerits theoretical and empirical justifications; (2)BNN++ still outperforms other algorithms on all\ntasks.\nG\nAdditional results for end-to-end training\nFinally, we provide the error bars for our main experiments in Table 7 and Table 8 for CNN back-\nbones and vision transformer backbones, respectively.\n19\n"
}