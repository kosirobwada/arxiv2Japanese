{
    "optim": "CustomSketching:\nSketch Concept Extraction for Sketch-based Image Synthesis and Editing\nChufeng Xiao, Hongbo FuB\nSchool of Creative Media, City University of Hong Kong\nchufengxiao@outlook.com, fuplus@gmail.com\n\"[v] on top of a wooden floor\"\n\"[v] with a pink solid background\"\n\"[v] at the beach\"\n\"[v] in the snow\"\n\"[v] in the jungle\"\n\"[v] in the office\"\nTraining Data\nTraining Data\nTraining Data\nTraining Data\nTraining Data\nTraining Data\nFigure 1. Given one or several sketch-image pairs as training data, our CustomSketching can learn a novel sketch concept into a text token\n[v] and specific sketches. We decompose a sketch into shape lines (blue strokes) and detail lines (red strokes) to reduce the ambiguity in a\nsketch. Users may input a text prompt and a dual-sketch to re-create or edit the concept at a fine-grained level.\nAbstract\nPersonalization techniques for large text-to-image (T2I)\nmodels allow users to incorporate new concepts from ref-\nerence images. However, existing methods primarily rely\non textual descriptions, leading to limited control over cus-\ntomized images and failing to support fine-grained and lo-\ncal editing (e.g., shape, pose, and details). In this paper, we\nidentify sketches as an intuitive and versatile representation\nthat can facilitate such control, e.g., contour lines captur-\ning shape information and flow lines representing texture.\nThis motivates us to explore a novel task of sketch concept\nextraction: given one or more sketch-image pairs, we aim\nto extract a special sketch concept that bridges the corre-\nspondence between the images and sketches, thus enabling\nsketch-based image synthesis and editing at a fine-grained\nlevel. To accomplish this, we introduce CustomSketching, a\ntwo-stage framework for extracting novel sketch concepts.\nConsidering that an object can often be depicted by a con-\ntour for general shapes and additional strokes for internal\ndetails, we introduce a dual-sketch representation to reduce\nthe inherent ambiguity in sketch depiction. We employ a\nshape loss and a regularization loss to balance fidelity and\neditability during optimization. Through extensive experi-\n1\narXiv:2402.17624v1  [cs.CV]  27 Feb 2024\nments, a user study, and several applications, we show our\nmethod is effective and superior to the adapted baselines.\n1. Introduction\nThe recent advent of large text-to-image (T2I) models\n[44, 47, 49] has opened up new avenues for image synthesis\ngiven text prompts. Based on such models, personalization\ntechniques like [21, 31, 48] have been proposed to learn\nnovel concepts on unseen reference images by fine-tuning\nthe pre-trained models. Users can employ text prompts to\ncreate novel images containing the learned concepts in di-\nverse contexts by leveraging the significant semantic priors\nof these powerful generative models.\nHowever, the existing personalization methods fail to\naccurately capture the spatial features of target objects in\nterms of their geometry and appearance. This limitation\narises due to their heavy reliance on textual descriptions\nduring the image generation process. While some following\nworks like [3, 15] have attempted to address this issue by in-\ncorporating explicit masks or additional spatial image fea-\ntures, they are still limited to providing precise controls and\nlocal editing on fine-grained object attributes (e.g., shape,\npose, details) for the target concept solely through text.\nTo achieve fine-grained controls, sketches can serve as\nan intuitive and versatile handle for providing explicit guid-\nance. T2I-Adapter [36] and ControlNet [70] have enabled\nthe T2I models to be conditioned on sketches by incorpo-\nrating an additional encoder network for sketch-based im-\nage generation.\nSuch conditional methods perform well\nwhen an input sketch depicts the general contour of an ob-\nject (e.g., the blue strokes in Figure 2 (b)). However, we\nobserved they struggle to interpret and differentiate other\ntypes of sketches corresponding to specific local features\nin realistic images. As illustrated in Figure 2, these meth-\nods fail to correctly interpret detail lines for clothing folds\nand flow lines for hair texture (the red strokes in (b)). The\nprimary reason behind the issue is that the sketch dataset\nused to train the conditional networks [36, 70] is inherently\nambiguous since it is generated automatically through edge\ndetection on photo-realistic images. Consequently, directly\nincorporating a pre-trained sketch encoder with personal-\nization techniques proves challenging when attempting to\ncustomize a novel concept guided by sketches.\nBased on the aforementioned observation, we propose\na novel task of sketch concept extraction for image syn-\nthesis and editing to tackle the issue of sketch ambiguity.\nThe key idea is to empower users to define personalized\nsketches corresponding to specific local features in photo-\nrealistic images. Users can sketch their desired concepts by\nfirst tracing upon one or more reference images and then\nmanipulating the learned concepts by sketching, as shown\nin Figure 1.\n\"a photo of a\nwoman with hair\"\n\"a photo of a\nclothes with folds\"\n(a) Reference\n& Prompt\n(b) Sketch\n(c) T2I-Adapter (d) ControlNet\n(e) Ours\nFigure 2. Given a text prompt (a, bottom) and a sketch (b) depict-\ning specific semantics (e.g., clothing folds and hair), T2I-adapter\n(c) and ControlNet (d) could not correctly interpret the out-of-\ndomain sketch types, while our method can extract such a novel\nsketch concept and reconstruct the reference image (a, top). Note\nthat the reference image is not used by (c) and (d), and their results\nare for reference only.\nTo achieve sketch-based editability and identity preser-\nvation, we propose a novel personalization pipeline called\nCustomSketching for extracting sketch concepts.\nThis\npipeline is built upon a pretrained T2I model and incorpo-\nrates additional encoders to extract features from the sketch\ninput. Since a single image may exhibit diverse local fea-\ntures corresponding to different types of sketches, we em-\nploy a dual-sketch representation via two sketch encoders\nto decouple shape and detail depiction. Our pipeline con-\nsists of two stages: in Stage I, we optimize a textual token\nfor global semantics but freeze the weights of the sketch en-\ncoders; in Stage II, we jointly fine-tune the weights of the\nsketch encoders and the learned token to reconstruct the ref-\nerence images in terms of local appearance and geometry.\nTo prevent overfitting, we perform data augmentation and\nintroduce a shape loss for sketch-guided shape constraint\nand a regularization loss for textual prior preservation.\nTo the best of our knowledge, our method is the first\nwork to extract sketch concepts using large T2I models,\nthus providing users with enhanced creative capabilities for\nediting real images. To evaluate our method, we collect\na new dataset including sketch-image pairs and the edited\nsketches, where each sketch comprises a dual representa-\ntion. Through qualitative and quantitative experiments, we\ndemonstrate the superiority and effectiveness of CustomS-\nketching, compared to the adapted baselines. Given the ab-\nsence of a definitive metric to measure the performance of\nimage editing, we conduct a user study to gather user in-\n2\nsights and feedback. Additionally, we showcase several ap-\nplications enabled by our work.\nThe contributions of our work can be summarized as fol-\nlows. 1) We propose the novel task of sketch concept ex-\ntraction. 2) We introduce a novel framework that enables a\nlarge T2I model to extract and manipulate a sketch concept\nvia sketching, thereby improving its editability and control-\nlability. 3) We create a new dataset for comprehensive eval-\nuations and demonstrate several sketch-based applications\nenabled by CustomSketching.\n2. Related Work\nText-to-Image Synthesis and Editing. Text-to-image gen-\neration has made significant strides in recent years, achiev-\ning remarkable performance. Early works [45, 64, 67–69]\nemployed RNN [17, 27] and GANs [6, 24, 29] to control\nimage generation, processing, and editing in specific sce-\nnarios, such as human faces [62], fashion [33], and coloriza-\ntion [73]. These works rely on well-prepared datasets tai-\nlored to the target scenarios, posing a bottleneck in dataset\navailability. To alleviate this limitation, subsequent studies\n[1, 5, 19, 22, 34, 39] adopted CLIP [43], a large language-\nimage representation model based on Transformer [56], to\nalign image-text features and achieve robust performance\nin text-driven image manipulation tasks. Nonetheless, these\napproaches are still confined to limited domains, challeng-\ning their extension to other domains.\nThe emergence of diffusion models [20, 26, 44, 47, 52,\n53] trained with large-scale image-text datasets allows for\nuniversal image generation from open-domain text, surpass-\ning previous works based on GANs. Leveraging the power\nof diffusion models, several approaches have been proposed\nto manipulate images globally using text [7, 8, 18, 30, 55]\nand locally using masks [37, 40, 60]. For example, Mokady\net al. [35] proposed an inversion method that first inverts\na real image into latent representations, given which the\nmethod enables text-based image editing (e.g., changing lo-\ncal objects or modifying global image styles) by manipulat-\ning cross-attention maps [25]. Blended Diffusion [2, 4] can\nmerge an existing object into a real image. However, these\napproaches face challenges in modifying the fine-grained\nobject attributes of real images due to the abstract nature of\nthe text. Building upon Stable Diffusion [47], our method\naddresses this issue by incorporating sketches as an intuitive\nhandle to manipulate real images. Inspired by [25], we in-\ntroduce a shape loss that leverages cross-attention maps to\nprovide guidance based on sketches.\nPersonalization Techniques. The personalization task\nis to produce image variations of a given concept in refer-\nence images. GAN-based methods for this task only focus\non the same category (e.g., aligned faces) [38, 46] or on a\nsingle image [57], and thus could not manipulate images\nin a new context. Most recently, diffusion-based methods\nbased on text-to-image models optimize a new [21] or rare\n[48] textual token to learn the novel concept and generate\nthe concept in diverse contexts via text prompting. For fast\npersonalization, many researchers [14, 15, 23, 28, 51, 61]\nintroduce a prior encoder with local and global mapping to\nsave optimization time. For multi-concept personalization,\nAvrahami et al. [3] fine-tuned a set of new tokens and the\nweights of a denoising network from a single image given\nmasks, while Kumari et al. [31] optimized only several lay-\ners of the network based on a few images. Unlike these two\nmethods, which need to fine-tune simultaneously the multi-\nconcepts that are desired in generation, our method can sep-\narately extract sketch concepts for diverse targets and then\nwork for multi-concept generation by plug-and-play with-\nout extra optimization (see Figure 8 (c)).\nHowever, the existing personalization works do not al-\nlow precise control for novel concept generation and thus\ncould not work for local or detailed editing (e.g., addition,\nremoval, modification) of the learned concept. To address\nthe issue, we introduce a new task of sketch concept ex-\ntraction by optimizing sketch encoder(s) given one or more\nsketch-image pairs.\nSketch-based Image Synthesis and Editing. As an in-\ntuitive and versatile representation, sketch has been exten-\nsively explored to achieve fine-grained geometry control in\nrealistic image synthesis and editing. For instance, Sangk-\nloy et al. [50] utilized colored scribbles to depict geome-\ntry and appearance and synthesized images of various cat-\negories such as bedrooms, cars, and faces. Similarly, Chen\nand Hays [13] employed freehand sketches to learn shape\nknowledge for diverse objects. Chen et al. [11, 12] and Liu\net al. [32] utilized line drawings for image synthesis, edit-\ning, and video editing of human faces. In SketchHairSalon\n[63], flow lines are used to represent unbraided hair, while\ncontour lines depict braided hair. For local editing, a par-\ntial sketch has been adopted for minor image editing, e.g.,\nFaceShop [42], Sketch2Edit [66], Draw2Edit [65]. Unlike\nthe previous works that train dedicated networks for specific\ndomains or limited object categories, our method is generic\nand few-shot, which can handle versatile sketches for im-\nage synthesis and editing using a pre-trained text-to-image\nmodel.\nRecently, sketch-based text-to-image diffusion models\nhave also been explored [16, 41, 59]. Voynov et al. [58]\nutilized sketches as a shape constraint for optimizing the la-\ntent map in a diffusion model, while T2I-Adapter [36] and\nControlNet [70] are two concurrent works that train an ex-\nternal sketch encoder connected to a pre-trained diffusion\nmodel to enable sketch control. However, directly integrat-\ning these methods with personalization techniques may not\naccurately extract sketch concepts for all types of sketches\n(Figure 2), since the models [36, 70] are biased towards\ntraining data, specifically edge maps automatically detected\n3\nSketch\nEncoder\n+ Noise\nSketch\nEncoder\nSketch\nEncoder\n+ Noise\n\"a photo of a [v]\"\n\"a photo of a [v]\"\nReconstruction\nReconstruction\nTraining Stage I\nTraining Stage II\n\"a photo of a [v] with white clothes in the street\"\nInference\nFrozen\nLearnable\nText-to-Image\nDIffusion Model\nText-to-Image\nDIffusion Model\nForeground Mask\nFigure 3. The pipeline of our CustomSketching, which extracts novel sketch concepts for fine-grained image synthesis and editing via a\ntwo-stage framework. During training, given one or a few sketch-image pairs, Stage I only optimizes a textual embedding of a newly added\ntoken [v] to represent the global semantics of the reference image(s), while Stage II jointly fine-tunes the token and two sketch encoders to\nreconstruct the concept in terms of local appearance and geometry. We adopt a dual-sketch representation to differentiate shape lines SC\nand detail lines SD. During inference, users may provide a text prompt and a dual-sketch to manipulate the learned concept.\nfrom images. We will establish this setup for the existing\npersonalization methods as baselines to compare with our\nmethod, though we are the first to customize novel sketch\nconcepts.\n3. Method\nBased on a pre-trained T2I diffusion model, our goal is to\nembed a new sketch concept into the model, enabling the\nsynthesis and manipulation of diverse semantics in refer-\nence images through sketching and prompting (see Figure\n1). To this end, we propose a novel framework, CustomS-\nketching, which extracts a sketch concept from one or more\nreference images I and their corresponding sketches S. As\nillustrated in Figure 3, the framework comprises two train-\ning stages to reconstruct the reference image. During in-\nference, users can flexibly control the generation of a target\nimage that satisfies the context described by a text and faith-\nfully reflects the input sketch in terms of geometry. In the\nfollowing, we will describe the method details.\nTwo-stage Optimization. To leverage the robust textual\nprior of a large T2I model, following TI [21], we introduce\na newly added textual token [v] to capture global semantics\nwhile utilizing sketch representations through sketch en-\ncoder(s) to capture local features. Directly incorporating the\npersonalization method [21] with a pre-trained encoder like\n[36] could not fully restore the local geometry and appear-\nance of the target image (see the results by TI-E in Figure\n4). It is because it fine-tunes merely textual embedding v\nfor the token [v]. However, through joint optimization of the\ntextual embedding and the weights of the sketch encoder(s),\nwe encountered challenges in disentangling the global and\nlocal representations, resulting in unsatisfactory reconstruc-\ntion (see Supp). To focus on learning separate features, in-\nspired by [3], we adopt a two-stage optimization strategy. In\nStage I, we optimize the textual embedding while freezing\nthe weights of a pre-trained sketch encoder [36], establish-\ning a pivotal initialization for the next stage. In Stage II, we\njointly fine-tune the embedding and two sketch encoders to\nrecover the target identity. Note that, in both stages, we\nfreeze the denoising network of the pre-trained model to\npreserve its prior knowledge for editing.\nDual Sketch Representations. In Stage I, we fix the\nlocal features from sketches to guide the learning of the\nglobal textual embedding.\nTo employ the prior knowl-\nedge of the sketch encoder [36], which was pre-trained\non a large-scale sketch dataset, we input a binary sketch\n(where blue and red lines in Figure 3 are represented as\nblack and the background as white) similar to the input used\nduring pre-training. However, this single sketch represen-\ntation inherently contains ambiguity since it combines the\nmajor contour sketch (blue lines, denoted as SC) indicat-\ning the general shape with other minor types of sketches\n(red lines, denoted as SD) capturing internal details (e.g.,\nhair flow, clothes fold, wrinkles). This inherent ambiguity\nis the primary factor that biases the pre-trained sketch en-\ncoder [36, 70] towards general shape, as illustrated in Figure\n2. Therefore, in Stage II, optimizing the weights of a sketch\nencoder using the single-sketch representation would still\n4\nresult in ambiguous image editing (see Section 4.2).\nTo address this issue, we propose using a dual-sketch\nrepresentation that decomposes a given sketch S into two\ndistinct types of sketches, namely SC and SD as mentioned\nabove, for Stage II. Instead of merging SC and SD into a\nsingle map and feeding it into a single encoder (see Supp),\nwe employ two separate sketch encoders to extract features\ncorresponding to each type of sketch individually. This con-\nfiguration enables us to capture more distinct and recogniz-\nable features for SC and SD, resulting in plausible perfor-\nmance in decomposing shape and details, compared to the\nsetting of the single-sketch representation. The features ex-\ntracted from both types of sketches are aggregated through\nsummation before being injected into the pre-trained T2I\nmodel.\nMasked Encoder. As our focus is sketching the concept\nin the foreground, the sketch map S often contains signif-\nicant blank areas representing the background. Therefore,\nfine-tuning the sketch encoder(s) on the entire map would\nlead to overfitting the background regions not represented\nin the sketch, consequently undermining the text-guided ed-\nitability of the T2I model (see Figure 7). To address it, we\napply a foreground mask M to remove the background fea-\ntures extracted from the encoder(s). The foreground mask\ncan either be generated automatically by filling a convex\npolygon following SC, or be manually drawn by users. In\nsummary, the sketch features are passed into the T2I model\nFm along with a prompt pv containing the token [v] to de-\nrive the fused features F. For Stage I, we denote it as:\nbFi = Fi\ne(S) · M i + Fi\nm(pv), i ∈ {1, 2, 3, 4},\n(1)\nwhile for Stage II:\nbFi = (Fi\nc(SC) + Fi\nd(SD)) · M i + Fi\nm(pv),\n(2)\nwhere Fi\ne(S) is the i-th layer sketch feature extracted by\nthe pre-trained encoder [36], while Fi\nc(SC) and Fi\nd(SD) are\ndual sketch representations from the fine-tuned encoders,\nand Mi is the resized mask fit to the feature size. We adopt\nfour layers of the features as used in [36].\nLoss Function. To optimize the sketch concept, which\ninvolves the embedding v and the weights of Fc and Fd, we\ncombine three types of losses for the text- and sketch-based\nproblem. Firstly, we utilize a classic diffusion loss with the\nforeground mask M to reconstruct the target image regard-\ning appearance and geometry. This loss encourages the op-\ntimization to concentrate on the foreground object depicted\nby the sketches, formulated as\nLrec = Ez,t,v,FS,ϵ [∥ϵ · M − ϵθ(zt, t, pv, FS) · M∥] , (3)\nwhere FS denotes the sketch features in the two stages, and\nϵθ is the denoising network of the T2I model. At each opti-\nmization step, we randomly sample a timestep t from [0, T]\nand add noise ϵ ∼ N(0, 1) to the image latent z0 to be zt .\nHowever, relying solely on the masked diffusion loss\nmay not provide sufficient constraints to ensure the faith-\nfulness between the sketch and the generated image. For\nexample, certain parts depicted by the sketch would be lost,\nor unexpected elements would be produced in the gener-\nated results, as shown in Figure 6. Motivated by previous\nworks [3, 10, 25] that leverage cross-attention maps of the\nT2I model to control the layout and semantics of the target,\nwe propose a shape loss based on the cross-attention map\nof the token [v]. The shape loss Lshape comprises a fore-\nground loss for guiding the concept shape to align with the\nsketch depiction via M, and a background loss for penal-\nizing foreground pixels that violate the background region.\nWe denote the shape loss as:\nLfg = ∥norm(Aθ(zt, v)) · M − M∥ ,\n(4)\nLbg = mean(Aθ(zt, v) · (1 − M)),\n(5)\nLshape = Lfg + Lbg,\n(6)\nwhere Aθ(zt, v) is the cross-attention maps given the la-\ntent zt and token [v]. norm(·) is to normalize the attention\nmap to [0, 1], while mean(·) computes the average attention\nvalue of background pixels.\nIn addition, the two-stage optimization may cause the\nfine-tuned embedding v to increase too large so that it over-\nfits the reference shape, thus damaging the sketch editability\n(see Figures 6 & 7). We, therefore, introduce a regulariza-\ntion loss for the embedding via an L2 norm:\nLreg = ∥v∥ .\n(7)\nIn total, the loss function for the two stages is:\nLtotal = Lrec + λshapeLshape + λregLreg,\n(8)\nwhere we set the weights λshape as 0.01 and λreg as 0.001\nempirically.\nImplementation Details. To avoid the method overfit-\nting a few training images, we adopt on-the-fly augmen-\ntation tricks (horizontal flip, translation, rotation) on the\nsketch-image pairs during optimization. Please find more\nimplementation details in Supp.\n4. Experiments\nWe have conducted extensive evaluations to quantitatively\nand qualitatively evaluate our method CustomSketching.\nWe first show the comparisons between our method and\nthe personalization baselines adapted to our proposed task.\nThen, we evaluate the effectiveness of our settings via an\nablation study. We further conduct a perceptive user study\non the edited results by the compared methods. In addition,\nwe implement several applications based on our method to\nshow the usefulness of the extracted sketch concepts. Please\nfind more details, comparisons, and results in Supp.\n5\n\"a photo of [v] on top of a wooden floor\"\n\"a photo of [v] with the Eiffel tower in the background\"\n\"a photo of [v] in the snow\"\n\"a photo of [v] with a city in the background\"\n\"a photo of [v] in an office \"\n\"a photo of [v] in an office \"\n\" a photo of [v] with the Eiffel tower in the background\"\n\" a photo of [v] with a mountain in the background\"\nOurs\nMC-E\nTI-E\nDB-E\nOurs\nMC-E\nTI-E\nDB-E\nSketch\nSketch\nOriginal Image\nFigure 4. Comparisons of the results generated by our method and three adapted baselines, given the same text prompt and sketch. In the\nsketch column, the top one is the annotated sketch corresponding to the original image for training while the bottom one is an edited sketch.\nDataset. Before comparisons, we prepare a dataset of\nimage-sketch pairs covering diverse categories (e.g., toys,\nhuman portraits, pets, buildings). We first collect images\nfrom the personalization works [21, 31] and the sketch-\nbased work [63]. Next, we invite three normal users with-\nout any professional training in drawing to trace the im-\nages with separate contour lines SC and detail lines SD\nand then edit several sketches initialized with one of the\ntraced sketches to depict a target object by changing its\nshape, pose, and/or details. Following the general instruc-\ntion that SC depicts a coarse shape while SD is inside the\nshape, users decided SC and SD by themselves and drew\nthem consistently for training and testing to personalize the\nsketch concept. Finally, we obtain 35 groups of concept\ndata. Each concept has 1-6 image-sketch pair(s) and 3-5\nedited sketches. In total, the dataset contains 102 traced\nsketches with the corresponding images for training and 159\nedited sketches without paired images. Moreover, we em-\nploy ten prompt templates for each concept, e.g., “a photo\nof [v] at the beach”, similar to [3]. Thus, the dataset includes\n2,610=(102+159)×10 sketch-text pairs (see Supp) for eval-\nuation.\nMetrics. We utilize prompt similarity, identity similar-\nity, and perceptual distance as evaluation metrics. Follow-\ning the prior work [3], the prompt similarity assesses the\ndistance between a text prompt and the corresponding pro-\nduced images using CLIP model [43]. For computing, the\nlearned token [v] in the prompt is replaced with its class,\ne.g., “a [v] in the office” is modified to “a woman in the\noffice”. The identity similarity measures how the method\npreserves the object identity of the original image when the\ncontext by text or the structure by sketch is changed. We\ncompute the metrics via DINO [9] features as Ruiz et al.\n[48] did. Additionally, we evaluate the perceptual distance\nvia the LPIPS metric [71] for the reconstruction error re-\ngarding appearance and geometry between the ground truth\nand the generated images given the traced sketches. For\nidentity similarity and perceptual similarity, we adopt the\nmasked version of the results and ground truth to focus on\nthe foreground parts depicted by sketches. Note that we\nevaluate prompt and identity similarity on all the sketch-\ntext pairs while computing perceptual similarity only on the\ntraced sketches with their paired images.\n4.1. Comparison\nTo our knowledge, we are the first work to extract sketch\nconcepts for image synthesis and editing. To fairly compare\nour method with the existing personalization techniques, we\nadapt two methods, TI [21] and DB [48], to fit our proposed\ntask by introducing a pre-trained sketch encoder [36] into\ntheir methods when training and testing. Note that we do\nnot optimize the weights of the encoder for the two meth-\nods to keep their method intact mostly, and we thus only use\na single masked encoder to preserve the pre-trained prior.\nThe two methods receive the dual-sketch representation en-\ncoded in one map (i.e., 255 for SC and 127 for SD) with a\n6\nTable 1. Quantitative comparisons for diverse methods.\nMethod\nPrompt ↑\nIdentity ↑\nPerceptual ↓\nDB-E\n0.641\n0.889\n0.182\nTI-E\n0.642\n0.867\n0.214\nMC-E\n0.633\n0.884\n0.16\nSingle-sketch\n0.622\n0.908\n0.146\nw/o Lshape\n0.639\n0.906\n0.150\nw/o Lreg\n0.618\n0.909\n0.142\nw/o Masked F\n0.620\n0.911\n0.141\nOurs\n0.632\n0.912\n0.134\nmask to have the same inputs as ours. Besides the tuning-\nbased methods, we also compare our method with a tuning-\nfree method, MasaCtrl [8], which can work for sketch-based\nediting. We directly adopt their released code that integrates\nthe sketch encoder [36] for comparison. All the compared\nmethods are based on SD v1.5 [47]. For simplicity, we refer\nto the three baselines as TI-E, DB-E, and MC-E.\nFigure 4 shows a qualitative comparison between our\nmethod and the baselines. Although the editing results by\nthe three baselines are generally faithful to the structure\nof the edited sketches, they could not preserve the iden-\ntity or style of the objects/subjects in the original images.\nSpecifically, DB-E can reconstruct the original images with\nsketches generally (see Supp), but when editing, it often\nloses the details depicted by the edited sketch and the cor-\nrespondence between the sketch and target concept defined\nby the training sketch-image pairs. TI-E cannot recover the\noriginal identity in both reconstruction and editing since it\nmerely optimizes high-level text embedding. MC-E tends\nto drift the result’s style from the original one. It is because\na) MC adopts a pre-trained sketch encoder with domain bias\nas discussed in Sec. 1, and thus it could not work well for\nnovel sketch concept; b) this training-free method edits a\nreal image by inverting it to a latent space to leverage the\ngenerative prior of a T2I model, but there is a domain gap\nbetween the generated images and real images. Our method\noutperforms the three baselines and maintains the original\nidentity and the sketch-image correspondence defined in the\nsketch concept.\nTable 1 presents the quantitative evaluation results in the\nthree metrics. It demonstrates our method achieves the best\nidentity preservation (identity similarity) and reconstruction\nquality (perceptual distance). However, our method sacri-\nfices slightly the prompt similarity since we focus on the re-\nconstruction of the foreground object with Lshape (see the\nablation study without Lshape). Such sacrifice is acceptable\nto trade off the concept re-creation and sketch faithfulness,\nas shown in Figures 4 and 5.\nPerceptive User Study. We performed a perceptive user\nstudy including two evaluations: text editability study and\nsketch editability study. We first prepared a subset (30 ran-\n(a) Text Editability\n(b) Sketch Editability\nFigure 5.\nBox plots of the ratings in the perceptive user study.\nEach value above the median line is the average rate for each\nmethod. The higher, the better.\ndomly picked concepts, 15 for prompt similarity, 15 for\nsketch faithfulness) of our collected dataset. For text ed-\nitability, we produced the results by the three baselines and\nour method given a traced sketch and a prompt randomly\npicked from one concept. A participant was given a refer-\nence image, a prompt (e.g., “a photo of the boots in the ref-\nerence image in the snow”), and the four generated results\nin random order. We asked the participants to rate “How\nthe result is consistent with the prompt” on a Likert scale\nof 1–5 (the higher, the better). For sketch editability, we\npresented each participant with a reference image with the\ntraced sketch, an edited sketch, and four results (in random\norder) and required them to rate “How the result is faithful\nto the edited sketch and consistent with the reference iden-\ntity”. From 40 participants, we received 600 responses for\neach method in each evaluation. As shown in Figure 5, the\nuser study reflects the superiority of our method to the base-\nlines in both evaluations.\n4.2. Ablation Study\nWe ablated one of the key settings of our method to validate\ntheir effectiveness, including 1) w/ single-sketch represen-\ntation; 2) w/o shape loss Lshape; 3) w/o regularization loss\nLreg; 4) w/o masked encoder F. As shown in Figure 6, us-\ning the single-sketch representation could not provide suf-\nficient constraints on shapes (e.g., the castle and bear toy)\nand details (e.g., the woman’s clothes), damaging the iden-\ntity preservation. Removing Lshape would produce redun-\ndant parts and weaken the concept reconstruction. Without\nLreg, the method would overfit to the original shape and\nworsen the sketch editability (see Figures 6 & 7). Addition-\nally, removing either Lreg or the masked F would affect a\nlot the text editability for background, shown as Figure 7.\nIt is because Lreg can prevent the global embedding from\nenlarging significantly to outweigh the background token,\nwhile the masked F can filter out the local background fea-\ntures from the empty region of the sketch. The quantitative\nresults in Table 1 further confirm the above conclusions.\n7\nSingle-sketch\nOurs\nTraining Image\nTraining Sketch\nEdited Sketch\nw/o  \nw/o  \nw/o Masked \nFigure 6. Comparisons of our results and those by the ablated variants, given the text prompt “A photo of [v] floating on top of water”.\nw/o  \nw/o Masked \nOurs\n\"...at the beach\"\n\"... in the office\"\n\"... with a mountain\nin background\"\n\"... in the snow\"\nTraining Data &\nEdited Sketch\nFigure 7. Comparisons of the results by ours and the ablated vari-\nants using one edited sketch and diverse prompts indicating differ-\nent contexts. The prefix of the prompt is “A photo of [v] ...”.\n4.3. Applications\nWe implemented four applications enabled by our method:\nlocal editing, concept transfer, multi-concept generation,\nand text-based style variation. We showcase the applica-\ntions in Figure 8 to demonstrate the effectiveness and ver-\nsatility of CustomSketching. Please refer to Supp for the\nimplementation details for each application.\nLocal Editing. After extracting a sketch concept from\nreference image(s), we can perform local editing on the\noriginal images, including modification, addition, and re-\nmoval. To keep the unedited region intact, we incorporate\nour method with an off-the-shelf local editing method by\nAvrahami et al. [4]. Users can edit the training sketch and\nprovide a mask for the region they want to manipulate for\nfine-grained local editing (see Figure 8 (a)).\nConcept Transfer. Given different concepts separately\nlearned from the corresponding sketch-image pairs, our\nmethod can transfer between the concepts ([Si]={[vi], Fi})\nwith similar semantics via sketches. Figure 8 (b) shows an\nexample of hairstyle transfer. Note that we also resort to [4]\nfor local transfer.\nMulti-concept Generation. For multi-concept gener-\nation, prior works [3, 31] need to fine-tune the model on\nall the concepts desired in generation jointly. Unlike these\nworks, which optimize the entire denoising network, we\nonly optimize [v] and F for one concept. This lightweight\nsetting enables our method to achieve plug-and-play multi-\nconcept generation by separately learning each concept and\nthen combining them freely without extra optimization.\nFigure 8 (c) presents two cases of the combinations among\nthree extracted sketch concepts ([S1], [S2], [S3]).\nText-based Style Variation.\nOur method decouples\nglobal semantics and local features of a reference image\nto a textual token [v] and a sketch encoder F. Thus, our\nmethod can be used to produce diverse style variations of\nthe target object while preserving its geometry (shape and\ndetails), as shown in Figure 8 (d). To this end, our method\ntakes as input the sketch (regarded as an intermediate rep-\nresentation of object geometry) and a style prompt with-\nout [v] (e.g., “a crayon drawing”) to control the target style.\nWe compare our method with PnP [54], a text-based image-\nto-image translation method, by feeding a masked image\nwith the style prompt to this method. Thanks to the given\nsketch, our method can better disentangle the geometry and\nstyle, thus offering more user controllability and flexibility\nvia sketching.\n8\n\"      and      \"\n\"      and      \"\n(a) Local Editing\n(b) Concept Transfer\nPnP\nOurs\n(c) Multi-concept Generation\n(d) Style Variation\n\"made in wooden\"\nInput\n\"golden 3D\nrendering style\"\n\"a crayon drawing\"\n\"flat cartoon\nillustration style\"\nTraining Data\nEditing Results\nFigure 8. Four applications enabled by our CustomSketching. For\n(c), the template of the prompt is “A photo of [Si] in an office”.\nPrompt: \"a photo of [v] in the jungle\"\nFigure 9. One failure case of our method. Our method could not\nchange the car’s tiny details by sketching thin strokes.\n5. Conclusion and Discussion\nWe proposed CustomSketching, a novel approach to extract\nsketch concepts for sketch-based image synthesis and edit-\ning based on a large T2I model. This method decouples\nreference image(s) into global semantics in a textual token\nand local features in two sketch encoders. We presented\na dual-sketch representation to differentiate the shape and\ndetails of one concept. In this way, our method empow-\ners users with high controllability in local and fine-grained\nimage editing. Extensive experiments and several applica-\ntions have shown the effectiveness and superiority of our\nproposed method to the alternative solutions. We will re-\nlease the dataset and code to the research community.\nWhile our method improves the controllability and flex-\nibility of the personalization task, it has several limitations.\nFirst, inherited from latent diffusion models, our method\nprocesses images in a low-resolution latent space (64×64).\nIt thus struggles to control an object’s tiny shape and details\nby sketching thin strokes. As shown in Figure 9, the car’s\ndetails could not be changed following the edited sketch.\nAnother limitation is the learning efficiency. Currently, our\nmethod requires almost 30 mins to learn one concept for\ntwo-stage optimization. In the future, we may use fast per-\nsonalization techniques [23, 28] to address this issue.\nReferences\n[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy Mitra, and\nPeter Wonka.\nClip2stylegan: Unsupervised extraction of\nstylegan edit directions. In ACM SIGGRAPH 2022 confer-\nence proceedings, pages 1–9, 2022. 3\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended\ndiffusion for text-driven editing of natural images. In CVPR,\npages 18208–18218, 2022. 3\n[3] Omri Avrahami, Kfir Aberman, Ohad Fried, Daniel Cohen-\nOr, and Dani Lischinski.\nBreak-a-scene:\nExtracting\nmultiple concepts from a single image.\narXiv preprint\narXiv:2305.16311, 2023. 2, 3, 4, 5, 6, 8, 13\n[4] Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended\nlatent diffusion. ACM TOG, 42(4):1–11, 2023. 3, 8, 15\n[5] David Bau, Alex Andonian, Audrey Cui, YeonHwan Park,\nAli Jahanian, Aude Oliva, and Antonio Torralba. Paint by\nword. arXiv preprint arXiv:2103.10951, 2021. 3\n[6] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis.\nIn ICLR, 2018. 3\n[7] Tim Brooks, Aleksander Holynski, and Alexei A Efros. In-\nstructpix2pix: Learning to follow image editing instructions.\nIn CVPR, pages 18392–18402, 2023. 3\n[8] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xi-\naohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mu-\ntual self-attention control for consistent image synthesis and\nediting. In ICCV, pages 22560–22570, 2023. 3, 7, 14\n[9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,\nJulien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-\ning properties in self-supervised vision transformers.\nIn\nICCV, pages 9650–9660, 2021. 6\n[10] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and\nDaniel Cohen-Or.\nAttend-and-excite: Attention-based se-\nmantic guidance for text-to-image diffusion models. ACM\nTOG, 42(4):1–10, 2023. 5\n[11] Shu-Yu Chen, Wanchao Su, Lin Gao, Shihong Xia, and\nHongbo Fu. Deepfacedrawing: Deep generation of face im-\nages from sketches. ACM TOG, 39(4):72–1, 2020. 3\n9\n[12] Shu-Yu Chen, Feng-Lin Liu, Yu-Kun Lai, Paul L Rosin,\nChunpeng Li, Hongbo Fu, and Lin Gao. Deepfaceediting:\ndeep face generation and editing with disentangled geome-\ntry and appearance control. ACM TOG, 40(4):1–15, 2021.\n3\n[13] Wengling Chen and James Hays. Sketchygan: Towards di-\nverse and realistic sketch to image synthesis. In CVPR, pages\n9416–9425, 2018. 3\n[14] Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui\nJia, Ming-Wei Chang, and William W Cohen. Subject-driven\ntext-to-image generation via apprenticeship learning. 2023.\n3\n[15] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao,\nand Hengshuang Zhao. Anydoor: Zero-shot object-level im-\nage customization. arXiv preprint arXiv:2307.09481, 2023.\n2, 3\n[16] Shin-I Cheng, Yu-Jie Chen, Wei-Chen Chiu, Hung-Yu\nTseng, and Hsin-Ying Lee. Adaptively-realistic image gen-\neration from stroke and sketch with diffusion model. In Pro-\nceedings of the IEEE/CVF Winter Conference on Applica-\ntions of Computer Vision, pages 4054–4062, 2023. 3\n[17] Kyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bahdanau,\nand Yoshua Bengio. On the properties of neural machine\ntranslation: Encoder-decoder approaches.\narXiv preprint\narXiv:1409.1259, 2014. 3\n[18] Guillaume Couairon, Jakob Verbeek, Holger Schwenk, and\nMatthieu Cord. Diffedit: Diffusion-based semantic image\nediting with mask guidance. In ICLR, 2022. 3\n[19] Katherine Crowson,\nStella\nBiderman,\nDaniel Kornis,\nDashiell Stander, Eric Hallahan, Louis Castricato, and Ed-\nward Raff. Vqgan-clip: Open domain image generation and\nediting with natural language guidance. In ECCV, pages 88–\n105. Springer, 2022. 3\n[20] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. Advances in neural informa-\ntion processing systems, 34:8780–8794, 2021. 3\n[21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,\nAmit Haim Bermano, Gal Chechik, and Daniel Cohen-or.\nAn image is worth one word: Personalizing text-to-image\ngeneration using textual inversion. In ICLR, 2022. 2, 3, 4, 6,\n13\n[22] Rinon Gal, Or Patashnik, Haggai Maron, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Stylegan-nada: Clip-\nguided domain adaptation of image generators. ACM TOG,\n41(4):1–13, 2022. 3\n[23] Rinon Gal, Moab Arar, Yuval Atzmon, Amit H Bermano,\nGal Chechik, and Daniel Cohen-Or. Encoder-based domain\ntuning for fast personalization of text-to-image models. ACM\nTOG, 42(4):1–13, 2023. 3, 9\n[24] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 3\n[25] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-or. Prompt-to-prompt image\nediting with cross-attention control. In ICLR, 2022. 3, 5, 13\n[26] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. NeurIPS, 33:6840–6851, 2020.\n3\n[27] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term\nmemory. Neural computation, 9(8):1735–1780, 1997. 3\n[28] Xuhui Jia, Yang Zhao, Kelvin CK Chan, Yandong Li, Han\nZhang, Boqing Gong, Tingbo Hou, Huisheng Wang, and\nYu-Chuan Su. Taming encoder for zero fine-tuning image\ncustomization with text-to-image diffusion models.\narXiv\npreprint arXiv:2304.02642, 2023. 3, 9\n[29] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, pages 4401–4410, 2019. 3\n[30] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen\nChang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic:\nText-based real image editing with diffusion models.\nIn\nCVPR, pages 6007–6017, 2023. 3\n[31] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli\nShechtman, and Jun-Yan Zhu.\nMulti-concept customiza-\ntion of text-to-image diffusion. In CVPR, pages 1931–1941,\n2023. 2, 3, 6, 8\n[32] Feng-Lin Liu, Shu-Yu Chen, Yukun Lai, Chunpeng Li, Yue-\nRen Jiang, Hongbo Fu, and Lin Gao. Deepfacevideoediting:\nsketch-based deep editing of face videos. ACM TOG, 41(4):\n167, 2022. 3\n[33] Yifang Men, Yiming Mao, Yuning Jiang, Wei-Ying Ma,\nand Zhouhui Lian.\nControllable person image synthesis\nwith attribute-decomposed gan. In CVPR, pages 5084–5093,\n2020. 3\n[34] Ron Mokady, Omer Tov, Michal Yarom, Oran Lang, Inbar\nMosseri, Tali Dekel, Daniel Cohen-Or, and Michal Irani.\nSelf-distilled stylegan: Towards generation from internet\nphotos. In ACM SIGGRAPH 2022 Conference Proceedings,\npages 1–9, 2022. 3\n[35] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and\nDaniel Cohen-Or. Null-text inversion for editing real images\nusing guided diffusion models. In CVPR, pages 6038–6047,\n2023. 3\n[36] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2,\n3, 4, 5, 6, 7, 13\n[37] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh,\nPranav Shyam,\nPamela Mishkin,\nBob Mcgrew,\nIlya\nSutskever, and Mark Chen.\nGlide: Towards photorealis-\ntic image generation and editing with text-guided diffusion\nmodels. In International Conference on Machine Learning,\npages 16784–16804. PMLR, 2022. 3\n[38] Yotam Nitzan, Kfir Aberman, Qiurui He, Orly Liba, Michal\nYarom, Yossi Gandelsman, Inbar Mosseri, Yael Pritch, and\nDaniel Cohen-Or. Mystyle: A personalized generative prior.\nACM TOG, 41(6):1–10, 2022. 3\n[39] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or,\nand Dani Lischinski. Styleclip: Text-driven manipulation of\nstylegan imagery. In ICCV, pages 2085–2094, 2021. 3\n10\n[40] Or Patashnik, Daniel Garibi, Idan Azuri, Hadar Averbuch-\nElor, and Daniel Cohen-Or. Localizing object-level shape\nvariations with text-to-image diffusion models. 2023. 3\n[41] Yichen Peng, Chunqi Zhao, Haoran Xie, Tsukasa Fukusato,\nand Kazunori Miyata.\nDifffacesketch: High-fidelity face\nimage synthesis with sketch-guided latent diffusion model.\narXiv preprint arXiv:2302.06908, 2023. 3\n[42] Tiziano Portenier, Qiyang Hu, Attila Szab´o, Siavash Ar-\njomand Bigdeli, Paolo Favaro, and Matthias Zwicker.\nFaceshop: deep sketch-based face image editing.\nACM\nTransactions on Graphics (TOG), 37(4):1–13, 2018. 3\n[43] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 3, 6\n[44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In ICML, pages 8821–\n8831. PMLR, 2021. 2, 3\n[45] Scott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Lo-\ngeswaran, Bernt Schiele, and Honglak Lee. Generative ad-\nversarial text to image synthesis. In International conference\non machine learning, pages 1060–1069. PMLR, 2016. 3\n[46] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan,\nYaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding\nin style: a stylegan encoder for image-to-image translation.\nIn CVPR, pages 2287–2296, 2021. 3\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684–\n10695, 2022. 2, 3, 7, 13\n[48] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration. In CVPR, pages 22500–22510, 2023. 2, 3, 6,\n13\n[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding. NIPS, 35:36479–36494, 2022. 2\n[50] Patsorn Sangkloy, Jingwan Lu, Chen Fang, Fisher Yu, and\nJames Hays. Scribbler: Controlling deep image synthesis\nwith sketch and color. In CVPR, pages 5400–5409, 2017. 3\n[51] Jing Shi, Wei Xiong, Zhe Lin, and Hyun Joon Jung. Instant-\nbooth: Personalized text-to-image generation without test-\ntime finetuning. arXiv preprint arXiv:2304.03411, 2023. 3\n[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 3\n[53] Yang Song and Stefano Ermon. Generative modeling by esti-\nmating gradients of the data distribution. NeurIPS, 32, 2019.\n3\n[54] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation.\nIn CVPR, pages 1921–1930,\n2023. 8, 16, 18\n[55] Dani Valevski, Matan Kalman, Eyal Molad, Eyal Segalis,\nYossi Matias, and Yaniv Leviathan. Unitune: Text-driven\nimage editing by fine tuning a diffusion model on a single\nimage. ACM TOG, 42(4):1–10, 2023. 3\n[56] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. NIPS, 30, 2017. 3\n[57] Yael Vinker, Eliahu Horwitz, Nir Zabari, and Yedid Hoshen.\nImage shape manipulation from a single augmented training\nsample. In ICCV, pages 13769–13778, 2021. 3\n[58] Andrey Voynov, Kfir Aberman, and Daniel Cohen-Or.\nSketch-guided text-to-image diffusion models. In ACM SIG-\nGRAPH 2023 Conference Proceedings, pages 1–11, 2023.\n3\n[59] Qiang Wang, Di Kong, Fengyin Lin, and Yonggang Qi.\nDiffsketching: Sketch control image synthesis with diffusion\nmodels. 2023. 3\n[60] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-\nTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah\nLaszlo, David J Fleet, Radu Soricut, et al. Imagen editor\nand editbench: Advancing and evaluating text-guided image\ninpainting. In CVPR, pages 18359–18369, 2023. 3\n[61] Yuxiang Wei, Yabo Zhang, Zhilong Ji, Jinfeng Bai, Lei\nZhang, and Wangmeng Zuo. Elite: Encoding visual con-\ncepts into textual embeddings for customized text-to-image\ngeneration. 2023. 3\n[62] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu.\nTedigan: Text-guided diverse face image generation and ma-\nnipulation. In CVPR, pages 2256–2265, 2021. 3\n[63] Chufeng Xiao, Deng Yu, Xiaoguang Han, Youyi Zheng, and\nHongbo Fu. Sketchhairsalon: deep sketch-based hair image\nsynthesis. ACM TOG, 40(6):1–16, 2021. 3, 6\n[64] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang,\nZhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-\ngrained text to image generation with attentional generative\nadversarial networks. In CVPR, pages 1316–1324, 2018. 3\n[65] Yiwen Xu, Ruoyu Guo, Maurice Pagnucco, and Yang Song.\nDraw2edit: Mask-free sketch-guided image manipulation. In\nProceedings of the 31st ACM International Conference on\nMultimedia, pages 7205–7215, 2023. 3\n[66] Yu Zeng, Zhe Lin, and Vishal M Patel. Sketchedit: Mask-\nfree local image manipulation with partial sketches. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 5951–5961, 2022. 3\n[67] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks. In ICCV, pages 5907–5915,\n2017. 3\n[68] Han Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiao-\ngang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stack-\ngan++: Realistic image synthesis with stacked generative ad-\nversarial networks. IEEE TPAMI, 41(8):1947–1962, 2018.\n[69] Han Zhang, Jing Yu Koh, Jason Baldridge, Honglak Lee, and\nYinfei Yang. Cross-modal contrastive learning for text-to-\nimage generation. In CVPR, pages 833–842, 2021. 3\n11\n[70] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nICCV, pages 3836–3847, 2023. 2, 3, 4, 14\n[71] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,\nand Oliver Wang. The unreasonable effectiveness of deep\nfeatures as a perceptual metric. In CVPR, pages 586–595,\n2018. 6\n[72] Yuechen Zhang, Jinbo Xing, Eric Lo, and Jiaya Jia. Real-\nworld image variation by aligning diffusion inversion chain.\nNIPS, 2023. 14\n[73] Changqing Zou, Haoran Mo, Chengying Gao, Ruofei Du,\nand Hongbo Fu.\nLanguage-based colorization of scene\nsketches. ACM TOG, 38(6):1–16, 2019. 3\n12\nCustomSketching:\nSketch Concept Extraction for Sketch-based Image Synthesis and Editing\nSupplementary Material\n6. Dataset\nFigure 19 shows a thumbnail of our created dataset, cov-\nering diverse object categories. For each object regarded\nas one concept, we invited three normal users without any\nprofessional training in drawing to trace separate contour\nlines SC and details lines SD over the reference images.\nOne training sketch traced over an image generally cost 30s-\n2min for an amateur, while a testing one cost less than 1min.\nThe sketch-image pairs are with purple borders in Figure\n19. Note that each concept has 1-6 image-sketch pair(s) for\ntraining, where the concepts of human portrait and clothing\nonly have a single pair. Then, the users were asked to create\n3-5 edited dual sketches (with yellow borders in Figure 19)\ninitialized from one of the traced sketches or drawn from\nscratch. In this way, we created the concepts with different\nfine-grained attributes (shape, pose, details) from the refer-\nence images, represented by the edited sketches. For each\ntraced or edited sketch, we used a polygon filling method\n(implemented via OpenCV v3) to automatically generate a\nforeground mask following SC. The automatically gener-\nated masks were generally accurate but the annotators were\nallowed to manually refine the masks if necessary. Finally,\nwe obtained 35 groups of concept data, with 102 traced\nsketches with paired images, 159 edited sketches, as well\nas foreground masks corresponding to both sketches. Sim-\nilar to [3], we set up ten prompt templates with the learned\ntextual token [v] as follows:\n• “A photo of [v] at the beach”\n• “A photo of [v] in the jungle”\n• “A photo of [v] in the snow”\n• “A photo of [v] in the street”\n• “A photo of [v] on top of a wooden floor”\n• “A photo of [v] with a city in the background”\n• “A photo of [v] with a mountain in the background”\n• “A photo of [v] with the Eiffel tower in the back-\nground”\n• “A photo of [v] floating on top of water”\n• “A photo of [v] in an office”\nTherefore, we have 2,610 = (102+159)×10 sketch-text\npairs for evaluation.\n7. Implementation Details\nOur method and all the compared baselines were based\non Stable Diffusion v1.5 [47]. A training image and its\ncorresponding sketch were both resized to 512×512. The\nsketch features extracted from a sketch encoder F were in-\njected into four layers of the encoder of the denoising U-\nnet, with resolutions of 64, 32, 16, and 8, following the\nsettings of [36]. For the optimization of Stage I, we only\nfine-tuned a newly added textual token [v] with a learn-\ning rate of 5e−4. The token was initialized using the class\nname of the target concept, e.g., “toy” for the toy ob-\nject. The sketch encoder for Stage I is a pre-trained model\n(t2iadapter sketch sd15v2) from [36] with frozen weights\nduring optimization. For Stage II, we jointly optimized the\ntoken [v] and two sketch encoders with a small learning\nrate of 2e−6, similar to [3]. The weights of the two sketch\nencoders were initialized with those of the pre-trained one\n[36] used in Stage I. During training, a text prompt as in-\nput was randomly selected from the list of text templates\nin [21], while during testing, the prompt was picked from\nour created dataset.\nEmpirically, we trained each stage\nin our experiment for 400 steps (batch size=16) using the\nAdam solver via the PyTorch framework.\nWe randomly\naugmented (with the probability of 0.5) the training data\nby translating each sketch-image pair in the range of [-\n0.2,0.2], rotating it in the range of [-45°,45°], and horizon-\ntal flip. We trained and tested our method CustomSketch-\ning on a PC with Intel i9-13900K, 128GB RAM, and a sin-\ngle NVIDIA GeForce RTX 4090. The two-stage optimiza-\ntion took around 30 mins, while one pass inference (DDPM\nsampling with 50 steps) cost around 3s.\nWe used cross-attention maps in each layer of the denois-\ning U-Net to compute shape loss Lshape. Following Hertz\net al. [25], we combined and averaged all the cross-attention\nmaps Aθ(zt, v) of the token [v]. The different layers of\nthe attention maps with diverse resolutions were resized to\n16 × 16 for computation.\n8. Experiments\nComparisons with SOTAs. In the main text, we adapted\nDB [48] and TI [21] with a pre-trained sketch encoder [36]\nto fit the task of sketch concept extraction, refer to DB-E\nand TI-E. Since DB learned a novel concept by binding a\nunique identifier (e.g., “sks”) with a specific subject in a\ntext prompt, we provided a text prompt like “a photo of a\nsks toy” for the toy category for training and testing. Note\nthat the weights of the sketch encoder in DB-E and TI-E\nwere frozen to keep the two methods intact mostly. In the\nSupp, we further adapted DB and TI with two learnable\nsketch encoders fed with the dual-sketch representation as\nours did, respectively referring to DB-FE and TI-FE. Con-\nsidering vanilla DB might have enough capacity to learn a\n13\nOurs\nTraining Data\nDB/E (     )\nDB/E (   )\nOurs\nTraining Data\nDB/E (     )\nDB/E (   )\nPrompt: \"a photo of [v] on top of a wooden floor\"\nFigure 10. Qualitative comparison between ours and vanilla DB with a pre-trained sketch T2I-adapter (DB/E). The results show DB/E fails\nin correctly reconstruction and editing the reference image.\nconcept without sketch condition, we also separately com-\npared our method with vanilla DB (denoted as DB/E), i.e.,\ntraining vanilla DB for one concept and testing it with a pre-\ntrained T2I-adapter (without fine-tuning). Fig. 10 shows\ntwo evaluation results on the sketch with only SC (DB/E\n(SC)) and the sketch with both types (DB/E (S)). It can be\neasily found that DB/E fails to correctly reconstruct the con-\ncept without sufficient sketch constraint and edit the concept\nusing detail strokes due to the domain gap existing in the\npre-trained sketch encoder. The above tuning-based meth-\nods (DB/E, DB-FE, DB-E, TI-FE, TI-E) had the same train-\ning parameters and augmentation tricks as ours.\nFor tuning-free methods, we compared our method with\nMS-E [8] in the main text, but we found it often drifted the\noriginal style of the reference images due to the gap be-\ntween the generated images and real images. A follow-up\nwork, RIVAL [72], was proposed to alleviate such a gap.\nRIVAL employed a pre-trained ControlNet [70] to enable\nsketch-based editing for real images. We also compared\nour method with the sketch-based version of RIVAL (de-\nnoted as RIVAL-E) by directly using their released code.\nThe tuning-based methods consist of an inversion step and\nan inference step. For the inversion step, we provided a\nreference image with the traced sketch and a text prompt\n(e.g., “a photo of a toy” for the toy category), while for\nthe inference step, we provided an edited sketch with a tar-\nget prompt (e.g., “a photo of a toy in the snow”). Note\nthat for the tuning-free methods, we used the single-sketch\nrepresentation for the sketch input to make the method com-\npatible with the prior of the pre-trained sketch encoder. We\nused the same random seed (seed=42) for our method and\nall the above baselines during inference.\nFigure 20 shows more qualitative comparisons.\nIt\ndemonstrates that our method performs better in sketch- and\ntext-based editing while preserving the annotated object’s\noriginal identity compared to all the baselines. DB-FE, TI-\nTable 2. Quantitative comparisons for diverse methods.\nMethod\nPrompt ↑\nIdentity ↑\nPerceptual ↓\nDB/E (SC)\n0.647\n0.868\n0.202\nDB/E (S)\n0.647\n0.870\n0.196\nDB-FE\n0.642\n0.879\n0.192\nDB-E\n0.641\n0.889\n0.182\nTI-FE\n0.634\n0.906\n0.165\nTI-E\n0.642\n0.867\n0.214\nRIVAL-E\n0.627\n0.899\n0.151\nMS-E\n0.633\n0.884\n0.16\nSingle-encoder\n0.623\n0.910\n0.142\nSingle-sketch\n0.622\n0.908\n0.146\nw/o Lshape\n0.639\n0.906\n0.150\nw/o Lreg\n0.618\n0.909\n0.142\nw/o Masked F\n0.620\n0.911\n0.141\nw/o Stage I\n0.632\n0.904\n0.164\nOurs\n0.632\n0.912\n0.134\nFE, and RIVAL-E can improve the reconstruction quality\na little in appearance and geometry, respectively compared\nto DB-E, TI-E, and MS-E. However, the three methods still\ncould not achieve satisfactory editing results. The quanti-\ntative results could also reflect such a tendency (see Table\n2).\nAblation Study. Figure 21 shows more results for com-\nparisons between our method and the ablated ones men-\ntioned in the main text. We show two more ablated vari-\nants here: 1) adopting a single encoder in Stage II with the\ndual-sketch representation, i.e., merging SC and SD into\none sketch map as input; 2) w/o Stage I, i.e., only jointly op-\ntimizing a newly added token and the two sketch encoders.\nAs shown in Figure 21 and Table 2, the single-encoder set-\nting could not effectively differentiate shape and details,\nthus causing worse sketch faithfulness and identity preser-\nvation than ours. Removing Stage I results in unsatisfactory\n14\nTraining Data Other-concept Sketches\nLow-quality Sketches\nTraining Data\nLow-quality Sketches\nPrompt: \"a photo of [v]\"\nOther-concept Sketches\nFigure 11. Diverse results given different sketches with the same text prompt and random seed.\nPrompt: \"a photo of [v] floating on top of water\"\nSeed = 0\n100\n200\n300\n400\n500\n600\n700\n800\n900\nTraining & Editing Input\nFigure 12. Multiple random seeds with the same text and sketch for sampling diverse results.\nreconstruction since the setting would mislead the optimiza-\ntion in disentangling the global semantics into [v] and local\nfeatures into F. Table 2 also confirms such a conclusion\n(see the identity similarity and perceptual distance).\nRobustness Evaluation. We show the robustness of our\nmethod from two aspects: 1) Inputing sketches differ-\nent from the training samples. Our method can effec-\ntively avoid the T2I-adapter overfitting on given sketches\nthanks to our optimization settings, thus tolerating sketches\ndifferent from the training data. This is why our method\ncan be successfully applied to concept transfer (see Main-\ntext Figure 8 & Figure 16). Figure 11 shows more results\ngiven two cases of different sketches, i.e., sketches from\nother concepts and low-quality sketches. 2) Multiple Ran-\ndom Seeds. We show diverse results given multiple random\nseeds with the same text and sketch (Figure 12). Since the\nforeground object is conditioned on the text and sketch, de-\nnoising with different seeds mainly varies the background\ngeneration, and our method can perform stable to make sure\nthe foreground object is always faithful to the sketch given\ndiverse seeds.\n9. Applications\nWe implemented four applications enabled by our CustomS-\nketching. Below, we show more results and the implemen-\ntation details.\nLocal Editing. Incorporating [4], our method can be\napplied to local image editing, which allows users to edit\na local region of a given real image via sketching while\nkeeping the unedited region intact. Figure 14 shows the\npipeline of such an application. After extracting a novel\nconcept [S]={[v], F} given reference sketch-image pair(s),\nusers can provide a blending mask MB and a part sketch\ninside the mask to indicate an editing input.\nThen, our\nmethod blends the local sketch with the original sketch to\nbe an edited sketch S fed into the learned dual-encoder F.\nGiven the extracted sketch feature and a prompt “a photo\nof a [v]”, the denoising U-Net produces a foreground latent,\nwhich is blended with the background latent inverted from\nthe original image via MB, to achieve the final editing re-\nsult. The two latents are blended during all the inference\ntime steps (T=50). Figure 15 presents more local editing\nresults for human portrait manipulation (Top) and virtual\ntry-on/clothing design (Bottom).\nConcept Transfer. Our method can transfer the learned\nconcepts locally or globally to a target object with similar\nsemantics, as shown in Figure 16. Similar to the pipeline\nof local editing (Figure 14), users may provide an editing\ninput to indicate local shape or structure to transfer a target\nconcept [S].\nMulti-concept Generation. Given a set of the extracted\nsketch concepts {Si}={[vi], Fi}, our method can directly\ncombine them without extra optimization. Figure 17 shows\nthe pipeline of multi-concept generation implemented by\nour method. Given an input sketch annotated with diverse\nconcepts, our method divides it into separate sketches fed\n15\n\"       and       and     \nin an office\"\n\"       and       with a mountain\nin the background\"\n\"       and       in the street\"\nFigure 13. Additional results of multi-concept generation enabled\nby our method. The prefix of the text prompt is “a photo of ...”.\ninto their corresponding dual-encoder Fi.\nThen, the ex-\ntracted features are masked respectively using Mi and then\naggregated together by summation, finally injected into the\npre-trained T2I diffusion model. The given prompt is in\nthe format of “[v1] and [v2] ... and [vi]” to cover multiple\nconcepts. Figure 13 shows more results of multi-concept\ngeneration.\nText-based Style Variation.\nOur method decouples\nglobal semantics and local features of a reference image\nto a textual token [v] and a sketch encoder F. Thus, our\nmethod can be used to produce diverse style variations of\nthe target object while preserving its geometry (shape and\ndetails), as shown in Figure 18. To this end, our method first\nextracts a concept [S]={[v], F} from sketch-image pair(s).\nThen, it takes as input the sketch (regarded as an inter-\nmediate representation of object geometry) fed to F and a\nstyle prompt without the learned [v] from the original image\n(e.g., “a crayon drawing”) to control the target style.\nWe\ncompared our method with PnP [54], a text-based image-to-\nimage translation method, by feeding a masked image (only\nwith a foreground object) to this method. PnP consists of\nan inversion step and an inference step. For comparison,\nwe provided an initial prompt (e.g., “a photo of a toy” for\nthe toy category) for inversion and a style prompt (e.g., “a\ncrayon drawing of a toy”) for inference to change the object\nstyle. Thanks to the given sketch, our method better dis-\nentangles the geometry and style, thus offering more user\ncontrollability and flexibility via sketching.\n16\nDual Sketch\nEncoder      \nImage\nEncoder\nDenoising U-Net\nDecoder\n  + Noise        \nOriginal Image\nOriginal Sketch\nEditing Input\nEdited Sketch\nBlending Mask\nEditing Result\nPrompt: \"a photo of a [v]\"\nFigure 14. The pipeline of local editing enabled by our method.\nTraining Sketch-image Pair\nOriginal Image\nEditing Input\nEditing Result\nOriginal Image\nEditing Input\nEditing Result\nFigure 15. Additional results of local editing enabled by our method. The top row is for human portrait manipulation (removing the glasses\nand changing the hair region), while the bottom row is for virtual try-on and clothing design.\nOriginal Sketch\n& Image\nTarget Concept\nEdited Sketch\nTransfer Result\nOriginal Sketch\n& Image\nTarget Concept\nEdited Sketch\nTransfer Result\nFigure 16. Additional results of concept transfer enabled by our method. The left half shows examples of local concept transfer for adding\na beard (Top) and adding a hair bun (Bottom). The right half shows examples of global concept transfer for changing the object semantics\nwhile preserving its shape and pose.\n17\n\"A photo of      and     \nin an office\"\nDual Sketch\nEncoder\nDual Sketch\nEncoder\nText-to-Image\nDIffusion Model\n(a) Learned Concepts\n(b) Inference of Multi-concept Generation\nInput Sketch\nResult\nFigure 17. The pipeline of multi-concept generation enabled by our method. Separately learning each concept (a), our method can directly\ncombine them for multi-concept generation during inference (b) without extra fine-tuning.\nPnP\nOurs\n\"made in wooden\"\nInput\n\"golden 3D\nrendering style\" \"a crayon drawing\"\n\"flat cartoon\nillustration style\"\n\"made in wooden\"\nInput\n\"golden 3D\nrendering style\" \"a crayon drawing\"\n\"flat cartoon\nillustration style\"\nFigure 18. Comparisons of the results by our method and PnP [54] for text-based style variation.\n18\nFigure 19. A thumbnail of our created dataset for training and testing. The pairs of reference images and the corresponding traced sketches\nare with purple borders, while the edited sketches are with yellow borders.\n19\nTraining Image Training Sketch Edited Sketch\nOurs\nDB-FE\nDB-E\nTI-FE\nRIVAL-E\nMS-E\nTI-E\n\"a photo of [v] with a city in the background\"\n\"a photo of [v] floating on top of water\"\n\"a photo of [v] on top of a wooden floor\"\n\"a photo of [v] at the beach\"\n\"a photo of [v] with the Eiffel tower in the background\"\n\"a photo of [v]with a mountain in the background\"\n\"a photo of [v] in the street\"\n\"a photo of [v] in the snow\"\n\"a photo of [v] in an office\"\n\"a photo of [v] in the jungle\"\n\"a photo of [v] at the beach\"\nFigure 20. Comparisons of the results generated by our method and the adapted state-of-the-art methods, given the same training data\n(sketch-image pairs in Columns 1 & 2), edited sketch (Column 3), and text prompt (at the bottom of each group of results).\n20\nTraining Image Training Sketch Edited Sketch\nOurs\nSingle-encoder\nSingle-sketch\nw/o \nw/o masked\nw/o Stage I\nw/o \n\"a photo of [v] with a city in the background\"\n\"a photo of [v] floating on top of water\"\n\"a photo of [v] on top of a wooden floor\"\n\"a photo of [v] at the beach\"\n\"a photo of [v] with the Eiffel tower in the background\"\n\"a photo of [v]with a mountain in the background\"\n\"a photo of [v] in the street\"\n\"a photo of [v] in the snow\"\n\"a photo of [v] in an office\"\n\"a photo of [v] in the jungle\"\nFigure 21. Comparisons of the results generated by our method and the ablated variants, given the same training data (sketch-image pairs\nin Columns 1 & 2), edited sketch (Column 3), and text prompt (at the bottom of each group of results).\n21\n"
}