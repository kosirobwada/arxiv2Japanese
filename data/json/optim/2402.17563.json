{
    "optim": "Structure-Guided Adversarial Training of Diffusion Models Ling Yang∗† Haotian Qian* Zhilong Zhang Jingwei Liu Bin Cui† Peking University yangling0818@163.com, zzdmqht@gmail.com, {zzl2018math, bin.cui}@pku.edu Abstract Diffusion models have demonstrated exceptional efficacy in various generative applications. While existing models focus on minimizing a weighted sum of denoising score matching losses for data distribution modeling, their train- ing primarily emphasizes instance-level optimization, over- looking valuable structural information within each mini- batch, indicative of pair-wise relationships among samples. To address this limitation, we introduce Structure-guided Adversarial training of Diffusion Models (SADM). In this pioneering approach, we compel the model to learn manifold structures between samples in each training batch. To ensure the model captures authentic manifold structures in the data distribution, we advocate adversarial training of the diffu- sion generator against a novel structure discriminator in a minimax game, distinguishing real manifold structures from the generated ones. SADM substantially improves existing diffusion transformers and outperforms existing methods in image generation and cross-domain fine-tuning tasks across 12 datasets, establishing a new state-of-the-art FID of 1.58 and 2.11 on ImageNet for class-conditional image genera- tion at resolutions of 256×256 and 512×512, respectively. 1. Introduction Diffusion models [19, 53–56, 69] have achieved remarkable generation quality in various tasks, including image genera- tion [11, 47–49, 63, 70, 71, 75], audio synthesis [4, 32, 46], and interdisciplinary applications [20, 23, 67]. Starting from tractable noise distribution, diffusion models generate data by progressively removing noise. This involves the model learning to reverse a pre-defined diffusion process that se- quentially introduces varying levels of noise to the data. The model is parameterized and undergoes training by optimizing the weighted sum of denoising score matching losses [19] for various noise levels [54], aiming to learn the recovery of clean images from corrupted images. *Contributed equally. †Corresponding authors. Noise Distribution Training data Distribution Our Structure-Guided Training Previous Instance-Level Training Figure 1. Comparison between previous instance-level training and our structure-guided training for diffusion models. Aiming to maximally model the data distribution, recent works [27, 39, 57, 60, 68, 72] attempt to improve the pre- cision of training diffusion models. For instance, many approaches design effective weighting schemes [7, 16, 29, 39, 57, 72] for maximum likelihood training or add an extra regularization term [9, 36, 43] to the denoising score loss. There are also some works to enhance the model expressive- ness by incorporating other generative models, such as VAE [30, 48, 60], GAN [65], and Normalizing Flows [28, 40, 74]. However, their improved training of diffusion models primar- ily concentrates on instance-level optimization, overlooking the valuable structural information among batch samples. This oversight is significant, as the incorporation of struc- tural details is crucial for aligning the learned distribution with the underlying data distribution. To mitigate the challenges posed by existing instance- level training methods, we propose Structure-guided Adversarial training of Diffusion Models (SADM). In con- trast to conventional instance-level training illustrated in Fig. 1, our approach guides diffusion training at a structural level. During batch training, the model is facilitated to learn manifold structures within batch samples, represented by pair-wise relationships in a low-dimensional feature space. To accurately learn real manifold structures in the data dis- tribution, we introduce a novel structure discriminator that distinguishes genuine manifold structures from generated arXiv:2402.17563v1  [cs.CV]  27 Feb 2024 ADM-G ADM + Structure-guided Adversarial Training FID: 4.59, Recall: 0.52 FID: 3.14, Recall: 0.61 Ground Truth Figure 2. Generated samples on ImageNet 256 × 256 with (i) ADM with Classifier Guidance (ADM-G) [11], (ii) ADM optimized by our Structure-guided Adversarial Training, and (iii) real samples in ground truth classes. We can significantly improve diffusion models qualitatively and quantitatively, and our generated sample distribution is overally more similar to real sample distribution. See Appendix C for more synthesis samples of our SOTA model. ones. For clarity, we alternatively refer to the diffusion mod- els optimized through our structure-guided training as joint sample diffusion, a concept theoretically proven to enhance diffusion model optimization. We assess the performance of our model across two pivotal tasks: image generation and cross-domain fine- tuning. The former involves training the diffusion model from scratch to evaluate its capability in capturing the en- tire data distribution. The latter leverages a pre-trained dif- fusion model, constructed on a large-scale source dataset, and fine-tunes it on a target dataset to assess transferabil- ity. Our extensive experiments consistently demonstrate that our approach significantly improves the model’s abil- ity to effectively capture the underlying data distribution (Fig. 2). SADM achieves state-of-the-art results across 12 image datasets, including ImageNet [10]. Furthermore, we observe its potential for facilitating rapid adaptation to new domains in cross-domain fine-tuning tasks. We summarize our contributions as follows: • To the best of our knowledge, we are the first to propose Structure-guided Adversarial Training to optimize dif- fusion models from a structural perspective. • We theoretically show that SADM is superior in capturing real data distribution, and can generalize to various image- and latent-based diffusion architectures (e.g., DiT [44]). • We substantially outperform existing methods on image generation and cross-domain fine-tuning tasks, achieving a new state-of-the-art FID of 1.58 and 2.11 on ImageNet at resolutions of 256×256 and 512×512, respectively. 2. Related Work In this work, we focus on improving the training of diffu- sion models. Here, we review previous related works and compare our SADM with them. Modifying Training Objectives of Diffusion Models A line of research modifies training objectives to achieve state- of-the-art likelihood [7, 39, 57, 70]. Song et al. [57] propose likelihood weighting to enable approximate maximum likeli- hood training of score-based diffusion models [54–56] while ContextDiff [72] introduces an effective shifting scheme for facilitating the diffusion and training processes of diffusion probabilistic models [19, 53] to achieve improved sample quality with stable training. Lai et al. [36] and [9] introduce an extra regularization term to the denoising score loss to satisfy some properties of the diffusion process. However, these improvements mainly focus on sample-level optimiza- tion, neglecting the rich structural information within batch samples, which is critical for aligning the learned distribu- tion and data distribution. Hence, we enforce the model to maximally learn the manifold structures of samples. Combining Additional Models for Diffusion Training Another line of research incorporates other models to im- prove the stability and precision of diffusion training. For example, INDM [28] expands the linear diffusion to trainable nonlinear diffusion through a normalizing flow to improve the training curve of diffusion models. Jolicoeur-Martineau et al. [25], Kim et al. [27] improve diffusion models with adversarial learning while Xiao et al. [65] model each de- noising step using a multimodal conditional GAN. LSGM [60] and LDM [48] conduct diffusion process in the seman- tic latent space obtained with a pre-trained VAE. Although these combinations strengthen the model expressiveness for capturing data distribution, they are still limited in modeling the underlying manifold structures within training samples. We propose a novel structure discriminator for adversarially learn the diffusion model from a structural perspective. 3. Preliminary Diffusion Models We consider diffusion models [19, 53, 54] specified in continuous time [4, 30, 56, 59]. Given samples x0 from a data distribution q0(x0), noise schedul- ing functions αt, σt, a diffusion model has latent variables x = {xt | t ∈ [0, 1]}, and the forward process is defined with q(xt|x0), a Gaussian process satisfying the following Markovian structure: q(xt|x0) = N(xt; αtx0, σ2 t I), (1) q(xt|xs) = N(xt; (αt/αs)xs, σ2 t|sI) (2) where 0 ≤ s < t ≤ 1 and σ2 t|s = (1 − eλt−λs)σ2 t , and λt = log[α2 t /σ2 t ] denotes the log signal-to-noise-ratio [30]. The goal of the diffusion model is to denoise xt ∼ q(xt|x0) by estimating ˆxθ(xt) ≈ x0. We train this denoising model ˆxθ using a weighted mean squared error loss Ex0,ϵ,t \u0002 w(λt)∥ˆxθ(xt) − x0∥2 2 \u0003 (3) over uniformly sampled times t ∈ [0, 1]. This loss can be justified as a weighted variational lower bound on the data log likelihood under the diffusion model [30] or as a form of denoising score matching [54, 61]. w(λt) is a pre-specified weighting function [30]. 4. Proposed Method We introduce the proposed SADM in detail (Fig. 3). In order to maximally learn the structural information between real samples, we propose structure-guided training of diffu- sion models in Sec. 4.1. Then we design a novel structure discriminator for adversarially optimizing the training pro- cedure from a structural perspective in Sec. 4.2. Finally, we alternatively interpret our structure-guided training of diffusion models as joint sample diffusion with theoretical analysis for bettwen understanding in Sec. 4.3. 4.1. Beyond Instance-Level Training We first review the previous instance-level training methods for diffusion models. Generally, they train the diffusion model with a finite sample version of Eq. (3): Lt = w(λt) P i∈B ||xi 0 − ˆxθ(xi t)||2 |B| , (4) where xi 0, xi t denote the ith ground truth samples and gen- erated samples in the mini-batch B at time step t. However, this objective function encourages the diffusion model to denoise by considering only the instance-level information, neglecting the group-level (structural) information in the mini-batch. Therefore, we enforce the sample predictions of the denoising network to maximally preserve the manifold structures between batch samples. Structural Constraint in Manifold More concretely, ground truth samples {xi 0}|B| i=1 are first projected from pixel space into embedding space using off-the-shelf pre-trained networks Ψ(·) : Rm×n → Rd, e.g., an Inception-V3 fea- ture extractor pre-trained on ImageNet [58] (we conduct ablation study in Appendix B). Then, the pair-wise relation- ships R(Ψ(xi 0), Ψ(xj 0)) are calculated within batch samples {xi 0}|B| i=1, which contain rich structural information in a low- dimensional manifold. This structural information can be expressed with an affinity matrix M({xi 0}|B| i=1) defined as:   R(Ψ1, Ψ1) R(Ψ1, Ψ2) · · · R(Ψ1, Ψ|B|) R(Ψ2, Ψ1) R(Ψ2, Ψ2) · · · R(Ψ2, Ψ|B|) ... ... ... ... R(Ψ|B|, Ψ1) R(Ψ|B|, Ψ2) · · · R(Ψ|B|, Ψ|B|)   (5) where Ψi is a short-hand for Ψ(xi 0), and R(·, ·) denotes the relational function, such as Euclidean distance. Kindly note that many off-the-shelf pre-trained networks are open-source and only work well on clean images, and thus fail to provide meaningful embeddings when the input is noisy. Therefore, we use the predicted clean samples ˆxi 0 for computing affinity matrix, and regularize the denoising network to minimize the structural distance between M({xi 0}|B| i=1) and M({ˆxi 0}|B| i=1). Adding this structural constraint into Eq. (4), the training objective for denoising network is: Lt = P i∈B ||xi 0 − ˆxθ(xi t)||2 |B| + D(M({xi 0}|B| i=1), M({ˆxi 0}|B| i=1)) (6) 𝑬𝒏𝒄𝒐𝒅𝒆𝒓 𝚿𝝓 Manifold Structure {𝑥!}  {𝑥\"} {𝑥#} {𝒙,𝟎} . . . Adversarial Optimization 𝑫𝒆𝒏𝒐𝒊𝒔𝒆𝒓 𝚾2𝜽 . . . Structure Discriminator . . . . . . Minimize Structure Distance  Maximize Structure Distance  Gradient Flow Denoising Process Diffusion Process Ground Truths Predict Embedding Space Distance  Structure-Guided  Training Pair-wise Relationships 𝓡(∎, ∎) 𝑫𝒆𝒏𝒐𝒊𝒔𝒆𝒓 𝑬𝒏𝒄𝒐𝒅𝒆𝒓 {𝚿𝝓(𝑥5$)} Real Fake {𝑥!}  {𝚿𝝓(𝑥$)}  ℳ({𝚿𝝓(𝑥5$)}) ℳ({𝚿𝝓(𝑥$)}) Figure 3. Overview of SADM. We minimize the structural distance between the generated samples (fake) and ground truth samples (real) in the manifold space for optimizing the denoiser, and maximize their structural distance for optimizing the encoder in structure discriminator. The denoiser and the structure discriminator are adversarially trained. where D(·, ·) denotes the distance metric between ground truth and predicted affinity matrices. In this way, the dif- fusion generator (denoiser) is optimized not only to make correct prediction for each instance, but also to preserve the manifold structures of batch samples. 4.2. Structure-Guided Adversarial Training As demonstrated above, we aim to maximally learn data distribution by aligning the manifold structures of denoiser’s predicted samples to those of ground truth samples in each training batch. At every training iteration, the manifold structures can be diverse and the denoiser tend to merely focus on some easy-to-learn structures, ultimately leading to trivial solutions that fail to capture the whole data distri- bution. In order to mitigate this problem and improve the expressiveness of denoiser, we adversarially learn the denois- ing network against a structure discriminator in a minimax game (Fig. 3), which is trained to distinguish the manifold structures between the real and generated batch samples. Structure Discriminator Normally, the discriminator in adversarial learning [14] would output the discrete value (0 or 1) to determine whether the input is real or fake. How- ever, such discrete discriminator can not be applied in dis- tinguishing manifold structures, because the generated and real sample sets would share similar pair-wise sample re- lations despite their different feature spaces. Thus, instead of learning a classification-based discriminator, we design a novel comparison-based structure discriminator to address this issue. Specifically, as illustrated in Fig. 3, the structure discrimi- nator consists of the aforementioned neural network Ψϕ with trainable parameters ϕ and the distance measure function D(·, ·) that outputs continuous value. It projects both real and generated samples into the embedding space, then the structure discriminator is trained against the denoiser for finding a better manifold (or embedding space) to distin- guish real sample set from generated set by maximizing their structural distance: max ϕ ηt P i,j∈B D \u0000R(Ψϕ(xi 0), Ψϕ(xj 0)), R(Ψϕ(ˆxi θ,t), Ψϕ(ˆxj θ,t))), |B|2 . (7) where we choose ηt = 1 t as a time-dependent weighting factor. We can use simple measurements for D(·, ·) (L2 dis- tance) and R(·, ·) (cosine similarity), as the critical semantic information has been already encoded by Ψϕ. Conversely, the denoiser is adversarially optimized for generating more realistic sample set to fool the structure discriminator by minimizing the structural distance. Final Optimization Objective The final training objective of our SADM consists of normal denoising score matching loss (Eq. (3)) and adversarial structural distance loss (Eq. (7)) at timestep t, which can be written as: Lt(θ) = w(λt) P i∈B ∥ˆxθ(xi t) − xi 0∥2 2 |B| + max ϕ P i,j∈B ηt∥Ψϕ(xi 0)T Ψϕ(xj 0) − Ψϕ(ˆxi θ,t)T Ψϕ(ˆxj θ,t)∥2 2 |B|2 . (8) In training, we iteratively optimize the feature extractor and the denoising network in Eq. (8). In an iteration, we first freeze θ and update ϕ by ascending along its gradient ∇ϕ P i,j∈B ηt∥Ψϕ(xi 0)T Ψϕ(xj 0) − Ψϕ(ˆxi θ,t)T Ψϕ(ˆxj θ,t)∥2 2 |B|2 , (9) then we freeze ϕ and update θ by descending along its gradi- ent ∇θ P i,j∈B ηt∥Ψϕ(xi 0)T Ψϕ(xj 0) − Ψϕ(ˆxi θ,t)T Ψϕ(ˆxj θ,t)∥2 2 |B|2 + ∇θ w(λt) P i∈B ∥ˆxθ(xi t) − xi 0∥2 2 |B| . (10) The proposed training algorithm is presented in Algorithm 1. Generalizing to Latent Diffusion Our proposed training algorithm applies not only to image diffusion but also to latent diffusion, such as LDM [48] and LSGM [60]. In this case, the intermediate results xt are latent codes rather than images. We can use the latent decoder (e.g., VAE decoder [31]) to project the generated latent codes to images and then use the same algorithm in the image domain. 4.3. Interpreting as Joint Sample Diffusion Relation-Conditioned Diffusion Process For better un- derstanding of our proposed structure-guided training, we interpret it as a joint sample diffusion model that simul- taneously perturbs and denoises a set of samples condi- tioned on the relation variable. Formally, let y0 = (xi 0, xj 0), where xi 0, xj 0 are independent random variables sampled from ground truth distribution q0. And the relation vari- able that encodes the structure information is defined as R = R(xi 0, xj 0) + γϵ, where γϵ denote a small gaussian noise added to the relation to avoid degenerated distribution. Then the forward diffusion jointly perturbs y0, conditioned on R: q(yt|y0, R) = N(yt; αty0, σ2 t I). (11) To reverse the diffusion process, we need to predict y0, or equivalently, learn the conditional score function ∇y log qt(yt|R) [11, 18]. This formulation allows us to utilize the auxiliary structural information R in the sampling process, which usually leads to better performance [12, 52]. Algorithm 1: SADM, our proposed algorithm. input :{αt}t∈[0,1], {σt}t∈[0,1] the noise schedule, |B| the batch size, w(λt), ηt the scaling factors for denoising score matching loss and adversarial loss, pre-trained encoder Ψϕ for structure discriminator. Initialize denoising network parameters θ; while θ has not converged do Sample a minibatch {xi 0}|B| i=1 ∼ q0 and {ϵj}|B| j=1 ∼ N(0, I) Sample t ∼ U[0, 1] Sample {xi t = αtxi 0 + σtϵi}|B| i=1, Freeze ϕ and update θ with its gradient ∇θ P i,j∈B ηt∥Ψϕ(xi 0)T Ψϕ(xj 0)−Ψϕ(ˆxi θ,t)T Ψϕ(ˆxj θ,t)∥2 2 |B|2 +∇θ w(λt) P i∈B ∥ˆxθ(xi t)−xi 0∥2 2 |B| Adversarially train denoising network and structure discriminator by iteratively updating their parameters θ, ϕ according to Eqs. (9) and (10). output :Denoising network θ. Learning Conditional Score To approximate the condi- tional score function, first we decompose it with Bayes’ rule, ∇y log qt(yt|R) = ∇y log qt(yt) + ∇y log qt(R|yt) = Σs=i,j∇x log qt(xs t) + ∇y log qt(R|yt). (12) To approximate the first term on the right side of Eq. (12), we only need to learn ˆxi θ,t, ˆxj θ,t, as in standard diffusion models [56]. However, the second term is intractable since qt(R|yt) involves the intractable posterior q(y0|yt). Note that qt(R|yt) = Z g(R|y0)q(y0|yt)dy0 = Ey0 [g(R|y0)|yt] , (13) where g(R|y0) is the density function of Gaussian distribu- tion with mean R(xi 0, xj 0) and variance γ2, by the definition of R. As a result, we can use g(R|ˆy0,t) = g(R|(ˆxi θ,t, ˆxj θ,t)) to approximate qt(R|yt), and train it with L2 loss: Eyt∥Ey0 [g(R|y0)|yt] − g(R|ˆy0(yt))∥2 2 = Eyt∥Ey0 [g(R|y0) − g(R|ˆy0(yt))|yt] ∥2 2 ≤ Ey0,yt∥g(R|y0) − g(R|ˆy0(yt))∥2 2 (Jensen Inequality) ≤ w(γ)Ey0,yt∥R(xi 0, xj 0) − R(ˆxi θ,t, ˆxj θ,t)∥2 2 = Lstructure t , (14) where w(γ) is a weighting scalar that depends only on γ. The objective function is the sum of the denoising score Table 1. Quantitative results for class-conditional generation on ImageNet 256×256 and 512×512. Model ImageNet 256×256 ImageNet 512×512 FID ↓ IS ↑ Precision ↑ Recall ↑ FID ↓ IS ↑ Precision ↑ Recall ↑ BigGAN-deep [2] 6.95 171.40 0.87 0.28 8.43 177.90 0.88 0.29 StyleGAN-XL [51] 2.30 265.12 0.78 0.53 2.41 267.75 0.77 0.52 ADM-G, ADM-U [11] 3.94 215.84 0.83 0.53 3.85 221.72 0.84 0.53 LDM-4-G [48] 3.60 247.67 0.87 0.48 - - - - RIN+NoiseSchedule [6] 3.52 186.20 - - 3.95 216.00 - - SimpleDiffusion [21] 2.44 256.30 - - 3.02 248.70 - - DiT-G++ [27] 1.83 281.53 0.78 0.64 - - - - MDT-G [13] 1.79 283.01 0.81 0.61 - - - - DiT-XL/2-G [44] 2.27 278.24 0.83 0.57 3.04 240.82 0.84 0.54 DiT-SADM (Ours) 1.58 298.46 0.86 0.66 2.11 251.82 0.87 0.63 matching objective and Lstructure t : Exi 0,ϵi,tw(λt)∥ˆxθ(xi t) − xi 0∥2 2 + Exj 0,ϵj,tw(λt)∥ˆxθ(xj t) − xj 0∥2 2 + Exi 0,ϵi,xj 0,ϵj,t \u0002 Lstructure t \u0003 , (15) which is a variational upper bound of negative log likelihood of the joint sample. Our objective in Eq. (6) can be viewed as a finite-sample version of Eq. (15), which trains the con- ditional diffusion model to utilize the structural information. 5. Experiments 5.1. Image Generation Experiment Setup We experiment on CIFAR-10 [34], CelebA/FFHQ 64x64 [38], and ImageNet 256x256 [10]. We utilize our SADM to facilitate the training of the diffu- sion backbones from Karras et al. [26], Vahdat et al. [60] on CIFAR-10 and FFHQ, from Kim et al. [29] on CelebA, and from Peebles and Xie [44] (DiT, Diffusion Transformer) on ImageNet. Evaluation Metrics We use Frechet Inception Distance (FID) [17] as the primary metric for capturing both quality and diversity due to its alignment with human judgement. We follow the evaluation procedure of ADM [11] for fair comparisons. For completeness, we also use Inception Score (IS) [50], Precision and Recall [35] as the main metrics for measuring diversity and distribution coverage. Implementation Details We train the denoising network from scratch and use the feature extractor of Inception-V3 [58] pre-trained on ImageNet for initializing the encoder of our structure discriminator. At the begining of training, we freeze the pre-trained discriminator encoder and train the de- noiser with structure-guided training objective in Eq. (6) un- til convergence, then we adversarially tune the denoiser and Table 2. Performance on CIFAR-10. Model Diffusion Space NFE↓ Unconditional Conditional NLL↓ FID↓ FID↓ VDM [30] Data 1000 2.49 7.41 - DDPM [19] Data 1000 3.75 3.17 - iDDPM [41] Data 1000 3.37 2.90 - Soft Truncation [29] Data 2000 2.91 2.47 - INDM [28] Latent 2000 3.09 2.28 - CLD-SGM [12] Data 312 3.31 2.25 - NCSN++ [56] Data 2000 3.45 2.20 - LSGM [60] Latent 138 3.43 2.10 - NCSN++-G [3] Data 2000 - - 2.25 EDM [26] Data 35 2.60 1.97 1.79 LSGM-G++ [27] Latent 138 3.42 1.94 - EDM-G++ [27] Data 35 2.55 1.77 1.64 SADM Latent 138 2.51 1.78 1.73 SADM Data 35 2.28 1.54 1.47 encoder with the objective in Eq. (8) for 3 or 4 rounds (500k steps) until they achieve a balance. This training paradigm keeps the same for unconditional and class-conditional gen- eration tasks, and can be easily generalized to score-based diffusion models [56, 60] by adding our structural constraint into the final objective functions. Main Results Our SADM achieves new state-of-the-art FIDs on all datasets including CIFAR-10, CelebA, FFHQ, and ImageNet. On ImageNet 256 × 256 and 512 × 512, we consistently achieve SOTA FIDs of 1.43 and 2.18 for class-conditional generation as illustrated in Tab. 1. Notably, We significantly improve the generation performance of DiT and outperform the previous best FID of MDT [13] solely through improved training algorithm without increasing the model complexity and inference time. From Tab. 2, we find that our SADM works well for both image diffusion (based on EDM) and latent diffusion (based on LSGM). In experiments, for fair comparisons, we use the same hyperpa- ADM-G MDT SADM (Ours) Figure 4. Qualitative comparion with ADM-G [11] and previous SOTA method MDT [13]. Our SADM can synthesize more realistic and high-quality samples while maintaining satisfying diversity. Table 3. FID performance on CelebA/FFHQ 64 × 64. Model Diffusion Space NFE↓ CelebA FFHQ DDPM++ [56] Data 131 2.32 - Soft Truncation [29] Data 131 1.90 - Soft Diffusion [8] Data 300 1.85 - INDM [28] Latent 132 1.75 - EDM [26] Data 79 - 2.39 Soft Truncation-G++ [27] Data 131 1.34 - EDM-G++ [27] Data 71 - 1.98 SADM Latent 131 1.28 1.85 SADM Data 71 1.16 1.71 rameters as EDM and LSGM to evaluate the effectiveness of our proposed training algorithm. And we also achieve significant performance improvement on facial datasets as demonstrated in Tab. 3. For qualitative results, we compare our SADM with ADM-G [11] and previous SOTA method MDT [13] in Fig. 4. These remarkable results demonstrate our SADM has the potential for generalizing to arbitrary diffusion architectures and can better learn the whole data distribution. 5.2. Cross-Domain Fine-Tuning Experiment Setup We conduct cross-domain fine-tuning tasks on diffusion image generation for evaluating the trans- ferability of proposed model, where we pre-train a diffusion model in source domain and adapt it to a target domain by fine-tuning. Following Xie et al. [66], we use ImageNet as source dataset, and choose eight commonly-used fine- grained datasets as target datasets: Food101 [1], SUN397 [64], DF-20M mini [45], Caltech101 [15], CUB-200-2011 [62], ArtBench-10 [37], Oxford Flowers [42] and Stanford Cars [33]. More details about datasets are in Appendix A. Implementation Details For fair comparison, we follow Xie et al. [66] to set all the hyper-parameters in both pre- training and fine-tuning stages, and use Diffusion Trans- former (DiT) [44] as the diffusion backbone. We pretrain DiT on ImageNet 256 × 256 with a learning rate of 0.0001 using DDPM objective. For target datasets, we fine-tune the pre-trained DiT with 24k structure-guide training steps and 4k adversarial training steps. We experiment with two fine-tuning settings, full and parameter-efficient, for com- prehensive evaluations. In parameter-efficient setting, fol- lowing Xie et al. [66], we freeze most of parameters in the pre-trained diffusion model and fine-tune the bias term, nor- malization, and class condition module. Main Results We achieve SOTA performance on all datasets in fine-tuning tasks as illustrated in Tab. 4. Re- markably, we significantly surpass DDPM in full fine-tuning and outperform DiffFit in parameter-efficient fine-tuning. The results sufficiently demonstrate our superior capability of capturing the whole data distribution, which enables better adaptation to new domains. Among all datasets, we achieve the best improvement over other methods on ArtBench- 10 which has distinct distribution from ImageNet, demon- Table 4. FID performance comparisons on 8 downstream datasets, all the models are pretrained on ImageNet 256×256. Method Dataset Food SUN DF-20M Caltech CUB-Bird ArtBench Oxford Flowers Standard Cars Average FID AdaptFormer [5] 13.67 11.47 22.38 35.76 7.73 38.43 21.24 10.73 20.17 BitFit [73] 9.17 9.11 17.78 34.21 8.81 24.53 20.31 10.64 16.82 VPT [24] 18.47 14.54 32.89 42.78 17.29 40.74 25.59 22.12 26.80 LoRA [22] 33.75 32.53 120.25 86.05 56.03 80.99 164.13 76.24 81.25 DiffFit [66] 6.96 8.55 17.35 33.84 5.48 20.87 20.18 9.90 15.39 Full Fine-tuning with DDPM 10.46 7.96 17.26 35.25 5.68 25.31 21.05 9.79 16.59 SADM (parameter-efficient) 5.74 7.92 16.58 32.03 5.04 18.23 19.37 9.26 14.27 SADM (full) 6.20 7.35 15.12 32.86 4.69 19.84 18.18 8.93 14.15 strating the out-of-distribution generalization ability of our SADM. More qualitative results are in Appendix C. 5.3. Model Analysis Heatmap Analysis To evaluate the ability to capture data distribution, we perform heatmap analysis in Fig. 5, where we provide DDPM and our SADM with 8 randomly-selected noisy images in test batch and visualize the correlations be- tween their denoised outputs. We observe that compared to DDPM, the overall heatmap pattern of our SADM is more closer to that of label affinity. The phenomenon demon- strates our SADM can precisely learn the manifold struc- tures within real data samples. DDPM Label Affinity SADM Figure 5. Heatmap visualization with 8 denoised samples. Ablation Study We conduct ablation study to validate the effectiveness of our algorithm in Tab. 5. Here, we base on DDPM [19] architecture, and progressively add our model components (structural guidance and structure discrimina- tor) into it for evaluating FID score on three datasets. We Table 5. Ablation study with FID performance. SG denotes struc- tural guidance, SAT denotes SG+Structure Discriminator. Dataset Model DDPM + Our SG + Our SAT ImageNet 4.59 3.57 3.14 CIFAR-10 3.17 2.64 2.33 CelebA 2.32 1.82 1.64 observe that each component can consistently improve the DDPM on all datasets, and the performance improvement of our structural guidance is more significant. The results fully demonstrate the effectiveness of our algorithm. More ablation studies about our model are in Appendix B. Contributing to Better Convergence To investigate the contribution of our structure-guided training to model conver- gence, we plot the training curve in Fig. 6. We conclude that compared to previous SOTA methods DiT [44] and MDT [13], the proposed structure-guided training enables faster and better model convergence because we optimize the diffu- sion models from a structural perspective, which essentially contributes to capturing the whole data distribution. 200 400 600 800 1000 1200 Training steps (k) 40 50 60 70 80 90 100 FID-50K DiT MDT Ours Figure 6. Comparison with SOTA methods on model convergence. 6. Conclusion We propose structure-guided adversarial training for opti- mizing diffusion models from a structural perspective. The proposed training algorithm can easily generalize to both image and latent diffusion models, and consistently improve existing diffusion models with theoretical derivations and empirical results. We achieve new SOTA performance on image generation and cross-domain fine-tuning tasks across 12 image datasets. For future work, we will extend our method to more challenging diffusion-based applications (e.g., text-to-image/video generation). References [1] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101–mining discriminative components with random forests. In ECCV, 2014. 7, 12 [2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. In International Conference on Learning Representations, 2018. 6 [3] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen Lo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping Chen, and Chun-Yi Lee. Denoising likelihood score matching for conditional score-based data generation. In International Conference on Learning Representations, 2022. 6 [4] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Moham- mad Norouzi, and William Chan. Wavegrad: Estimating gradients for waveform generation. In International Confer- ence on Learning Representations, 2020. 1, 3 [5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib- ing Song, Jue Wang, and Ping Luo. Adaptformer: Adapting vision transformers for scalable visual recognition. arXiv, 2022. 8 [6] Ting Chen. On the importance of noise scheduling for diffu- sion models. arXiv preprint arXiv:2301.10972, 2023. 6 [7] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Kim, Hyunwoo Kim, and Sungroh Yoon. Perception prior- itized training of diffusion models. In CVPR, pages 11472– 11481, 2022. 1, 2 [8] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexan- dros G Dimakis, and Peyman Milanfar. Soft diffusion: Score matching for general corruptions. arXiv preprint arXiv:2209.05442, 2022. 7 [9] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and Constantinos Daskalakis. Consistent diffusion models: Mit- igating sampling drift by learning to be consistent. arXiv preprint arXiv:2302.09057, 2023. 1, 2 [10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248–255, 2009. 2, 6 [11] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:8780–8794, 2021. 1, 2, 5, 6, 7, 14, 15, 16, 17, 18 [12] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score- based generative modeling with critically-damped langevin diffusion. In International Conference on Learning Represen- tations, 2022. 5, 6 [13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is a strong image synthe- sizer. arXiv preprint arXiv:2303.14389, 2023. 6, 7, 8 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. 4 [15] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256 object category dataset. 2007. 7, 12 [16] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong Chen, Han Hu, Xin Geng, and Baining Guo. Efficient diffu- sion training via min-snr weighting strategy. arXiv preprint arXiv:2303.09556, 2023. 1 [17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern- hard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017. 6 [18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 5 [19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. In NeurIPS, pages 6840–6851, 2020. 1, 2, 3, 6, 8 [20] Emiel Hoogeboom, Vıctor Garcia Satorras, Cl´ement Vignac, and Max Welling. Equivariant diffusion for molecule genera- tion in 3d. In International Conference on Machine Learning, pages 8867–8887. PMLR, 2022. 1 [21] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple diffusion: End-to-end diffusion for high resolution images. arXiv preprint arXiv:2301.11093, 2023. 6 [22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arxiv, 2021. 8 [23] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang, Wentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, Bin CUI, and Wenming Yang. Protein-ligand interaction prior for binding-aware 3d molecule diffusion models. In The Twelfth International Conference on Learning Representations, 2024. 1 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, 2022. 8 [25] Alexia Jolicoeur-Martineau, R´emi Pich´e-Taillefer, Ioannis Mitliagkas, and Remi Tachet des Combes. Adversarial score matching and improved sampling for image generation. In International Conference on Learning Representations, 2020. 3 [26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. 2022. 6, 7 [27] Dongjun Kim, Yeongmin Kim, Wanmo Kang, and Il-Chul Moon. Refining generative process with discriminator guidance in score-based diffusion models. arXiv preprint arXiv:2211.17091, 2022. 1, 3, 6, 7 [28] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee, Wanmo Kang, and Il-chul Moon. Maximum likelihood train- ing of implicit nonlinear diffusion model. Advances in Neural Information Processing Systems, 35:32270–32284, 2022. 1, 3, 6, 7 [29] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal training technique of score-based diffusion model for high precision score estimation. In International Conference on Machine Learning, pages 11201–11228. PMLR, 2022. 1, 6, 7 [30] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS, 34:21696–21707, 2021. 1, 3, 6 [31] Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 5 [32] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. In International Conference on Learning Repre- sentations, 2020. 1 [33] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In ICCV workshops, 2013. 7, 13 [34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6 [35] Tuomas Kynk¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. Advances in Neural Information Processing Systems, 32, 2019. 6 [36] Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu Uesaka, Yuki Mitsufuji, and Stefano Ermon. Regularizing score-based models with score fokker-planck equations. In NeurIPS 2022 Workshop on Score-Based Methods, 2022. 1, 2 [37] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The artbench dataset: Benchmarking generative models with art- works. arXiv, 2022. 7, 12 [38] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, pages 3730–3738, 2015. 6 [39] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Maximum likelihood training for score- based diffusion odes by high order denoising score matching. In International Conference on Machine Learning, pages 14429–14460. PMLR, 2022. 1, 2 [40] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In CVPR, pages 2837–2845, 2021. 1 [41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, ICML, 2021. 6 [42] Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In 2008 Sixth Indian Conference on Computer Vision, Graphics & Image Processing, 2008. 7, 13 [43] Mang Ning, Enver Sangineto, Angelo Porrello, Simone Calderara, and Rita Cucchiara. Input perturbation reduces ex- posure bias in diffusion models. In International Conference on Machine Learning, pages 26245–26265. PMLR, 2023. 1 [44] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF Inter- national Conference on Computer Vision, pages 4195–4205, 2023. 2, 6, 7, 8 [45] Luk´aˇs Picek, Milan ˇSulc, Jiˇr´ı Matas, Thomas S Jeppesen, Ja- cob Heilmann-Clausen, Thomas Læssøe, and Tobias Frøslev. Danish fungi 2020-not just another image recognition dataset. In WACV, 2022. 7, 12 [46] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima Sadekova, and Mikhail Kudinov. Grad-tts: A diffusion proba- bilistic model for text-to-speech. In International Conference on Machine Learning, pages 8599–8608. PMLR, 2021. 1 [47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image genera- tion with clip latents. arXiv preprint arXiv:2204.06125, 2022. 1 [48] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684– 10695, 2022. 1, 3, 5, 6 [49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho- torealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. 1 [50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 6 [51] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan- xl: Scaling stylegan to large diverse datasets. In ACM SIG- GRAPH, pages 1–10, 2022. 6 [52] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath. Where to diffuse, how to diffuse, and how to get back: Auto- mated learning for multivariate diffusions. In The Eleventh International Conference on Learning Representations, 2022. 5 [53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Confer- ence on Machine Learning, pages 2256–2265, 2015. 1, 2, 3 [54] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In NeurIPS, 2019. 1, 2, 3 [55] Yang Song and Stefano Ermon. Improved techniques for training score-based generative models. In NeurIPS, pages 12438–12448, 2020. [56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab- hishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2020. 1, 2, 3, 5, 6, 7 [57] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-based diffusion models. Advances in Neural Information Processing Systems, 34:1415– 1428, 2021. 1, 2 [58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception ar- chitecture for computer vision. In CVPR, pages 2818–2826, 2016. 3, 6 [59] Belinda Tzen and Maxim Raginsky. Neural stochastic differ- ential equations: Deep latent gaussian models in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019. 3 [60] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287–11302, 2021. 1, 3, 5, 6 [61] Pascal Vincent. A connection between score matching and denoising autoencoders. Neural Computation, 23(7):1661– 1674, 2011. 3 [62] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per- ona, and Serge J. Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 7, 12 [63] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-gan: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022. 1 [64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recog- nition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, 2010. 7, 12 [65] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. In International Conference on Learning Representations, 2021. 1, 3 [66] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou, Zhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Unlocking transferability of large diffusion models via simple parameter- efficient fine-tuning. In ICCV, pages 4230–4239, 2023. 7, 8 [67] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Er- mon, and Jian Tang. Geodiff: A geometric diffusion model for molecular conformation generation. In International Con- ference on Learning Representations, 2021. 1 [68] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong, Max Tegmark, and Tommi Jaakkola. Pfgm++: Unlocking the potential of physics-inspired generative models. arXiv preprint arXiv:2302.04265, 2023. 1 [69] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run- sheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming- Hsuan Yang. Diffusion models: A comprehensive survey of methods and applications. ACM Computing Surveys, 56(4): 1–39, 2023. 1 [70] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, and Bin Cui. Improving diffusion-based image synthesis with context prediction. Ad- vances in Neural Information Processing Systems, 36, 2024. 1, 2 [71] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and Bin Cui. Mastering text-to-image diffusion: Re- captioning, planning, and generating with multimodal llms. arXiv preprint arXiv:2401.11708, 2024. 1 [72] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu, Minkai Xu, Stefano Ermon, and Bin CUI. Cross-modal con- textualized diffusion models for text-guided visual generation and editing. In The Twelfth International Conference on Learning Representations, 2024. 1, 2 [73] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. arXiv, 2021. 8 [74] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing flow. Advances in Neural Information Processing Systems, 34:16280–16291, 2021. 1 [75] Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake Xie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin Cui. Realcompo: Dynamic equilibrium between realism and compositionality improves text-to-image diffusion models. arXiv preprint arXiv:2402.12908, 2024. 1 A. More Implementation Details A.1. Training and Sampling Details We present the training and sampling details of our SADM on different datasets in Tab. 6 for better reproducing our method. We are dedicated to open-sourcing the training and inference code upon paper acceptance. Table 6. Training and sampling configurations in SADM. CIFAR-10 CelebA/FFHQ ImageNet Latent Image Latent Image Image Latent Training of SADM Based Diffusion Model LSGM EDM LSGM EDM ADM DiT Sample Relation Measurement R cosine similarity cosine similarity cosine similarity cosine similarity cosine similarity cosine similarity Structural Distance Metric D L2 distance L2 distance L2 distance L2 distance L2 distance L2 distance Encoder Ψϕ of Structure Discriminator Inception V3 Inception V3 Inception V3 Inception V3 Inception V3 Inception V3 Round of Adversarial Training 2 2 3 3 4 4 Sampling of SADM SDE LVP WVE LVP WVE LVP LVP Solver PFODE PFODE PFODE PFODE DDPM DDPM Solver accuracy of sθ 1st-order 2nd-order 1st-order 2nd-order 1st-order 1st-order Solver type of sθ RK45 Heun RK45 Heun Euler (DDPM) Euler (DDPM) NFE 138 35 131 71 250 250 Classifier Guidance ✗ ✗ ✗ ✗ ✓ ✓ wCG t 0 0 0 0 Adaptive Adaptive A.2. Datasets Food101 [1]. This dataset contains 101 food categories, totaling 101,000 images. Each category includes 750 training images and 250 manually reviewed test images. The training images were kept intentionally uncleaned, preserving some degree of noise, primarily vivid colors and occasionally incorrect labels. All images have been adjusted to a maximum side length of 512 pixels. SUN 397 [64]. The SUN benchmark database comprises 108,753 images labeled into 397 distinct categories. The quantities of images vary among the categories, however, each category is represented by a minimum of 100 images. These images are commonly used in scene understanding applications. DF20M [45]. DF20 is a new fine-grained dataset and benchmark featuring highly accurate class labels based on the taxonomy of observations submitted to the Danish Fungal Atlas. The dataset has a well-defined class hierarchy and a rich observational metadata. It is characterized by a highly imbalanced long-tailed class distribution and a negligible error rate. Importantly, DF20 has no intersection with ImageNet, ensuring unbiased comparison of models fine-tuned from ImageNet checkpoints. Caltech 101 [15]. The Caltech 101 dataset comprises photos of objects within 101 distinct categories, with roughly 40 to 800 images allocated to each category. The majority of the categories have around 50 images. Each image is approximately 300×200 pixels in size. CUB-200-2011 [62]. CUB-200-2011 (Caltech-UCSD Birds-200-2011) is an expansion of the CUB-200 dataset by approxi- mately doubling the number of images per category and adding new annotations for part locations. The dataset consists of 11,788 images divided into 200 categories. ArtBench-10 [37]. ArtBench-10 is a class-balanced, standardized dataset comprising 60,000 high-quality images of artwork annotated with clean and precise labels. It offers several advantages over previous artwork datasets including balanced class distribution, high-quality images, and standardized data collection and pre-processing procedures. It contains 5,000 training images and 1,000 testing images per style. Oxford Flowers [42]. The Oxford 102 Flowers Dataset contains high quality images of 102 commonly occurring flower categories in the United Kingdom. The number of images per category range between 40 and 258. This extensive dataset provides an excellent resource for various computer vision applications, especially those focused on flower recognition and classification. Stanford Cars [33]. In the Stanford Cars dataset, there are 16,185 images that display 196 distinct classes of cars. These images are divided into a training and a testing set: 8,144 images for training and 8,041 images for testing. The distribution of samples among classes is almost balanced. Each class represents a specific make, model, and year combination, e.g., the 2012 Tesla Model S or the 2012 BMW M3 coupe. B. Ablation Study In the main text, we have conducted ablation study on our structural guidance and structure discriminator, and find both of them have a critical impact on the final model performance. In this section, we conduct more detailed ablation study on the designs in structure discriminator for better understanding of our model. B.1. Encoder of Structure Discriminator We here conduct ablation study on the encoder choice in our structure discriminator, and we compare with ResNet-18 and Transformer (ViT) architectures that are pre-trained on ImageNet in Fig. 7. In the ablation study, we evaluate the FID performance in three datasets with different encoders. From the results, we can find that Inception and ViT are both better than ResNet-18 because they are superior in capturing the visual semantics of images, thus extracting more informative manifold structures. Overall, the encoder choice does not have an obvious impact on the model performance. ImageNet CIFAR-10 CelebA 1.0 1.2 1.4 1.6 1.8 2.0 FID Ablation study ResNet Transformers Inception-V3 Figure 7. Ablation study on the encoder of structure discriminator in ImageNet, CIFAR-10, and CelebA datasets. B.2. Metric of Structure Discriminator In main text, we use cosine similarity for R and L2 distance for D. Here we conduct ablation study on the choice of these metrics, and put the results in Tab. 7. In the ablation study, we fix the R or D and change the other metric. We find that using cosine similarity and L2 distance can achieve a similar result, and L1 distance is slightly worse than other metrics. Overall, our model is robust to the choice of metrics. B.3. Round of Adversarial Training We further conduct ablation study on the rounds of our structure-guided adversarial training in Fig. 8. We find that in the initial round, the model performance can be significantly enhanced regarding FID score, demonstrating the effectiveness of our Table 7. Ablation study on R and D in ImageNet 256×256. Module Metric L1 distance L2 distance cosine similarity Sample Relation R 1.65 1.56 1.58 Structural Distance D 1.63 1.58 1.60 structure discriminator. After few rounds, the model performance tends to converge as the diffusion denoiser and structure discriminator in SADM have achieved a balance. 0 1 2 3 4 5 6 Rounds of Adversarial Training 1.6 1.7 1.8 1.9 2.0 2.1 2.2 2.3 FID Ablation study Figure 8. Ablation study on the round of our structure-guided adversarial training in ImageNet. C. More Qualitative Comparisons We here show more qualitative comparison results between our SADM and ADM [11]. Fig. 9 and Fig. 10 show the generated samples on CelebA and FFHQ datasets in unconditional image generation task, and Fig. 11 and Fig. 12 show the generated samples on CUB-200 and Oxford-Flowers datasets in cross-domain fine-tuning task. We observe that our SADM can comprehensively achieve improvements over previous diffusion models in fidelity and quality, demonstrating the superiority of our new training algorithm. ADM SADM (Ours) Figure 9. Random generated samples of ADM [11] and our SADM on unconditional CelebA. ADM SADM (Ours) Figure 10. Random generated samples of ADM [11] and our SADM on unconditional FFHQ. ADM SADM (Ours) Figure 11. Random generated samples of the diffusion model fine-tuned by ADM [11] and our SADM on unconditional CUB-200. ADM SADM (Ours) Figure 12. Random generated samples of the diffusion model fine-tuned by ADM [11] and our SADM on unconditional Oxford-Flowers. "
}