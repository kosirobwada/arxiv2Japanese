{
    "optim": "Structure-Guided Adversarial Training of Diffusion Models\nLing Yang∗†\nHaotian Qian* Zhilong Zhang\nJingwei Liu\nBin Cui†\nPeking University\nyangling0818@163.com, zzdmqht@gmail.com, {zzl2018math, bin.cui}@pku.edu\nAbstract\nDiffusion models have demonstrated exceptional efficacy\nin various generative applications. While existing models\nfocus on minimizing a weighted sum of denoising score\nmatching losses for data distribution modeling, their train-\ning primarily emphasizes instance-level optimization, over-\nlooking valuable structural information within each mini-\nbatch, indicative of pair-wise relationships among samples.\nTo address this limitation, we introduce Structure-guided\nAdversarial training of Diffusion Models (SADM). In this\npioneering approach, we compel the model to learn manifold\nstructures between samples in each training batch. To ensure\nthe model captures authentic manifold structures in the data\ndistribution, we advocate adversarial training of the diffu-\nsion generator against a novel structure discriminator in a\nminimax game, distinguishing real manifold structures from\nthe generated ones. SADM substantially improves existing\ndiffusion transformers and outperforms existing methods in\nimage generation and cross-domain fine-tuning tasks across\n12 datasets, establishing a new state-of-the-art FID of 1.58\nand 2.11 on ImageNet for class-conditional image genera-\ntion at resolutions of 256×256 and 512×512, respectively.\n1. Introduction\nDiffusion models [19, 53–56, 69] have achieved remarkable\ngeneration quality in various tasks, including image genera-\ntion [11, 47–49, 63, 70, 71, 75], audio synthesis [4, 32, 46],\nand interdisciplinary applications [20, 23, 67]. Starting from\ntractable noise distribution, diffusion models generate data\nby progressively removing noise. This involves the model\nlearning to reverse a pre-defined diffusion process that se-\nquentially introduces varying levels of noise to the data. The\nmodel is parameterized and undergoes training by optimizing\nthe weighted sum of denoising score matching losses [19]\nfor various noise levels [54], aiming to learn the recovery of\nclean images from corrupted images.\n*Contributed equally.\n†Corresponding authors.\nNoise Distribution\nTraining data Distribution\nOur Structure-Guided Training\nPrevious Instance-Level Training\nFigure 1. Comparison between previous instance-level training and\nour structure-guided training for diffusion models.\nAiming to maximally model the data distribution, recent\nworks [27, 39, 57, 60, 68, 72] attempt to improve the pre-\ncision of training diffusion models. For instance, many\napproaches design effective weighting schemes [7, 16, 29,\n39, 57, 72] for maximum likelihood training or add an extra\nregularization term [9, 36, 43] to the denoising score loss.\nThere are also some works to enhance the model expressive-\nness by incorporating other generative models, such as VAE\n[30, 48, 60], GAN [65], and Normalizing Flows [28, 40, 74].\nHowever, their improved training of diffusion models primar-\nily concentrates on instance-level optimization, overlooking\nthe valuable structural information among batch samples.\nThis oversight is significant, as the incorporation of struc-\ntural details is crucial for aligning the learned distribution\nwith the underlying data distribution.\nTo mitigate the challenges posed by existing instance-\nlevel training methods,\nwe propose Structure-guided\nAdversarial training of Diffusion Models (SADM). In con-\ntrast to conventional instance-level training illustrated in\nFig. 1, our approach guides diffusion training at a structural\nlevel. During batch training, the model is facilitated to learn\nmanifold structures within batch samples, represented by\npair-wise relationships in a low-dimensional feature space.\nTo accurately learn real manifold structures in the data dis-\ntribution, we introduce a novel structure discriminator that\ndistinguishes genuine manifold structures from generated\narXiv:2402.17563v1  [cs.CV]  27 Feb 2024\nADM-G\nADM + Structure-guided Adversarial Training\nFID: 4.59, Recall: 0.52\nFID: 3.14, Recall: 0.61\nGround Truth\nFigure 2. Generated samples on ImageNet 256 × 256 with (i) ADM with Classifier Guidance (ADM-G) [11], (ii) ADM optimized by our\nStructure-guided Adversarial Training, and (iii) real samples in ground truth classes. We can significantly improve diffusion models\nqualitatively and quantitatively, and our generated sample distribution is overally more similar to real sample distribution. See Appendix C\nfor more synthesis samples of our SOTA model.\nones. For clarity, we alternatively refer to the diffusion mod-\nels optimized through our structure-guided training as joint\nsample diffusion, a concept theoretically proven to enhance\ndiffusion model optimization.\nWe assess the performance of our model across two\npivotal tasks: image generation and cross-domain fine-\ntuning. The former involves training the diffusion model\nfrom scratch to evaluate its capability in capturing the en-\ntire data distribution. The latter leverages a pre-trained dif-\nfusion model, constructed on a large-scale source dataset,\nand fine-tunes it on a target dataset to assess transferabil-\nity. Our extensive experiments consistently demonstrate\nthat our approach significantly improves the model’s abil-\nity to effectively capture the underlying data distribution\n(Fig. 2). SADM achieves state-of-the-art results across 12\nimage datasets, including ImageNet [10]. Furthermore, we\nobserve its potential for facilitating rapid adaptation to new\ndomains in cross-domain fine-tuning tasks. We summarize\nour contributions as follows:\n• To the best of our knowledge, we are the first to propose\nStructure-guided Adversarial Training to optimize dif-\nfusion models from a structural perspective.\n• We theoretically show that SADM is superior in capturing\nreal data distribution, and can generalize to various image-\nand latent-based diffusion architectures (e.g., DiT [44]).\n• We substantially outperform existing methods on image\ngeneration and cross-domain fine-tuning tasks, achieving\na new state-of-the-art FID of 1.58 and 2.11 on ImageNet\nat resolutions of 256×256 and 512×512, respectively.\n2. Related Work\nIn this work, we focus on improving the training of diffu-\nsion models. Here, we review previous related works and\ncompare our SADM with them.\nModifying Training Objectives of Diffusion Models\nA\nline of research modifies training objectives to achieve state-\nof-the-art likelihood [7, 39, 57, 70]. Song et al. [57] propose\nlikelihood weighting to enable approximate maximum likeli-\nhood training of score-based diffusion models [54–56] while\nContextDiff [72] introduces an effective shifting scheme for\nfacilitating the diffusion and training processes of diffusion\nprobabilistic models [19, 53] to achieve improved sample\nquality with stable training. Lai et al. [36] and [9] introduce\nan extra regularization term to the denoising score loss to\nsatisfy some properties of the diffusion process. However,\nthese improvements mainly focus on sample-level optimiza-\ntion, neglecting the rich structural information within batch\nsamples, which is critical for aligning the learned distribu-\ntion and data distribution. Hence, we enforce the model to\nmaximally learn the manifold structures of samples.\nCombining Additional Models for Diffusion Training\nAnother line of research incorporates other models to im-\nprove the stability and precision of diffusion training. For\nexample, INDM [28] expands the linear diffusion to trainable\nnonlinear diffusion through a normalizing flow to improve\nthe training curve of diffusion models. Jolicoeur-Martineau\net al. [25], Kim et al. [27] improve diffusion models with\nadversarial learning while Xiao et al. [65] model each de-\nnoising step using a multimodal conditional GAN. LSGM\n[60] and LDM [48] conduct diffusion process in the seman-\ntic latent space obtained with a pre-trained VAE. Although\nthese combinations strengthen the model expressiveness for\ncapturing data distribution, they are still limited in modeling\nthe underlying manifold structures within training samples.\nWe propose a novel structure discriminator for adversarially\nlearn the diffusion model from a structural perspective.\n3. Preliminary\nDiffusion Models\nWe consider diffusion models [19, 53,\n54] specified in continuous time [4, 30, 56, 59]. Given\nsamples x0 from a data distribution q0(x0), noise schedul-\ning functions αt, σt, a diffusion model has latent variables\nx = {xt | t ∈ [0, 1]}, and the forward process is defined\nwith q(xt|x0), a Gaussian process satisfying the following\nMarkovian structure:\nq(xt|x0) = N(xt; αtx0, σ2\nt I),\n(1)\nq(xt|xs) = N(xt; (αt/αs)xs, σ2\nt|sI)\n(2)\nwhere 0 ≤ s < t ≤ 1 and σ2\nt|s = (1 − eλt−λs)σ2\nt , and\nλt = log[α2\nt /σ2\nt ] denotes the log signal-to-noise-ratio [30].\nThe goal of the diffusion model is to denoise xt ∼ q(xt|x0)\nby estimating ˆxθ(xt) ≈ x0. We train this denoising model\nˆxθ using a weighted mean squared error loss\nEx0,ϵ,t\n\u0002\nw(λt)∥ˆxθ(xt) − x0∥2\n2\n\u0003\n(3)\nover uniformly sampled times t ∈ [0, 1]. This loss can be\njustified as a weighted variational lower bound on the data\nlog likelihood under the diffusion model [30] or as a form of\ndenoising score matching [54, 61]. w(λt) is a pre-specified\nweighting function [30].\n4. Proposed Method\nWe introduce the proposed SADM in detail (Fig. 3). In\norder to maximally learn the structural information between\nreal samples, we propose structure-guided training of diffu-\nsion models in Sec. 4.1. Then we design a novel structure\ndiscriminator for adversarially optimizing the training pro-\ncedure from a structural perspective in Sec. 4.2. Finally,\nwe alternatively interpret our structure-guided training of\ndiffusion models as joint sample diffusion with theoretical\nanalysis for bettwen understanding in Sec. 4.3.\n4.1. Beyond Instance-Level Training\nWe first review the previous instance-level training methods\nfor diffusion models. Generally, they train the diffusion\nmodel with a finite sample version of Eq. (3):\nLt = w(λt)\nP\ni∈B ||xi\n0 − ˆxθ(xi\nt)||2\n|B|\n,\n(4)\nwhere xi\n0, xi\nt denote the ith ground truth samples and gen-\nerated samples in the mini-batch B at time step t. However,\nthis objective function encourages the diffusion model to\ndenoise by considering only the instance-level information,\nneglecting the group-level (structural) information in the\nmini-batch. Therefore, we enforce the sample predictions of\nthe denoising network to maximally preserve the manifold\nstructures between batch samples.\nStructural Constraint in Manifold\nMore concretely,\nground truth samples {xi\n0}|B|\ni=1 are first projected from pixel\nspace into embedding space using off-the-shelf pre-trained\nnetworks Ψ(·) : Rm×n → Rd, e.g., an Inception-V3 fea-\nture extractor pre-trained on ImageNet [58] (we conduct\nablation study in Appendix B). Then, the pair-wise relation-\nships R(Ψ(xi\n0), Ψ(xj\n0)) are calculated within batch samples\n{xi\n0}|B|\ni=1, which contain rich structural information in a low-\ndimensional manifold. This structural information can be\nexpressed with an affinity matrix M({xi\n0}|B|\ni=1) defined as:\n\n\nR(Ψ1, Ψ1)\nR(Ψ1, Ψ2)\n· · ·\nR(Ψ1, Ψ|B|)\nR(Ψ2, Ψ1)\nR(Ψ2, Ψ2)\n· · ·\nR(Ψ2, Ψ|B|)\n...\n...\n...\n...\nR(Ψ|B|, Ψ1)\nR(Ψ|B|, Ψ2)\n· · ·\nR(Ψ|B|, Ψ|B|)\n\n\n(5)\nwhere Ψi is a short-hand for Ψ(xi\n0), and R(·, ·) denotes the\nrelational function, such as Euclidean distance. Kindly note\nthat many off-the-shelf pre-trained networks are open-source\nand only work well on clean images, and thus fail to provide\nmeaningful embeddings when the input is noisy. Therefore,\nwe use the predicted clean samples ˆxi\n0 for computing affinity\nmatrix, and regularize the denoising network to minimize the\nstructural distance between M({xi\n0}|B|\ni=1) and M({ˆxi\n0}|B|\ni=1).\nAdding this structural constraint into Eq. (4), the training\nobjective for denoising network is:\nLt =\nP\ni∈B ||xi\n0 − ˆxθ(xi\nt)||2\n|B|\n+ D(M({xi\n0}|B|\ni=1), M({ˆxi\n0}|B|\ni=1))\n(6)\n𝑬𝒏𝒄𝒐𝒅𝒆𝒓 𝚿𝝓\nManifold Structure\n{𝑥!} \n{𝑥\"}\n{𝑥#}\n{𝒙,𝟎}\n. . .\nAdversarial Optimization\n𝑫𝒆𝒏𝒐𝒊𝒔𝒆𝒓 𝚾2𝜽\n. . .\nStructure Discriminator\n. . .\n. . .\nMinimize Structure Distance \nMaximize Structure Distance \nGradient Flow\nDenoising Process\nDiffusion Process\nGround Truths\nPredict\nEmbedding Space\nDistance \nStructure-Guided \nTraining\nPair-wise Relationships\n𝓡(∎, ∎)\n𝑫𝒆𝒏𝒐𝒊𝒔𝒆𝒓\n𝑬𝒏𝒄𝒐𝒅𝒆𝒓\n{𝚿𝝓(𝑥5$)}\nReal\nFake\n{𝑥!} \n{𝚿𝝓(𝑥$)} \nℳ({𝚿𝝓(𝑥5$)})\nℳ({𝚿𝝓(𝑥$)})\nFigure 3. Overview of SADM. We minimize the structural distance between the generated samples (fake) and ground truth samples (real) in\nthe manifold space for optimizing the denoiser, and maximize their structural distance for optimizing the encoder in structure discriminator.\nThe denoiser and the structure discriminator are adversarially trained.\nwhere D(·, ·) denotes the distance metric between ground\ntruth and predicted affinity matrices. In this way, the dif-\nfusion generator (denoiser) is optimized not only to make\ncorrect prediction for each instance, but also to preserve the\nmanifold structures of batch samples.\n4.2. Structure-Guided Adversarial Training\nAs demonstrated above, we aim to maximally learn data\ndistribution by aligning the manifold structures of denoiser’s\npredicted samples to those of ground truth samples in each\ntraining batch. At every training iteration, the manifold\nstructures can be diverse and the denoiser tend to merely\nfocus on some easy-to-learn structures, ultimately leading\nto trivial solutions that fail to capture the whole data distri-\nbution. In order to mitigate this problem and improve the\nexpressiveness of denoiser, we adversarially learn the denois-\ning network against a structure discriminator in a minimax\ngame (Fig. 3), which is trained to distinguish the manifold\nstructures between the real and generated batch samples.\nStructure Discriminator\nNormally, the discriminator in\nadversarial learning [14] would output the discrete value (0\nor 1) to determine whether the input is real or fake. How-\never, such discrete discriminator can not be applied in dis-\ntinguishing manifold structures, because the generated and\nreal sample sets would share similar pair-wise sample re-\nlations despite their different feature spaces. Thus, instead\nof learning a classification-based discriminator, we design a\nnovel comparison-based structure discriminator to address\nthis issue.\nSpecifically, as illustrated in Fig. 3, the structure discrimi-\nnator consists of the aforementioned neural network Ψϕ with\ntrainable parameters ϕ and the distance measure function\nD(·, ·) that outputs continuous value. It projects both real\nand generated samples into the embedding space, then the\nstructure discriminator is trained against the denoiser for\nfinding a better manifold (or embedding space) to distin-\nguish real sample set from generated set by maximizing their\nstructural distance:\nmax\nϕ\nηt\nP\ni,j∈B D\n\u0000R(Ψϕ(xi\n0), Ψϕ(xj\n0)), R(Ψϕ(ˆxi\nθ,t), Ψϕ(ˆxj\nθ,t))),\n|B|2\n.\n(7)\nwhere we choose ηt =\n1\nt as a time-dependent weighting\nfactor. We can use simple measurements for D(·, ·) (L2 dis-\ntance) and R(·, ·) (cosine similarity), as the critical semantic\ninformation has been already encoded by Ψϕ. Conversely,\nthe denoiser is adversarially optimized for generating more\nrealistic sample set to fool the structure discriminator by\nminimizing the structural distance.\nFinal Optimization Objective\nThe final training objective\nof our SADM consists of normal denoising score matching\nloss (Eq. (3)) and adversarial structural distance loss (Eq. (7))\nat timestep t, which can be written as:\nLt(θ) = w(λt) P\ni∈B ∥ˆxθ(xi\nt) − xi\n0∥2\n2\n|B|\n+ max\nϕ\nP\ni,j∈B ηt∥Ψϕ(xi\n0)T Ψϕ(xj\n0) − Ψϕ(ˆxi\nθ,t)T Ψϕ(ˆxj\nθ,t)∥2\n2\n|B|2\n.\n(8)\nIn training, we iteratively optimize the feature extractor and\nthe denoising network in Eq. (8). In an iteration, we first\nfreeze θ and update ϕ by ascending along its gradient\n∇ϕ\nP\ni,j∈B ηt∥Ψϕ(xi\n0)T Ψϕ(xj\n0) − Ψϕ(ˆxi\nθ,t)T Ψϕ(ˆxj\nθ,t)∥2\n2\n|B|2\n,\n(9)\nthen we freeze ϕ and update θ by descending along its gradi-\nent\n∇θ\nP\ni,j∈B ηt∥Ψϕ(xi\n0)T Ψϕ(xj\n0) − Ψϕ(ˆxi\nθ,t)T Ψϕ(ˆxj\nθ,t)∥2\n2\n|B|2\n+ ∇θ\nw(λt) P\ni∈B ∥ˆxθ(xi\nt) − xi\n0∥2\n2\n|B|\n.\n(10)\nThe proposed training algorithm is presented in Algorithm 1.\nGeneralizing to Latent Diffusion\nOur proposed training\nalgorithm applies not only to image diffusion but also to\nlatent diffusion, such as LDM [48] and LSGM [60]. In this\ncase, the intermediate results xt are latent codes rather than\nimages. We can use the latent decoder (e.g., VAE decoder\n[31]) to project the generated latent codes to images and then\nuse the same algorithm in the image domain.\n4.3. Interpreting as Joint Sample Diffusion\nRelation-Conditioned Diffusion Process\nFor better un-\nderstanding of our proposed structure-guided training, we\ninterpret it as a joint sample diffusion model that simul-\ntaneously perturbs and denoises a set of samples condi-\ntioned on the relation variable. Formally, let y0 = (xi\n0, xj\n0),\nwhere xi\n0, xj\n0 are independent random variables sampled\nfrom ground truth distribution q0. And the relation vari-\nable that encodes the structure information is defined as\nR = R(xi\n0, xj\n0) + γϵ, where γϵ denote a small gaussian\nnoise added to the relation to avoid degenerated distribution.\nThen the forward diffusion jointly perturbs y0, conditioned\non R:\nq(yt|y0, R) = N(yt; αty0, σ2\nt I).\n(11)\nTo reverse the diffusion process, we need to predict\ny0, or equivalently, learn the conditional score function\n∇y log qt(yt|R) [11, 18]. This formulation allows us to\nutilize the auxiliary structural information R in the sampling\nprocess, which usually leads to better performance [12, 52].\nAlgorithm 1: SADM, our proposed algorithm.\ninput :{αt}t∈[0,1], {σt}t∈[0,1] the noise schedule,\n|B| the batch size, w(λt), ηt the scaling\nfactors for denoising score matching loss\nand adversarial loss, pre-trained encoder Ψϕ\nfor structure discriminator.\nInitialize denoising network parameters θ;\nwhile θ has not converged do\nSample a minibatch {xi\n0}|B|\ni=1 ∼ q0 and\n{ϵj}|B|\nj=1 ∼ N(0, I)\nSample t ∼ U[0, 1]\nSample {xi\nt = αtxi\n0 + σtϵi}|B|\ni=1,\nFreeze ϕ and update θ with its gradient\n∇θ\nP\ni,j∈B ηt∥Ψϕ(xi\n0)T Ψϕ(xj\n0)−Ψϕ(ˆxi\nθ,t)T Ψϕ(ˆxj\nθ,t)∥2\n2\n|B|2\n+∇θ\nw(λt) P\ni∈B ∥ˆxθ(xi\nt)−xi\n0∥2\n2\n|B|\nAdversarially train denoising network and structure\ndiscriminator by iteratively updating their\nparameters θ, ϕ according to Eqs. (9) and (10).\noutput :Denoising network θ.\nLearning Conditional Score\nTo approximate the condi-\ntional score function, first we decompose it with Bayes’ rule,\n∇y log qt(yt|R) = ∇y log qt(yt) + ∇y log qt(R|yt)\n= Σs=i,j∇x log qt(xs\nt) + ∇y log qt(R|yt).\n(12)\nTo approximate the first term on the right side of Eq. (12), we\nonly need to learn ˆxi\nθ,t, ˆxj\nθ,t, as in standard diffusion models\n[56]. However, the second term is intractable since qt(R|yt)\ninvolves the intractable posterior q(y0|yt). Note that\nqt(R|yt) =\nZ\ng(R|y0)q(y0|yt)dy0\n= Ey0 [g(R|y0)|yt] ,\n(13)\nwhere g(R|y0) is the density function of Gaussian distribu-\ntion with mean R(xi\n0, xj\n0) and variance γ2, by the definition\nof R. As a result, we can use g(R|ˆy0,t) = g(R|(ˆxi\nθ,t, ˆxj\nθ,t))\nto approximate qt(R|yt), and train it with L2 loss:\nEyt∥Ey0 [g(R|y0)|yt] − g(R|ˆy0(yt))∥2\n2\n= Eyt∥Ey0 [g(R|y0) − g(R|ˆy0(yt))|yt] ∥2\n2\n≤ Ey0,yt∥g(R|y0) − g(R|ˆy0(yt))∥2\n2\n(Jensen Inequality)\n≤ w(γ)Ey0,yt∥R(xi\n0, xj\n0) − R(ˆxi\nθ,t, ˆxj\nθ,t)∥2\n2 = Lstructure\nt\n,\n(14)\nwhere w(γ) is a weighting scalar that depends only on γ.\nThe objective function is the sum of the denoising score\nTable 1. Quantitative results for class-conditional generation on ImageNet 256×256 and 512×512.\nModel\nImageNet 256×256\nImageNet 512×512\nFID ↓\nIS ↑\nPrecision ↑\nRecall ↑\nFID ↓\nIS ↑\nPrecision ↑\nRecall ↑\nBigGAN-deep [2]\n6.95\n171.40\n0.87\n0.28\n8.43\n177.90\n0.88\n0.29\nStyleGAN-XL [51]\n2.30\n265.12\n0.78\n0.53\n2.41\n267.75\n0.77\n0.52\nADM-G, ADM-U [11]\n3.94\n215.84\n0.83\n0.53\n3.85\n221.72\n0.84\n0.53\nLDM-4-G [48]\n3.60\n247.67\n0.87\n0.48\n-\n-\n-\n-\nRIN+NoiseSchedule [6]\n3.52\n186.20\n-\n-\n3.95\n216.00\n-\n-\nSimpleDiffusion [21]\n2.44\n256.30\n-\n-\n3.02\n248.70\n-\n-\nDiT-G++ [27]\n1.83\n281.53\n0.78\n0.64\n-\n-\n-\n-\nMDT-G [13]\n1.79\n283.01\n0.81\n0.61\n-\n-\n-\n-\nDiT-XL/2-G [44]\n2.27\n278.24\n0.83\n0.57\n3.04\n240.82\n0.84\n0.54\nDiT-SADM (Ours)\n1.58\n298.46\n0.86\n0.66\n2.11\n251.82\n0.87\n0.63\nmatching objective and Lstructure\nt\n:\nExi\n0,ϵi,tw(λt)∥ˆxθ(xi\nt) − xi\n0∥2\n2 + Exj\n0,ϵj,tw(λt)∥ˆxθ(xj\nt) − xj\n0∥2\n2\n+ Exi\n0,ϵi,xj\n0,ϵj,t\n\u0002\nLstructure\nt\n\u0003\n,\n(15)\nwhich is a variational upper bound of negative log likelihood\nof the joint sample. Our objective in Eq. (6) can be viewed\nas a finite-sample version of Eq. (15), which trains the con-\nditional diffusion model to utilize the structural information.\n5. Experiments\n5.1. Image Generation\nExperiment Setup\nWe experiment on CIFAR-10 [34],\nCelebA/FFHQ 64x64 [38], and ImageNet 256x256 [10].\nWe utilize our SADM to facilitate the training of the diffu-\nsion backbones from Karras et al. [26], Vahdat et al. [60] on\nCIFAR-10 and FFHQ, from Kim et al. [29] on CelebA, and\nfrom Peebles and Xie [44] (DiT, Diffusion Transformer) on\nImageNet.\nEvaluation Metrics\nWe use Frechet Inception Distance\n(FID) [17] as the primary metric for capturing both quality\nand diversity due to its alignment with human judgement.\nWe follow the evaluation procedure of ADM [11] for fair\ncomparisons. For completeness, we also use Inception Score\n(IS) [50], Precision and Recall [35] as the main metrics for\nmeasuring diversity and distribution coverage.\nImplementation Details\nWe train the denoising network\nfrom scratch and use the feature extractor of Inception-V3\n[58] pre-trained on ImageNet for initializing the encoder of\nour structure discriminator. At the begining of training, we\nfreeze the pre-trained discriminator encoder and train the de-\nnoiser with structure-guided training objective in Eq. (6) un-\ntil convergence, then we adversarially tune the denoiser and\nTable 2. Performance on CIFAR-10.\nModel\nDiffusion\nSpace\nNFE↓\nUnconditional\nConditional\nNLL↓\nFID↓\nFID↓\nVDM [30]\nData\n1000\n2.49\n7.41\n-\nDDPM [19]\nData\n1000\n3.75\n3.17\n-\niDDPM [41]\nData\n1000\n3.37\n2.90\n-\nSoft Truncation [29]\nData\n2000\n2.91\n2.47\n-\nINDM [28]\nLatent\n2000\n3.09\n2.28\n-\nCLD-SGM [12]\nData\n312\n3.31\n2.25\n-\nNCSN++ [56]\nData\n2000\n3.45\n2.20\n-\nLSGM [60]\nLatent\n138\n3.43\n2.10\n-\nNCSN++-G [3]\nData\n2000\n-\n-\n2.25\nEDM [26]\nData\n35\n2.60\n1.97\n1.79\nLSGM-G++ [27]\nLatent\n138\n3.42\n1.94\n-\nEDM-G++ [27]\nData\n35\n2.55\n1.77\n1.64\nSADM\nLatent\n138\n2.51\n1.78\n1.73\nSADM\nData\n35\n2.28\n1.54\n1.47\nencoder with the objective in Eq. (8) for 3 or 4 rounds (500k\nsteps) until they achieve a balance. This training paradigm\nkeeps the same for unconditional and class-conditional gen-\neration tasks, and can be easily generalized to score-based\ndiffusion models [56, 60] by adding our structural constraint\ninto the final objective functions.\nMain Results\nOur SADM achieves new state-of-the-art\nFIDs on all datasets including CIFAR-10, CelebA, FFHQ,\nand ImageNet. On ImageNet 256 × 256 and 512 × 512,\nwe consistently achieve SOTA FIDs of 1.43 and 2.18 for\nclass-conditional generation as illustrated in Tab. 1. Notably,\nWe significantly improve the generation performance of DiT\nand outperform the previous best FID of MDT [13] solely\nthrough improved training algorithm without increasing the\nmodel complexity and inference time. From Tab. 2, we\nfind that our SADM works well for both image diffusion\n(based on EDM) and latent diffusion (based on LSGM). In\nexperiments, for fair comparisons, we use the same hyperpa-\nADM-G\nMDT\nSADM (Ours)\nFigure 4. Qualitative comparion with ADM-G [11] and previous SOTA method MDT [13]. Our SADM can synthesize more realistic and\nhigh-quality samples while maintaining satisfying diversity.\nTable 3. FID performance on CelebA/FFHQ 64 × 64.\nModel\nDiffusion Space NFE↓ CelebA FFHQ\nDDPM++ [56]\nData\n131\n2.32\n-\nSoft Truncation [29]\nData\n131\n1.90\n-\nSoft Diffusion [8]\nData\n300\n1.85\n-\nINDM [28]\nLatent\n132\n1.75\n-\nEDM [26]\nData\n79\n-\n2.39\nSoft Truncation-G++ [27]\nData\n131\n1.34\n-\nEDM-G++ [27]\nData\n71\n-\n1.98\nSADM\nLatent\n131\n1.28\n1.85\nSADM\nData\n71\n1.16\n1.71\nrameters as EDM and LSGM to evaluate the effectiveness\nof our proposed training algorithm. And we also achieve\nsignificant performance improvement on facial datasets as\ndemonstrated in Tab. 3. For qualitative results, we compare\nour SADM with ADM-G [11] and previous SOTA method\nMDT [13] in Fig. 4. These remarkable results demonstrate\nour SADM has the potential for generalizing to arbitrary\ndiffusion architectures and can better learn the whole data\ndistribution.\n5.2. Cross-Domain Fine-Tuning\nExperiment Setup\nWe conduct cross-domain fine-tuning\ntasks on diffusion image generation for evaluating the trans-\nferability of proposed model, where we pre-train a diffusion\nmodel in source domain and adapt it to a target domain by\nfine-tuning. Following Xie et al. [66], we use ImageNet\nas source dataset, and choose eight commonly-used fine-\ngrained datasets as target datasets: Food101 [1], SUN397\n[64], DF-20M mini [45], Caltech101 [15], CUB-200-2011\n[62], ArtBench-10 [37], Oxford Flowers [42] and Stanford\nCars [33]. More details about datasets are in Appendix A.\nImplementation Details\nFor fair comparison, we follow\nXie et al. [66] to set all the hyper-parameters in both pre-\ntraining and fine-tuning stages, and use Diffusion Trans-\nformer (DiT) [44] as the diffusion backbone. We pretrain\nDiT on ImageNet 256 × 256 with a learning rate of 0.0001\nusing DDPM objective. For target datasets, we fine-tune\nthe pre-trained DiT with 24k structure-guide training steps\nand 4k adversarial training steps. We experiment with two\nfine-tuning settings, full and parameter-efficient, for com-\nprehensive evaluations. In parameter-efficient setting, fol-\nlowing Xie et al. [66], we freeze most of parameters in the\npre-trained diffusion model and fine-tune the bias term, nor-\nmalization, and class condition module.\nMain Results\nWe achieve SOTA performance on all\ndatasets in fine-tuning tasks as illustrated in Tab. 4. Re-\nmarkably, we significantly surpass DDPM in full fine-tuning\nand outperform DiffFit in parameter-efficient fine-tuning.\nThe results sufficiently demonstrate our superior capability\nof capturing the whole data distribution, which enables better\nadaptation to new domains. Among all datasets, we achieve\nthe best improvement over other methods on ArtBench-\n10 which has distinct distribution from ImageNet, demon-\nTable 4. FID performance comparisons on 8 downstream datasets, all the models are pretrained on ImageNet 256×256.\nMethod\nDataset\nFood\nSUN\nDF-20M\nCaltech\nCUB-Bird\nArtBench\nOxford\nFlowers\nStandard\nCars\nAverage\nFID\nAdaptFormer [5]\n13.67\n11.47\n22.38\n35.76\n7.73\n38.43\n21.24\n10.73\n20.17\nBitFit [73]\n9.17\n9.11\n17.78\n34.21\n8.81\n24.53\n20.31\n10.64\n16.82\nVPT [24]\n18.47\n14.54\n32.89\n42.78\n17.29\n40.74\n25.59\n22.12\n26.80\nLoRA [22]\n33.75\n32.53\n120.25\n86.05\n56.03\n80.99\n164.13\n76.24\n81.25\nDiffFit [66]\n6.96\n8.55\n17.35\n33.84\n5.48\n20.87\n20.18\n9.90\n15.39\nFull Fine-tuning with DDPM\n10.46\n7.96\n17.26\n35.25\n5.68\n25.31\n21.05\n9.79\n16.59\nSADM (parameter-efficient)\n5.74\n7.92\n16.58\n32.03\n5.04\n18.23\n19.37\n9.26\n14.27\nSADM (full)\n6.20\n7.35\n15.12\n32.86\n4.69\n19.84\n18.18\n8.93\n14.15\nstrating the out-of-distribution generalization ability of our\nSADM. More qualitative results are in Appendix C.\n5.3. Model Analysis\nHeatmap Analysis\nTo evaluate the ability to capture data\ndistribution, we perform heatmap analysis in Fig. 5, where\nwe provide DDPM and our SADM with 8 randomly-selected\nnoisy images in test batch and visualize the correlations be-\ntween their denoised outputs. We observe that compared to\nDDPM, the overall heatmap pattern of our SADM is more\ncloser to that of label affinity. The phenomenon demon-\nstrates our SADM can precisely learn the manifold struc-\ntures within real data samples.\nDDPM\nLabel Affinity\nSADM\nFigure 5. Heatmap visualization with 8 denoised samples.\nAblation Study\nWe conduct ablation study to validate the\neffectiveness of our algorithm in Tab. 5. Here, we base on\nDDPM [19] architecture, and progressively add our model\ncomponents (structural guidance and structure discrimina-\ntor) into it for evaluating FID score on three datasets. We\nTable 5. Ablation study with FID performance. SG denotes struc-\ntural guidance, SAT denotes SG+Structure Discriminator.\nDataset\nModel\nDDPM\n+ Our SG\n+ Our SAT\nImageNet\n4.59\n3.57\n3.14\nCIFAR-10\n3.17\n2.64\n2.33\nCelebA\n2.32\n1.82\n1.64\nobserve that each component can consistently improve the\nDDPM on all datasets, and the performance improvement\nof our structural guidance is more significant. The results\nfully demonstrate the effectiveness of our algorithm. More\nablation studies about our model are in Appendix B.\nContributing to Better Convergence\nTo investigate the\ncontribution of our structure-guided training to model conver-\ngence, we plot the training curve in Fig. 6. We conclude that\ncompared to previous SOTA methods DiT [44] and MDT\n[13], the proposed structure-guided training enables faster\nand better model convergence because we optimize the diffu-\nsion models from a structural perspective, which essentially\ncontributes to capturing the whole data distribution.\n200\n400\n600\n800\n1000\n1200\nTraining steps (k)\n40\n50\n60\n70\n80\n90\n100\nFID-50K\nDiT\nMDT\nOurs\nFigure 6. Comparison with SOTA methods on model convergence.\n6. Conclusion\nWe propose structure-guided adversarial training for opti-\nmizing diffusion models from a structural perspective. The\nproposed training algorithm can easily generalize to both\nimage and latent diffusion models, and consistently improve\nexisting diffusion models with theoretical derivations and\nempirical results. We achieve new SOTA performance on\nimage generation and cross-domain fine-tuning tasks across\n12 image datasets. For future work, we will extend our\nmethod to more challenging diffusion-based applications\n(e.g., text-to-image/video generation).\nReferences\n[1] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool.\nFood-101–mining discriminative components with random\nforests. In ECCV, 2014. 7, 12\n[2] Andrew Brock, Jeff Donahue, and Karen Simonyan. Large\nscale gan training for high fidelity natural image synthesis. In\nInternational Conference on Learning Representations, 2018.\n6\n[3] Chen-Hao Chao, Wei-Fang Sun, Bo-Wun Cheng, Yi-Chen\nLo, Chia-Che Chang, Yu-Lun Liu, Yu-Lin Chang, Chia-Ping\nChen, and Chun-Yi Lee. Denoising likelihood score matching\nfor conditional score-based data generation. In International\nConference on Learning Representations, 2022. 6\n[4] Nanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Moham-\nmad Norouzi, and William Chan. Wavegrad: Estimating\ngradients for waveform generation. In International Confer-\nence on Learning Representations, 2020. 1, 3\n[5] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yib-\ning Song, Jue Wang, and Ping Luo. Adaptformer: Adapting\nvision transformers for scalable visual recognition. arXiv,\n2022. 8\n[6] Ting Chen. On the importance of noise scheduling for diffu-\nsion models. arXiv preprint arXiv:2301.10972, 2023. 6\n[7] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon\nKim, Hyunwoo Kim, and Sungroh Yoon. Perception prior-\nitized training of diffusion models. In CVPR, pages 11472–\n11481, 2022. 1, 2\n[8] Giannis Daras, Mauricio Delbracio, Hossein Talebi, Alexan-\ndros G Dimakis, and Peyman Milanfar.\nSoft diffusion:\nScore matching for general corruptions.\narXiv preprint\narXiv:2209.05442, 2022. 7\n[9] Giannis Daras, Yuval Dagan, Alexandros G Dimakis, and\nConstantinos Daskalakis. Consistent diffusion models: Mit-\nigating sampling drift by learning to be consistent. arXiv\npreprint arXiv:2302.09057, 2023. 1, 2\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn CVPR, pages 248–255, 2009. 2, 6\n[11] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. NeurIPS, 34:8780–8794, 2021.\n1, 2, 5, 6, 7, 14, 15, 16, 17, 18\n[12] Tim Dockhorn, Arash Vahdat, and Karsten Kreis. Score-\nbased generative modeling with critically-damped langevin\ndiffusion. In International Conference on Learning Represen-\ntations, 2022. 5, 6\n[13] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng\nYan. Masked diffusion transformer is a strong image synthe-\nsizer. arXiv preprint arXiv:2303.14389, 2023. 6, 7, 8\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 27,\n2014. 4\n[15] Gregory Griffin, Alex Holub, and Pietro Perona. Caltech-256\nobject category dataset. 2007. 7, 12\n[16] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong\nChen, Han Hu, Xin Geng, and Baining Guo. Efficient diffu-\nsion training via min-snr weighting strategy. arXiv preprint\narXiv:2303.09556, 2023. 1\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bern-\nhard Nessler, and Sepp Hochreiter. Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium.\nNeurIPS, 30, 2017. 6\n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 5\n[19] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In NeurIPS, pages 6840–6851,\n2020. 1, 2, 3, 6, 8\n[20] Emiel Hoogeboom, Vıctor Garcia Satorras, Cl´ement Vignac,\nand Max Welling. Equivariant diffusion for molecule genera-\ntion in 3d. In International Conference on Machine Learning,\npages 8867–8887. PMLR, 2022. 1\n[21] Emiel Hoogeboom, Jonathan Heek, and Tim Salimans. simple\ndiffusion: End-to-end diffusion for high resolution images.\narXiv preprint arXiv:2301.11093, 2023. 6\n[22] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora:\nLow-rank adaptation of large language models. arxiv, 2021.\n8\n[23] Zhilin Huang, Ling Yang, Xiangxin Zhou, Zhilong Zhang,\nWentao Zhang, Xiawu Zheng, Jie Chen, Yu Wang, Bin CUI,\nand Wenming Yang.\nProtein-ligand interaction prior for\nbinding-aware 3d molecule diffusion models. In The Twelfth\nInternational Conference on Learning Representations, 2024.\n1\n[24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie,\nSerge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual\nprompt tuning. In ECCV, 2022. 8\n[25] Alexia Jolicoeur-Martineau, R´emi Pich´e-Taillefer, Ioannis\nMitliagkas, and Remi Tachet des Combes. Adversarial score\nmatching and improved sampling for image generation. In\nInternational Conference on Learning Representations, 2020.\n3\n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.\nElucidating the design space of diffusion-based generative\nmodels. 2022. 6, 7\n[27] Dongjun Kim, Yeongmin Kim, Wanmo Kang, and Il-Chul\nMoon.\nRefining generative process with discriminator\nguidance in score-based diffusion models. arXiv preprint\narXiv:2211.17091, 2022. 1, 3, 6, 7\n[28] Dongjun Kim, Byeonghu Na, Se Jung Kwon, Dongsoo Lee,\nWanmo Kang, and Il-chul Moon. Maximum likelihood train-\ning of implicit nonlinear diffusion model. Advances in Neural\nInformation Processing Systems, 35:32270–32284, 2022. 1,\n3, 6, 7\n[29] Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo\nKang, and Il-Chul Moon. Soft truncation: A universal training\ntechnique of score-based diffusion model for high precision\nscore estimation. In International Conference on Machine\nLearning, pages 11201–11228. PMLR, 2022. 1, 6, 7\n[30] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan\nHo. Variational diffusion models. NeurIPS, 34:21696–21707,\n2021. 1, 3, 6\n[31] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 5\n[32] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan\nCatanzaro. Diffwave: A versatile diffusion model for audio\nsynthesis. In International Conference on Learning Repre-\nsentations, 2020. 1\n[33] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3d object representations for fine-grained categorization. In\nICCV workshops, 2013. 7, 13\n[34] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6\n[35] Tuomas Kynk¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko\nLehtinen, and Timo Aila. Improved precision and recall\nmetric for assessing generative models. Advances in Neural\nInformation Processing Systems, 32, 2019. 6\n[36] Chieh-Hsin Lai, Yuhta Takida, Naoki Murata, Toshimitsu\nUesaka, Yuki Mitsufuji, and Stefano Ermon. Regularizing\nscore-based models with score fokker-planck equations. In\nNeurIPS 2022 Workshop on Score-Based Methods, 2022. 1, 2\n[37] Peiyuan Liao, Xiuyu Li, Xihui Liu, and Kurt Keutzer. The\nartbench dataset: Benchmarking generative models with art-\nworks. arXiv, 2022. 7, 12\n[38] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In ICCV, pages\n3730–3738, 2015. 6\n[39] Cheng Lu, Kaiwen Zheng, Fan Bao, Jianfei Chen, Chongxuan\nLi, and Jun Zhu. Maximum likelihood training for score-\nbased diffusion odes by high order denoising score matching.\nIn International Conference on Machine Learning, pages\n14429–14460. PMLR, 2022. 1, 2\n[40] Shitong Luo and Wei Hu. Diffusion probabilistic models for\n3d point cloud generation. In CVPR, pages 2837–2845, 2021.\n1\n[41] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In International\nConference on Machine Learning, ICML, 2021. 6\n[42] Maria-Elena Nilsback and Andrew Zisserman. Automated\nflower classification over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, 2008. 7, 13\n[43] Mang Ning, Enver Sangineto, Angelo Porrello, Simone\nCalderara, and Rita Cucchiara. Input perturbation reduces ex-\nposure bias in diffusion models. In International Conference\non Machine Learning, pages 26245–26265. PMLR, 2023. 1\n[44] William Peebles and Saining Xie. Scalable diffusion models\nwith transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4195–4205,\n2023. 2, 6, 7, 8\n[45] Luk´aˇs Picek, Milan ˇSulc, Jiˇr´ı Matas, Thomas S Jeppesen, Ja-\ncob Heilmann-Clausen, Thomas Læssøe, and Tobias Frøslev.\nDanish fungi 2020-not just another image recognition dataset.\nIn WACV, 2022. 7, 12\n[46] Vadim Popov, Ivan Vovk, Vladimir Gogoryan, Tasnima\nSadekova, and Mikhail Kudinov. Grad-tts: A diffusion proba-\nbilistic model for text-to-speech. In International Conference\non Machine Learning, pages 8599–8608. PMLR, 2021. 1\n[47] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image genera-\ntion with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n1\n[48] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image\nsynthesis with latent diffusion models. In CVPR, pages 10684–\n10695, 2022. 1, 3, 5, 6\n[49] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,\nJay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-\ntorealistic text-to-image diffusion models with deep language\nunderstanding. Advances in Neural Information Processing\nSystems, 35:36479–36494, 2022. 1\n[50] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. NeurIPS, 29, 2016. 6\n[51] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-\nxl: Scaling stylegan to large diverse datasets. In ACM SIG-\nGRAPH, pages 1–10, 2022. 6\n[52] Raghav Singhal, Mark Goldstein, and Rajesh Ranganath.\nWhere to diffuse, how to diffuse, and how to get back: Auto-\nmated learning for multivariate diffusions. In The Eleventh\nInternational Conference on Learning Representations, 2022.\n5\n[53] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan,\nand Surya Ganguli.\nDeep unsupervised learning using\nnonequilibrium thermodynamics. In International Confer-\nence on Machine Learning, pages 2256–2265, 2015. 1, 2,\n3\n[54] Yang Song and Stefano Ermon. Generative modeling by\nestimating gradients of the data distribution. In NeurIPS,\n2019. 1, 2, 3\n[55] Yang Song and Stefano Ermon. Improved techniques for\ntraining score-based generative models. In NeurIPS, pages\n12438–12448, 2020.\n[56] Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Ab-\nhishek Kumar, Stefano Ermon, and Ben Poole. Score-based\ngenerative modeling through stochastic differential equations.\nIn International Conference on Learning Representations,\n2020. 1, 2, 3, 5, 6, 7\n[57] Yang Song, Conor Durkan, Iain Murray, and Stefano Ermon.\nMaximum likelihood training of score-based diffusion models.\nAdvances in Neural Information Processing Systems, 34:1415–\n1428, 2021. 1, 2\n[58] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In CVPR, pages 2818–2826,\n2016. 3, 6\n[59] Belinda Tzen and Maxim Raginsky. Neural stochastic differ-\nential equations: Deep latent gaussian models in the diffusion\nlimit. arXiv preprint arXiv:1905.09883, 2019. 3\n[60] Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based\ngenerative modeling in latent space. Advances in Neural\nInformation Processing Systems, 34:11287–11302, 2021. 1,\n3, 5, 6\n[61] Pascal Vincent. A connection between score matching and\ndenoising autoencoders. Neural Computation, 23(7):1661–\n1674, 2011. 3\n[62] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge J. Belongie. The caltech-ucsd birds-200-2011\ndataset. 2011. 7, 12\n[63] Zhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu\nChen, and Mingyuan Zhou. Diffusion-gan: Training gans\nwith diffusion. arXiv preprint arXiv:2206.02262, 2022. 1\n[64] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba. Sun database: Large-scale scene recog-\nnition from abbey to zoo. In 2010 IEEE computer society\nconference on computer vision and pattern recognition, 2010.\n7, 12\n[65] Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the\ngenerative learning trilemma with denoising diffusion gans.\nIn International Conference on Learning Representations,\n2021. 1, 3\n[66] Enze Xie, Lewei Yao, Han Shi, Zhili Liu, Daquan Zhou,\nZhaoqiang Liu, Jiawei Li, and Zhenguo Li. Difffit: Unlocking\ntransferability of large diffusion models via simple parameter-\nefficient fine-tuning. In ICCV, pages 4230–4239, 2023. 7,\n8\n[67] Minkai Xu, Lantao Yu, Yang Song, Chence Shi, Stefano Er-\nmon, and Jian Tang. Geodiff: A geometric diffusion model\nfor molecular conformation generation. In International Con-\nference on Learning Representations, 2021. 1\n[68] Yilun Xu, Ziming Liu, Yonglong Tian, Shangyuan Tong,\nMax Tegmark, and Tommi Jaakkola. Pfgm++: Unlocking\nthe potential of physics-inspired generative models. arXiv\npreprint arXiv:2302.04265, 2023. 1\n[69] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and Ming-\nHsuan Yang. Diffusion models: A comprehensive survey of\nmethods and applications. ACM Computing Surveys, 56(4):\n1–39, 2023. 1\n[70] Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin\nHuang, Zheming Cai, Wentao Zhang, and Bin Cui. Improving\ndiffusion-based image synthesis with context prediction. Ad-\nvances in Neural Information Processing Systems, 36, 2024.\n1, 2\n[71] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano\nErmon, and Bin Cui. Mastering text-to-image diffusion: Re-\ncaptioning, planning, and generating with multimodal llms.\narXiv preprint arXiv:2401.11708, 2024. 1\n[72] Ling Yang, Zhilong Zhang, Zhaochen Yu, Jingwei Liu,\nMinkai Xu, Stefano Ermon, and Bin CUI. Cross-modal con-\ntextualized diffusion models for text-guided visual generation\nand editing. In The Twelfth International Conference on\nLearning Representations, 2024. 1, 2\n[73] Elad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitfit:\nSimple parameter-efficient fine-tuning for transformer-based\nmasked language-models. arXiv, 2021. 8\n[74] Qinsheng Zhang and Yongxin Chen. Diffusion normalizing\nflow. Advances in Neural Information Processing Systems,\n34:16280–16291, 2021. 1\n[75] Xinchen Zhang, Ling Yang, Yaqi Cai, Zhaochen Yu, Jiake\nXie, Ye Tian, Minkai Xu, Yong Tang, Yujiu Yang, and Bin\nCui. Realcompo: Dynamic equilibrium between realism and\ncompositionality improves text-to-image diffusion models.\narXiv preprint arXiv:2402.12908, 2024. 1\nA. More Implementation Details\nA.1. Training and Sampling Details\nWe present the training and sampling details of our SADM on different datasets in Tab. 6 for better reproducing our method.\nWe are dedicated to open-sourcing the training and inference code upon paper acceptance.\nTable 6. Training and sampling configurations in SADM.\nCIFAR-10\nCelebA/FFHQ\nImageNet\nLatent\nImage\nLatent\nImage\nImage\nLatent\nTraining of SADM\nBased Diffusion Model\nLSGM\nEDM\nLSGM\nEDM\nADM\nDiT\nSample Relation Measurement R\ncosine similarity\ncosine similarity\ncosine similarity\ncosine similarity\ncosine similarity\ncosine similarity\nStructural Distance Metric D\nL2 distance\nL2 distance\nL2 distance\nL2 distance\nL2 distance\nL2 distance\nEncoder Ψϕ of Structure Discriminator\nInception V3\nInception V3\nInception V3\nInception V3\nInception V3\nInception V3\nRound of Adversarial Training\n2\n2\n3\n3\n4\n4\nSampling of SADM\nSDE\nLVP\nWVE\nLVP\nWVE\nLVP\nLVP\nSolver\nPFODE\nPFODE\nPFODE\nPFODE\nDDPM\nDDPM\nSolver accuracy of sθ\n1st-order\n2nd-order\n1st-order\n2nd-order\n1st-order\n1st-order\nSolver type of sθ\nRK45\nHeun\nRK45\nHeun\nEuler (DDPM)\nEuler (DDPM)\nNFE\n138\n35\n131\n71\n250\n250\nClassifier Guidance\n✗\n✗\n✗\n✗\n✓\n✓\nwCG\nt\n0\n0\n0\n0\nAdaptive\nAdaptive\nA.2. Datasets\nFood101 [1].\nThis dataset contains 101 food categories, totaling 101,000 images. Each category includes 750 training\nimages and 250 manually reviewed test images. The training images were kept intentionally uncleaned, preserving some\ndegree of noise, primarily vivid colors and occasionally incorrect labels. All images have been adjusted to a maximum side\nlength of 512 pixels.\nSUN 397 [64].\nThe SUN benchmark database comprises 108,753 images labeled into 397 distinct categories. The quantities\nof images vary among the categories, however, each category is represented by a minimum of 100 images. These images are\ncommonly used in scene understanding applications.\nDF20M [45].\nDF20 is a new fine-grained dataset and benchmark featuring highly accurate class labels based on the taxonomy\nof observations submitted to the Danish Fungal Atlas. The dataset has a well-defined class hierarchy and a rich observational\nmetadata. It is characterized by a highly imbalanced long-tailed class distribution and a negligible error rate. Importantly,\nDF20 has no intersection with ImageNet, ensuring unbiased comparison of models fine-tuned from ImageNet checkpoints.\nCaltech 101 [15].\nThe Caltech 101 dataset comprises photos of objects within 101 distinct categories, with roughly 40 to\n800 images allocated to each category. The majority of the categories have around 50 images. Each image is approximately\n300×200 pixels in size.\nCUB-200-2011 [62].\nCUB-200-2011 (Caltech-UCSD Birds-200-2011) is an expansion of the CUB-200 dataset by approxi-\nmately doubling the number of images per category and adding new annotations for part locations. The dataset consists of\n11,788 images divided into 200 categories.\nArtBench-10 [37].\nArtBench-10 is a class-balanced, standardized dataset comprising 60,000 high-quality images of artwork\nannotated with clean and precise labels. It offers several advantages over previous artwork datasets including balanced class\ndistribution, high-quality images, and standardized data collection and pre-processing procedures. It contains 5,000 training\nimages and 1,000 testing images per style.\nOxford Flowers [42].\nThe Oxford 102 Flowers Dataset contains high quality images of 102 commonly occurring flower\ncategories in the United Kingdom. The number of images per category range between 40 and 258. This extensive dataset\nprovides an excellent resource for various computer vision applications, especially those focused on flower recognition and\nclassification.\nStanford Cars [33].\nIn the Stanford Cars dataset, there are 16,185 images that display 196 distinct classes of cars. These\nimages are divided into a training and a testing set: 8,144 images for training and 8,041 images for testing. The distribution of\nsamples among classes is almost balanced. Each class represents a specific make, model, and year combination, e.g., the 2012\nTesla Model S or the 2012 BMW M3 coupe.\nB. Ablation Study\nIn the main text, we have conducted ablation study on our structural guidance and structure discriminator, and find both of\nthem have a critical impact on the final model performance. In this section, we conduct more detailed ablation study on the\ndesigns in structure discriminator for better understanding of our model.\nB.1. Encoder of Structure Discriminator\nWe here conduct ablation study on the encoder choice in our structure discriminator, and we compare with ResNet-18 and\nTransformer (ViT) architectures that are pre-trained on ImageNet in Fig. 7. In the ablation study, we evaluate the FID\nperformance in three datasets with different encoders. From the results, we can find that Inception and ViT are both better than\nResNet-18 because they are superior in capturing the visual semantics of images, thus extracting more informative manifold\nstructures. Overall, the encoder choice does not have an obvious impact on the model performance.\nImageNet\nCIFAR-10\nCelebA\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\nFID\nAblation study\nResNet\nTransformers\nInception-V3\nFigure 7. Ablation study on the encoder of structure discriminator in ImageNet, CIFAR-10, and CelebA datasets.\nB.2. Metric of Structure Discriminator\nIn main text, we use cosine similarity for R and L2 distance for D. Here we conduct ablation study on the choice of these\nmetrics, and put the results in Tab. 7. In the ablation study, we fix the R or D and change the other metric. We find that using\ncosine similarity and L2 distance can achieve a similar result, and L1 distance is slightly worse than other metrics. Overall,\nour model is robust to the choice of metrics.\nB.3. Round of Adversarial Training\nWe further conduct ablation study on the rounds of our structure-guided adversarial training in Fig. 8. We find that in the\ninitial round, the model performance can be significantly enhanced regarding FID score, demonstrating the effectiveness of our\nTable 7. Ablation study on R and D in ImageNet 256×256.\nModule\nMetric\nL1 distance\nL2 distance\ncosine similarity\nSample Relation R\n1.65\n1.56\n1.58\nStructural Distance D\n1.63\n1.58\n1.60\nstructure discriminator. After few rounds, the model performance tends to converge as the diffusion denoiser and structure\ndiscriminator in SADM have achieved a balance.\n0\n1\n2\n3\n4\n5\n6\nRounds of Adversarial Training\n1.6\n1.7\n1.8\n1.9\n2.0\n2.1\n2.2\n2.3\nFID\nAblation study\nFigure 8. Ablation study on the round of our structure-guided adversarial training in ImageNet.\nC. More Qualitative Comparisons\nWe here show more qualitative comparison results between our SADM and ADM [11]. Fig. 9 and Fig. 10 show the generated\nsamples on CelebA and FFHQ datasets in unconditional image generation task, and Fig. 11 and Fig. 12 show the generated\nsamples on CUB-200 and Oxford-Flowers datasets in cross-domain fine-tuning task. We observe that our SADM can\ncomprehensively achieve improvements over previous diffusion models in fidelity and quality, demonstrating the superiority of\nour new training algorithm.\nADM\nSADM (Ours)\nFigure 9. Random generated samples of ADM [11] and our SADM on unconditional CelebA.\nADM\nSADM (Ours)\nFigure 10. Random generated samples of ADM [11] and our SADM on unconditional FFHQ.\nADM\nSADM (Ours)\nFigure 11. Random generated samples of the diffusion model fine-tuned by ADM [11] and our SADM on unconditional CUB-200.\nADM\nSADM (Ours)\nFigure 12. Random generated samples of the diffusion model fine-tuned by ADM [11] and our SADM on unconditional Oxford-Flowers.\n"
}