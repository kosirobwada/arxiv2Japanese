{
    "optim": "Deterministic Cache-Oblivious Funnelselect\nGerth Stølting Brodal #\nAarhus University, Denmark\nSebastian Wild #\nUniversity of Liverpool, UK\nAbstract\nIn the multiple-selection problem one is given an unsorted array S of N elements and an array\nof q query ranks r1 < · · · < rq, and the task is to return, in sorted order, the q elements in S of\nrank r1, . . . , rq, respectively. The asymptotic deterministic comparison complexity of the problem\nwas settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O complexity\nwas achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we presented a cache-oblivious\nalgorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from\nthe cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop\nand Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on sampling for\ncheaply finding many good pivots. In this paper we present deterministic funnelselect, achieving\nthe same optional I/O complexity cache-obliviously without randomization. Our new algorithm\nessentially replaces a single (in expectation) reversed-funnel computation using random pivots\nby a recursive algorithm using multiple reversed-funnel computations. To meet the I/O bound,\nthis requires a carefully chosen subproblem size based on the entropy of the sequence of query\nranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized\nfunnelselect. The resulting worst-case I/O bound is O\u0000Pq+1\ni=1\n∆i\nB · logM/B\nN\n∆i + N\nB\n\u0001\n, where B is the\nexternal memory block size, M ≥ B1+ε is the internal memory size, for some constant ε > 0, and\n∆i = ri − ri−1 (assuming r0 = 0 and rq+1 = N + 1).\n2012 ACM Subject Classification Theory of computation → Design and analysis of algorithms\nKeywords and phrases Multiple selection, cache-oblivious algorithm, entropy bounds\nFunding Gerth Stølting Brodal: Independent Research Fund Denmark, grant 9131-00113B.\nSebastian Wild: Engineering and Physical Sciences Research Council grant EP/X039447/1.\n1\nIntroduction\nWe present the first optimal deterministic cache-oblivious algorithm for the multiple-selection\nproblem. In the multiple-selection problem one is given an unsorted array S of N elements\nand an array R of q query ranks in increasing order r1 < · · · < rq, and the task is to return,\nin sorted order, the q elements of S of rank r1, . . . , rq, respectively; (see Figure 1 for an\nexample).\nOn top of immediate applications, the multiple-selection problem is of interest as it\ngives a natural common generalization of (single) selection by rank (using a single query\nrank r1 = r) and fully sorting an array (corresponding to selecting every index as a query\nrank, i.e., q = N and ri = i for i = 1, . . . , N). It thus allows us to quantitatively study\nS 67\n1\n30\n2\n45\n3\n33\n4\n15\n5\n99\n6\n26\n7\n90\n8\n55\n9\n9\n10\n96\n11\n45\n12\n95\n13\n31\n14\n3\n15\nr1 r2 r3\nr4\n3\n1\n9\n2\n15\n3\n26\n4\n30\n5\n31\n6\n33\n7\n45\n8\n45\n9\n55\n10\n67\n11\n90\n12\n95\n13\n96\n14\n99\n15\nr1 r2 r3\nr4\nimaginary sorted S\nFigure 1 Example input with N = 15, q = 4 and R[1..q] = [1, 2, 3, 8]. The expected output 3, 9,\n15, 45 is obvious from the sorted array (right). (The sorted array is for illustration only; the goal of\nefficient multiple-selection algorithms is to avoid ever fully sorting the input.)\narXiv:2402.17631v1  [cs.DS]  27 Feb 2024\n2\nDeterministic Cache-Oblivious Funnelselect\nthe transition between these two foundational problems, which are of different complexity\nand each have their distinct set of algorithms. For example, the behavior of selection and\nsorting with respect to external memory is quite different: For single selection, the textbook\nmedian-of-medians algorithm [4] simultaneously works with optimal cost in internal memory,\nexternal memory, and the cache-oblivious model (models are defined below). For sorting,\nby contrast, the introduction of each model required a substantially modified algorithm to\nachieve optimal costs: Standard binary mergesort is optimal in internal memory, but requires\n≈ M/B-way merging to be optimal in external memory, where M is the internal memory\nsize and B the external memory block size, measured in elements [1]; achieving the same\ncache obliviously, i.e., without knowledge of B and M, requires the judiciously chosen buffer\nsizes from the recursive constructions of funnelsort [11].\nSince multiple selection simultaneously generalizes both problems, it is not surprising\nthat also here subsequent refinements were necessary going from internal to external to\ncache-oblivious; the most recent result being our algorithm funnelselect [6]. However, all\nalgorithms mentioned above for single selection and sorting are deterministic. By constrast,\nfunnelselect is inherently relying on randomization and known deterministic external-memory\nalgorithms [2, 14] are crucially relying on the knowledge of M and B. Prior to this work it thus\nremained open whether a single deterministic cache-oblivious algorithm exists that smoothly\ninterpolates between selection and sorting without having to resort to randomization.\nIn this paper, we answer this question in the affirmative. Our algorithm determinis-\ntic funnelselect draws on techniques from cache-oblivious sorting (funnelsort) and existing\nmultiple-selection algorithms, but it follows a rather different approach to our earlier random-\nized algorithm [6] and previous (cache-conscious) external-memory algorithms. A detailed\ncomparison is given below.\n1.1\nModel of computation and previous work\nOur results are in the cache-oblivious model of Frigo, Leiserson, Prokop and Ramachan-\ndran [12], a hierarchical-memory model with an infinite external memory and an internal\nmemory of capacity M elements, where data is transferred between internal and external\nmemory in blocks of B consecutive elements. Algorithms are compared by their I/O cost,\ni.e., the number of block transfers or I/Os (input/output operations). This is similar to the\nexternal-memory model by Aggarwal and Vitter [1]. Crucially, in the cache-oblivious model,\nalgorithms do not know M and B and I/Os are assumed to be performed automatically\nby an optimal (offline) paging algorithm. Cache-oblivious algorithms hence work for any\nparameters M and B, and they even adapt to multi-level memory hierarchies (under certain\nconditions [12]).\nThe multiple-selection problem was first formally addressed by Chambers [7], who\nconsidered it a generalization of quickselect [13]. Prodinger [16] proved that Chambers’\nalgorithm achieves an optimal expected running time up to constant factors: O(B +N), where\nB = Pq+1\ni=1 ∆i lg N\n∆i with ∆i = ri −ri−1, for 1 ≤ i ≤ q +1, assuming r0 = 0 and rq+1 = N +1,\nand lg denoting the binary logarithm. We call B the (query-rank) entropy of the sequence of\nquery ranks [2]. It should be noted that B + N = O(N(1 + lg q)), but the latter bound does\nnot take the location of query ranks into account; for example, if q = Θ\n\u0000√n\n\u0001\nqueries are\nin a range of size O(N/ lg N), i.e., rq − r1 = O(N/ lg N), then the entropy bound is O(N)\nwhereas the latter N(1 + lg q) = Θ(N lg N).\nDobkin and Munro [8] showed that B − O(N) comparisons are necessary to find all ranks\nr1, . . . , rq (in the worst case). Deterministic algorithms with that same O(B + N) running\ntime are also known [8, 15], but as for single selection, the deterministic algorithms were\nG. S. Brodal and S. Wild\n3\nTable 1 Algorithms for selection and multiple selection. CO = cache-oblivious, E = expected,\nwc = worst-case bounds. Note that Barbay et al. assume a tall cache M ≥ B1+ε, whereas Hu et al.\ndo not.\nReference\nComparisons\nI/Os\nComments\nSingle selection\nHoare [13]\nE\n2 ln 2B + 2N + o(N)\nO(N/B)\nCO, randomized\nFloyd & Rivest [10]\nE\nN + min{r, N−r} + o(N)\nO(N/B)\nCO, randomized\nBlum et al. [4]\nwc\n5.4305N\nO(N/B)\nCO, deterministic\nSchönhage et al. [17]\nwc\n3N + o(N)\n?\nmedian, deterministic\nDor & Zwick [9]\nwc\n2.95 + o(N)\n?\nmedian, deterministic\nMultiple selection\nChambers [7, 16]\nE\n2 ln 2B + O(N)\nO((B + N)/B)\nCO, randomized\nDobkin & Munro [8]\nwc\n3B + O(N)\nO((B + N)/B)\nCO, deterministic\nKaligosi et al. [15]\nwc\nB + o(B) + O(N)\nO((B + N)/B)\nCO, deterministic\nHu et al. [14]\nwc\nO(N lg(q))\nO(N/B logM/B(q/B))\ndeterministic\nwc\nO(B + N)\nO(BI/O + N/B)\n(from closer analysis)\nBarbay et al. [2]\nwc\nB + o(B) + O(N)\nO(BI/O + N/B)\nonline, determ., M ≥ B1+ε\nBrodal & Wild [6]\nE\nO(B + N)\nO(BI/O + N/B)\nCO, randomized, M ≥ B1+ε\nThis paper\nwc\nO(B + N)\nO(BI/O + N/B)\nCO, deterministic, M ≥ B1+ε\npresented later than the randomized algorithms and require more sophistication. Multiple\nselection in external-memory was studied by Hu et al. [14] and Barbay et al. [2]. Their\nalgorithms have an I/O cost of O\n\u0000BI/O + N\nB\n\u0001\n, where the “I/O entropy” BI/O =\nB\nB lg(M/B).\nAn I/O cost of Ω(BI/O) − O( N\nB ) is known to be necessary [2, 6]. A more comprehensive\nhistory of the multiple-selection problem appears in [6]; Table 1 gives an overview.\nWe note that many existing time- and comparison-optimal multiple-selection algorithms\nare actually already cache oblivious, but they are not optimal with respect to the number of\nI/Os performed when analyzed in the cache-oblivious model (the obtained I/O bounds are a\nfactor lg(M/B) away from being optimal).\n1.2\nResult\nOur main result is the cache-oblivious algorithm deterministic funnelselect achieving the\nfollowing efficiency (see Theorem 10 for the full statement and proof).\n▶ Theorem 1. There exists a deterministic cache-oblivious algorithm solving the multiple-\nselection problem using O(B + N) comparisons and O\n\u0000BI/O + N\nB\n\u0001\nI/Os in the worst case,\nassuming a tall cache M ≥ B1+ε.\nAt the high level, our algorithm uses the standard overall idea of a recursive partitioning\nalgorithm and pruning recursive calls containing no rank queries, an idea dating back to the\nfirst algorithm by Chambers [7]. In the cache-aware external-memory model, I/O efficient\nalgorithms are essentially obtained by replacing binary partitioning (as used in [7]) by an\nexternal-memory Θ(M/B)-way partitioning [2, 14]. Unfortunately, in the cache-oblivious\nmodel this is not possible, since the parameters M and B are unknown to the algorithm.\nTo be I/O efficient in the cache oblivious model, both our previous algorithm randomized\nfunnelselect [6] and our new algorithm deterministic funnelselect apply a cache-oblivious\nmulti-way k-partitioner to distribute elements into k buckets given a set of k − 1 pivot\nelements, essentially reversing the computation done by the k-merger used by funnelsort [11].\nThe k-partitioner is a balanced binary tree of k − 1 pipelined binary partitioners.\n4\nDeterministic Cache-Oblivious Funnelselect\nThe key difference between our randomized and deterministic algorithms is that in our\nrandomized algorithm we use a single N Θ(ε)-way partitioner using randomly selected pivots\nand truncate work inside the partitioner for subproblems that (with high probability) will not\ncontain any rank queries. This is done by estimating the ranks of the pivots through sampling\nand pruning subproblems estimated to be sufficiently far from any query ranks. In our\ndeterministic version, we choose k smaller and deterministically compute pivots, such that all\nelements are pushed all the way down through a k-partitioner without truncation (eliminating\nthe need to know the (approximate) ranks of the pivots before the k-partitioning is finished),\nwhile we choose k such that the buckets with unresolved rank queries (that we have to\nrecursive on) in total contain at most half of the elements. To compute k, we apply a linear-\ntime weighted-median finding algorithm on ∆1, . . . , ∆q+1. While randomized funnelselect can\nhandle buckets with unresolved rank queries directly using sorting, deterministic funnelselect\nneeds to recursively perform multiple-selection on the buckets to achieve the desired I/O\nperformance.\n2\nPreliminaries\nThroughout the paper we assume that the input to a multiple-selection algorithm is given\nas two arrays S[1..N] and R[1..q], where S is an unsorted array of N elements from a\ntotally ordered universe, and R is a sorted array r1, . . . , rq of q distinct query ranks, where\n1 ≤ r1 < · · · < rq ≤ N. The array S is allowed to contain duplicate elements. Our task is\nto produce/report an array of the q order statistics S(r1), . . . , S(rq), where S(r) is the rth\nsmallest element in S, i.e., the element at index r in an array storing S after sorting it.\nOur new deterministic cache-oblivious multiple-selection algorithm makes use of the\nfollowing three existing cache-oblivious results for single selection, weighted selection, sorting,\nand multi-way partitioning.\n▶ Lemma 2 (Blum, Floyd, Pratt, Rivest, Tarjan [4, Theorem 1]). Selecting the k-th smallest\nelement in an unsorted array of N elements can be done with O(N) comparisons and O\n\u00001+ N\nB\n\u0001\nI/Os in the cache-oblivious model.\n▶ Remark 3 (Median of medians: I/O cost). Although the original paper by Blum et al. [4]\npredates the cache-oblivious model [11] by decades, analyzing the algorithm in the cache-\noblivious model with a stack-oriented memory allocator gives a linear I/O cost, since the\nalgorithm is based on repeatedly scanning geometrically decreasing subproblems.\n▶ Remark 4 (Median of medians: duplicates). The original algorithm in [4] assumes that all\nelements are distinct, but the algorithm can be extended to handle duplicates (by performing\na three-way partition of the elements into those less-than, equal-to, and greater-than a pivot,\nrespectively), and to return a triple S≤, p, S≥, that is a partition of S, where p is the element\nof rank k, S≤ are the elements of rank 1, . . . , k−1 in arbitrary order, and S≥ are the elements\nof rank k + 1, . . . , |S| in arbitrary order (where duplicate elements are assigned consecutive\nranks in an arbitrary order).\nIn the weighted selection problem we are giving an array of N elements, each with an\nassociated non-negative weight, and a target weight W, where the goal is to return the k-th\nsmallest element, for the smallest possible k, where the sum of the weights of the k smallest\nelements at least W. A linear-time weighted-selection algorithm can be derived from the\nunweighted algorithm by Blum et al. [4] (Lemma 2) – as hinted by Shamos in [18] and spelled\nout in detail by Bleich and Overton [3] – by computing the weighted rank of the pivot. The\nG. S. Brodal and S. Wild\n5\nweighted selection algorithm follows essentially the same recursion as [4], and it similarly\nfollows that it is cache oblivious and performs O\n\u00001 + N\nB\n\u0001\nI/Os.\n▶ Lemma 5 (Bleich, Overton [3]). Weighted selection in an unsorted array of N weighted\nelements can be done with O(N) comparisons and O\n\u00001 + N\nB\n\u0001\nI/Os in the cache-oblivious\nmodel.\n▶ Lemma 6 (Frigo, Leiserson, Prokop, Ramanchandran [12, Theorem 7], Brodal, Fagerberg [5,\nTheorem 2]). Funnelsort sorts an array of N elements using O\n\u0000 N\nB (1 + logM N)\n\u0001\nI/Os in a\ncache-oblivious model with a tall-cache assumption M ≥ B1+ε, for constant ε > 0.\n▶ Remark 7 (Tall and taller). The original description of funnelsort by Frigo et al. [11] assumed\nthe tall cache assumption M = Ω(B2), whereas [5] observed that this could be relaxed to\nthe weaker tall cache assumption M = Ω\n\u0000B1+ε\u0001\n. I/O optimality of funnelsort follows from a\nmatching external-memory lower bound by Aggarwal and Vitter [1, Theorem 3.1].\nThe key innovation in our previous randomized algorithm funnelselect [6] is the k-\npartitioner (Figure 2), a cache-oblivious and I/O-efficient multi-way partitioning algorithm\nto distribute a batch of elements around k − 1 given pivots into k buckets; the precise\ncharacteristics are summarized in the following lemma.\nP1\nP3\nP5\nP7\nP9\nP11\nP13\nP15\nP2\nP6\nP10\nP14\nP4\nP12\nP8\ninput array\noutput\nbuckets\nkd/2\nkd/4\nkd/4\n√\nk-partitioner\n√\nk-partitioners\nmiddle buffers\nFigure 2 A k-partitioner for k = 16 buckets. Content in the buffers is shaded; buffers are filled\nbottom-to-top; when full, they are flushed and then consumed from the bottom. The figure shows\nthe situation where the input buffer for P6 is being flushed down to its children (by partitioning\nelements around pivot P6). The flush at P6 was triggered during flushing P4’s input buffer, which in\nturn has been called while flushing P8 (the input).\nBuffer sizes for the three internal levels are shown next to the buffers. k-partitioners are defined\nrecursively from a\n√\nk-partitioner at the top, a collection of\n√\nk middle buffers, and\n√\nk further\n√\nk-partitioners, each partitioning from one middle buffer to\n√\nk output buffers. (All sizes here\nignore floors and ceilings; for the precise definition valid for all k, see [6].)\n6\nDeterministic Cache-Oblivious Funnelselect\n▶ Lemma 8 (Brodal and Wild [6, Lemma 3]). Given an unsorted array of N ≥ kd elements\nand k − 1 pivots P1 ≤ · · · ≤ Pk−1, a k-partitioner can partition the elements into k buckets\nS1, . . . , Sk, such all elements x in bucket Si satisfy Pi−1 ≤ x ≤ Pi. The algorithm is cache-\noblivious and performs O(N lg k) comparisons and O\n\u0000k + N\nB (1 + logM k)\n\u0001\nI/Os, provided\na tall-cache assumption M ≥ B1+ε and d ≥ max{1 + 2/ε, 2}. The working space for the\nk-partitioner (ignoring input and output buffers) is O\n\u0000k(d+1)/2\u0001\n. This is also the time required\nto construct a k-partitioner (again ignoring input and output buffers).\nThe k-partitioners are structurally similar to the k-mergers from funnelsort for merging\nk runs cache obliviously. In [6] we pipeline the partitioning by essentially reversing the com-\nputations done by funnelsort, and replace each binary merging node by a binary partitioning\nnode.\n3\nDeterministic multiple-selection\nIn this section we present our deterministic cache-oblivious multiple-selection algorithm\nthat performs optimal O(B + N) comparisons and O\n\u0000BI/O + N\nB\n\u0001\nI/Os, under a tall-cache\nassumption M ≥ B1+ε. Detailed pseudo-code is given in Algorithm 1 and Algorithm 2, and\nthe basic idea is illustrated in Figure 3.\nP1\nP2\nP3\nS1\nS2\nS3\nS4\nr0\nr1\nr2\nr3\nr4\nr5\nr6\nr7\nr8 rq+1\nrmin\n2\nrmax\n2\nrmin\n4\nrmax\n4\n∆1\n∆2\n∆3\n∆4\n∆5\n∆6\n∆7\n∆8\n∆9\nFigure 3 Deterministic multiple selection. The partition of an array S into buckets S1, . . . , S4\nseparated by pivots P1, . . . , P3, and query ranks r1, . . . , r8. In the example the maximum allowed\nbucket size is ∆ = ∆1, since ∆1 + ∆2 + ∆3 + ∆4 + ∆6 + ∆7 + ∆8 + ∆9 ≥ |S|/2 + 1 and\n∆2 + ∆3 + ∆4 + ∆6 + ∆7 + ∆8 + ∆9 < |S|/2 + 1. Black squares are pivots and the shaded regions in\nbuckets are the subproblems to recurse on.\nGiven a tall-cache assumption M ≥ B1+ε, we let d = max{1 + 2/ε, 2}. The algorithm\nfollows the general idea of making a recursive multi-way partition of the array of elements\nand to only recurse on subproblems with unresolved rank queries. For two consecutive query\nranks ri−1 and ri, we say that the ∆i = ri − ri−1 elements of rank ri−1 + 1, . . . , ri are in a\ngap of size ∆i. We choose a parameter ∆, such that at least half of the elements are in gaps\nof size ≤ ∆ and simultaneously at least half (rounded down) of the elements are in gaps of\nsize ≥ ∆. To compute ∆ (Algorithm 1, line 4), we compute ∆i = ri − ri−1 by a scan over\nthe query ranks r1, . . . , rq (and r0 = 0 and rq+1 = N + 1), and perform weighted selection\n(Lemma 5) among ∆1, . . . , ∆q+1, where ∆i has weight wi = ∆i, and return the smallest ∆\nwhere P\n∆i≤∆ wi ≥ N/2 + 1.\nFor the case when ∆ is small compared to N (formally, (2N)d ≥ ∆d+1 or N 1+\n1\n1+ε ≥ ∆2),\nwe simply solve the multiple-selection problem by sorting the elements (cache-obliviously\nusing funnelsort [12]), and report the elements with ranks r1, . . . , rq by a single scan over the\nsorted elements. The condition on ∆ implies BI/O = Ω(SortM,B(N)), where SortM,B(N) =\nΘ\n\u0000 N\nB\n\u00001+logM/B\nN\nB\n\u0001\u0001\nis the number of I/Os required to sort N elements in external memory [1],\nso this is within a constant factor of the I/O lower bound (detailed analysis in Section 4).\nOtherwise, we create a k-partition, where k = Θ\n\u0000 N\n∆\n\u0001\nas follows (MultiPartition in\nAlgorithm 2): We repeatedly distribute batches of ∆ elements into a set of buckets separated\nG. S. Brodal and S. Wild\n7\nAlgorithm 1 Deterministic cache-oblivious multiple-selection.\n1: procedure DeterministicFunnelselect(S[1..N], R[1..q])\n2:\nif q > 0 then\n3:\n∆i ← R[i] − R[i − 1] for i = 1, . . . , q + 1, assuming R[0] = 0 and R[q + 1] = N + 1\n4:\n∆ ← min\n\b\n∆i ∈ {∆1, . . . , ∆q+1}\n\f\f P\nj∈{1,...,q+1}:∆j≤∆i ∆j ≥ N/2 + 1\n\t\n5:\nif (2N)d ≥ ∆d+1 or N 1+\n1\n1+ε ≥ ∆2 then\n▷ BI/O = Ω(SortM,B(N))\n6:\nS ← Funnelsort(S)\n7:\nReport S[R[1]], . . . , S[R[q]]\n8:\nelse\n9:\n(P1, . . . , Pk−1), (S1, . . . , Sk) ← MultiPartition(S, ∆)\n10:\n¯r0 ← 0\n11:\nfor i ← 1, . . . , k do\n12:\n¯ri ← ¯ri−1 + |Si| + 1\n▷ ¯ri is rank of Pi\n13:\nRi ← {r | r ∈ R ∧ ¯ri−1 < r < ¯ri}\n▷ Rank queries to bucket Si\n14:\nif |Ri| > 0 then\n15:\nrmax\ni\n← max(Ri)\n16:\n¯Si, pmax, S≥ ← Select(Si, rmax\ni\n− ¯ri−1)\n17:\nif |Ri| > 1 then\n18:\nrmin\ni\n← min(Ri)\n19:\nS≤, pmin, ¯Si ← Select( ¯Si, rmin\ni\n− ¯ri−1)\n20:\nReport pmin\n21:\nif |Ri| > 2 then\n22:\n¯Ri ← {r − rmin\ni\n| r ∈ Ri \\ {rmin\ni\n, rmax\ni\n}}\n23:\nDeterministicFunnelselect( ¯Si, ¯Ri)\n24:\nReport pmax\n25:\nif ri ∈ R then\n26:\nReport Pi\nAlgorithm 2 Given an array S with N elements and a bucket capacity ∆, where (2N)\nd\nd+1 ≤ ∆ ≤ N,\npartition S into k buckets S1, . . . , Sk separated by k − 1 pivots P1, . . . , Pk−1, where \u0004 ∆\n2\n\u0005\n≤ |Si| ≤ ∆.\n1: procedure MultiPartition(S[1..N], ∆)\n2:\nRequires (2N)\nd\nd+1 ≤ ∆ ≤ N\n3:\nk ← 1, S1 ← {}\n▷ Initially only one empty bucket and no pivots\n4:\nfor i ← 1 to N step ∆ do\n5:\n¯S ← S[i.. min(i + ∆ − 1, N)]\n▷ Next batch to distribute to buckets\n6:\nDistribute ¯S to buckets S1, . . . Sk using pivots P1, . . . , Pk−1 with a k-partitioner\n7:\nwhile there exists a bucket Sj with |Sj| > ∆ do\n▷ Split bucket Sj\n8:\nS≤, p, S≥ ← Select(Sj, ⌈|Sj|/2⌉)\n9:\nRename Sj+1, . . . , Sk to Sj+2, . . . , Sk+1 and Pj, . . . , Pk−1 to Pj+1, . . . , Pk\n10:\nSj ← S≤, Pj ← p, Sj+1 ← S≥\n11:\nk ← k + 1\n12:\nreturn (P1, . . . , Pk−1), (S1, . . . , Sk)\n8\nDeterministic Cache-Oblivious Funnelselect\nby pivot elements. Initially we have one empty bucket and no pivot. Whenever a bucket\nreaches size > ∆, the bucket is split into two buckets of size ≤ ∆ separated by a new pivot\nusing the (cache-oblivious) linear-time median selection algorithm (Lemma 2). To distribute\na batch of elements into the current set of buckets we use a cache-oblivious k-partitioner\n(Lemma 8, which depends on the tall-cache assumption parameter d) built using the current\nset of pivots. Note that we need to construct a new k-partitioner after each batch of ∆\nelements has been distributed, since the number of buckets and pivots can increase. For the\ncomputation to be I/O efficient, we allocate in memory space for a\n\u0004 2N\n∆\n\u0005\n-partitioner followed\nby space for\n\u0004 2N\n∆\n\u0005\nbuckets of capacity 2∆ (in the proof of Lemma 9 we argue that the number\nof buckets created is at most 2N\n∆ and each bucket will never exceed 2∆ elements). The space\nfor the partitioner is reused for each new batch, and whenever a bucket is split into two\nnew buckets, one bucket remains in the old bucket’s allocated space and the other bucket is\nplaced in next available slot for a bucket. This ensures all buckets are stored consecutively\nin memory, albeit in arbitrary order.\nAfter having constructed the buckets we compute the ranks of the pivots from the bucket\nsizes, and consider the gaps with at least one unresolved rank query. If the rank of a pivot\ncoincides with a query rank, we report this pivot just after having considered the preceding\nbucket. Before recursing on the elements in a bucket, we first find the minimum and maximum\nquery ranks rmin and rmax in the bucket by a scan over the bucket’s query ranks, and find\nand report the corresponding elements in the bucket using linear-time selection (Lemma 2).\nFinally, we only recurse on the elements between ranks rmin and rmax, provided there are any\nunresolved rank queries to the bucket. This ensures that when recursing on a subproblem\nof size ¯N, all elements in the subproblem are in gaps of size < ¯N in the original input.\nBy reporting the elements at the appropriate times during the recursion, elements will be\nreported in increasing order.\nThe partitioning of an array S into buckets is illustrated in Figure 3. The crucial property\nis that for a gap ∆i ≥ ∆, the two query ranks ri−1 and ri defining the gap cannot be in the\nsame bucket, implying that no element in this gap will be part of a recursive subproblem\n(see, e.g., gaps ∆1 and ∆5 in Figure 3).\nPseudocode for our algorithm is shown in Algorithm 1 and Algorithm 2. We assume\nSelect(S, k) is the deterministic linear-time selection algorithm from Lemma 2, and that it\nreturns a triple S≤, p, S≥, that is a partition of S, where p is the element of rank k, S≤ are\nthe elements of rank 1, . . . , k −1 in arbitrary order, and S≥ the elements of rank k +1, . . . , |S|\nin arbitrary order.\n4\nAnalysis\nWe first analyze the number of comparisons and I/Os performed by MultiPartition in\nAlgorithm 2, that deterministically performs a k-way partition of N elements into k = O\n\u0000 N\n∆\n\u0001\nbuckets separated by k − 1 pivots, where each bucket has size at most ∆. The following\nlemma summarizes the precise properties of MultiPartition.\n▶ Lemma 9. For N ≥ ∆ and ∆d+1 ≥ (2N)d, MultiPartition creates k ≤ 2N\n∆ buckets\nand k − 1 pivots, each bucket has size at most ∆, and performs O(N lg k) comparisons and\nO\n\u0000k2 + N\nB (1 + logM k)\n\u0001\nI/Os.\nProof. We first bound the sizes of the buckets created by MultiPartition. The algorithm\nrepeatedly distributes batches of at most ∆ elements to buckets and splits all overflowing\nbuckets of size > ∆ before considering the next batch.\nIt is an invariant that before\nG. S. Brodal and S. Wild\n9\ndistributing a batch, all buckets have size at most ∆. Furthermore, as soon as the first\nbucket is split, all buckets have size at least\n\u0004 ∆\n2\n\u0005\n, since whenever an overflowing bucket of\nsize s > ∆ is split the new buckets have initial sizes\n\u0004 s−1\n2\n\u0005\nand\n\u0006 s−1\n2\n\u0007\n. Here “−1” is due to\none element becomes a pivot. The smallest bucket size is achieved when s = ∆ + 1, where\nthe smallest bucket size is\n\u0004 ∆+1−1\n2\n\u0005\n=\n\u0004 ∆\n2\n\u0005\n. Note that the buckets after the split have size\nat most ∆, since all buckets had at most ∆ elements before the distribution of a batch of\nat most ∆ elements to the buckets, i.e., s ≤ 2∆. To bound the total number of buckets k\ncreated, observe that if ∆ = N then no bucket will be split and k = 1. Otherwise, ∆ < N\nand at least two buckets are created, and k\n\u0004 ∆\n2\n\u0005\n+ k − 1 ≤ N, since all buckets have size at\nleast\n\u0004 ∆\n2\n\u0005\nand there are k − 1 pivots. We have N ≥ k\n\u0000 ∆\n2 − 1\n2\n\u0001\n+ k − 1 = k∆\n2 + k\n2 − 1 ≥ k∆\n2 ,\nsince k ≥ 2, i.e., the total number of buckets created k ≤ 2N\n∆ .\nTo analyze the number of comparisons and I/Os performed, we need to consider the\n\u0006 N\n∆\n\u0007\ndistribution steps and at most 2N\n∆ −1 bucket splittings. Since each bucket splitting involves at\nmost 2∆ elements, each bucket splitting can be performed cache-obliviously by a linear-time\nselection algorithm (Lemma 2) using O(∆) comparisons and O\n\u00001 + ∆\nB\n\u0001\nI/Os, assuming each\nbucket is stored in a buffer of 2∆ consecutive memory cells. In total the k −1 = Θ\n\u0000 N\n∆\n\u0001\nbucket\nsplittings require O(N) comparisons and O\n\u0000k + N\nB\n\u0001\nI/Os. A k-partitioner for partitioning ∆\nelements uses O(∆ lg k) comparisons and O\n\u0000k + ∆\nB (1 + logM k)\n\u0001\nI/Os (Lemma 8), assuming\nk is sufficiently small according to the tall-cache assumption (see below). This includes the\ncost of constructing the k-partitioner. The total cost for all\n\u0006 N\n∆\n\u0007\ndistribution steps becomes\nO(N lg k) comparisons and O\n\u0000k N\n∆ + N\nB (1 + logM k)\n\u0001\n= O\n\u0000k2 + N\nB (1 + logM k)\n\u0001\nI/Os.\nBy Lemma 8, the tall-cache assumption M ≥ B1+ε implies that for a k-partitioner\nand an input of size ∆, it is required that ∆ ≥ kd for the I/O bounds to hold (recall\nd = max{1 + 2/ε, 2}). The input assumption ∆ ≥\n\u0000 2N\n∆\n\u0001d together with k ≤ 2N\n∆ ensure that\n∆ ≥ kd.\n◀\nWe now prove our main result that DeterministicFunnelselect in Algorithm 1 is an\noptimal deterministic cache-oblivious multiple-selection algorithm. Crucial to the analysis\nis to show that the choice of ∆ balances early pruning of buckets without queries with\nsimultaneously achieving efficient I/O bounds.\n▶ Theorem 10. DeterministicFunnelselect performs O(B + N) comparisons and\nO\n\u0000BI/O + N\nB\n\u0001\nI/Os cache-obliviously in a cache model with tall assumption M ≥ B1+ε, for\nsome constant ε > 0.\nProof. We first consider the consequences of the choice of ∆. By the choice of ∆, we have\nP\n∆i<∆ ∆i < N/2 + 1. Since each bucket Si has size at most ∆, and we only recurse on\nsubsets that are (the union of) gaps where the two bounding rank queries of the gaps are\nboth in the same bucket, we only recurse on gaps with ∆i < ∆ elements (see Figure 3).\nA recursive subproblem between query ranks rs and rt, where 1 ≤ s < t ≤ q, contains\nrt − rs − 1 = (Pt\ni=s+1 ∆i) − 1 elements. It follows that\n(A) all recursive subproblems in total contain at most P\n∆i<∆ ∆i − 1 < N/2 elements and\neach subproblem has size ≤ ∆ − 2.\n(B) P\n∆i≤∆ ∆i ≥ N/2 + 1, i.e., at least N/2 elements are in gaps of size at most ∆.\nTo analyze the number of comparisons performed, we use a potential argument where\none unit of potential can pay for O(1) comparisons, and all comparisons performed can be\ncharged to the released potential. We define the potential of an element x in a gap of size ∆i\nto be 1 + lg N\n∆i , where N is the size of the current recursive subproblem x resides in. The\ntotal initial potential is at most N + Pq+1\ni=1 ∆i lg N\n∆i = O(B + N).\n10\nDeterministic Cache-Oblivious Funnelselect\nWe first consider the number of comparisons for the non-sorting case (Algorithm 1,\nlines 9–26). If an element x in a gap of size ∆i ≤ ∆ participates in a recursive call of\nsize < ∆, the potential released for x is at least\n\u00001 + lg N\n∆i\n\u0001\n−\n\u00001 + lg ∆\n∆i\n\u0001\n= lg N\n∆. If an\nelement x in a gap of size ∆i ≤ ∆ does not participate in a recursive call, the potential\nreleased for x is 1+lg N\n∆i ≥ 1+lg N\n∆. Finally, elements in gaps of size > ∆ will not participate\nin recursive calls, and will each release at least potential 1. It follows that the released\npotential is at least N\n2 + N\n2 lg N\n∆, since at least N/2 elements are in gaps of size ≤ ∆ (property\n(B), contributing the second summand) and at most N/2 elements are in gaps of size < ∆\nand participate in recursive calls (property (A)), i.e., at least N/2 elements are in gaps\nof size ≥ ∆ (contributing the first summand). By Lemma 9, MultiPartition requires\nO(N lg k) comparisons, and since k = O(N/∆) this can be covered by the released potential.\nThe additional comparisons required for computing ∆ with a linear-time weighted section\nalgorithm (Lemma 5) and performing Select (Lemma 2) at most twice on each bucket\nrequire in total at most O(N) comparisons, and can also be charged to the released potential.\nIt follows that for the non-sorting case the released potential can cover for all comparisons\nperformed.\nIn the sorting case, a single call to Funnelsort is performed causing O(N lg N) compar-\nisons (Lemma 6). No further recursive calls are made and the potential of all elements is\nreleased. At least N + N\n2 lg N\n∆ potential is released, since at least N/2 elements are in gaps\nof size ≤ ∆ (property (B)). In the sorting case, either (2N)d ≥ ∆d+1 or N 1+\n1\n1+ε ≥ ∆2. If\n(2N)d ≥ ∆d+1, we have ∆ ≤ (2N)\nd\nd+1 and N\n∆ ≥ N/(2N)\nd\nd+1 ≥ 1\n2N\n1\nd+1 . It follows that the\nreleased potential is at least N + N\n2 lg\n\u0000 1\n2N\n1\nd+1 \u0001\n≥\n1\n2(d+1)N lg N, covering the cost for the com-\nparisons. Otherwise, N 1+\n1\n1+ε ≥ ∆2, i.e., ∆ ≤ N\n1\n2\n\u00001+\n1\n1+ε\n\u0001\nand we have N\n∆ ≥ N/N\n1\n2\n\u00001+\n1\n1+ε\n\u0001\n=\nN\nε\n2(1+ε) and the potential released is at least N + N\n2 lg N\n∆ ≥ N +\nε\n4(1+ε)N lg N and can cover\nthe cost for the comparisons. Note that the comparison bound depends on the tall-cache\nparameters ε and d.\nTo analyze the I/O cost we assign an I/O potential to an element x in gap of size ∆i\nof\n1\nB\n\u00001 + logM\nN\n∆i\n\u0001\n, where N is the size of the current subproblem x resides in. Similar\nto the comparison potential, it follows that the non-sorting case releases I/O potential\n1\n2\n\u0000 N\nB + N\nB logM\nN\n∆\n\u0001\n. The number of I/Os required is O\n\u00001 + N\nB\n\u0001\nI/Os for scanning the input\nand computing ∆ using weighted selection (Lemma 5), O\n\u0000k + N\nB\n\u0001\nI/Os for selecting the\nminimum and maximum rank elements in each bucket (Lemma 2), and O\n\u0000k2+ N\nB (1+logM k)\n\u0001\nI/Os for the k-partitioning (Lemma 8), i.e., in total O\n\u0000k2 + N\nB (1 + logM k)\n\u0001\nI/Os. It follows\nthat the I/O cost can be charged to the released potential, provided k2 = O\n\u0000 N\nB\n\u0001\n. To address\nthis, we need to consider two cases depending on the size N of a subproblem. If the problem\ncompletely fits in internal memory together with all the geometric decreasing recursive\nsubproblems, assuming a stack-oriented memory allocation, then considering this problem\nwill in total cost O\n\u00001 + N\nB\n\u0001\nI/Os, including all recursive subproblems. That means, there\nexists a constant c > 0 such that for N ≤ cM, the I/O cost for handling such problems\ncan be charged to the parent subproblem creating the subproblem. It follows that we only\nneed to consider the I/O cost for subproblems of size N ≥ cM. Since M ≥ B1+ε, we\nhave N ≥ cM ≥ cB1+ε, i.e., B ≤\n\u0000 N\nc\n\u00011/(1+ε). Since k = O\n\u0000 N\n∆\n\u0001\n, to prove k2 = O\n\u0000 N\nB\n\u0001\nit is\nsufficient to prove\n\u0000 N\n∆\n\u00012 = O\n\u0010\nN\n(N/c)1/(1+ε)\n\u0011\n. This holds, e.g., when N 1+\n1\n1+ε ≤ ∆2, which\nis always fulfilled in the non-sorting case. For the sorting case, we have similarly to the\ncomparison potential that Ω\n\u0000 N\nB logM N\n\u0001\nI/O potential is released, which can cover the I/O\ncost for cache-oblivious sorting (Lemma 6).\n◀\nG. S. Brodal and S. Wild\n11\n5\nConclusion\nWith deterministic funnelselect, we close the gap left in previous work and obtain an\nI/O-optimal cache-oblivious multiple-selection algorithm that does not need to resort to\nrandomization to achieve its performance. This settles the complexity of the multiple-\nselection problem in the cache-oblivious model (including the fine-grained analysis based on\nthe query-rank entropy B).\nThere are open questions left in other variants of the problem. Like randomized funnelse-\nlect [6], deterministic funnelselect cannot deal with queries arriving in an online fashion, one\nafter the other. This problem has been addressed in the external-memory model [2], but no\ncache-oblivious I/O-optimal solution is known.\nConcerning the transition from single selection by rank to sorting, which multiple selection\nallows us to study, some questions remain unanswered. For example, in the cache-oblivious\nmodel, it is known that sorting with optimal I/O-complexity is only possible under a tall-\ncache assumption (such as the one made in this work); for single selection, however, such\na restriction is not necessary. It would be interesting to study the transition between the\nproblems and find out, how “sorting-like” a multiple-selection instance has to be to likewise\nrequire a tall cache for I/O-optimal cache-oblivious algorithms.\nAnother direction for future work are parallel algorithms for multiple selection that are\nalso cache-oblivious and I/O efficient.\nReferences\n1\nAlok Aggarwal and Jeffrey Scott Vitter. The input/output complexity of sorting and related\nproblems. Commun. ACM, 31(9):1116–1127, 1988. doi:10.1145/48529.48535.\n2\nJérémy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jon Sorenson. Near-optimal online\nmultiselection in internal and external memory. Journal of Discrete Algorithms, 36:3–17, jan\n2016. doi:10.1016/j.jda.2015.11.001.\n3\nChaya Bleich and Michael L. Overton. A linear-time algorithm for the weighted median\nproblem. Technical Report 75, New Yourk University, Department of Computer Science, April\n1983. URL: https://archive.org/details/lineartimealgori00blei/.\n4\nManuel Blum, Robert W. Floyd, Vaughan R. Pratt, Ronald L. Rivest, and Robert Endre\nTarjan. Time bounds for selection. J. Comput. Syst. Sci., 7(4):448–461, 1973. doi:10.1016/\nS0022-0000(73)80033-9.\n5\nGerth Stølting Brodal and Rolf Fagerberg. Cache oblivious distribution sweeping. In Peter Wid-\nmayer, Francisco Triguero Ruiz, Rafael Morales Bueno, Matthew Hennessy, Stephan J. Eiden-\nbenz, and Ricardo Conejo, editors, Automata, Languages and Programming, 29th International\nColloquium, ICALP 2002, Malaga, Spain, July 8-13, 2002, Proceedings, volume 2380 of Lecture\nNotes in Computer Science, pages 426–438. Springer, 2002. doi:10.1007/3-540-45465-9_37.\n6\nGerth Stølting Brodal and Sebastian Wild. Funnelselect: Cache-oblivious multiple selection.\nIn Inge Li Gørtz, Martin Farach-Colton, Simon J. Puglisi, and Grzegorz Herman, editors, 31st\nAnnual European Symposium on Algorithms, ESA 2023, September 4-6, 2023, Amsterdam,\nThe Netherlands, volume 274 of LIPIcs, pages 25:1–25:17. Schloss Dagstuhl - Leibniz-Zentrum\nfür Informatik, 2023. doi:10.4230/LIPICS.ESA.2023.25.\n7\nJ. M. Chambers. Partial sorting [M1] (algorithm 410). Commun. ACM, 14(5):357–358, 1971.\ndoi:10.1145/362588.362602.\n8\nDavid P. Dobkin and J. Ian Munro. Optimal time minimal space selection algorithms. J.\nACM, 28(3):454–461, 1981. doi:10.1145/322261.322264.\n9\nDorit Dor and Uri Zwick. Selecting the median. SIAM Journal on Computing, 28(5):1722–1758,\n1999. doi:10.1137/s0097539795288611.\n12\nDeterministic Cache-Oblivious Funnelselect\n10\nRobert W. Floyd and Ronald L. Rivest. Expected time bounds for selection. Communications\nof the ACM, 18(3):165–172, March 1975. doi:10.1145/360680.360691.\n11\nMatteo Frigo, Charles E. Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache-\noblivious algorithms. In 40th Annual Symposium on Foundations of Computer Science, FOCS\n’99, 17-18 October, 1999, New York, NY, USA, pages 285–298. IEEE Computer Society, 1999.\ndoi:10.1109/SFFCS.1999.814600.\n12\nMatteo Frigo, Charles E. Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache-\noblivious algorithms. ACM Trans. Algorithms, 8(1):4:1–4:22, 2012. doi:10.1145/2071379.\n2071383.\n13\nC. A. R. Hoare. Algorithm 65: find. Commun. ACM, 4(7):321–322, 1961. doi:10.1145/\n366622.366647.\n14\nXiaocheng Hu, Yufei Tao, Yi Yang, and Shuigeng Zhou. Finding approximate partitions and\nsplitters in external memory. In Proceedings of the 26th ACM symposium on Parallelism in\nalgorithms and architectures. ACM, June 2014. doi:10.1145/2612669.2612691.\n15\nKanela Kaligosi, Kurt Mehlhorn, J. Ian Munro, and Peter Sanders. Towards optimal multiple\nselection. In Luís Caires, Giuseppe F. Italiano, Luís Monteiro, Catuscia Palamidessi, and\nMoti Yung, editors, Automata, Languages and Programming, 32nd International Colloquium,\nICALP 2005, Lisbon, Portugal, July 11-15, 2005, Proceedings, volume 3580 of Lecture Notes\nin Computer Science, pages 103–114. Springer, 2005. doi:10.1007/11523468_9.\n16\nHelmut Prodinger.\nMultiple Quickselect – Hoare’s Find algorithm for several elements.\nInformation Processing Letters, 56(3):123–129, November 1995. doi:10.1016/0020-0190(95)\n00150-b.\n17\nArnold Schönhage, Mike Paterson, and Nicholas Pippenger. Finding the median. J. Comput.\nSyst. Sci., 13(2):184–199, 1976. doi:10.1016/S0022-0000(76)80029-3.\n18\nMichael Ian Shamos. Geometry and statistics: Problems at the interface. In Joseph Frederick\nTraub, editor, Algorithms and Complexity: New Directions and Recent Results, pages 251–\n280. Academic Press, 1976. URL: http://euro.ecom.cmu.edu/people/faculty/mshamos/\n1976Stat.pdf.\n"
}