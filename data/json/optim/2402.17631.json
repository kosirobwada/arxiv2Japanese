{
    "optim": "Deterministic Cache-Oblivious Funnelselect Gerth Stølting Brodal # Aarhus University, Denmark Sebastian Wild # University of Liverpool, UK Abstract In the multiple-selection problem one is given an unsorted array S of N elements and an array of q query ranks r1 < · · · < rq, and the task is to return, in sorted order, the q elements in S of rank r1, . . . , rq, respectively. The asymptotic deterministic comparison complexity of the problem was settled by Dobkin and Munro [JACM 1981]. In the I/O model an optimal I/O complexity was achieved by Hu et al. [SPAA 2014]. Recently [ESA 2023], we presented a cache-oblivious algorithm with matching I/O complexity, named funnelselect, since it heavily borrows ideas from the cache-oblivious sorting algorithm funnelsort from the seminal paper by Frigo, Leiserson, Prokop and Ramachandran [FOCS 1999]. Funnelselect is inherently randomized as it relies on sampling for cheaply finding many good pivots. In this paper we present deterministic funnelselect, achieving the same optional I/O complexity cache-obliviously without randomization. Our new algorithm essentially replaces a single (in expectation) reversed-funnel computation using random pivots by a recursive algorithm using multiple reversed-funnel computations. To meet the I/O bound, this requires a carefully chosen subproblem size based on the entropy of the sequence of query ranks; deterministic funnelselect thus raises distinct technical challenges not met by randomized funnelselect. The resulting worst-case I/O bound is O\u0000Pq+1 i=1 ∆i B · logM/B N ∆i + N B \u0001 , where B is the external memory block size, M ≥ B1+ε is the internal memory size, for some constant ε > 0, and ∆i = ri − ri−1 (assuming r0 = 0 and rq+1 = N + 1). 2012 ACM Subject Classification Theory of computation → Design and analysis of algorithms Keywords and phrases Multiple selection, cache-oblivious algorithm, entropy bounds Funding Gerth Stølting Brodal: Independent Research Fund Denmark, grant 9131-00113B. Sebastian Wild: Engineering and Physical Sciences Research Council grant EP/X039447/1. 1 Introduction We present the first optimal deterministic cache-oblivious algorithm for the multiple-selection problem. In the multiple-selection problem one is given an unsorted array S of N elements and an array R of q query ranks in increasing order r1 < · · · < rq, and the task is to return, in sorted order, the q elements of S of rank r1, . . . , rq, respectively; (see Figure 1 for an example). On top of immediate applications, the multiple-selection problem is of interest as it gives a natural common generalization of (single) selection by rank (using a single query rank r1 = r) and fully sorting an array (corresponding to selecting every index as a query rank, i.e., q = N and ri = i for i = 1, . . . , N). It thus allows us to quantitatively study S 67 1 30 2 45 3 33 4 15 5 99 6 26 7 90 8 55 9 9 10 96 11 45 12 95 13 31 14 3 15 r1 r2 r3 r4 3 1 9 2 15 3 26 4 30 5 31 6 33 7 45 8 45 9 55 10 67 11 90 12 95 13 96 14 99 15 r1 r2 r3 r4 imaginary sorted S Figure 1 Example input with N = 15, q = 4 and R[1..q] = [1, 2, 3, 8]. The expected output 3, 9, 15, 45 is obvious from the sorted array (right). (The sorted array is for illustration only; the goal of efficient multiple-selection algorithms is to avoid ever fully sorting the input.) arXiv:2402.17631v1  [cs.DS]  27 Feb 2024 2 Deterministic Cache-Oblivious Funnelselect the transition between these two foundational problems, which are of different complexity and each have their distinct set of algorithms. For example, the behavior of selection and sorting with respect to external memory is quite different: For single selection, the textbook median-of-medians algorithm [4] simultaneously works with optimal cost in internal memory, external memory, and the cache-oblivious model (models are defined below). For sorting, by contrast, the introduction of each model required a substantially modified algorithm to achieve optimal costs: Standard binary mergesort is optimal in internal memory, but requires ≈ M/B-way merging to be optimal in external memory, where M is the internal memory size and B the external memory block size, measured in elements [1]; achieving the same cache obliviously, i.e., without knowledge of B and M, requires the judiciously chosen buffer sizes from the recursive constructions of funnelsort [11]. Since multiple selection simultaneously generalizes both problems, it is not surprising that also here subsequent refinements were necessary going from internal to external to cache-oblivious; the most recent result being our algorithm funnelselect [6]. However, all algorithms mentioned above for single selection and sorting are deterministic. By constrast, funnelselect is inherently relying on randomization and known deterministic external-memory algorithms [2, 14] are crucially relying on the knowledge of M and B. Prior to this work it thus remained open whether a single deterministic cache-oblivious algorithm exists that smoothly interpolates between selection and sorting without having to resort to randomization. In this paper, we answer this question in the affirmative. Our algorithm determinis- tic funnelselect draws on techniques from cache-oblivious sorting (funnelsort) and existing multiple-selection algorithms, but it follows a rather different approach to our earlier random- ized algorithm [6] and previous (cache-conscious) external-memory algorithms. A detailed comparison is given below. 1.1 Model of computation and previous work Our results are in the cache-oblivious model of Frigo, Leiserson, Prokop and Ramachan- dran [12], a hierarchical-memory model with an infinite external memory and an internal memory of capacity M elements, where data is transferred between internal and external memory in blocks of B consecutive elements. Algorithms are compared by their I/O cost, i.e., the number of block transfers or I/Os (input/output operations). This is similar to the external-memory model by Aggarwal and Vitter [1]. Crucially, in the cache-oblivious model, algorithms do not know M and B and I/Os are assumed to be performed automatically by an optimal (offline) paging algorithm. Cache-oblivious algorithms hence work for any parameters M and B, and they even adapt to multi-level memory hierarchies (under certain conditions [12]). The multiple-selection problem was first formally addressed by Chambers [7], who considered it a generalization of quickselect [13]. Prodinger [16] proved that Chambers’ algorithm achieves an optimal expected running time up to constant factors: O(B +N), where B = Pq+1 i=1 ∆i lg N ∆i with ∆i = ri −ri−1, for 1 ≤ i ≤ q +1, assuming r0 = 0 and rq+1 = N +1, and lg denoting the binary logarithm. We call B the (query-rank) entropy of the sequence of query ranks [2]. It should be noted that B + N = O(N(1 + lg q)), but the latter bound does not take the location of query ranks into account; for example, if q = Θ \u0000√n \u0001 queries are in a range of size O(N/ lg N), i.e., rq − r1 = O(N/ lg N), then the entropy bound is O(N) whereas the latter N(1 + lg q) = Θ(N lg N). Dobkin and Munro [8] showed that B − O(N) comparisons are necessary to find all ranks r1, . . . , rq (in the worst case). Deterministic algorithms with that same O(B + N) running time are also known [8, 15], but as for single selection, the deterministic algorithms were G. S. Brodal and S. Wild 3 Table 1 Algorithms for selection and multiple selection. CO = cache-oblivious, E = expected, wc = worst-case bounds. Note that Barbay et al. assume a tall cache M ≥ B1+ε, whereas Hu et al. do not. Reference Comparisons I/Os Comments Single selection Hoare [13] E 2 ln 2B + 2N + o(N) O(N/B) CO, randomized Floyd & Rivest [10] E N + min{r, N−r} + o(N) O(N/B) CO, randomized Blum et al. [4] wc 5.4305N O(N/B) CO, deterministic Schönhage et al. [17] wc 3N + o(N) ? median, deterministic Dor & Zwick [9] wc 2.95 + o(N) ? median, deterministic Multiple selection Chambers [7, 16] E 2 ln 2B + O(N) O((B + N)/B) CO, randomized Dobkin & Munro [8] wc 3B + O(N) O((B + N)/B) CO, deterministic Kaligosi et al. [15] wc B + o(B) + O(N) O((B + N)/B) CO, deterministic Hu et al. [14] wc O(N lg(q)) O(N/B logM/B(q/B)) deterministic wc O(B + N) O(BI/O + N/B) (from closer analysis) Barbay et al. [2] wc B + o(B) + O(N) O(BI/O + N/B) online, determ., M ≥ B1+ε Brodal & Wild [6] E O(B + N) O(BI/O + N/B) CO, randomized, M ≥ B1+ε This paper wc O(B + N) O(BI/O + N/B) CO, deterministic, M ≥ B1+ε presented later than the randomized algorithms and require more sophistication. Multiple selection in external-memory was studied by Hu et al. [14] and Barbay et al. [2]. Their algorithms have an I/O cost of O \u0000BI/O + N B \u0001 , where the “I/O entropy” BI/O = B B lg(M/B). An I/O cost of Ω(BI/O) − O( N B ) is known to be necessary [2, 6]. A more comprehensive history of the multiple-selection problem appears in [6]; Table 1 gives an overview. We note that many existing time- and comparison-optimal multiple-selection algorithms are actually already cache oblivious, but they are not optimal with respect to the number of I/Os performed when analyzed in the cache-oblivious model (the obtained I/O bounds are a factor lg(M/B) away from being optimal). 1.2 Result Our main result is the cache-oblivious algorithm deterministic funnelselect achieving the following efficiency (see Theorem 10 for the full statement and proof). ▶ Theorem 1. There exists a deterministic cache-oblivious algorithm solving the multiple- selection problem using O(B + N) comparisons and O \u0000BI/O + N B \u0001 I/Os in the worst case, assuming a tall cache M ≥ B1+ε. At the high level, our algorithm uses the standard overall idea of a recursive partitioning algorithm and pruning recursive calls containing no rank queries, an idea dating back to the first algorithm by Chambers [7]. In the cache-aware external-memory model, I/O efficient algorithms are essentially obtained by replacing binary partitioning (as used in [7]) by an external-memory Θ(M/B)-way partitioning [2, 14]. Unfortunately, in the cache-oblivious model this is not possible, since the parameters M and B are unknown to the algorithm. To be I/O efficient in the cache oblivious model, both our previous algorithm randomized funnelselect [6] and our new algorithm deterministic funnelselect apply a cache-oblivious multi-way k-partitioner to distribute elements into k buckets given a set of k − 1 pivot elements, essentially reversing the computation done by the k-merger used by funnelsort [11]. The k-partitioner is a balanced binary tree of k − 1 pipelined binary partitioners. 4 Deterministic Cache-Oblivious Funnelselect The key difference between our randomized and deterministic algorithms is that in our randomized algorithm we use a single N Θ(ε)-way partitioner using randomly selected pivots and truncate work inside the partitioner for subproblems that (with high probability) will not contain any rank queries. This is done by estimating the ranks of the pivots through sampling and pruning subproblems estimated to be sufficiently far from any query ranks. In our deterministic version, we choose k smaller and deterministically compute pivots, such that all elements are pushed all the way down through a k-partitioner without truncation (eliminating the need to know the (approximate) ranks of the pivots before the k-partitioning is finished), while we choose k such that the buckets with unresolved rank queries (that we have to recursive on) in total contain at most half of the elements. To compute k, we apply a linear- time weighted-median finding algorithm on ∆1, . . . , ∆q+1. While randomized funnelselect can handle buckets with unresolved rank queries directly using sorting, deterministic funnelselect needs to recursively perform multiple-selection on the buckets to achieve the desired I/O performance. 2 Preliminaries Throughout the paper we assume that the input to a multiple-selection algorithm is given as two arrays S[1..N] and R[1..q], where S is an unsorted array of N elements from a totally ordered universe, and R is a sorted array r1, . . . , rq of q distinct query ranks, where 1 ≤ r1 < · · · < rq ≤ N. The array S is allowed to contain duplicate elements. Our task is to produce/report an array of the q order statistics S(r1), . . . , S(rq), where S(r) is the rth smallest element in S, i.e., the element at index r in an array storing S after sorting it. Our new deterministic cache-oblivious multiple-selection algorithm makes use of the following three existing cache-oblivious results for single selection, weighted selection, sorting, and multi-way partitioning. ▶ Lemma 2 (Blum, Floyd, Pratt, Rivest, Tarjan [4, Theorem 1]). Selecting the k-th smallest element in an unsorted array of N elements can be done with O(N) comparisons and O \u00001+ N B \u0001 I/Os in the cache-oblivious model. ▶ Remark 3 (Median of medians: I/O cost). Although the original paper by Blum et al. [4] predates the cache-oblivious model [11] by decades, analyzing the algorithm in the cache- oblivious model with a stack-oriented memory allocator gives a linear I/O cost, since the algorithm is based on repeatedly scanning geometrically decreasing subproblems. ▶ Remark 4 (Median of medians: duplicates). The original algorithm in [4] assumes that all elements are distinct, but the algorithm can be extended to handle duplicates (by performing a three-way partition of the elements into those less-than, equal-to, and greater-than a pivot, respectively), and to return a triple S≤, p, S≥, that is a partition of S, where p is the element of rank k, S≤ are the elements of rank 1, . . . , k−1 in arbitrary order, and S≥ are the elements of rank k + 1, . . . , |S| in arbitrary order (where duplicate elements are assigned consecutive ranks in an arbitrary order). In the weighted selection problem we are giving an array of N elements, each with an associated non-negative weight, and a target weight W, where the goal is to return the k-th smallest element, for the smallest possible k, where the sum of the weights of the k smallest elements at least W. A linear-time weighted-selection algorithm can be derived from the unweighted algorithm by Blum et al. [4] (Lemma 2) – as hinted by Shamos in [18] and spelled out in detail by Bleich and Overton [3] – by computing the weighted rank of the pivot. The G. S. Brodal and S. Wild 5 weighted selection algorithm follows essentially the same recursion as [4], and it similarly follows that it is cache oblivious and performs O \u00001 + N B \u0001 I/Os. ▶ Lemma 5 (Bleich, Overton [3]). Weighted selection in an unsorted array of N weighted elements can be done with O(N) comparisons and O \u00001 + N B \u0001 I/Os in the cache-oblivious model. ▶ Lemma 6 (Frigo, Leiserson, Prokop, Ramanchandran [12, Theorem 7], Brodal, Fagerberg [5, Theorem 2]). Funnelsort sorts an array of N elements using O \u0000 N B (1 + logM N) \u0001 I/Os in a cache-oblivious model with a tall-cache assumption M ≥ B1+ε, for constant ε > 0. ▶ Remark 7 (Tall and taller). The original description of funnelsort by Frigo et al. [11] assumed the tall cache assumption M = Ω(B2), whereas [5] observed that this could be relaxed to the weaker tall cache assumption M = Ω \u0000B1+ε\u0001 . I/O optimality of funnelsort follows from a matching external-memory lower bound by Aggarwal and Vitter [1, Theorem 3.1]. The key innovation in our previous randomized algorithm funnelselect [6] is the k- partitioner (Figure 2), a cache-oblivious and I/O-efficient multi-way partitioning algorithm to distribute a batch of elements around k − 1 given pivots into k buckets; the precise characteristics are summarized in the following lemma. P1 P3 P5 P7 P9 P11 P13 P15 P2 P6 P10 P14 P4 P12 P8 input array output buckets kd/2 kd/4 kd/4 √ k-partitioner √ k-partitioners middle buffers Figure 2 A k-partitioner for k = 16 buckets. Content in the buffers is shaded; buffers are filled bottom-to-top; when full, they are flushed and then consumed from the bottom. The figure shows the situation where the input buffer for P6 is being flushed down to its children (by partitioning elements around pivot P6). The flush at P6 was triggered during flushing P4’s input buffer, which in turn has been called while flushing P8 (the input). Buffer sizes for the three internal levels are shown next to the buffers. k-partitioners are defined recursively from a √ k-partitioner at the top, a collection of √ k middle buffers, and √ k further √ k-partitioners, each partitioning from one middle buffer to √ k output buffers. (All sizes here ignore floors and ceilings; for the precise definition valid for all k, see [6].) 6 Deterministic Cache-Oblivious Funnelselect ▶ Lemma 8 (Brodal and Wild [6, Lemma 3]). Given an unsorted array of N ≥ kd elements and k − 1 pivots P1 ≤ · · · ≤ Pk−1, a k-partitioner can partition the elements into k buckets S1, . . . , Sk, such all elements x in bucket Si satisfy Pi−1 ≤ x ≤ Pi. The algorithm is cache- oblivious and performs O(N lg k) comparisons and O \u0000k + N B (1 + logM k) \u0001 I/Os, provided a tall-cache assumption M ≥ B1+ε and d ≥ max{1 + 2/ε, 2}. The working space for the k-partitioner (ignoring input and output buffers) is O \u0000k(d+1)/2\u0001 . This is also the time required to construct a k-partitioner (again ignoring input and output buffers). The k-partitioners are structurally similar to the k-mergers from funnelsort for merging k runs cache obliviously. In [6] we pipeline the partitioning by essentially reversing the com- putations done by funnelsort, and replace each binary merging node by a binary partitioning node. 3 Deterministic multiple-selection In this section we present our deterministic cache-oblivious multiple-selection algorithm that performs optimal O(B + N) comparisons and O \u0000BI/O + N B \u0001 I/Os, under a tall-cache assumption M ≥ B1+ε. Detailed pseudo-code is given in Algorithm 1 and Algorithm 2, and the basic idea is illustrated in Figure 3. P1 P2 P3 S1 S2 S3 S4 r0 r1 r2 r3 r4 r5 r6 r7 r8 rq+1 rmin 2 rmax 2 rmin 4 rmax 4 ∆1 ∆2 ∆3 ∆4 ∆5 ∆6 ∆7 ∆8 ∆9 Figure 3 Deterministic multiple selection. The partition of an array S into buckets S1, . . . , S4 separated by pivots P1, . . . , P3, and query ranks r1, . . . , r8. In the example the maximum allowed bucket size is ∆ = ∆1, since ∆1 + ∆2 + ∆3 + ∆4 + ∆6 + ∆7 + ∆8 + ∆9 ≥ |S|/2 + 1 and ∆2 + ∆3 + ∆4 + ∆6 + ∆7 + ∆8 + ∆9 < |S|/2 + 1. Black squares are pivots and the shaded regions in buckets are the subproblems to recurse on. Given a tall-cache assumption M ≥ B1+ε, we let d = max{1 + 2/ε, 2}. The algorithm follows the general idea of making a recursive multi-way partition of the array of elements and to only recurse on subproblems with unresolved rank queries. For two consecutive query ranks ri−1 and ri, we say that the ∆i = ri − ri−1 elements of rank ri−1 + 1, . . . , ri are in a gap of size ∆i. We choose a parameter ∆, such that at least half of the elements are in gaps of size ≤ ∆ and simultaneously at least half (rounded down) of the elements are in gaps of size ≥ ∆. To compute ∆ (Algorithm 1, line 4), we compute ∆i = ri − ri−1 by a scan over the query ranks r1, . . . , rq (and r0 = 0 and rq+1 = N + 1), and perform weighted selection (Lemma 5) among ∆1, . . . , ∆q+1, where ∆i has weight wi = ∆i, and return the smallest ∆ where P ∆i≤∆ wi ≥ N/2 + 1. For the case when ∆ is small compared to N (formally, (2N)d ≥ ∆d+1 or N 1+ 1 1+ε ≥ ∆2), we simply solve the multiple-selection problem by sorting the elements (cache-obliviously using funnelsort [12]), and report the elements with ranks r1, . . . , rq by a single scan over the sorted elements. The condition on ∆ implies BI/O = Ω(SortM,B(N)), where SortM,B(N) = Θ \u0000 N B \u00001+logM/B N B \u0001\u0001 is the number of I/Os required to sort N elements in external memory [1], so this is within a constant factor of the I/O lower bound (detailed analysis in Section 4). Otherwise, we create a k-partition, where k = Θ \u0000 N ∆ \u0001 as follows (MultiPartition in Algorithm 2): We repeatedly distribute batches of ∆ elements into a set of buckets separated G. S. Brodal and S. Wild 7 Algorithm 1 Deterministic cache-oblivious multiple-selection. 1: procedure DeterministicFunnelselect(S[1..N], R[1..q]) 2: if q > 0 then 3: ∆i ← R[i] − R[i − 1] for i = 1, . . . , q + 1, assuming R[0] = 0 and R[q + 1] = N + 1 4: ∆ ← min \b ∆i ∈ {∆1, . . . , ∆q+1} \f\f P j∈{1,...,q+1}:∆j≤∆i ∆j ≥ N/2 + 1 \t 5: if (2N)d ≥ ∆d+1 or N 1+ 1 1+ε ≥ ∆2 then ▷ BI/O = Ω(SortM,B(N)) 6: S ← Funnelsort(S) 7: Report S[R[1]], . . . , S[R[q]] 8: else 9: (P1, . . . , Pk−1), (S1, . . . , Sk) ← MultiPartition(S, ∆) 10: ¯r0 ← 0 11: for i ← 1, . . . , k do 12: ¯ri ← ¯ri−1 + |Si| + 1 ▷ ¯ri is rank of Pi 13: Ri ← {r | r ∈ R ∧ ¯ri−1 < r < ¯ri} ▷ Rank queries to bucket Si 14: if |Ri| > 0 then 15: rmax i ← max(Ri) 16: ¯Si, pmax, S≥ ← Select(Si, rmax i − ¯ri−1) 17: if |Ri| > 1 then 18: rmin i ← min(Ri) 19: S≤, pmin, ¯Si ← Select( ¯Si, rmin i − ¯ri−1) 20: Report pmin 21: if |Ri| > 2 then 22: ¯Ri ← {r − rmin i | r ∈ Ri \\ {rmin i , rmax i }} 23: DeterministicFunnelselect( ¯Si, ¯Ri) 24: Report pmax 25: if ri ∈ R then 26: Report Pi Algorithm 2 Given an array S with N elements and a bucket capacity ∆, where (2N) d d+1 ≤ ∆ ≤ N, partition S into k buckets S1, . . . , Sk separated by k − 1 pivots P1, . . . , Pk−1, where \u0004 ∆ 2 \u0005 ≤ |Si| ≤ ∆. 1: procedure MultiPartition(S[1..N], ∆) 2: Requires (2N) d d+1 ≤ ∆ ≤ N 3: k ← 1, S1 ← {} ▷ Initially only one empty bucket and no pivots 4: for i ← 1 to N step ∆ do 5: ¯S ← S[i.. min(i + ∆ − 1, N)] ▷ Next batch to distribute to buckets 6: Distribute ¯S to buckets S1, . . . Sk using pivots P1, . . . , Pk−1 with a k-partitioner 7: while there exists a bucket Sj with |Sj| > ∆ do ▷ Split bucket Sj 8: S≤, p, S≥ ← Select(Sj, ⌈|Sj|/2⌉) 9: Rename Sj+1, . . . , Sk to Sj+2, . . . , Sk+1 and Pj, . . . , Pk−1 to Pj+1, . . . , Pk 10: Sj ← S≤, Pj ← p, Sj+1 ← S≥ 11: k ← k + 1 12: return (P1, . . . , Pk−1), (S1, . . . , Sk) 8 Deterministic Cache-Oblivious Funnelselect by pivot elements. Initially we have one empty bucket and no pivot. Whenever a bucket reaches size > ∆, the bucket is split into two buckets of size ≤ ∆ separated by a new pivot using the (cache-oblivious) linear-time median selection algorithm (Lemma 2). To distribute a batch of elements into the current set of buckets we use a cache-oblivious k-partitioner (Lemma 8, which depends on the tall-cache assumption parameter d) built using the current set of pivots. Note that we need to construct a new k-partitioner after each batch of ∆ elements has been distributed, since the number of buckets and pivots can increase. For the computation to be I/O efficient, we allocate in memory space for a \u0004 2N ∆ \u0005 -partitioner followed by space for \u0004 2N ∆ \u0005 buckets of capacity 2∆ (in the proof of Lemma 9 we argue that the number of buckets created is at most 2N ∆ and each bucket will never exceed 2∆ elements). The space for the partitioner is reused for each new batch, and whenever a bucket is split into two new buckets, one bucket remains in the old bucket’s allocated space and the other bucket is placed in next available slot for a bucket. This ensures all buckets are stored consecutively in memory, albeit in arbitrary order. After having constructed the buckets we compute the ranks of the pivots from the bucket sizes, and consider the gaps with at least one unresolved rank query. If the rank of a pivot coincides with a query rank, we report this pivot just after having considered the preceding bucket. Before recursing on the elements in a bucket, we first find the minimum and maximum query ranks rmin and rmax in the bucket by a scan over the bucket’s query ranks, and find and report the corresponding elements in the bucket using linear-time selection (Lemma 2). Finally, we only recurse on the elements between ranks rmin and rmax, provided there are any unresolved rank queries to the bucket. This ensures that when recursing on a subproblem of size ¯N, all elements in the subproblem are in gaps of size < ¯N in the original input. By reporting the elements at the appropriate times during the recursion, elements will be reported in increasing order. The partitioning of an array S into buckets is illustrated in Figure 3. The crucial property is that for a gap ∆i ≥ ∆, the two query ranks ri−1 and ri defining the gap cannot be in the same bucket, implying that no element in this gap will be part of a recursive subproblem (see, e.g., gaps ∆1 and ∆5 in Figure 3). Pseudocode for our algorithm is shown in Algorithm 1 and Algorithm 2. We assume Select(S, k) is the deterministic linear-time selection algorithm from Lemma 2, and that it returns a triple S≤, p, S≥, that is a partition of S, where p is the element of rank k, S≤ are the elements of rank 1, . . . , k −1 in arbitrary order, and S≥ the elements of rank k +1, . . . , |S| in arbitrary order. 4 Analysis We first analyze the number of comparisons and I/Os performed by MultiPartition in Algorithm 2, that deterministically performs a k-way partition of N elements into k = O \u0000 N ∆ \u0001 buckets separated by k − 1 pivots, where each bucket has size at most ∆. The following lemma summarizes the precise properties of MultiPartition. ▶ Lemma 9. For N ≥ ∆ and ∆d+1 ≥ (2N)d, MultiPartition creates k ≤ 2N ∆ buckets and k − 1 pivots, each bucket has size at most ∆, and performs O(N lg k) comparisons and O \u0000k2 + N B (1 + logM k) \u0001 I/Os. Proof. We first bound the sizes of the buckets created by MultiPartition. The algorithm repeatedly distributes batches of at most ∆ elements to buckets and splits all overflowing buckets of size > ∆ before considering the next batch. It is an invariant that before G. S. Brodal and S. Wild 9 distributing a batch, all buckets have size at most ∆. Furthermore, as soon as the first bucket is split, all buckets have size at least \u0004 ∆ 2 \u0005 , since whenever an overflowing bucket of size s > ∆ is split the new buckets have initial sizes \u0004 s−1 2 \u0005 and \u0006 s−1 2 \u0007 . Here “−1” is due to one element becomes a pivot. The smallest bucket size is achieved when s = ∆ + 1, where the smallest bucket size is \u0004 ∆+1−1 2 \u0005 = \u0004 ∆ 2 \u0005 . Note that the buckets after the split have size at most ∆, since all buckets had at most ∆ elements before the distribution of a batch of at most ∆ elements to the buckets, i.e., s ≤ 2∆. To bound the total number of buckets k created, observe that if ∆ = N then no bucket will be split and k = 1. Otherwise, ∆ < N and at least two buckets are created, and k \u0004 ∆ 2 \u0005 + k − 1 ≤ N, since all buckets have size at least \u0004 ∆ 2 \u0005 and there are k − 1 pivots. We have N ≥ k \u0000 ∆ 2 − 1 2 \u0001 + k − 1 = k∆ 2 + k 2 − 1 ≥ k∆ 2 , since k ≥ 2, i.e., the total number of buckets created k ≤ 2N ∆ . To analyze the number of comparisons and I/Os performed, we need to consider the \u0006 N ∆ \u0007 distribution steps and at most 2N ∆ −1 bucket splittings. Since each bucket splitting involves at most 2∆ elements, each bucket splitting can be performed cache-obliviously by a linear-time selection algorithm (Lemma 2) using O(∆) comparisons and O \u00001 + ∆ B \u0001 I/Os, assuming each bucket is stored in a buffer of 2∆ consecutive memory cells. In total the k −1 = Θ \u0000 N ∆ \u0001 bucket splittings require O(N) comparisons and O \u0000k + N B \u0001 I/Os. A k-partitioner for partitioning ∆ elements uses O(∆ lg k) comparisons and O \u0000k + ∆ B (1 + logM k) \u0001 I/Os (Lemma 8), assuming k is sufficiently small according to the tall-cache assumption (see below). This includes the cost of constructing the k-partitioner. The total cost for all \u0006 N ∆ \u0007 distribution steps becomes O(N lg k) comparisons and O \u0000k N ∆ + N B (1 + logM k) \u0001 = O \u0000k2 + N B (1 + logM k) \u0001 I/Os. By Lemma 8, the tall-cache assumption M ≥ B1+ε implies that for a k-partitioner and an input of size ∆, it is required that ∆ ≥ kd for the I/O bounds to hold (recall d = max{1 + 2/ε, 2}). The input assumption ∆ ≥ \u0000 2N ∆ \u0001d together with k ≤ 2N ∆ ensure that ∆ ≥ kd. ◀ We now prove our main result that DeterministicFunnelselect in Algorithm 1 is an optimal deterministic cache-oblivious multiple-selection algorithm. Crucial to the analysis is to show that the choice of ∆ balances early pruning of buckets without queries with simultaneously achieving efficient I/O bounds. ▶ Theorem 10. DeterministicFunnelselect performs O(B + N) comparisons and O \u0000BI/O + N B \u0001 I/Os cache-obliviously in a cache model with tall assumption M ≥ B1+ε, for some constant ε > 0. Proof. We first consider the consequences of the choice of ∆. By the choice of ∆, we have P ∆i<∆ ∆i < N/2 + 1. Since each bucket Si has size at most ∆, and we only recurse on subsets that are (the union of) gaps where the two bounding rank queries of the gaps are both in the same bucket, we only recurse on gaps with ∆i < ∆ elements (see Figure 3). A recursive subproblem between query ranks rs and rt, where 1 ≤ s < t ≤ q, contains rt − rs − 1 = (Pt i=s+1 ∆i) − 1 elements. It follows that (A) all recursive subproblems in total contain at most P ∆i<∆ ∆i − 1 < N/2 elements and each subproblem has size ≤ ∆ − 2. (B) P ∆i≤∆ ∆i ≥ N/2 + 1, i.e., at least N/2 elements are in gaps of size at most ∆. To analyze the number of comparisons performed, we use a potential argument where one unit of potential can pay for O(1) comparisons, and all comparisons performed can be charged to the released potential. We define the potential of an element x in a gap of size ∆i to be 1 + lg N ∆i , where N is the size of the current recursive subproblem x resides in. The total initial potential is at most N + Pq+1 i=1 ∆i lg N ∆i = O(B + N). 10 Deterministic Cache-Oblivious Funnelselect We first consider the number of comparisons for the non-sorting case (Algorithm 1, lines 9–26). If an element x in a gap of size ∆i ≤ ∆ participates in a recursive call of size < ∆, the potential released for x is at least \u00001 + lg N ∆i \u0001 − \u00001 + lg ∆ ∆i \u0001 = lg N ∆. If an element x in a gap of size ∆i ≤ ∆ does not participate in a recursive call, the potential released for x is 1+lg N ∆i ≥ 1+lg N ∆. Finally, elements in gaps of size > ∆ will not participate in recursive calls, and will each release at least potential 1. It follows that the released potential is at least N 2 + N 2 lg N ∆, since at least N/2 elements are in gaps of size ≤ ∆ (property (B), contributing the second summand) and at most N/2 elements are in gaps of size < ∆ and participate in recursive calls (property (A)), i.e., at least N/2 elements are in gaps of size ≥ ∆ (contributing the first summand). By Lemma 9, MultiPartition requires O(N lg k) comparisons, and since k = O(N/∆) this can be covered by the released potential. The additional comparisons required for computing ∆ with a linear-time weighted section algorithm (Lemma 5) and performing Select (Lemma 2) at most twice on each bucket require in total at most O(N) comparisons, and can also be charged to the released potential. It follows that for the non-sorting case the released potential can cover for all comparisons performed. In the sorting case, a single call to Funnelsort is performed causing O(N lg N) compar- isons (Lemma 6). No further recursive calls are made and the potential of all elements is released. At least N + N 2 lg N ∆ potential is released, since at least N/2 elements are in gaps of size ≤ ∆ (property (B)). In the sorting case, either (2N)d ≥ ∆d+1 or N 1+ 1 1+ε ≥ ∆2. If (2N)d ≥ ∆d+1, we have ∆ ≤ (2N) d d+1 and N ∆ ≥ N/(2N) d d+1 ≥ 1 2N 1 d+1 . It follows that the released potential is at least N + N 2 lg \u0000 1 2N 1 d+1 \u0001 ≥ 1 2(d+1)N lg N, covering the cost for the com- parisons. Otherwise, N 1+ 1 1+ε ≥ ∆2, i.e., ∆ ≤ N 1 2 \u00001+ 1 1+ε \u0001 and we have N ∆ ≥ N/N 1 2 \u00001+ 1 1+ε \u0001 = N ε 2(1+ε) and the potential released is at least N + N 2 lg N ∆ ≥ N + ε 4(1+ε)N lg N and can cover the cost for the comparisons. Note that the comparison bound depends on the tall-cache parameters ε and d. To analyze the I/O cost we assign an I/O potential to an element x in gap of size ∆i of 1 B \u00001 + logM N ∆i \u0001 , where N is the size of the current subproblem x resides in. Similar to the comparison potential, it follows that the non-sorting case releases I/O potential 1 2 \u0000 N B + N B logM N ∆ \u0001 . The number of I/Os required is O \u00001 + N B \u0001 I/Os for scanning the input and computing ∆ using weighted selection (Lemma 5), O \u0000k + N B \u0001 I/Os for selecting the minimum and maximum rank elements in each bucket (Lemma 2), and O \u0000k2+ N B (1+logM k) \u0001 I/Os for the k-partitioning (Lemma 8), i.e., in total O \u0000k2 + N B (1 + logM k) \u0001 I/Os. It follows that the I/O cost can be charged to the released potential, provided k2 = O \u0000 N B \u0001 . To address this, we need to consider two cases depending on the size N of a subproblem. If the problem completely fits in internal memory together with all the geometric decreasing recursive subproblems, assuming a stack-oriented memory allocation, then considering this problem will in total cost O \u00001 + N B \u0001 I/Os, including all recursive subproblems. That means, there exists a constant c > 0 such that for N ≤ cM, the I/O cost for handling such problems can be charged to the parent subproblem creating the subproblem. It follows that we only need to consider the I/O cost for subproblems of size N ≥ cM. Since M ≥ B1+ε, we have N ≥ cM ≥ cB1+ε, i.e., B ≤ \u0000 N c \u00011/(1+ε). Since k = O \u0000 N ∆ \u0001 , to prove k2 = O \u0000 N B \u0001 it is sufficient to prove \u0000 N ∆ \u00012 = O \u0010 N (N/c)1/(1+ε) \u0011 . This holds, e.g., when N 1+ 1 1+ε ≤ ∆2, which is always fulfilled in the non-sorting case. For the sorting case, we have similarly to the comparison potential that Ω \u0000 N B logM N \u0001 I/O potential is released, which can cover the I/O cost for cache-oblivious sorting (Lemma 6). ◀ G. S. Brodal and S. Wild 11 5 Conclusion With deterministic funnelselect, we close the gap left in previous work and obtain an I/O-optimal cache-oblivious multiple-selection algorithm that does not need to resort to randomization to achieve its performance. This settles the complexity of the multiple- selection problem in the cache-oblivious model (including the fine-grained analysis based on the query-rank entropy B). There are open questions left in other variants of the problem. Like randomized funnelse- lect [6], deterministic funnelselect cannot deal with queries arriving in an online fashion, one after the other. This problem has been addressed in the external-memory model [2], but no cache-oblivious I/O-optimal solution is known. Concerning the transition from single selection by rank to sorting, which multiple selection allows us to study, some questions remain unanswered. For example, in the cache-oblivious model, it is known that sorting with optimal I/O-complexity is only possible under a tall- cache assumption (such as the one made in this work); for single selection, however, such a restriction is not necessary. It would be interesting to study the transition between the problems and find out, how “sorting-like” a multiple-selection instance has to be to likewise require a tall cache for I/O-optimal cache-oblivious algorithms. Another direction for future work are parallel algorithms for multiple selection that are also cache-oblivious and I/O efficient. References 1 Alok Aggarwal and Jeffrey Scott Vitter. The input/output complexity of sorting and related problems. Commun. ACM, 31(9):1116–1127, 1988. doi:10.1145/48529.48535. 2 Jérémy Barbay, Ankur Gupta, Srinivasa Rao Satti, and Jon Sorenson. Near-optimal online multiselection in internal and external memory. Journal of Discrete Algorithms, 36:3–17, jan 2016. doi:10.1016/j.jda.2015.11.001. 3 Chaya Bleich and Michael L. Overton. A linear-time algorithm for the weighted median problem. Technical Report 75, New Yourk University, Department of Computer Science, April 1983. URL: https://archive.org/details/lineartimealgori00blei/. 4 Manuel Blum, Robert W. Floyd, Vaughan R. Pratt, Ronald L. Rivest, and Robert Endre Tarjan. Time bounds for selection. J. Comput. Syst. Sci., 7(4):448–461, 1973. doi:10.1016/ S0022-0000(73)80033-9. 5 Gerth Stølting Brodal and Rolf Fagerberg. Cache oblivious distribution sweeping. In Peter Wid- mayer, Francisco Triguero Ruiz, Rafael Morales Bueno, Matthew Hennessy, Stephan J. Eiden- benz, and Ricardo Conejo, editors, Automata, Languages and Programming, 29th International Colloquium, ICALP 2002, Malaga, Spain, July 8-13, 2002, Proceedings, volume 2380 of Lecture Notes in Computer Science, pages 426–438. Springer, 2002. doi:10.1007/3-540-45465-9_37. 6 Gerth Stølting Brodal and Sebastian Wild. Funnelselect: Cache-oblivious multiple selection. In Inge Li Gørtz, Martin Farach-Colton, Simon J. Puglisi, and Grzegorz Herman, editors, 31st Annual European Symposium on Algorithms, ESA 2023, September 4-6, 2023, Amsterdam, The Netherlands, volume 274 of LIPIcs, pages 25:1–25:17. Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2023. doi:10.4230/LIPICS.ESA.2023.25. 7 J. M. Chambers. Partial sorting [M1] (algorithm 410). Commun. ACM, 14(5):357–358, 1971. doi:10.1145/362588.362602. 8 David P. Dobkin and J. Ian Munro. Optimal time minimal space selection algorithms. J. ACM, 28(3):454–461, 1981. doi:10.1145/322261.322264. 9 Dorit Dor and Uri Zwick. Selecting the median. SIAM Journal on Computing, 28(5):1722–1758, 1999. doi:10.1137/s0097539795288611. 12 Deterministic Cache-Oblivious Funnelselect 10 Robert W. Floyd and Ronald L. Rivest. Expected time bounds for selection. Communications of the ACM, 18(3):165–172, March 1975. doi:10.1145/360680.360691. 11 Matteo Frigo, Charles E. Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache- oblivious algorithms. In 40th Annual Symposium on Foundations of Computer Science, FOCS ’99, 17-18 October, 1999, New York, NY, USA, pages 285–298. IEEE Computer Society, 1999. doi:10.1109/SFFCS.1999.814600. 12 Matteo Frigo, Charles E. Leiserson, Harald Prokop, and Sridhar Ramachandran. Cache- oblivious algorithms. ACM Trans. Algorithms, 8(1):4:1–4:22, 2012. doi:10.1145/2071379. 2071383. 13 C. A. R. Hoare. Algorithm 65: find. Commun. ACM, 4(7):321–322, 1961. doi:10.1145/ 366622.366647. 14 Xiaocheng Hu, Yufei Tao, Yi Yang, and Shuigeng Zhou. Finding approximate partitions and splitters in external memory. In Proceedings of the 26th ACM symposium on Parallelism in algorithms and architectures. ACM, June 2014. doi:10.1145/2612669.2612691. 15 Kanela Kaligosi, Kurt Mehlhorn, J. Ian Munro, and Peter Sanders. Towards optimal multiple selection. In Luís Caires, Giuseppe F. Italiano, Luís Monteiro, Catuscia Palamidessi, and Moti Yung, editors, Automata, Languages and Programming, 32nd International Colloquium, ICALP 2005, Lisbon, Portugal, July 11-15, 2005, Proceedings, volume 3580 of Lecture Notes in Computer Science, pages 103–114. Springer, 2005. doi:10.1007/11523468_9. 16 Helmut Prodinger. Multiple Quickselect – Hoare’s Find algorithm for several elements. Information Processing Letters, 56(3):123–129, November 1995. doi:10.1016/0020-0190(95) 00150-b. 17 Arnold Schönhage, Mike Paterson, and Nicholas Pippenger. Finding the median. J. Comput. Syst. Sci., 13(2):184–199, 1976. doi:10.1016/S0022-0000(76)80029-3. 18 Michael Ian Shamos. Geometry and statistics: Problems at the interface. In Joseph Frederick Traub, editor, Algorithms and Complexity: New Directions and Recent Results, pages 251– 280. Academic Press, 1976. URL: http://euro.ecom.cmu.edu/people/faculty/mshamos/ 1976Stat.pdf. "
}