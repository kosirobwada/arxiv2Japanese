{
    "optim": "Towards Optimal Learning of Language Models\nYuxian Gu1,2‚àó,\nLi Dong2,\nYaru Hao2,\nQingxiu Dong2,\nMinlie Huang1,\nFuru Wei2\n1The CoAI Group, Tsinghua University\n2Microsoft Research\nhttps://aka.ms/GeneralAI\nAbstract\nThis work studies the general principles of improving the learning of language\nmodels (LMs), which aims at reducing the necessary training steps for achieving\nsuperior performance. Specifically, we present a theory for the optimal learning of\nLMs. We first propose an objective that optimizes LM learning by maximizing the\ndata compression ratio in an ‚ÄúLM-training-as-lossless-compression‚Äù view. Then,\nwe derive a theorem, named Learning Law, to reveal the properties of the dynamics\nin the optimal learning process under our objective. The theorem is then validated\nby experiments on a linear classification and a real-world language modeling task.\nFinally, we empirically verify that the optimal learning of LMs essentially stems\nfrom the improvement of the coefficients in the scaling law of LMs, indicating great\npromise and significance for designing practical learning acceleration methods.\nOur code can be found at https://aka.ms/LearningLaw.\nConventional LM Learning\nOptimal LM Learning\n>\n>\n>\nTraining Steps\nLoss\n0\nùëá \nConventional LM Learning\nOptimal LM Learning\nLearning Speed(           |        )  \nArea Under Loss Curve(           |       )  \nData Compression Ratio(       |           )  \nData Compression Ratio(       |            )  \nArea Under Loss Curve(            |\n)  \nLearning Speed(            |        )  \nFigure 1: Our objective is to minimize the area under loss curve, which is equivalent to maximizing\nthe compression ratio of training corpus in the ‚ÄúLM-training-as-lossless-compression‚Äù view. A\nlearning law is proposed to reveal the training dynamics of the above optimal learning.\nConventional LM Learning\n(Near-)Optimal LM Learning\n0\n1000\n2000\n3000\n4000\n4.5\n5.0\n2.41√ó Speedup\n4.0\n3.5\nTraining Steps\nLoss\nFigure 2: Optimal learning gets the theoreti-\ncal speedup upper bound of Transformer LM\ntraining on TinyStories corpus [EL23].\nScaling Laws\nB\nŒ≤\nConventional LM Learning\n3.16 √ó 108\n0.12\n(Near-)Optimal LM Learning\n1.99 √ó 107\n0.14\nTable 1: The (near-)optimal LM learning improves\nthe scaling laws [KMH+20] over conventional LM\ntraining. The coefficients B, Œ≤ are used to fit the\nloss curves in Figure 2, i.e., Loss = (B/t)Œ≤.\n‚àóContribution during an internship at Microsoft Research. ‚ü®guyx21@mails.tsinghua.edu.cn‚ü©\narXiv:2402.17759v1  [cs.CL]  27 Feb 2024\n1\nIntroduction\nWith the thriving of language models (LMs; HZD+21, BHA+21), there is an increasing focus on\nimproving the learning [PR12, WSSC19] of LMs, which aims at accelerating the learning speed\nand achieving a certain model performance with as few training steps as possible [WWL+23].\nThis focus helps humans explore the limits of LMs given the rapid growth of their computational\ndemand [HBM+22], and promotes democratization [CMS+23] of large language models (LLMs;\nBMR+20, Ope22, Ope23, CND+23, ADF+23), which is valuable for both research communities\nand industry sectors [TLI+23, TMS+23, JSM+23].\nIn this paper, we present a theory for optimal learning of LMs. Unlike prior works exploring practical\nacceleration methods at the model-level [XYH+20, ZH20], optimizer-level [YLR+20, LLH+24], or\ndata-level [TSAM23, ATS+23, XPD+23], our work demonstrates the principles of optimizing the\nLM learning speed, including the optimization objective, the property of optimal learning dynamics,\nand the essential improvement of the learning acceleration.\nSpecifically, for the optimization objective, we propose to minimize the area under the loss curve\n(AUC; CM03), which has a clear physical significance: the description length when we view the next-\ntoken-prediction LM training process as lossless compression of the training data [Bel19, MCKX22,\nRae23]. As shown in Figure 1, a learning process with the smallest loss AUC corresponds to the\nhighest compression ratio. Simultaneously, the loss in this process also converges to a small value at\nthe highest rate, given sufficiently large total training steps. Therefore, we consider optimizing LM\nlearning equivalent to maximizing the corresponding compression ratio of the learning process,\nand adopt the latter as the optimization objective in our theory. Similar objectives are also employed\nto interpret the remarkable generalization performance of recent LLMs [VNK+23, DRD+24].\nWe then derive a theorem, named Learning Law, that characterizes the property of dynamics in the\nLM learning process that achieves the optimum of our objective. Here, a learning process is induced\nby a learning policy that determines which data points the LM learns as the training progresses. In\nthis way, we solve the optimal learning policy in the sense that the corresponding compression ratio\nis maximized, and obtain our Learning Law (see Theorem 3.1 for a formal expression):\nLearning Law\nAll examples have the same contribution to the LM in the optimal learning process.\nOptimal LM Learning\nConventional LM Learning \nContribution of the ùëõ!\" example\nContribution of the ùëõ!\" example\nA\nB\nCompression Ratio \nof Optimal Learning\nConventional \nLearning\nCompression Ratio\nSimilarity of Example Contributions\nFigure 3: A: 3-D illustration of Learning Law (Theorem 3.1). In the optimal learning process, all\ntraining examples should have the same contribution to LM learning, where the contribution is defined\nas the dot-product of the gradient on individual samples (‚àálm, ‚àáln, and ‚àálk) and the gradient of a\ndesired loss (‚àáL). See Section 3.2 for rigorous notation definitions. B: Experimental evidence of\nLearning Law. When LM learning approaches the optimum, the similarity of example contributions\ntends to +‚àû, which means all examples have the same contribution to the LM.\nAs shown in Figure 3, the contribution of an example is defined as the dot-product of its gradient\nand the gradient of a desired loss2 , which measures its influence on the LM in the desired learning\n2Note that the desired loss is not necessarily the same as the training loss as discussed in Section 2.\n2\ndirection. Learning Law also suggests a matching of local and global learning speed in the optimal\nlearning process, which interprets the optimal learning policy as a dynamic data re-weighting strategy\nthat encourages the LM to learn highly contributive examples and simultaneously avoid over-fitting\nthem. Similar mechanisms are also found critical to the best teaching methods for humans in\npsychological research [Met09, KPA12].\nWe examine our theory by experiments on linear classification tasks based on Perceptron3 [MP43]\nand real-world language modeling tasks based on Transformer [VSP+17]. We first design a gradient-\nbased method to search for the optimal learning policy under our objective. Then, we verify that\nthe dynamics of the learning process induced by the found near-optimal policy aligns well with\nour Learning Law. Finally, as shown in Table 1, we provide empirical evidence showing that the\nnear-optimal learning policy essentially improves the coefficients in the training step scaling law of\nLMs [KMH+20], which leads to 5.50√ó and 2.41√ó speedup to Perceptron and Transformer learning,\nrespectively. This emphasizes the promise and significance of exploring more scalable methods to\noptimize the learning policy in practice and accelerate the training of LLMs.\n2\nProblem Formulation\nWe consider LM training on a large-scale dataset with N examples {xtrn\nn }N\nn=1 for a sufficiently\nlarge total training time steps T. Let Œ≥n,t denote the weight of the nth training example at the time\nstep t, a learning policy is represented by a time-variant distribution over N training examples\nŒ≥t = [Œ≥1,t, Œ≥2,t, ¬∑ ¬∑ ¬∑ , Œ≥n,t]‚ä§, satisfying PN\nn=1 Œ≥n,t = 1 and Œ≥n,t ‚â• 0 for 1 ‚â§ n ‚â§ N, 0 ‚â§ t ‚â§ T ‚àí 1.\nThe conventionally trained LM learns with a policy Œ≥c\nn,t =\n1\nN (conventional learning). Recent\nworks [PLKS20, ABL+22] have shown that theories derived based on Gradient Decent (GD) offer\ninsights into other gradient-based algorithms [KB15]. Therefore, for simplicity, we assume the LM is\ntrained with GD for t = 0, 1, ¬∑ ¬∑ ¬∑ , T ‚àí 1:\nLtrn\nt (Œ∏t) =\nN\nX\nn=1\nŒ≥n,tl(xtrn\nn , Œ∏t),\nŒ∏t+1 = Œ∏t ‚àí Œ∑‚àáLtrn\nt (Œ∏t),\n(1)\nwhere Œ∏t ‚àà RD is the model parameters flattened into a D-dimensional vector at the time step t,\nŒ∑ is the learning rate, and l(¬∑, ¬∑) is the loss function of the learning problem. For LMs, l(¬∑, ¬∑) is\ntypically the Maximum Likelihood Estimation (MLE) loss: l(x, Œ∏t) = ‚àí log pŒ∏t(x), where x is a\ntext sequence. Following [XSML23] and [MBR+22], we focus on the learning speed reflected by\nthe reduction rate of a desired loss Ldsr computed on K examples {xdsr\nk }K\nk=1 that do not necessarily\nfollow the same distribution as the training examples:\nLdsr(Œ∏t) = 1\nK\nK\nX\nk=1\nl(xdsr\nk , Œ∏t).\n(2)\nThis formulation applies to a broad of practical scenarios including classical machine learning using\na validation set to prevent over-fitting [Vap99], large-scale pre-training relying on a carefully curated\nheld-out corpus to evaluate generalization performance [KMH+20], and domain adaptation where a\nnatural difference exists between training and target distribution [XSML23]. As such, we search for\nthe learning policy Œ≥t that maximizes the reduction rate of Ldsr(Œ∏t) to optimize LM learning.\nHowever, direct analysis of this optimization problem is difficult due to the discreteness of GD.\nTherefore, we focus on the continuous limit of GD by considering the corresponding gradient flow of\nEquation 1 for t ‚àà [0, T], which is more amenable to theoretical analysis [SDBD20]:\nd\ndtŒ∏(t) = ‚àí‚àáLtrn(Œ∏(t), t) = ‚àí‚àá\nN\nX\nn=1\nŒ≥n(t)l(xtrn\nn , Œ∏(t)),\n(3)\nwhere Œ≥n(t) is a smooth interpolation function of Œ≥n,t. According to the results in numerical analysis,\nGD defined in Equation 1 is the Euler method to approximately solve the initial value problem of\nthe gradient flow in Equation 3, and Œ∏(t) ‚âà Œ∏t when Œ∑ is sufficiently small [EC21]. In Section 4, we\nshow that the results derived from this limit align well with the experiments in discrete settings.\n3In Appendix A.3, we provide a lossless data compression view of the Perceptron training, indicating that\nour theory also applies.\n3\n3\nTheory for Optimal Learning of LMs\nIn this section, we present our theory in the continuous limit of GD. We first propose an objective\nfor ‚Äúmaximizing the reduction rate of Ldsr by optimizing the learning policy‚Äù. Then, we derive our\nmain theorem, named Learning Law, which introduces a necessary condition for the dynamics of the\nlearning process induced by the policy that achieves the optimum of the objective.\n3.1\nObjective: Maximizing Compression Ratio\nWe characterize the reduction rate of Ldsr with the area under the curve of Ldsr(Œ∏(t)) (AUC of Ldsr)\nand minimize this area to achieve high learning speed:\nmin\nŒ≥(t)\nZ T\n0\nLdsr(Œ∏Œ≥(t))dt,\ns.t.\nN\nX\nn=1\nŒ≥n(t) = 1,\nŒ≥n(t) ‚â• 0, n = 1, 2, ¬∑ ¬∑ ¬∑ , N,\n(4)\nwhere Œ≥(t) = [Œ≥1(t), Œ≥2(t), ¬∑ ¬∑ ¬∑ , Œ≥n(t)]‚ä§ and Œ∏Œ≥(t) is an alias of Œ∏(t) satisfying Equation 3 to\nemphasize its dependency on Œ≥(t). As shown in Figure 1, for sufficiently large T, a learning process\nwith minimal loss AUC owns the highest loss reduction rate. Interestingly, the AUC of Ldsr has a\nphysical significance from the ‚ÄúLM-training-as-lossless-compression‚Äù view [Rae23]: the resulting\ndescription length of compressing data drawn from the desired data distribution. Therefore,\nEquation 4 is equivalent to maximizing the corresponding compression ratio. Note that unlike\n[DRD+24] that studies encoding data using a well-trained LM, we view the entire LM training as a\ncompression process. We provide more discussion of these two perspectives in Section 5. Besides,\nthere are still slight differences between our statement and that in prior works viewing the training\nprocess as lossless compression [Bel19, MCKX22, Rae23]: we consider the desired loss AUC of\nGD training for multiple epochs, while the previous statement is about the training loss AUC with\nsingle-epoch SGD training. More discussion about this difference can be found in Appendix A.2.\n3.2\nLearning Law\nEquation 4 defines an Optimal Control problem that can be solved by Maximum Principle [Pon18].\nHowever, we find the solution hard to interpret and verify in practical LM learning. Therefore, in this\nwork, we derive a looser necessary condition for the optimum of Equation 4.\nTheorem 3.1 (Learning Law). When an LM is trained with an optimal learning policy, which\nyields a learning process corresponding to a maximum compression ratio on the desired data\ndistribution, the following condition holds for 0 < t ‚â§ T and any m, n such that Œ≥m(t) > 0,\nŒ≥n(t) > 0:\n‚àáL ¬∑ ‚àálm = ‚àáL ¬∑ ‚àáln = Const,\n(5)\nwhere ‚àáL = ‚àáLdsr(Œ∏(t)) = ‚àá 1\nK\nPK\nk=1 l(xdsr\nk , Œ∏(t)), ‚àálm = ‚àál(xtrn\nm , Œ∏(t)), ‚àáln =\n‚àál(xtrn\nn , Œ∏(t)), and ¬∑ is dot-product. Const = ‚àí d\ndtLdsr(Œ∏(t)) is the desired loss change rate\nover time and is independent of n and m.\nTo\nprove\nTheorem\n3.1,\nwe\napply\nthe\nEuler-Lagrange\n(EL)\nequation\n[GS+00]\nand\nKarush‚ÄìKuhn‚ÄìTucker (KKT) conditions [Ber16] to Equation 4, which results in the condition:\n‚àáLdsr(Œ∏(t)) ¬∑ ‚àál(xtrn\nn , Œ∏(t)) = ‚àí d\ndtLdsr(Œ∏(t)). A full proof is shown in Appendix B.\n‚àáL ¬∑ ‚àáln in Equation 5 represents the contribution of the training example xtrn\nn to Ldsr(Œ∏(t)), which\nis maximized when the gradient on xtrn\nn shares the same direction with the gradient of Ldsr(Œ∏(t)).\nWe denote CTn(t) = ‚àáL ¬∑ ‚àáln = ‚àáLdsr(Œ∏(t)) ¬∑ ‚àál(xtrn\nn , Œ∏(t)) for convenience in the rest of\nthe paper. Note that when the model is converged (‚àáLtrn(Œ∏(t), t) ‚âà 0), CTn(t) can be viewed as\nan approximation of the Influence Function [KL17] by setting the Hessian matrix of Ltrn(Œ∏, t) at\nŒ∏ = Œ∏(t) to an identity matrix [PLKS20]. In essence, Equation 5 means CTn(t) equals a value\n4\nindependent of n. Since the zero-weight examples (Œ≥n(t) = 0) are typically noisy (verified in Section\n4.5), Theorem 3.1 suggests that all non-noisy examples should be identically contributive to the\nLM in the optimal learning process. In the following, we provide more discussion of this theorem.\n3.3\nDiscussion\nTheorem 3.1 suggests a matching of the local and global learning.\nAnother interpretation of\nCTn(t) is the ‚Äúlocal learning speed‚Äù: how fast the LM learns the knowledge in xtrn\nn that is helpful to\nreduce Ldsr. This is because the dot-product operation in CTn(t) can be viewed as the projection\nof the individual loss descending velocity ‚àál(xtrn\nn , Œ∏(t)) on the desired direction. Correspondingly,\nd\ndtLdsr(Œ∏(t)) represents the LM‚Äôs ‚Äúglobal learning speed‚Äù: how fast the LM gets better by learning\nall individual xtrn\nn . As a result, CTn(t) = Const = ‚àí d\ndtLdsr(Œ∏(t)) in Theorem 3.1 indicates that the\nlocal learning speed should match the global learning speed in the optimal learning process.\nThe optimal learning policy establishes a dynamic data re-weighting strategy.\nGenerally,\nas the learning of LM progresses, CTn(t) drops because the gradient norm on each example\n||‚àál(xtrn\nn , Œ∏(t))|| decreases as the LM fits xtrn\nn . In addition, the direction of ‚àál(xtrn\nn , Œ∏(t)) diverges\nfrom ‚àáLdsr(Œ∏(t)) due to the possible discrepancy between the distribution of xtrn\nn and xdsr\nk , which also\ncontributes to the decrease of CTn(t). Therefore, Theorem 3.1 guarantees that highly contributive\nexample xtrn\nn with high CTn(t) obtains large weights for training, in order to reduce CTn(t) to\nmeet the value of other examples. On the other hand, Theorem 3.1 also ensures that the weights\nof xtrn\nn are lowered before the LM over-fits it because CTn(t) should not be too small to match the\nglobal learning speed. Altogether, this forms a dynamic training data re-weighting strategy, which is\nintuitively essential for the optimal learning policy that maximizes the learning speed of an LM.\nTheorem 3.1 is a necessary condition for the optimal learning dynamics.\nThis is because the E-L\nequation and KKT conditions are necessary conditions for the global optimum when the optimization\nproblem is non-convex. Therefore, a learning process satisfying Theorem 3.1 is not guaranteed\noptimal. For example, by setting Œ≥1(t) = 1 and Œ≥2(t) = Œ≥3(t) = ¬∑ ¬∑ ¬∑ = Œ≥N(t) = 0, Equation 5 is\nsatisfied, regardless of the values of CTn(t). This learning policy corresponds to using SGD with\nmini-batch size = 1, which is unlikely to be the optimal [MKAT18]. Therefore, searching for the\noptimal policy according to Theorem 3.1 may need regularization terms in practice, which we leave\nfor future work to explore.\n4\nExperiments\nWe conduct experiments in the discrete setting of Equation 1, where the conclusions derived from the\ncontinuous limits in Section 3 are still applicable when Œ∑ is sufficiently small [EC21]. We first design\na method to find the optimal learning policy Œ≥t ‚àà RN for 0 ‚â§ t ‚â§ T ‚àí 1, by explicitly minimizing\nthe AUC of Ldsr(Œ∏t) in the discrete setting, which maximizes the corresponding compression ratio\nof data drawn from the desired distribution. Then we examine our Learning Law (Theorem 3.1) on\nthe learning process induced by the found policies. Finally, we empirically verify that maximizing\nthe compression ratio essentially improves the scaling law coefficients [KMH+20], indicating the\npractical significance and promise of our theory.\n4.1\nFinding the Optimal Learning Policy\nTo find the optimal Œ≥t, we directly solve the discrete version of the optimization problem defined in\nEquation 4 with a Proximal Gradient Method [BC11]:\nJ(Œ≥) =\nT\nX\nt=1\nLdsr(Œ∏t),\nŒ≥t ‚Üê Proj [Œ≥t ‚àí œµ‚àáŒ≥tJ(Œ≥)] , 0 ‚â§ t ‚â§ T ‚àí 1,\n(6)\nwhere J(Œ≥) is a discrete approximation of the integral in Equation 4, œµ is the learning rate and Proj[¬∑]\nprojects a point in RN to the N-simplex, ensuring that Œ≥t is a probability distribution over N training\nexamples. The optimization process can be implemented efficiently using dynamic programming and\nJacobian-Vector-Product in PyTorch [PGM+19], which is described in detail in Appendix C.\n5\n0\n100\n200\n300\n400\n500\nOptimization Epochs\n0.15\n0.20\n0.25\nAUC of the Desired Loss\n4\n5\n6\n7\nCompression Rate (CR )\nLearning Oplicy Optimization Loss\nCompression Rate\nNear-Optimal Learning\nConventional Learning\n(a) Perceptron Linear Classification\n0\n5\n10\n15\nOptimization Epochs\n3.8\n3.9\n4.0\n4.1\nAUC of the Desired Loss\n3.0\n3.1\n3.2\n3.3\nCompression Rate (CR )\nLearning Oplicy Optimization Loss\nCompression Rate\nNear-Optimal Learning\nConventional Learning\n(b) Transformer Language Modeling\nFigure 4: Learning policy optimization results in Perceptron linear classification (a) and Transformer\nlanguage modeling tasks (b). We plot the learning policy optimization loss J(Œ≥) (solid lines),\ndefined in Equation 6, which represents the area under the curve (AUC) of the desired Perceptron or\nTransformer loss. We also show the corresponding compression ratio of the training process (dashed\nlines) in an \"LM-as-Lossless-Compression\" view. The optimization starts from conventional learning\nand smoothly converges to near-optimal learning with low loss AUC and high comprehension rate.\n0\n500\n1000\n1500\n2000\nTraining Time Steps t\n0.100\n0.09\n0.20\nDesired Loss Ldsr(\nt)\n5.50 √ó  Speedup\nConventional Learning\nNear-Optimal Learning\n(a) Perceptron Linear Classification\n0\n1000\n2000\n3000\n4000\nTraining Time Steps t\n4.0\n5.0\nDesired Loss Ldsr(\nt)\n2.41 √ó  Speedup\nConventional Learning\nNear-Optimal Learning\n(b) Transformer Language Modeling\nFigure 5: Curves of the desired loss Ldsr(Œ∏t) when the model is trained using the conventional and\nthe near-optimal learning policy. The near-optimal learning process achieves 5.50√ó speedup in\nPerceptron linear classification (a) and 2.41√ó speedup in Transformer language modeling (b).\n4.2\nExperimental Setup\nWe conduct experiments on a linear classification task based on Perceptron [MP43] and a language\nmodeling task based on Transformer [VSP+17]. See Appendix D for hyper-parameter configurations.\nPerceptron Linear Classification.\nWe adopt a teacher-student setting [Eng01] where each example\nxn = (zn, yn) is a pair of D-dimensional vector zn ‚àà RD drawn i.i.d. from Gaussian distribution,\nand a scalar yn = sign(T ¬∑ zn) given the ground truth weight T ‚àà RD. We introduce a shift between\nthe training and the desired data distribution to reflect their differences. The data are learned by an\none-layer Perception parameterized by Œ∏ ‚àà RD: on = œÉ(Œ∏ ¬∑ zn) =\n1\n1+exp(‚àíŒ∏¬∑zn), which is trained\nwith Maximum Likelihood Estimation (MLE) loss l(xn, Œ∏) = ‚àí log oyn\nn (1 ‚àí on)1‚àíyn. In Appendix\nA.3, we show that Perceptron can be viewed as a one-step LM, which means our theory still applies.\nTransformer Language Modeling.\nConsidering the computation cost of the optimal policy search-\ning, we adopt a two-layer Transformer with about 1.7M parameters and train it on TinyStories [EL23],\na high-quality pre-training corpus. We add perturbations to the training examples (see Appendix D\nfor details), which mimics the relatively low quality of the pre-training corpus in practice. Since our\ntheoretical derivation is generally applicable, we believe that our theory also applies to larger LMs.\nTo migrate the risk of over-fitting the K examples used to compute Ldsr(Œ∏t) in Section 4.1, we\nadditionally construct a held-out test set with K examples from the desired data distribution in both\nPerceptron linear classification and Transformer language modeling experiments. In the following,\n6\n10\n1\n100\nDesired Loss Ldsr(\nt)\n0.00\n0.25\n0.50\n0.75\nSNRt\n4\n5\n6\nCR\n(a) Perceptron Linear Classification\n4 √ó 100\n6 √ó 100\nDesired Loss Ldsr(\nt)\n0\n2\n4\n6\nSNRt\n3.0\n3.1\n3.2\nCR\n(b) Transformer Language Modeling\nFigure 6: Empirical evidence of our Learning Law (Theorem 3.1) in Perceptron linear classification\n(a) and Transformer language modeling (b) tasks. We measure the degree of similarity in contribution\namong different samples by SIMt, the Signal-Noise-Ratio of the contribution CTn,t of training\nexamples, calculated as the mean divided by the standard deviation of CTn,t across examples\n(Equation 7). Higher SIMt means better contribution similarity. We plot SIMt with respect to the\ndesired loss Ldsr(Œ∏t) under different learning processes. Each line is a certain learning process, whose\ncolor means the corresponding compression ratio (CR). Runs with higher CR generally get higher\nSIMt throughout learning, indicating that the example contributions are more similar to each other in\na learning process closer to the optimum, which is in line with our Learning Law (Theorem 3.1).\nwe compute and report the evaluation metrics by treating the test examples, unseen during the\npolicy optimization, as xdsr\nk in Equation 2.\n4.3\nLearning Policy Optimization Results\nA near-optimal learning policy can be found with the method in Section 4.1.\nIn Figure 4, we\nshow the optimization process of finding the optimal learning policy. We plot the learning policy\noptimization loss J(Œ≥), which is also the AUC of Ldsr(Œ∏t) in the learning process induced by Œ≥t, and\nthe corresponding compression ratio CR =\nT log |V |\nPT\nt=1 Ldsr(Œ∏t), where V is the size of the label / vocabulary\nspace for Perceptron / transformer (see Appendix A.1 for more explanation). The curve of J(Œ≥) is\nsmooth and almost converges at the end, indicating that a near-optimal learning policy is found.\nThe near-optimal learning policy yields a high acceleration ratio of the learning speed.\nIn\nFigure 5, we plot the curve of Ldsr(Œ∏t) when the Perceptron and Transformer are trained under the\nconventional and near-optimal learning policies. The near-optimal policies significantly improve the\nloss AUC, bringing about acceleration 5.50√ó and 2.41√ó at the end of the Perceptron and Transformer\ntraining, respectively. Note that all reported metrics are computed on the test set unseen during the\npolicy optimization, suggesting that the near-optimal policy does not over-fit the specific examples\nused to compute Ldsr(Œ∏t) but helps the model learn faster on the desired distribution.\n4.4\nDirect Verification of Learning Law (Theorem 3.1)\nWe examine the similarity between CTn,t which is the discrete version of the individual sample\ncontribution CTn(t) in a certain learning policy and satisfies CTn,t = CTn(t) for t = 1, 2, ¬∑ ¬∑ ¬∑ , T.\nThe similarity (SIM) is measured by the Signal-Noise-Ratio of CTn,t:\nSIMt = CTt\nsCT,t\n,\n(7)\nwhere CTt = PN\nn=1 Œ≥n,tCTn,t is the weighted mean and sCT,t =\nr PN\nn=1 1[Œ≥n,tÃ∏=0](CTn,t‚àíCTt)\n2\nPN\nn=1 1[Œ≥n,tÃ∏=0]‚àí1\nis the standard deviation of CTn,t for training examples with non-zero weight. The higher SIMt\nmeans that the training examples have more similar CTn,t. Note that SIMt is dimensionless, which\navoids the impact of the absolute value scale change of CTn,t during learning. We also consider\nSIM = 1\nT\nPT\nt=1 SIMt, which summarizes the similarities of CTn,t throughout the learning process.\n7\nCompression Ratio \nof Optimal Learning\nConventional \nLearning\n(a) Perceptron Linear Classification\nCompression Ratio \nof Optimal Learning\nConventional \nLearning\n(b) Transformer Language Modeling\nFigure 7: Empirical evidence of the Learning Law (Theorem 3.1) in Perceptron linear classi-\nfication (a) and Transformer language modeling (b) tasks.\nFollowing Figure 6, we consider\nSIM = 1\nT\nPT\nt=1 SIMt, which summarizes the similarity of the training example contributions in a\nlearning process. We plot the relationship between SIM and CR, and observe an evident tendency\nthat SIM ‚Üí +‚àû when CR approaches a certain value, which can be fit by SIM = log\n\u0010\na\nb‚àíCR\n\u0011c\n.\nWhen the learning process approaches the optimum (CR ‚Üí b), the standard deviations of training\nexample contributions should be zero to allow SIM ‚Üí +‚àû. This verifies Learning Law (Theorem\n3.1) that all training examples have the same contribution to the model in optimal learning.\nHigher compression ratio correlates with higher sample contribution similarities.\nIn Figure\n6, we examine the value of SIMt in the learning process induced by each policy found along the\noptimization process of Œ≥t. Since the found policies bring about faster convergence, we plot SIMt\nwith respect to Ldsr(Œ∏t), rather than t. In this way, SIMt are compared at the same ‚Äústage‚Äù of the\nmodel learning, migrating the impact of different convergence speeds. Figure 6 demonstrates that\nthe learning process with a higher compression ratio (CR) generally keeps higher SIMt in model\nlearning, indicating that the contributions CTn,t of individual samples are more similar to each other\nthroughout the learning process, which aligns with our Learning Law (Theorem 3.1).\nSample contributions tend to be equal when the learning process approaches the optimum.\nIn Figure 7, we plot SIM with respect to CR for each learning process. We observe an evident\ntendency that SIM ‚Üí +‚àû when CR approaches a certain value. Accordingly, we use the function\nSIM = log\n\u0010\na\nb‚àíCR\n\u0011c\nto fit the tendency of the experimental observations. Figure 7 indicates that\nwhen the learning process continuously improves until the optimum (CR ‚Üí b), the standard deviation\nof CTn,t should be zero to allow SIM ‚Üí +‚àû. This verifies Learning Law (Theorem 3.1) that the\ncontributions of non-zero-weight training samples (CTn,t) are identical in optimal learning.\n4.5\nProperties of Zero-Weight Examples\nThe experiments in Section 4.4 mostly focus on the non-zero-weight examples. In this section, we\nprovide more empirical evidence for Learning Law (Theorem 3.1) by examining the properties of the\nexamples with Œ≥n,t = 0. We derive three properties of the optimal learning dynamics from Theorem\n3.1 and then verify them through experiments. The first property guarantees that examples with\nnon-positive contributions receive Œ≥n,t = 0, indicating that the ‚Äúnoisy‚Äù examples at each time step\nare excluded by the optimal learning policy:\nProperty 4.1. The training example xtrn\nn whose CTn,t ‚â§ 0 gets Œ≥n,t = 0 before the model converges.\nProof. Before convergence, dLdsr(Œ∏(t))\ndt\n< 0 holds, indicating CTn,t > 0 for xtrn\nn that satisfies\nŒ≥n,t > 0, according to Theorem 3.1. Therefore, CTn,t ‚â§ 0 ‚áí Œ≥n,t = 0.\n8\n4\n5\n6\n7\nCompresson Rate (CR )\n0\n20\n40\n60\n80\nFraction of \nn, t = 0 (%)\nNear-Optimal Learning\nConventional Learning\n(a) Perceptron Linear Classification\n3.0\n3.1\n3.2\n3.3\nCompresson Rate (CR )\n0\n20\n40\n60\n80\nFraction of \nn, t = 0 (%)\nNear-Optimal Learning\nConventional Learning\n(b) Transformer Langnauge Modeling\nFigure 8: Empirical evidence of Property 4.1: non-contributive and noisy examples are excluded in\noptimal learning. The y-axis is the fraction of zero-weight examples among those with CTn,t ‚â§ 0 at\nthe same time step. Each point represents a learning policy, which tends to assign the example weight\nŒ≥n,t = 0 to 100% of noisy and non-contributive data when it approaches the optimum.\n0.0\n0.5\n1.0\nn, t / max\nn\n{\nn, t }\n0.0\n0.5\n1.0\nCumulative Probility\n4\n5\n6\nCR\nFigure 9: Empirical evidence of Property 4.2: perfectly learned examples are ignored in optimal\nlearning. We plot the cumulative distribution function (CDF) of the example weights Œ≥n,t that satisfies\nl(xtrn\nn , Œ∏t) < 1 √ó 10‚àí6. Each line corresponds to a learning process. A large fraction of low-loss\nexamples (perfectly learned) in the near-optimal learning obtain small Œ≥n,t values (ignored), and this\ntendency becomes more evident when the learning approaches its optimum (CR increases).\nEmpirical Evidence. We calculate the fraction of zero-weight examples (Œ≥n,t = 0) among all\nexamples with non-positive contributions at t (CTn,t ‚â§ 0):\nP\nn,t 1[Œ≥n,t=0]1[CTn,t‚â§0]\nP\nn,t 1[CTn,t‚â§0]\nand plot this\nfraction with respect to the CR value of the corresponding learning process in Figure 8. We can see\nthat when the learning process approaches the optimum, the fraction tends to 100%, indicating that\nthe non-contributive examples are discarded.\nThe second property is derived only for Perceptron linear classification, which indicates that the\noptimal learning policy will ignore those perfectly learned training examples:\nProperty 4.2. For Perceptrons, the perfectly learned xtrn\nn , whose margin (2ytrn\nn\n‚àí 1)Œ∏t ¬∑ ztrn\nn\n‚Üí +‚àû\nat the time step t, gets Œ≥n,t = 0 in the optimal learning policy when the model is yet converged.\nProof. When (2ytrn\nn ‚àí 1)Œ∏t ¬∑ ztrn\nn ‚Üí +‚àû, we have otrn\nn ‚àí ytrn\nn ‚Üí 0, which means ‚àál(xtrn\nn , Œ∏t) =\n(otrn\nn ‚àí ytrn\nn )ztrn\nn ‚Üí 0 and CTn,t ‚Üí 0. Assuming Œ≥n,t Ã∏= 0, according to Theorem 3.1, we have\nCTn,t = ‚àí d\ndtLdsr(Œ∏(t)) in the optimal learning process, which means that\n\f\f d\ndtLdsr(Œ∏(t))\n\f\f should\nbe arbitrarily small. This does not hold when the model is not converged. Therefore, we have\nŒ≥n,t = 0.\nEmpirical Evidence.\nIn Figure 9, we plot the cumulative probability distribution function of\nŒ≥n,t\nmaxn{Œ≥n,t} for the well-learned Perceptron training examples xtrn\nn with near-zero per-instance training\nloss: l(xtrn\nn , Œ∏) < 1 √ó 10‚àí6. Figure 9 shows that for the near-optimal policy, more than 90% well-\nlearned examples have relatively low Œ≥n,t (< 0.2 maxn {Œ≥n,t}). This trend becomes more evident as\nthe learning policy approaches the optimum (CR increases), which verifies Property 4.2.\n9\n(a) Perceptron Linear Classification\n(b) Transformer Language Modeling\nFigure 10: Empirical evidence of Property 4.3: redundant training examples are discarded in optimal\nlearning. We randomly sample 2048 training examples satisfying CTn,t > 0 (contributive and\nunlearned examples) throughout the near-optimal learning process and show the dynamics of the\nexample weight Œ≥n,t (represented by the color in (a) and (b)). Since Perceptron converges quickly,\nwe only plot its Œ≥n,t dynamics for t ‚â§ 50. The near-optimal policies assign Œ≥n,t = 0 to redundant\nexamples in addition to the perfectly learned and non-contributive data points.\nThe third property suggests that the optimal learning policy will discard the ‚Äúredundant‚Äù training\nexamples. Although this property is derived from Perceptron linear classification, we empirically\nfind that it also applies to Transformer language modeling. We call a set {xn}N\nn=1 has ‚Äúredundant‚Äù\nexamples when the example inputs in the set are linearly correlated, i.e., there exist K scalars\n{Œ±n}N\nn=1, not all zero, such that PN\nn=1 Œ±nzn = 0.\nProperty 4.3. For Perceptrons, if the training set {xtrn\nn }N\nn=1 has redundant examples, with probability\n1, at least one example xtrn\ni\ngets Œ≥i,t = 0 at the time step t when the model is yet converged in the\noptimal learning process.\nProof. Given that {xtrn\nn }N\nn=1 has redundant examples, there exist scalars {Œ±n}N\nn=1, not all zero,\nsuch that PN\nn=1 Œ±nztrn\nn\n=\n0, which means PN\nn=1\nŒ±n\notrn\nn ‚àíytrn\nn CTn,t\n=\n0.\nAssuming ‚àÄ1\n‚â§\nn ‚â§ N, Œ≥n,t Ã∏= 0, according to Theorem 3.1, we have CTn,t = ‚àí d\ndtLdsr(Œ∏(t)), suggesting\n\u0010PN\nn=1\nŒ±n\notrn\nn ‚àíytrn\nn\n\u0011\nd\ndtLdsr(Œ∏(t)) = 0. For i.i.d. inputs {ztrn\nn }N\nn=1, with probability 1, PN\nn=1\nŒ±n\notrn\nn ‚àíytrn\nn Ã∏=\n0, which means d\ndtLdsr(Œ∏(t)) = 0. This does not hold when the model is yet converged. Therefore,\nwe have the property that ‚àÉ1 ‚â§ n0 ‚â§ N, such that Œ≥n0,t = 0.\nEmpirical Evidence. In Figure 10, we visualize the dynamics of the Œ≥n,t value satisfying CTn,t > 0\nthroughout the learning process of Perceptron and Transformer. For Perceptron, the model dimension\n(128) is lower than the number of training examples (4096), which means the training dataset is\nredundant. Figure 10(a) shows that, given the absence of the non-contributive examples, a large\nfraction of Œ≥t still receives relatively small values before the model converges, which is caused by the\nredundancy of the training set. In Figure 10(b), we observe a similar phenomenon for Transformer,\nalthough the dimension of Œ∏t is larger than the number of training instances. We suspect the reason is\nthat the intrinsic dimension of Transformer is usually much smaller than the dimension of Œ∏t [AGZ21],\nwhich leads to the redundancy of the training set.\n4.6\nEssence of Learning Acceleration\nWe investigate the essential improvement brought by the near-optimal learning policy in the per-\nspective of the scaling laws of LMs [KMH+20], which reveals a power law between the increase of\ntraining steps and the reduction of the test loss (Ldsr(Œ∏t)) after a warming-up stage t0:\nLdsr(Œ∏t) =\n\u0012B\nt\n\u0013Œ≤\n, t > t0,\n(8)\nwhere (B, Œ≤) are scaling law coefficients. In the following, we study the scaling properties of the\nlearning processes induced by the conventional and near-optimal learning policies.\n10\nFigure 11: Illustration of the scaling law [KMH+20]: Ldsr(Œ∏t) = (B/t)Œ≤ for conventional and\nnear-optimal LM learning in Transformer language modeling. We fit the loss curves by the scaling\nlaw to obtain the correlation coefficient r2 and show the loss curve (solid lines) together with the fit\ncurve (dashed lines) in a log-log plot. The scaling law fits well for both conventional and near-optimal\nLM learning. The near-optimal LM learning essentially improves the coefficients (B, Œ≤) in the\nscaling law by 96.6% and 21.2%, which shows great potential for speedup in training LLMs.\nT\nN\n| ‚àÜB\nB | (%)\n| ‚àÜŒ≤\nŒ≤ | (%)\nAR\n1K\n212\n88.5\n10.0\n2.16\n2K\n213\n94.9\n18.0\n2.31\n4K\n214\n93.7\n18.7\n2.41\n8K\n215\n94.8\n19.0\n2.48\nTable 2: The improvements of the scaling law coefficients brought by the near-optimal learning\npolicy for different total training steps (T) and data sizes (N) in Transformer language modeling.\nThe vocabulary size increases with the growth of N (see Appendix D for details). AR stands for the\nacceleration ratio as defined in Equation 9. The improvements hold for larger T and N.\nThe near-optimal learning policy improves the scaling law coefficients of LMs.\nIn Figure 11,\nwe fit the Transformer‚Äôs loss curves induced by the conventional and near-optimal learning policies\nwith Equation 8 by setting t0 = 4004. We observe that the near-optimal learning process still follows\nthe scaling law, with B and Œ≤ improved by 96.6% and 21.2% respectively. Additionally, Table 2\nshows that the improvement holds for the near-optimal policies found in the setting of larger T and\nN. We let N grow with T to ensure the sufficiency of training data [HBM+22]. The improvement\nof scaling law coefficients, especially Œ≤, provides significant potential in boosting the speed of LLM\nlearning by taking advantage of power law growth. For two learning policies Œ≥(1) and Œ≥(2) which\ninduce two loss curves Ldsr\nŒ≥(1)(Œ∏t) and Ldsr\nŒ≥(2)(Œ∏t) with two sets of scaling law coefficients (B1, Œ≤1)\nand (B2, Œ≤2), the acceleration ratio of Œ≥(2) over Œ≥(1) is:\nAR =\nT\narg min\nt\nn\nLdsr\nŒ≥(2)(Œ∏t) ‚â§ Ldsr\nŒ≥(1)(Œ∏T )\no = B\nŒ≤1\nŒ≤2\n1\nB2\nT 1‚àí Œ≤1\nŒ≤2 .\n(9)\nFor an LM pre-trained for 10M steps, we will obtain more than 9√ó acceleration at the end of the\ntraining if the scaling property of the LM is improved as in Figure 11 and Table 2. Based on\nthe recent experience in training LLMs [TLI+23, TMS+23], models are far from fully converged\nunder the current training budget, which means small models (like 7B) have the potential to reach\nthe performance of large models (like 65B), given enough training steps. However, according to\nChinchilla‚Äôs law [HBM+22], extending the training steps requires more computation than enlarging\nthe model to achieve a certain performance. Therefore, by optimizing the learning policy to improve\n4In practice, we convert Equation 8 to ln Ldsr(Œ∏t) = ‚àíŒ≤ ln t + Œ≤ ln B and perform linear regression.\n11\nlearning speed, the cost of training well-performed small models can be largely reduced, which is\nbeneficial both for open-source endeavors in the LM research community and for the efficiency of\nindustrial products. This indicates the promise and significance of designing practical learning policy\noptimization approaches, and our theory can be a valuable guide.\n5\nRelated Work\nImproving the Learning Speed of Language Model.\nThere is a broad range of works that propose\napproaches to accelerate LM learning speed such as modifying model architectures [XYH+20,\nZH20] or optimizers [YLR+20, LLH+24, ZHSJ20]. There are also works studying the pre-training\ndata programming to speed up LM convergence, such as data de-duplication [TSAM23, ATS+23],\ndomain mixture [XPD+23], intrinsic task discovery [GHLH22], and online data selection or re-\nordering [CRB+23, GAH23, APRW23], which can be viewed as special cases of optimizing learning\npolicy. Unlike these works, we investigate the principles of optimizing LM learning in this paper.\nLanguage Modeling and Lossless Compression.\nThe recent success of LLMs calls for new\ninterpretations beyond classical statistic learning theory for the fact that larger model sizes constantly\ncause better downstream generalization [NKB+19, WTB+22]. One of the interpretations is to view\nthe next-token-prediction training process of an LM as lossless data compression [Bel19, MCKX22,\nRae23]. In this perspective, larger LMs have higher compression ratios, corresponding to better\nmodeling of data generation regularities. It is worth noting that some recent works [VNK+23,\nDRD+24] explore using well-trained LMs as compressors and thus the model sizes should be\ncounted into the compressed data. Unlike these works, viewing LM training as compression does not\nrequire including the model parameters in the compressed data (see Appendix A.1 for a constructive\nproof) and thus is more compatible with the model size scaling law of LMs [KMH+20].\n6\nDiscussion and Conclusion\nSummary.\nIn this work, we establish a theory for the optimal learning of LMs. We propose\nan objective that maximizes the compression ratio in an LM-training-as-losses-compression view.\nThen we derive a theorem, named Learning Law, suggesting that all examples should be equally\ncontributive to the LM in the optimal learning process, which is then validated by experiments in\nlinear classification and real-world language modeling tasks. Finally, we empirically show that the\noptimal learning process essentially improves the scaling law coefficients of LMs, which sheds light\non future works that design practical learning acceleration approaches.\nLimitations.\nOne limitation of our work is that the experiments are conducted on relatively small\nscales. This is because our method to find the near-optimal learning policy corresponds to training a\nneural network with L √ó T layers, where L is the layers of the LM and T is the LM‚Äôs total training\nsteps (see Appendix C for details). This leads to a high computational overhead when L and T scale\nup. However, since the theoretical derivation is generally applicable, we believe that our theory\ncan be applied to LLMs. Another limitation is that our derivation assumes the LM is trained with\nfull-batch GD, rather than some more commonly used techniques like mini-batch Adam [KB15].\nSince these methods are essentially gradient-based, our theory can still offer insights to future LM\nlearning acceleration studies based on these techniques [PLKS20, ABL+22].\nFuture Work.\nWe believe that an important direction of future work is designing practical methods\nto find the optimal learning policies based on our theory for the large-scale training of LMs. Indeed,\nthere are non-negligible challenges in this direction. Since the learning law provides a necessary\ncondition for the learning policy‚Äôs optimality, more regularization conditions may be required to\nprevent sub-optimal solutions. In addition, the approach to finding the optimal learning policy should\nbe efficient enough without contributing much to the overall computation cost. Nevertheless, our\nwork demonstrates the promise and potential of this direction. According to recent works on LLMs\ntraining [TLI+23, TMS+23, JSM+23], the losses are still far from convergence, which means that\nsmall models have the potential to reach the similar performance as large models, but are hindered\nby the computation overhead brought by the large total training steps. The optimal learning policy\npotentially brings about a large acceleration of training with the help of the power-law growth in\nEquation 9, which makes it possible to explore the limits of LMs given (inevitably) constrained\ncomputation and train a well-performed small LM that replaces current LLMs in practice.\n12\nReferences\n[ABL+22] Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob\nAndreas, and Kelvin Guu. Towards tracing knowledge in language models back to the\ntraining data. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Findings\nof EMNLP, 2022.\n[ADF+23] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre\nPassos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, et al. Palm 2\ntechnical report. arXiv preprint arXiv:2305.10403, 2023.\n[AGZ21] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality\nexplains the effectiveness of language model fine-tuning. In Proceedings of ACL, 2021.\n[APRW23] Alon Albalak, Liangming Pan, Colin Raffel, and William Yang Wang. Efficient online\ndata mixing for language model pre-training. In NeurIPS 2023 Workshop on R0-FoMo:\nRobustness of Few-shot and Zero-shot Learning in Large Foundation Models, 2023.\n[ATS+23] Amro Kamal Mohamed Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and\nAri S Morcos. SemDeDup: Data-efficient learning at web-scale through semantic\ndeduplication. In ICLR 2023 Workshop on Mathematical and Empirical Understanding\nof Foundation Models, 2023.\n[BC11] HH Bauschke and PL Combettes. Convex Analysis and Monotone Operator Theory in\nHilbert Spaces. Springer, 2011.\n[Bel19] Fabrice Bellard. NNCP: Lossless data compression with neural networks, 2019.\n[Ber16] Dimitri Bertsekas. Nonlinear programming, volume 4. Athena scientific, 2016.\n[BHA+21] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney\nvon Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al.\nOn the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258,\n2021.\n[BMR+20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, et al. Language models\nare few-shot learners. In Proceedings of NeurIPS, 2020.\n[CM03] Corinna Cortes and Mehryar Mohri. AUC optimization vs. error rate minimization. In\nProceedings of NeurIPS, 2003.\n[CMS+23] Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Prithvi\nPrabhu, Jeff Gambera, Mark Landry, Shivam Bansal, Ryan Chesler, et al. h2oGPT:\nDemocratizing large language models. arXiv preprint arXiv:2306.08161, 2023.\n[CND+23] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra,\nAdam Roberts, et al. PaLM: Scaling language modeling with pathways. JMLR, 2023.\n[CRB+23] Mayee F Chen, Nicholas Roberts, Kush Bhatia, Jue WANG, Ce Zhang, Frederic Sala,\nand Christopher Re. Skill-it! a data-driven skills framework for understanding and\ntraining language models. In Proceedings of NeurIPS, 2023.\n[DRD+24] Gr√©goire Del√©tang, Anian Ruoss, Paul-Ambroise Duquenne, Elliot Catt, Tim Genewein,\nChristopher Mattern, Jordi Grau-Moya, Li Kevin Wenliang, Matthew Aitchison, Laurent\nOrseau, et al. Language modeling is compression. In Proceddings of ICLR, 2024.\n[EC21] Omer Elkabetz and Nadav Cohen. Continuous vs. discrete optimization of deep neural\nnetworks. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors,\nProceedings of NeurIPS, 2021.\n[EL23] Ronen Eldan and Yuanzhi Li. TinyStories: How small can language models be and still\nspeak coherent english? arXiv preprint arXiv:2305.07759, 2023.\n[Eng01] Andreas Engel. Statistical mechanics of learning. Cambridge University Press, 2001.\n13\n[GAH23] David Grangier, Pierre Ablin, and Awni Hannun. Bilevel optimization to learn training\ndistributions for language modeling under domain shift. In NeurIPS 2023 Workshop on\nDistribution Shifts: New Frontiers with Foundation Models, 2023.\n[GHLH22] Yuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang. PPT: Pre-trained prompt tuning\nfor few-shor learning. In Proceedings of ACL, 2022.\n[GS+00] Izrail Moiseevitch Gelfand, Richard A Silverman, et al. Calculus of variations. Courier\nCorporation, 2000.\n[HBD+20] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of\nneural text degeneration. In Proceedings of ICLR, 2020.\n[HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor\nCai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl,\nAidan Clark, et al. Training compute-optimal large language models. arXiv preprint\narXiv:2203.15556, 2022.\n[HZD+21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past,\npresent and future. AI Open, 2021.\n[HZRS16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for\nimage recognition. In Proceedings of CVPR, 2016.\n[JSM+23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-\ndra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume\nLample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023.\n[KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In\nProceedings of ICLR, 2015.\n[KEC22] Ilia Kulikov, Maksim Eremeev, and Kyunghyun Cho. Characterizing and addressing the\nissue of over-smoothing in neural autoregressive sequence modeling. In Proceedings of\nAACL, 2022.\n[KL17] Pang Wei Koh and Percy Liang. Understanding black-box predictions via influence\nfunctions. In Proceedings of ICML, 2017.\n[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws\nfor neural language models. arXiv preprint arXiv:2001.08361, 2020.\n[KPA12] Celeste Kidd, Steven T Piantadosi, and Richard N Aslin. The goldilocks effect: Human\ninfants allocate attention to visual sequences that are neither too simple nor too complex.\nPloS one, 7(5):e36399, 2012.\n[LLH+24] Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable\nstochastic second-order optimizer for language model pre-training. In Proceedings of\nICLR, 2024.\n[LXLM23] Hong Liu, Sang Michael Xie, Zhiyuan Li, and Tengyu Ma. Same pre-training loss,\nbetter downstream: Implicit bias matters for language models. In Proceedings of ICML,\n2023.\n[MBR+22] S√∂ren Mindermann, Jan M Brauner, Muhammed T Razzak, Mrinank Sharma, Andreas\nKirsch, Winnie Xu, Benedikt H√∂ltgen, Aidan N Gomez, Adrien Morisot, Sebastian\nFarquhar, et al. Prioritized training on points that are learnable, worth learning, and not\nyet learnt. In Proceedings of ICML, 2022.\n[MCKX22] Yu Mao, Yufei Cui, Tei-Wei Kuo, and Chun Jason Xue. Trace: A fast transformer-based\ngeneral-purpose lossless compressor. In Proceedings of WWW, New York, NY, USA,\n2022.\n[Met09] Janet Metcalfe. Metacognitive judgments and control of study. Current directions in\npsychological science, 18(3):159‚Äì163, 2009.\n14\n[MKAT18] Sam McCandlish, Jared Kaplan, Dario Amodei, and OpenAI Dota Team. An empirical\nmodel of large-batch training. arXiv preprint arXiv:1812.06162, 2018.\n[MP43] Warren S McCulloch and Walter Pitts. A logical calculus of the ideas immanent in\nnervous activity. The bulletin of mathematical biophysics, 1943.\n[NKB+19] Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, and Ilya\nSutskever. Deep Double Descent: Where bigger models and more data hurt. In\nProceedings of ICLR, 2019.\n[Ope22] OpenAI. OpenAI: Introducing chatgpt, 2022.\n[Ope23] OpenAI. GPT-4 technical report, 2023.\n[PGM+19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory\nChanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch:\nAn imperative style, high-performance deep learning library. In Proceedings of NeurIPS,\n2019.\n[PLKS20] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating\ntraining data influence by tracing gradient descent. In NeurIPS, 2020.\n[Pon18] Lev Semenovich Pontryagin. Mathematical theory of optimal processes. Routledge,\n2018.\n[PR12] Warren B Powell and Ilya O Ryzhov. Optimal learning, volume 841. John Wiley &\nSons, 2012.\n[Rae23] Jack Rae. Compression for agi, 2023.\n[RM87] David E. Rumelhart and James L. McClelland. Learning internal representations by error\npropagation. In Parallel Distributed Processing: Explorations in the Microstructure of\nCognition: Foundations, 1987.\n[RRRH20] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. ZeRO: Memory\noptimizations toward training trillion parameter models. In Proceedings of SC20, 2020.\n[SDBD20] Samuel L Smith, Benoit Dherin, David Barrett, and Soham De. On the origin of implicit\nregularization in stochastic gradient descent. In Proceedings of ICLR, 2020.\n[TLI+23] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,\nTimoth√©e Lacroix, Baptiste Rozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien\nRodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\n[TMS+23] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine\nBabaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al.\nLlama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288,\n2023.\n[TSAM23] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari S Morcos. D4: Improving\nllm pretraining via document de-duplication and diversification. In Proceedings of\nNeurIPS, 2023.\n[Vap99] Vladimir Vapnik. The nature of statistical learning theory. Springer science & business\nmedia, 1999.\n[VNK+23] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-\nFrancois Chamberland, and Srinivas Shakkottai. LLMZip: Lossless text compression\nusing large language models. arXiv preprint arXiv:2306.04050, 2023.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, ≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceedings\nof NeurIPS, 2017.\n15\n[WKR+19] Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason\nWeston. Neural text generation with unlikelihood training. In Proceedings of ICLR,\n2019.\n[WSSC19] Robert C Wilson, Amitai Shenhav, Mark Straccia, and Jonathan D Cohen. The eighty\nfive percent rule for optimal learning. Nature communications, 10(1):4646, 2019.\n[WTB+22] Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud,\net al. Emergent abilities of large language models. TMLR, 2022.\n[WWL+23] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Zhongnan Qu, Shen Yan,\nYi Zhu, Quanlu Zhang, Mosharaf Chowdhury, et al. Efficient large language models: A\nsurvey. arXiv preprint arXiv:2312.03863, 2023.\n[XPD+23] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy\nLiang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. DoReMi: Optimizing data mixtures\nspeeds up language model pretraining. In Proceedings of NeurIPS, 2023.\n[XSML23] Sang Michael Xie, Shibani Santurkar, Tengyu Ma, and Percy Liang. Data selection for\nlanguage models via importance resampling. In Proceedings of NeurIPS, 2023.\n[XYH+20] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai\nZhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the\ntransformer architecture. In Proceedings of ICML, 2020.\n[YLR+20] Yang You, Jing Li, Sashank Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojana-\npalli, Xiaodan Song, James Demmel, Kurt Keutzer, and Cho-Jui Hsieh. Large batch\noptimization for deep learning: Training bert in 76 minutes. In Proceedings of ICLR,\n2020.\n[ZH20] Minjia Zhang and Yuxiong He. Accelerating training of transformer-based language\nmodels with progressive layer dropping. Proceedings of NeurIPS, 2020.\n[ZHSJ20] Jingzhao Zhang, Tianxing He, Suvrit Sra, and Ali Jadbabaie. Why gradient clipping\naccelerates training: A theoretical justification for adaptivity. In Proceedings of ICLR,\n2020.\n16\nA\nDiscussion of LM Training as Lossless Compression\nA.1\nOriginal View: Compressing the Training Data.\nThe idea of using an LM to compress data originates from the literature in the lossless text compression\nfield [Bel19, MCKX22], and is recently adopted to interpret the essence of the next-token-prediction-\nbased pre-training of LMs [Rae23]. We restate its core spirit by the following Theorem and the\nconstructive proof from [Rae23]:\nTheorem A.1. Consider an LM trained on a text corpus D with M tokens using mini-batch next-token-\nprediction for one epoch. Let B be the number of tokens in a batch and Lt be the batch-averaged\ntraining loss at the time step t. Assume that M is divisible by B. The training process can be viewed\nas lossless compression of the training data. The description length of the compressed data C is\nd(C) =\nM/B\nX\nt=1\nB ¬∑ Lt + d(LM),\n(10)\nwhere d(LM) is the length of the necessary code represented by a 0-1 string to run the LM training.\nProof. The basic idea of the proof is to construct a lossless encoding and decoding process for\nD with the LM. Let pŒ∏t(¬∑|w<m) be the output distribution of the LM parameterized by Œ∏t at the\ntime step t, conditioning on the token prefix w<m = [wm‚àí1, wm‚àí2, ¬∑ ¬∑ ¬∑ , w1]. For simplicity,\nwe assume that the LM is trained using mini-batch Stochastic Gradient Decent (SGD) with a\nlearning rate Œ∑, where each batch is linearized to a continuous list of tokens. The batch-averaged\ntraining loss is Lt = ‚àí 1\nB\nPB\nm=1 log pŒ∏t(wm|w<m)5. The encoding the decoding process are\ndescribed in Algorithm 1 and 2. Basically, the main body of the algorithms other than the blue-\ncolored parts implements the LM training. For encoding, the description length of a token wm\nis ‚àí log pŒ∏t(wm|w<m) according to Arithmetic Coding 6 , and thus the compressed length of a\nbatch W = {wm}B\nm=1 is PB\nm=1 [‚àí log pŒ∏t(wm|w<m)] = B ¬∑ Lt. d(C) equals the sum of per-batch\ndescription lengths throughout the training plus the length of the code for LM training. Therefore, we\nget d(C) = B ¬∑ PM/B\nt=1 ¬∑Lt + d(LM). For decoding, since the code for LM training is the same as\nthat in encoding, we have Œ∏‚Ä≤\n1 = Œ∏1, and thus w‚Ä≤\nm = wm for any steps in Algorithm 2, which can be\neasily proved by mathematical induction. As a result, D can be completely reconstructed from C,\nindicating the encoding (compression) is lossless.\nRemark 1. The description length of the compressed data d(C) is approximately proportional to the\narea under the training loss curve (AUC) when M ‚â´ 1 because the size of LM training codes is\nmuch smaller than that of the compressed corpus and thus d(C) ‚âà PM/B\nt=1 B ¬∑ Lt = B ¬∑ AUC.\nRemark 2. Let V be the vocabulary size of the LM and assume M ‚â´ 1. The corresponding\ncompression ratio of the learning process in Theorem A.1 is CR = M log V\nd(C)\n‚âà\nM log V\nPM/B\nt=1\nB¬∑Lt ‚àù\n1\nAUC.\nAs the LM fits the data, we generally have Lt < log V because log V is the loss for a randomly\ninitialized LM. This means the compression is valid, resulting in a compression ratio CR > 1.\nAltogether, Theorem A.1 bridges a connection between data compression and LM training. Generally,\na higher compression ratio indicates that the compression algorithm models the underlying data\nknowledge better and corresponds to a better performed LM, as stated in the following remark:\nRemark 3. The LM‚Äôs ability to model the knowledge in data is characterized by the corresponding\nlossless compression ratio of its learning process, which is inversely proportional to the loss AUC.\nNote that the model parameters are not included in the calculation of d(C), and enlarging the model\nsizes typically reduces the loss AUC, which explains the remarkable performance of LLMs. In\naddition, d(C) relates to the whole LM training process, not just the final loss. This is in line with\nthe fact that larger LMs tend to perform better than smaller models, even if their final losses are the\nsame [LXLM23]. This observation supports the perspective that LM training can be conceptualized\nas a process of lossless data compression.\n5log(¬∑) stands for log2(¬∑) in the following sections.\n6https://en.wikipedia.org/wiki/Arithmetic_coding\n17\nAlgorithm 1 Encoding\nInput: Training corpus D\nInput: The code for LM training as a 0-1 string\nOutput: Compressed data C: list of 0-1 strings\nInitialize C to an empty list\nAppend the LM training code to C\nInitialize the LM‚Äôs parameters to Œ∏1\nfor t ‚Üê 1 to M/B do\nGet a batch of tokens W = {wm}B\nm=1\nfrom the training corpus D\nfor m ‚Üê 1 to B, wm ‚àà W do\nEncode wm to a 0-1 string s with Arith-\nmetic Coding based on pŒ∏t(¬∑|w<m)\nAppend the 0-1 string s to C\nend for\nLt ‚Üê ‚àí 1\nB\nPB\nm=1 log pŒ∏t(wm|w<m)\nŒ∏t+1 ‚Üê Œ∏t ‚àí Œ∑‚àáLt\nend for\nAlgorithm 2 Decoding\nInput: Compressed data C: list of 0-1 string\nOutput: Training corpus D\nGet the LM training code from the first string in C\nPop the first string from C\nInitialize D to an empty list\nInitialize the LM‚Äôs parameters to Œ∏‚Ä≤\n1\nfor t ‚Üê 1 to M/B do\nGet a batch of 0-1 strings S = {sm}B\nm=1 from\nthe compressed data C\nfor k ‚Üê 1 to B, sm ‚àà S do\nDecode w‚Ä≤\nm from sm with Arithmetic Coding\nbased on pŒ∏‚Ä≤\nt(¬∑|w‚Ä≤\n<m)\nAppend the token w‚Ä≤\nm to D\nend for\nLt ‚Üê ‚àí 1\nB\nPB\nm=1 log pŒ∏‚Ä≤\nt(w‚Ä≤\nm|w‚Ä≤\n<m)\nŒ∏‚Ä≤\nt+1 ‚Üê Œ∏‚Ä≤\nt ‚àí Œ∑‚àáLt\nend for\nA.2\nOur View: Compressing Data from the Desired Distribution.\nAlthough we also focus on the loss AUC throughout the paper, our setting differs from that in\nAppendix A.1: (1) we assume the LM is trained with full-batch Gradient Descent (GD) for multiple\nepochs while Theorem A.1 lies in the scenario where the LM is trained with SGD for only one epoch;\n(2) we consider Ldsr computed on data other than the training examples, while pŒ∏t in Equation 10 is\ncomputed on the training data. However, although not entirely rigorous, we argue that Remark 3 still\nholds despite the differences in (1) and (2). The reason is that: regarding (1), mini-batch SGD is an\napproximation of GD, which means they share the similar training dynamics when the batch size of\nSGD is large enough; regarding (2), just like the training loss AUC, the AUC of Ldsr can be viewed as\nthe description length of compressing examples from the desired data distribution during the learning\nprocess. In this way, Remark 3 indicates that minimizing the AUC of Ldsr corresponds to optimizing\nthe data compression on the desired distribution, which improves the LM‚Äôs ability to model the\ndesired data knowledge. This is more of practical concern because in most scenarios, the model\nperformance is measured on a dataset other than the training set, such as the validation set in classical\nmachine learning [Vap99], the high-quality held-out corpus in large-scaling pre-training [KMH+20],\nor the target set in domain adaption [XSML23].\nA.3\nPerceptron Training as Lossless Compression\nViewing model training as lossless compression stems from the next-token-prediction learning\nparadigm of LMs. We show that this perspective also fits in the one-epoch Maximum Likelihood\nEstimation (MLE) training of Perceptrons on the linear classification task, where the label of each\nexample is compressed given the input vectors. Specifically, the proof in Appendix A.1 still applies\nif we treat linear classification as a one-step language modeling with vocabulary size V = 2.\nFollowing the notation in Section 4.2, for a Perceptron parameterized by Œ∏t at the time step t, its\nprobability of outputting y conditioning on z is pŒ∏t(y|z) = oy(1 ‚àí o)1‚àíy, where o = œÉ(Œ∏t ¬∑ z).\nFor a batch Bt = {(zn, yn)}B\nn=1, the batch-averaged loss is Lt = ‚àí 1\nB\nPB\nn=1 log pŒ∏t(yn|zn). With\nAlgorithm 1 and 2 applied for encoding and decoding, the description length of the compressed\nBt is PB\nn=1 [‚àí log pŒ∏t(yn|zn)] = B ¬∑ Lt, which means Theorem A.1 still holds and the discussion\nin Appendix A.2 also applies. For a dataset with N examples in total, the compression ratio\nCR ‚âà\nN log V\nPN/B\nt=1 B¬∑Lt =\nN\nPN/B\nt=1 B¬∑Lt . For a randomly initialized Œ∏1, L1 ‚âà 1, and as the model trains,\nLt ‚Üí 0, indicating a valid data compressing process of compression ratio CR > 1.\n18\nB\nProof of Theorem 3.1\nTheorem 3.1 essentially reflects the property of the dynamics in the learning process induced by the\noptimal learning policy Œ≥(t) for the problem defined in Equation 4. We choose the accumulation\nof each Œ≥n(t) over time: Œìn(t) =\nR t\n0 Œ≥n(t‚Ä≤)dt‚Ä≤, as a set of free variables that Œ∏(t) depends on to\nsolve the optimization problem. In this way, the problem is simplified by considering a scalar that\nsummarizes ‚Äúhow much‚Äù an example is used for training until t, rather than the whole trajectory of\nŒ≥n(t). As such, ÀôŒìn(t) = d\ndtŒìn(t) = Œ≥n(t) and Equation 4 becomes:\nmin\nŒì(t),Œ≥(t)\nZ T\n0\nLdsr(Œ∏Œì,Œ≥(t))dt,\ns.t.\nN\nX\nn=1\nÀôŒìn(t) = 1,\nÀôŒìn(t) ‚â• 0, n = 1, 2, ¬∑ ¬∑ ¬∑ , N,\n(11)\nwhere Œì(t) = [Œì1(t), Œì2(t), ¬∑ ¬∑ ¬∑ , ŒìN(t)]‚ä§, and Œ∏Œì,Œ≥(t) is an alias of Œ∏(t) to show its dependency on\nŒì and Œ≥. Let L be the Lagrangian depending on {Œìn(t)}N\nn=1 and { ÀôŒìn(t)}N\nn=1:\nL = Ldsr(t) + Œª(t)(\nN\nX\nn=1\nÀôŒìn(t) ‚àí 1) +\nN\nX\nn=1\n¬µn(t) ÀôŒìn(t),\n(12)\nwhere Œª(t) and ¬µn(t) are Lagrange multipliers and Ldsr(t) = Ldsr(Œ∏Œì,Œ≥(t)) = Ldsr(Œ∏(t)). To achieve\nthe optimum of Equation 11, L should satisfy the Euler-Lagrange (EL) Equation [GS+00]:\nd\ndt\n‚àÇL\n‚àÇ ÀôŒìn\n‚àí ‚àÇL\n‚àÇŒìn\n= 0.\n(13)\nTogether with other constraints in the Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions [Ber16], we get the\nfollowing formulas that characterize the optimum of the Equation 11:\nÔ£±\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£≤\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£¥\nÔ£≥\n‚àÇLdsr(t)\n‚àÇŒìn\n= ÀôŒª(t) + Àô¬µn(t),\nN\nX\nn=1\nÀôŒìn(t) = 1,\nÀôŒìn(t) ‚â• 0,\n¬µn(t) ‚â• 0,\n¬µn(t) ÀôŒìn(t) = 0.\n(14)\nNote that we only consider the E-L Equations on {Œìn(t)}N\nn=1 and { ÀôŒìn(t)}N\nn=1, part of the free\nvariables in the original problem, which also depends on the Œ≥(t) trajectory. Since KKT conditions\nare necessary conditions to the optimization problem, Equation 14 is also necessary. In the following,\nwe simplify Equation 14 to reach Theorem 3.1.\nSimplifying Equation 14.\nWe study the examples with non-zero weights during training. For\nŒ≥n(t) = ÀôŒìn(t) > 0 we have ¬µn(t) = 0 according to ¬µn(t) ÀôŒìn(t) = 0 in Equation 14 and the\nfollowing Lemma bridges a connection between ¬µn(t) and Àô¬µn(t):\nLemma B.1. For ¬µn(t) ‚àà C1[0, T], ¬µn(t) = 0 ‚áí Àô¬µn(t) = 0 at the specific time step t.\nProof. Assuming ‚àÉt0 ‚àà [0, T], s.t. ¬µn(t0) = 0 but Àô¬µn(t0) Ã∏= 0, we let Àô¬µn(t0) > 0 without loss of\ngenerality. Then ‚àÉŒ¥t > 0, s.t. ¬µn(t0 ‚àí Œ¥t) < 0 according to ¬µn(t) ‚àà C1[0, T], which contradicts\n¬µn(t) ‚â• 0 in Equation 14. Therefore, we have Àô¬µn(t0) = 0.\nAs such, Œ≥n(t) > 0 ‚áí ¬µn(t) = 0 ‚áí Àô¬µn(t) = 0 (Lemma B.1), which means:\n‚àÇLdsr(t)\n‚àÇŒìm\n= ‚àÇLdsr(t)\n‚àÇŒìn\n= ÀôŒª(t), for Œ≥m(t) > 0 and Œ≥n(t) > 0.\n(15)\nNote that ÀôŒª(t) is independent of m and n. Equation 15 already resembles Equation 5 in their formats,\nif we have ‚àÇLdsr(t)\n‚àÇŒìn\n‚àù ‚àáL ¬∑ ‚àáln, where ‚àáL = ‚àáLdsr(Œ∏(t)) and ‚àáln = ‚àál(xtrn\nn , Œ∏(t)).\n19\nInterpreting ‚àÇLdsr(t)\n‚àÇŒìn\n.\n‚àÇLdsr(t)\n‚àÇŒìn\nmeasures how the change of Œìn(t) influence the change of Ldsr(t)\nat the time step t when other free variables are fixed. Specifically, if Œìn(t) changes by a small\nvalue Œìn(t) ‚Üí Œìn(t) + ‚àÜŒìn(t), then Ldsr(t) correspondingly changes by a small value Ldsr(t) ‚Üí\nLdsr(t) + ‚àÜLdsr(t), and ‚àÇLdsr(t)\n‚àÇŒìn\n= ‚àÜLdsr(t)\n‚àÜŒìn(t) . Then, we consider dLdsr(t)\ndt\nwith Equation 3:\ndLdsr(t)\ndt\n= ‚àáLdsr(Œ∏(t)) ¬∑ dŒ∏(t)\ndt\n= ‚àí\nN\nX\nn=1\nŒ≥n(t)‚àáLdsr(Œ∏(t)) ¬∑ ‚àál(xtrn\nn , Œ∏(t))\n= ‚àí\nN\nX\nn=1\ndŒìn(t)\ndt\n‚àáL ¬∑ ‚àáln.\n(16)\nAs a result, for a small ‚àÜt, we have:\nLdsr(t + ‚àÜt) ‚àí Ldsr(t) = ‚àí\nN\nX\nn=1\n[Œìn(t + ‚àÜt) ‚àí Œìn(t)] ‚àáL ¬∑ ‚àáln.\n(17)\nNow we consider the change of Ldsr(t) and Œìn at t + ‚àÜt. Since ‚àáL ¬∑ ‚àáln is computed at the time\nstep t, it is not affected by the variants. Therefore, we get:\n‚àÜLdsr(t + ‚àÜt) = ‚àí‚àÜŒìn(t + ‚àÜt)‚àáL ¬∑ ‚àáln,\n(18)\nWhen ‚àÜt ‚Üí 0, ‚àÜLdsr(t+‚àÜt)\n‚àÜŒìn(t+‚àÜt) ‚Üí ‚àÜLdsr(t)\n‚àÜŒìn(t) = ‚àÇLdsr\n‚àÇŒìn , which means:\n‚àÇLdsr(t)\n‚àÇŒìn\n= ‚àí‚àáL ¬∑ ‚àáln.\n(19)\nBy substituting Equation 19 into Equation 15, we obtain that for the mth and nth training examples\nsatisfying Œ≥m(t) > 0 and Œ≥n(t) > 0 the following equation holds:\n‚àáL ¬∑ ‚àálm = ‚àáL ¬∑ ‚àáln = ‚àí ÀôŒª(t) = Const,\n(20)\nwhere Const stands for ‚Äúa constant independent of m and n‚Äù. Equation 20 is essentially equivalent\nto Equation 5.\nProving Const = ‚àí dLdsr(t)\ndt\n.\nBy substituting ‚àáL ¬∑ ‚àáln with Const in Equation 16, we get:\ndLdsr(t)\ndt\n= ‚àí\nN\nX\nn=1\ndŒìn(t)\ndt\n¬∑ Const,\n= ‚àíConst\nN\nX\nn=1\nŒ≥n(t),\n= ‚àíConst.\n(21)\nAs such, by combining Equation 20 with Equation 21, we complete the proof of Theorem 3.1.\nC\nDetails of Learning Policy Optimization\nIn Section 4.1, we search for the optimal learning policy by Proximal Gradient Decent [BC11].\nSpecifically, we view the whole learning process in 0 ‚â§ t ‚â§ T as a neural network with T layers\nparameterized by Œ≥ = [Œ≥0, ¬∑ ¬∑ ¬∑ , Œ≥t‚àí1] ‚àà RN√óT . As illustrated in Figure 12, each layer of the network\nconsists of the gradient update function and a residual connection [HZRS16], where the ‚Äúhidden\nstates‚Äù are Œ∏t. Then, we adopt Backward Propagation (BP; RM87) to compute ‚àáŒ≥tJ(Œ≥) in Equation\n6. The backward operation at each layer is:\n‚àÇJ\n‚àÇŒ≥t\n= ‚àíŒ∑\nT\nX\nt‚Ä≤=t+1\n‚àáLdsr(Œ∏t‚Ä≤) ‚àÇŒ∏t‚Ä≤\n‚àÇŒ∏t+1\nGtrn(Œ∏t)\n‚àÇŒ∏t‚Ä≤\n‚àÇŒ∏t+1\n= ‚àÇŒ∏t‚Ä≤\n‚àÇŒ∏t+2\n\u0002\nI ‚àí Œ∑Htrn(Œ∏t+1)\n\u0003\n,\n(22)\n20\n¬∑¬∑¬∑\n¬∑¬∑¬∑\n+\nFigure 12: The architecture of the equivalent neural network to find the optimal learning policy, Each\nlayer consists of the gradient update and a residual connection.\nwhere Gtrn(Œ∏t) = [‚àál(xtrn\n1 , Œ∏t), ¬∑ ¬∑ ¬∑ , ‚àál(xtrn\nN , Œ∏t)] , I is the identity matrix, and Htrn(Œ∏t+1) is the\nHessain matrix of Ltrn(Œ∏) at Œ∏ = Œ∏t+1. We implement the BP operations with dynamic programming\nand Jacobian-Vector-Product7 in PyTorch [PGM+19] for efficiency. To reduce the single-device GPU\nmemory use, we also implement an activation partition algorithm inspired by ZeRO-2 [RRRH20],\nwhere the ‚Äúhidden states‚Äù Œ∏t in one model are stored in different GPU devices.\nD\nHyper-Parameter Configurations\nPerceptron Linear Classification.\nFollowing the teacher-setting described in Section 4.2, we use\nD = 128 and T is randomly drawn from a Gaussian distribution T ‚àº N(0,\n‚àö\nDI). We generate\nN = 4096 training inputs ztrn from N(0, 3I), and M = 512 target inputs zdsr from N(0.51, I),\nwhere 1 = [1, 1, ¬∑ ¬∑ ¬∑ , 1]‚ä§ ‚àà RD. For each learning policy, we initialize Œ≥n,0 =\n1\nN and train the\nPerceptron with Œ∑ = 0.1 for T = 2000 time steps, which is sufficient for the model to converge. For\nlearning policy optimization, we initialize the learning policy to the constant policy Œ≥c\nn,t = 1\nN , setting\nœµ = 5 √ó 10‚àí6 and train the network for 500 epochs.\nTransformer Language Modeling.\nWe conduct experiments based on a two-layer Transformer\nwith 128 hidden dimensions and 8 attention heads. For all experiments except that in Table 2, we\nrandomly sample N = 16,384 examples as xtrn\nn and K = 512 examples as xdsr\nk with the max sequence\nlength 64 from the TinyStories [EL23] corpus8. We use the BPE tokenizer of Mistral [JSM+23]\nand construct a vocabulary with 5K tokens by mapping the infrequent tokens to [UNK]. The model\ncontains about 1.7M parameters. To reflect the difference between the training and desired distribution,\nwe add perturbations to 50% training sentences by iteratively applying one of the following operations\n20 times: (1) replacing one token with a random token in the vocabulary [HBD+20]; (2) deleting\nthe last token [KEC22]; (3) repeating one token in a sentence [WKR+19]. This corresponds to the\nfact that the large-scale pre-training corpus tends to be more noisy than the desired set (the carefully\ncurated held-out corpus or high-quality downstream data) to evaluate the model generalization\nperformance in practice. We set Œ∑ = 0.1, T = 4, 000, Œ≥n,0 =\n1\nN for each learning policy. We start\nfrom the constant policy and optimize the learning policy for 15 epochs using Œ∑ = 0.1, 0.2, 0.4.\nŒ∑ = 0.4 yields the lowest loss at the end of the training. Therefore, we only plot the optimization\nprocess for Œ∑ = 0.4 in Figure 4(b) and 5(b). For experiments in Table 2, we vary the total training\nsteps and the corresponding training data sizes and simultaneously, change the vocabulary sizes to\nadapt to different data sizes. We use vocabulary sizes 4K, 4.5K, 5K, and 6K for N = 212, 213, 214,\nand 215, respectively.\n7https://pytorch.org/docs/stable/func.api.html\n8https://huggingface.co/datasets/roneneldan/TinyStories/tree/main\n21\n"
}