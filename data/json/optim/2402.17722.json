{
    "optim": "Taming Nonconvex Stochastic Mirror Descent with General Bregman Divergence Ilyas Fatkhullin Niao He ETH Z¨urich ETH Z¨urich Abstract This paper revisits the convergence of Stochas- tic Mirror Descent (SMD) in the contemporary nonconvex optimization setting. Existing re- sults for batch-free nonconvex SMD restrict the choice of the distance generating func- tion (DGF) to be differentiable with Lipschitz continuous gradients, thereby excluding im- portant setups such as Shannon entropy. In this work, we present a new convergence anal- ysis of nonconvex SMD supporting general DGF, that overcomes the above limitations and relies solely on the standard assumptions. Moreover, our convergence is established with respect to the Bregman Forward-Backward envelope, which is a stronger measure than the commonly used squared norm of gradient mapping. We further extend our results to guarantee high probability convergence under sub-Gaussian noise and global convergence un- der the generalized Bregman Proximal Polyak-  Lojasiewicz condition. Additionally, we illus- trate the advantages of our improved SMD theory in various nonconvex machine learning tasks by harnessing nonsmooth DGFs. No- tably, in the context of nonconvex differen- tially private (DP) learning, our theory yields a simple algorithm with a (nearly) dimension- independent utility bound. For the problem of training linear neural networks, we develop provably convergent stochastic algorithms. 1 INTRODUCTION We consider stochastic composite optimization min x∈X E [f(x, ξ)] + r(x), (1) Proceedings of the 27th International Conference on Artifi- cial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume 238. Copyright 2024 by the au- thor(s). where F(x) := E [f(x, ξ)] is differentiable and (possi- bly) nonconvex, r(·) is convex, proper and lower-semi- continuous, X is a closed convex subset of Rd. The random variable (r.v.) ξ is distributed according to an unknown distribution P. We denote Φ := F + r and let Φ∗ := infx∈X Φ(x) > −∞. A popular algorithm for solving (1) is Stochastic Mirror Descent (SMD), which has an update rule xt+1 = arg min x∈X ηt(⟨∇f(xt, ξt), x⟩ + r(x)) + Dω(x, xt), (2) where Dω(x, y) is the Bregman divergence between points x, y ∈ X induced by a distance generating function (DGF) ω(·); see Section 2 for the definitions. When r(·) = 0, X = Rd and ω(x) = 1 2 ∥x∥2 2, we have Dω(x, y) = 1 2 ∥x − y∥2 2, and SMD reduces to the stan- dard Stochastic Gradient Descent (SGD). However, it is often useful to consider more general (non-Euclidean) DGFs. SMD with general DGF was originally proposed in the pioneering work of Nemirovskij and Yudin [1979, 1983], and later found many fruitful applications [Ben-Tal et al., 2001, Shalev-Shwartz, 2012, Arora et al., 2012] leveraging nonsmooth instances of DGFs. In the last few decades, SMD has been extensively analyzed in the convex setting under various assumption, e.g., [Beck and Teboulle, 2003, Lan, 2012, Allen-Zhu and Orecchia, 2014, Birnbaum et al., 2011], including relative smooth- ness [Lu et al., 2018, Bauschke et al., 2017, Dragomir et al., 2021, Hanzely et al., 2021] and stochastic op- timization [Lu, 2019, Nazin et al., 2019, Zhou et al., 2020b, Hanzely and Richt´arik, 2021, Vural et al., 2022, Liu et al., 2023, Nguyen et al., 2023]. However, despite the vast theoretical progress, convergence analysis of nonconvex SMD with general DGF still remains elusive. 1.1 Related Work We now discuss the related work in the nonconvex stochastic setting. In the unconstrained Euclidean case, Ghadimi and Lan [2013] propose the first non- asymptotic analysis of nonconvex SGD. Later, Ghadimi arXiv:2402.17722v1  [math.OC]  27 Feb 2024 Taming Nonconvex Stochastic Mirror Descent et al. [2016] consider the more general composite prob- lem (1) with arbitrary convex r(·), X, and propose a modified algorithm using large mini-batches. Unfor- tunately, the use of large mini-batch appears to be crucial in the proof proposed in [Ghadimi et al., 2016] even in Euclidean setting. Later, Davis and Drusvy- atskiy [2019] address this issue by proposing a different analysis for Prox-SGD (method (2) with ω(x) = 1 2 ∥x∥2 2). Their elegant proof, using the notion of the so-called Moreau envelope, allows them to remove the large batch requirement in the Euclidean setting. However, their analysis crucially relies on the use of Euclidean geome- try and appears difficult to extend to the more general nonsmooth DGFs of interest. In particular, the subse- quent works [Zhang and He, 2018, Davis et al., 2018] do consider more general DGF and derive convergence rates for (2). However, both works assume a smooth DGF to justify their proposed convergence measures, see our Section 4.2 for a more detailed comparison. Another line of work uses momentum or variance re- duced estimators, e.g., [Zhang, 2021, Huang et al., 2022, Fatkhullin et al., 2023c, Ding et al., 2023], but agian their analysis is limited to the Euclidean geometry. 1.2 Contributions • In this work, we develop a new convergence analy- sis for SMD under the general assumptions of rela- tive smoothness and bounded variance of stochas- tic gradients. Importantly, unlike the prior work, our analysis naturally accommodates general non- smooth DGFs, including the important case of Shannon entropy. Moreover, our analysis (i) works for any batch size, (ii) does not require the bounded gradients assumption, (iii) supports any closed convex set X, and (iv) guarantees con- vergence on a strong stationarity measure – the Bregman Forward-Backward envelope. • We further demonstrate the flexibility of our proof technique by extending it in two directions. First, we perform a high probability analysis under the sub-Gaussian noise improving upon the previously known rates under weaker assumptions. Next, we establish the global convergence in the function value for SMD under the generalized version of the Proximal Polyak- Lojasiewicz condition. In both cases, when specialized to the unconstrained Euclidean setup, our rates can recover the state- of-the-art bounds, up to small absolute constants. • Finally, we demonstrate the importance of our gen- eral theory in various machine learning contexts, including differential privacy, policy optimization in reinforcement learning, and training deep lin- ear neural networks. For each of the considered problems, our new SMD theory allows us to ei- ther improve convergence rates or design provably convergent stochastic algorithms. In all cases, we leverage nonsmooth DGFs to attain the result. Our Techniques. The key idea of our analysis is the use of a new Lyapunov function in the form of a weighted sum of the function value Φ(·) and its Breg- man Moreau envelope Φ1/ρ(·): λt := ηt−1ρ(Φ(xt) − Φ∗) + Φ1/ρ(xt) − Φ∗. We recall that the classic analysis of (large batch) SMD in [Ghadimi et al., 2016] uses the function value as a Lyapunov function, i.e., λt,1 := Φ(xt) − Φ∗. While this approach is very intuitive and matches with anal- ysis in unconstrained case, it seems very difficult to generalize to more general constrained problem (1) even in the Euclidean setting. On the other hand, the analysis pioneered in [Davis and Drusvyatskiy, 2019] uses λt,2 := Φ1/ρ(xt) − Φ∗ as a Lyapunov function, which does not seem straightforward to extend into non-Euclidean setups, unless the smoothness of DGF is additionally imposed. Our Lyapunov function con- tains a weighted average of the above two quantities, i.e., λt = ηt−1ρλt,1 + λt,2, where {ηt}t≥0 is the step- size sequence (with η−1 = η0), ρ > 0. This modified Lyapunov function allows to better utilize (relative) smoothness of F(·) in the analysis. Namely, both upper and lower bound inequalities in Assumption 3.1 will be used in the proof. 2 PRELIMINARIES We fix an arbitrary norm ∥·∥ defined on X ⊂ Rd, and denote by ∥·∥∗ := supz:∥z∥≤1⟨·, z⟩ its dual. The Euclidean norm is denoted by ∥·∥2. We denote by δX the indicator function of a convex set X, i.e., δX (x) = 0 if x ∈ X and +∞ otherwise. For a closed proper function Φ : Rd → R ∪ {+∞} with dom Φ := \b x ∈ Rd | Φ(x) < +∞ \t , the Fr´echet subd- ifferential at a point x ∈ Rd is denoted by ∂Φ(x) and is defined as a set of points g ∈ Rd such that Φ(y) ≥ Φ(x) + ⟨g, y − x⟩ + o(∥y − x∥), ∀y ∈ Rd if x ∈ dom Φ. We set ∂Φ(x) = ∅ if x /∈ dom Φ [Davis and Grimmer, 2019].1 We denote by cl(X) and ri(X) the closure and the relative interior of X respectively. Let S ⊂ Rd be an open set and ω : cl(S) → R be continuously differentiable on S. Then we say that ω(·) is a distance generating function (DGF) (with zone S) if it is 1-strongly convex w.r.t. ∥·∥ on cl(S). We assume throughout that S is chosen such that ri(X) ⊂ S [Chen 1When Φ = δX , the function is convex and ∂δX (x) coin- cides with the usual subdifferential in the convex analysis. Ilyas Fatkhullin, Niao He and Teboulle, 1993].2 For simplicity, we let dom r = Rd. The Bregman divergence [Bregman, 1967] induced by ω(·) is Dω(x, y) := ω(x)−ω(y)−⟨∇ω(y), x−y⟩ for x, y ∈ S. We denote by Dsym ω (x, y) := Dω(x, y) + Dω(y, x) a symmetrized Bregman divergence. For any Φ : Rd → R ∪ {+∞} and a real ρ > 0, the Bregman Moreau envelope and the proximal operator are defined respectively by Φ1/ρ(x) := min y∈X [Φ(y) + ρDω(y, x)] , proxΦ/ρ(x) := arg min y∈X [Φ(y) + ρDω(y, x)] . A point x ∈ X ∩ S is called a first-order stationary point (FOSP) of (1) if 0 ∈ ∂(Φ+δX )(x) for Φ := F +r. 2.1 FOSP Measures We define three different measures of first-order sta- tionarity for a candidate solution x ∈ X ∩ S. (i) Bregman Proximal Mapping (BPM) ∆ρ(x) := ρ2Dsym ω (ˆx, x), ˆx := proxΦ/ρ(x). (ii) Bregman Gradient Mapping (BGM) ∆+ ρ (x) := ρ2Dsym ω (x+, x), x+ := arg min y∈X ⟨∇F(x), y⟩ + r(y) + ρDω(y, x). (iii) Bregman Forward-Backward Envelope (BFBE) Dρ(x) := −2ρ min y∈X Qρ(x, y), Qρ(x, y) := ⟨∇F(x), y − x⟩ + ρDω(y, x) + r(y) − r(x). In unconstrained Euclidean case, i.e., X = Rd, r(·) = 0 and ω(x) = 1 2 ∥x∥2 2, we have ∆+ ρ (x) = Dρ(x) = ∥∇F(x)∥2 2, which is the standard stationarity measure in non-convex optimization.3 Note that all three quan- tities presented above are measures of FOSP in the sense that if one of them ∆ρ(x) , ∆+ ρ (x) or Dρ(x) is 2Following Chen and Teboulle [1993], we can verify that, in the Euclidean setup (ω(x) = 1 2 ∥x∥2 2), one can set S = Rd; in the simplex setup (ω(x) = Pd i=1 x(i) log x(i)), the choice S = n x ∈ Rd | x(i) > 0 for all i ∈ [d] o is suitable. 3This is, however, not true for BPM, which reduces to the gradient norm of a surrogate loss, i.e., ∆ρ(x) = \r\r∇F1/ρ(x) \r\r2 2 for ρ > ℓ, where ℓ is a smoothness constant of F(·). zero for some x ∈ X ∩ S, then 0 ∈ ∂(Φ + δX )(x). How- ever, it is more practical to understand what happens if one of them is only ε-close to zero. In Section 4.1 we establish the connections between these quantities for any x ∈ X ∩ S, and find that BFBE, Dρ(x), is the strongest among the three. We should mention that the use of BFBE is not new for the analysis of opti- mization methods. In the Euclidean case, BFBE was initially proposed in [Patrinos and Bemporad, 2013], and its properties were later analyzed in [Stella et al., 2017, Liu and Pong, 2017]. Later, Ahookhosh et al. [2021] consider BFBE in general non-Euclidean setting. However, to our knowledge it was not considered in the context of stochastic even in the Euclidean setup. 3 ASSUMPTIONS Throughout the paper we make the following basic assumptions on F(·) and the stochastic gradients. Assumption 3.1 (Relative smoothness [Bauschke et al., 2017, Lu et al., 2018]). A differentiable function F : X ∩ S → R is said to be ℓ-relatively smooth on X ∩S with respect to (w.r.t.) ω(·) if for all x, y ∈ X ∩S −ℓDω(x, y) ≤ F(x)−F(y)−⟨∇F(y), x−y⟩ ≤ ℓDω(x, y). We denote such class of functions as (ℓ, ω)-smooth. It is known that smoothness w.r.t. ∥·∥, i.e., ∥∇F(x) − ∇F(y)∥∗ ≤ ℓ ∥x − y∥ for all x, y ∈ X ∩ S, implies Assumption 3.1 [Nesterov, 2018]. Assumption 3.2. We have access to a stochastic or- acle that outputs a random vector ∇f(x, ξ) for any given x ∈ X, such that E [∇f(x, ξ)] = ∇F(x), E h ∥∇f(x, ξ) − ∇F(x)∥2 ∗ i ≤ σ2, where the expectation is taken w.r.t. ξ ∼ P. 4 MAIN RESULTS 4.1 Connections between FOSP Measures We start by establishing the connections between intro- duced convergence measures. It turns out that BPM and BGM are essentially equivalent, i.e., differ only by a small (absolute) multiplicative constant. Lemma 4.1 (BPM ≈ BGM). Let F(·) be (ℓ, ω)-smooth and p Dsym ω (x, y) be a metric. Then for any x ∈ X ∩S, and ρ, s > 0 such that ρ > ℓ/s + 2ℓ, it holds ∆ρ(x) C(ℓ, ρ, s) ≤ ∆+ ρ (x) ≤ C(ℓ, ρ, s)∆ρ(x), Taming Nonconvex Stochastic Mirror Descent where C(ℓ, ρ, s) := (1+s)(ρ−ℓ)+(1+s−1)ℓ ρ−ℓ−(1+s−1)ℓ . In particular, for s = 1, ρ = 4ℓ, we have C(ℓ, ρ, s) = 8, and 1 8∆4ℓ(x) ≤ ∆+ 4ℓ(x) ≤ 8∆4ℓ(x). This result is in a similar spirit to Theorem 4.5 in [Drusvyatskiy and Paquette, 2019]. However, their proof only works in the Euclidean setting and does not readily extend to other DGFs. Our proof is dif- ferent and can accommodate a possibly nonsmooth DGF.4 Next, we examine the relation between BGM and BFBE. Lemma 4.2 (BFBE > BGM). For any x ∈ X ∩ S 2Dρ/2(x) ≥ ∆+ ρ (x). There is an instance of problem (1) with ℓ = 1, X = [0, 1] and arg miny∈X Φ(x) = 0 such that for any ρ ∈ [1, 2], ρ1 ≥ 1 and x ∈ (0, 1] it holds Dρ(x) ∆+ ρ1(x) ≥ 2 |x|. The above lemma implies that BFBE is a strictly stronger convergence measure than previously consid- ered BGM and BPM. Moreover, the difference between BFBE and BGM can be arbitrarily large even when x is close to the optimum! This effect is actually very common and happens already in the Euclidean case with classical regularizer r(x) = ∥x∥1. The explanation for this phenomenon is simple. Notice that BGM is defined in the primal terms, i.e., the squared distance between x and x+, while BFBE is defined in the func- tional terms (the minimum value of Qρ(x, y) over y). Therefore, BFBE unlike BGM scales with the value of r(x) = |x| rather than x2. We conclude from Lemma 4.1 and 4.2 that Dρ(x) is the strongest convergence measure among the three. In the subsequent sections we aim to establish convergence of SMD directly w.r.t. BFBE instead of using BGM or BPM. 4.2 Convergence to FOSP in Expectation We start with our key result, which establishes conver- gence of SMD in expectation. 4Unfortunately, it is unclear if the above result holds for arbitrary ω(·) that does not induce a metric. Note that, in general, DGF might not induce a metric even for popular choices of ω(·). For instance, the Shannon entropy induces p Dsym ω (x, y) that does not satisfy the triangle inequality, see, e.g., Theorem 3 in [Acharyya et al., 2013] for details. Theorem 4.3. Let Assumptions 3.1 and 3.2 hold. Let the sequence {ηt}t≥0 be non-increasing with η0 ≤ 1/(2ℓ), and ¯xT be randomly chosen from the iterates x0, . . . , xT −1 with probabilities pt = ηt/ PT −1 t=0 ηt. Then E [D3ℓ(¯xT )] ≤ 3λ0 + 6ℓσ2 PT −1 t=0 η2 t PT −1 t=0 ηt , (3) where λ0 := Φ1/ρ(x0) − Φ∗ + Φ(x0) − Φ∗. If we set constant step-size ηt = min \u001a 1 2ℓ, q λ0 σ2ℓT \u001b , then E [D3ℓ(¯xT )] = O   ℓλ0 T + r σ2ℓλ0 T ! . Proof sketch: We start with Step I. Deterministic descent w.r.t. BFBE. We show that for any ρ1 ≥ ρ + ℓ Φ1/ρ(x) ≤ Φ (x) − 1 2ρ1 Dρ1(x). This inequality corresponds to deterministic descent on Φ(·) of the Bregman Proximal Point Method. It will be useful in the next step to derive a recursion on Dρ1(x). Step II. One step progress on the Lyapunov function. This step is the most technical one and consists of showing a progress on a carefully chosen Lyapunov function λt := Φ1/ρ(xt)−Φ∗+ηt−1ρ(Φ(xt)− Φ∗), where η−1 = η0, ρ > 0: λt+1 ≤ λt − ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)), where ψt := ∇f(xt, ξt) − ∇F(xt). Step III. Dealing with stochastic terms. The goal of this step is to control the stochastic terms in the above inequality using Assumption 3.2. It remains to telescope and set the step-sizes to derive the final result. When specialized to the unconstrained Euclidean set- ting, the result of Theorem 4.3 recovers (up to a small absolute constant) previously established convergence bounds for SGD [Ghadimi and Lan, 2013] (since in this case we have D3ℓ(¯xT ) = ∥∇F(¯xT )∥2 2), which is known to be optimal [Arjevani et al., 2023, Drori and Shamir, 2020, Yang et al., 2023]. However, already in the com- posite setting (when r(·) ̸= 0), our result is stronger Ilyas Fatkhullin, Niao He than previously derived bounds for Prox-SGD [Davis et al., 2018] because BFBE can be much larger than BPM/BGM even in the Euclidean case as we have seen in Lemma 4.2. In the more general non-Euclidean case, compared to Theorem 2 in [Ghadimi et al., 2016], our method does not require using large batches, and our proof works for any batch size. Moreover, [Ghadimi et al., 2016] relies on the stronger assumptions: smoothness and bounded variance in the primal norm. Furthermore, a much weaker convergence measure is used in [Ghadimi et al., 2016]: the squared norm of the difference between xt and x+ t .5 Davis et al. [2018] derive convergence of SMD w.r.t. the Bregman divergence between ˆxt and xt, i.e., Dω(ˆxt, xt). Such convergence measure is not satisfactory for two reasons. First, for a general DGF of interest, the Breg- man divergence is not symmetric, and it can happen that Dω(ˆxt, xt) vanishes, while Dω(xt, ˆxt) does not (see, e.g., Proposition 2 in [Bauschke et al., 2017]). Second, to justify this measure the authors in [Davis et al., 2018] assume ω(·) to be twice differentiable and no- tice that 2ρ2Dω(ˆxt, xt) ≥ \r\r(∇2ω(xt))−1∇Φ1/ρ(xt) \r\r2 ∗, where ∇Φ1/ρ(·) is the gradient of the Moreau envelope of Φ(·). However, the latter measure also does not seem to be sufficient either: even if we additionally as- sume the uniform smoothness of ω(·), it is unclear how ∇Φ1/ρ(·) is connected to the standard convergence mea- sures such as the gradient mapping in non-Euclidean setting. In the concurrent work to [Davis et al., 2018], Zhang and He [2018] derive convergence of SMD on the BPM. They also notice that if ω(·) is differentiable and smooth (i.e., ∇ω(·) is M-Lipschitz continuous) on X, then dist2(0, ∂(Φ + δX )(ˆxt)) ≤ M∆ρ(xt). However, we argue that such assumption is very strong since commonly used DGFs such as Shannon entropy are not smooth. Moreover, the analysis in [Zhang and He, 2018] uses bounded gradients (BG) assumption, which fails to hold even for a quadratic function if X is unbounded.6 4.3 High Probability Convergence to FOSP under Sub-Gaussian Noise While convergence in expectation for a randomly se- lected point ¯xT is classical and widely accepted in stochastic optimization, it does not necessarily guaran- tee convergence for a single run of the method. In this section, we extend our Theorem 4.3 to guarantee con- 5Notice that ρ2 \r\rx − x+\r\r2 ≤ ∆+ ρ (x) ≤ 2Dρ/2(x), where the first inequality holds by strong convexity of ω(·), and the second is due to Lemma 4.2. 6Not saying about the general relatively smooth func- tions, for which BG can fail even on a compact domain. vergence for a single run of SMD with high probability. To obtain high probability bounds, we replace our As- sumption 3.2 with the following commonly used “light tail” assumption on the stochastic noise distribution. Assumption 4.4. We have access to a stochastic or- acle that outputs a random vector ∇f(x, ξ) for any given x ∈ X, such that E [∇f(x, ξ)] = ∇F(x), and ∥∇f(x, ξ) − ∇F(x)∥∗ is σ-sub-Gaussian r.v. 7 Theorem 4.5. Let Assumptions 3.1 and 4.4 hold. Let the sequence {ηt}t≥0 be non-increasing with η0 ≤ 1/(2ℓ). Then with probability at least 1 − β 1 PT −1 t=0 ηt T −1 X t=0 ηt D5ℓ(xt) ≤ 5eλ0 + 60σ2ℓ PT −1 t=0 η2 t 2 PT −1 t=0 ηt , where eλ0 := 3 (Φ(x0) − Φ∗) + 8 η0σ2 log (1/β) . To our knowledge, the above theorem is the first high probability bound for nonconvex SMD without use of large batches. If we use large mini-batch,8 then the above theorem implies O \u0010 1 ε2 + σ2 ε2 log (1/β) + σ2 ε4 \u0011 sample complexity to ensure min0≤t≤T −1 D5ℓ(xt) ≤ ε2. Compared to the bound derived in [Ghadimi et al., 2016], which is O \u0010 1 ε2 log (1/β) + σ2 ε4 log (1/β) \u0011 9, our sample complexity is better by a factor of log (1/β). Moreover, our Assumptions 3.1 and 4.4 are weaker than in [Ghadimi et al., 2016]. When specialized to the Euclidean setup and setting the specific step-size sequences, our Theorem 4.5 can recover (up to an abso- lute constant) recently derived high probability bounds for nonconvex SGD [Liu et al., 2023]. However, unlike [Liu et al., 2023], our theorem holds for any square summable step-sizes and accommodates more general (non-Euclidean) norm in Assumption 4.4. We will demonstrate the crucial benefit of using non-Euclidean setup later in Section 5.1. 4.4 Global Convergence under Generalized Proximal P L condition In this subsection, we are interested in global conver- gence of SMD for structured nonconvex problems. We first introduce the following generalization of Proximal Polyak-Lojasiewicz (Prox-P L) condition [Polyak, 1963, Lojasiewicz, 1963, Lezanski, 1963]. 7A random variable X is called σ-sub-Gaussian if E \u0002 exp(λ2X2) \u0003 ≤ exp(λ2σ2) for all λ ∈ R with |λ| ≤ 1/σ. 8Which reduces σ2 to σ2/B for mini-batch of size B. 9Discarding the samples for post-proccesing step in equa- tion (71) therin. Taming Nonconvex Stochastic Mirror Descent Assumption 4.6 (α-Bregman Prox-P L). There exists α ∈ [1, 2] and µ > 0 such that for some ρ ≥ 3ℓ and all x ∈ X ∩ S Dρ(x) ≥ 2µ(Φ(x) − Φ∗) 2/α. (4) The above assumption generalizes Prox-P L condition studied in [Karimi et al., 2016, J Reddi et al., 2016, Li and Li, 2018] in two ways. First, we have Dρ(x) defined w.r.t. an arbitrary non-Euclidean DGF. Sec- ond, we consider α ∈ [1, 2] instead of fixing α = 2. We will demonstrate later in Section 5.2 that both of these generalizations are important in some nonconvex problems and the flexibility of choosing ω(·) can reduce the total sample complexity. We now state the global convergence of SMD. Theorem 4.7. Let Assumptions 3.1, 3.2 and 4.6 hold. For any ε > 0, there exists a choice of step-sizes {ηt}t≥0 for method (2) such that mint≤T E \u0002 Φ(x+ t ) − Φ∗\u0003 ≤ ε after T = O \u0012ℓΛ0 µ 1 ε 2−α α log \u0012ℓΛ0 µε \u0013 + ℓΛ0σ2 µ2 1 ε 4−α α \u0013 . The above result implies that after at most T iterations SMD will find a point xt which is one Mirror Descent step away from a point that is ε-close to Φ∗ in the function value. In the unconstrained Euclidean setting, the above sample complexity matches with that of SGD [Fatkhullin et al., 2022].10 In the special case α = 2, it implies the linear convergence rate in deterministic case and O \u0000ε−1\u0001 sample complexity in the stochastic case. The linear convergence and O \u0000ε−1\u0001 sample com- plexity of SMD were previously shown under relative smoothness and relative strong convexity, e.g., in [Lu et al., 2018, Hanzely and Richt´arik, 2021]. Our result under Assumption 4.6 is more general since the rela- tive strong convexity of F(·) implies (4) with α = 2, see Lemma F.4. It is also known that such rates are optimal for α = 2 in the Euclidean setting [Yue et al., 2023, Agarwal et al., 2009]. 5 NEW INSIGHTS FOR MACHINE LEARNING In this section, we dive into the context of several ma- chine learning applications. We illustrate how each of our Theorems 4.3, 4.5 and 4.7 can be applied to 10We use a different step-size sequence {ηt}t≥0 compared to ηt = 1/tζ, ζ > 0 used in [Fatkhullin et al., 2022], see Appendix D. This allows us to derive noise adaptive rates, i.e., if σ = 0, then we recover the iteration complexity of (deterministic) mirror descent. specific problems; either yielding faster convergence than existing algorithms or allowing us to design prov- ably convergent schemes. Interestingly, the presented problems are very diverse and allow us to demonstrate different aspects of our assumptions. In all presented examples, we crucially rely on the choice of nonsmooth DGFs, which was not theoretically possible to handle in the prior work on SMD. 5.1 DP Learning in ℓ2 and ℓ1 Settings In differentially private (DP) stochastic nonconvex op- timization, the goal is to design a private algorithm to minimize the population loss of type (1) over a subset of a d-dimensional space given n i.i.d. samples, ξ1, . . . , ξn, drawn from a distribution P. Denote by S := \b ξ1, . . . , ξn\t , the sampled dataset, and by ∇F(x) := Pn i=1 ∇f(x, ξi), the gradient of the empiri- cal loss F(x) := Pn i=1 f(x, ξi) based on dataset S. The classical notion to quantify the privacy quality is Definition 5.1 ((ϵ, δ)-DP [Dwork et al., 2006]). A randomized algorithm M is (ϵ, δ)-differentially private if for any pair of datasets S, S′ that differ in exactly one data point and for any event Y ⊆ Range(M) in the output range of M, we have Pr (M(S) ∈ Y) ≤ eϵ Pr (M (S′) ∈ Y) + δ, where the probability is w.r.t. the randomness of M. There are several common techniques to ensure privacy, which include output [Wu et al., 2017, Zhang et al., 2017], objective function [Chaudhuri et al., 2011, Kifer et al., 2012, Iyengar et al., 2019] or gradient perturba- tions [Bassily et al., 2014, Wang et al., 2017]. Most recent works on nonconvex DP learning focus on the latter approach. The key idea of gradient perturbation is to inject an artificial Gaussian noise bt ∼ N(0, σ2 GId) into the evaluated gradient. The parameter σ2 G should be carefully chosen to ensure privacy, which can be guaranteed by the moments accountant Lemma 5.2 (Theorem 1 in [Abadi et al., 2016]). As- sume that ∥∇F(x)∥2 ≤ G for all x ∈ X. There exist constants c1, c2 > 0 so that given the number of iter- ations T ≥ 0, for any ϵ ≤ c1T , the gradient method using ∇F(xt)+bt, bt ∼ N(0, σ2 GId) as the gradient esti- mator is (ϵ, δ)-DP for any δ > 0 if σ2 G ≥ c2 G2T log(1/δ) n2ϵ2 . ℓ2 Setting. For instance, the DP-Prox-GD iterates xt+1 = proxηtr(xt−ηt(∇F(xt)+bt)), bt ∼ N(0, σ2 GId), where proxηtr(x) := arg miny∈X \u0010 r(y) + 1 2ηt ∥y − x∥2 2 \u0011 . Our Theorem 4.5 immediately implies the high proba- bility utility bound for DP-Prox-GD: 1 T T −1 X t=0 E [D5ℓ(xt)] = O  p d log (1/δ) log(1/β) nϵ ! , (5) Ilyas Fatkhullin, Niao He where β ∈ (0, 1) is the failure probability, see Corol- lary E.1 for more details and the dependence on omitted constants. To our knowledge, nonconvex utility bound of DP-Prox-GD was previously studied only in the un- constrained setting (r(·) = 0, X = Rd), e.g., [Wang et al., 2017, 2019, Zhou et al., 2020a] or in expectation, e.g., [Wang and Xu, 2019]. Our bound (5) generalizes these works to non-trivial X and r(·). ℓ1 Setting. One issue with the above utility bound is the polynomial dimension dependence. In certain cases, this dependence can be significantly improved, e.g., when the optimization is defined on a unit simplex X = n x ∈ Rd| Pd i=1 x(i) ≤ 1, x(i) ≥ 0 o . Notably, it makes a big difference which norm we use to mea- sure the variance of bt, e.g., E ∥bt∥2 2 = d σ2 G and E ∥bt∥2 ∞ ≤ 2 log(d) σ2 G. Therefore, using ∥·∥∞ norm is more favorable. Motivated by this difference, we con- sider the differentially private mirror descent (DP-MD): xt+1 = arg min y∈X ηt(⟨∇F(xt)+bt, y⟩+r(y))+Dω(y, xt), where bt ∼ N(0, σ2 GId) and ω(x) = Pd i=1 x(i) log x(i).11 Using our high probability guarantee Theorem 4.5, we can derive Corollary 5.3. Let F(·) be (ℓ, ω)-smooth for ω(·), X defined above, and ∥∇F(x)∥2 ≤ G for all x ∈ X. Set ηt = 1 2ℓ, T = nϵ √ ℓ G√ log(d) log(1/δ) log(1/β), λ0 := Φ(x0) − Φ∗. Then DP-MD is (ϵ, δ)-DP and with probability 1 − β satisfies 12 1 T T −1 X t=0 D5ℓ(xt) = O   G p ℓλ0 log(d) log (1/δ) log (1/β) nϵ ! , The above result establishes a (nearly) dimension inde- pendent utility bound for DP-MD, and improves the one of DP-Prox-GD in (5) by a factor of p d/log(d). Several previous works in DP learning literature have shown the improved dimension dependence in ℓ1 setting, e.g., [Asi et al., 2021, Gopi et al., 2023, Bassily et al., 2021b,a, Wang and Xu, 2019]. However, Asi et al. [2021], Gopi et al. [2023], Bassily et al. [2021b] assume convex F(·), and, therefore, are not directly comparable with our result. Bassily et al. [2021a], and Wang and Xu [2019] obtain nonconvex utility bounds in expectation, how- ever, their techniques are different. Both above men- tioned works rely on the linear minimization oracle and derive convergence on the Frank-Wolfe (FW) gap.13 11It is known that such ω(·) is 1-strongly convex w.r.t. ∥·∥1 on a unit simplex [Beck and Teboulle, 2003]. 12The result can be easily extended to the case when only stochastic gradients ∇f(xt, ξi t) are used instead of ∇F(xt). 13At least when restricted to Euclidean setting, FW gap is a weaker convergence measure than BFBE, see Lemma 4.2 and F.5. Moreover, Bassily et al. [2021a] use a complicated dou- ble loop algorithm based on momentum-based variance reduction technique. 5.2 Policy Optimization in Reinforcement Learning (RL) Consider a discounted Markov decision process (DMDP) M = {S, A, P, R, γ, p}. Here S is a state space with cardinality |S|; A is an action space with cardinality |A|; P is a transition model, where P(s′|s, a) is the transition probability to state s′ from a given state s when action a is applied; R : S × A → [0, 1] is a reward function for a state-action pair (s, a); γ ∈ [0, 1) is the discount factor; and p is the initial state dis- tribution. Being at state sh ∈ S an RL agent takes an action ah ∈ A and transitions to another state sh+1 according to P and receives an immediate re- ward rh ∼ R(sh, ah). A (stationary) policy π speci- fies a (randomized) decision rule depending only on the current state sh, i.e., for each s ∈ S, πs ∈ ∆(A) determines the next action a ∼ πs, where ∆(A) := \b πs ∈ R|A|| P s∈S πsa = 1, πsa ≥ 0 for all a ∈ A \t de- notes the probability simplex supported on A. The goal of RL agent is to maximize V + p (π) := E \" ∞ X h=0 γhrh # , π ∈ X := ∆(A)|S|, (6) where expectation is w.r.t. the initial state distribution s0 ∼ p, the transition model P and the policy π. We define Vp(π) := −V + p (π) and adopt the minimization formulation of DMDP, i.e., minπ∈X Vp(π). It is known that Vp(π) is smooth, but nonconvex in π. Moreover, a property similar to Proximal P L (Assump- tion 4.6) was recently established for (6) [Agarwal et al., 2021, Xiao, 2022]. That is we have for any π, π′ ∈ X: ∥∇Vp(π) − ∇Vp (π′)∥2,2 ≤ LF ∥π − π′∥2,2 , Vp(π) − V ⋆ p ≤ C max π′∈X ⟨∇Vµ(π), π − π′⟩ , (7) where LF := 2γ|A| (1−γ)3 , C := 1 1−γ \r\r\r dp(π⋆) µ \r\r\r ∞, ∥·∥2,2 de- notes the Frobenius norm (Lemma 4 and 54 in [Agarwal et al., 2021]). Therefore, this problem serves well to demonstrate the application of our theory to show convergence of policy gradient (PG) methods. PG methods is the promising class of algorithms that generate a sequence of policies πt by evaluating the gradients ∇Vµ(πt) (or their stochastic estimates b∇Vµ(πt)), where µ ∈ ∆(A) is some distribution (not necessarily equal to p). One of the most basic variants is the Projected Stochastic Policy Gradient: P-SPG: πt+1 = projX \u0010 πt − ηt b∇Vµ(πt) \u0011 , Taming Nonconvex Stochastic Mirror Descent where projX (·) denotes the Euclidean projection onto X. Given that the variance of stochastic gradients b∇Vµ(πt) is bounded in the Euclidean norm by σ2 F ,14 our Theorems 4.3 and 4.7 imply the following Corollary 5.4. For any ε > 0, P-SPG guarantees: (i) min0≤t≤T −1 E [D3LF (πt)] ≤ ε2 after T = O \u0012 |A| (1 − γ)3ε2 + σ2 F |A| (1 − γ)3ε4 \u0013 , (ii) mint≤T E \u0002 Vp(π+ t ) − V ∗ p \u0003 ≤ ε after T = e O \u0012 |A||S| (1 − γ)5ε + σ2 F |A|2|S|2 (1 − γ)7ε3 \u0013 . Convergence of P-SPG was studied (in deterministic case) in [Agarwal et al., 2021] using the notion of gradient mapping. Recently, an improved analysis was provided in [Xiao, 2022] with iteration complex- ity T = O \u0010 |A||S| (1−γ)5ε \u0011 to achieve Vp(πT ) − V ∗ p ≤ ε. If σF = 0, our iteration complexity in (ii) recovers the one in [Xiao, 2022], albeit with a different proof. Improving dependence on |A|. Notice that the above sample complexity bounds depend on the cardi- nality of the action space, which can be large in practice. The key reason for this is that the analysis of P-SPG (Prox-SGD) requires to measure the smoothness constant LF of Vp(π) in the Euclidean (Frobenius) norm, which inevitably depends on the cardinality of the action space |A|. Let us instead consider (2, 1)-matrix norm ∥·∥2,1, i.e., ∥π∥2 2,1 = P s∈S \u0000P a∈A |πsa| \u00012.15 Now, we show that the dependence on |A| in the smoothness constant can be completely removed if (2, 1)-norm is used. Proposition 5.5. For any π, π′ ∈ X, it holds that ∥∇Vp(π) − ∇Vp (π′)∥2,∞ ≤ 2γ (1 − γ)3 ∥π − π′∥2,1 . Consider Stochastic Mirror Policy Gradient (SMPG), that is SMD with the matrix form of Shannon entropy ω(π) := P s∈S P a∈A πsa log πsa.16 The stochastic gradients in SMD are replaced by b∇Vµ(πt) := (b∇1Vµ(πt), . . . , b∇|S|Vµ(πt)). Define Et := (E1 t , . . . , E|S| t ), then SMPG can be written in a closed from. For all s ∈ S πt+1 = πt ⊙ Et, Es t := exp \u0010 −ηt b∇sVµ(πt) \u0011 P a∈A exp \u0010 −ηt b∇sVµ(πt) \u0011, 14The variance of b∇Vµ(·) can be bounded under reason- able assumptions or using appropriate exploration strategies, e.g., ϵ-greedy or Boltzmann, see [Daskalakis et al., 2020, Cesa-Bianchi et al., 2017, Xiao, 2022, Johnson et al., 2023]. 15Its dual satisfies ∥π∥2 2,∞ = P s∈S (maxa∈A |πsa|)2. 16It is 1-strongly convex w.r.t. ∥·∥2,1 norm. where ⊙ denotes an element-wise multiplication of ma- trices and exp(·) is an element-wise exponential. The sample complexity can be derived from Theorem 4.7 using Proposition 5.5 under the bounded variance as- sumption (in dual norm ∥·∥2,∞). Corollary 5.6. For any ε > 0, SMPG guarantees that min0≤t≤T −1 E [Dρ(πt)] ≤ ε2 with ρ := 6γ(1−γ)−3 after T = O   1 (1 − γ)3ε2 + σ2 2,∞ (1 − γ)3ε4 ! . Notice that compared to the bound for P-SPG the above sample complexity is better at least by a factor of |A|. Moreover, σ2,∞ can be much smaller than σF . It should be noted, however, that Dρ(πt) in Corollar- ies 5.4 and 5.6 are induced by different ω(·) and thus induce different FOSP measures. Also it remains un- clear how to establish a global convergence of SMPG in the function value. The technical difficulty arises be- cause the condition (7) might not imply Assumption 4.6 under non-smooth DGF. Remark 5.7. While this example serves well to illus- trate the application of our general theory and potential advantages of SMPG compared to P-SPG, it does not mean that SMPG is the most suitable algorithm for solv- ing (6). In fact, there are other specialized algorithms in RL literature, which have better theoretical sample complexities than shown above. For example, Natural Policy Gradient (NPG) (also known as exponentiated Q-descent or Policy Mirror Descent) [Kakade, 2001, Agarwal et al., 2021, Lan, 2023, Xiao, 2022, Zhan et al., 2023, Khodadadian et al., 2021] achieves faster conver- gence in terms of ε. However, a notable difference of SMPG compared to NPG is that the latter uses a Q- function instead of the policy gradient b∇Vµ(π), see the derivation of NPG in Section 4 in [Xiao, 2022]. Another popular approach to problem (6) is the use of soft-max policy parametrization instead of directly solving the problem over X. In this direction, different variants of PG method were developed and analyzed, see, e.g., [Zhang et al., 2020b, 2021, Barakat et al., 2023]. Remark 5.8. The special cases and variants of Prox- P L condition were previously used to derive global convergence of PG methods [Daskalakis et al., 2020, Kumar et al., 2023] including continuous state action- spaces in RL [Ding et al., 2022, Fatkhullin et al., 2023a] and classical control tasks [Fazel et al., 2018, Fatkhullin and Polyak, 2021, Zhao et al., 2022, Wu et al., 2023]. An alternative approach to global convergence of P-SPG, based on hidden convexity of (6), was recently studied in [Fatkhullin et al., 2023b]. Ilyas Fatkhullin, Niao He 5.3 Training Autoencoder Model using SMD In this section, we showcase how we can harness gen- eral Bregman divergence in SMD to address modern machine learning problems involving linear neural net- works, where the objectives go beyond the smooth regime considered in the existing theoretical analysis [Kawaguchi, 2016]. More specifically, assume that the F(·) is twice differ- entiable and its Hessian is bounded by the polynomial of ∥x∥2, i.e., there exist r, L, Lr ≥ 0 such that \r\r∇2F(x) \r\r op ≤ L + Lr ∥x∥r 2 for all x ∈ Rd.17 (8) The following result (initially appeared in [Lu et al., 2018, Lu, 2019]) shows that for any r ≥ 0, the above condition implies relative smoothness (Assump- tion 3.1). Proposition 5.9 (Proposition 2.1. in [Lu et al., 2018]). Suppose F(·) is twice differentiable and satisfies (8). Then F(·) is ℓ-smooth relative to ω(x) = 1 r+2∥x∥r+2 2 + 1 2∥x∥2 2 with ℓ := max {L, Lr}. To design a provably convergent scheme for such prob- lems, it remains to solve SMD subproblem with DGF specified in the above proposition. Luckily, this is possible ct = (1 + ∥xt∥r 2) xt − ηt∇f(xt, ξt), (9) xt+1 = (1 + θr ∗)−1 ct, (10) where θ∗ ≥ 0 is the unique solution to θr+1 +θ = ∥ct∥2. Convergence to a FOSP of the above method follows immediately from our Theorem 4.3, see Appendix E.3 for more details. For r = 1, 2, the solution θ∗ can be found in a closed form, while for larger values of r it can be solved using a bisection method. To illustrate the empirical performance of the above scheme, we consider two layer autoencoder problem min W1 ∈ Rde×df W2 ∈ Rdf ×de \" F(W) := 1 n n X i=1 ∥W2W1ai − ai∥2 2 # , (11) where ai ∈ Rdf are flattened representations of im- ages, W = [W1, W2] are learned parameters of the model.18 The above problem is nonconvex and globally nonsmooth in W since its Hessian norm grows as a polynomial in the norm of W. Therefore, SGD can easily diverge if poorly initialized. However, condition 17Compare to (L0, L1)-smoothness condition studied in [Zhang et al., 2020a]. 18In previous notations, we have x = vec(W) ∈ R2 de df . 10−5 10−3 10−1 101 step-size 10−1 101 103 105 107 F(xT) Clip SGD SMDr1 SMDr2 SGD Figure 1: Sensitivity to step-size choice for SMDr1, SMDr2, SGD and Clip SGD (with clipping radius 1). The plot shows the function value F(xT ) after T = 104 iterations for each step-size. The star markers correspond to the actual runs, and the lines linearly interpolate between them. (8) can be verified for some r0 ≥ 2 and the scheme (9), (10) provably converges for any r ≥ r0.19 We focus on r = 1, 2 and call the corresponding meth- ods SMDr1 and SMDr2. We compare these algorithms to the standard SGD, which corresponds to r = 0. For comparison, we also include a popular heuristic algo- rithm, Clip SGD, which is known to mitigate the problem of exploding gradients. We use constant step-sizes for each method. Figure 1 reports the final training loss for each step-size in the range \b 2−19, 2−18, . . . , 27\t . We observe from Figure 1 that SMDr1 and SMDr2 al- low for much larger step-sizes than SGD. Moreover, increasing r also increases the robustness to the step- size choice, i.e., the lower part of the curve becomes wider. At the same time, in the high accuracy regime, Clip SGD still outperforms other algorithms on this task. Note, however, that Clip SGD might not converge in general in the stochastic setting for a constant clipping parameter, see, e. g., [Koloskova et al., 2023]. Acknowledgments This work is supported by ETH AI Center Doctoral Fel- lowship and Swiss National Science Foundation (SNSF) Project Funding No. 200021-207343. References M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov, K. Talwar, and L. Zhang. Deep learn- ing with differential privacy. In Proceedings of the 19Computation of the Hessian and use of Cauchy-Schwarz inequality implies (8) for any number of layers in (11). Taming Nonconvex Stochastic Mirror Descent 2016 ACM SIGSAC conference on computer and communications security, pages 308–318, 2016. S. Acharyya, A. Banerjee, and D. Boley. Bregman divergences and triangle inequality. In Proceedings of the 2013 SIAM International Conference on Data Mining, pages 476–484. SIAM, 2013. A. Agarwal, M. J. Wainwright, P. Bartlett, and P. Ravikumar. Information-theoretic lower bounds on the oracle complexity of convex optimization. Ad- vances in Neural Information Processing Systems, 22, 2009. A. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan. On the theory of policy gradient methods: Optimal- ity, approximation, and distribution shift. Journal of Machine Learning Research, 22(98):1–76, 2021. M. Ahookhosh, A. Themelis, and P. Patrinos. A breg- man forward-backward linesearch algorithm for non- convex composite optimization: superlinear conver- gence to nonisolated local minima. SIAM Journal on Optimization, 31(1):653–685, 2021. Z. Allen-Zhu and L. Orecchia. Linear coupling: An ultimate unification of gradient and mirror descent. arXiv preprint arXiv:1407.1537, 2014. Y. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster, N. Srebro, and B. Woodworth. Lower bounds for non-convex stochastic optimization. Mathematical Programming, 199(1-2):165–214, 2023. S. Arora, E. Hazan, and S. Kale. The multiplicative weights update method: a meta-algorithm and ap- plications. Theory of computing, 8(1):121–164, 2012. H. Asi, V. Feldman, T. Koren, and K. Talwar. Private stochastic convex optimization: Optimal rates in l1 geometry. In Proceedings of the 38th International Conference on Machine Learning, volume 139, pages 393–403, 2021. K. Balasubramanian and S. Ghadimi. Zeroth-order nonconvex stochastic optimization: Handling con- straints, high dimensionality, and saddle points. Foundations of Computational Mathematics, pages 1–42, 2022. A. Barakat, I. Fatkhullin, and N. He. Reinforcement learning with general utilities: Simpler variance re- duction and large state-action space. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 1753–1800, 2023. R. Bassily, A. Smith, and A. Thakurta. Private empiri- cal risk minimization: Efficient algorithms and tight error bounds. In 2014 IEEE 55th annual symposium on foundations of computer science, pages 464–473. IEEE, 2014. R. Bassily, C. Guzm´an, and M. Menart. Differen- tially private stochastic optimization: New results in convex and non-convex settings. Advances in Neu- ral Information Processing Systems, 34:9317–9329, 2021a. R. Bassily, C. Guzm´an, and A. Nandi. Non-euclidean differentially private stochastic convex optimization. In Conference on Learning Theory, pages 474–499. PMLR, 2021b. H. H. Bauschke, J. Bolte, and M. Teboulle. A de- scent lemma beyond lipschitz gradient continuity: first-order methods revisited and applications. Math- ematics of Operations Research, 42(2):330–348, 2017. A. Beck and M. Teboulle. Mirror descent and nonlinear projected subgradient methods for convex optimiza- tion. Operations Research Letters, 31(3):167–175, 2003. A. Ben-Tal, T. Margalit, and A. Nemirovski. The ordered subsets mirror descent optimization method with applications to tomography. SIAM Journal on Optimization, 12(1):79–108, 2001. B. Birnbaum, N. R. Devanur, and L. Xiao. Distributed algorithms via gradient descent for fisher markets. In Proceedings of the 12th ACM conference on Elec- tronic commerce, pages 127–136, 2011. L. M. Bregman. The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR computational mathematics and mathematical physics, 7(3):200–217, 1967. N. Cesa-Bianchi, C. Gentile, G. Lugosi, and G. Neu. Boltzmann exploration done right. Advances in Neu- ral Information Processing Systems, 30, 2017. K. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Dif- ferentially private empirical risk minimization. Jour- nal of Machine Learning Research, 12(3), 2011. G. Chen and M. Teboulle. Convergence analysis of a proximal-like minimization algorithm using bregman functions. SIAM Journal on Optimization, 3(3):538– 543, 1993. C. Daskalakis, D. J. Foster, and N. Golowich. Indepen- dent policy gradient methods for competitive rein- forcement learning. Advances in Neural Information Processing Systems, 33:5527–5540, 2020. D. Davis and D. Drusvyatskiy. Stochastic model-based minimization of weakly convex functions. SIAM Journal on Optimization, 29(1):207–239, 2019. D. Davis and B. Grimmer. Proximally guided stochas- tic subgradient method for nonsmooth, nonconvex problems. SIAM Journal on Optimization, 29(3): 1908–1930, 2019. D. Davis, D. Drusvyatskiy, and K. J. MacPhee. Stochastic model-based minimization under high- Ilyas Fatkhullin, Niao He order growth. arXiv preprint arXiv:1807.00255, 2018. K. Ding, J. Li, and K.-C. Toh. Nonconvex stochastic bregman proximal gradient method with application to deep learning. arXiv preprint arXiv:2306.14522, 2023. Y. Ding, J. Zhang, and J. Lavaei. On the global op- timum convergence of momentum-based policy gra- dient. In International Conference on Artificial In- telligence and Statistics, pages 1910–1934. PMLR, 2022. R.-A. Dragomir, A. B. Taylor, A. d’Aspremont, and J. Bolte. Optimal complexity and certification of bregman first-order methods. Mathematical Pro- gramming, pages 1–43, 2021. Y. Drori and O. Shamir. The complexity of finding stationary points with stochastic gradient descent. In International Conference on Machine Learning, pages 2658–2667. PMLR, 2020. D. Drusvyatskiy and C. Paquette. Efficiency of mini- mizing compositions of convex functions and smooth maps. Mathematical Programming, 178:503–558, 2019. C. Dwork, F. McSherry, K. Nissim, and A. Smith. Cal- ibrating noise to sensitivity in private data analysis. In Theory of Cryptography: Third Theory of Cryp- tography Conference, TCC 2006, New York, NY, USA, March 4-7, 2006. Proceedings 3, pages 265–284. Springer, 2006. I. Fatkhullin and B. Polyak. Optimizing static lin- ear feedback: Gradient method. SIAM Journal on Control and Optimization, 59(5):3887–3911, 2021. I. Fatkhullin, J. Etesami, N. He, and N. Kiyavash. Sharp analysis of stochastic optimization under global Kurdyka- Lojasiewicz inequality. Advances in Neural Information Processing Systems, 2022. I. Fatkhullin, A. Barakat, A. Kireeva, and N. He. Stochastic policy gradient methods: Improved sam- ple complexity for Fisher-non-degenerate policies. In Proceedings of the 40th International Conference on Machine Learning, volume 202, pages 9827–9869, 2023a. I. Fatkhullin, N. He, and Y. Hu. Stochastic opti- mization under hidden convexity. arXiv preprint arXiv:2401.00108v1, 2023b. I. Fatkhullin, A. Tyurin, and P. Richt´arik. Momen- tum provably improves error feedback! Advances in Neural Information Processing Systems, 2023c. M. Faw, L. Rout, C. Caramanis, and S. Shakkottai. Beyond uniform smoothness: A stopped analysis of adaptive sgd. arXiv preprint arXiv:2302.06570, 2023. M. Fazel, R. Ge, S. Kakade, and M. Mesbahi. Global convergence of policy gradient methods for the linear quadratic regulator. In International Conference on Machine Learning, pages 1467–1476. PMLR, 2018. S. Ghadimi and G. Lan. Stochastic first-and zeroth- order methods for nonconvex stochastic program- ming. SIAM Journal on Optimization, 23(4):2341– 2368, 2013. S. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochas- tic approximation methods for nonconvex stochastic composite optimization. Mathematical Programming, 155(1-2):267–305, 2016. S. Gopi, Y. T. Lee, D. Liu, R. Shen, and K. Tian. Pri- vate convex optimization in general norms. In Pro- ceedings of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pages 5068–5089. SIAM, 2023. F. Hanzely and P. Richt´arik. Fastest rates for stochastic mirror descent methods. Computational Optimiza- tion and Applications, 79:717–766, 2021. F. Hanzely, P. Richtarik, and L. Xiao. Accelerated breg- man proximal gradient methods for relatively smooth convex optimization. Computational Optimization and Applications, 79:405–440, 2021. F. Huang, S. Gao, and H. Huang. Bregman gradient policy optimization. In International Conference on Learning Representations, 2022. F. H¨ubler, J. Yang, X. Li, and N. He. Parameter- agnostic optimization under relaxed smoothness. arXiv preprint arXiv:2311.03252, 2023. R. Iyengar, J. P. Near, D. Song, O. Thakkar, A. Thakurta, and L. Wang. Towards practical differ- entially private convex optimization. In 2019 IEEE Symposium on Security and Privacy (SP), pages 299– 316. IEEE, 2019. S. J Reddi, S. Sra, B. Poczos, and A. J. Smola. Proximal stochastic methods for nonsmooth nonconvex finite- sum optimization. Advances in Neural Information Processing Systems, 29, 2016. E. Johnson, C. Pike-Burke, and P. Rebeschini. Optimal convergence rate for exact policy mirror descent in discounted markov decision processes. arXiv preprint arXiv:2302.11381, 2023. S. M. Kakade. A natural policy gradient. Advances in Neural Information Processing Systems, 14, 2001. H. Karimi, J. Nutini, and M. Schmidt. Linear conver- gence of gradient and proximal-gradient methods un- der the Polyak- Lojasiewicz condition. arXiv preprint arXiv:1608.04636v4, 2016. K. Kawaguchi. Deep learning without poor local min- ima. Advances in Neural Information Processing Systems, 29, 2016. Taming Nonconvex Stochastic Mirror Descent S. Khodadadian, P. R. Jhunjhunwala, S. M. Varma, and S. T. Maguluri. On the linear convergence of natural policy gradient algorithm. In 2021 60th IEEE Conference on Decision and Control (CDC), pages 3794–3799. IEEE, 2021. D. Kifer, A. Smith, and A. Thakurta. Private convex empirical risk minimization and high-dimensional regression. In Conference on Learning Theory, pages 25–1. JMLR Workshop and Conference Proceedings, 2012. A. Koloskova, H. Hendrikx, and S. U. Stich. Revis- iting gradient clipping: Stochastic bias and tight convergence guarantees. In Proceedings of the 40th International Conference on Machine Learning, 2023. N. Kumar, I. Usmanova, K. Y. Levy, and S. Mannor. Towards faster global convergence of robust policy gradient methods. In Sixteenth European Workshop on Reinforcement Learning, 2023. G. Lan. An optimal method for stochastic composite optimization. Mathematical Programming, 133(1-2): 365–397, 2012. G. Lan. Policy mirror descent for reinforcement learn- ing: Linear convergence, new sampling complexity, and generalized problem classes. Mathematical pro- gramming, 198(1):1059–1106, 2023. T. Lezanski. ¨Uber das minimumproblem f¨ur funktionale in banachschen r¨aumen. Mathematische Annalen, 152:271–274, 1963. Z. Li and J. Li. A simple proximal stochastic gradi- ent method for nonsmooth nonconvex optimization. Advances in Neural Information Processing Systems, 2018. T. Liu and T. K. Pong. Further properties of the forward–backward envelope with applications to difference-of-convex programming. Computational Optimization and Applications, 67:489–520, 2017. Z. Liu, T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. High probability convergence of stochas- tic gradient methods. In International Conference on Machine Learning, pages 21884–21914. PMLR, 2023. S. Lojasiewicz. Une propri´et´e topologique des sous- ensembles analytiques r´eels. Les ´equations aux d´eriv´ees partielles, 117:87–89, 1963. H. Lu. “relative continuity” for non-lipschitz nons- mooth convex optimization using stochastic (or de- terministic) mirror descent. INFORMS Journal on Optimization, 1(4):288–303, 2019. H. Lu, R. M. Freund, and Y. Nesterov. Relatively smooth convex optimization by first-order methods, and applications. SIAM Journal on Optimization, 28(1):333–354, 2018. A. V. Nazin, A. S. Nemirovsky, A. B. Tsybakov, and A. B. Juditsky. Algorithms of robust stochastic opti- mization based on mirror descent method. Automa- tion and Remote Control, 80:1607–1627, 2019. A. Nemirovskij and D. Yudin. Efficient methods of solving convex programming problems of high dimen- sionality. Ekonomika i matem. methody (in Russian), XV(1), 1979. A. Nemirovskij and D. Yudin. Problem complexity and method efficiency in optimization. SIAM Review, 1983. Y. Nesterov. Lectures on convex optimization, volume 137. Springer, 2018. T. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen. Improved convergence in high probability of clipped gradient methods with heavy tailed noise. Advances in Neural Information Processing Systems, 2023. P. Patrinos and A. Bemporad. Proximal newton meth- ods for convex composite optimization. In 52nd IEEE Conference on Decision and Control, pages 2358–2363. IEEE, 2013. B. T. Polyak. Gradient methods for the minimisation of functionals. USSR Computational Mathematics and Mathematical Physics, 3(4):864–878, 1963. S. Shalev-Shwartz. Online learning and online convex optimization. Foundations and Trends® in Machine Learning, 4(2):107–194, 2012. L. Stella, A. Themelis, and P. Patrinos. Forward– backward quasi-newton methods for nonsmooth op- timization problems. Computational Optimization and Applications, 67(3):443–487, 2017. S. U. Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint arXiv:1907.04232, 2019. R. Van Handel. Probability in high dimension. Lecture Notes (Princeton University), 2014. N. M. Vural, L. Yu, K. Balasubramanian, S. Volgu- shev, and M. A. Erdogdu. Mirror descent strikes again: Optimal stochastic convex optimization under infinite noise variance. In Conference on Learning Theory, pages 65–102. PMLR, 2022. D. Wang and J. Xu. Differentially private empiri- cal risk minimization with smooth non-convex loss functions: A non-stationary view. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 1182–1189, 2019. D. Wang, M. Ye, and J. Xu. Differentially private em- pirical risk minimization revisited: Faster and more general. Advances in Neural Information Processing Systems, 30, 2017. Ilyas Fatkhullin, Niao He D. Wang, C. Chen, and J. Xu. Differentially private empirical risk minimization with non-convex loss functions. In International Conference on Machine Learning, pages 6526–6535. PMLR, 2019. J. Wu, A. Barakat, I. Fatkhullin, and N. He. Learn- ing zero-sum linear quadratic games with improved sample complexity. In 62nd IEEE Conference on Decision and Control, 2023. X. Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and J. Naughton. Bolt-on differential privacy for scalable stochastic gradient descent-based analytics. In Pro- ceedings of the 2017 ACM International Conference on Management of Data, pages 1307–1322, 2017. H. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learn- ing algorithms. arXiv preprint arXiv:1708.07747, 2017. L. Xiao. On the convergence rates of policy gradient methods. The Journal of Machine Learning Research, 23(1):12887–12922, 2022. J. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of one coin: the limits of untuned sgd and the power of adaptive methods. arXiv preprint arXiv:2305.12475, 2023. P. Yue, C. Fang, and Z. Lin. On the lower bound of minimizing polyak- lojasiewicz functions. In The Thirty Sixth Annual Conference on Learning Theory, pages 2948–2968. PMLR, 2023. W. Zhan, S. Cen, B. Huang, Y. Chen, J. D. Lee, and Y. Chi. Policy mirror descent for regularized rein- forcement learning: A generalized framework with linear convergence. SIAM Journal on Optimization, 33(2):1061–1091, 2023. J. Zhang, K. Zheng, W. Mou, and L. Wang. Efficient private erm for smooth objectives. arXiv preprint arXiv:1703.09947, 2017. J. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradi- ent clipping accelerates training: A theoretical justi- fication for adaptivity. In International Conference on Learning Representations, 2020a. J. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and M. Wang. Variational policy gradient method for reinforcement learning with general utilities. Ad- vances in Neural Information Processing Systems, 33:4572–4583, 2020b. J. Zhang, C. Ni, C. Szepesvari, M. Wang, et al. On the convergence and sample efficiency of variance- reduced policy gradient method. Advances in Neural Information Processing Systems, 34:2228–2240, 2021. L. Zhang. Variance reduction for non-convex stochastic optimization: General analysis and new applications. Master’s thesis, ETH Zurich, 2021. S. Zhang and N. He. On the convergence rate of stochas- tic mirror descent for nonsmooth nonconvex opti- mization. arXiv preprint arXiv:1806.04781, 2018. F. Zhao, X. Fu, and K. You. Global conver- gence of policy gradient methods for output feed- back linear quadratic control. arXiv preprint arXiv:2211.04051v1, 2022. Y. Zhou, X. Chen, M. Hong, Z. S. Wu, and A. Banerjee. Private stochastic non-convex optimization: Adap- tive algorithms and tighter generalization bounds. arXiv preprint arXiv:2006.13501, 2020a. Z. Zhou, P. Mertikopoulos, N. Bambos, S. P. Boyd, and P. W. Glynn. On the convergence of mirror de- scent beyond stochastic convex programming. SIAM Journal on Optimization, 30(1):687–716, 2020b. Taming Nonconvex Stochastic Mirror Descent Contents 1 INTRODUCTION 1 1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 2 PRELIMINARIES 2 2.1 FOSP Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 ASSUMPTIONS 3 4 MAIN RESULTS 3 4.1 Connections between FOSP Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4.2 Convergence to FOSP in Expectation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 4.3 High Probability Convergence to FOSP under Sub-Gaussian Noise . . . . . . . . . . . . . . . . . 5 4.4 Global Convergence under Generalized Proximal P L condition . . . . . . . . . . . . . . . . . . . 5 5 NEW INSIGHTS FOR MACHINE LEARNING 6 5.1 DP Learning in ℓ2 and ℓ1 Settings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 5.2 Policy Optimization in Reinforcement Learning (RL) . . . . . . . . . . . . . . . . . . . . . . . . . 7 5.3 Training Autoencoder Model using SMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 A Proofs of Lemma 4.1 and 4.2: Connections Between FOSP 15 B Proof of Theorem 4.3: Convergence to FOSP in Expectation 18 C Proof of Theorem 4.5: High Probability Convergence to FOSP under Sub-Gaussian Noise 20 D Proof of Theorem 4.7. Global Convergence under Generalized Proximal P L Condition 22 E Proofs for Applications 23 E.1 Differentially Private Learning in ℓ2 and ℓ1 Settings . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.2 Policy Optimization in Reinforecement Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 E.3 Training Autoencoder Model using SMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 F Useful Lemma 26 Ilyas Fatkhullin, Niao He A Proofs of Lemma 4.1 and 4.2: Connections Between FOSP BPM and BGM are equivalent up to a constant factor. The following lemma establishes that if F(·) is (ℓ, ω)-smooth, then the distance between ˆx and x+ is small. Lemma A.1. For any ρ > ℓ, we have for all x ∈ X ∩ S ρ2Dsym ω (ˆx, x+) ≤ ℓ ρ − ℓ \u0000∆ρ(x) + ∆+ ρ (x) \u0001 . Proof. Recall that ˆx := arg miny∈X F(y)+r(y)+ρDω(y, x), and x+ := arg miny∈X ⟨∇F(x), y⟩+r(y)+ρDω(y, x). By the optimality conditions for x+ and ˆx, there exist s+ ∈ ∂(r + δX )(x+) and ˆs ∈ ∂(r + δX )(ˆx), such that 0 = ∇F(ˆx) + ˆs + ρ ∇ω(ˆx) − ρ ∇ω(x), 0 = ∇F(x) + s+ + ρ ∇ω(x+) − ρ ∇ω(x), Subtracting these equalities, we obtain ρ ∇ω(ˆx) − ρ ∇ω(x+) = s+ − ˆs + ∇F(x) − ∇F(ˆx). By Lemma F.1-1 (three point identity) with x = z and using the above identity, we have ρ(Dω(ˆx, x+) + Dω(x+, ˆx)) = ⟨ρ ∇ω(ˆx) − ρ ∇ω(x+), ˆx − x+⟩ = ⟨s+ − ˆs + ∇F(x) − ∇F(ˆx), ˆx − x+⟩ (i) ≤ ⟨∇F(x) − ∇F(ˆx), ˆx − x+⟩ (12) where in (i) we use convexity of (r + δX )(·). By relative smoothness of F(·), we have for any x, y, z ∈ X ∩ S F(x) − F(y) − ⟨∇F(y), x − y⟩ ≤ ℓDω(x, y), F(z) − F(x) − ⟨∇F(x), z − x⟩ ≤ ℓDω(z, x), −ℓDω(z, y) ≤ F(z) − F(y) − ⟨∇F(y), z − y⟩. Adding the above inequalities gives for any x, y, z ∈ X ∩ S ⟨∇F(x) − ∇F(y), y − z⟩ ≤ ℓDω(x, y) + ℓDω(z, x) + ℓDω(z, y). Applying the above inequality with x = x, y = ˆx, z = x+, we further bound (12) ρ(Dω(ˆx, x+) + Dω(x+, ˆx)) ≤ ℓDω(x, ˆx) + ℓDω(x+, x) + ℓDω(x+, ˆx) ≤ ℓDω(x, ˆx) + ℓDω(x+, x) + ℓDω(x+, ˆx) + ℓDω(ˆx, x+). Therefore, for any ρ > ℓ, we have ρ2 \u0000Dω(ˆx, x+) + Dω(x+, ˆx) \u0001 ≤ ℓρ2 ρ − ℓ(Dω(x, ˆx) + Dω(x+, x)) ≤ ℓ ρ − ℓ \u0000∆ρ(x) + ∆+ ρ (x) \u0001 . Proof of Lemma 4.1. For any x, y, z ∈ X ∩ S and s > 0, we have Dsym ω (x, y) ≤ \u0012q Dsym ω (x, z) + q Dsym ω (y, z) \u00132 ≤ (1 + s) Dsym ω (x, z) + \u00001 + s−1\u0001 Dsym ω (y, z). Applying Lemma A.1 together with the above inequality, we have ∆+ ρ (x) = ρ2Dsym ω (x, x+) ≤ (1 + s)ρ2Dsym ω (x, ˆx) + (1 + s−1)ρ2Dsym ω (x+, ˆx) ≤ (1 + s)ρ2Dsym ω (x, ˆx) + (1 + s−1)ℓ ρ − ℓ \u0000∆ρ(x) + ∆+ ρ (x) \u0001 ≤ \u0012 1 + s + (1 + s−1)ℓ ρ − ℓ \u0013 ∆ρ(x) + (1 + s−1)ℓ ρ − ℓ ∆+ ρ (x). Taming Nonconvex Stochastic Mirror Descent Rearranging, we obtain the upper bound on ∆+ ρ (x). For the lower bound, we act similarly and derive ∆ρ(x) = ρ2Dsym ω (x, ˆx) ≤ (1 + s)ρ2Dsym ω (x, x+) + (1 + s−1)ρ2Dsym ω (ˆx, x+) ≤ (1 + s)ρ2Dsym ω (x, x+) + (1 + s−1)ℓ ρ − ℓ \u0000∆ρ(x) + ∆+ ρ (x) \u0001 ≤ \u0012 1 + s + (1 + s−1)ℓ ρ − ℓ \u0013 ∆+ ρ (x) + (1 + s−1)ℓ ρ − ℓ ∆ρ(x). Combining the above two inequalities, we have ∆ρ(x) C(ℓ,ρ,s) ≤ ∆+ ρ (x) ≤ C(ℓ, ρ, s)∆ρ(x). Remark A.2. Notice that our intermediate Lemma A.1 does not require ω(·) to induce a metric and shows that ˆx and x+ are close if ∆ρ(x) and ∆+ ρ (x) are small. However, the proof of Lemma 4.1 crucially relies on triangle inequality. Therefore, it is unclear whether convergence of SMD in ∆ρ(x) (that was established in [Zhang and He, 2018] under BG assumption) implies convergence in ∆+ ρ (x). In our main Theorems 4.3, 4.5 and 4.7, we bypass this issue and directly establish convergence on Dρ(x), that is a stronger measure than ∆+ ρ (x) (and stronger than ∆ρ(x) if p Dsym ω (x, y) is a metric). Moreover, as we have seen in Section 2, Dρ(x) seems to be a more natural FOSP measure since it reduces to ∥∇F(x)∥2 in unconstrained case. BFBE is strictly larger than BGM. Now we state the proof of Lemma 4.2, which consists of two parts. Proof of Lemma 4.2. 1. BFBE is not smaller than BGM. Recall that x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + ρDω(y, x). By the optimality condition, there exists u+ ∈ ∂r(x+) such that 0 = ∇F(x) + ρ(∇ω(x+) − ∇ω(x)) + u+. Thus, by convexity of r(·) r(x) ≥ r(x+) + ⟨u+, x − x+⟩ = r(x+) + ρ⟨ ∇ω(x) − ∇ω(x+), x − x+⟩ − ⟨∇F(x), x − x+⟩ = r(x+) + ρ(Dω(x, x+) + Dω(x+, x)) − ⟨∇F(x), x − x+⟩. Using the above inequality and the definition of Dρ(x), we derive for any ρ, ρ1 > 0 1 2ρ1 Dρ1(x) := − min y∈X {⟨∇F(x), y − x⟩ + ρ1Dω(y, x) + r(y) − r(x)} = ⟨∇F(x), x − x+⟩ − ρ1Dω(x+, x) + r(x) − r(x+) ≥ ρ(Dω(x, x+) + Dω(x+, x)) − ρ1Dω(x+, x) ≥ (ρ − ρ1)(Dω(x+, x) + Dω(x+, x)). Recalling the definition of ∆+ ρ (x) and setting ρ1 = ρ/2, it remains to conclude that Dρ/2(x) ≥ 1 2∆+ ρ (x). 2. BFBE can be much larger than BGM. The following example shows how large can be the ratio of BFBE and BGM. Consider minimizing Φ(x) = F(x) + r(x) over X = [0, 1] ⊂ R1 with F(x) = 1 2x2, r(x) = |x|. We can compute the proximal operator of the absolute value as proxr/ρ(x) = sign(x) max n 0, |x| − 1 ρ o , where sign(x) = 1, if x ≥ 0 and sign(x) = −1 otherwise. For any x ∈ [0, 1] and ρ ≥ 1, we can compute x+ = proxr/ρ(x − ρ−1x) = 0. Therefore, we have ∆+ ρ (x) = x2, Dρ(x) = −2ρ(⟨∇F(x), x+ − x⟩ + ρ 2(x+ − x)2 + |x+| − |x|) = 2ρ|x| + 2ρ \u0010 1 − ρ 2 \u0011 x2. In particular, taking arbitrary ρ = ρ1 ≥ 1 in the first equality and ρ ≤ 2 in the second equality, we have for any x ∈ (0, 1] Dρ(x) ∆+ ρ1(x) ≥ 2ρ|x| x2 ≥ 2 |x|. Ilyas Fatkhullin, Niao He We conclude that Dρ(x) can be arbitrary larger than ∆+ ρ1(x) even when x is close to the optimum x∗ = 0. This implies that the opposite inequality in Lemma 4.2 does not hold in general even when ρ and ρ1 are allowed to be different. Therefore, BFBE is strictly stronger convergence measure than BGM. Taming Nonconvex Stochastic Mirror Descent B Proof of Theorem 4.3: Convergence to FOSP in Expectation Proof. Step I. Deterministic descent w.r.t. Forward-Backward Envelope. Define ˆx := proxΦ/ρ(x). Notice that for any x ∈ X ∩ S, we have for any x+ ∈ X ∩ S Φ1/ρ(x) = Φ(ˆx) + ρDω(ˆx, x) ≤ Φ(x+) + ρDω(x+, x). We set x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + ρ1Dω(y, x) with ρ1 ≥ ρ + ℓ. Then by relative smoothness (upper bound) of F(·) Φ1/ρ(x) ≤ Φ \u0000x+\u0001 + ρDω(x+, x) = F \u0000x+\u0001 + r \u0000x+\u0001 + ρDω(x+, x) ≤ F (x) +  ∇F (x) , x+ − x \u000b + ℓDω(x+, x) + r \u0000x+\u0001 + ρDω(x+, x) = Φ (x) +  ∇F (x) , x+ − x \u000b + (ρ + ℓ)Dω(x+, x) + r \u0000x+\u0001 − r (x) = Φ (x) − 1 2ρ1 Dρ1(x) + (ρ + ℓ − ρ1) Dω(x+, x) ≤ Φ (x) − 1 2ρ1 Dρ1(x). (13) where the last equality holds by definitions of x+, Dρ(x) and the last step is due to condition ρ1 ≥ ρ + ℓ. Step II. One step progress on the Lyapunov function. By the update rule of xt+1 and applying Lemma F.1, Item 2 with z = xt, z+ = xt+1, x = ˆxt, we have ηt⟨∇f(xt, ξt), ˆxt − xt+1⟩ + ηt(r(ˆxt) − r(xt+1)) ≥ Dω(ˆxt, xt+1) + Dω(xt+1, xt) − Dω(ˆxt, xt). (14) By the optimality of ˆxt+1 and using the above inequality, we derive Φ1/ρ (xt+1) = Φ \u0000ˆxt+1\u0001 + ρDω(ˆxt+1, xt+1) ≤ Φ \u0000ˆxt\u0001 + ρDω(ˆxt, xt+1) (14) ≤ Φ \u0000ˆxt\u0001 + ηtρ⟨∇f(xt, ξt), ˆxt − xt+1⟩ + ρDω(ˆxt, xt) − ρDω(xt+1, xt) + ηtρ(r(ˆxt) − r(xt+1)) = Φ1/ρ (xt) + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩) + ρηt⟨∇f(xt, ξt), xt − xt+1⟩ − ρDω(xt+1, xt) + ηtρ(r(xt) − r(xt+1)). We define λt := Φ1/ρ (xt) − Φ∗ + ηt−1ρ(Φ(xt) − Φ∗), ψt := ∇f(xt, ξt) − ∇F(xt) . Then using the above inequality λt+1 := Φ1/ρ (xt+1) − Φ∗ + ηtρ(Φ(xt+1) − Φ∗) ≤ Φ1/ρ (xt) − Φ∗ + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩) +ρ(ηt⟨∇f(xt, ξt) − ∇F(xt), xt − xt+1⟩ − Dω(xt+1, xt)) +ηtρ(r(xt) + F(xt+1) + ⟨∇F(xt), xt − xt+1⟩ − Φ∗) (i) ≤ Φ1/ρ (xt) − Φ∗ + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩) +ρ(ηt⟨∇f(xt, ξt) − ∇F(xt), xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)) +ηtρ(r(xt) + F(xt) − Φ∗) = λt + (ηt − ηt−1)ρ(Φ(xt) − Φ∗) + ηtρ(r(ˆxt) − r(xt) + ⟨∇F(xt), ˆxt − xt⟩) + ρηt⟨ψt, ˆxt − xt⟩ +ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)) (ii) ≤ λt + (ηt − ηt−1)ρ(Φ(xt) − Φ∗) − ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) − ηtρ(ρ − ℓ)Dω(ˆxt, xt) + ρηt⟨ψt, ˆxt − xt⟩ +ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)) (iii) ≤ λt − ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)) (15) −ηtρ(ρ − ℓ)Dω(ˆxt, xt), Ilyas Fatkhullin, Niao He where (i) follows by relative smoothness (upper bound), i.e., F(xt+1) ≤ F(xt)−⟨∇F(xt), xt−xt+1⟩+ℓDω(xt+1, xt). The inequality (ii) follows from relative smoothness (lower bound) of F(·) and (13) (with ρ1 = ρ + ℓ) since r(ˆxt) − r(xt) + ⟨∇F(xt), ˆxt − xt⟩ ≤ r(ˆxt) − r(xt) + F(ˆxt) − F(xt) + ℓDω(ˆxt, xt) = Φ1/ρ(xt) − Φ(xt) + (ℓ − ρ)Dω(ˆxt, xt) ≤ Φ1/ρ(xt) − Φ(xt) + (ℓ − ρ)Dω(ˆxt, xt) ≤ − 1 2(ρ + ℓ)Dρ+ℓ(xt) − (ρ − ℓ)Dω(ˆxt, xt). The inequality (iii) holds since the sequence {ηt}t≥0 is non-increasing. Step III. Dealing with stochastic terms. Using Dω(xt+1, xt) ≥ 1 2 ∥xt+1 − xt∥2 and the bound on the variance of stochastic gradients, we have E \u0002 ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt) \u0003 ≤ E \u0014 ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)1 2 ∥xt+1 − xt∥2 \u0015 ≤ η2 t 2(1 − ηtℓ)E h\r\rψt\r\r2 ∗ i ≤ η2 t σ2 2(1 − ηtℓ). (16) Define Λt := E [λt]. Then combining (15) with (16) and setting ρ = 2ℓ, ηt ≤ 1/(2ℓ), we derive for any non-increasing step-sizes ηt Λt+1 ≤ Λt − ηtρ 2(ρ + ℓ)E [Dρ+ℓ(xt)] + ρη2 t σ2 2(1 − ηtℓ) ≤ Λt − ηt 3 E [D3ℓ(xt)] + 2ℓη2 t σ2. (17) It remains to telescope and conclude the proof since T −1 X t=0 ηtE [D3ℓ(xt)] ≤ 3Λ0 + 6ℓσ2 T −1 X t=0 η2 t , where the Lyapunov function in the initial point can be bounded (setting η−1 = η0) as Λ0 = λ0 = Φ1/ρ(x0) − Φ∗ + η−1ρ(Φ(x0) − Φ∗) ≤ Φ1/ρ(x0) − Φ∗ + Φ(x0) − Φ∗. The proof for constant step-size follows immediately from (3). Taming Nonconvex Stochastic Mirror Descent C Proof of Theorem 4.5: High Probability Convergence to FOSP under Sub-Gaussian Noise Proof. We recall the definitions of λt = Φ1/ρ (xt) − Φ∗ + ηt−1ρ(Φ(xt) − Φ∗), ψt = ∇f(xt, ξt) − ∇F(xt), and invoke (15) from the proof of Theorem 4.3 λt+1 = λt − ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρηt⟨ψt, xt − xt+1⟩ − ρ(1 − ηtℓ)Dω(xt+1, xt) −ηtρ(ρ − ℓ)Dω(ˆxt, xt) ≤ λt − ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρη2 t ∥ψt∥2 ∗ 2(1 − ηtℓ) − ηtρ(ρ − ℓ)Dω(ˆxt, xt). (18) Define a (normalization) scalar w := ρ−ℓ 6σ2ρη0 > 0, and a sequence St := PT −1 τ=t Zτ, where Zt := w \u0012 λt+1 − λt + ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) \u0013 . Now we define the filtration Ft = {x0, ξ0, x1, . . . , ξt−1, xt} and compute the moment generating function (MGF) of Zt for any 0 ≤ t ≤ T − 1 E [exp(Zt)|Ft] = E \u0014 exp \u0012 w \u0012 λt+1 − λt + ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) \u0013\u0013 |Ft \u0015 (18) ≤ E \" exp   wρηt⟨ψt, ˆxt − xt⟩ + wρη2 t ∥ψt∥2 ∗ 2(1 − ηtℓ) − wηtρ(ρ − ℓ)Dω(ˆxt, xt) ! |Ft # = exp (−wηtρ(ρ − ℓ)Dω(ˆxt, xt)) E \" exp   wρηt⟨ψt, ˆxt − xt⟩ + wρη2 t ∥ψt∥2 ∗ 2(1 − ηtℓ) ! |Ft # (i) ≤ exp (−wηtρ(ρ − ℓ)Dω(ˆxt, xt)) exp \u0012 3σ2w2ρ2η2 t ∥ˆxt − xt∥2 + 3σ2wρη2 t 2(1 − ηtℓ) \u0013 (ii) ≤ exp \u0012 3σ2wρη2 t 2(1 − ηtℓ) \u0013 , where in (i) we apply Lemma F.2, which uses that ∥ψt∥∗ is σ-sub-Gaussian. Inequality (ii) holds by the fact that ∥ˆxt − xt∥2 ≤ 2Dω(ˆxt, xt), and the choice of w, which guarantess that 6σ2w2ρ2η2 t ≤ wηtρ(ρ − ℓ) for any t ≥ 0. Now to compute the MGF of St we use derive E [exp(St)|Ft] = E [E [exp(St+1 + Zt)|Ft+1] |Ft] = E [exp(Zt)E [exp(St+1)|Ft+1] |Ft] . Thus, by induction we have E [S0] ≤ exp   3σ2ρw 2 T −1 X t=0 η2 t (1 − ηtℓ) ! ≤ exp   3σ2ρw T −1 X t=0 η2 t ! . where the last inequality holds by the condition ηt ≤ 1/(2ℓ). Consequently, by Markov’s inequality, Pr   S0 ≥ 3σ2ρ T −1 X t=0 wtη2 t + log (1/β) ! ≤ β. Then with probability at least 1 − β, we have T −1 X t=0 w \u0012 λt+1 − λt + ηtρ 2(ρ + ℓ)Dρ+ℓ(xt) \u0013 ≤ 3σ2wρ T −1 X t=0 η2 t + log (1/β) . Ilyas Fatkhullin, Niao He Telescoping the above inequality, setting ρ = 4ℓ, and dividing by the sum of step-sizes: 1 PT −1 t=0 ηt T −1 X t=0 ηt D5ℓ(xt) ≤ λ0 + 1 w log (1/β) + 12σ2ℓ PT −1 t=0 η2 t 2 5 PT −1 t=0 ηt = λ0 + 8 η0σ2 log (1/β) + 12σ2ℓ PT −1 t=0 η2 t 2 5 PT −1 t=0 ηt . It remains to bound the Lyapunov function in the initial point setting η−1 = η0: λ0 = Φ1/ρ(x0) − Φ∗ + η−1ρ(Φ(x0) − Φ∗) ≤ Φ1/ρ(x0) − Φ∗ + 2(Φ(x0) − Φ∗). Taming Nonconvex Stochastic Mirror Descent D Proof of Theorem 4.7. Global Convergence under Generalized Proximal P L Condition Proof of Theorem 4.7. Invoking (17), and substituting the value of ρ = 2ℓ and using ηt ≤ 1 2ℓ, we derive under Assumption 4.6 Λt+1 ≤ Λt − ηt 3 E [D3ℓ(xt)] + 2ℓη2σ2 ≤ Λt − 2µηt 3 E h (Φ(xt) − Φ∗) 2/αi + 2ℓη2σ2, Notice that Φ(x) ≥ Φ1/ρ(x) for any x ∈ X, thus E [Φ(xt) − Φ∗] ≥ 1 1 + ηt−1ρE \u0002 Φ1/ρ(xt) − Φ∗\u0003 + ηt−1ρ 1 + ηt−1ρE [Φ(xt) − Φ∗] = Λt 1 + ηt−1ρ ≥ 1 2Λt. By Jensen’s inequality for z → z 2/α, we have E \u0002 (Φ(xt) − Φ∗) 2/α\u0003 ≥ (E [Φ(xt) − Φ∗]) 2/α. Combining the above inequalities, we get E \u0002 (Φ(xt) − Φ∗) 2/α\u0003 ≥ 1 2Λ 2/α t . Thus, we can derive a recursion Λt+1 ≤ Λt − ηtµ 3 Λ 2/α t + 2ℓσ2η2 t . Assume that for τ = 0, . . . , t, we have Λτ ≥ ε (otherwise we have reached ε-accuracy). Then Λt+1 ≤   1 − ηtµε 2−α α 3 ! Λt + 2ℓσ2η2 t , For any T ≥ 0, we select the step-size sequence ηt as follows: ηt =      1 2ℓ if t < ⌈T/2⌉ and T ≤ 6ℓ µ ε 2−α α , 6 µ ε 2−α α \u0012 t+ 12ℓ µ ε− 2−α α −⌈T/2⌉ \u0013 otherwise. By Lemma F.6, we have ΛT +1 = O   ℓΛ0 µ ε 2−α α exp   −µ ε 2−α α T ℓ ! + ℓσ2 Tµ2ε 2(2−α) α ! . Recalling the definition of Λt and using Lemma F.3, we bound ΛT +1 ≥ Φ1/ρ(xT +1) − Φ∗ ≥ Φ(x+ T +1) − Φ∗, where x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + ℓDω(y, x). The sample complexity to reach Φ(x+ T +1) − Φ∗ ≤ ε is T = O \u0012ℓΛ0 µ 1 ε 2−α α log \u0012ℓΛ0 µε \u0013 + ℓΛ0σ2 µ2 1 ε 4−α α \u0013 . Ilyas Fatkhullin, Niao He E Proofs for Applications E.1 Differentially Private Learning in ℓ2 and ℓ1 Settings Proof of Corollary 5.3. Notice that by Lemma F.7 ∥bt∥∞ is σ-sub-Gaussian r.v. with σ2 = 2 log(2d)σ2 G.20 We invoke the result of Theorem 4.5 with ηt = η0 = 1 2ℓ, and obtain 1 T T −1 X t=0 D5ℓ(xt) = O   λ0 η0T + σ2η0ℓ + σ2 log \u0010 1 β \u0011 T   = O \u0012ℓλ0 T + σ2 log \u0012 1 β \u0013\u0013 = O   ℓλ0 T + G2T log(d) log \u0000 1 δ \u0001 n2ϵ2 log \u0012 1 β \u0013! = O     G r ℓλ0 log(d) log \u0000 1 δ \u0001 log \u0010 1 β \u0011 nϵ     , where the last equality follows by the choice of T. It remains to notice that λ0 = Φ1/ρ(x0) − Φ∗ + 2(Φ(x0) − Φ∗) ≤ 3(Φ(x0) − Φ∗). Corollary E.1. Let F(·) be differentiable on a convex set X with L-Lipschitz continuous gradient w.r.t. Euclidean norm, and ∥∇F(x)∥2 ≤ G for all x ∈ X. Set ηt = 1 2L, T = nϵ √ L G√ d log(1/δ) log(1/β), λ0 := Φ(x0) − Φ∗. Then DP-Prox-GD is (ϵ, δ)-DP and with probability 1 − β satisfies 1 T T −1 X t=0 D5ℓ(xt) = O   G p ℓλ0d log (1/δ) log (1/β) nϵ ! , Proof. The proof follows the same lines as the proof of Corollary 5.3. The only difference is that instead of the infinity norm of the noise, we bound the Euclidean norm, i.e., ∥bt∥2 is σ-sub-Gaussian r.v. with σ2 = d σ2 G. E.2 Policy Optimization in Reinforecement Learning Prox-P L condition. Now we will verify Assumption 4.6 with α = 1 holds for our RL problem. The result is similar to Lemma 5 in [Xiao, 2022]. The only difference is that we have π instead of π+ on the left hand side of the inequality. Lemma E.2. Let ω(π) = 1 2 ∥π∥2 2,2. Then for any π ∈ X we have Vp(π) − V ⋆ p ≤ 2 p 2|S| 1 − γ \r\r\r\r dp (π⋆) µ \r\r\r\r ∞ q ∆+ ρ (π) if ρ ≥ GV,∥·∥2,2/DX,∥·∥2,2, where GV,∥·∥2,2 := maxπ∈Π h ∥∇Vp(π)∥2,2 i . Proof. It was shown in Lemma 4 in [Agarwal et al., 2021] that the following (variational) gradient domination condition holds. Vp(π) − V ⋆ p ≤ 1 1 − γ \r\r\r\r dp (π⋆) µ \r\r\r\r ∞ max π′∈X ⟨∇Vµ(π), π − π′⟩ for any π ∈ X. By Lemma F.5, we have max π′∈X ⟨∇Vµ(π), π − π′⟩ ≤ \u0010 DX,∥·∥2,2 + ρ−1GV,∥·∥2,2 \u0011 q ∆+ ρ (π) ≤ 2DX,∥·∥2,2 q ∆+ ρ (π) 20Here we used the fact that max1≤i≤d |ξi| = max {ξ1, −ξ1, . . . , ξd, −ξd} and applied Lemma F.7 with n = 2d. Taming Nonconvex Stochastic Mirror Descent where the last inequality holds for ρ ≥ GV,∥·∥2,2/DX,∥·∥2,2. It remains to notice that DX,∥·∥2,2 ≤ p 2|S|. Smoothness in (2, 1)-norm. Proof of Proposition 5.5 For any π, π′ ∈ X, it holds that ∥∇Vp(π) − ∇Vp (π′)∥2,∞ ≤ 2γ (1 − γ)3 ∥π − π′∥2,1 , The estimate of the smoothness constant follows directly from the proof of Lemma 54 in [Agarwal et al., 2021], since using (2, 1) norm we have P a∈A |ua,s| ≤ 1, and the perturbation ua,s belongs to the probability simplex ua,s ∈ ∆(A). E.3 Training Autoencoder Model using SMD Derivation of SMDr1 and SMDr2. Recall the choice of DGF from subsection 5.3 ω(x) = 1 r + 2∥x∥r+2 2 + 1 2∥x∥2 2. Notice that we have ∇ω(x) = ∥x∥r 2 x + x. The update rule of SMD with X = Rd, r(x) = 0 and the above choice of DGF satisfies ηt∇f(xt, ξt) + ∇ω(xt+1) − ∇ω(xt) = 0. Define ct := ∇ω(xt) − ηt∇f(xt, ξt) = xt − ηt∇f(xt, ξt) + ∥xt∥r 2 xt. Thus, it remains to solve for xt+1 ∇ω(xt+1) = (∥xt+1∥r 2 + 1) xt+1 = ct, (19) which is equivalent to solving the following simple univariate equation of θ ≥ 0: θr+1 + θ = ∥ct∥2 . (20) For r = 1, 2, it has an explicit form solution for any ∥ct∥2 . We have θ∗ = −1 + p 1 + 4 ∥ct∥2 2 for r = 1. and obtain the following method ct = xt − ηt∇f(xt, ξt) + ∥xt∥2 xt, xt+1 = 2ct 1 + p 1 + 4 ∥ct∥2 . For r = 2, an explicit form solution can be written using Cardano’s formula. We use Python Sympy library for symbolic calculation to solve for θ in this case. More generally, (19) implies for any r > 0, we have ct = (1 + ∥x∥r 2) xt − ηt∇f(xt, ξt), xt+1 = ct 1 + θr∗ , where θ∗ is the solution to (20), which can be solved using a bisection method up to the machine accuracy. Corollary E.3. Let F(·) : Rd → R be twice differentiable and satisfy (8). Let Assumption 3.2 hold with ∥·∥2. Suppose the sequence {ηt}t≥0 be non-increasing with η0 ≤ 1/(2ℓ), and ¯xT ∈ X be randomly chosen from the iterates x0, . . . , xT −1 with probabilities pt = ηt/ PT −1 t=0 ηt. Then for (9), (10), we have E h ∥∇F(¯xT )∥2 2 i ≤ 6(F(x0) − F ∗) + 6ℓσ2 PT −1 t=0 η2 t PT −1 t=0 ηt , where F ∗ := miny∈Rd F(y). Ilyas Fatkhullin, Niao He Additional experimental details. We use Fashion-MNIST dataset [Xiao et al., 2017] for training with images of dimensions df = 28 × 28 = 784. The encoding dimension is fixed to de = 64. The dataset is of size 50000 images. In all experiments, we use the mini-batch of size 100. We initialize the parameters W of the model with a normal distribution with mean 1 and the standard deviation 0.01. Remark E.4. A momentum variant of the scheme (9), (10) was recently explored in [Ding et al., 2023] with promising empirical results on image classification and language modeling tasks. We hope that our simpler variant without momentum can be also helpful in these tasks. Additional discussion about (L0, L1)-smoothness. Recently, some works, e.g., [Zhang et al., 2020a, Faw et al., 2023, H¨ubler et al., 2023], consider adaptive gradient methods such as gradient clipping, AdaGrad-Norm and gradient normalization under (L0, L1)-smoothness, i.e., F(·) is twice differentiable and for some L0, L1 ≥ 0 satisfies ∥∇F(x)∥op ≤ L0 + L1 ∥∇F(x)∥2 for all x ∈ Rd. The authors in [Zhang et al., 2020a, Faw et al., 2023] justify the theoretical benefits of the popular adaptive schemes by the fact that, unlike SGD, they provably work under this weaker (L0, L1)-smoothness. Moreover, Zhang et al. [2020a] empirically verify that (L0, L1)-smoothness condition holds on the optimization trajectory when training modern language and image classification models. Our polynomial grow condition is weaker than (L0, L1)-smoothness as long as the gradient norm grows at most as a polynomial in ∥x∥2. Unlike the approach taken in the above mentioned works, the convergence of our algorithm with the choice of DGF as in Proposition 5.9 follows directly from Theorem 4.3 and does not require a separate analysis. Taming Nonconvex Stochastic Mirror Descent F Useful Lemma The following lemma is standard [Lu et al., 2018] and the proof can be found, e.g., in [Chen and Teboulle, 1993]. Lemma F.1. 1. The Bregman divergence satisfies the three-point identity: Dω(x, y) + Dω(y, z) = Dω(x, z) + ⟨∇ω(z) − ∇ω(y), x − y⟩ for all y, z ∈ S and x ∈ cl(S). 2. Let ϕ(·) be a closed proper convex function on Rd, z ∈ S and z+ := arg minx∈X {ϕ(x) + ρDω(x, z)} for ρ > 0, then ϕ(x) + ρDω(x, z) ≥ ϕ(z+) + ρDω(z+, z) + ρDω(x, z+) for all x ∈ cl(S). To establish high probability convergence, we use the technical lemma by Liu et al. [2023]. Lemma F.2 (Lemma 2.2. in [Liu et al., 2023]). Suppose X ∈ Rd such that E[X] = 0 and ∥X∥∗ is a σ-sub-Gaussian random variable, then for any a ∈ Rd, 0 ≤ b ≤ 1 2σ, E \u0002 exp \u0000⟨a, X⟩ + b2∥X∥2 ∗ \u0001\u0003 ≤ exp \u00003 \u0000∥a∥2 + b2\u0001 σ2\u0001 . The following lemma shows the connection between Φ1/ρ and Φ. Similar result in the Euclidean setting has previously appeared, e.g., in [Stella et al., 2017]. Lemma F.3. Let F(·) be (ℓ, ω)-smooth. Then for any ρ ≥ 2ℓ and x ∈ X ∩ S we have Φ1/ρ(x) ≥ Φ(x+), where x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + (ρ − ℓ)Dω(y, x). Proof. By Assumption 3.1 (lower bound), we have for any x, y ∈ X ∩ S Φ(y) + ρDω(y, x) ≥ F(x) + ⟨∇F(x), y − x⟩ + r(y) + (ρ − ℓ)Dω(y, x). Minimizing both sides over y ∈ X ∩ S, we have Φ1/ρ(x) ≥ F(x) + ⟨∇F(x), x+ − x⟩ + r(x+) + (ρ − ℓ)Dω(x+, x) ≥ F(x+) + r(x+) + (ρ − 2ℓ)Dω(x+, x) ≥ Φ(x+), where the first equality holds by the definitions of Φ1/ρ and x+. The second inequality uses Assumption 3.1 (upper bound). The following lemma shows that our Assumption 4.6 is more general than relative strong convexity [Lu et al., 2018]. In the Euclidean case, the same result was derived by Karimi et al. [2016]. Lemma F.4 (Relative strong convexity implies 2-Bregman Prox-P L). Let F(·) be µ-relatively strongly convex w.r.t. ω(·), i.e., for all x, y ∈ X ∩ S F(y) ≥ F(x) + ⟨∇F(x), y − x⟩ + µDω(y, x). (21) Then Assumption 4.6 holds wth α = 2 and any ρ ≥ µ, i.e., Dρ(x) ≥ 2µ (Φ(x) − Φ∗) . Proof. Adding r(y) to both sides of (21), we have Φ(y) ≥ Φ(x) + ⟨∇F(x), y − x⟩ + µDω(y, x) + r(y) − r(x) = Φ(x) + Qµ(x, y). Minimizing both sides over y ∈ X ∩ S, we get Φ∗ ≥ Φ(x) + min y∈X Qµ(x, y) = Φ(x) − 1 2µDµ(x). Rearranging and noticing that Dµ(x) ≤ Dρ(x) for any x ∈ X and ρ ≥ µ, we obtain the result. Ilyas Fatkhullin, Niao He The following lemma connects the Frank-Wolfe gap with the norm of the gradient mapping in the Euclidean case. Lemma F.5 (Lemma 2.2 in [Balasubramanian and Ghadimi, 2022]). Let ω(x) := 1 2 ∥x∥2 2, X be a compact set with diameter DX,∥·∥2 := maxx,y∈X ∥x − y∥2 and r(·) = 0. Then for any ρ > 0 max y∈X ⟨∇F(x), x − y⟩ ≤ \u0000DX,∥·∥2 + ρ−1GF,∥·∥2 \u0001 q ∆+ ρ (x), where GF,∥·∥2 := maxx∈X ∥∇F(x)∥2. We report the special case of Lemma 3 by Stich [2019]. Lemma F.6 (Lemma 3 in [Stich, 2019]). Let {rt}t≥0 and {ηt}t≥0 be two non-negative sequences with ηt ≤ 1 d that satisfy the relation rt+1 ≤ (1 − aηt) rt + c η2 t , where a > 0, c ≥ 0. For any T ≥ 0, set ηt = ( 1 d if t < ⌈T/2⌉ and T ≤ 2d a , 1 a( 2d a +t−⌈T/2⌉) otherwise. Then we have rt+1 ≤ 32 d r0 a exp \u0012 −aT 2d \u0013 + 36 c a2T . The next lemma is standard and the proof can be found, e.g., in [Van Handel, 2014]. Lemma F.7 (Maximal tail inequality, Lemma 5.1 and 5.2 in [Van Handel, 2014]). Let ξi be a σ-sub-Gaussian random variable for every i = 1, . . . , n. Then \u0012 E \u0014 max 1≤i≤n ξi \u0015\u00132 ≤ E \u0014 max 1≤i≤n ξ2 i \u0015 ≤ 2σ2 log(n), Pr \u0012 max 1≤i≤n ξi ≥ p 2σ2 log(n) + λ \u0013 ≤ e− λ2 2σ2 for all λ ≥ 0. "
}