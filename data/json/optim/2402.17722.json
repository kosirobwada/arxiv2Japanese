{
    "optim": "Taming Nonconvex Stochastic Mirror Descent\nwith General Bregman Divergence\nIlyas Fatkhullin\nNiao He\nETH Z¨urich\nETH Z¨urich\nAbstract\nThis paper revisits the convergence of Stochas-\ntic Mirror Descent (SMD) in the contemporary\nnonconvex optimization setting. Existing re-\nsults for batch-free nonconvex SMD restrict\nthe choice of the distance generating func-\ntion (DGF) to be differentiable with Lipschitz\ncontinuous gradients, thereby excluding im-\nportant setups such as Shannon entropy. In\nthis work, we present a new convergence anal-\nysis of nonconvex SMD supporting general\nDGF, that overcomes the above limitations\nand relies solely on the standard assumptions.\nMoreover, our convergence is established with\nrespect to the Bregman Forward-Backward\nenvelope, which is a stronger measure than\nthe commonly used squared norm of gradient\nmapping. We further extend our results to\nguarantee high probability convergence under\nsub-Gaussian noise and global convergence un-\nder the generalized Bregman Proximal Polyak-\n Lojasiewicz condition. Additionally, we illus-\ntrate the advantages of our improved SMD\ntheory in various nonconvex machine learning\ntasks by harnessing nonsmooth DGFs. No-\ntably, in the context of nonconvex differen-\ntially private (DP) learning, our theory yields\na simple algorithm with a (nearly) dimension-\nindependent utility bound. For the problem\nof training linear neural networks, we develop\nprovably convergent stochastic algorithms.\n1\nINTRODUCTION\nWe consider stochastic composite optimization\nmin\nx∈X E [f(x, ξ)] + r(x),\n(1)\nProceedings of the 27th International Conference on Artifi-\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\nSpain. PMLR: Volume 238. Copyright 2024 by the au-\nthor(s).\nwhere F(x) := E [f(x, ξ)] is differentiable and (possi-\nbly) nonconvex, r(·) is convex, proper and lower-semi-\ncontinuous, X is a closed convex subset of Rd. The\nrandom variable (r.v.) ξ is distributed according to an\nunknown distribution P. We denote Φ := F + r and\nlet Φ∗ := infx∈X Φ(x) > −∞.\nA popular algorithm for solving (1) is Stochastic Mirror\nDescent (SMD), which has an update rule\nxt+1 = arg min\nx∈X\nηt(⟨∇f(xt, ξt), x⟩ + r(x)) + Dω(x, xt),\n(2)\nwhere Dω(x, y) is the Bregman divergence between\npoints x, y ∈ X induced by a distance generating\nfunction (DGF) ω(·); see Section 2 for the definitions.\nWhen r(·) = 0, X = Rd and ω(x) = 1\n2 ∥x∥2\n2, we have\nDω(x, y) = 1\n2 ∥x − y∥2\n2, and SMD reduces to the stan-\ndard Stochastic Gradient Descent (SGD). However, it is\noften useful to consider more general (non-Euclidean)\nDGFs.\nSMD with general DGF was originally proposed in the\npioneering work of Nemirovskij and Yudin [1979, 1983],\nand later found many fruitful applications [Ben-Tal\net al., 2001, Shalev-Shwartz, 2012, Arora et al., 2012]\nleveraging nonsmooth instances of DGFs. In the last\nfew decades, SMD has been extensively analyzed in the\nconvex setting under various assumption, e.g., [Beck\nand Teboulle, 2003, Lan, 2012, Allen-Zhu and Orecchia,\n2014, Birnbaum et al., 2011], including relative smooth-\nness [Lu et al., 2018, Bauschke et al., 2017, Dragomir\net al., 2021, Hanzely et al., 2021] and stochastic op-\ntimization [Lu, 2019, Nazin et al., 2019, Zhou et al.,\n2020b, Hanzely and Richt´arik, 2021, Vural et al., 2022,\nLiu et al., 2023, Nguyen et al., 2023]. However, despite\nthe vast theoretical progress, convergence analysis of\nnonconvex SMD with general DGF still remains elusive.\n1.1\nRelated Work\nWe now discuss the related work in the nonconvex\nstochastic setting.\nIn the unconstrained Euclidean\ncase, Ghadimi and Lan [2013] propose the first non-\nasymptotic analysis of nonconvex SGD. Later, Ghadimi\narXiv:2402.17722v1  [math.OC]  27 Feb 2024\nTaming Nonconvex Stochastic Mirror Descent\net al. [2016] consider the more general composite prob-\nlem (1) with arbitrary convex r(·), X, and propose a\nmodified algorithm using large mini-batches. Unfor-\ntunately, the use of large mini-batch appears to be\ncrucial in the proof proposed in [Ghadimi et al., 2016]\neven in Euclidean setting. Later, Davis and Drusvy-\natskiy [2019] address this issue by proposing a different\nanalysis for Prox-SGD (method (2) with ω(x) = 1\n2 ∥x∥2\n2).\nTheir elegant proof, using the notion of the so-called\nMoreau envelope, allows them to remove the large batch\nrequirement in the Euclidean setting. However, their\nanalysis crucially relies on the use of Euclidean geome-\ntry and appears difficult to extend to the more general\nnonsmooth DGFs of interest. In particular, the subse-\nquent works [Zhang and He, 2018, Davis et al., 2018]\ndo consider more general DGF and derive convergence\nrates for (2). However, both works assume a smooth\nDGF to justify their proposed convergence measures,\nsee our Section 4.2 for a more detailed comparison.\nAnother line of work uses momentum or variance re-\nduced estimators, e.g., [Zhang, 2021, Huang et al., 2022,\nFatkhullin et al., 2023c, Ding et al., 2023], but agian\ntheir analysis is limited to the Euclidean geometry.\n1.2\nContributions\n• In this work, we develop a new convergence analy-\nsis for SMD under the general assumptions of rela-\ntive smoothness and bounded variance of stochas-\ntic gradients. Importantly, unlike the prior work,\nour analysis naturally accommodates general non-\nsmooth DGFs, including the important case of\nShannon entropy.\nMoreover, our analysis (i)\nworks for any batch size, (ii) does not require\nthe bounded gradients assumption, (iii) supports\nany closed convex set X, and (iv) guarantees con-\nvergence on a strong stationarity measure – the\nBregman Forward-Backward envelope.\n• We further demonstrate the flexibility of our proof\ntechnique by extending it in two directions. First,\nwe perform a high probability analysis under the\nsub-Gaussian noise improving upon the previously\nknown rates under weaker assumptions. Next, we\nestablish the global convergence in the function\nvalue for SMD under the generalized version of\nthe Proximal Polyak- Lojasiewicz condition.\nIn\nboth cases, when specialized to the unconstrained\nEuclidean setup, our rates can recover the state-\nof-the-art bounds, up to small absolute constants.\n• Finally, we demonstrate the importance of our gen-\neral theory in various machine learning contexts,\nincluding differential privacy, policy optimization\nin reinforcement learning, and training deep lin-\near neural networks. For each of the considered\nproblems, our new SMD theory allows us to ei-\nther improve convergence rates or design provably\nconvergent stochastic algorithms. In all cases, we\nleverage nonsmooth DGFs to attain the result.\nOur Techniques.\nThe key idea of our analysis is\nthe use of a new Lyapunov function in the form of a\nweighted sum of the function value Φ(·) and its Breg-\nman Moreau envelope Φ1/ρ(·):\nλt := ηt−1ρ(Φ(xt) − Φ∗) + Φ1/ρ(xt) − Φ∗.\nWe recall that the classic analysis of (large batch) SMD\nin [Ghadimi et al., 2016] uses the function value as\na Lyapunov function, i.e., λt,1 := Φ(xt) − Φ∗. While\nthis approach is very intuitive and matches with anal-\nysis in unconstrained case, it seems very difficult to\ngeneralize to more general constrained problem (1)\neven in the Euclidean setting. On the other hand, the\nanalysis pioneered in [Davis and Drusvyatskiy, 2019]\nuses λt,2 := Φ1/ρ(xt) − Φ∗ as a Lyapunov function,\nwhich does not seem straightforward to extend into\nnon-Euclidean setups, unless the smoothness of DGF\nis additionally imposed. Our Lyapunov function con-\ntains a weighted average of the above two quantities,\ni.e., λt = ηt−1ρλt,1 + λt,2, where {ηt}t≥0 is the step-\nsize sequence (with η−1 = η0), ρ > 0. This modified\nLyapunov function allows to better utilize (relative)\nsmoothness of F(·) in the analysis. Namely, both upper\nand lower bound inequalities in Assumption 3.1 will be\nused in the proof.\n2\nPRELIMINARIES\nWe fix an arbitrary norm ∥·∥ defined on X ⊂ Rd,\nand denote by ∥·∥∗ := supz:∥z∥≤1⟨·, z⟩ its dual. The\nEuclidean norm is denoted by ∥·∥2.\nWe denote by\nδX the indicator function of a convex set X, i.e.,\nδX (x) = 0 if x ∈ X and +∞ otherwise.\nFor a\nclosed proper function Φ : Rd → R ∪ {+∞} with\ndom Φ :=\n\b\nx ∈ Rd | Φ(x) < +∞\n\t\n, the Fr´echet subd-\nifferential at a point x ∈ Rd is denoted by ∂Φ(x)\nand is defined as a set of points g ∈ Rd such that\nΦ(y) ≥ Φ(x) + ⟨g, y − x⟩ + o(∥y − x∥), ∀y ∈ Rd if\nx ∈ dom Φ. We set ∂Φ(x) = ∅ if x /∈ dom Φ [Davis and\nGrimmer, 2019].1 We denote by cl(X) and ri(X) the\nclosure and the relative interior of X respectively.\nLet S ⊂ Rd be an open set and ω : cl(S) → R be\ncontinuously differentiable on S. Then we say that ω(·)\nis a distance generating function (DGF) (with zone S)\nif it is 1-strongly convex w.r.t. ∥·∥ on cl(S). We assume\nthroughout that S is chosen such that ri(X) ⊂ S [Chen\n1When Φ = δX , the function is convex and ∂δX (x) coin-\ncides with the usual subdifferential in the convex analysis.\nIlyas Fatkhullin, Niao He\nand Teboulle, 1993].2 For simplicity, we let dom r = Rd.\nThe Bregman divergence [Bregman, 1967] induced by\nω(·) is\nDω(x, y) := ω(x)−ω(y)−⟨∇ω(y), x−y⟩\nfor x, y ∈ S.\nWe denote by Dsym\nω\n(x, y) := Dω(x, y) + Dω(y, x) a\nsymmetrized Bregman divergence.\nFor any Φ : Rd → R ∪ {+∞} and a real ρ > 0, the\nBregman Moreau envelope and the proximal operator\nare defined respectively by\nΦ1/ρ(x) := min\ny∈X [Φ(y) + ρDω(y, x)] ,\nproxΦ/ρ(x) := arg min\ny∈X\n[Φ(y) + ρDω(y, x)] .\nA point x ∈ X ∩ S is called a first-order stationary\npoint (FOSP) of (1) if 0 ∈ ∂(Φ+δX )(x) for Φ := F +r.\n2.1\nFOSP Measures\nWe define three different measures of first-order sta-\ntionarity for a candidate solution x ∈ X ∩ S.\n(i) Bregman Proximal Mapping (BPM)\n∆ρ(x) := ρ2Dsym\nω\n(ˆx, x),\nˆx := proxΦ/ρ(x).\n(ii) Bregman Gradient Mapping (BGM)\n∆+\nρ (x) := ρ2Dsym\nω\n(x+, x),\nx+ := arg min\ny∈X\n⟨∇F(x), y⟩ + r(y) + ρDω(y, x).\n(iii) Bregman Forward-Backward Envelope (BFBE)\nDρ(x) := −2ρ min\ny∈X Qρ(x, y),\nQρ(x, y) := ⟨∇F(x), y − x⟩ + ρDω(y, x) + r(y) − r(x).\nIn unconstrained Euclidean case, i.e., X = Rd, r(·) = 0\nand ω(x) =\n1\n2 ∥x∥2\n2, we have ∆+\nρ (x) = Dρ(x) =\n∥∇F(x)∥2\n2, which is the standard stationarity measure\nin non-convex optimization.3 Note that all three quan-\ntities presented above are measures of FOSP in the\nsense that if one of them ∆ρ(x) , ∆+\nρ (x) or Dρ(x) is\n2Following Chen and Teboulle [1993], we can verify that,\nin the Euclidean setup (ω(x) = 1\n2 ∥x∥2\n2), one can set S = Rd;\nin the simplex setup (ω(x) = Pd\ni=1 x(i) log x(i)), the choice\nS =\nn\nx ∈ Rd | x(i) > 0 for all i ∈ [d]\no\nis suitable.\n3This is, however, not true for BPM, which reduces\nto the gradient norm of a surrogate loss, i.e., ∆ρ(x) =\n\r\r∇F1/ρ(x)\n\r\r2\n2 for ρ > ℓ, where ℓ is a smoothness constant\nof F(·).\nzero for some x ∈ X ∩ S, then 0 ∈ ∂(Φ + δX )(x). How-\never, it is more practical to understand what happens\nif one of them is only ε-close to zero. In Section 4.1\nwe establish the connections between these quantities\nfor any x ∈ X ∩ S, and find that BFBE, Dρ(x), is the\nstrongest among the three. We should mention that\nthe use of BFBE is not new for the analysis of opti-\nmization methods. In the Euclidean case, BFBE was\ninitially proposed in [Patrinos and Bemporad, 2013],\nand its properties were later analyzed in [Stella et al.,\n2017, Liu and Pong, 2017]. Later, Ahookhosh et al.\n[2021] consider BFBE in general non-Euclidean setting.\nHowever, to our knowledge it was not considered in\nthe context of stochastic even in the Euclidean setup.\n3\nASSUMPTIONS\nThroughout the paper we make the following basic\nassumptions on F(·) and the stochastic gradients.\nAssumption 3.1 (Relative smoothness [Bauschke\net al., 2017, Lu et al., 2018]). A differentiable function\nF : X ∩ S → R is said to be ℓ-relatively smooth on\nX ∩S with respect to (w.r.t.) ω(·) if for all x, y ∈ X ∩S\n−ℓDω(x, y) ≤ F(x)−F(y)−⟨∇F(y), x−y⟩ ≤ ℓDω(x, y).\nWe denote such class of functions as (ℓ, ω)-smooth.\nIt\nis\nknown\nthat\nsmoothness\nw.r.t. ∥·∥,\ni.e.,\n∥∇F(x) − ∇F(y)∥∗ ≤ ℓ ∥x − y∥ for all x, y ∈ X ∩ S,\nimplies Assumption 3.1 [Nesterov, 2018].\nAssumption 3.2. We have access to a stochastic or-\nacle that outputs a random vector ∇f(x, ξ) for any\ngiven x ∈ X, such that E [∇f(x, ξ)] = ∇F(x),\nE\nh\n∥∇f(x, ξ) − ∇F(x)∥2\n∗\ni\n≤ σ2,\nwhere the expectation is taken w.r.t. ξ ∼ P.\n4\nMAIN RESULTS\n4.1\nConnections between FOSP Measures\nWe start by establishing the connections between intro-\nduced convergence measures. It turns out that BPM\nand BGM are essentially equivalent, i.e., differ only by\na small (absolute) multiplicative constant.\nLemma 4.1 (BPM ≈ BGM). Let F(·) be (ℓ, ω)-smooth\nand\np\nDsym\nω\n(x, y) be a metric. Then for any x ∈ X ∩S,\nand ρ, s > 0 such that ρ > ℓ/s + 2ℓ, it holds\n∆ρ(x)\nC(ℓ, ρ, s) ≤ ∆+\nρ (x) ≤ C(ℓ, ρ, s)∆ρ(x),\nTaming Nonconvex Stochastic Mirror Descent\nwhere C(ℓ, ρ, s) := (1+s)(ρ−ℓ)+(1+s−1)ℓ\nρ−ℓ−(1+s−1)ℓ\n. In particular,\nfor s = 1, ρ = 4ℓ, we have C(ℓ, ρ, s) = 8, and\n1\n8∆4ℓ(x) ≤ ∆+\n4ℓ(x) ≤ 8∆4ℓ(x).\nThis result is in a similar spirit to Theorem 4.5 in\n[Drusvyatskiy and Paquette, 2019].\nHowever, their\nproof only works in the Euclidean setting and does\nnot readily extend to other DGFs. Our proof is dif-\nferent and can accommodate a possibly nonsmooth\nDGF.4 Next, we examine the relation between BGM\nand BFBE.\nLemma 4.2 (BFBE > BGM). For any x ∈ X ∩ S\n2Dρ/2(x) ≥ ∆+\nρ (x).\nThere is an instance of problem (1) with ℓ = 1, X =\n[0, 1] and arg miny∈X Φ(x) = 0 such that for any ρ ∈\n[1, 2], ρ1 ≥ 1 and x ∈ (0, 1] it holds\nDρ(x)\n∆+\nρ1(x) ≥ 2\n|x|.\nThe above lemma implies that BFBE is a strictly\nstronger convergence measure than previously consid-\nered BGM and BPM. Moreover, the difference between\nBFBE and BGM can be arbitrarily large even when\nx is close to the optimum! This effect is actually very\ncommon and happens already in the Euclidean case\nwith classical regularizer r(x) = ∥x∥1. The explanation\nfor this phenomenon is simple. Notice that BGM is\ndefined in the primal terms, i.e., the squared distance\nbetween x and x+, while BFBE is defined in the func-\ntional terms (the minimum value of Qρ(x, y) over y).\nTherefore, BFBE unlike BGM scales with the value of\nr(x) = |x| rather than x2.\nWe conclude from Lemma 4.1 and 4.2 that Dρ(x) is the\nstrongest convergence measure among the three. In the\nsubsequent sections we aim to establish convergence\nof SMD directly w.r.t. BFBE instead of using BGM or\nBPM.\n4.2\nConvergence to FOSP in Expectation\nWe start with our key result, which establishes conver-\ngence of SMD in expectation.\n4Unfortunately, it is unclear if the above result holds for\narbitrary ω(·) that does not induce a metric. Note that, in\ngeneral, DGF might not induce a metric even for popular\nchoices of ω(·). For instance, the Shannon entropy induces\np\nDsym\nω\n(x, y) that does not satisfy the triangle inequality,\nsee, e.g., Theorem 3 in [Acharyya et al., 2013] for details.\nTheorem 4.3. Let Assumptions 3.1 and 3.2\nhold. Let the sequence {ηt}t≥0 be non-increasing\nwith η0 ≤ 1/(2ℓ), and ¯xT be randomly chosen\nfrom the iterates x0, . . . , xT −1 with probabilities\npt = ηt/ PT −1\nt=0 ηt. Then\nE [D3ℓ(¯xT )] ≤ 3λ0 + 6ℓσ2 PT −1\nt=0 η2\nt\nPT −1\nt=0 ηt\n,\n(3)\nwhere λ0 := Φ1/ρ(x0) − Φ∗ + Φ(x0) − Φ∗. If we\nset constant step-size ηt = min\n\u001a\n1\n2ℓ,\nq\nλ0\nσ2ℓT\n\u001b\n,\nthen\nE [D3ℓ(¯xT )] = O\n \nℓλ0\nT\n+\nr\nσ2ℓλ0\nT\n!\n.\nProof sketch: We start with\nStep I. Deterministic descent w.r.t. BFBE. We\nshow that for any ρ1 ≥ ρ + ℓ\nΦ1/ρ(x) ≤ Φ (x) −\n1\n2ρ1\nDρ1(x).\nThis inequality corresponds to deterministic descent\non Φ(·) of the Bregman Proximal Point Method. It\nwill be useful in the next step to derive a recursion on\nDρ1(x).\nStep II. One step progress on the Lyapunov\nfunction. This step is the most technical one and\nconsists of showing a progress on a carefully chosen\nLyapunov function λt := Φ1/ρ(xt)−Φ∗+ηt−1ρ(Φ(xt)−\nΦ∗), where η−1 = η0, ρ > 0:\nλt+1 ≤ λt −\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩\n+ ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)),\nwhere ψt := ∇f(xt, ξt) − ∇F(xt).\nStep III. Dealing with stochastic terms. The goal\nof this step is to control the stochastic terms in the\nabove inequality using Assumption 3.2.\nIt remains to telescope and set the step-sizes to derive\nthe final result.\nWhen specialized to the unconstrained Euclidean set-\nting, the result of Theorem 4.3 recovers (up to a small\nabsolute constant) previously established convergence\nbounds for SGD [Ghadimi and Lan, 2013] (since in this\ncase we have D3ℓ(¯xT ) = ∥∇F(¯xT )∥2\n2), which is known\nto be optimal [Arjevani et al., 2023, Drori and Shamir,\n2020, Yang et al., 2023]. However, already in the com-\nposite setting (when r(·) ̸= 0), our result is stronger\nIlyas Fatkhullin, Niao He\nthan previously derived bounds for Prox-SGD [Davis\net al., 2018] because BFBE can be much larger than\nBPM/BGM even in the Euclidean case as we have seen\nin Lemma 4.2.\nIn the more general non-Euclidean case, compared to\nTheorem 2 in [Ghadimi et al., 2016], our method does\nnot require using large batches, and our proof works for\nany batch size. Moreover, [Ghadimi et al., 2016] relies\non the stronger assumptions: smoothness and bounded\nvariance in the primal norm. Furthermore, a much\nweaker convergence measure is used in [Ghadimi et al.,\n2016]: the squared norm of the difference between xt\nand x+\nt .5\nDavis et al. [2018] derive convergence of SMD w.r.t. the\nBregman divergence between ˆxt and xt, i.e., Dω(ˆxt, xt).\nSuch convergence measure is not satisfactory for two\nreasons. First, for a general DGF of interest, the Breg-\nman divergence is not symmetric, and it can happen\nthat Dω(ˆxt, xt) vanishes, while Dω(xt, ˆxt) does not (see,\ne.g., Proposition 2 in [Bauschke et al., 2017]). Second,\nto justify this measure the authors in [Davis et al.,\n2018] assume ω(·) to be twice differentiable and no-\ntice that 2ρ2Dω(ˆxt, xt) ≥\n\r\r(∇2ω(xt))−1∇Φ1/ρ(xt)\n\r\r2\n∗,\nwhere ∇Φ1/ρ(·) is the gradient of the Moreau envelope\nof Φ(·). However, the latter measure also does not\nseem to be sufficient either: even if we additionally as-\nsume the uniform smoothness of ω(·), it is unclear how\n∇Φ1/ρ(·) is connected to the standard convergence mea-\nsures such as the gradient mapping in non-Euclidean\nsetting. In the concurrent work to [Davis et al., 2018],\nZhang and He [2018] derive convergence of SMD on\nthe BPM. They also notice that if ω(·) is differentiable\nand smooth (i.e., ∇ω(·) is M-Lipschitz continuous) on\nX, then dist2(0, ∂(Φ + δX )(ˆxt)) ≤ M∆ρ(xt). However,\nwe argue that such assumption is very strong since\ncommonly used DGFs such as Shannon entropy are\nnot smooth.\nMoreover, the analysis in [Zhang and\nHe, 2018] uses bounded gradients (BG) assumption,\nwhich fails to hold even for a quadratic function if X\nis unbounded.6\n4.3\nHigh Probability Convergence to FOSP\nunder Sub-Gaussian Noise\nWhile convergence in expectation for a randomly se-\nlected point ¯xT is classical and widely accepted in\nstochastic optimization, it does not necessarily guaran-\ntee convergence for a single run of the method. In this\nsection, we extend our Theorem 4.3 to guarantee con-\n5Notice that ρ2 \r\rx − x+\r\r2 ≤ ∆+\nρ (x) ≤ 2Dρ/2(x), where\nthe first inequality holds by strong convexity of ω(·), and\nthe second is due to Lemma 4.2.\n6Not saying about the general relatively smooth func-\ntions, for which BG can fail even on a compact domain.\nvergence for a single run of SMD with high probability.\nTo obtain high probability bounds, we replace our As-\nsumption 3.2 with the following commonly used “light\ntail” assumption on the stochastic noise distribution.\nAssumption 4.4. We have access to a stochastic or-\nacle that outputs a random vector ∇f(x, ξ) for any\ngiven x ∈ X, such that E [∇f(x, ξ)] = ∇F(x), and\n∥∇f(x, ξ) − ∇F(x)∥∗\nis σ-sub-Gaussian r.v. 7\nTheorem 4.5. Let Assumptions 3.1 and 4.4\nhold. Let the sequence {ηt}t≥0 be non-increasing\nwith η0 ≤ 1/(2ℓ). Then with probability at least\n1 − β\n1\nPT −1\nt=0 ηt\nT −1\nX\nt=0\nηt D5ℓ(xt) ≤ 5eλ0 + 60σ2ℓ PT −1\nt=0 η2\nt\n2 PT −1\nt=0 ηt\n,\nwhere eλ0 := 3 (Φ(x0) − Φ∗) + 8 η0σ2 log (1/β) .\nTo our knowledge, the above theorem is the first high\nprobability bound for nonconvex SMD without use\nof large batches. If we use large mini-batch,8 then\nthe above theorem implies O\n\u0010\n1\nε2 + σ2\nε2 log (1/β) + σ2\nε4\n\u0011\nsample complexity to ensure min0≤t≤T −1 D5ℓ(xt) ≤\nε2.\nCompared to the bound derived in [Ghadimi\net al., 2016], which is O\n\u0010\n1\nε2 log (1/β) + σ2\nε4 log (1/β)\n\u0011\n9,\nour sample complexity is better by a factor of log (1/β).\nMoreover, our Assumptions 3.1 and 4.4 are weaker\nthan in [Ghadimi et al., 2016]. When specialized to\nthe Euclidean setup and setting the specific step-size\nsequences, our Theorem 4.5 can recover (up to an abso-\nlute constant) recently derived high probability bounds\nfor nonconvex SGD [Liu et al., 2023]. However, unlike\n[Liu et al., 2023], our theorem holds for any square\nsummable step-sizes and accommodates more general\n(non-Euclidean) norm in Assumption 4.4.\nWe will\ndemonstrate the crucial benefit of using non-Euclidean\nsetup later in Section 5.1.\n4.4\nGlobal Convergence under Generalized\nProximal P L condition\nIn this subsection, we are interested in global conver-\ngence of SMD for structured nonconvex problems. We\nfirst introduce the following generalization of Proximal\nPolyak-Lojasiewicz (Prox-P L) condition [Polyak, 1963,\nLojasiewicz, 1963, Lezanski, 1963].\n7A random variable X is called σ-sub-Gaussian if\nE\n\u0002\nexp(λ2X2)\n\u0003\n≤ exp(λ2σ2) for all λ ∈ R with |λ| ≤ 1/σ.\n8Which reduces σ2 to σ2/B for mini-batch of size B.\n9Discarding the samples for post-proccesing step in equa-\ntion (71) therin.\nTaming Nonconvex Stochastic Mirror Descent\nAssumption 4.6 (α-Bregman Prox-P L). There exists\nα ∈ [1, 2] and µ > 0 such that for some ρ ≥ 3ℓ and all\nx ∈ X ∩ S\nDρ(x) ≥ 2µ(Φ(x) − Φ∗)\n2/α.\n(4)\nThe above assumption generalizes Prox-P L condition\nstudied in [Karimi et al., 2016, J Reddi et al., 2016,\nLi and Li, 2018] in two ways. First, we have Dρ(x)\ndefined w.r.t. an arbitrary non-Euclidean DGF. Sec-\nond, we consider α ∈ [1, 2] instead of fixing α = 2.\nWe will demonstrate later in Section 5.2 that both of\nthese generalizations are important in some nonconvex\nproblems and the flexibility of choosing ω(·) can reduce\nthe total sample complexity. We now state the global\nconvergence of SMD.\nTheorem 4.7. Let Assumptions 3.1, 3.2 and\n4.6 hold. For any ε > 0, there exists a choice\nof step-sizes {ηt}t≥0 for method (2) such that\nmint≤T E\n\u0002\nΦ(x+\nt ) − Φ∗\u0003\n≤ ε after\nT = O\n\u0012ℓΛ0\nµ\n1\nε\n2−α\nα\nlog\n\u0012ℓΛ0\nµε\n\u0013\n+ ℓΛ0σ2\nµ2\n1\nε\n4−α\nα\n\u0013\n.\nThe above result implies that after at most T iterations\nSMD will find a point xt which is one Mirror Descent\nstep away from a point that is ε-close to Φ∗ in the\nfunction value. In the unconstrained Euclidean setting,\nthe above sample complexity matches with that of SGD\n[Fatkhullin et al., 2022].10 In the special case α = 2,\nit implies the linear convergence rate in deterministic\ncase and O\n\u0000ε−1\u0001\nsample complexity in the stochastic\ncase. The linear convergence and O\n\u0000ε−1\u0001\nsample com-\nplexity of SMD were previously shown under relative\nsmoothness and relative strong convexity, e.g., in [Lu\net al., 2018, Hanzely and Richt´arik, 2021]. Our result\nunder Assumption 4.6 is more general since the rela-\ntive strong convexity of F(·) implies (4) with α = 2,\nsee Lemma F.4. It is also known that such rates are\noptimal for α = 2 in the Euclidean setting [Yue et al.,\n2023, Agarwal et al., 2009].\n5\nNEW INSIGHTS FOR MACHINE\nLEARNING\nIn this section, we dive into the context of several ma-\nchine learning applications. We illustrate how each\nof our Theorems 4.3, 4.5 and 4.7 can be applied to\n10We use a different step-size sequence {ηt}t≥0 compared\nto ηt = 1/tζ, ζ > 0 used in [Fatkhullin et al., 2022], see\nAppendix D. This allows us to derive noise adaptive rates,\ni.e., if σ = 0, then we recover the iteration complexity of\n(deterministic) mirror descent.\nspecific problems; either yielding faster convergence\nthan existing algorithms or allowing us to design prov-\nably convergent schemes. Interestingly, the presented\nproblems are very diverse and allow us to demonstrate\ndifferent aspects of our assumptions. In all presented\nexamples, we crucially rely on the choice of nonsmooth\nDGFs, which was not theoretically possible to handle\nin the prior work on SMD.\n5.1\nDP Learning in ℓ2 and ℓ1 Settings\nIn differentially private (DP) stochastic nonconvex op-\ntimization, the goal is to design a private algorithm\nto minimize the population loss of type (1) over a\nsubset of a d-dimensional space given n i.i.d. samples,\nξ1, . . . , ξn, drawn from a distribution P.\nDenote\nby S :=\n\b\nξ1, . . . , ξn\t\n, the sampled dataset, and by\n∇F(x) := Pn\ni=1 ∇f(x, ξi), the gradient of the empiri-\ncal loss F(x) := Pn\ni=1 f(x, ξi) based on dataset S. The\nclassical notion to quantify the privacy quality is\nDefinition 5.1 ((ϵ, δ)-DP [Dwork et al., 2006]). A\nrandomized algorithm M is (ϵ, δ)-differentially private\nif for any pair of datasets S, S′ that differ in exactly\none data point and for any event Y ⊆ Range(M) in\nthe output range of M, we have\nPr (M(S) ∈ Y) ≤ eϵ Pr (M (S′) ∈ Y) + δ,\nwhere the probability is w.r.t. the randomness of M.\nThere are several common techniques to ensure privacy,\nwhich include output [Wu et al., 2017, Zhang et al.,\n2017], objective function [Chaudhuri et al., 2011, Kifer\net al., 2012, Iyengar et al., 2019] or gradient perturba-\ntions [Bassily et al., 2014, Wang et al., 2017]. Most\nrecent works on nonconvex DP learning focus on the\nlatter approach. The key idea of gradient perturbation\nis to inject an artificial Gaussian noise bt ∼ N(0, σ2\nGId)\ninto the evaluated gradient. The parameter σ2\nG should\nbe carefully chosen to ensure privacy, which can be\nguaranteed by the moments accountant\nLemma 5.2 (Theorem 1 in [Abadi et al., 2016]). As-\nsume that ∥∇F(x)∥2 ≤ G for all x ∈ X. There exist\nconstants c1, c2 > 0 so that given the number of iter-\nations T ≥ 0, for any ϵ ≤ c1T , the gradient method\nusing ∇F(xt)+bt, bt ∼ N(0, σ2\nGId) as the gradient esti-\nmator is (ϵ, δ)-DP for any δ > 0 if σ2\nG ≥ c2\nG2T log(1/δ)\nn2ϵ2\n.\nℓ2 Setting. For instance, the DP-Prox-GD iterates\nxt+1 = proxηtr(xt−ηt(∇F(xt)+bt)), bt ∼ N(0, σ2\nGId),\nwhere proxηtr(x) := arg miny∈X\n\u0010\nr(y) +\n1\n2ηt ∥y − x∥2\n2\n\u0011\n.\nOur Theorem 4.5 immediately implies the high proba-\nbility utility bound for DP-Prox-GD:\n1\nT\nT −1\nX\nt=0\nE [D5ℓ(xt)] = O\n p\nd log (1/δ) log(1/β)\nnϵ\n!\n, (5)\nIlyas Fatkhullin, Niao He\nwhere β ∈ (0, 1) is the failure probability, see Corol-\nlary E.1 for more details and the dependence on omitted\nconstants. To our knowledge, nonconvex utility bound\nof DP-Prox-GD was previously studied only in the un-\nconstrained setting (r(·) = 0, X = Rd), e.g., [Wang\net al., 2017, 2019, Zhou et al., 2020a] or in expectation,\ne.g., [Wang and Xu, 2019]. Our bound (5) generalizes\nthese works to non-trivial X and r(·).\nℓ1 Setting. One issue with the above utility bound\nis the polynomial dimension dependence. In certain\ncases, this dependence can be significantly improved,\ne.g., when the optimization is defined on a unit simplex\nX =\nn\nx ∈ Rd| Pd\ni=1 x(i) ≤ 1, x(i) ≥ 0\no\n.\nNotably, it\nmakes a big difference which norm we use to mea-\nsure the variance of bt, e.g., E ∥bt∥2\n2 = d σ2\nG and\nE ∥bt∥2\n∞ ≤ 2 log(d) σ2\nG. Therefore, using ∥·∥∞ norm\nis more favorable. Motivated by this difference, we con-\nsider the differentially private mirror descent (DP-MD):\nxt+1 = arg min\ny∈X\nηt(⟨∇F(xt)+bt, y⟩+r(y))+Dω(y, xt),\nwhere bt ∼ N(0, σ2\nGId) and ω(x) = Pd\ni=1 x(i) log x(i).11\nUsing our high probability guarantee Theorem 4.5, we\ncan derive\nCorollary 5.3. Let F(·) be (ℓ, ω)-smooth for ω(·), X\ndefined above, and ∥∇F(x)∥2 ≤ G for all x ∈ X. Set\nηt =\n1\n2ℓ, T =\nnϵ\n√\nℓ\nG√\nlog(d) log(1/δ) log(1/β), λ0 := Φ(x0) − Φ∗.\nThen DP-MD is (ϵ, δ)-DP and with probability 1 − β\nsatisfies 12\n1\nT\nT −1\nX\nt=0\nD5ℓ(xt) = O\n \nG\np\nℓλ0 log(d) log (1/δ) log (1/β)\nnϵ\n!\n,\nThe above result establishes a (nearly) dimension inde-\npendent utility bound for DP-MD, and improves the one\nof DP-Prox-GD in (5) by a factor of\np\nd/log(d). Several\nprevious works in DP learning literature have shown the\nimproved dimension dependence in ℓ1 setting, e.g., [Asi\net al., 2021, Gopi et al., 2023, Bassily et al., 2021b,a,\nWang and Xu, 2019]. However, Asi et al. [2021], Gopi\net al. [2023], Bassily et al. [2021b] assume convex F(·),\nand, therefore, are not directly comparable with our\nresult. Bassily et al. [2021a], and Wang and Xu [2019]\nobtain nonconvex utility bounds in expectation, how-\never, their techniques are different. Both above men-\ntioned works rely on the linear minimization oracle and\nderive convergence on the Frank-Wolfe (FW) gap.13\n11It is known that such ω(·) is 1-strongly convex w.r.t. ∥·∥1\non a unit simplex [Beck and Teboulle, 2003].\n12The result can be easily extended to the case when only\nstochastic gradients ∇f(xt, ξi\nt) are used instead of ∇F(xt).\n13At least when restricted to Euclidean setting, FW gap is\na weaker convergence measure than BFBE, see Lemma 4.2\nand F.5.\nMoreover, Bassily et al. [2021a] use a complicated dou-\nble loop algorithm based on momentum-based variance\nreduction technique.\n5.2\nPolicy Optimization in Reinforcement\nLearning (RL)\nConsider\na\ndiscounted\nMarkov\ndecision\nprocess\n(DMDP) M = {S, A, P, R, γ, p}.\nHere S is a state\nspace with cardinality |S|; A is an action space with\ncardinality |A|; P is a transition model, where P(s′|s, a)\nis the transition probability to state s′ from a given\nstate s when action a is applied; R : S × A → [0, 1] is a\nreward function for a state-action pair (s, a); γ ∈ [0, 1)\nis the discount factor; and p is the initial state dis-\ntribution. Being at state sh ∈ S an RL agent takes\nan action ah ∈ A and transitions to another state\nsh+1 according to P and receives an immediate re-\nward rh ∼ R(sh, ah). A (stationary) policy π speci-\nfies a (randomized) decision rule depending only on\nthe current state sh, i.e., for each s ∈ S, πs ∈ ∆(A)\ndetermines the next action a ∼ πs, where ∆(A) :=\n\b\nπs ∈ R|A|| P\ns∈S πsa = 1, πsa ≥ 0 for all a ∈ A\n\t\nde-\nnotes the probability simplex supported on A. The\ngoal of RL agent is to maximize\nV +\np (π) := E\n\" ∞\nX\nh=0\nγhrh\n#\n,\nπ ∈ X := ∆(A)|S|,\n(6)\nwhere expectation is w.r.t. the initial state distribution\ns0 ∼ p, the transition model P and the policy π. We\ndefine Vp(π) := −V +\np (π) and adopt the minimization\nformulation of DMDP, i.e., minπ∈X Vp(π).\nIt is known that Vp(π) is smooth, but nonconvex in π.\nMoreover, a property similar to Proximal P L (Assump-\ntion 4.6) was recently established for (6) [Agarwal et al.,\n2021, Xiao, 2022]. That is we have for any π, π′ ∈ X:\n∥∇Vp(π) − ∇Vp (π′)∥2,2 ≤ LF ∥π − π′∥2,2 ,\nVp(π) − V ⋆\np ≤ C max\nπ′∈X ⟨∇Vµ(π), π − π′⟩ ,\n(7)\nwhere LF :=\n2γ|A|\n(1−γ)3 , C :=\n1\n1−γ\n\r\r\r dp(π⋆)\nµ\n\r\r\r\n∞, ∥·∥2,2 de-\nnotes the Frobenius norm (Lemma 4 and 54 in [Agarwal\net al., 2021]).\nTherefore, this problem serves well to demonstrate\nthe application of our theory to show convergence of\npolicy gradient (PG) methods. PG methods is the\npromising class of algorithms that generate a sequence\nof policies πt by evaluating the gradients ∇Vµ(πt) (or\ntheir stochastic estimates b∇Vµ(πt)), where µ ∈ ∆(A)\nis some distribution (not necessarily equal to p). One\nof the most basic variants is the\nProjected Stochastic Policy Gradient:\nP-SPG:\nπt+1 = projX\n\u0010\nπt − ηt b∇Vµ(πt)\n\u0011\n,\nTaming Nonconvex Stochastic Mirror Descent\nwhere projX (·) denotes the Euclidean projection onto\nX. Given that the variance of stochastic gradients\nb∇Vµ(πt) is bounded in the Euclidean norm by σ2\nF ,14\nour Theorems 4.3 and 4.7 imply the following\nCorollary 5.4. For any ε > 0, P-SPG guarantees:\n(i) min0≤t≤T −1 E [D3LF (πt)] ≤ ε2 after\nT = O\n\u0012\n|A|\n(1 − γ)3ε2 +\nσ2\nF |A|\n(1 − γ)3ε4\n\u0013\n,\n(ii) mint≤T E\n\u0002\nVp(π+\nt ) − V ∗\np\n\u0003\n≤ ε after\nT = e\nO\n\u0012\n|A||S|\n(1 − γ)5ε + σ2\nF |A|2|S|2\n(1 − γ)7ε3\n\u0013\n.\nConvergence of P-SPG was studied (in deterministic\ncase) in [Agarwal et al., 2021] using the notion of\ngradient mapping.\nRecently, an improved analysis\nwas provided in [Xiao, 2022] with iteration complex-\nity T = O\n\u0010\n|A||S|\n(1−γ)5ε\n\u0011\nto achieve Vp(πT ) − V ∗\np ≤ ε. If\nσF = 0, our iteration complexity in (ii) recovers the\none in [Xiao, 2022], albeit with a different proof.\nImproving dependence on |A|. Notice that the\nabove sample complexity bounds depend on the cardi-\nnality of the action space, which can be large in practice.\nThe key reason for this is that the analysis of P-SPG\n(Prox-SGD) requires to measure the smoothness constant\nLF of Vp(π) in the Euclidean (Frobenius) norm, which\ninevitably depends on the cardinality of the action\nspace |A|. Let us instead consider (2, 1)-matrix norm\n∥·∥2,1, i.e., ∥π∥2\n2,1 = P\ns∈S\n\u0000P\na∈A |πsa|\n\u00012.15 Now, we\nshow that the dependence on |A| in the smoothness\nconstant can be completely removed if (2, 1)-norm is\nused.\nProposition 5.5. For any π, π′ ∈ X, it holds that\n∥∇Vp(π) − ∇Vp (π′)∥2,∞ ≤\n2γ\n(1 − γ)3 ∥π − π′∥2,1 .\nConsider Stochastic\nMirror\nPolicy\nGradient\n(SMPG), that is\nSMD with the matrix form of\nShannon entropy ω(π) := P\ns∈S\nP\na∈A πsa log πsa.16\nThe stochastic gradients in SMD are replaced by\nb∇Vµ(πt) := (b∇1Vµ(πt), . . . , b∇|S|Vµ(πt)). Define Et :=\n(E1\nt , . . . , E|S|\nt\n), then SMPG can be written in a closed\nfrom. For all s ∈ S\nπt+1 = πt ⊙ Et,\nEs\nt :=\nexp\n\u0010\n−ηt b∇sVµ(πt)\n\u0011\nP\na∈A exp\n\u0010\n−ηt b∇sVµ(πt)\n\u0011,\n14The variance of b∇Vµ(·) can be bounded under reason-\nable assumptions or using appropriate exploration strategies,\ne.g., ϵ-greedy or Boltzmann, see [Daskalakis et al., 2020,\nCesa-Bianchi et al., 2017, Xiao, 2022, Johnson et al., 2023].\n15Its dual satisfies ∥π∥2\n2,∞ = P\ns∈S (maxa∈A |πsa|)2.\n16It is 1-strongly convex w.r.t. ∥·∥2,1 norm.\nwhere ⊙ denotes an element-wise multiplication of ma-\ntrices and exp(·) is an element-wise exponential. The\nsample complexity can be derived from Theorem 4.7\nusing Proposition 5.5 under the bounded variance as-\nsumption (in dual norm ∥·∥2,∞).\nCorollary 5.6. For any ε > 0, SMPG guarantees that\nmin0≤t≤T −1 E [Dρ(πt)] ≤ ε2 with ρ := 6γ(1−γ)−3 after\nT = O\n \n1\n(1 − γ)3ε2 +\nσ2\n2,∞\n(1 − γ)3ε4\n!\n.\nNotice that compared to the bound for P-SPG the above\nsample complexity is better at least by a factor of |A|.\nMoreover, σ2,∞ can be much smaller than σF .\nIt should be noted, however, that Dρ(πt) in Corollar-\nies 5.4 and 5.6 are induced by different ω(·) and thus\ninduce different FOSP measures. Also it remains un-\nclear how to establish a global convergence of SMPG in\nthe function value. The technical difficulty arises be-\ncause the condition (7) might not imply Assumption 4.6\nunder non-smooth DGF.\nRemark 5.7. While this example serves well to illus-\ntrate the application of our general theory and potential\nadvantages of SMPG compared to P-SPG, it does not\nmean that SMPG is the most suitable algorithm for solv-\ning (6). In fact, there are other specialized algorithms\nin RL literature, which have better theoretical sample\ncomplexities than shown above. For example, Natural\nPolicy Gradient (NPG) (also known as exponentiated\nQ-descent or Policy Mirror Descent) [Kakade, 2001,\nAgarwal et al., 2021, Lan, 2023, Xiao, 2022, Zhan et al.,\n2023, Khodadadian et al., 2021] achieves faster conver-\ngence in terms of ε. However, a notable difference of\nSMPG compared to NPG is that the latter uses a Q-\nfunction instead of the policy gradient b∇Vµ(π), see the\nderivation of NPG in Section 4 in [Xiao, 2022]. Another\npopular approach to problem (6) is the use of soft-max\npolicy parametrization instead of directly solving the\nproblem over X. In this direction, different variants\nof PG method were developed and analyzed, see, e.g.,\n[Zhang et al., 2020b, 2021, Barakat et al., 2023].\nRemark 5.8. The special cases and variants of Prox-\nP L condition were previously used to derive global\nconvergence of PG methods [Daskalakis et al., 2020,\nKumar et al., 2023] including continuous state action-\nspaces in RL [Ding et al., 2022, Fatkhullin et al., 2023a]\nand classical control tasks [Fazel et al., 2018, Fatkhullin\nand Polyak, 2021, Zhao et al., 2022, Wu et al., 2023].\nAn alternative approach to global convergence of P-SPG,\nbased on hidden convexity of (6), was recently studied\nin [Fatkhullin et al., 2023b].\nIlyas Fatkhullin, Niao He\n5.3\nTraining Autoencoder Model using SMD\nIn this section, we showcase how we can harness gen-\neral Bregman divergence in SMD to address modern\nmachine learning problems involving linear neural net-\nworks, where the objectives go beyond the smooth\nregime considered in the existing theoretical analysis\n[Kawaguchi, 2016].\nMore specifically, assume that the F(·) is twice differ-\nentiable and its Hessian is bounded by the polynomial\nof ∥x∥2, i.e., there exist r, L, Lr ≥ 0 such that\n\r\r∇2F(x)\n\r\r\nop ≤ L + Lr ∥x∥r\n2\nfor all x ∈ Rd.17 (8)\nThe following result (initially appeared in [Lu et al.,\n2018, Lu, 2019]) shows that for any r ≥ 0, the\nabove condition implies relative smoothness (Assump-\ntion 3.1).\nProposition 5.9 (Proposition 2.1. in [Lu et al., 2018]).\nSuppose F(·) is twice differentiable and satisfies (8).\nThen F(·) is ℓ-smooth relative to ω(x) =\n1\nr+2∥x∥r+2\n2\n+\n1\n2∥x∥2\n2 with ℓ := max {L, Lr}.\nTo design a provably convergent scheme for such prob-\nlems, it remains to solve SMD subproblem with DGF\nspecified in the above proposition.\nLuckily, this is\npossible\nct\n=\n(1 + ∥xt∥r\n2) xt − ηt∇f(xt, ξt),\n(9)\nxt+1\n=\n(1 + θr\n∗)−1 ct,\n(10)\nwhere θ∗ ≥ 0 is the unique solution to θr+1 +θ = ∥ct∥2.\nConvergence to a FOSP of the above method follows\nimmediately from our Theorem 4.3, see Appendix E.3\nfor more details. For r = 1, 2, the solution θ∗ can be\nfound in a closed form, while for larger values of r it\ncan be solved using a bisection method.\nTo illustrate the empirical performance of the above\nscheme, we consider two layer autoencoder problem\nmin\nW1 ∈ Rde×df\nW2 ∈ Rdf ×de\n\"\nF(W) := 1\nn\nn\nX\ni=1\n∥W2W1ai − ai∥2\n2\n#\n,\n(11)\nwhere ai ∈ Rdf are flattened representations of im-\nages, W = [W1, W2] are learned parameters of the\nmodel.18 The above problem is nonconvex and globally\nnonsmooth in W since its Hessian norm grows as a\npolynomial in the norm of W. Therefore, SGD can\neasily diverge if poorly initialized. However, condition\n17Compare to (L0, L1)-smoothness condition studied in\n[Zhang et al., 2020a].\n18In previous notations, we have x = vec(W) ∈ R2 de df .\n10−5\n10−3\n10−1\n101\nstep-size\n10−1\n101\n103\n105\n107\nF(xT)\nClip SGD\nSMDr1\nSMDr2\nSGD\nFigure 1: Sensitivity to step-size choice for SMDr1, SMDr2,\nSGD and Clip SGD (with clipping radius 1). The plot shows\nthe function value F(xT ) after T = 104 iterations for each\nstep-size. The star markers correspond to the actual runs,\nand the lines linearly interpolate between them.\n(8) can be verified for some r0 ≥ 2 and the scheme (9),\n(10) provably converges for any r ≥ r0.19\nWe focus on r = 1, 2 and call the corresponding meth-\nods SMDr1 and SMDr2. We compare these algorithms\nto the standard SGD, which corresponds to r = 0. For\ncomparison, we also include a popular heuristic algo-\nrithm, Clip SGD, which is known to mitigate the problem\nof exploding gradients. We use constant step-sizes for\neach method. Figure 1 reports the final training loss\nfor each step-size in the range\n\b\n2−19, 2−18, . . . , 27\t\n.\nWe observe from Figure 1 that SMDr1 and SMDr2 al-\nlow for much larger step-sizes than SGD. Moreover,\nincreasing r also increases the robustness to the step-\nsize choice, i.e., the lower part of the curve becomes\nwider. At the same time, in the high accuracy regime,\nClip SGD still outperforms other algorithms on this task.\nNote, however, that Clip SGD might not converge in\ngeneral in the stochastic setting for a constant clipping\nparameter, see, e. g., [Koloskova et al., 2023].\nAcknowledgments\nThis work is supported by ETH AI Center Doctoral Fel-\nlowship and Swiss National Science Foundation (SNSF)\nProject Funding No. 200021-207343.\nReferences\nM. Abadi, A. Chu, I. Goodfellow, H. B. McMahan,\nI. Mironov, K. Talwar, and L. Zhang. Deep learn-\ning with differential privacy. In Proceedings of the\n19Computation of the Hessian and use of Cauchy-Schwarz\ninequality implies (8) for any number of layers in (11).\nTaming Nonconvex Stochastic Mirror Descent\n2016 ACM SIGSAC conference on computer and\ncommunications security, pages 308–318, 2016.\nS. Acharyya, A. Banerjee, and D. Boley. Bregman\ndivergences and triangle inequality. In Proceedings\nof the 2013 SIAM International Conference on Data\nMining, pages 476–484. SIAM, 2013.\nA. Agarwal, M. J. Wainwright, P. Bartlett, and\nP. Ravikumar. Information-theoretic lower bounds\non the oracle complexity of convex optimization. Ad-\nvances in Neural Information Processing Systems,\n22, 2009.\nA. Agarwal, S. M. Kakade, J. D. Lee, and G. Mahajan.\nOn the theory of policy gradient methods: Optimal-\nity, approximation, and distribution shift. Journal\nof Machine Learning Research, 22(98):1–76, 2021.\nM. Ahookhosh, A. Themelis, and P. Patrinos. A breg-\nman forward-backward linesearch algorithm for non-\nconvex composite optimization: superlinear conver-\ngence to nonisolated local minima. SIAM Journal\non Optimization, 31(1):653–685, 2021.\nZ. Allen-Zhu and L. Orecchia. Linear coupling: An\nultimate unification of gradient and mirror descent.\narXiv preprint arXiv:1407.1537, 2014.\nY. Arjevani, Y. Carmon, J. C. Duchi, D. J. Foster,\nN. Srebro, and B. Woodworth. Lower bounds for\nnon-convex stochastic optimization. Mathematical\nProgramming, 199(1-2):165–214, 2023.\nS. Arora, E. Hazan, and S. Kale. The multiplicative\nweights update method: a meta-algorithm and ap-\nplications. Theory of computing, 8(1):121–164, 2012.\nH. Asi, V. Feldman, T. Koren, and K. Talwar. Private\nstochastic convex optimization: Optimal rates in l1\ngeometry. In Proceedings of the 38th International\nConference on Machine Learning, volume 139, pages\n393–403, 2021.\nK. Balasubramanian and S. Ghadimi. Zeroth-order\nnonconvex stochastic optimization: Handling con-\nstraints, high dimensionality, and saddle points.\nFoundations of Computational Mathematics, pages\n1–42, 2022.\nA. Barakat, I. Fatkhullin, and N. He. Reinforcement\nlearning with general utilities: Simpler variance re-\nduction and large state-action space. In Proceedings\nof the 40th International Conference on Machine\nLearning, volume 202, pages 1753–1800, 2023.\nR. Bassily, A. Smith, and A. Thakurta. Private empiri-\ncal risk minimization: Efficient algorithms and tight\nerror bounds. In 2014 IEEE 55th annual symposium\non foundations of computer science, pages 464–473.\nIEEE, 2014.\nR. Bassily, C. Guzm´an, and M. Menart.\nDifferen-\ntially private stochastic optimization: New results in\nconvex and non-convex settings. Advances in Neu-\nral Information Processing Systems, 34:9317–9329,\n2021a.\nR. Bassily, C. Guzm´an, and A. Nandi. Non-euclidean\ndifferentially private stochastic convex optimization.\nIn Conference on Learning Theory, pages 474–499.\nPMLR, 2021b.\nH. H. Bauschke, J. Bolte, and M. Teboulle.\nA de-\nscent lemma beyond lipschitz gradient continuity:\nfirst-order methods revisited and applications. Math-\nematics of Operations Research, 42(2):330–348, 2017.\nA. Beck and M. Teboulle. Mirror descent and nonlinear\nprojected subgradient methods for convex optimiza-\ntion. Operations Research Letters, 31(3):167–175,\n2003.\nA. Ben-Tal, T. Margalit, and A. Nemirovski.\nThe\nordered subsets mirror descent optimization method\nwith applications to tomography. SIAM Journal on\nOptimization, 12(1):79–108, 2001.\nB. Birnbaum, N. R. Devanur, and L. Xiao. Distributed\nalgorithms via gradient descent for fisher markets.\nIn Proceedings of the 12th ACM conference on Elec-\ntronic commerce, pages 127–136, 2011.\nL. M. Bregman.\nThe relaxation method of finding\nthe common point of convex sets and its application\nto the solution of problems in convex programming.\nUSSR computational mathematics and mathematical\nphysics, 7(3):200–217, 1967.\nN. Cesa-Bianchi, C. Gentile, G. Lugosi, and G. Neu.\nBoltzmann exploration done right. Advances in Neu-\nral Information Processing Systems, 30, 2017.\nK. Chaudhuri, C. Monteleoni, and A. D. Sarwate. Dif-\nferentially private empirical risk minimization. Jour-\nnal of Machine Learning Research, 12(3), 2011.\nG. Chen and M. Teboulle. Convergence analysis of a\nproximal-like minimization algorithm using bregman\nfunctions. SIAM Journal on Optimization, 3(3):538–\n543, 1993.\nC. Daskalakis, D. J. Foster, and N. Golowich. Indepen-\ndent policy gradient methods for competitive rein-\nforcement learning. Advances in Neural Information\nProcessing Systems, 33:5527–5540, 2020.\nD. Davis and D. Drusvyatskiy. Stochastic model-based\nminimization of weakly convex functions.\nSIAM\nJournal on Optimization, 29(1):207–239, 2019.\nD. Davis and B. Grimmer. Proximally guided stochas-\ntic subgradient method for nonsmooth, nonconvex\nproblems. SIAM Journal on Optimization, 29(3):\n1908–1930, 2019.\nD. Davis, D. Drusvyatskiy, and K. J. MacPhee.\nStochastic model-based minimization under high-\nIlyas Fatkhullin, Niao He\norder growth.\narXiv preprint arXiv:1807.00255,\n2018.\nK. Ding, J. Li, and K.-C. Toh. Nonconvex stochastic\nbregman proximal gradient method with application\nto deep learning. arXiv preprint arXiv:2306.14522,\n2023.\nY. Ding, J. Zhang, and J. Lavaei. On the global op-\ntimum convergence of momentum-based policy gra-\ndient. In International Conference on Artificial In-\ntelligence and Statistics, pages 1910–1934. PMLR,\n2022.\nR.-A. Dragomir, A. B. Taylor, A. d’Aspremont, and\nJ. Bolte. Optimal complexity and certification of\nbregman first-order methods.\nMathematical Pro-\ngramming, pages 1–43, 2021.\nY. Drori and O. Shamir. The complexity of finding\nstationary points with stochastic gradient descent.\nIn International Conference on Machine Learning,\npages 2658–2667. PMLR, 2020.\nD. Drusvyatskiy and C. Paquette. Efficiency of mini-\nmizing compositions of convex functions and smooth\nmaps.\nMathematical Programming, 178:503–558,\n2019.\nC. Dwork, F. McSherry, K. Nissim, and A. Smith. Cal-\nibrating noise to sensitivity in private data analysis.\nIn Theory of Cryptography: Third Theory of Cryp-\ntography Conference, TCC 2006, New York, NY,\nUSA, March 4-7, 2006. Proceedings 3, pages 265–284.\nSpringer, 2006.\nI. Fatkhullin and B. Polyak.\nOptimizing static lin-\near feedback: Gradient method. SIAM Journal on\nControl and Optimization, 59(5):3887–3911, 2021.\nI. Fatkhullin, J. Etesami, N. He, and N. Kiyavash.\nSharp analysis of stochastic optimization under\nglobal Kurdyka- Lojasiewicz inequality.\nAdvances\nin Neural Information Processing Systems, 2022.\nI. Fatkhullin, A. Barakat, A. Kireeva, and N. He.\nStochastic policy gradient methods: Improved sam-\nple complexity for Fisher-non-degenerate policies. In\nProceedings of the 40th International Conference on\nMachine Learning, volume 202, pages 9827–9869,\n2023a.\nI. Fatkhullin, N. He, and Y. Hu.\nStochastic opti-\nmization under hidden convexity. arXiv preprint\narXiv:2401.00108v1, 2023b.\nI. Fatkhullin, A. Tyurin, and P. Richt´arik. Momen-\ntum provably improves error feedback! Advances in\nNeural Information Processing Systems, 2023c.\nM. Faw, L. Rout, C. Caramanis, and S. Shakkottai.\nBeyond uniform smoothness: A stopped analysis\nof adaptive sgd. arXiv preprint arXiv:2302.06570,\n2023.\nM. Fazel, R. Ge, S. Kakade, and M. Mesbahi. Global\nconvergence of policy gradient methods for the linear\nquadratic regulator. In International Conference on\nMachine Learning, pages 1467–1476. PMLR, 2018.\nS. Ghadimi and G. Lan. Stochastic first-and zeroth-\norder methods for nonconvex stochastic program-\nming. SIAM Journal on Optimization, 23(4):2341–\n2368, 2013.\nS. Ghadimi, G. Lan, and H. Zhang. Mini-batch stochas-\ntic approximation methods for nonconvex stochastic\ncomposite optimization. Mathematical Programming,\n155(1-2):267–305, 2016.\nS. Gopi, Y. T. Lee, D. Liu, R. Shen, and K. Tian. Pri-\nvate convex optimization in general norms. In Pro-\nceedings of the 2023 Annual ACM-SIAM Symposium\non Discrete Algorithms (SODA), pages 5068–5089.\nSIAM, 2023.\nF. Hanzely and P. Richt´arik. Fastest rates for stochastic\nmirror descent methods. Computational Optimiza-\ntion and Applications, 79:717–766, 2021.\nF. Hanzely, P. Richtarik, and L. Xiao. Accelerated breg-\nman proximal gradient methods for relatively smooth\nconvex optimization. Computational Optimization\nand Applications, 79:405–440, 2021.\nF. Huang, S. Gao, and H. Huang. Bregman gradient\npolicy optimization. In International Conference on\nLearning Representations, 2022.\nF. H¨ubler, J. Yang, X. Li, and N. He.\nParameter-\nagnostic optimization under relaxed smoothness.\narXiv preprint arXiv:2311.03252, 2023.\nR. Iyengar,\nJ. P. Near,\nD. Song,\nO. Thakkar,\nA. Thakurta, and L. Wang. Towards practical differ-\nentially private convex optimization. In 2019 IEEE\nSymposium on Security and Privacy (SP), pages 299–\n316. IEEE, 2019.\nS. J Reddi, S. Sra, B. Poczos, and A. J. Smola. Proximal\nstochastic methods for nonsmooth nonconvex finite-\nsum optimization. Advances in Neural Information\nProcessing Systems, 29, 2016.\nE. Johnson, C. Pike-Burke, and P. Rebeschini. Optimal\nconvergence rate for exact policy mirror descent in\ndiscounted markov decision processes. arXiv preprint\narXiv:2302.11381, 2023.\nS. M. Kakade. A natural policy gradient. Advances in\nNeural Information Processing Systems, 14, 2001.\nH. Karimi, J. Nutini, and M. Schmidt. Linear conver-\ngence of gradient and proximal-gradient methods un-\nder the Polyak- Lojasiewicz condition. arXiv preprint\narXiv:1608.04636v4, 2016.\nK. Kawaguchi. Deep learning without poor local min-\nima.\nAdvances in Neural Information Processing\nSystems, 29, 2016.\nTaming Nonconvex Stochastic Mirror Descent\nS. Khodadadian, P. R. Jhunjhunwala, S. M. Varma,\nand S. T. Maguluri. On the linear convergence of\nnatural policy gradient algorithm. In 2021 60th IEEE\nConference on Decision and Control (CDC), pages\n3794–3799. IEEE, 2021.\nD. Kifer, A. Smith, and A. Thakurta. Private convex\nempirical risk minimization and high-dimensional\nregression. In Conference on Learning Theory, pages\n25–1. JMLR Workshop and Conference Proceedings,\n2012.\nA. Koloskova, H. Hendrikx, and S. U. Stich. Revis-\niting gradient clipping: Stochastic bias and tight\nconvergence guarantees. In Proceedings of the 40th\nInternational Conference on Machine Learning, 2023.\nN. Kumar, I. Usmanova, K. Y. Levy, and S. Mannor.\nTowards faster global convergence of robust policy\ngradient methods. In Sixteenth European Workshop\non Reinforcement Learning, 2023.\nG. Lan. An optimal method for stochastic composite\noptimization. Mathematical Programming, 133(1-2):\n365–397, 2012.\nG. Lan. Policy mirror descent for reinforcement learn-\ning: Linear convergence, new sampling complexity,\nand generalized problem classes. Mathematical pro-\ngramming, 198(1):1059–1106, 2023.\nT. Lezanski. ¨Uber das minimumproblem f¨ur funktionale\nin banachschen r¨aumen. Mathematische Annalen,\n152:271–274, 1963.\nZ. Li and J. Li. A simple proximal stochastic gradi-\nent method for nonsmooth nonconvex optimization.\nAdvances in Neural Information Processing Systems,\n2018.\nT. Liu and T. K. Pong.\nFurther properties of the\nforward–backward envelope with applications to\ndifference-of-convex programming. Computational\nOptimization and Applications, 67:489–520, 2017.\nZ. Liu, T. D. Nguyen, T. H. Nguyen, A. Ene, and\nH. Nguyen. High probability convergence of stochas-\ntic gradient methods. In International Conference\non Machine Learning, pages 21884–21914. PMLR,\n2023.\nS. Lojasiewicz. Une propri´et´e topologique des sous-\nensembles analytiques r´eels.\nLes ´equations aux\nd´eriv´ees partielles, 117:87–89, 1963.\nH. Lu. “relative continuity” for non-lipschitz nons-\nmooth convex optimization using stochastic (or de-\nterministic) mirror descent. INFORMS Journal on\nOptimization, 1(4):288–303, 2019.\nH. Lu, R. M. Freund, and Y. Nesterov. Relatively\nsmooth convex optimization by first-order methods,\nand applications. SIAM Journal on Optimization,\n28(1):333–354, 2018.\nA. V. Nazin, A. S. Nemirovsky, A. B. Tsybakov, and\nA. B. Juditsky. Algorithms of robust stochastic opti-\nmization based on mirror descent method. Automa-\ntion and Remote Control, 80:1607–1627, 2019.\nA. Nemirovskij and D. Yudin. Efficient methods of\nsolving convex programming problems of high dimen-\nsionality. Ekonomika i matem. methody (in Russian),\nXV(1), 1979.\nA. Nemirovskij and D. Yudin. Problem complexity and\nmethod efficiency in optimization. SIAM Review,\n1983.\nY. Nesterov. Lectures on convex optimization, volume\n137. Springer, 2018.\nT. D. Nguyen, T. H. Nguyen, A. Ene, and H. Nguyen.\nImproved convergence in high probability of clipped\ngradient methods with heavy tailed noise. Advances\nin Neural Information Processing Systems, 2023.\nP. Patrinos and A. Bemporad. Proximal newton meth-\nods for convex composite optimization.\nIn 52nd\nIEEE Conference on Decision and Control, pages\n2358–2363. IEEE, 2013.\nB. T. Polyak. Gradient methods for the minimisation\nof functionals. USSR Computational Mathematics\nand Mathematical Physics, 3(4):864–878, 1963.\nS. Shalev-Shwartz. Online learning and online convex\noptimization. Foundations and Trends® in Machine\nLearning, 4(2):107–194, 2012.\nL. Stella, A. Themelis, and P. Patrinos.\nForward–\nbackward quasi-newton methods for nonsmooth op-\ntimization problems. Computational Optimization\nand Applications, 67(3):443–487, 2017.\nS. U. Stich. Unified optimal analysis of the (stochastic)\ngradient method. arXiv preprint arXiv:1907.04232,\n2019.\nR. Van Handel. Probability in high dimension. Lecture\nNotes (Princeton University), 2014.\nN. M. Vural, L. Yu, K. Balasubramanian, S. Volgu-\nshev, and M. A. Erdogdu. Mirror descent strikes\nagain: Optimal stochastic convex optimization under\ninfinite noise variance. In Conference on Learning\nTheory, pages 65–102. PMLR, 2022.\nD. Wang and J. Xu.\nDifferentially private empiri-\ncal risk minimization with smooth non-convex loss\nfunctions: A non-stationary view. In Proceedings\nof the AAAI Conference on Artificial Intelligence,\nvolume 33, pages 1182–1189, 2019.\nD. Wang, M. Ye, and J. Xu. Differentially private em-\npirical risk minimization revisited: Faster and more\ngeneral. Advances in Neural Information Processing\nSystems, 30, 2017.\nIlyas Fatkhullin, Niao He\nD. Wang, C. Chen, and J. Xu. Differentially private\nempirical risk minimization with non-convex loss\nfunctions. In International Conference on Machine\nLearning, pages 6526–6535. PMLR, 2019.\nJ. Wu, A. Barakat, I. Fatkhullin, and N. He. Learn-\ning zero-sum linear quadratic games with improved\nsample complexity. In 62nd IEEE Conference on\nDecision and Control, 2023.\nX. Wu, F. Li, A. Kumar, K. Chaudhuri, S. Jha, and\nJ. Naughton. Bolt-on differential privacy for scalable\nstochastic gradient descent-based analytics. In Pro-\nceedings of the 2017 ACM International Conference\non Management of Data, pages 1307–1322, 2017.\nH. Xiao, K. Rasul, and R. Vollgraf. Fashion-mnist: a\nnovel image dataset for benchmarking machine learn-\ning algorithms. arXiv preprint arXiv:1708.07747,\n2017.\nL. Xiao. On the convergence rates of policy gradient\nmethods. The Journal of Machine Learning Research,\n23(1):12887–12922, 2022.\nJ. Yang, X. Li, I. Fatkhullin, and N. He. Two sides of\none coin: the limits of untuned sgd and the power of\nadaptive methods. arXiv preprint arXiv:2305.12475,\n2023.\nP. Yue, C. Fang, and Z. Lin. On the lower bound\nof minimizing polyak- lojasiewicz functions. In The\nThirty Sixth Annual Conference on Learning Theory,\npages 2948–2968. PMLR, 2023.\nW. Zhan, S. Cen, B. Huang, Y. Chen, J. D. Lee, and\nY. Chi. Policy mirror descent for regularized rein-\nforcement learning: A generalized framework with\nlinear convergence. SIAM Journal on Optimization,\n33(2):1061–1091, 2023.\nJ. Zhang, K. Zheng, W. Mou, and L. Wang. Efficient\nprivate erm for smooth objectives. arXiv preprint\narXiv:1703.09947, 2017.\nJ. Zhang, T. He, S. Sra, and A. Jadbabaie. Why gradi-\nent clipping accelerates training: A theoretical justi-\nfication for adaptivity. In International Conference\non Learning Representations, 2020a.\nJ. Zhang, A. Koppel, A. S. Bedi, C. Szepesvari, and\nM. Wang. Variational policy gradient method for\nreinforcement learning with general utilities. Ad-\nvances in Neural Information Processing Systems,\n33:4572–4583, 2020b.\nJ. Zhang, C. Ni, C. Szepesvari, M. Wang, et al. On\nthe convergence and sample efficiency of variance-\nreduced policy gradient method. Advances in Neural\nInformation Processing Systems, 34:2228–2240, 2021.\nL. Zhang. Variance reduction for non-convex stochastic\noptimization: General analysis and new applications.\nMaster’s thesis, ETH Zurich, 2021.\nS. Zhang and N. He. On the convergence rate of stochas-\ntic mirror descent for nonsmooth nonconvex opti-\nmization. arXiv preprint arXiv:1806.04781, 2018.\nF. Zhao,\nX. Fu,\nand K. You.\nGlobal conver-\ngence of policy gradient methods for output feed-\nback linear quadratic control.\narXiv preprint\narXiv:2211.04051v1, 2022.\nY. Zhou, X. Chen, M. Hong, Z. S. Wu, and A. Banerjee.\nPrivate stochastic non-convex optimization: Adap-\ntive algorithms and tighter generalization bounds.\narXiv preprint arXiv:2006.13501, 2020a.\nZ. Zhou, P. Mertikopoulos, N. Bambos, S. P. Boyd,\nand P. W. Glynn. On the convergence of mirror de-\nscent beyond stochastic convex programming. SIAM\nJournal on Optimization, 30(1):687–716, 2020b.\nTaming Nonconvex Stochastic Mirror Descent\nContents\n1\nINTRODUCTION\n1\n1.1\nRelated Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n1\n1.2\nContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n2\n2\nPRELIMINARIES\n2\n2.1\nFOSP Measures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n3\nASSUMPTIONS\n3\n4\nMAIN RESULTS\n3\n4.1\nConnections between FOSP Measures\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3\n4.2\nConvergence to FOSP in Expectation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n4.3\nHigh Probability Convergence to FOSP under Sub-Gaussian Noise . . . . . . . . . . . . . . . . .\n5\n4.4\nGlobal Convergence under Generalized Proximal P L condition\n. . . . . . . . . . . . . . . . . . .\n5\n5\nNEW INSIGHTS FOR MACHINE LEARNING\n6\n5.1\nDP Learning in ℓ2 and ℓ1 Settings\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n5.2\nPolicy Optimization in Reinforcement Learning (RL) . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n5.3\nTraining Autoencoder Model using SMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\nA Proofs of Lemma 4.1 and 4.2: Connections Between FOSP\n15\nB Proof of Theorem 4.3: Convergence to FOSP in Expectation\n18\nC Proof of Theorem 4.5: High Probability Convergence to FOSP under Sub-Gaussian Noise\n20\nD Proof of Theorem 4.7. Global Convergence under Generalized Proximal P L Condition\n22\nE Proofs for Applications\n23\nE.1\nDifferentially Private Learning in ℓ2 and ℓ1 Settings\n. . . . . . . . . . . . . . . . . . . . . . . . .\n23\nE.2\nPolicy Optimization in Reinforecement Learning\n. . . . . . . . . . . . . . . . . . . . . . . . . . .\n23\nE.3\nTraining Autoencoder Model using SMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nF Useful Lemma\n26\nIlyas Fatkhullin, Niao He\nA\nProofs of Lemma 4.1 and 4.2: Connections Between FOSP\nBPM and BGM are equivalent up to a constant factor.\nThe following lemma establishes that if F(·) is (ℓ, ω)-smooth, then the distance between ˆx and x+ is small.\nLemma A.1. For any ρ > ℓ, we have for all x ∈ X ∩ S\nρ2Dsym\nω\n(ˆx, x+) ≤\nℓ\nρ − ℓ\n\u0000∆ρ(x) + ∆+\nρ (x)\n\u0001\n.\nProof. Recall that ˆx := arg miny∈X F(y)+r(y)+ρDω(y, x), and x+ := arg miny∈X ⟨∇F(x), y⟩+r(y)+ρDω(y, x).\nBy the optimality conditions for x+ and ˆx, there exist s+ ∈ ∂(r + δX )(x+) and ˆs ∈ ∂(r + δX )(ˆx), such that\n0 = ∇F(ˆx) + ˆs + ρ ∇ω(ˆx) − ρ ∇ω(x),\n0 = ∇F(x) + s+ + ρ ∇ω(x+) − ρ ∇ω(x),\nSubtracting these equalities, we obtain ρ ∇ω(ˆx) − ρ ∇ω(x+) = s+ − ˆs + ∇F(x) − ∇F(ˆx).\nBy Lemma F.1-1 (three point identity) with x = z and using the above identity, we have\nρ(Dω(ˆx, x+) + Dω(x+, ˆx))\n=\n⟨ρ ∇ω(ˆx) − ρ ∇ω(x+), ˆx − x+⟩\n=\n⟨s+ − ˆs + ∇F(x) − ∇F(ˆx), ˆx − x+⟩\n(i)\n≤\n⟨∇F(x) − ∇F(ˆx), ˆx − x+⟩\n(12)\nwhere in (i) we use convexity of (r + δX )(·). By relative smoothness of F(·), we have for any x, y, z ∈ X ∩ S\nF(x) − F(y) − ⟨∇F(y), x − y⟩ ≤ ℓDω(x, y),\nF(z) − F(x) − ⟨∇F(x), z − x⟩ ≤ ℓDω(z, x),\n−ℓDω(z, y) ≤ F(z) − F(y) − ⟨∇F(y), z − y⟩.\nAdding the above inequalities gives for any x, y, z ∈ X ∩ S\n⟨∇F(x) − ∇F(y), y − z⟩ ≤ ℓDω(x, y) + ℓDω(z, x) + ℓDω(z, y).\nApplying the above inequality with x = x, y = ˆx, z = x+, we further bound (12)\nρ(Dω(ˆx, x+) + Dω(x+, ˆx))\n≤\nℓDω(x, ˆx) + ℓDω(x+, x) + ℓDω(x+, ˆx)\n≤\nℓDω(x, ˆx) + ℓDω(x+, x) + ℓDω(x+, ˆx) + ℓDω(ˆx, x+).\nTherefore, for any ρ > ℓ, we have\nρ2 \u0000Dω(ˆx, x+) + Dω(x+, ˆx)\n\u0001\n≤ ℓρ2\nρ − ℓ(Dω(x, ˆx) + Dω(x+, x)) ≤\nℓ\nρ − ℓ\n\u0000∆ρ(x) + ∆+\nρ (x)\n\u0001\n.\nProof of Lemma 4.1. For any x, y, z ∈ X ∩ S and s > 0, we have\nDsym\nω\n(x, y) ≤\n\u0012q\nDsym\nω\n(x, z) +\nq\nDsym\nω\n(y, z)\n\u00132\n≤ (1 + s) Dsym\nω\n(x, z) +\n\u00001 + s−1\u0001\nDsym\nω\n(y, z).\nApplying Lemma A.1 together with the above inequality, we have\n∆+\nρ (x) = ρ2Dsym\nω\n(x, x+)\n≤\n(1 + s)ρ2Dsym\nω\n(x, ˆx) + (1 + s−1)ρ2Dsym\nω\n(x+, ˆx)\n≤\n(1 + s)ρ2Dsym\nω\n(x, ˆx) + (1 + s−1)ℓ\nρ − ℓ\n\u0000∆ρ(x) + ∆+\nρ (x)\n\u0001\n≤\n\u0012\n1 + s + (1 + s−1)ℓ\nρ − ℓ\n\u0013\n∆ρ(x) + (1 + s−1)ℓ\nρ − ℓ\n∆+\nρ (x).\nTaming Nonconvex Stochastic Mirror Descent\nRearranging, we obtain the upper bound on ∆+\nρ (x). For the lower bound, we act similarly and derive\n∆ρ(x) = ρ2Dsym\nω\n(x, ˆx)\n≤\n(1 + s)ρ2Dsym\nω\n(x, x+) + (1 + s−1)ρ2Dsym\nω\n(ˆx, x+)\n≤\n(1 + s)ρ2Dsym\nω\n(x, x+) + (1 + s−1)ℓ\nρ − ℓ\n\u0000∆ρ(x) + ∆+\nρ (x)\n\u0001\n≤\n\u0012\n1 + s + (1 + s−1)ℓ\nρ − ℓ\n\u0013\n∆+\nρ (x) + (1 + s−1)ℓ\nρ − ℓ\n∆ρ(x).\nCombining the above two inequalities, we have\n∆ρ(x)\nC(ℓ,ρ,s) ≤ ∆+\nρ (x) ≤ C(ℓ, ρ, s)∆ρ(x).\nRemark A.2. Notice that our intermediate Lemma A.1 does not require ω(·) to induce a metric and shows that ˆx\nand x+ are close if ∆ρ(x) and ∆+\nρ (x) are small. However, the proof of Lemma 4.1 crucially relies on triangle\ninequality. Therefore, it is unclear whether convergence of SMD in ∆ρ(x) (that was established in [Zhang and He,\n2018] under BG assumption) implies convergence in ∆+\nρ (x). In our main Theorems 4.3, 4.5 and 4.7, we bypass\nthis issue and directly establish convergence on Dρ(x), that is a stronger measure than ∆+\nρ (x) (and stronger than\n∆ρ(x) if\np\nDsym\nω\n(x, y) is a metric). Moreover, as we have seen in Section 2, Dρ(x) seems to be a more natural\nFOSP measure since it reduces to ∥∇F(x)∥2 in unconstrained case.\nBFBE is strictly larger than BGM.\nNow we state the proof of Lemma 4.2, which consists of two parts.\nProof of Lemma 4.2. 1. BFBE is not smaller than BGM. Recall that x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) +\nρDω(y, x). By the optimality condition, there exists u+ ∈ ∂r(x+) such that\n0 = ∇F(x) + ρ(∇ω(x+) − ∇ω(x)) + u+.\nThus, by convexity of r(·)\nr(x)\n≥\nr(x+) + ⟨u+, x − x+⟩ = r(x+) + ρ⟨ ∇ω(x) − ∇ω(x+), x − x+⟩ − ⟨∇F(x), x − x+⟩\n=\nr(x+) + ρ(Dω(x, x+) + Dω(x+, x)) − ⟨∇F(x), x − x+⟩.\nUsing the above inequality and the definition of Dρ(x), we derive for any ρ, ρ1 > 0\n1\n2ρ1\nDρ1(x)\n:=\n− min\ny∈X {⟨∇F(x), y − x⟩ + ρ1Dω(y, x) + r(y) − r(x)}\n=\n⟨∇F(x), x − x+⟩ − ρ1Dω(x+, x) + r(x) − r(x+)\n≥\nρ(Dω(x, x+) + Dω(x+, x)) − ρ1Dω(x+, x)\n≥\n(ρ − ρ1)(Dω(x+, x) + Dω(x+, x)).\nRecalling the definition of ∆+\nρ (x) and setting ρ1 = ρ/2, it remains to conclude that Dρ/2(x) ≥ 1\n2∆+\nρ (x).\n2. BFBE can be much larger than BGM.\nThe following example shows how large can be the ratio of BFBE and BGM. Consider minimizing Φ(x) =\nF(x) + r(x) over X = [0, 1] ⊂ R1 with F(x) = 1\n2x2, r(x) = |x|. We can compute the proximal operator of the\nabsolute value as proxr/ρ(x) = sign(x) max\nn\n0, |x| − 1\nρ\no\n, where sign(x) = 1, if x ≥ 0 and sign(x) = −1 otherwise.\nFor any x ∈ [0, 1] and ρ ≥ 1, we can compute x+ = proxr/ρ(x − ρ−1x) = 0. Therefore, we have\n∆+\nρ (x) = x2,\nDρ(x) = −2ρ(⟨∇F(x), x+ − x⟩ + ρ\n2(x+ − x)2 + |x+| − |x|) = 2ρ|x| + 2ρ\n\u0010\n1 − ρ\n2\n\u0011\nx2.\nIn particular, taking arbitrary ρ = ρ1 ≥ 1 in the first equality and ρ ≤ 2 in the second equality, we have for any\nx ∈ (0, 1]\nDρ(x)\n∆+\nρ1(x) ≥ 2ρ|x|\nx2\n≥ 2\n|x|.\nIlyas Fatkhullin, Niao He\nWe conclude that Dρ(x) can be arbitrary larger than ∆+\nρ1(x) even when x is close to the optimum x∗ = 0. This\nimplies that the opposite inequality in Lemma 4.2 does not hold in general even when ρ and ρ1 are allowed to be\ndifferent. Therefore, BFBE is strictly stronger convergence measure than BGM.\nTaming Nonconvex Stochastic Mirror Descent\nB\nProof of Theorem 4.3: Convergence to FOSP in Expectation\nProof. Step I. Deterministic descent w.r.t. Forward-Backward Envelope.\nDefine ˆx := proxΦ/ρ(x). Notice that for any x ∈ X ∩ S, we have for any x+ ∈ X ∩ S\nΦ1/ρ(x) = Φ(ˆx) + ρDω(ˆx, x) ≤ Φ(x+) + ρDω(x+, x).\nWe set x+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + ρ1Dω(y, x) with ρ1 ≥ ρ + ℓ. Then by relative smoothness (upper\nbound) of F(·)\nΦ1/ρ(x)\n≤\nΦ\n\u0000x+\u0001\n+ ρDω(x+, x)\n=\nF\n\u0000x+\u0001\n+ r\n\u0000x+\u0001\n+ ρDω(x+, x)\n≤\nF (x) +\n\n∇F (x) , x+ − x\n\u000b\n+ ℓDω(x+, x) + r\n\u0000x+\u0001\n+ ρDω(x+, x)\n=\nΦ (x) +\n\n∇F (x) , x+ − x\n\u000b\n+ (ρ + ℓ)Dω(x+, x) + r\n\u0000x+\u0001\n− r (x)\n=\nΦ (x) −\n1\n2ρ1\nDρ1(x) + (ρ + ℓ − ρ1) Dω(x+, x)\n≤\nΦ (x) −\n1\n2ρ1\nDρ1(x).\n(13)\nwhere the last equality holds by definitions of x+, Dρ(x) and the last step is due to condition ρ1 ≥ ρ + ℓ.\nStep II. One step progress on the Lyapunov function.\nBy the update rule of xt+1 and applying Lemma F.1, Item 2 with z = xt, z+ = xt+1, x = ˆxt, we have\nηt⟨∇f(xt, ξt), ˆxt − xt+1⟩ + ηt(r(ˆxt) − r(xt+1)) ≥ Dω(ˆxt, xt+1) + Dω(xt+1, xt) − Dω(ˆxt, xt).\n(14)\nBy the optimality of ˆxt+1 and using the above inequality, we derive\nΦ1/ρ (xt+1) = Φ\n\u0000ˆxt+1\u0001\n+ ρDω(ˆxt+1, xt+1)\n≤ Φ\n\u0000ˆxt\u0001\n+ ρDω(ˆxt, xt+1)\n(14)\n≤ Φ\n\u0000ˆxt\u0001\n+ ηtρ⟨∇f(xt, ξt), ˆxt − xt+1⟩ + ρDω(ˆxt, xt) − ρDω(xt+1, xt) + ηtρ(r(ˆxt) − r(xt+1))\n= Φ1/ρ (xt) + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩) + ρηt⟨∇f(xt, ξt), xt − xt+1⟩\n− ρDω(xt+1, xt) + ηtρ(r(xt) − r(xt+1)).\nWe define λt := Φ1/ρ (xt) − Φ∗ + ηt−1ρ(Φ(xt) − Φ∗), ψt := ∇f(xt, ξt) − ∇F(xt) . Then using the above inequality\nλt+1\n:=\nΦ1/ρ (xt+1) − Φ∗ + ηtρ(Φ(xt+1) − Φ∗)\n≤\nΦ1/ρ (xt) − Φ∗ + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩)\n+ρ(ηt⟨∇f(xt, ξt) − ∇F(xt), xt − xt+1⟩ − Dω(xt+1, xt))\n+ηtρ(r(xt) + F(xt+1) + ⟨∇F(xt), xt − xt+1⟩ − Φ∗)\n(i)\n≤\nΦ1/ρ (xt) − Φ∗ + ηtρ(r(ˆxt) − r(xt) + ⟨∇f(xt, ξt), ˆxt − xt⟩)\n+ρ(ηt⟨∇f(xt, ξt) − ∇F(xt), xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt))\n+ηtρ(r(xt) + F(xt) − Φ∗)\n=\nλt + (ηt − ηt−1)ρ(Φ(xt) − Φ∗) + ηtρ(r(ˆxt) − r(xt) + ⟨∇F(xt), ˆxt − xt⟩) + ρηt⟨ψt, ˆxt − xt⟩\n+ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt))\n(ii)\n≤\nλt + (ηt − ηt−1)ρ(Φ(xt) − Φ∗) −\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt) − ηtρ(ρ − ℓ)Dω(ˆxt, xt) + ρηt⟨ψt, ˆxt − xt⟩\n+ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt))\n(iii)\n≤\nλt −\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρ(ηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt))\n(15)\n−ηtρ(ρ − ℓ)Dω(ˆxt, xt),\nIlyas Fatkhullin, Niao He\nwhere (i) follows by relative smoothness (upper bound), i.e., F(xt+1) ≤ F(xt)−⟨∇F(xt), xt−xt+1⟩+ℓDω(xt+1, xt).\nThe inequality (ii) follows from relative smoothness (lower bound) of F(·) and (13) (with ρ1 = ρ + ℓ) since\nr(ˆxt) − r(xt) + ⟨∇F(xt), ˆxt − xt⟩\n≤\nr(ˆxt) − r(xt) + F(ˆxt) − F(xt) + ℓDω(ˆxt, xt)\n=\nΦ1/ρ(xt) − Φ(xt) + (ℓ − ρ)Dω(ˆxt, xt)\n≤\nΦ1/ρ(xt) − Φ(xt) + (ℓ − ρ)Dω(ˆxt, xt)\n≤\n−\n1\n2(ρ + ℓ)Dρ+ℓ(xt) − (ρ − ℓ)Dω(ˆxt, xt).\nThe inequality (iii) holds since the sequence {ηt}t≥0 is non-increasing.\nStep III. Dealing with stochastic terms. Using Dω(xt+1, xt) ≥ 1\n2 ∥xt+1 − xt∥2 and the bound on the variance\nof stochastic gradients, we have\nE\n\u0002\nηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)Dω(xt+1, xt)\n\u0003\n≤\nE\n\u0014\nηt⟨ψt, xt − xt+1⟩ − (1 − ηtℓ)1\n2 ∥xt+1 − xt∥2\n\u0015\n≤\nη2\nt\n2(1 − ηtℓ)E\nh\r\rψt\r\r2\n∗\ni\n≤\nη2\nt σ2\n2(1 − ηtℓ).\n(16)\nDefine Λt := E [λt].\nThen combining (15) with (16) and setting ρ = 2ℓ, ηt ≤ 1/(2ℓ), we derive for any\nnon-increasing step-sizes ηt\nΛt+1\n≤\nΛt −\nηtρ\n2(ρ + ℓ)E [Dρ+ℓ(xt)] +\nρη2\nt σ2\n2(1 − ηtℓ) ≤ Λt − ηt\n3 E [D3ℓ(xt)] + 2ℓη2\nt σ2.\n(17)\nIt remains to telescope and conclude the proof since\nT −1\nX\nt=0\nηtE [D3ℓ(xt)] ≤ 3Λ0 + 6ℓσ2\nT −1\nX\nt=0\nη2\nt ,\nwhere the Lyapunov function in the initial point can be bounded (setting η−1 = η0) as\nΛ0 = λ0 = Φ1/ρ(x0) − Φ∗ + η−1ρ(Φ(x0) − Φ∗) ≤ Φ1/ρ(x0) − Φ∗ + Φ(x0) − Φ∗.\nThe proof for constant step-size follows immediately from (3).\nTaming Nonconvex Stochastic Mirror Descent\nC\nProof of Theorem 4.5: High Probability Convergence to FOSP under\nSub-Gaussian Noise\nProof. We recall the definitions of λt = Φ1/ρ (xt) − Φ∗ + ηt−1ρ(Φ(xt) − Φ∗), ψt = ∇f(xt, ξt) − ∇F(xt), and invoke\n(15) from the proof of Theorem 4.3\nλt+1\n=\nλt −\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρηt⟨ψt, xt − xt+1⟩ − ρ(1 − ηtℓ)Dω(xt+1, xt)\n−ηtρ(ρ − ℓ)Dω(ˆxt, xt)\n≤\nλt −\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt) + ρηt⟨ψt, ˆxt − xt⟩ + ρη2\nt ∥ψt∥2\n∗\n2(1 − ηtℓ) − ηtρ(ρ − ℓ)Dω(ˆxt, xt).\n(18)\nDefine a (normalization) scalar w :=\nρ−ℓ\n6σ2ρη0 > 0, and a sequence St := PT −1\nτ=t Zτ, where\nZt := w\n\u0012\nλt+1 − λt +\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt)\n\u0013\n.\nNow we define the filtration Ft = {x0, ξ0, x1, . . . , ξt−1, xt} and compute the moment generating function (MGF)\nof Zt for any 0 ≤ t ≤ T − 1\nE [exp(Zt)|Ft]\n=\nE\n\u0014\nexp\n\u0012\nw\n\u0012\nλt+1 − λt +\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt)\n\u0013\u0013\n|Ft\n\u0015\n(18)\n≤\nE\n\"\nexp\n \nwρηt⟨ψt, ˆxt − xt⟩ + wρη2\nt ∥ψt∥2\n∗\n2(1 − ηtℓ) − wηtρ(ρ − ℓ)Dω(ˆxt, xt)\n!\n|Ft\n#\n=\nexp (−wηtρ(ρ − ℓ)Dω(ˆxt, xt)) E\n\"\nexp\n \nwρηt⟨ψt, ˆxt − xt⟩ + wρη2\nt ∥ψt∥2\n∗\n2(1 − ηtℓ)\n!\n|Ft\n#\n(i)\n≤\nexp (−wηtρ(ρ − ℓ)Dω(ˆxt, xt)) exp\n\u0012\n3σ2w2ρ2η2\nt ∥ˆxt − xt∥2 + 3σ2wρη2\nt\n2(1 − ηtℓ)\n\u0013\n(ii)\n≤\nexp\n\u0012 3σ2wρη2\nt\n2(1 − ηtℓ)\n\u0013\n,\nwhere in (i) we apply Lemma F.2, which uses that ∥ψt∥∗ is σ-sub-Gaussian. Inequality (ii) holds by the fact that\n∥ˆxt − xt∥2 ≤ 2Dω(ˆxt, xt), and the choice of w, which guarantess that 6σ2w2ρ2η2\nt ≤ wηtρ(ρ − ℓ) for any t ≥ 0.\nNow to compute the MGF of St we use derive\nE [exp(St)|Ft] = E [E [exp(St+1 + Zt)|Ft+1] |Ft] = E [exp(Zt)E [exp(St+1)|Ft+1] |Ft] .\nThus, by induction we have\nE [S0] ≤ exp\n \n3σ2ρw\n2\nT −1\nX\nt=0\nη2\nt\n(1 − ηtℓ)\n!\n≤ exp\n \n3σ2ρw\nT −1\nX\nt=0\nη2\nt\n!\n.\nwhere the last inequality holds by the condition ηt ≤ 1/(2ℓ). Consequently, by Markov’s inequality,\nPr\n \nS0 ≥ 3σ2ρ\nT −1\nX\nt=0\nwtη2\nt + log (1/β)\n!\n≤ β.\nThen with probability at least 1 − β, we have\nT −1\nX\nt=0\nw\n\u0012\nλt+1 − λt +\nηtρ\n2(ρ + ℓ)Dρ+ℓ(xt)\n\u0013\n≤ 3σ2wρ\nT −1\nX\nt=0\nη2\nt + log (1/β) .\nIlyas Fatkhullin, Niao He\nTelescoping the above inequality, setting ρ = 4ℓ, and dividing by the sum of step-sizes:\n1\nPT −1\nt=0 ηt\nT −1\nX\nt=0\nηt D5ℓ(xt)\n≤\nλ0 + 1\nw log (1/β) + 12σ2ℓ PT −1\nt=0 η2\nt\n2\n5\nPT −1\nt=0 ηt\n=\nλ0 + 8 η0σ2 log (1/β) + 12σ2ℓ PT −1\nt=0 η2\nt\n2\n5\nPT −1\nt=0 ηt\n.\nIt remains to bound the Lyapunov function in the initial point setting η−1 = η0:\nλ0 = Φ1/ρ(x0) − Φ∗ + η−1ρ(Φ(x0) − Φ∗) ≤ Φ1/ρ(x0) − Φ∗ + 2(Φ(x0) − Φ∗).\nTaming Nonconvex Stochastic Mirror Descent\nD\nProof of Theorem 4.7. Global Convergence under Generalized Proximal P L\nCondition\nProof of Theorem 4.7. Invoking (17), and substituting the value of ρ = 2ℓ and using ηt ≤\n1\n2ℓ, we derive under\nAssumption 4.6\nΛt+1 ≤ Λt − ηt\n3 E [D3ℓ(xt)] + 2ℓη2σ2 ≤ Λt − 2µηt\n3\nE\nh\n(Φ(xt) − Φ∗)\n2/αi\n+ 2ℓη2σ2,\nNotice that Φ(x) ≥ Φ1/ρ(x) for any x ∈ X, thus\nE [Φ(xt) − Φ∗] ≥\n1\n1 + ηt−1ρE\n\u0002\nΦ1/ρ(xt) − Φ∗\u0003\n+\nηt−1ρ\n1 + ηt−1ρE [Φ(xt) − Φ∗] =\nΛt\n1 + ηt−1ρ ≥ 1\n2Λt.\nBy Jensen’s inequality for z → z\n2/α, we have E\n\u0002\n(Φ(xt) − Φ∗)\n2/α\u0003\n≥ (E [Φ(xt) − Φ∗])\n2/α. Combining the above\ninequalities, we get E\n\u0002\n(Φ(xt) − Φ∗)\n2/α\u0003\n≥ 1\n2Λ\n2/α\nt\n. Thus, we can derive a recursion\nΛt+1 ≤ Λt − ηtµ\n3 Λ\n2/α\nt\n+ 2ℓσ2η2\nt .\nAssume that for τ = 0, . . . , t, we have Λτ ≥ ε (otherwise we have reached ε-accuracy). Then\nΛt+1 ≤\n \n1 − ηtµε\n2−α\nα\n3\n!\nΛt + 2ℓσ2η2\nt ,\nFor any T ≥ 0, we select the step-size sequence ηt as follows:\nηt =\n\n\n\n\n\n1\n2ℓ\nif t < ⌈T/2⌉ and T ≤\n6ℓ\nµ ε\n2−α\nα ,\n6\nµ ε\n2−α\nα\n\u0012\nt+ 12ℓ\nµ ε− 2−α\nα\n−⌈T/2⌉\n\u0013\notherwise.\nBy Lemma F.6, we have\nΛT +1 = O\n \nℓΛ0\nµ ε\n2−α\nα\nexp\n \n−µ ε\n2−α\nα T\nℓ\n!\n+\nℓσ2\nTµ2ε\n2(2−α)\nα\n!\n.\nRecalling the definition of Λt and using Lemma F.3, we bound ΛT +1 ≥ Φ1/ρ(xT +1) − Φ∗ ≥ Φ(x+\nT +1) − Φ∗, where\nx+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + ℓDω(y, x). The sample complexity to reach Φ(x+\nT +1) − Φ∗ ≤ ε is\nT = O\n\u0012ℓΛ0\nµ\n1\nε\n2−α\nα\nlog\n\u0012ℓΛ0\nµε\n\u0013\n+ ℓΛ0σ2\nµ2\n1\nε\n4−α\nα\n\u0013\n.\nIlyas Fatkhullin, Niao He\nE\nProofs for Applications\nE.1\nDifferentially Private Learning in ℓ2 and ℓ1 Settings\nProof of Corollary 5.3. Notice that by Lemma F.7 ∥bt∥∞ is σ-sub-Gaussian r.v. with σ2 = 2 log(2d)σ2\nG.20 We\ninvoke the result of Theorem 4.5 with ηt = η0 =\n1\n2ℓ, and obtain\n1\nT\nT −1\nX\nt=0\nD5ℓ(xt)\n=\nO\n\n λ0\nη0T + σ2η0ℓ +\nσ2 log\n\u0010\n1\nβ\n\u0011\nT\n\n = O\n\u0012ℓλ0\nT\n+ σ2 log\n\u0012 1\nβ\n\u0013\u0013\n=\nO\n \nℓλ0\nT\n+ G2T log(d) log\n\u0000 1\nδ\n\u0001\nn2ϵ2\nlog\n\u0012 1\nβ\n\u0013!\n=\nO\n\n\n\n\nG\nr\nℓλ0 log(d) log\n\u0000 1\nδ\n\u0001\nlog\n\u0010\n1\nβ\n\u0011\nnϵ\n\n\n\n ,\nwhere the last equality follows by the choice of T. It remains to notice that λ0 = Φ1/ρ(x0) − Φ∗ + 2(Φ(x0) − Φ∗) ≤\n3(Φ(x0) − Φ∗).\nCorollary E.1. Let F(·) be differentiable on a convex set X with L-Lipschitz continuous gradient w.r.t. Euclidean\nnorm, and ∥∇F(x)∥2 ≤ G for all x ∈ X. Set ηt =\n1\n2L, T =\nnϵ\n√\nL\nG√\nd log(1/δ) log(1/β), λ0 := Φ(x0) − Φ∗. Then\nDP-Prox-GD is (ϵ, δ)-DP and with probability 1 − β satisfies\n1\nT\nT −1\nX\nt=0\nD5ℓ(xt) = O\n \nG\np\nℓλ0d log (1/δ) log (1/β)\nnϵ\n!\n,\nProof. The proof follows the same lines as the proof of Corollary 5.3. The only difference is that instead of the\ninfinity norm of the noise, we bound the Euclidean norm, i.e., ∥bt∥2 is σ-sub-Gaussian r.v. with σ2 = d σ2\nG.\nE.2\nPolicy Optimization in Reinforecement Learning\nProx-P L condition.\nNow we will verify Assumption 4.6 with α = 1 holds for our RL problem. The result is similar to Lemma 5 in\n[Xiao, 2022]. The only difference is that we have π instead of π+ on the left hand side of the inequality.\nLemma E.2. Let ω(π) = 1\n2 ∥π∥2\n2,2. Then for any π ∈ X we have\nVp(π) − V ⋆\np ≤ 2\np\n2|S|\n1 − γ\n\r\r\r\r\ndp (π⋆)\nµ\n\r\r\r\r\n∞\nq\n∆+\nρ (π)\nif ρ ≥ GV,∥·∥2,2/DX,∥·∥2,2, where GV,∥·∥2,2 := maxπ∈Π\nh\n∥∇Vp(π)∥2,2\ni\n.\nProof. It was shown in Lemma 4 in [Agarwal et al., 2021] that the following (variational) gradient domination\ncondition holds.\nVp(π) − V ⋆\np ≤\n1\n1 − γ\n\r\r\r\r\ndp (π⋆)\nµ\n\r\r\r\r\n∞\nmax\nπ′∈X ⟨∇Vµ(π), π − π′⟩\nfor any π ∈ X.\nBy Lemma F.5, we have\nmax\nπ′∈X ⟨∇Vµ(π), π − π′⟩ ≤\n\u0010\nDX,∥·∥2,2 + ρ−1GV,∥·∥2,2\n\u0011 q\n∆+\nρ (π) ≤ 2DX,∥·∥2,2\nq\n∆+\nρ (π)\n20Here we used the fact that max1≤i≤d |ξi| = max {ξ1, −ξ1, . . . , ξd, −ξd} and applied Lemma F.7 with n = 2d.\nTaming Nonconvex Stochastic Mirror Descent\nwhere the last inequality holds for ρ ≥ GV,∥·∥2,2/DX,∥·∥2,2. It remains to notice that DX,∥·∥2,2 ≤\np\n2|S|.\nSmoothness in (2, 1)-norm. Proof of Proposition 5.5\nFor any π, π′ ∈ X, it holds that\n∥∇Vp(π) − ∇Vp (π′)∥2,∞ ≤\n2γ\n(1 − γ)3 ∥π − π′∥2,1 ,\nThe estimate of the smoothness constant follows directly from the proof of Lemma 54 in [Agarwal et al., 2021],\nsince using (2, 1) norm we have P\na∈A |ua,s| ≤ 1, and the perturbation ua,s belongs to the probability simplex\nua,s ∈ ∆(A).\nE.3\nTraining Autoencoder Model using SMD\nDerivation of SMDr1 and SMDr2. Recall the choice of DGF from subsection 5.3\nω(x) =\n1\nr + 2∥x∥r+2\n2\n+ 1\n2∥x∥2\n2.\nNotice that we have ∇ω(x) = ∥x∥r\n2 x + x. The update rule of SMD with X = Rd, r(x) = 0 and the above choice\nof DGF satisfies\nηt∇f(xt, ξt) + ∇ω(xt+1) − ∇ω(xt) = 0.\nDefine ct := ∇ω(xt) − ηt∇f(xt, ξt) = xt − ηt∇f(xt, ξt) + ∥xt∥r\n2 xt. Thus, it remains to solve for xt+1\n∇ω(xt+1) = (∥xt+1∥r\n2 + 1) xt+1 = ct,\n(19)\nwhich is equivalent to solving the following simple univariate equation of θ ≥ 0:\nθr+1 + θ = ∥ct∥2 .\n(20)\nFor r = 1, 2, it has an explicit form solution for any ∥ct∥2 . We have\nθ∗ = −1 +\np\n1 + 4 ∥ct∥2\n2\nfor r = 1.\nand obtain the following method\nct\n=\nxt − ηt∇f(xt, ξt) + ∥xt∥2 xt,\nxt+1\n=\n2ct\n1 +\np\n1 + 4 ∥ct∥2\n.\nFor r = 2, an explicit form solution can be written using Cardano’s formula. We use Python Sympy library for\nsymbolic calculation to solve for θ in this case.\nMore generally, (19) implies for any r > 0, we have\nct\n=\n(1 + ∥x∥r\n2) xt − ηt∇f(xt, ξt),\nxt+1\n=\nct\n1 + θr∗\n,\nwhere θ∗ is the solution to (20), which can be solved using a bisection method up to the machine accuracy.\nCorollary E.3. Let F(·) : Rd → R be twice differentiable and satisfy (8). Let Assumption 3.2 hold with ∥·∥2.\nSuppose the sequence {ηt}t≥0 be non-increasing with η0 ≤ 1/(2ℓ), and ¯xT ∈ X be randomly chosen from the\niterates x0, . . . , xT −1 with probabilities pt = ηt/ PT −1\nt=0 ηt. Then for (9), (10), we have\nE\nh\n∥∇F(¯xT )∥2\n2\ni\n≤ 6(F(x0) − F ∗) + 6ℓσ2 PT −1\nt=0 η2\nt\nPT −1\nt=0 ηt\n,\nwhere F ∗ := miny∈Rd F(y).\nIlyas Fatkhullin, Niao He\nAdditional experimental details. We use Fashion-MNIST dataset [Xiao et al., 2017] for training with images\nof dimensions df = 28 × 28 = 784. The encoding dimension is fixed to de = 64. The dataset is of size 50000\nimages. In all experiments, we use the mini-batch of size 100. We initialize the parameters W of the model with\na normal distribution with mean 1 and the standard deviation 0.01.\nRemark E.4. A momentum variant of the scheme (9), (10) was recently explored in [Ding et al., 2023] with\npromising empirical results on image classification and language modeling tasks. We hope that our simpler variant\nwithout momentum can be also helpful in these tasks.\nAdditional discussion about (L0, L1)-smoothness. Recently, some works, e.g., [Zhang et al., 2020a, Faw\net al., 2023, H¨ubler et al., 2023], consider adaptive gradient methods such as gradient clipping, AdaGrad-Norm\nand gradient normalization under (L0, L1)-smoothness, i.e., F(·) is twice differentiable and for some L0, L1 ≥ 0\nsatisfies ∥∇F(x)∥op ≤ L0 + L1 ∥∇F(x)∥2 for all x ∈ Rd. The authors in [Zhang et al., 2020a, Faw et al., 2023]\njustify the theoretical benefits of the popular adaptive schemes by the fact that, unlike SGD, they provably work\nunder this weaker (L0, L1)-smoothness. Moreover, Zhang et al. [2020a] empirically verify that (L0, L1)-smoothness\ncondition holds on the optimization trajectory when training modern language and image classification models.\nOur polynomial grow condition is weaker than (L0, L1)-smoothness as long as the gradient norm grows at most as\na polynomial in ∥x∥2. Unlike the approach taken in the above mentioned works, the convergence of our algorithm\nwith the choice of DGF as in Proposition 5.9 follows directly from Theorem 4.3 and does not require a separate\nanalysis.\nTaming Nonconvex Stochastic Mirror Descent\nF\nUseful Lemma\nThe following lemma is standard [Lu et al., 2018] and the proof can be found, e.g., in [Chen and Teboulle, 1993].\nLemma F.1.\n1. The Bregman divergence satisfies the three-point identity:\nDω(x, y) + Dω(y, z) = Dω(x, z) + ⟨∇ω(z) − ∇ω(y), x − y⟩\nfor all y, z ∈ S and x ∈ cl(S).\n2. Let ϕ(·) be a closed proper convex function on Rd, z ∈ S and z+ := arg minx∈X {ϕ(x) + ρDω(x, z)} for ρ > 0,\nthen\nϕ(x) + ρDω(x, z) ≥ ϕ(z+) + ρDω(z+, z) + ρDω(x, z+)\nfor all x ∈ cl(S).\nTo establish high probability convergence, we use the technical lemma by Liu et al. [2023].\nLemma F.2 (Lemma 2.2. in [Liu et al., 2023]). Suppose X ∈ Rd such that E[X] = 0 and ∥X∥∗ is a σ-sub-Gaussian\nrandom variable, then for any a ∈ Rd, 0 ≤ b ≤\n1\n2σ,\nE\n\u0002\nexp\n\u0000⟨a, X⟩ + b2∥X∥2\n∗\n\u0001\u0003\n≤ exp\n\u00003\n\u0000∥a∥2 + b2\u0001\nσ2\u0001\n.\nThe following lemma shows the connection between Φ1/ρ and Φ. Similar result in the Euclidean setting has\npreviously appeared, e.g., in [Stella et al., 2017].\nLemma F.3. Let F(·) be (ℓ, ω)-smooth. Then for any ρ ≥ 2ℓ and x ∈ X ∩ S we have Φ1/ρ(x) ≥ Φ(x+), where\nx+ := arg miny∈X ⟨∇F(x), y⟩ + r(y) + (ρ − ℓ)Dω(y, x).\nProof. By Assumption 3.1 (lower bound), we have for any x, y ∈ X ∩ S\nΦ(y) + ρDω(y, x) ≥ F(x) + ⟨∇F(x), y − x⟩ + r(y) + (ρ − ℓ)Dω(y, x).\nMinimizing both sides over y ∈ X ∩ S, we have\nΦ1/ρ(x)\n≥\nF(x) + ⟨∇F(x), x+ − x⟩ + r(x+) + (ρ − ℓ)Dω(x+, x)\n≥\nF(x+) + r(x+) + (ρ − 2ℓ)Dω(x+, x) ≥ Φ(x+),\nwhere the first equality holds by the definitions of Φ1/ρ and x+. The second inequality uses Assumption 3.1\n(upper bound).\nThe following lemma shows that our Assumption 4.6 is more general than relative strong convexity [Lu et al.,\n2018]. In the Euclidean case, the same result was derived by Karimi et al. [2016].\nLemma F.4 (Relative strong convexity implies 2-Bregman Prox-P L). Let F(·) be µ-relatively strongly convex\nw.r.t. ω(·), i.e., for all x, y ∈ X ∩ S\nF(y) ≥ F(x) + ⟨∇F(x), y − x⟩ + µDω(y, x).\n(21)\nThen Assumption 4.6 holds wth α = 2 and any ρ ≥ µ, i.e., Dρ(x) ≥ 2µ (Φ(x) − Φ∗) .\nProof. Adding r(y) to both sides of (21), we have\nΦ(y) ≥ Φ(x) + ⟨∇F(x), y − x⟩ + µDω(y, x) + r(y) − r(x) = Φ(x) + Qµ(x, y).\nMinimizing both sides over y ∈ X ∩ S, we get\nΦ∗ ≥ Φ(x) + min\ny∈X Qµ(x, y) = Φ(x) − 1\n2µDµ(x).\nRearranging and noticing that Dµ(x) ≤ Dρ(x) for any x ∈ X and ρ ≥ µ, we obtain the result.\nIlyas Fatkhullin, Niao He\nThe following lemma connects the Frank-Wolfe gap with the norm of the gradient mapping in the Euclidean case.\nLemma F.5 (Lemma 2.2 in [Balasubramanian and Ghadimi, 2022]). Let ω(x) := 1\n2 ∥x∥2\n2, X be a compact set\nwith diameter DX,∥·∥2 := maxx,y∈X ∥x − y∥2 and r(·) = 0. Then for any ρ > 0\nmax\ny∈X ⟨∇F(x), x − y⟩ ≤\n\u0000DX,∥·∥2 + ρ−1GF,∥·∥2\n\u0001 q\n∆+\nρ (x),\nwhere GF,∥·∥2 := maxx∈X ∥∇F(x)∥2.\nWe report the special case of Lemma 3 by Stich [2019].\nLemma F.6 (Lemma 3 in [Stich, 2019]). Let {rt}t≥0 and {ηt}t≥0 be two non-negative sequences with ηt ≤ 1\nd\nthat satisfy the relation\nrt+1 ≤ (1 − aηt) rt + c η2\nt ,\nwhere a > 0, c ≥ 0. For any T ≥ 0, set\nηt =\n( 1\nd\nif t < ⌈T/2⌉ and T ≤ 2d\na ,\n1\na( 2d\na +t−⌈T/2⌉)\notherwise.\nThen we have\nrt+1 ≤ 32 d r0\na\nexp\n\u0012\n−aT\n2d\n\u0013\n+ 36 c\na2T .\nThe next lemma is standard and the proof can be found, e.g., in [Van Handel, 2014].\nLemma F.7 (Maximal tail inequality, Lemma 5.1 and 5.2 in [Van Handel, 2014]). Let ξi be a σ-sub-Gaussian\nrandom variable for every i = 1, . . . , n. Then\n\u0012\nE\n\u0014\nmax\n1≤i≤n ξi\n\u0015\u00132\n≤ E\n\u0014\nmax\n1≤i≤n ξ2\ni\n\u0015\n≤ 2σ2 log(n),\nPr\n\u0012\nmax\n1≤i≤n ξi ≥\np\n2σ2 log(n) + λ\n\u0013\n≤ e− λ2\n2σ2\nfor all λ ≥ 0.\n"
}