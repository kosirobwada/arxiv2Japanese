{
    "optim": "Unleashing the Potential of Large Language Models as Prompt Optimizers:\nAn Analogical Analysis with Gradient-based Model Optimizers\nXinyu Tang1,3*, Xiaolei Wang1,3*, Wayne Xin Zhao1,3†,\nSiyuan Lu4 , Yaliang Li5 and Ji-Rong Wen1,2,3\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3Beijing Key Laboratory of Big Data Management and Analysis Methods\n4International School, Beijing University of Posts and Telecommunications 5Alibaba Group\ntxy20010310@163.com, wxl1999@foxmail.com, batmanfly@gmail.com\nAbstract\nAutomatic prompt optimization is an important\napproach to improving the performance of large\nlanguage models (LLMs).\nRecent research\ndemonstrates the potential of using LLMs as\nprompt optimizers, which can generate im-\nproved task prompts via iterative refinement. In\nthis paper, we propose a novel perspective to in-\nvestigate the design of LLM-based prompt op-\ntimizers, by drawing an analogy with gradient-\nbased model optimizers. To connect these two\napproaches, we identify two pivotal factors in\nmodel parameter learning: update direction and\nupdate method. Focused on the two aspects,\nwe borrow the theoretical framework and learn-\ning methods from gradient-based optimization\nto design improved strategies for LLM-based\nprompt optimizers. By systematically analyz-\ning a rich set of improvement strategies, we fur-\nther develop a capable Gradient-inspired LLM-\nbased Prompt Optimizer called GPO. At each\nstep, it first retrieves relevant prompts from\nthe optimization trajectory as the update direc-\ntion. Then, it utilizes the generation-based re-\nfinement strategy to perform the update, while\ncontrolling the edit distance through a cosine-\nbased decay strategy. Extensive experiments\ndemonstrate the effectiveness and efficiency\nof GPO. In particular, GPO brings an addi-\ntional improvement of up to 56.8% on Big-\nBench Hard and 55.3% on MMLU compared\nto baseline methods. The code is available at\nhttps://github.com/RUCAIBox/GPO.\n1\nIntroduction\nNowadays, prompting has become the pivotal ap-\nproach to unleashing the power of large language\nmodels (LLMs) (Zhao et al., 2023; Touvron et al.,\n2023; OpenAI, 2023). However, prompt engineer-\ning is not easy and requires extensive trial-and-error\nefforts since LLMs are sensitive to prompts (Zhao\n* Equal contribution.\n† Corresponding author.\net al., 2021; Lu et al., 2022; Wei et al., 2023). Al-\nthough general guidelines for high-quality prompts\nexist (Kojima et al., 2022; Amatriain, 2024), they\ncannot always lead to optimal task performance.\nTo improve the task performance of LLMs,\nautomatic prompt optimization has been pro-\nposed (Zhou et al., 2023). Early work either di-\nrectly performs discrete optimization through meth-\nods like reinforcement learning (Deng et al., 2022;\nZhang et al., 2023) or performs continuous opti-\nmization in the embedding space of LLMs (Shin\net al., 2020; Lin et al., 2023; Chen et al., 2023).\nHowever, these methods often require access to\nthe logits or internal states of LLMs, which is in-\nfeasible for those only accessible through APIs.\nIn addition, they need to be specially trained for\neach task. Considering these issues, recent work\nproposes to model the optimization problem in nat-\nural language and using LLMs as prompt optimiz-\ners (Zhou et al., 2023; Yang et al., 2023; Ma et al.,\n2024). In this approach, LLMs can perform op-\ntimization with only outputs from the task model\nand quickly adapt to various tasks without training.\nHowever, such a method raises a new challenge for\nthe design of meta-prompt, which is the prompt for\nLLMs to perform prompt optimization.\nTo tackle this issue, we aim to investigate the\ndesign of meta-prompts. In existing approaches,\nmeta-prompts are often hand-crafted (Yang et al.,\n2023) or optimized with heuristic algorithms (Fer-\nnando et al., 2023). Despite the flexibility, these\nstudies still lack principled guidelines about their\ndesigns. Our work is inspired by the great success\nof gradient-based optimizers in model optimiza-\ntion, which have been systemically studied in both\ntheory and practice (Sun et al., 2020; Abdulkadirov\net al., 2023). Since both optimizers target enhanc-\ning model performance through optimization, it is\nfeasible to connect the two different approaches via\nanalogical analysis. In this way, we can borrow the\ntheoretical framework and extensive research of\n1\narXiv:2402.17564v1  [cs.CL]  27 Feb 2024\ngradient-based model optimizers to enhance LLM-\nbased prompt optimizers.\nSpecifically, in this paper, we take the gradient-\nbased optimization approach from machine learn-\ning as reference, and identify two key factors for\nmodel parameter learning, namely update direc-\ntion (i.e., gradient calculation) and update method\n(i.e., gradient descent). By analogy, in LLM-based\nprompt optimizers, the update direction can refer\nto suitable prompt information informing the LLM\nabout how to produce more effective task prompts,\nwhile the update method can refer to how the LLM\nutilizes such information to improve prompts. By\ndrawing such an analogy, we can develop a more\nformal framework for LLM-based prompt optimiz-\ners and provide guidelines to design more princi-\npled meta-prompts.\nFurthermore, we develop a capable Gradient-\ninspired LLM-based Prompt Optimizer called\nGPO, with the best configuration from our system-\natic study. We evaluate its performance across com-\nplex reasoning, knowledge-intensive, and common\nNLP tasks in various evaluation settings. When\nusing Llama-2-7b-chat as the task model, the\nprompts produced by GPO surpass the instruction\n“Let’s think step by step” by 18.5% on Big-Bench\nHard (BBH) and 7.6% on MMLU. Furthermore,\nGPO produces an additional improvement of up to\n56.8% on BBH and 55.3% on MMLU compared\nwith baseline methods while using fewer tokens.\nTo the best of our knowledge, it is the first time\nthat a systematic study has been conducted for\nLLM-based prompt optimizers. More specifically,\nit has been studied by analogy with gradient-based\nmodel optimizers, which we believe is useful to\nseek theoretical foundations and extend feasible\napproaches for prompt optimization.\n2\nRelated Work\nOur work is related to the following two directions.\nPrompt Engineering and Optimization. Prompt\nengineering aims to find suitable prompts as the\ninput for LLMs to perform various tasks. To re-\nduce human efforts, researchers have explored au-\ntomatic prompt optimization, which can be cate-\ngorized into continuous and discrete optimization\nmethods. Discrete methods (Deng et al., 2022; Xu\net al., 2022) directly optimize the natural language\nprompts through methods like reinforcement learn-\ning (Deng et al., 2022; Zhang et al., 2023) and\nediting (Xu et al., 2022; Prasad et al., 2023). In\ncontrast, continuous methods perform optimization\nin the embedding space of LLMs, allowing for op-\ntimization through gradient (Li and Liang, 2021;\nLester et al., 2021). We focus on discrete methods,\nespecially LLM-based prompt optimizers.\nLLM-based Prompt Optimizers. Due to the un-\nprecedented capabilities of LLMs, recent work\nstarts to utilize them as prompt optimizers. One\nline of work (Guo et al., 2023; Yang and Li, 2023)\ncombines LLMs with evolutionary algorithms to\nperform prompt optimization.\nAnother line of\nwork (Pryzant et al., 2023; Yang et al., 2023) aims\nto adapt concepts and techniques from gradient-\nbased model optimizers (e.g., gradient (Pryzant\net al., 2023) and momentum (Yang et al., 2023))\nto LLM-based prompt optimizers. However, no\ncomprehensive guidelines exist for using LLMs\nas prompt optimizers. We aim to tackle this with\na systematic investigation, which is conducted by\nanalogy with gradient-based model optimizers.\n3\nAn Analogical Analysis Between\nGradient-Based Model Optimizer and\nLLM-Based Prompt Optimizer\nIn this section, we present an analogical analysis\nbetween model optimization and prompt optimiza-\ntion to build their connections and further improve\nexisting LLM-based prompt optimizers.\n3.1\nTask Formulation\nPrompt optimization aims to find the optimal task\nprompt p∗ in the format of natural language that\nmaximizes the performance on a specific task\ndataset D when using an LLM as the task model\nMT . To perform such optimization, our idea is\nto develop a prompt optimizer, which can be built\nupon some search algorithm (e.g., evolutionary al-\ngorithms (Guo et al., 2023)) or an LLM (Yang et al.,\n2023). In this paper, we focus on using an LLM as\nthe prompt optimizer MO. Formally, the problem\nof prompt optimization can be formulated as:\np∗ = arg max\np∼MO\nE⟨x,y⟩∈D [F(MT (x; p), y)], (1)\nwhere p is the prompt generated by the LLM-based\nprompt optimizer MO, MT (x; p) represents the\noutput from the task model for input x conditioned\non the prompt p, and the function F(·) calculates\nthe task performance based on some measurement.\nFor the LLM-based prompt optimizer, it requires\nanother prompt to perform the optimization for\n2\nthe task prompt, which is usually called meta-\nprompt (Yang et al., 2023; Ye et al., 2023). For\nexample, for the mathematical reasoning task, the\nprompt can be “Let’s solve the problem”, while the\nmeta-prompt may be “Improve the prompt to help\na model better perform mathematical reasoning”.\n3.2\nAnalogical Analysis for Prompt\nOptimization\nSince both prompt optimizers and model opti-\nmizers target enhancing the model performance\nthrough some optimization algorithm, in this part,\nwe aim to draw inspiration from the design of\ngradient-based model optimizers to conduct a sys-\ntematic analysis of LLM-based prompt optimizers.\n3.2.1\nRevisiting Gradient-Based Optimizers\nSimilar to prompt optimization, model optimiza-\ntion aims to find the optimal values of model pa-\nrameters that minimize the loss function. In model\noptimization, gradient-based optimizers are the\nmost widely used approaches, which iteratively\nupdate model parameters in the direction of the\nnegative gradient of the loss function. To moti-\nvate our approach, we take the fundamental opti-\nmizer known as gradient descent (Boyd and Van-\ndenberghe, 2014) for discussion.\nIn the basic form of gradient descent, a single\noptimization step can be formulated as follows:\nΘk+1 = Θk − τkgk,\n(2)\nwhere Θk and Θk+1 are the values of model pa-\nrameters at the last and current steps, τk and gk are\nthe learning rate and gradient at the current step.\nGradient descent can be improved by focusing on\ntwo elements in the formula: τk and gk. For τk,\nlearning rate schedulers (Gotmare et al., 2019) are\nproposed to dynamically adjust the learning rate.\nFor gk, the concept of momentum (Sutskever et al.,\n2013) is introduced to include historical gradients,\nand its computation can be expressed as follows:\nvk+1 = βvk + gk = Pk\ni=1 βk−igi, where β repre-\nsents the momentum coefficient.\nDespite various gradient-based optimizers, they\nmainly model two key factors, namely update di-\nrection (e.g., gradient gk) and update method (e.g.,\ndirect descent update by subtracting τkgk). Our\napproach is inspired by the observation that ex-\nisting LLM-based prompt optimization methods\nalso implicitly employ the two aspects. For exam-\nple, OPRO (Yang et al., 2023) uses previous task\nprompts along with their performance to guide the\nFactor\nGradient-based\nmodel optimizer\nLLM-based\nprompt optimizer\nUpdate\ndirection\nModel value\nPrompt\nGradient\nReflection\nMomentum\nTrajectory\nUpdate\nmethod\nLearning rate\nEdit distance\nDescent\nEditing / Generation\nTable 1: Analogy between glossaries in model optimizer\nand prompt optimizer.\ndirection of update and utilizes in-context learn-\ning based generation as the update method. The\ncomparison between glossaries is shown in Table 1.\nHowever, the efforts in existing work only initially\nexplore the design of the two key factors, and we\naim to conduct more in-depth and systematic in-\nvestigations by borrowing the idea of research in\ngradient-based optimization.\n3.2.2\nAnalogical Prompt Optimization\nStrategies for “Update Direction”\nIn prompt optimization, there are no explicit gradi-\nents for controlling the update direction. However,\nwe can incorporate the concept of gradient into\nthe design of meta-prompts to improve the prompt\noptimization process.\nAnalogical “Gradient” Forms. The basic func-\ntion of the gradient is to inform how the optimiza-\ntion process should adjust according to the model\nperformance. To mimic similar effects, for an LLM\nwith fixed parameters, it is only feasible to design\nsuitable prompting strategies to adjust the output.\nHere, we consider two analogical forms to implic-\nitly support the gradient-like function.\n• Prompt+performance. One straightforward\nmethod is to include the last-round task prompt\nand the corresponding model performance into the\nmeta-prompt for LLM-based optimizer MO. It\nleverages the capacity of LLMs to reason about\nhow to improve prompting optimization.\n• Prompt+performance+reflection.\nAnother\nway to solve the barrier of the gradient is to lever-\nage the reflection capability of LLMs (Pryzant\net al., 2023). With the reflection mechanism, LLMs\ncan generate feedback from past failures, which\ncan be used to improve performance. Such feed-\nback can be seen as a form of “semantic” gradient\nsignals (Pryzant et al., 2023).\nAnalogical “Momentum” Forms. Inspired by the\nmomentum method (Sutskever et al., 2013) in gradi-\n3\nent descent, we consider enhancing the aforemen-\ntioned basic form of meta-prompt by leveraging\nthe intermediate results accumulated in the prompt\noptimization process. A straightforward way is to\ndirectly include them into the meta-prompt. How-\never, it might be limited by the context length of\nLLMs and affected by the accumulated noise. To\nbetter utilize the optimization trajectory, we pro-\npose two alternative methods.\n• Summarization-based trajectory. One direct\napproach is to summarize the intermediate results\nfrom the optimization trajectory. Specifically, at\neach step, we use an instruction (e.g., “Your task is\nto summarize the problems in the previous prompt\nand the current prompt.”) to let the LLM perform\nsummarization using the summary in the last step\nand the result in the current step.\n• Retrieval-based trajectory. Another approach\nis to dynamically retrieve k pieces of gradients\nfrom the optimization trajectory. Specifically, we\nconsider the following three strategies for selection:\n(1) recency: selecting k nearest gradients; (2) rele-\nvance: selecting k most relevant gradients, which\nare measured by the semantic similarity based on\nthe BGE model (Xiao et al., 2023); (3) importance:\nselecting k most important gradients, which is mea-\nsured by the performance gain.\n3.2.3\nAnalogical Prompt Optimization\nStrategies for “Update Method”\nAnother important factor to consider is the update\nmethod for prompt optimization. In gradient-based\nmodel optimizers, the optimization process is con-\ntrolled by the learning rate and fulfilled by gradient\ndescent on parameters. Accordingly, we explore\ntwo analogical aspects: prompt variation control\nand meta-prompt refinement.\nPrompt Variation Control. After setting the opti-\nmization direction, it is important to adjust the pa-\nrameter optimization process with a suitable learn-\ning rate. This is because using a either too large\nor too small rate can result in an oscillating or\nslowly converging optimization process. Similarly,\nwithout a suitable control mechanism in prompt\noptimization, the LLM-based optimizer might over-\nshoot the optimal prompt or oscillate in the opti-\nmization process. To mimic the similar effects of\nlearning rate, our idea is to control the variation\ndegree of prompt optimization, which is measured\nby the edit distance between two task prompts at\nconsecutive iterations. Specifically, following Ye\net al. (2023), we add an instruction (i.e., “You are\nallowed to change up to X words in the current\nprompt.”) into the meta-prompt to limit the num-\nber of words that can be modified. Accordingly,\nwe propose two control methods as follows:\n• Decay-based constraint. To avoid overshoot-\ning the optimal solution, the decay strategy is pro-\nposed to gradually reduce the learning rate (Iz-\nmailov et al., 2018). Here, we borrow the idea\nto control the maximum edit distance and consider\ngradually reducing its value following either a lin-\near or cosine curve. In addition, following the\ncommon practice in training LLMs (Touvron et al.,\n2023), we reduce the constraint to approximately\n20% of its maximum value until convergence.\n• Warmup-based constraint. To avoid instability\nin the early stage of optimization, the warmup strat-\negy is proposed to gradually increase the learning\nrate at the beginning (Goyal et al., 2017). Similarly,\nwe adopt the widely used linear warmup schedule\nto gradually increase the constraint for the maxi-\nmum edit distance to its initially set value in the\ninitial 5% steps.\nPrompt Refinement Strategy. During the opti-\nmization process, the task prompt would be itera-\ntively refined, to improve the corresponding model\nperformance. By analogy with the subtraction op-\neration in gradient-based optimizers (i.e., −τkgk in\nEq. (2)), we introduce two methods to update the\ntask prompt accordingly.\n• Editing-based refinement. The first method\ndirectly edits the last-round task prompt to improve\nperformance. Specifically, we add an instruction\n(i.e., “Modify the current prompt and get a new\nimproved promp”) into the meta-prompt, which\nrequires the LLM to edit the task prompt from the\nlast step according to the update direction. This\nmethod allows for effectively exploiting the exist-\ning prompt, leading to a gradual performance im-\nprovement through a relatively stable optimization\nprocess.\n• Generation-based refinement. In addition to\ndirect edits, we can also leverage the in-context\nlearning capability of LLMs to generate refined\ntask prompts. Specifically, we present the informa-\ntion regarding the update direction in the format\nof demonstration (e.g., “Below are the previous\nprompts with their scores. The score ranges from 0\nto 100, and a higher score indicates better quality.\nPrompt: {prompt}. Score: {score}.”). Then, the\nLLM can follow the demonstration to generate a\n4\nInitial prompt: “Let’s think step by step.” Performance: 31.25\nGradient\nform\nMomentum form\nNone\nS-T\nR-T\nRecency\nRelevance\nImportance\nP+M\n41.07\n41.03\n41.93\n42.53\n41.84\nP+M+R\n40.34\n40.63\n41.55\n40.81\n39.73\nTable 2: The performance comparison between differ-\nent update directions. “P” denotes prompt, “M” de-\nnotes performance, “R” denotes reflection, “S-T” de-\nnotes summarization-based trajectory, and “R-T” de-\nnotes retrieval-based trajectory.\nnew task prompt. Compared with the editing-based\nupdating method, the generation-based approach\nexplores a wider range of prompt variations, which\nhas the potential to yield improved performance\nbut also makes the optimization process unstable.\n3.3\nAnalogical Analysis Experiments\nIn this part, we first describe the experiment set-\nting for our analogical analysis, and then detail our\nfindings from the experiment.\n3.3.1\nExperiment Setup\nWe select a dataset from each type of task in\nBBH (Suzgun et al., 2023) to create a lite BBH\nbenchmark for our analysis: i) Navigate (binary\nchoice); ii) Movie Recommendation (multiple\nchoice); iii) Object Counting (numeric response);\niv) Word Sorting (free response). We adopt ex-\nact match as the metric for performance evalu-\nation. We employ Llama-2-7b-chat (Touvron\net al., 2023) as the task model and gpt-3.5-turbo\nas the prompt optimizer. The initial task prompt\nis “Let’s think step by step.” Other details are pre-\nsented in Appendix B.\n3.3.2\nEmpirical Findings\nFindings For Update Direction. The results for\nthe update direction of prompt optimization are\npresented in Table 2. Here are the main findings:\n• With regard to the form of gradient, prompt+\nperformance achieves better performance than\nprompt+performance+reflection, which brings an\nimprovement of up to 31% compared with the ini-\ntial task prompt.\nThe substantial improvement\nbrought by prompts can be attributed to their rich\nsemantic information about the task, which can\nactivate the task-specific knowledge of LLMs for\noptimization. In contrast, LLMs are known to be\nInitial prompt: “Let’s think step by step.” Performance: 31.25\nRefinement strategy\nEditing\nGeneration\nVariation\nControl\nNo control\n42.53\n42.61\nFixed\n42.91\n43.09\n+Warmup\n41.76\n40.08\nLinear decay\n42.68\n42.86\n+Warmup\n41.47\n41.12\nCosine decay\n42.95\n43.75\n+Warmup\n40.19\n41.29\nTable 3: The performance comparison between different\nupdate methods.\nlimited in their capabilities of reflection (Huang\net al., 2023), which may lead to inaccurate updates.\n• The analogical concept of momentum can fur-\nther improve the performance of prompt optimiza-\ntion. Among various designs, relevance-based tra-\njectory emerges as the most effective one, which\nbrings an additional 15% improvement. This can\nbe attributed to the fact that LLMs can learn more\nfrom prompts that are contextually relevant, while\nit might be challenging for LLMs to fully under-\nstand the signal of recency or importance. By con-\ntrast, the summarization-based trajectory proves to\nbe less helpful. This is because summarization only\ncaptures common aspects of the trajectory while\nneglecting details that may be crucial.\nFindings For Update Method. To investigate\nthe update method for prompt optimization, we\nconduct experiments using the best configuration\nfound in the previous experiments. The results\nfor the update method of prompt optimization are\npresented in Table 3. Here are the key findings:\n• In general, generation-based refinement per-\nforms better than editing-based refinement, which\nbrings an improvement of up to 36% compared\nwith the initial task prompt. This may be because\nLLMs are not specially trained for prompt opti-\nmization, and the demonstrations in the generation-\nbased strategy can provide guidance about this task,\nthus helping LLMs learn to perform better.\n• Among various controlling methods for\nprompt variation, cosine decay-based constraint\nachieves the best performance, bringing an ad-\nditional 10% improvement.\nHowever, unlike\ngradient-based model optimization, the warmup\nstrategy does not yield improvement. This finding\nsuggests that, at the early stage of prompt optimiza-\ntion, exploration plays a crucial role, while stability\nbecomes more important in the later stage.\n5\n4\nOur Prompt Optimizer: GPO\nIn this section, we present our novel gradient-\ninspired LLM-based prompt optimizer called GPO,\nwhich leverages the insights gained from our sys-\ntematic study. Our approach proposes a novel de-\nsign of the meta-prompt, aiming to unleash the\npotential of LLMs as prompt optimizers.\nIterative Prompt Optimization. GPO performs\nprompt optimization through a multi-step iterative\nprocess. At each step, the LLM first generates\nmultiple candidate task prompts based on a meta-\nprompt. The meta-prompt serves as the input that\nguides the LLM in optimizing its prompts. Subse-\nquently, the task prompt with the best performance\nis selected for the next iteration. This iterative pro-\ncess continues until either the performance of the\ntask prompt reaches a plateau or the predefined\nmaximum number of optimization steps is reached.\nThe Design of Meta-Prompt. As the input to\nthe LLM, our meta-prompt consists of two key\ncomponents: update direction and update method.\n• Update direction. To determine the update\ndirection, we leverage the retrieval-based optimiza-\ntion trajectory in Section 3.2.2. This trajectory\ncomprises a collection of past task prompts, along\nwith their model performance. They are selected\nusing a relevance-based strategy and are sorted in\nascending order based on their performance.\n• Update method. After the update direction\nis determined, we further employ the generation-\nbased refinement strategy in Section 3.2.3 to up-\ndate the task prompt. Specifically, we present the\ntrajectory in the format of demonstration in the\nmeta-prompt. Then, the LLM can follow these\ndemonstrations to generate a new task prompt via\nin-context learning. Additionally, we implement\nthe cosine-based decay strategy in Section 3.2.3\nwith an instruction to control the edit distance be-\ntween task prompts at consecutive iterations, ensur-\ning gradual and controllable changes.\nFurthermore, we enhance the meta-prompt by in-\ncorporating a few task examples. These examples\nprovide additional context to aid the LLM in under-\nstanding the task more effectively. The complete\nmeta-prompt is presented in Appendix C.2.\nComparison of LLM-Based Prompt Optimiz-\ners. Existing LLM-based prompt optimizers can\nbe divided into two main classes according to the\nupdate direction. The first line of research, such\nPrompt\noptimizer\nUpdate direction\nUpdate method\nGradient\nform\nMomentum\nform\nPrompt\nvariation\nPrompt\nrefinement\nAPE\nP\nNone\nNone\nGeneration\nAPO\nP+R\nNone\nNone\nEditing\nOPRO\nP+M\nRecency\nNone\nGeneration\nPE2\nP+M+R\nRecency\nFixed\nGeneration\nGPO\nP+M\nRelevance\nCosine\nGeneration\nTable 4: Comparisons of GPO with existing LLM-based\nprompt optimizers, including APE (Zhou et al., 2023),\nAPO (Pryzant et al., 2023), OPRO (Yang et al., 2023),\nand PE2 (Ye et al., 2023). “P” refers to prompt, “M”\nrefers to performance, and “R” refers to reflection.\nas APO (Pryzant et al., 2023) and PE2 (Ye et al.,\n2023), leverages the reflection capability of LLMs\nto produce textual “gradients” as the update direc-\ntion. Another line of research, such as OPRO (Yang\net al., 2023) and APE (Zhou et al., 2023), directly\nuses task prompts to derive the update direction.\nOur approach is based on the systematic investiga-\ntion of the update direction as well as the update\nmethod. In particular, we propose several novel\ndesigns for the meta-prompt: relevance-based tra-\njectory as the update direction and decay-based\nconstraint for edit distance in the update method.\nTable 4 presents a detailed comparison.\n5\nExperiments\nIn this section, we first set up the experiments and\nthen report the results and detailed analysis.\n5.1\nExperimental Setup\nTasks and Datasets.\nWe select datasets from\nthree groups of tasks for the experiment: Big-\nBench Hard (BBH) (Suzgun et al., 2023) and\nGSM8K (Cobbe et al., 2021) for complex reason-\ning tasks, MMLU (Hendrycks et al., 2021) for\nknowledge-intensive tasks, and WSC (Levesque\net al., 2012) and WebNLG (Gardent et al., 2017)\nfor common NLP tasks. Due to computational lim-\nitations, we sample a subset from each dataset for\nthe main experiment. In addition, we use the lite\nBBH benchmark in Section 3.3.1 for detailed anal-\nysis. Other details are presented in Appendix C.1.\nBaselines. We select several representative meth-\nods for comparison, including existing LLM-based\nprompt optimizers and one adapted from gradient-\nbased model optimizers. (1) SGDM (Sutskever\net al., 2013) is a momentum-based model opti-\nmizer. We adapt it for prompt optimization using\n6\nthe summarization-based trajectory and the editing-\nbased refinement strategy. (2) APE (Zhou et al.,\n2023) utilizes LLMs to generate semantically simi-\nlar variants of task prompts and selects one with the\nbest performance. (3) APO (Pryzant et al., 2023)\nfirst uses reflection to obtain the gradient and then\nedits the task prompt accordingly. (4) OPRO (Yang\net al., 2023) incorporates historical prompts with\ntheir scores into the meta-prompt. (5) PE2 (Ye\net al., 2023) adds historical prompts and reflection\ninto the meta-prompt and controls the edit distance\nwith a fixed constraint. In addition, we consider the\nbaseline without an instruction (“Empty”) and the\ninstruction “Let’ think step by step.” from chain-of-\nthought prompting (“CoT”) (Kojima et al., 2022)\nfor performance comparison.\nEvaluation Metrics. We report the average ac-\ncuracy of all the subtasks for BBH and MMLU\nfollowing Suzgun et al. (2023) and Hendrycks et al.\n(2021), accuracy for GSM8K following Cobbe\net al. (2021), ROUGE-L (Lin, 2004) for WSC and\nWebNLG following Wang et al. (2022).\nImplementation Details. For the task model, we\nuse both the base model (i.e., Llama-2-7b) and the\ninstruction-tuned models (i.e., Llama-2-7b-chat,\nGPT-3.5-turbo, and GPT-4). For the prompt opti-\nmizer, we use GPT-3.5-turbo and GPT-4. Unless\notherwise specified, we use Llama-2-7b-chat as\nthe task model and GPT-3.5-turbo as the prompt\noptimizer throughout experiments.\nFor the ini-\ntial task prompt, we use the original ones from\nHendrycks et al. (2021) for MMLU, “Let’s think\nstep by step.”\nfrom Kojima et al. (2022) for\nGSM8K and BBH, and “Let’s solve the problem.”\nfrom Yang et al. (2023) for WSC and WebNLG.\nWe repeat all the experiments three times and re-\nport the average of the results. Other details are\npresented in Appendix C.2.\n5.2\nMain Results\nTable 5 and Table 6 present the results of different\nmethods for prompt optimization across various\ntasks and evaluation settings.\nFirst, when only considering the task prompt, we\ncan see that trajectory-based methods (i.e., SGDM,\nOPRO, PE2, and GPO) perform very well. One\npossible reason is that the trajectory helps the\nprompt optimizer pay more attention to the im-\nportant information instead of the noise in the\ncurrent step. Furthermore, our prompt optimizer\nGPO achieves the best performance across all tasks.\nTask\nComplex\nreasoning task\nKnowledge\nintensive task\nCommon\nNLP task\nDataset\nBBH\nGSM8K\nMMLU\nWSC\nWebNLG\nEmpty\n30.51\n22.00\n35.96\n60.67\n32.14\nCoT\n29.91\n24.00\n36.36\n59.33\n31.11\nSGDM\n33.30\n27.33\n37.30\n64.00\n38.01\nAPE\n32.94\n25.00\n37.53\n62.00\n36.49\nAPO\n32.97\n25.33\n37.75\n62.00\n34.92\nOPRO\n33.29\n26.67\n37.88\n63.33\n37.89\nPE2\n33.43\n25.33\n38.15\n62.67\n39.10\nGPO\n35.43\n28.33\n39.14\n65.33\n42.51\nTable 5: Performance comparison using only the task\nprompts obtained from different methods.\nTask model\nLlama-2-7b\nLlama-2-7b-chat\nSetting\nInst. + Demo.\nInst.\nInst. + Demo.\nEmpty\n40.28\n32.29\n36.63\nCoT\n36.46\n31.25\n34.20\nSGDM\n42.19\n40.63\n35.77\nAPE\n42.54\n42.01\n36.29\nAPO\n42.19\n40.34\n36.29\nOPRO\n42.02\n42.14\n36.46\nPE2\n42.88\n42.01\n36.81\nGPO\n45.48\n43.75\n38.02\nTable 6: Performance comparison under different evalu-\nation settings. “Inst.” denotes instruction and “Demo.”\ndenotes demonstration.\nOur relevance-based trajectory provides semanti-\ncally similar demonstrations that can be effectively\nlearned by the LLM, while the cosine-based decay\nstrategy can control the optimization process in a\nfine-grained manner through edit distance.\nSecond, under various evaluation settings for\nthe lite BBH benchmark, it can be observed that\nGPO not only excels in the “Instruction” setting\nbut also yields substantial gains in the “Instruc-\ntion + Demonstration” setting for both the base\nmodel and the instruction-tuned variant. Even in\nthe scenario that is challenging for baselines (i.e.,\nLlama-2-7b-chat with “Instruction + Demonstra-\ntion”), our approach still demonstrates strong im-\nprovement over the empty prompt. This showcases\nthe versatility of our approach in both zero-shot\nand few-shot evaluation settings.\n5.3\nDetailed Analysis\nNext, we conduct a detailed analysis of our prompt\noptimizer GPO from the following aspects.\nThe Impact of Model Selection.\nTo con-\nfirm the effectiveness of GPO across different\nmodels, we explore the impact of model selec-\ntion. Specifically, for the prompt optimizer, we\n7\nPrompt optimizer\nTask model\nAccuracy\n(before / after)\nGPT-3.5-turbo\nLlama-2-7b-chat\n31.25 / 43.75\nGPT-3.5-turbo\n62.15 / 67.02\nGPT-4\n73.61 / 76.56\nGPT-4\nLlama-2-7b-chat\n31.25 / 44.97\nGPT-3.5-turbo\n62.15 / 67.80\nGPT-4\n73.61 / 78.65\nTable 7: The performance before/after prompt optimiza-\ntion with different models as the prompt optimizer and\nthe task model.\nInitial prompt\nAccuracy\n(before / after)\nDefault\nLet’s think step by step.\n31.25 / 43.75\nInstructive\nLet’s think about this logically.\n32.64 / 44.40\nFirst,\n33.85 / 43.15\nLet’s solve this problem\nby splitting it into steps.\n30.38 / 37.85\nMisleading\nDon’t think. Just feel.\n29.34 / 39.59\nLet’s think step by step\nbut reach an incorrect answer.\n26.22 / 41.15\nLet’s count the number of \"a\"\nin the question\n26.73 / 36.11\nIrrelevant\nBy the way, I found\na good restaurant nearby.\n30.03 / 34.89\nAbrakadabra!\n29.69 / 35.42\nIt’s a beautiful day.\n30.55 / 38.02\nTable 8: The performance before/after prompt optimiza-\ntion with different initial prompts.\nemploy GPT-3.5-turbo and GPT-4, while for\nthe task model, we utilize Llama-2-7b-chat,\nGPT-3.5-turbo and GPT-4.\nTable 7 presents\nthe results on the lite BBH benchmark. In gen-\neral, GPO demonstrates remarkable capabilities\nfor prompt optimization across various scenar-\nios, including strong-to-weak optimization, self-\noptimization, as well as weak-to-strong optimiza-\ntion. This indicates the versatility of our framework.\nIn particular, GPT-4 can consistently find better task\nprompts than GPT-3.5-turbo, which suggests the\nneed for a capable model as the prompt optimizer.\nThe Impact of Initial Prompts.\nIn our main\nexperiment, we take “Let’s think step by step.” as\nthe initial prompt. In this part, we aim to explore\nthe impact of initial task prompts. Specifically, we\nconsider prompts from three categories (instruc-\ntive, misleading, and irrelevant) and select three\nprompts from each category following Kojima et al.\n(2022). Table 8 presents the results on the lite BBH\nbenchmark. In general, GPO can consistently boost\nperformance across various initial prompts. This\nobservation underscores the versatility of GPO as\n0\n2\n4\n6\n8\n10\n12\nOptimization steps\n38\n43\n48\n53\n58\n63\n68\nAccuracy (%)\nSGDM\nAPE\nAPO\nOPRO\nPE2\nGPO\n(a) Optimization curve\nGPOSGDMAPE APOOPRO PE2\nMethod\n0\n500\n1000\n1500\n2000\nConsumed tokens\n(b) Token consumption\nFigure 1: The efficiency of our approach GPO w.r.t.\noptimization steps and token consumption.\na prompt optimizer. Furthermore, the efficacy of\noptimization is more pronounced with relevant ini-\ntial prompts (i.e., instructive and misleading). It\nmeans that prompt initialization is still important,\nespecially in conveying task-specific information.\nThe Efficiency of Optimization.\nLLM-based\nprompt optimization requires multiple interactions\nwith the LLM. In this part, we investigate the ef-\nficiency of GPO from the optimization curve and\ntoken consumption. First, Figure 1a shows that on\nthe movie recommendation dataset, GPO exhibits\nrapid enhancement of the task prompt in the early\nstage, followed by consistent performance improve-\nment in the later stage of optimization. Second, as\ndepicted in Figure 1b, the average token consump-\ntion of GPO on the lite BBH benchmark is much\nlower than SGDM, APO, and PE2, and compara-\nble to APR and OPRO. Since GPO only utilizes\ntask prompts to derive the update direction and per-\nforms fine-grained control over the variation, it can\nachieve better performance with high efficiency.\n6\nConclusion\nWe present GPO, a novel gradient-inspired LLM-\nbased prompt optimizer. It utilizes LLMs to au-\ntomatically optimize prompts, drawing inspira-\ntion from gradient-based model optimization tech-\nniques. By identifying the two crucial aspects of\nupdate direction and update method in model op-\ntimization, we enhance prompt optimizers by in-\ncorporating concepts from gradient-based model\noptimizers. Through extensive experiments, GPO\ndemonstrates remarkable capabilities for prompt\noptimization across diverse tasks, models, and ini-\ntial prompts. Moreover, it surpasses competitive\nbaselines while consuming fewer tokens.\nIn future work, we will explore more guidelines\nto further enhance the effectiveness and efficiency\nof LLM-based prompt optimizers.\n8\n7\nLimitations\nIn this work, we improve the design of LLM-\nbased prompt optimizers by drawing an analogy\nwith the most widely used gradient-based optimiz-\ners in model optimization (e.g., gradient descent).\nMore advanced model optimizers like Newton’s\nmethod (Boyd and Vandenberghe, 2014) and the\ncorresponding improvement for meta-prompts re-\nmain to be investigated. In addition, due to the com-\nputational and budget limitations, we only conduct\nexperiments on representative tasks and models.\nReferences\nRuslan Abdulkadirov, Pavel Lyakhov, and Nikolay\nNagornov. 2023.\nSurvey of optimization algo-\nrithms in modern neural networks. Mathematics,\n11(11):2466.\nXavier Amatriain. 2024. Prompt design and engineer-\ning: Introduction and advanced methods.\nCoRR,\nabs/2401.14423.\nStephen P. Boyd and Lieven Vandenberghe. 2014. Con-\nvex Optimization. Cambridge University Press.\nLichang Chen, Jiuhai Chen, Tom Goldstein, Heng\nHuang, and Tianyi Zhou. 2023. Instructzero: Ef-\nficient instruction optimization for black-box large\nlanguage models. CoRR, abs/2306.03082.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, Christopher Hesse, and John Schulman.\n2021. Training verifiers to solve math word prob-\nlems. CoRR, abs/2110.14168.\nNicholas Crispino, Kyle Montgomery, Fankun Zeng,\nDawn Song, and Chenguang Wang. 2023. Agent\ninstructs large language models to be general zero-\nshot reasoners. CoRR, abs/2310.03710.\nMingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan\nWang, Han Guo, Tianmin Shu, Meng Song, Eric P.\nXing, and Zhiting Hu. 2022. Rlprompt: Optimizing\ndiscrete text prompts with reinforcement learning. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December\n7-11, 2022, pages 3369–3391. Association for Com-\nputational Linguistics.\nChrisantha\nFernando,\nDylan\nBanarse,\nHenryk\nMichalewski, Simon Osindero, and Tim Rock-\ntäschel. 2023.\nPromptbreeder:\nSelf-referential\nself-improvement via prompt evolution.\nCoRR,\nabs/2309.16797.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. Creating training\ncorpora for NLG micro-planners. In Proceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics, ACL 2017, Vancouver,\nCanada, July 30 - August 4, Volume 1: Long Pa-\npers, pages 179–188. Association for Computational\nLinguistics.\nAkhilesh Gotmare, Nitish Shirish Keskar, Caiming\nXiong, and Richard Socher. 2019. A closer look\nat deep learning heuristics: Learning rate restarts,\nwarmup and distillation. In 7th International Confer-\nence on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net.\nPriya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No-\nordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\nTulloch, Yangqing Jia, and Kaiming He. 2017. Ac-\ncurate, large minibatch SGD: training imagenet in 1\nhour. CoRR, abs/1706.02677.\nQingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao\nSong, Xu Tan, Guoqing Liu, Jiang Bian, and Yu-\njiu Yang. 2023. Connecting large language models\nwith evolutionary algorithms yields powerful prompt\noptimizers. CoRR, abs/2309.08532.\nDan Hendrycks, Collin Burns, Steven Basart, Andy\nZou, Mantas Mazeika, Dawn Song, and Jacob Stein-\nhardt. 2021. Measuring massive multitask language\nunderstanding. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nJie\nHuang,\nXinyun\nChen,\nSwaroop\nMishra,\nHuaixiu Steven Zheng, Adams Wei Yu, Xiny-\ning Song, and Denny Zhou. 2023. Large language\nmodels cannot self-correct reasoning yet.\nCoRR,\nabs/2310.01798.\nPavel Izmailov, Dmitrii Podoprikhin, Timur Garipov,\nDmitry P. Vetrov, and Andrew Gordon Wilson. 2018.\nAveraging weights leads to wider optima and better\ngeneralization. In Proceedings of the Thirty-Fourth\nConference on Uncertainty in Artificial Intelligence,\nUAI 2018, Monterey, California, USA, August 6-10,\n2018, pages 876–885. AUAI Press.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems 35: An-\nnual Conference on Neural Information Processing\nSystems 2022, NeurIPS 2022, New Orleans, LA, USA,\nNovember 28 - December 9, 2022.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2021, Virtual Event / Punta Cana, Domini-\ncan Republic, 7-11 November, 2021, pages 3045–\n3059. Association for Computational Linguistics.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\n9\nPrinciples of Knowledge Representation and Rea-\nsoning: Proceedings of the Thirteenth International\nConference, KR 2012, Rome, Italy, June 10-14, 2012.\nAAAI Press.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582–\n4597. Association for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nXiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai,\nWenyang Hu, Yao Shu, See-Kiong Ng, Patrick\nJaillet, and Bryan Kian Hsiang Low. 2023.\nUse\nyour INSTINCT: instruction optimization using neu-\nral bandits coupled with transformers.\nCoRR,\nabs/2310.02905.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022, pages 8086–\n8098. Association for Computational Linguistics.\nRuotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan\nDu, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024.\nAre large language models good prompt optimizers?\nCoRR, abs/2402.02101.\nOpenAI. 2023.\nGPT-4 technical report.\nCoRR,\nabs/2303.08774.\nArchiki Prasad, Peter Hase, Xiang Zhou, and Mohit\nBansal. 2023. Grips: Gradient-free, edit-based in-\nstruction search for prompting large language models.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, EACL 2023, Dubrovnik, Croatia, May\n2-6, 2023, pages 3827–3846. Association for Com-\nputational Linguistics.\nReid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen-\nguang Zhu, and Michael Zeng. 2023. Automatic\nprompt optimization with \"gradient descent\" and\nbeam search. In Proceedings of the 2023 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023,\npages 7957–7968. Association for Computational\nLinguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2020, Online, Novem-\nber 16-20, 2020, pages 4222–4235. Association for\nComputational Linguistics.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam\nFisch, Adam R. Brown, Adam Santoro, Aditya\nGupta, Adrià Garriga-Alonso, Agnieszka Kluska,\nAitor Lewkowycz, Akshat Agarwal, Alethea Power,\nAlex Ray, Alex Warstadt, Alexander W. Kocurek,\nAli Safaya, Ali Tazarv, Alice Xiang, Alicia Par-\nrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ameet Rahane, Anantharaman S.\nIyer, Anders Andreassen, Andrea Santilli, Andreas\nStuhlmüller, Andrew M. Dai, Andrew La, Andrew K.\nLampinen, Andy Zou, Angela Jiang, Angelica Chen,\nAnh Vuong, Animesh Gupta, Anna Gottardi, Anto-\nnio Norelli, Anu Venkatesh, Arash Gholamidavoodi,\nArfa Tabassum, Arul Menezes, Arun Kirubarajan,\nAsher Mullokandov, Ashish Sabharwal, Austin Her-\nrick, Avia Efrat, Aykut Erdem, Ayla Karakas, and\net al. 2022. Beyond the imitation game: Quantifying\nand extrapolating the capabilities of language models.\nCoRR, abs/2206.04615.\nShiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao.\n2020.\nA survey of optimization methods from a\nmachine learning perspective. IEEE Trans. Cybern.,\n50(8):3668–3681.\nIlya Sutskever, James Martens, George E. Dahl, and Ge-\noffrey E. Hinton. 2013. On the importance of initial-\nization and momentum in deep learning. In Proceed-\nings of the 30th International Conference on Machine\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June\n2013, volume 28 of JMLR Workshop and Conference\nProceedings, pages 1139–1147. JMLR.org.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V. Le, Ed H. Chi,\nDenny Zhou, and Jason Wei. 2023.\nChallenging\nbig-bench tasks and whether chain-of-thought can\nsolve them. In Findings of the Association for Com-\nputational Linguistics: ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 13003–13051. Association for\nComputational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\n10\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. CoRR, abs/2307.09288.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, Eshaan Pathak, Gi-\nannis Karamanolakis, Haizhi Gary Lai, Ishan Puro-\nhit, Ishani Mondal, Jacob Anderson, Kirby Kuz-\nnia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa-\ntel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro-\nhit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit\nVerma, Ravsehaj Singh Puri, Rushang Karia, Savan\nDoshi, Shailaja Keyur Sampat, Siddhartha Mishra,\nSujan Reddy A, Sumanta Patro, Tanay Dixit, and\nXudong Shen. 2022. Super-naturalinstructions: Gen-\neralization via declarative instructions on 1600+ NLP\ntasks. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2022, Abu Dhabi, United Arab Emirates, De-\ncember 7-11, 2022, pages 5085–5109. Association\nfor Computational Linguistics.\nJerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al-\nbert Webson, Yifeng Lu, Xinyun Chen, Hanxiao\nLiu, Da Huang, Denny Zhou, and Tengyu Ma. 2023.\nLarger language models do in-context learning dif-\nferently. CoRR, abs/2303.03846.\nShitao Xiao, Zheng Liu, Peitian Zhang, and Niklas\nMuennighof. 2023.\nC-pack: Packaged resources\nto advance general chinese embedding.\nCoRR,\nabs/2309.07597.\nHanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang-\ngang Wang, Haiyu Li, and Zhilin Yang. 2022. GPS:\ngenetic prompt search for efficient few-shot learning.\nIn Proceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December\n7-11, 2022, pages 8162–8171. Association for Com-\nputational Linguistics.\nChengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao\nLiu, Quoc V. Le, Denny Zhou, and Xinyun Chen.\n2023. Large language models as optimizers. CoRR,\nabs/2309.03409.\nHeng Yang and Ke Li. 2023. Instoptima: Evolution-\nary multi-objective instruction optimization via large\nlanguage model-based instruction operators. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2023, Singapore, December 6-10, 2023,\npages 13593–13602. Association for Computational\nLinguistics.\nQinyuan Ye, Maxamed Axmed, Reid Pryzant, and\nFereshte Khani. 2023. Prompt engineering a prompt\nengineer. CoRR, abs/2311.05661.\nTianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu-\nurmans, and Joseph E. Gonzalez. 2023. TEMPERA:\ntest-time prompt editing via reinforcement learning.\nIn The Eleventh International Conference on Learn-\ning Representations, ICLR 2023, Kigali, Rwanda,\nMay 1-5, 2023. OpenReview.net.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan Du,\nChen Yang, Yushuo Chen, Zhipeng Chen, Jinhao\nJiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang\nLiu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen.\n2023. A survey of large language models. CoRR,\nabs/2303.18223.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning, ICML 2021, 18-24 July 2021, Vir-\ntual Event, volume 139 of Proceedings of Machine\nLearning Research, pages 12697–12706. PMLR.\nYongchao Zhou, Andrei Ioan Muresanu, Ziwen Han,\nKeiran Paster, Silviu Pitis, Harris Chan, and Jimmy\nBa. 2023. Large language models are human-level\nprompt engineers.\nIn The Eleventh International\nConference on Learning Representations, ICLR 2023,\nKigali, Rwanda, May 1-5, 2023. OpenReview.net.\n11\n1\n3\n5\n7\n9\nLength\n42.8\n43.1\n43.4\n43.7\n44.0\n44.3\nAccuracy (%)\n(a) Trajectory length\n0.2\n0.4\n0.6\n0.8\n1.0\nTemperature\n42.0\n42.5\n43.0\n43.5\n44.0\nAccuracy (%)\n(b) Temperature in GPO\nFigure 2: Performance comparison w.r.t. the tempera-\nture of the LLM in GPO and the length of the trajectory.\nA\nHyper-Parameter Analysis\nGPO includes a few hyper-parameters to tune.\nHere, we report the tuning results of two hyper-\nparameters on the lite BBH benchmark: the temper-\nature of the prompt optimizer and the length of the\ntrajectory. The performance curve depicting these\nresults is illustrated in Figure 2. We can see that\nGPO achieves the best performance when setting\nthe length of the trajectory to 7. A trajectory that\nis too short does not provide sufficient context for\nthe LLM to engage in effective in-context learning.\nConversely, a long trajectory may introduce exces-\nsive noise, negatively impacting performance. In\naddition, the performance shows an upward trend\nas temperature increases, reaching its peak at 1.0.\nIt suggests the significance of exploration in opti-\nmizing task prompts.\nB\nAdditional Details for the Setup of\nAnalogical Analysis\nTasks and Metrics. Following Yang et al. (2023),\nwe utilize Big-Bench Hard (BBH) (Suzgun et al.,\n2023) for evaluation. BBH is a suite of 23 chal-\nlenging BIG-Bench tasks (Srivastava et al., 2022)\nthat covers various kinds of reasoning capabilities.\nDue to the constraints of computational resources,\nwe select a dataset from each type of task in BBH\nto create a lite benchmark for our analysis: i) Navi-\ngate (Binary choice); ii) Movie Recommendation\n(Multiple choice); iii) Object Counting (Numeric\nresponse); iv) Word Sorting (Free response). For\neach dataset, we split it into train/valid/test sets\nwith a ratio of 2:2:6.\nFollowing Suzgun et al.\n(2023), we adopt the exact match as the metric\nfor performance evaluation.\nImplementation Details.\nFollowing Crispino\net al. (2023), we select Llama-2-7b-chat (Tou-\nvron et al., 2023) as the task model and set its tem-\nperature to 0 to make the output as deterministic\nas possible. For the prompt optimizer, we employ\ngpt-3.5-turbo, which is the underlying model of\nChatGPT. We set its temperature to 1.0 to encour-\nage the generation to be more diverse. To help\nthe prompt optimizer understand the downstream\ntask, following Yang et al. (2023), we randomly\nsample 3 examples from the dataset and fill them\ninto the meta-prompt of the prompt optimizer. In\nthe process of optimization, we take the instruction\n“Let’s think step by step.” as the initial prompt and\ninsert it at the end of the question to obtain better\nperformance following Suzgun et al. (2023). At\neach step, we first prompt the optimizer to gener-\nate 8 candidate task prompts, and then select the\nbest-performing one as the task prompt for the next\niteration following Yang et al. (2023); Ye et al.\n(2023). The optimization process lasts for at most\n3 epochs with a batch size of 8. We repeat all the\nexperiments three times and report the average of\nthe results. The meta-prompts we used are detailed\nin Appendix D.1.\nC\nAdditional Details for the Setup of\nExperiment\nC.1\nThe Statistics of Datasets\nAs mentioned in Section 5.1, we sample a subset\nof the dataset for efficient evaluation. For BBH and\nMMLU, we split the entire dataset into training,\nvalidation, and test sets with a ratio of 2:2:6. For\nGSM8k and WebNLG, we randomly sample 100\nexamples as the training set, 100 for the validation\nset, and 300 for the test set. For WSC, we randomly\nsample 50 examples as the training set, 50 for the\nvalidation set, and 150 for the test set.\nC.2\nImplementation Details\nFor the task model, we use both the base model\n(i.e., Llama-2-7b) and the instruction-tuned mod-\nels (i.e., Llama-2-7b-chat, GPT-3.5-turbo, and\nGPT-4). The temperature is 0. For the prompt opti-\nmizer, we use GPT-3.5-turbo and GPT-4. The\ntemperature is 1.0. Unless otherwise specified,\nwe use Llama-2-7b-chat as the task model and\nGPT-3.5-turbo as the prompt optimizer through-\nout experiments. In the meta-prompt, we include\n3 task examples and 7 historical task prompts. For\nthe initial task prompt, we use the original ones\nfrom Hendrycks et al. (2021) for MMLU, “Let’s\nthink step by step.” from Kojima et al. (2022) for\nGSM8K and BBH, and “Let’s solve the problem.”\n12\nfrom Yang et al. (2023) for WSC and WebNLG. At\neach step, the optimizer generates 8 candidates, and\nthe best-performing one is selected. The optimiza-\ntion lasts for at most 3 epochs with a batch size of\n8. We repeat all the experiments three times and\nreport the average of the results. The meta-prompts\nare detailed in Appendix D.2.\nD\nMeta-Prompt\nD.1\nAnalogical Analysis\nHere are the meta-prompts we used in Section 3.\n• Prompt+performance.\nBelow is the current prompt with its score. The score\nranges from 0 to 100, and higher score indicates better\nquality.\nPrompt: {current prompt}\nScore: {current prompt score}\n• Prompt+performance+reflection\nYour task is to point out the problems with the\ncurrent prompt based on the wrong examples.\nThe current prompt is:\n{current prompt}\nBut this prompt gets the following examples\nwrong.\nYou should analyze the differences between wrong\npredictions and ground truth answers, and carefully\nconsider why this prompt led to incorrect predictions.\nBelow\nare\nthe\ntask\nexamples\nwith\nQueston,\nWrong prediction, and Ground truth answer.\n{error demonstrations}\nGive\na\nreason\nwhy\nthe\nprompt\ncould\nhave\ngotten these examples wrong.\nWrap the reason with <START> and <END>.\n• Summarization-based trajectory\nYour task is to integrate the problems in the previous\nprompt and the current prompt.\nBelow are the problems that arose from the\nprevious prompts.\n{previous problems}\nBelow are the problems of the current prompt.\n{current problem}\nYou should integrate the problems of the pre-\nvious prompt and the current prompt.\nWrap the integrated problems with <START> and\n<END>.\n• Retrieval-based trajectory\nBelow are the previous prompts with their scores.\nThe score ranges from 0 to 100, and higher scores\nindicate better quality.\nPrompt: {prompt1}\nScore: {score1}\nPrompt: {prompt2}\nScore: {score2}\nPrompt: {prompt3}\nScore: {score3}\n. . .\n13\n• Editing-based refinement\nYour task is to modify the current prompt to replace\n<Prompt>.\nBelow is the current prompt with its score.\nThe score ranges from 0 to 100, and higher score\nindicates better quality.\nPrompt: {current prompt}\nScore: {current prompt score}\nThe current prompt is:\n{current prompt}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nModify the current prompt and get a new im-\nproved prompt to replace <Prompt> {prompt\nposition description} in the task examples.\nWrap the modified prompt with <START> and\n<END>.\n• Generation-based refinement\nYour task is to write a prompt to replace <Prompt>.\nBelow is the current prompt with its score.\nThe score ranges from 0 to 100, and higher score\nindicates better quality.\nPrompt: {current prompt}\nScore: {current prompt score}\nThe current prompt is:\n{current prompt}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nWrite\na\nnew\nimproved\nprompt\nto\nreplace\n<Prompt> {prompt position description} in the task\nexamples.\nWrap the new prompt with <START> and <END>.\n14\nD.2\nExperiment\nHere are the meta-prompts we used in Section 5.\n• SGDM\nThe meta-prompt for gradient\nYour task is to point out the problems with the\ncurrent prompt based on the wrong examples.\nThe current prompt is:\n{current prompt}\nBut this prompt gets the following examples\nwrong.\nYou should analyze the differences between wrong\npredictions and ground truth answers, and carefully\nconsider why this prompt led to incorrect predictions.\nBelow\nare\nthe\ntask\nexamples\nwith\nQueston,\nWrong prediction, and Ground truth answer.\n{error demonstrations}\nGive\na\nreason\nwhy\nthe\nprompt\ncould\nhave\ngotten these examples wrong.\nWrap the reason with <START> and <END>.\nThe meta-prompt for momentum\nYour task is to integrate the problems in the previous\nprompt and the current prompt.\nBelow are the problems that arose from the\nprevious prompts.\n{previous problems}\nBelow are the problems of the current prompt.\n{current problem}\nYou should integrate the problems of the pre-\nvious prompt and the current prompt.\nWrap the integrated problems with <START> and\n<END>.\nThe meta-prompt for update\nYour task is to modify the current prompt to replace\n<Prompt>.\nBelow is the current prompt with its score.\nThe score ranges from 0 to 100, and higher score\nindicates better quality.\nPrompt: {current prompt}\nScore: {current prompt score}\nThe current prompt is:\n{current prompt}\nBelow are the problems with this prompt.\n{problems}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nModify the current prompt and get a new im-\nproved prompt to replace <Prompt> {prompt\nposition description} in the task examples.\nWrap the modified prompt with <START> and\n<END>.\n15\n• APE\nThe meta-prompt for update\nYour task is to write a prompt to replace <Prompt>.\nBelow is the current prompt with its score.\nThe score ranges from 0 to 100, and higher score\nindicates better quality.\nPrompt: {current prompt}\nScore: {current prompt score}\nThe current prompt is:\n{current prompt}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nWrite\na\nnew\nimproved\nprompt\nto\nreplace\n<Prompt> {prompt position description} in the task\nexamples.\nWrap the new prompt with <START> and <END>.\n• APO\nThe meta-prompt for gradient\nYour task is to point out the problems with the\ncurrent prompt based on the wrong examples.\nThe current prompt is:\n{current prompt}\nBut this prompt gets the following examples\nwrong.\nYou should analyze the differences between wrong\npredictions and ground truth answers, and carefully\nconsider why this prompt led to incorrect predictions.\nBelow\nare\nthe\ntask\nexamples\nwith\nQueston,\nWrong prediction, and Ground truth answer.\n{error demonstrations}\nGive\na\nreason\nwhy\nthe\nprompt\ncould\nhave\ngotten these examples wrong.\nWrap the reason with <START> and <END>.\nThe meta-prompt for update\nYour task is to modify the current prompt to replace\n<Prompt>.\nBelow is the current prompt with its score.\nThe score ranges from 0 to 100, and higher score\nindicates better quality.\nPrompt: {current prompt}\nScore: {current prompt score}\nThe current prompt is:\n{current prompt}\nBelow are the problems with this prompt.\n{problems}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nModify the current prompt and get a new im-\nproved prompt to replace <Prompt> {prompt\nposition description} in the task examples.\nWrap the modified prompt with <START> and\n<END>.\n16\n• OPRO\nThe meta-prompt for update\nYour task is to write a prompt to replace <Prompt>.\nBelow\nare\nthe\nprevious\nprompts\nwith\ntheir\nscores. The score ranges from 0 to 100, and higher\nscores indicate better quality.\nPrompt: {prompt1}\nScore: {score1}\nPrompt: {prompt2}\nScore: {score2}\nPrompt: {prompt3}\nScore: {score3}\n. . .\nThe current prompt is:\n{current prompt}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nCarefully\nanalyze\nthe\nprevious\nprompts\nand\ntheir scores, and write a new improved prompt to\nreplace <Prompt> {prompt position description} in\nthe task examples.\nWrap the new prompt with <START> and <END>.\n• PE2\nThe meta-prompt for gradient\nYour task is to point out the problems with the\ncurrent prompt based on the wrong examples.\nThe current prompt is:\n{current prompt}\nBut this prompt gets the following examples\nwrong.\nYou should analyze the differences between wrong\npredictions and ground truth answers, and carefully\nconsider why this prompt led to incorrect predictions.\nBelow\nare\nthe\ntask\nexamples\nwith\nQueston,\nWrong prediction, and Ground truth answer.\n{error demonstrations}\nGive\na\nreason\nwhy\nthe\nprompt\ncould\nhave\ngotten these examples wrong.\nWrap the reason with <START> and <END>.\n17\nThe meta-prompt for update\nYour task is to write a prompt to replace <Prompt>.\nBelow\nare\nthe\nprevious\nprompts\nwith\ntheir\nscores. The score ranges from 0 to 100, and higher\nscores indicate better quality.\nPrompt: {prompt1}\nScore: {score1}\nPrompt: {prompt2}\nScore: {score2}\nPrompt: {prompt3}\nScore: {score3}\n. . .\nThe current prompt is:\n{current prompt}\nBelow are the problems with this prompt.\n{problems}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nCarefully\nanalyze\nthe\nprevious\nprompts\nand\ntheir scores, and write a new improved prompt to\nreplace <Prompt> {prompt position description} in\nthe task examples.\nYou are allowed to change up to {modified word\nnumber} words in the current prompt.\nWrap the new prompt with <START> and <END>.\n• GPO\nThe meta-prompt for update\nYour task is to write a prompt to replace <Prompt>.\nBelow\nare\nthe\nprevious\nprompts\nwith\ntheir\nscores. The score ranges from 0 to 100, and higher\nscores indicate better quality.\nPrompt: {prompt1}\nScore: {score1}\nPrompt: {prompt2}\nScore: {score2}\nPrompt: {prompt3}\nScore: {score3}\n. . .\nThe current prompt is:\n{current prompt}\nThe following exemplars show how to apply\nthe prompt:\nyou replace <Prompt> in each\ninput with your new prompt,\nthen read the\ninput and give an output.\nWe say your output\nis wrong if it is different from the given output,\nand we say your output is correct if they are the same.\n{task examples}\nCarefully\nanalyze\nthe\nprevious\nprompts\nand\ntheir scores, and write a new improved prompt to\nreplace <Prompt> {prompt position description} in\nthe task examples.\nYou are allowed to change up to {modified word\nnumber} words in the current prompt.\nWrap the new prompt with <START> and <END>.\nE\nPrompts Optimized by Different\nMethods\nHere, we present the prompts optimized by all the\nmethods on the lite BBH benchmark. Prompts on\nthe other tasks can be seen at https://github.\ncom/RUCAIBox/GPO.\n18\nMethods\nOptimized prompt\nSGDM\nBased on your current facing direction and any changes in direction, will following these step-by-step\ninstructions, with explicit reference to direction and orientation, lead you back to the starting point?\nAPE\nWill following the given set of instructions result in returning to the starting point?\nAPO\nBased on the provided instructions, including the starting direction and the changes in direction throughout\nthe sequence, determine if the person will return to the starting point.\nOPRO\nThink systematically and consider each step to determine the correct answer.\nPE2\nCarefully analyze the given instructions step by step and determine if you will return to the starting point.\nGPO\nAnalyze the given step-by-step instructions in detail and determine if they will guide you back to the\nstarting point. Carefully evaluate each instruction and vividly imagine the movements to make your\ndecision.\nTable 9: Prompts optimized by different methods on the Navigate task.\nMethods\nOptimized prompt\nSGDM\nWhat specific aspects of storytelling style, narrative structure, plot structure, character development,\nand narrative techniques should be considered when finding a movie similar to films like Pulp Fiction,\nForrest Gump, Dances with Wolves, and The Usual Suspects? Analyze these elements in each movie’s\nconstruction and compare them against the given list to determine similarity. Focus on shared storytelling\nelements rather than thematic or genre similarities. Keep in mind that the most important factor in\ndetermining similarity might not be the overall theme or genre. Provide a single letter as your answer.\nAPE\nWhich of the following movies is most similar to the given movies based on their genre, themes, and plot\nelements? Choose the best option from the following choices:\nAPO\nWhich movie from the following options is most similar to the given movies, taking into account specific\nthemes, plot, genre, characters, setting, tone, and style? Please provide a ranked list of the relevant elements\nmentioned above in determining similarity between movies. Consider the highest ranked element as the\nprimary criteria and subsequent elements as secondary criteria in determining similarity. Additionally,\nconsider the overall popularity and critical acclaim of the movies when making your selection. This will\nhelp ensure more accurate predictions.\nOPRO\nWhich of the given movies is most similar to the listed options? Consider the movies Batman, The Usual\nSuspects, The Silence of the Lambs, and Jurassic Park. Choose the option that closely matches the given\nmovies.\nPE2\nSelect the movie option that is most closely related to the given list of movies after conducting a meticulous\nanalysis.\nGPO\nThoroughly analyze the themes, genres, and narrative elements of each film to identify the movie that best\naligns with the provided options. Make a well-informed decision based on your comprehensive evaluation.\nTable 10: Prompts optimized by different methods on the Movie Recommendation task.\n19\nMethods\nOptimized prompt\nSGDM\nCount the number of items\nAPE\nDevelop a systematic approach to accurately determine the total number of items by counting each\nindividual item separately and recording their corresponding quantities.\nAPO\nWhat is the total count of mentioned items, considering each item individually without categorization and\ncounting duplicates as separate items? Please provide the correct count as your output.\nOPRO\nHow can we solve the problem by breaking it down step by step?\nPE2\nBy employing a systematic and thorough approach, meticulously analyze each item in a step-by-step\nmanner to precisely determine the total count.\nGPO\nLet’s break down the problem systematically by deconstructing it into individual steps and accurately\ncomputing the total number of objects.\nTable 11: Prompts optimized by different methods on the Object Counting task.\nMethods\nOptimized prompt\nSGDM\nSort the given words, alphabetically, in case-sensitive order, considering the entire word, including all\ncharacters. The sorting should be done in ascending order based on the lowercase versions of the words,\nwhile preserving the original case and considering the entire word.\nAPE\nHow would you sort the given list of words in alphabetical order, considering both uppercase and lowercase\nletters?\nAPO\nSort the given list of words in lowercase alphabetical order, taking into account all characters including\nspecial characters and numbers. Ensure that the sorting process is case-insensitive and considers all\ncharacters, including special characters and numbers. Convert all letters to lowercase before sorting.\nConsider special characters and numbers in the sorting process.\nOPRO\nArrange the given words in alphabetical order by considering only the first letter of each word and exclude\npunctuation or special characters.\nPE2\nAnalyze the given words and provide a sorted list in alphabetical order.\nGPO\nArrange the following words in alphabetical order:\nTable 12: Prompts optimized by different methods on the Word Sorting task.\n20\n"
}