{
    "optim": "Unleashing the Potential of Large Language Models as Prompt Optimizers: An Analogical Analysis with Gradient-based Model Optimizers Xinyu Tang1,3*, Xiaolei Wang1,3*, Wayne Xin Zhao1,3†, Siyuan Lu4 , Yaliang Li5 and Ji-Rong Wen1,2,3 1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Information, Renmin University of China 3Beijing Key Laboratory of Big Data Management and Analysis Methods 4International School, Beijing University of Posts and Telecommunications 5Alibaba Group txy20010310@163.com, wxl1999@foxmail.com, batmanfly@gmail.com Abstract Automatic prompt optimization is an important approach to improving the performance of large language models (LLMs). Recent research demonstrates the potential of using LLMs as prompt optimizers, which can generate im- proved task prompts via iterative refinement. In this paper, we propose a novel perspective to in- vestigate the design of LLM-based prompt op- timizers, by drawing an analogy with gradient- based model optimizers. To connect these two approaches, we identify two pivotal factors in model parameter learning: update direction and update method. Focused on the two aspects, we borrow the theoretical framework and learn- ing methods from gradient-based optimization to design improved strategies for LLM-based prompt optimizers. By systematically analyz- ing a rich set of improvement strategies, we fur- ther develop a capable Gradient-inspired LLM- based Prompt Optimizer called GPO. At each step, it first retrieves relevant prompts from the optimization trajectory as the update direc- tion. Then, it utilizes the generation-based re- finement strategy to perform the update, while controlling the edit distance through a cosine- based decay strategy. Extensive experiments demonstrate the effectiveness and efficiency of GPO. In particular, GPO brings an addi- tional improvement of up to 56.8% on Big- Bench Hard and 55.3% on MMLU compared to baseline methods. The code is available at https://github.com/RUCAIBox/GPO. 1 Introduction Nowadays, prompting has become the pivotal ap- proach to unleashing the power of large language models (LLMs) (Zhao et al., 2023; Touvron et al., 2023; OpenAI, 2023). However, prompt engineer- ing is not easy and requires extensive trial-and-error efforts since LLMs are sensitive to prompts (Zhao * Equal contribution. † Corresponding author. et al., 2021; Lu et al., 2022; Wei et al., 2023). Al- though general guidelines for high-quality prompts exist (Kojima et al., 2022; Amatriain, 2024), they cannot always lead to optimal task performance. To improve the task performance of LLMs, automatic prompt optimization has been pro- posed (Zhou et al., 2023). Early work either di- rectly performs discrete optimization through meth- ods like reinforcement learning (Deng et al., 2022; Zhang et al., 2023) or performs continuous opti- mization in the embedding space of LLMs (Shin et al., 2020; Lin et al., 2023; Chen et al., 2023). However, these methods often require access to the logits or internal states of LLMs, which is in- feasible for those only accessible through APIs. In addition, they need to be specially trained for each task. Considering these issues, recent work proposes to model the optimization problem in nat- ural language and using LLMs as prompt optimiz- ers (Zhou et al., 2023; Yang et al., 2023; Ma et al., 2024). In this approach, LLMs can perform op- timization with only outputs from the task model and quickly adapt to various tasks without training. However, such a method raises a new challenge for the design of meta-prompt, which is the prompt for LLMs to perform prompt optimization. To tackle this issue, we aim to investigate the design of meta-prompts. In existing approaches, meta-prompts are often hand-crafted (Yang et al., 2023) or optimized with heuristic algorithms (Fer- nando et al., 2023). Despite the flexibility, these studies still lack principled guidelines about their designs. Our work is inspired by the great success of gradient-based optimizers in model optimiza- tion, which have been systemically studied in both theory and practice (Sun et al., 2020; Abdulkadirov et al., 2023). Since both optimizers target enhanc- ing model performance through optimization, it is feasible to connect the two different approaches via analogical analysis. In this way, we can borrow the theoretical framework and extensive research of 1 arXiv:2402.17564v1  [cs.CL]  27 Feb 2024 gradient-based model optimizers to enhance LLM- based prompt optimizers. Specifically, in this paper, we take the gradient- based optimization approach from machine learn- ing as reference, and identify two key factors for model parameter learning, namely update direc- tion (i.e., gradient calculation) and update method (i.e., gradient descent). By analogy, in LLM-based prompt optimizers, the update direction can refer to suitable prompt information informing the LLM about how to produce more effective task prompts, while the update method can refer to how the LLM utilizes such information to improve prompts. By drawing such an analogy, we can develop a more formal framework for LLM-based prompt optimiz- ers and provide guidelines to design more princi- pled meta-prompts. Furthermore, we develop a capable Gradient- inspired LLM-based Prompt Optimizer called GPO, with the best configuration from our system- atic study. We evaluate its performance across com- plex reasoning, knowledge-intensive, and common NLP tasks in various evaluation settings. When using Llama-2-7b-chat as the task model, the prompts produced by GPO surpass the instruction “Let’s think step by step” by 18.5% on Big-Bench Hard (BBH) and 7.6% on MMLU. Furthermore, GPO produces an additional improvement of up to 56.8% on BBH and 55.3% on MMLU compared with baseline methods while using fewer tokens. To the best of our knowledge, it is the first time that a systematic study has been conducted for LLM-based prompt optimizers. More specifically, it has been studied by analogy with gradient-based model optimizers, which we believe is useful to seek theoretical foundations and extend feasible approaches for prompt optimization. 2 Related Work Our work is related to the following two directions. Prompt Engineering and Optimization. Prompt engineering aims to find suitable prompts as the input for LLMs to perform various tasks. To re- duce human efforts, researchers have explored au- tomatic prompt optimization, which can be cate- gorized into continuous and discrete optimization methods. Discrete methods (Deng et al., 2022; Xu et al., 2022) directly optimize the natural language prompts through methods like reinforcement learn- ing (Deng et al., 2022; Zhang et al., 2023) and editing (Xu et al., 2022; Prasad et al., 2023). In contrast, continuous methods perform optimization in the embedding space of LLMs, allowing for op- timization through gradient (Li and Liang, 2021; Lester et al., 2021). We focus on discrete methods, especially LLM-based prompt optimizers. LLM-based Prompt Optimizers. Due to the un- precedented capabilities of LLMs, recent work starts to utilize them as prompt optimizers. One line of work (Guo et al., 2023; Yang and Li, 2023) combines LLMs with evolutionary algorithms to perform prompt optimization. Another line of work (Pryzant et al., 2023; Yang et al., 2023) aims to adapt concepts and techniques from gradient- based model optimizers (e.g., gradient (Pryzant et al., 2023) and momentum (Yang et al., 2023)) to LLM-based prompt optimizers. However, no comprehensive guidelines exist for using LLMs as prompt optimizers. We aim to tackle this with a systematic investigation, which is conducted by analogy with gradient-based model optimizers. 3 An Analogical Analysis Between Gradient-Based Model Optimizer and LLM-Based Prompt Optimizer In this section, we present an analogical analysis between model optimization and prompt optimiza- tion to build their connections and further improve existing LLM-based prompt optimizers. 3.1 Task Formulation Prompt optimization aims to find the optimal task prompt p∗ in the format of natural language that maximizes the performance on a specific task dataset D when using an LLM as the task model MT . To perform such optimization, our idea is to develop a prompt optimizer, which can be built upon some search algorithm (e.g., evolutionary al- gorithms (Guo et al., 2023)) or an LLM (Yang et al., 2023). In this paper, we focus on using an LLM as the prompt optimizer MO. Formally, the problem of prompt optimization can be formulated as: p∗ = arg max p∼MO E⟨x,y⟩∈D [F(MT (x; p), y)], (1) where p is the prompt generated by the LLM-based prompt optimizer MO, MT (x; p) represents the output from the task model for input x conditioned on the prompt p, and the function F(·) calculates the task performance based on some measurement. For the LLM-based prompt optimizer, it requires another prompt to perform the optimization for 2 the task prompt, which is usually called meta- prompt (Yang et al., 2023; Ye et al., 2023). For example, for the mathematical reasoning task, the prompt can be “Let’s solve the problem”, while the meta-prompt may be “Improve the prompt to help a model better perform mathematical reasoning”. 3.2 Analogical Analysis for Prompt Optimization Since both prompt optimizers and model opti- mizers target enhancing the model performance through some optimization algorithm, in this part, we aim to draw inspiration from the design of gradient-based model optimizers to conduct a sys- tematic analysis of LLM-based prompt optimizers. 3.2.1 Revisiting Gradient-Based Optimizers Similar to prompt optimization, model optimiza- tion aims to find the optimal values of model pa- rameters that minimize the loss function. In model optimization, gradient-based optimizers are the most widely used approaches, which iteratively update model parameters in the direction of the negative gradient of the loss function. To moti- vate our approach, we take the fundamental opti- mizer known as gradient descent (Boyd and Van- denberghe, 2014) for discussion. In the basic form of gradient descent, a single optimization step can be formulated as follows: Θk+1 = Θk − τkgk, (2) where Θk and Θk+1 are the values of model pa- rameters at the last and current steps, τk and gk are the learning rate and gradient at the current step. Gradient descent can be improved by focusing on two elements in the formula: τk and gk. For τk, learning rate schedulers (Gotmare et al., 2019) are proposed to dynamically adjust the learning rate. For gk, the concept of momentum (Sutskever et al., 2013) is introduced to include historical gradients, and its computation can be expressed as follows: vk+1 = βvk + gk = Pk i=1 βk−igi, where β repre- sents the momentum coefficient. Despite various gradient-based optimizers, they mainly model two key factors, namely update di- rection (e.g., gradient gk) and update method (e.g., direct descent update by subtracting τkgk). Our approach is inspired by the observation that ex- isting LLM-based prompt optimization methods also implicitly employ the two aspects. For exam- ple, OPRO (Yang et al., 2023) uses previous task prompts along with their performance to guide the Factor Gradient-based model optimizer LLM-based prompt optimizer Update direction Model value Prompt Gradient Reflection Momentum Trajectory Update method Learning rate Edit distance Descent Editing / Generation Table 1: Analogy between glossaries in model optimizer and prompt optimizer. direction of update and utilizes in-context learn- ing based generation as the update method. The comparison between glossaries is shown in Table 1. However, the efforts in existing work only initially explore the design of the two key factors, and we aim to conduct more in-depth and systematic in- vestigations by borrowing the idea of research in gradient-based optimization. 3.2.2 Analogical Prompt Optimization Strategies for “Update Direction” In prompt optimization, there are no explicit gradi- ents for controlling the update direction. However, we can incorporate the concept of gradient into the design of meta-prompts to improve the prompt optimization process. Analogical “Gradient” Forms. The basic func- tion of the gradient is to inform how the optimiza- tion process should adjust according to the model performance. To mimic similar effects, for an LLM with fixed parameters, it is only feasible to design suitable prompting strategies to adjust the output. Here, we consider two analogical forms to implic- itly support the gradient-like function. • Prompt+performance. One straightforward method is to include the last-round task prompt and the corresponding model performance into the meta-prompt for LLM-based optimizer MO. It leverages the capacity of LLMs to reason about how to improve prompting optimization. • Prompt+performance+reflection. Another way to solve the barrier of the gradient is to lever- age the reflection capability of LLMs (Pryzant et al., 2023). With the reflection mechanism, LLMs can generate feedback from past failures, which can be used to improve performance. Such feed- back can be seen as a form of “semantic” gradient signals (Pryzant et al., 2023). Analogical “Momentum” Forms. Inspired by the momentum method (Sutskever et al., 2013) in gradi- 3 ent descent, we consider enhancing the aforemen- tioned basic form of meta-prompt by leveraging the intermediate results accumulated in the prompt optimization process. A straightforward way is to directly include them into the meta-prompt. How- ever, it might be limited by the context length of LLMs and affected by the accumulated noise. To better utilize the optimization trajectory, we pro- pose two alternative methods. • Summarization-based trajectory. One direct approach is to summarize the intermediate results from the optimization trajectory. Specifically, at each step, we use an instruction (e.g., “Your task is to summarize the problems in the previous prompt and the current prompt.”) to let the LLM perform summarization using the summary in the last step and the result in the current step. • Retrieval-based trajectory. Another approach is to dynamically retrieve k pieces of gradients from the optimization trajectory. Specifically, we consider the following three strategies for selection: (1) recency: selecting k nearest gradients; (2) rele- vance: selecting k most relevant gradients, which are measured by the semantic similarity based on the BGE model (Xiao et al., 2023); (3) importance: selecting k most important gradients, which is mea- sured by the performance gain. 3.2.3 Analogical Prompt Optimization Strategies for “Update Method” Another important factor to consider is the update method for prompt optimization. In gradient-based model optimizers, the optimization process is con- trolled by the learning rate and fulfilled by gradient descent on parameters. Accordingly, we explore two analogical aspects: prompt variation control and meta-prompt refinement. Prompt Variation Control. After setting the opti- mization direction, it is important to adjust the pa- rameter optimization process with a suitable learn- ing rate. This is because using a either too large or too small rate can result in an oscillating or slowly converging optimization process. Similarly, without a suitable control mechanism in prompt optimization, the LLM-based optimizer might over- shoot the optimal prompt or oscillate in the opti- mization process. To mimic the similar effects of learning rate, our idea is to control the variation degree of prompt optimization, which is measured by the edit distance between two task prompts at consecutive iterations. Specifically, following Ye et al. (2023), we add an instruction (i.e., “You are allowed to change up to X words in the current prompt.”) into the meta-prompt to limit the num- ber of words that can be modified. Accordingly, we propose two control methods as follows: • Decay-based constraint. To avoid overshoot- ing the optimal solution, the decay strategy is pro- posed to gradually reduce the learning rate (Iz- mailov et al., 2018). Here, we borrow the idea to control the maximum edit distance and consider gradually reducing its value following either a lin- ear or cosine curve. In addition, following the common practice in training LLMs (Touvron et al., 2023), we reduce the constraint to approximately 20% of its maximum value until convergence. • Warmup-based constraint. To avoid instability in the early stage of optimization, the warmup strat- egy is proposed to gradually increase the learning rate at the beginning (Goyal et al., 2017). Similarly, we adopt the widely used linear warmup schedule to gradually increase the constraint for the maxi- mum edit distance to its initially set value in the initial 5% steps. Prompt Refinement Strategy. During the opti- mization process, the task prompt would be itera- tively refined, to improve the corresponding model performance. By analogy with the subtraction op- eration in gradient-based optimizers (i.e., −τkgk in Eq. (2)), we introduce two methods to update the task prompt accordingly. • Editing-based refinement. The first method directly edits the last-round task prompt to improve performance. Specifically, we add an instruction (i.e., “Modify the current prompt and get a new improved promp”) into the meta-prompt, which requires the LLM to edit the task prompt from the last step according to the update direction. This method allows for effectively exploiting the exist- ing prompt, leading to a gradual performance im- provement through a relatively stable optimization process. • Generation-based refinement. In addition to direct edits, we can also leverage the in-context learning capability of LLMs to generate refined task prompts. Specifically, we present the informa- tion regarding the update direction in the format of demonstration (e.g., “Below are the previous prompts with their scores. The score ranges from 0 to 100, and a higher score indicates better quality. Prompt: {prompt}. Score: {score}.”). Then, the LLM can follow the demonstration to generate a 4 Initial prompt: “Let’s think step by step.” Performance: 31.25 Gradient form Momentum form None S-T R-T Recency Relevance Importance P+M 41.07 41.03 41.93 42.53 41.84 P+M+R 40.34 40.63 41.55 40.81 39.73 Table 2: The performance comparison between differ- ent update directions. “P” denotes prompt, “M” de- notes performance, “R” denotes reflection, “S-T” de- notes summarization-based trajectory, and “R-T” de- notes retrieval-based trajectory. new task prompt. Compared with the editing-based updating method, the generation-based approach explores a wider range of prompt variations, which has the potential to yield improved performance but also makes the optimization process unstable. 3.3 Analogical Analysis Experiments In this part, we first describe the experiment set- ting for our analogical analysis, and then detail our findings from the experiment. 3.3.1 Experiment Setup We select a dataset from each type of task in BBH (Suzgun et al., 2023) to create a lite BBH benchmark for our analysis: i) Navigate (binary choice); ii) Movie Recommendation (multiple choice); iii) Object Counting (numeric response); iv) Word Sorting (free response). We adopt ex- act match as the metric for performance evalu- ation. We employ Llama-2-7b-chat (Touvron et al., 2023) as the task model and gpt-3.5-turbo as the prompt optimizer. The initial task prompt is “Let’s think step by step.” Other details are pre- sented in Appendix B. 3.3.2 Empirical Findings Findings For Update Direction. The results for the update direction of prompt optimization are presented in Table 2. Here are the main findings: • With regard to the form of gradient, prompt+ performance achieves better performance than prompt+performance+reflection, which brings an improvement of up to 31% compared with the ini- tial task prompt. The substantial improvement brought by prompts can be attributed to their rich semantic information about the task, which can activate the task-specific knowledge of LLMs for optimization. In contrast, LLMs are known to be Initial prompt: “Let’s think step by step.” Performance: 31.25 Refinement strategy Editing Generation Variation Control No control 42.53 42.61 Fixed 42.91 43.09 +Warmup 41.76 40.08 Linear decay 42.68 42.86 +Warmup 41.47 41.12 Cosine decay 42.95 43.75 +Warmup 40.19 41.29 Table 3: The performance comparison between different update methods. limited in their capabilities of reflection (Huang et al., 2023), which may lead to inaccurate updates. • The analogical concept of momentum can fur- ther improve the performance of prompt optimiza- tion. Among various designs, relevance-based tra- jectory emerges as the most effective one, which brings an additional 15% improvement. This can be attributed to the fact that LLMs can learn more from prompts that are contextually relevant, while it might be challenging for LLMs to fully under- stand the signal of recency or importance. By con- trast, the summarization-based trajectory proves to be less helpful. This is because summarization only captures common aspects of the trajectory while neglecting details that may be crucial. Findings For Update Method. To investigate the update method for prompt optimization, we conduct experiments using the best configuration found in the previous experiments. The results for the update method of prompt optimization are presented in Table 3. Here are the key findings: • In general, generation-based refinement per- forms better than editing-based refinement, which brings an improvement of up to 36% compared with the initial task prompt. This may be because LLMs are not specially trained for prompt opti- mization, and the demonstrations in the generation- based strategy can provide guidance about this task, thus helping LLMs learn to perform better. • Among various controlling methods for prompt variation, cosine decay-based constraint achieves the best performance, bringing an ad- ditional 10% improvement. However, unlike gradient-based model optimization, the warmup strategy does not yield improvement. This finding suggests that, at the early stage of prompt optimiza- tion, exploration plays a crucial role, while stability becomes more important in the later stage. 5 4 Our Prompt Optimizer: GPO In this section, we present our novel gradient- inspired LLM-based prompt optimizer called GPO, which leverages the insights gained from our sys- tematic study. Our approach proposes a novel de- sign of the meta-prompt, aiming to unleash the potential of LLMs as prompt optimizers. Iterative Prompt Optimization. GPO performs prompt optimization through a multi-step iterative process. At each step, the LLM first generates multiple candidate task prompts based on a meta- prompt. The meta-prompt serves as the input that guides the LLM in optimizing its prompts. Subse- quently, the task prompt with the best performance is selected for the next iteration. This iterative pro- cess continues until either the performance of the task prompt reaches a plateau or the predefined maximum number of optimization steps is reached. The Design of Meta-Prompt. As the input to the LLM, our meta-prompt consists of two key components: update direction and update method. • Update direction. To determine the update direction, we leverage the retrieval-based optimiza- tion trajectory in Section 3.2.2. This trajectory comprises a collection of past task prompts, along with their model performance. They are selected using a relevance-based strategy and are sorted in ascending order based on their performance. • Update method. After the update direction is determined, we further employ the generation- based refinement strategy in Section 3.2.3 to up- date the task prompt. Specifically, we present the trajectory in the format of demonstration in the meta-prompt. Then, the LLM can follow these demonstrations to generate a new task prompt via in-context learning. Additionally, we implement the cosine-based decay strategy in Section 3.2.3 with an instruction to control the edit distance be- tween task prompts at consecutive iterations, ensur- ing gradual and controllable changes. Furthermore, we enhance the meta-prompt by in- corporating a few task examples. These examples provide additional context to aid the LLM in under- standing the task more effectively. The complete meta-prompt is presented in Appendix C.2. Comparison of LLM-Based Prompt Optimiz- ers. Existing LLM-based prompt optimizers can be divided into two main classes according to the update direction. The first line of research, such Prompt optimizer Update direction Update method Gradient form Momentum form Prompt variation Prompt refinement APE P None None Generation APO P+R None None Editing OPRO P+M Recency None Generation PE2 P+M+R Recency Fixed Generation GPO P+M Relevance Cosine Generation Table 4: Comparisons of GPO with existing LLM-based prompt optimizers, including APE (Zhou et al., 2023), APO (Pryzant et al., 2023), OPRO (Yang et al., 2023), and PE2 (Ye et al., 2023). “P” refers to prompt, “M” refers to performance, and “R” refers to reflection. as APO (Pryzant et al., 2023) and PE2 (Ye et al., 2023), leverages the reflection capability of LLMs to produce textual “gradients” as the update direc- tion. Another line of research, such as OPRO (Yang et al., 2023) and APE (Zhou et al., 2023), directly uses task prompts to derive the update direction. Our approach is based on the systematic investiga- tion of the update direction as well as the update method. In particular, we propose several novel designs for the meta-prompt: relevance-based tra- jectory as the update direction and decay-based constraint for edit distance in the update method. Table 4 presents a detailed comparison. 5 Experiments In this section, we first set up the experiments and then report the results and detailed analysis. 5.1 Experimental Setup Tasks and Datasets. We select datasets from three groups of tasks for the experiment: Big- Bench Hard (BBH) (Suzgun et al., 2023) and GSM8K (Cobbe et al., 2021) for complex reason- ing tasks, MMLU (Hendrycks et al., 2021) for knowledge-intensive tasks, and WSC (Levesque et al., 2012) and WebNLG (Gardent et al., 2017) for common NLP tasks. Due to computational lim- itations, we sample a subset from each dataset for the main experiment. In addition, we use the lite BBH benchmark in Section 3.3.1 for detailed anal- ysis. Other details are presented in Appendix C.1. Baselines. We select several representative meth- ods for comparison, including existing LLM-based prompt optimizers and one adapted from gradient- based model optimizers. (1) SGDM (Sutskever et al., 2013) is a momentum-based model opti- mizer. We adapt it for prompt optimization using 6 the summarization-based trajectory and the editing- based refinement strategy. (2) APE (Zhou et al., 2023) utilizes LLMs to generate semantically simi- lar variants of task prompts and selects one with the best performance. (3) APO (Pryzant et al., 2023) first uses reflection to obtain the gradient and then edits the task prompt accordingly. (4) OPRO (Yang et al., 2023) incorporates historical prompts with their scores into the meta-prompt. (5) PE2 (Ye et al., 2023) adds historical prompts and reflection into the meta-prompt and controls the edit distance with a fixed constraint. In addition, we consider the baseline without an instruction (“Empty”) and the instruction “Let’ think step by step.” from chain-of- thought prompting (“CoT”) (Kojima et al., 2022) for performance comparison. Evaluation Metrics. We report the average ac- curacy of all the subtasks for BBH and MMLU following Suzgun et al. (2023) and Hendrycks et al. (2021), accuracy for GSM8K following Cobbe et al. (2021), ROUGE-L (Lin, 2004) for WSC and WebNLG following Wang et al. (2022). Implementation Details. For the task model, we use both the base model (i.e., Llama-2-7b) and the instruction-tuned models (i.e., Llama-2-7b-chat, GPT-3.5-turbo, and GPT-4). For the prompt opti- mizer, we use GPT-3.5-turbo and GPT-4. Unless otherwise specified, we use Llama-2-7b-chat as the task model and GPT-3.5-turbo as the prompt optimizer throughout experiments. For the ini- tial task prompt, we use the original ones from Hendrycks et al. (2021) for MMLU, “Let’s think step by step.” from Kojima et al. (2022) for GSM8K and BBH, and “Let’s solve the problem.” from Yang et al. (2023) for WSC and WebNLG. We repeat all the experiments three times and re- port the average of the results. Other details are presented in Appendix C.2. 5.2 Main Results Table 5 and Table 6 present the results of different methods for prompt optimization across various tasks and evaluation settings. First, when only considering the task prompt, we can see that trajectory-based methods (i.e., SGDM, OPRO, PE2, and GPO) perform very well. One possible reason is that the trajectory helps the prompt optimizer pay more attention to the im- portant information instead of the noise in the current step. Furthermore, our prompt optimizer GPO achieves the best performance across all tasks. Task Complex reasoning task Knowledge intensive task Common NLP task Dataset BBH GSM8K MMLU WSC WebNLG Empty 30.51 22.00 35.96 60.67 32.14 CoT 29.91 24.00 36.36 59.33 31.11 SGDM 33.30 27.33 37.30 64.00 38.01 APE 32.94 25.00 37.53 62.00 36.49 APO 32.97 25.33 37.75 62.00 34.92 OPRO 33.29 26.67 37.88 63.33 37.89 PE2 33.43 25.33 38.15 62.67 39.10 GPO 35.43 28.33 39.14 65.33 42.51 Table 5: Performance comparison using only the task prompts obtained from different methods. Task model Llama-2-7b Llama-2-7b-chat Setting Inst. + Demo. Inst. Inst. + Demo. Empty 40.28 32.29 36.63 CoT 36.46 31.25 34.20 SGDM 42.19 40.63 35.77 APE 42.54 42.01 36.29 APO 42.19 40.34 36.29 OPRO 42.02 42.14 36.46 PE2 42.88 42.01 36.81 GPO 45.48 43.75 38.02 Table 6: Performance comparison under different evalu- ation settings. “Inst.” denotes instruction and “Demo.” denotes demonstration. Our relevance-based trajectory provides semanti- cally similar demonstrations that can be effectively learned by the LLM, while the cosine-based decay strategy can control the optimization process in a fine-grained manner through edit distance. Second, under various evaluation settings for the lite BBH benchmark, it can be observed that GPO not only excels in the “Instruction” setting but also yields substantial gains in the “Instruc- tion + Demonstration” setting for both the base model and the instruction-tuned variant. Even in the scenario that is challenging for baselines (i.e., Llama-2-7b-chat with “Instruction + Demonstra- tion”), our approach still demonstrates strong im- provement over the empty prompt. This showcases the versatility of our approach in both zero-shot and few-shot evaluation settings. 5.3 Detailed Analysis Next, we conduct a detailed analysis of our prompt optimizer GPO from the following aspects. The Impact of Model Selection. To con- firm the effectiveness of GPO across different models, we explore the impact of model selec- tion. Specifically, for the prompt optimizer, we 7 Prompt optimizer Task model Accuracy (before / after) GPT-3.5-turbo Llama-2-7b-chat 31.25 / 43.75 GPT-3.5-turbo 62.15 / 67.02 GPT-4 73.61 / 76.56 GPT-4 Llama-2-7b-chat 31.25 / 44.97 GPT-3.5-turbo 62.15 / 67.80 GPT-4 73.61 / 78.65 Table 7: The performance before/after prompt optimiza- tion with different models as the prompt optimizer and the task model. Initial prompt Accuracy (before / after) Default Let’s think step by step. 31.25 / 43.75 Instructive Let’s think about this logically. 32.64 / 44.40 First, 33.85 / 43.15 Let’s solve this problem by splitting it into steps. 30.38 / 37.85 Misleading Don’t think. Just feel. 29.34 / 39.59 Let’s think step by step but reach an incorrect answer. 26.22 / 41.15 Let’s count the number of \"a\" in the question 26.73 / 36.11 Irrelevant By the way, I found a good restaurant nearby. 30.03 / 34.89 Abrakadabra! 29.69 / 35.42 It’s a beautiful day. 30.55 / 38.02 Table 8: The performance before/after prompt optimiza- tion with different initial prompts. employ GPT-3.5-turbo and GPT-4, while for the task model, we utilize Llama-2-7b-chat, GPT-3.5-turbo and GPT-4. Table 7 presents the results on the lite BBH benchmark. In gen- eral, GPO demonstrates remarkable capabilities for prompt optimization across various scenar- ios, including strong-to-weak optimization, self- optimization, as well as weak-to-strong optimiza- tion. This indicates the versatility of our framework. In particular, GPT-4 can consistently find better task prompts than GPT-3.5-turbo, which suggests the need for a capable model as the prompt optimizer. The Impact of Initial Prompts. In our main experiment, we take “Let’s think step by step.” as the initial prompt. In this part, we aim to explore the impact of initial task prompts. Specifically, we consider prompts from three categories (instruc- tive, misleading, and irrelevant) and select three prompts from each category following Kojima et al. (2022). Table 8 presents the results on the lite BBH benchmark. In general, GPO can consistently boost performance across various initial prompts. This observation underscores the versatility of GPO as 0 2 4 6 8 10 12 Optimization steps 38 43 48 53 58 63 68 Accuracy (%) SGDM APE APO OPRO PE2 GPO (a) Optimization curve GPOSGDMAPE APOOPRO PE2 Method 0 500 1000 1500 2000 Consumed tokens (b) Token consumption Figure 1: The efficiency of our approach GPO w.r.t. optimization steps and token consumption. a prompt optimizer. Furthermore, the efficacy of optimization is more pronounced with relevant ini- tial prompts (i.e., instructive and misleading). It means that prompt initialization is still important, especially in conveying task-specific information. The Efficiency of Optimization. LLM-based prompt optimization requires multiple interactions with the LLM. In this part, we investigate the ef- ficiency of GPO from the optimization curve and token consumption. First, Figure 1a shows that on the movie recommendation dataset, GPO exhibits rapid enhancement of the task prompt in the early stage, followed by consistent performance improve- ment in the later stage of optimization. Second, as depicted in Figure 1b, the average token consump- tion of GPO on the lite BBH benchmark is much lower than SGDM, APO, and PE2, and compara- ble to APR and OPRO. Since GPO only utilizes task prompts to derive the update direction and per- forms fine-grained control over the variation, it can achieve better performance with high efficiency. 6 Conclusion We present GPO, a novel gradient-inspired LLM- based prompt optimizer. It utilizes LLMs to au- tomatically optimize prompts, drawing inspira- tion from gradient-based model optimization tech- niques. By identifying the two crucial aspects of update direction and update method in model op- timization, we enhance prompt optimizers by in- corporating concepts from gradient-based model optimizers. Through extensive experiments, GPO demonstrates remarkable capabilities for prompt optimization across diverse tasks, models, and ini- tial prompts. Moreover, it surpasses competitive baselines while consuming fewer tokens. In future work, we will explore more guidelines to further enhance the effectiveness and efficiency of LLM-based prompt optimizers. 8 7 Limitations In this work, we improve the design of LLM- based prompt optimizers by drawing an analogy with the most widely used gradient-based optimiz- ers in model optimization (e.g., gradient descent). More advanced model optimizers like Newton’s method (Boyd and Vandenberghe, 2014) and the corresponding improvement for meta-prompts re- main to be investigated. In addition, due to the com- putational and budget limitations, we only conduct experiments on representative tasks and models. References Ruslan Abdulkadirov, Pavel Lyakhov, and Nikolay Nagornov. 2023. Survey of optimization algo- rithms in modern neural networks. Mathematics, 11(11):2466. Xavier Amatriain. 2024. Prompt design and engineer- ing: Introduction and advanced methods. CoRR, abs/2401.14423. Stephen P. Boyd and Lieven Vandenberghe. 2014. Con- vex Optimization. Cambridge University Press. Lichang Chen, Jiuhai Chen, Tom Goldstein, Heng Huang, and Tianyi Zhou. 2023. Instructzero: Ef- ficient instruction optimization for black-box large language models. CoRR, abs/2306.03082. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word prob- lems. CoRR, abs/2110.14168. Nicholas Crispino, Kyle Montgomery, Fankun Zeng, Dawn Song, and Chenguang Wang. 2023. Agent instructs large language models to be general zero- shot reasoners. CoRR, abs/2310.03710. Mingkai Deng, Jianyu Wang, Cheng-Ping Hsieh, Yihan Wang, Han Guo, Tianmin Shu, Meng Song, Eric P. Xing, and Zhiting Hu. 2022. Rlprompt: Optimizing discrete text prompts with reinforcement learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 3369–3391. Association for Com- putational Linguistics. Chrisantha Fernando, Dylan Banarse, Henryk Michalewski, Simon Osindero, and Tim Rock- täschel. 2023. Promptbreeder: Self-referential self-improvement via prompt evolution. CoRR, abs/2309.16797. Claire Gardent, Anastasia Shimorina, Shashi Narayan, and Laura Perez-Beltrachini. 2017. Creating training corpora for NLG micro-planners. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, ACL 2017, Vancouver, Canada, July 30 - August 4, Volume 1: Long Pa- pers, pages 179–188. Association for Computational Linguistics. Akhilesh Gotmare, Nitish Shirish Keskar, Caiming Xiong, and Richard Socher. 2019. A closer look at deep learning heuristics: Learning rate restarts, warmup and distillation. In 7th International Confer- ence on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter No- ordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Ac- curate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677. Qingyan Guo, Rui Wang, Junliang Guo, Bei Li, Kaitao Song, Xu Tan, Guoqing Liu, Jiang Bian, and Yu- jiu Yang. 2023. Connecting large language models with evolutionary algorithms yields powerful prompt optimizers. CoRR, abs/2309.08532. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein- hardt. 2021. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Jie Huang, Xinyun Chen, Swaroop Mishra, Huaixiu Steven Zheng, Adams Wei Yu, Xiny- ing Song, and Denny Zhou. 2023. Large language models cannot self-correct reasoning yet. CoRR, abs/2310.01798. Pavel Izmailov, Dmitrii Podoprikhin, Timur Garipov, Dmitry P. Vetrov, and Andrew Gordon Wilson. 2018. Averaging weights leads to wider optima and better generalization. In Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018, Monterey, California, USA, August 6-10, 2018, pages 876–885. AUAI Press. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu- taka Matsuo, and Yusuke Iwasawa. 2022. Large lan- guage models are zero-shot reasoners. In Advances in Neural Information Processing Systems 35: An- nual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Domini- can Republic, 7-11 November, 2021, pages 3045– 3059. Association for Computational Linguistics. Hector J. Levesque, Ernest Davis, and Leora Morgen- stern. 2012. The winograd schema challenge. In 9 Principles of Knowledge Representation and Rea- soning: Proceedings of the Thirteenth International Conference, KR 2012, Rome, Italy, June 10-14, 2012. AAAI Press. Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Asso- ciation for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 4582– 4597. Association for Computational Linguistics. Chin-Yew Lin. 2004. ROUGE: A package for auto- matic evaluation of summaries. In Text Summariza- tion Branches Out, pages 74–81, Barcelona, Spain. Association for Computational Linguistics. Xiaoqiang Lin, Zhaoxuan Wu, Zhongxiang Dai, Wenyang Hu, Yao Shu, See-Kiong Ng, Patrick Jaillet, and Bryan Kian Hsiang Low. 2023. Use your INSTINCT: instruction optimization using neu- ral bandits coupled with transformers. CoRR, abs/2310.02905. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming few- shot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Compu- tational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022, pages 8086– 8098. Association for Computational Linguistics. Ruotian Ma, Xiaolei Wang, Xin Zhou, Jian Li, Nan Du, Tao Gui, Qi Zhang, and Xuanjing Huang. 2024. Are large language models good prompt optimizers? CoRR, abs/2402.02101. OpenAI. 2023. GPT-4 technical report. CoRR, abs/2303.08774. Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. 2023. Grips: Gradient-free, edit-based in- struction search for prompting large language models. In Proceedings of the 17th Conference of the Euro- pean Chapter of the Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May 2-6, 2023, pages 3827–3846. Association for Com- putational Linguistics. Reid Pryzant, Dan Iter, Jerry Li, Yin Tat Lee, Chen- guang Zhu, and Michael Zeng. 2023. Automatic prompt optimization with \"gradient descent\" and beam search. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Process- ing, EMNLP 2023, Singapore, December 6-10, 2023, pages 7957–7968. Association for Computational Linguistics. Taylor Shin, Yasaman Razeghi, Robert L. Logan IV, Eric Wallace, and Sameer Singh. 2020. Autoprompt: Eliciting knowledge from language models with au- tomatically generated prompts. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, Novem- ber 16-20, 2020, pages 4222–4235. Association for Computational Linguistics. Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R. Brown, Adam Santoro, Aditya Gupta, Adrià Garriga-Alonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat Agarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexander W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Alicia Par- rish, Allen Nie, Aman Hussain, Amanda Askell, Amanda Dsouza, Ameet Rahane, Anantharaman S. Iyer, Anders Andreassen, Andrea Santilli, Andreas Stuhlmüller, Andrew M. Dai, Andrew La, Andrew K. Lampinen, Andy Zou, Angela Jiang, Angelica Chen, Anh Vuong, Animesh Gupta, Anna Gottardi, Anto- nio Norelli, Anu Venkatesh, Arash Gholamidavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubarajan, Asher Mullokandov, Ashish Sabharwal, Austin Her- rick, Avia Efrat, Aykut Erdem, Ayla Karakas, and et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. CoRR, abs/2206.04615. Shiliang Sun, Zehui Cao, Han Zhu, and Jing Zhao. 2020. A survey of optimization methods from a machine learning perspective. IEEE Trans. Cybern., 50(8):3668–3681. Ilya Sutskever, James Martens, George E. Dahl, and Ge- offrey E. Hinton. 2013. On the importance of initial- ization and momentum in deep learning. In Proceed- ings of the 30th International Conference on Machine Learning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013, volume 28 of JMLR Workshop and Conference Proceedings, pages 1139–1147. JMLR.org. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Se- bastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2023. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Com- putational Linguistics: ACL 2023, Toronto, Canada, July 9-14, 2023, pages 13003–13051. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al- bert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton- Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, An- thony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di- ana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar- tinet, Todor Mihaylov, Pushkar Mishra, Igor Moly- bog, Yixin Nie, Andrew Poulton, Jeremy Reizen- stein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subrama- nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- 10 lor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurélien Ro- driguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine- tuned chat models. CoRR, abs/2307.09288. Yizhong Wang, Swaroop Mishra, Pegah Alipoormo- labashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva Naik, Arjun Ashok, Arut Selvan Dhanasekaran, An- jana Arunkumar, David Stap, Eshaan Pathak, Gi- annis Karamanolakis, Haizhi Gary Lai, Ishan Puro- hit, Ishani Mondal, Jacob Anderson, Kirby Kuz- nia, Krima Doshi, Kuntal Kumar Pal, Maitreya Pa- tel, Mehrad Moradshahi, Mihir Parmar, Mirali Puro- hit, Neeraj Varshney, Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. 2022. Super-naturalinstructions: Gen- eralization via declarative instructions on 1600+ NLP tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, De- cember 7-11, 2022, pages 5085–5109. Association for Computational Linguistics. Jerry W. Wei, Jason Wei, Yi Tay, Dustin Tran, Al- bert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, and Tengyu Ma. 2023. Larger language models do in-context learning dif- ferently. CoRR, abs/2303.03846. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighof. 2023. C-pack: Packaged resources to advance general chinese embedding. CoRR, abs/2309.07597. Hanwei Xu, Yujun Chen, Yulun Du, Nan Shao, Yang- gang Wang, Haiyu Li, and Zhilin Yang. 2022. GPS: genetic prompt search for efficient few-shot learning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 8162–8171. Association for Com- putational Linguistics. Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun Chen. 2023. Large language models as optimizers. CoRR, abs/2309.03409. Heng Yang and Ke Li. 2023. Instoptima: Evolution- ary multi-objective instruction optimization via large language model-based instruction operators. In Find- ings of the Association for Computational Linguis- tics: EMNLP 2023, Singapore, December 6-10, 2023, pages 13593–13602. Association for Computational Linguistics. Qinyuan Ye, Maxamed Axmed, Reid Pryzant, and Fereshte Khani. 2023. Prompt engineering a prompt engineer. CoRR, abs/2311.05661. Tianjun Zhang, Xuezhi Wang, Denny Zhou, Dale Schu- urmans, and Joseph E. Gonzalez. 2023. TEMPERA: test-time prompt editing via reinforcement learning. In The Eleventh International Conference on Learn- ing Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Be- ichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A survey of large language models. CoRR, abs/2303.18223. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improv- ing few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Vir- tual Event, volume 139 of Proceedings of Machine Learning Research, pages 12697–12706. PMLR. Yongchao Zhou, Andrei Ioan Muresanu, Ziwen Han, Keiran Paster, Silviu Pitis, Harris Chan, and Jimmy Ba. 2023. Large language models are human-level prompt engineers. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. 11 1 3 5 7 9 Length 42.8 43.1 43.4 43.7 44.0 44.3 Accuracy (%) (a) Trajectory length 0.2 0.4 0.6 0.8 1.0 Temperature 42.0 42.5 43.0 43.5 44.0 Accuracy (%) (b) Temperature in GPO Figure 2: Performance comparison w.r.t. the tempera- ture of the LLM in GPO and the length of the trajectory. A Hyper-Parameter Analysis GPO includes a few hyper-parameters to tune. Here, we report the tuning results of two hyper- parameters on the lite BBH benchmark: the temper- ature of the prompt optimizer and the length of the trajectory. The performance curve depicting these results is illustrated in Figure 2. We can see that GPO achieves the best performance when setting the length of the trajectory to 7. A trajectory that is too short does not provide sufficient context for the LLM to engage in effective in-context learning. Conversely, a long trajectory may introduce exces- sive noise, negatively impacting performance. In addition, the performance shows an upward trend as temperature increases, reaching its peak at 1.0. It suggests the significance of exploration in opti- mizing task prompts. B Additional Details for the Setup of Analogical Analysis Tasks and Metrics. Following Yang et al. (2023), we utilize Big-Bench Hard (BBH) (Suzgun et al., 2023) for evaluation. BBH is a suite of 23 chal- lenging BIG-Bench tasks (Srivastava et al., 2022) that covers various kinds of reasoning capabilities. Due to the constraints of computational resources, we select a dataset from each type of task in BBH to create a lite benchmark for our analysis: i) Navi- gate (Binary choice); ii) Movie Recommendation (Multiple choice); iii) Object Counting (Numeric response); iv) Word Sorting (Free response). For each dataset, we split it into train/valid/test sets with a ratio of 2:2:6. Following Suzgun et al. (2023), we adopt the exact match as the metric for performance evaluation. Implementation Details. Following Crispino et al. (2023), we select Llama-2-7b-chat (Tou- vron et al., 2023) as the task model and set its tem- perature to 0 to make the output as deterministic as possible. For the prompt optimizer, we employ gpt-3.5-turbo, which is the underlying model of ChatGPT. We set its temperature to 1.0 to encour- age the generation to be more diverse. To help the prompt optimizer understand the downstream task, following Yang et al. (2023), we randomly sample 3 examples from the dataset and fill them into the meta-prompt of the prompt optimizer. In the process of optimization, we take the instruction “Let’s think step by step.” as the initial prompt and insert it at the end of the question to obtain better performance following Suzgun et al. (2023). At each step, we first prompt the optimizer to gener- ate 8 candidate task prompts, and then select the best-performing one as the task prompt for the next iteration following Yang et al. (2023); Ye et al. (2023). The optimization process lasts for at most 3 epochs with a batch size of 8. We repeat all the experiments three times and report the average of the results. The meta-prompts we used are detailed in Appendix D.1. C Additional Details for the Setup of Experiment C.1 The Statistics of Datasets As mentioned in Section 5.1, we sample a subset of the dataset for efficient evaluation. For BBH and MMLU, we split the entire dataset into training, validation, and test sets with a ratio of 2:2:6. For GSM8k and WebNLG, we randomly sample 100 examples as the training set, 100 for the validation set, and 300 for the test set. For WSC, we randomly sample 50 examples as the training set, 50 for the validation set, and 150 for the test set. C.2 Implementation Details For the task model, we use both the base model (i.e., Llama-2-7b) and the instruction-tuned mod- els (i.e., Llama-2-7b-chat, GPT-3.5-turbo, and GPT-4). The temperature is 0. For the prompt opti- mizer, we use GPT-3.5-turbo and GPT-4. The temperature is 1.0. Unless otherwise specified, we use Llama-2-7b-chat as the task model and GPT-3.5-turbo as the prompt optimizer through- out experiments. In the meta-prompt, we include 3 task examples and 7 historical task prompts. For the initial task prompt, we use the original ones from Hendrycks et al. (2021) for MMLU, “Let’s think step by step.” from Kojima et al. (2022) for GSM8K and BBH, and “Let’s solve the problem.” 12 from Yang et al. (2023) for WSC and WebNLG. At each step, the optimizer generates 8 candidates, and the best-performing one is selected. The optimiza- tion lasts for at most 3 epochs with a batch size of 8. We repeat all the experiments three times and report the average of the results. The meta-prompts are detailed in Appendix D.2. D Meta-Prompt D.1 Analogical Analysis Here are the meta-prompts we used in Section 3. • Prompt+performance. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} • Prompt+performance+reflection Your task is to point out the problems with the current prompt based on the wrong examples. The current prompt is: {current prompt} But this prompt gets the following examples wrong. You should analyze the differences between wrong predictions and ground truth answers, and carefully consider why this prompt led to incorrect predictions. Below are the task examples with Queston, Wrong prediction, and Ground truth answer. {error demonstrations} Give a reason why the prompt could have gotten these examples wrong. Wrap the reason with <START> and <END>. • Summarization-based trajectory Your task is to integrate the problems in the previous prompt and the current prompt. Below are the problems that arose from the previous prompts. {previous problems} Below are the problems of the current prompt. {current problem} You should integrate the problems of the pre- vious prompt and the current prompt. Wrap the integrated problems with <START> and <END>. • Retrieval-based trajectory Below are the previous prompts with their scores. The score ranges from 0 to 100, and higher scores indicate better quality. Prompt: {prompt1} Score: {score1} Prompt: {prompt2} Score: {score2} Prompt: {prompt3} Score: {score3} . . . 13 • Editing-based refinement Your task is to modify the current prompt to replace <Prompt>. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} The current prompt is: {current prompt} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Modify the current prompt and get a new im- proved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the modified prompt with <START> and <END>. • Generation-based refinement Your task is to write a prompt to replace <Prompt>. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} The current prompt is: {current prompt} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Write a new improved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the new prompt with <START> and <END>. 14 D.2 Experiment Here are the meta-prompts we used in Section 5. • SGDM The meta-prompt for gradient Your task is to point out the problems with the current prompt based on the wrong examples. The current prompt is: {current prompt} But this prompt gets the following examples wrong. You should analyze the differences between wrong predictions and ground truth answers, and carefully consider why this prompt led to incorrect predictions. Below are the task examples with Queston, Wrong prediction, and Ground truth answer. {error demonstrations} Give a reason why the prompt could have gotten these examples wrong. Wrap the reason with <START> and <END>. The meta-prompt for momentum Your task is to integrate the problems in the previous prompt and the current prompt. Below are the problems that arose from the previous prompts. {previous problems} Below are the problems of the current prompt. {current problem} You should integrate the problems of the pre- vious prompt and the current prompt. Wrap the integrated problems with <START> and <END>. The meta-prompt for update Your task is to modify the current prompt to replace <Prompt>. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} The current prompt is: {current prompt} Below are the problems with this prompt. {problems} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Modify the current prompt and get a new im- proved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the modified prompt with <START> and <END>. 15 • APE The meta-prompt for update Your task is to write a prompt to replace <Prompt>. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} The current prompt is: {current prompt} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Write a new improved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the new prompt with <START> and <END>. • APO The meta-prompt for gradient Your task is to point out the problems with the current prompt based on the wrong examples. The current prompt is: {current prompt} But this prompt gets the following examples wrong. You should analyze the differences between wrong predictions and ground truth answers, and carefully consider why this prompt led to incorrect predictions. Below are the task examples with Queston, Wrong prediction, and Ground truth answer. {error demonstrations} Give a reason why the prompt could have gotten these examples wrong. Wrap the reason with <START> and <END>. The meta-prompt for update Your task is to modify the current prompt to replace <Prompt>. Below is the current prompt with its score. The score ranges from 0 to 100, and higher score indicates better quality. Prompt: {current prompt} Score: {current prompt score} The current prompt is: {current prompt} Below are the problems with this prompt. {problems} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Modify the current prompt and get a new im- proved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the modified prompt with <START> and <END>. 16 • OPRO The meta-prompt for update Your task is to write a prompt to replace <Prompt>. Below are the previous prompts with their scores. The score ranges from 0 to 100, and higher scores indicate better quality. Prompt: {prompt1} Score: {score1} Prompt: {prompt2} Score: {score2} Prompt: {prompt3} Score: {score3} . . . The current prompt is: {current prompt} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Carefully analyze the previous prompts and their scores, and write a new improved prompt to replace <Prompt> {prompt position description} in the task examples. Wrap the new prompt with <START> and <END>. • PE2 The meta-prompt for gradient Your task is to point out the problems with the current prompt based on the wrong examples. The current prompt is: {current prompt} But this prompt gets the following examples wrong. You should analyze the differences between wrong predictions and ground truth answers, and carefully consider why this prompt led to incorrect predictions. Below are the task examples with Queston, Wrong prediction, and Ground truth answer. {error demonstrations} Give a reason why the prompt could have gotten these examples wrong. Wrap the reason with <START> and <END>. 17 The meta-prompt for update Your task is to write a prompt to replace <Prompt>. Below are the previous prompts with their scores. The score ranges from 0 to 100, and higher scores indicate better quality. Prompt: {prompt1} Score: {score1} Prompt: {prompt2} Score: {score2} Prompt: {prompt3} Score: {score3} . . . The current prompt is: {current prompt} Below are the problems with this prompt. {problems} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Carefully analyze the previous prompts and their scores, and write a new improved prompt to replace <Prompt> {prompt position description} in the task examples. You are allowed to change up to {modified word number} words in the current prompt. Wrap the new prompt with <START> and <END>. • GPO The meta-prompt for update Your task is to write a prompt to replace <Prompt>. Below are the previous prompts with their scores. The score ranges from 0 to 100, and higher scores indicate better quality. Prompt: {prompt1} Score: {score1} Prompt: {prompt2} Score: {score2} Prompt: {prompt3} Score: {score3} . . . The current prompt is: {current prompt} The following exemplars show how to apply the prompt: you replace <Prompt> in each input with your new prompt, then read the input and give an output. We say your output is wrong if it is different from the given output, and we say your output is correct if they are the same. {task examples} Carefully analyze the previous prompts and their scores, and write a new improved prompt to replace <Prompt> {prompt position description} in the task examples. You are allowed to change up to {modified word number} words in the current prompt. Wrap the new prompt with <START> and <END>. E Prompts Optimized by Different Methods Here, we present the prompts optimized by all the methods on the lite BBH benchmark. Prompts on the other tasks can be seen at https://github. com/RUCAIBox/GPO. 18 Methods Optimized prompt SGDM Based on your current facing direction and any changes in direction, will following these step-by-step instructions, with explicit reference to direction and orientation, lead you back to the starting point? APE Will following the given set of instructions result in returning to the starting point? APO Based on the provided instructions, including the starting direction and the changes in direction throughout the sequence, determine if the person will return to the starting point. OPRO Think systematically and consider each step to determine the correct answer. PE2 Carefully analyze the given instructions step by step and determine if you will return to the starting point. GPO Analyze the given step-by-step instructions in detail and determine if they will guide you back to the starting point. Carefully evaluate each instruction and vividly imagine the movements to make your decision. Table 9: Prompts optimized by different methods on the Navigate task. Methods Optimized prompt SGDM What specific aspects of storytelling style, narrative structure, plot structure, character development, and narrative techniques should be considered when finding a movie similar to films like Pulp Fiction, Forrest Gump, Dances with Wolves, and The Usual Suspects? Analyze these elements in each movie’s construction and compare them against the given list to determine similarity. Focus on shared storytelling elements rather than thematic or genre similarities. Keep in mind that the most important factor in determining similarity might not be the overall theme or genre. Provide a single letter as your answer. APE Which of the following movies is most similar to the given movies based on their genre, themes, and plot elements? Choose the best option from the following choices: APO Which movie from the following options is most similar to the given movies, taking into account specific themes, plot, genre, characters, setting, tone, and style? Please provide a ranked list of the relevant elements mentioned above in determining similarity between movies. Consider the highest ranked element as the primary criteria and subsequent elements as secondary criteria in determining similarity. Additionally, consider the overall popularity and critical acclaim of the movies when making your selection. This will help ensure more accurate predictions. OPRO Which of the given movies is most similar to the listed options? Consider the movies Batman, The Usual Suspects, The Silence of the Lambs, and Jurassic Park. Choose the option that closely matches the given movies. PE2 Select the movie option that is most closely related to the given list of movies after conducting a meticulous analysis. GPO Thoroughly analyze the themes, genres, and narrative elements of each film to identify the movie that best aligns with the provided options. Make a well-informed decision based on your comprehensive evaluation. Table 10: Prompts optimized by different methods on the Movie Recommendation task. 19 Methods Optimized prompt SGDM Count the number of items APE Develop a systematic approach to accurately determine the total number of items by counting each individual item separately and recording their corresponding quantities. APO What is the total count of mentioned items, considering each item individually without categorization and counting duplicates as separate items? Please provide the correct count as your output. OPRO How can we solve the problem by breaking it down step by step? PE2 By employing a systematic and thorough approach, meticulously analyze each item in a step-by-step manner to precisely determine the total count. GPO Let’s break down the problem systematically by deconstructing it into individual steps and accurately computing the total number of objects. Table 11: Prompts optimized by different methods on the Object Counting task. Methods Optimized prompt SGDM Sort the given words, alphabetically, in case-sensitive order, considering the entire word, including all characters. The sorting should be done in ascending order based on the lowercase versions of the words, while preserving the original case and considering the entire word. APE How would you sort the given list of words in alphabetical order, considering both uppercase and lowercase letters? APO Sort the given list of words in lowercase alphabetical order, taking into account all characters including special characters and numbers. Ensure that the sorting process is case-insensitive and considers all characters, including special characters and numbers. Convert all letters to lowercase before sorting. Consider special characters and numbers in the sorting process. OPRO Arrange the given words in alphabetical order by considering only the first letter of each word and exclude punctuation or special characters. PE2 Analyze the given words and provide a sorted list in alphabetical order. GPO Arrange the following words in alphabetical order: Table 12: Prompts optimized by different methods on the Word Sorting task. 20 "
}