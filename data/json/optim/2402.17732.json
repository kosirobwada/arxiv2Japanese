{
    "optim": "Batched Nonparametric Contextual Bandits Rong Jiang1 and Cong Ma2 1Committee on Computational and Applied Mathematics, University of Chicago 2Department of Statistics, University of Chicago February 28, 2024 Abstract We study nonparametric contextual bandits under batch constraints, where the expected reward for each action is modeled as a smooth function of covariates, and the policy updates are made at the end of each batch of observations. We establish a minimax regret lower bound for this setting and propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret (up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins, carefully aligning their widths with the batch size. We also show the suboptimality of static binning under batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest that a nearly constant number of policy updates can attain optimal regret in the fully online setting. 1 Introduction Recent years have witnessed substantial progress in the field of sequential decision making under uncertainty. Especially noteworthy are the advancements in personalized decision making, where the decision maker uses side-information to make customized decision for a user. The contextual bandit framework has been widely adopted to model such problems because of its capability and elegance [35, 53, 6]. In this framework, one interacts with an environment for a number of rounds: at each round, one is given a context, picks an action, and receives a reward. One can update the action-assignment policy based on previous observations and the goal is to maximize the expected cumulative rewards. For example, in online news recommendation, a recommendation algorithm selects an article for each newly arrived user based on the user’s contextual information, and observes whether the user clicks the article or not. The goal is to try to maximize the number of clicks received. Apart from news recommendation, contextual bandits have found numerous applications in other fields such as clinical trials, personalized medicine, and online advertising [30, 62, 13]. At the core of designing a contextual bandit algorithm is deciding how to update the policy based on prior observations. A standard metric of performance for bandit algorithms is regret, which is the expected difference between the cumulative rewards obtained by an oracle who knows the optimal action for every context and that obtained by the actual algorithm under consideration. Many existing regret optimal bandit algorithms require a policy update per observation (unit) [4, 1, 39, 34]. At a first glance, such frequent policy updates are needed so that the algorithm can quickly learn the optimal action under each context and reduce regret. However, this kind of algorithm ignores an important concern in the practice of sequential decision making—the batch constraint. In many real world scenarios, the data often arrive in batches: the statistician can only observe the outcomes of the policy at the end of a batch, and then decides what to do for the next batch. For example, this batch constraint is ubiquitous in clinical trials: statisticians need to divide the participants into batches, determine a treatment allocation policy before the batch starts, and then observe all the outcomes at the end of the batch [49]. Policy updates are made per batch instead of per unit. In fact, it is infeasible to apply unit-wise policy update in this case because observing the effect of a treatment takes time and if one waits for the result before deciding how to treat the next patient, the entire experiment will take too long to complete when the number of participants is huge. The batch constraint also appears in areas such as online 1 arXiv:2402.17732v1  [math.ST]  27 Feb 2024 marketing, crowdsourcing, and simulations [8, 50, 31, 15]. Clearly, the batch constraint presents additional challenges to online learning. Indeed, from an information perspective, the statistician’s information set is largely restricted since she can only observe all the responses at the end of a batch. The following questions naturally arise: Given a batch budget M and a total number of T rounds, how should the statistician determine the size of each batch, and how should she update the policy after each batch? Can the statistician design batch learning algorithms that achieve regret performances on par with the fully online setting using as few policy updates as possible? 1.1 Main contributions In this work, we address the aforementioned questions under a classical framework for personalized decision making—nonparametric contextual bandits [48, 39]. In this framework, the expected reward associated with each treatment (or arm in the language of bandits) is modeled as a nonparametric smooth function of the covariates [59]. In the fully online setup, seminal works [48, 39] establish the minimax optimal regret bounds for the nonparametric contextual bandits. Nevertheless, under the more challenging setting with the batch constraint, the fundamental limits for nonparametric bandits remain unknown. Our paper aims to bridge this gap. More concretely, we make the following three novel contributions: • First, we establish a minimax regret lower bound for the nonparametric bandits with the batch con- straint M. Our proof relies on a simple but useful insight that the worst-case regret over the entire horizon is greater than the worst-case regret over the first i batches for all 1 ≤ i ≤ M. To fully exploit this insight, for each different batch number i, we construct different families of hard instances to target this batch, leading to a maximal regret over this batch. • In addition, we demonstrate that the aforementioned lower bound is tight by providing a matching upper bound (up to log factors). Specifically, we design a novel algorithm—Batched Successive Elimi- nation with Dynamic Binning (BaSEDB)—for the nonparametric bandits with batch constraints. BaSEDB progressively splits the covariate space into smaller bins whose widths are carefully selected to align well with the corresponding batch size. The delicate interplay between the batch size and the bin width is crucial for obtaining the optimal regret in the batch setting. • On the other hand, we show the suboptimality of static binning under the batch constraint by proving an algorithm-specific lower bound. Unlike the fully online setting where policies that use a fixed number of bins can attain the optimal regret [39], our lower bound indicates that batched successive elimination with static binning is strictly suboptimal. This highlights the necessity of dynamic binning in some sense under the batch setting, which is uncommon in classical nonparametric estimation. It is also worth mentioning that an immediate consequence of our results is that M ≳ log log T number of batches suffices to achieve the optimal regret in the fully online setting. In other words, we can use a nearly constant number of policy updates in practice to achieve the optimal regret obtained by policies that require one update per round. 1.2 Related work Nonparametric contextual bandits. [58] introduced the mathematical framework of contextual bandit. The theory of contextual bandits in the fully online setting has been continuously developed in the past few decades. On one hand, [4, 1, 23, 6, 7, 41] obtained learning guarantees for linear contextual bandits in both low and high dimensional settings. On the other hand, [59] introduced the nonparametric approach to model the mean reward function. [48] proved a minimax lower bound on the regret of nonparametric bandit and developed an upper-confidence-bound (UCB) based policy to achieve a near-optimal rate. [39] improved this result and proposed the Adaptively Binned Successive Elimination (ABSE) policy that can also adapt to the unknown margin parameter. Further insights in this nonparametric setting were developed in subsequent works [42, 43, 45, 24, 27, 52, 25, 10, 51, 9]. The smoothness assumption is also adopted in another line of work [37, 36, 33, 11] on the continuum-armed bandit problems. However in contrast to what we study, the reward is assumed to be a Lipschitz function of the action, and the covariates are not taken into considerations. 2 Batch learning. The batch constraint has received increasing attention in recent years. [40, 21] considered the multi-armed bandit problem under the batch setting and showed that O(log log T) batches are adequate in achieving the rate-optimal regret, compared to the fully online setting. [26, 47] extended batch learning to the (generalized) linear contextual bandits and [46, 56, 17] further studied the setting with high-dimensional covariates. [29, 28] established batch learning guarantees for the Thompson sampling algorithm. [18] consid- ered Lipschitz continuum-armed bandit problem with the batch constraint. Inference for batched bandits was considered in [60]. A concept related to batch learning in literature is called delayed feedback [14, 13, 55, 19]. These works consider the setting where rewards are observed with delay and analyze effects of delay on the regret. [32, 2] studied delayed feedback in nonparametric bandits and the key difference to batch learning is that the batch size is given, whereas in our case, it is a design choice by the statistician. Batch learning’s focus is different to that of delayed feedback in the sense that the former gives the decision maker discretion to choose the batch size which makes it possible to approximate the optimal standard online regret with a small number of batches. Finally, the notion switching cost is intimately related to the batch constraint. [12] studied online learning with low switching cost and obtained minimax optimal regret with O(log log T) batches. [5, 61, 20, 57, 44] developed regret guarantees with low switching cost for reinforcement learning. Low switching cost can be interpreted as infrequent policy updates, but it does not require the learner to divide the samples into batches with feedback only becoming available at the end of a batch. 2 Problem setup We begin by introducing the problem setup for nonparametric bandits with the batch constraint. A two-arm nonparametric bandit with horizon T ≥ 1 is specified by a sequence of independent and identically distributed random vectors (Xt, Y (1) t , Y (−1) t ), for t = 1, 2, . . . , T, (1) where Xt is sampled from a distribution PX. Throughout the paper, we assume that Xt ∈ X := [0, 1]d, and PX has a density (w.r.t. the Lebesgue measure) that is bounded below and above by some constants c, ¯c > 0, respectively. For k ∈ {1, −1} and t ≥ 1, we assume that Y (k) t ∈ [0, 1] and that E[Y (k) t | Xt] = f (k)(Xt). Here f (k) is the unknown mean reward function for the arm k. Without the batch constraint, the game of nonparametric bandits plays sequentially. At each step t, the statistician observes the context Xt, and pulls an action At ∈ {1, −1} according to a rule πt : X 7→ {1, −1}. Then she receives the corresponding reward Y (At) t . In this case, the rule πt for selecting the action at time t is allowed to depend on all the observations strictly anterior to t. In an M-batch game, the statistician is asked to divide the horizon [1 : T] into M disjoint batches [1 : t1], [t1 + 1 : t2], . . . , [tM−1 + 1, T]. In contrast to the case without the batch constraint, only the rewards associated with timesteps prior to the current batch are observed and available for making decisions for the current batch. More formally, an M-batch policy is composed of a pair (Γ, π), where Γ = {t0,t1,...,tM} is a partition of the entire time horizon T that satisfies 0 = t0 < t1 < ... < tM−1 < tM = T, and π = {πt}T t=1 is a sequence of random functions πt : X 7→ {1, −1}. Let Γ(t) be the batch index for the time t, i.e., Γ(t) is the unique integer such that tΓ(t)−1 < t ≤ tΓ(t). Then at time t, the available information for πt is only {Xl}t l=1 ∪ {Y (Al) l }Γ(t)−1 l=1 , which we denote by Ft. The statistician’s policy πt at time t is allowed to depend on Ft. The goal of the statistician is to design an M-batch policy (Γ, π) that can compete with an oracle that has perfect knowledge (i.e., the law of (Xt, Y (1) t , Y (−1) t )) of the environment. Formally, we define the cumulative regret as RT (π) := E \" T X t=1 \u0010 f ⋆(Xt) − f (πt(Xt))(Xt) \u0011# , (2) where f ⋆(x) := maxk∈{1,−1} f (k)(x) is the maximum mean reward one could obtain on the context x. Note here we omit the dependence on Γ for simplicity. 3 2.1 Assumptions We adopt two standard assumptions in the nonparametric bandits literature [48, 39]. The first assumption is on the smoothness of the mean reward functions. Assumption 1 (Smoothness). We assume that the reward function for each arm is (β, L)-smooth, that is, there exist β ∈ (0, 1] and L > 0 such that for k ∈ {1, −1}, |f (k)(x) − f (k)(x′)| ≤ L∥x − x′∥β 2 holds for all x, x′ ∈ X. The second assumption is about the separation between the two reward functions. Assumption 2 (Margin). We assume that the reward functions satisfy the margin condition with parameter α > 0, that is there exist δ0 ∈ (0, 1) and D0 > 0 such that PX \u0010 0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ δ \u0011 ≤ D0δα holds for all δ ∈ [0, δ0]. Assumption 2 is related to the margin condition in classification [38, 54, 3] and is introduced to bandits in [22, 48, 39]. The margin parameter affects the complexity of the problem. Intuitively, a small α, say α ≈ 0, means the two mean functions are entangled with each other in many regions and hence it is challenging to distinguish them; a large α, on the other hand, means the two reward functions are mostly well-separated. From now on, we use F(α, β) to denote the class of nonparametric bandit instances (i.e., distributions over (1)) that satisfy Assumptions 1-2. Remark 1. Throughout the paper, we assume that αβ ≤ 1. By proposition 2.1 from [48], when αβ > 1, one of the arms will dominate the other one for the entire covariate space. The instance is reduced to a multi-armed bandit without covariates which is not the interest of the current paper. Therefore, we focus on the case αβ ≤ 1 hereafter. 3 Fundamental limits of batched nonparametric bandits Somewhat unconventionally, we start with stating a minimax lower bound, as well as its proof, for regret minimization in batched nonparametric contextual bandits. As we will soon see, the proof of the lower bound is extremely instrumental in our development of an optimal M-batch policy (Γ, π), to be detailed in Section 4. Recall that F(α, β) denotes the class of nonparametric bandit instances (i.e., distributions over (1)) that obey Assumptions 1-2. We have the following minimax lower bound for any M-batch policy, in which we define γ := β(1 + α) 2β + d ∈ (0, 1). Theorem 1. Suppose that αβ ≤ 1, and assume that PX is the uniform distribution on X = [0, 1]d. For any M-batch policy (Γ, π), there exists a nonparametric bandit instance in F(α, β) such that the regret of (Γ, π) on this instance is lower bounded by E[RT (π)] ≥ ˜DT 1−γ 1−γM , where ˜D > 0 is a constant independent of T and M. See Section 3.1 for the proof of this lower bound. As a sanity check, one sees that as M increases, the lower bound decreases. This is intuitive, as the policy is more powerful as M increases. As a result, the problem of batched nonparametric bandits becomes easier. 4 3.1 Proof of Theorem 1 Let (Γ, π) be the M-batch policy under consideration, with Γ = {t0 = 0, t1, t2, . . . , tM = T}. Throughout this proof, we consider Bernoulli reward distributions, that is Y (1) t , Y (−1) t are Bernoulli random variables with mean f (1)(Xt), and f (−1)(Xt), respectively. In addition, we fix f (−1)(x) = 1 2. Let f be the mean reward function of the first arm. To make the dependence on the reward instance clear, we write the cumulative regret up to time n as Rn(π; f). Our proof relies on a simple observation: the worst-case regret over [T] is larger than the worst-case regret over the first i batches. Formally, we have sup (f, 1 2 )∈F(α,β) RT (π; f) ≥ max 1≤i≤M sup (f, 1 2 )∈F(α,β) Rti(π; f). (3) Though simple, this observation lends us freedom on choosing different families of instances in F(α, β) targeting different batch indices i. Our proof consists of four steps. In Step 1, we reduce bounding the regret of a policy to lower bounding its inferior sampling rate to be defined. In Step 2, we detail the choice of different families of instances for each different batch index i. Then in Step 3, we apply an Assouad-type of argument to lower bound the average inferior sampling rate of the family of hard instances. Lastly in Step 4, we combine the arguments to complete the proof. Step 1: Relating regret to inferior sampling rate. Given an M-batch policy, we define its inferior sampling rate at time n on an instance (f, 1 2) to be Sn(π; f) := E \" n X t=1 1{πt(Xt) ̸= π⋆(Xt), f(Xt) ̸= 1 2} # . In words, Sn(π; f) counts the number of times π selects the strictly suboptimal arm up to time n. Thanks to the following lemma, we can reduce lower bounding the regret to the inferior sampling rate. Lemma 1 (Lemma 3.1 in [48]). Suppose that (f, 1 2) ∈ F(α, β). Then for any 1 ≤ n ≤ T, we have Sn(π; f) ≤ Dn 1 1+α Rn(π; f) α 1+α , for some constant D > 0. As an immediate consequence of the above lemma, we obtain sup (f, 1 2 )∈F(α,β) RT (π; f) ≥ max 1≤i≤M sup (f, 1 2 )∈F(α,β) ( 1 D) 1+α α t − 1 i α (Sti(π; f)) 1+α α = ( 1 D) 1+α α max 1≤i≤M t − 1 α i \" sup (f, 1 2 )∈F(α,β) Sti(π; f) # 1+α α . From now on, we focus on lower bounding sup(f, 1 2 )∈F(α,β) Sti(π; f). Step 2: Introducing the family of reward instances for ti. Our construction of the family of hard instances is adapted from [48]. Define z1 = 1, and zi = ⌈(ti−11/(2β+d)⌉ for i = 2, 3, . . . , M. Henceforth, we will fix some i and write zi as z. We partition [0, 1]d into zd bins with equal width. Denote the bins by Cj for j = 1, ..., zd, and let qj be the center of Cj. Define a set of binary sequences Ωs := {±1}s, with s := ⌈zd−αβ⌉. For each ω ∈ Ωs we define a function fω : [0, 1]d 7→ R: fω(x) = 1 2 + s X j=1 ωjφj(x), 5 where φj(x) = Dϕz−βϕ(2z(x−qj))1{x ∈ Cj} with ϕ(x) = (1−∥x∥∞)β1{∥x∥∞ ≤ 1}, and Dϕ = min(2−βL, 1/4). In all, we consider the family of reward instances Cz := \u001a f (1)(x) = fω(x), f (−1)(x) = 1 2 | ω ∈ Ωs \u001b . With slight abuse of notation, we also use Cz to denote {fω : ω ∈ Ωs}. It is straightforward to check that Cz ⊆ F(α, β). Step 3: Lower bounding the inferior sampling rate. Fix some i ∈ [M], and consider z = zi. Since Cz ⊆ F(α, β), we have sup (f, 1 2 )∈F(α,β) Sti(π; f) ≥ sup f∈Cz Sti(π; f). Using the definitions of Cz and Sti(π; f), we have sup f∈Cz Sti(π; f) = sup ω∈Ωs Eπ,fω \" ti X t=1 1{πt(Xt) ̸= sign(fω(Xt) − 1 2), fω(Xt) ̸= 1 2} # ≥ 1 2s X ω∈Ωs Eπ,fω \" ti X t=1 1{πt(Xt) ̸= sign(fω(Xt) − 1 2), fω(Xt) ̸= 1 2} # . Since fω(x) = 1 2 for x /∈ ∪j=1,...sCj, we further obtain sup f∈Cz Sti(π; f) ≥ 1 2s X ω∈Ωs ti X t=1 s X j=1 Et π,fω [1{πt(Xt) ̸= ωj, Xt ∈ Cj}] . (4) Here we use Pt π,fω to denote the joint distribution of {Xl}t l=1 ∪ {Y πl(Xl) l }Γ(t)−1 l=1 , where Γ(t) is the batch index for t, i.e., the unique integer such that tΓ(t)−1 < t ≤ tΓ(t). We use Et π,fω to denote the corresponding expectation. Expand the right hand side of (4) to see that sup f∈Cz Sti(π; f) ≥ 1 2s s X j=1 ti X t=1 X ω[−j]∈Ωs−1 X h∈{±1} Et π,fωh [−j] [1{πt(Xt) ̸= h, Xt ∈ Cj}] | {z } Wj,t,ω[−j] , (5) where ωh [−j] is the same as ω except for the j-th entry being h. Note that here we use the fact that for fωh [−j], the optimal arm in the bin Cj is h. We then relate Wj,t,ω[−j] to a binary testing error, Wj,t,ω[−j] = 1 zd X h∈{±1} Pt π,fωh [−j] (πt(Xt) ̸= h | Xt ∈ Cj) ≥ 1 4zd exp \u0014 −KL(Pt π,fω−1 [−j] , Pt π,fω1 [−j] ) \u0015 , (6) where the second step invokes Le Cam’s method. Under the batch setting, at time t, the available information is only up to tΓ(t)−1. Consequently, the KL divergence KL(Pt π,fω−1 [−j] , Pt π,fω1 [−j] ) can be controlled by KL(Pt−1 π,fω−1 [−j] , Pt−1 π,fω1 [−j] ) (i) ≤ 8Eπ,fω−1 [−j] [ tΓ(t)−1 X t=1 (fω−1 [−j](Xt) − fω1 [−j](Xt))21{πt(Xt) = 1}] (ii) ≤ 32D2 ϕz−2βEπ,fω−1 [−j] [ tΓ(t)−1 X t=1 1{πt(Xt) = 1, Xt ∈ Cj}] 6 (iii) = 32D2 ϕz−(2β+d) tΓ(t)−1 X t=1 Pt π,fω−1 [−j] (πt(Xt) = 1 | Xt ∈ Cj) (iv) ≤ 32D2 ϕz−(2β+d)tΓ(t)−1. (7) Here, step (i) uses the standard decomposition of KL divergence and Bernoulli reward structure; step (ii) is due to the definition of fω; step (iii) uses P(Xt ∈ Cj) = 1/zd, and step (iv) arises from Pt π,fω−1 [−j] (πt(Xt) = 1 | Xt ∈ Cj) ≤ 1 for any 1 ≤ t ≤ T. Combining (5), (6), and (7), we arrive at sup f∈Cz Sti(π; f) ≥ 1 8 s X j=1 ti X t=1 1 zd exp \u0010 −32D2 ϕz−(2β+d)tΓ(t)−1 \u0011 ≥ 1 8 zd−αβ X j=1 i X l=1 tl − tl−1 zd exp \u0010 −32D2 ϕz−(2β+d)tl−1 \u0011 ≥ 1 8 zd−αβ X j=1 i X l=1 tl − tl−1 zd exp \u0010 −32D2 ϕz−(2β+d)ti−1 \u0011 , where the second line uses the fact that s = ⌈zd−αβ⌉, and the last inequality holds since tl−1 ≤ ti−1 for all 1 ≤ l ≤ i. Now recall that z = zi = ⌈(ti−1)1/(2β+d)⌉ for i ≥ 1, and z = 1 for i = 1. We can continue the lower bound to see that sup f∈Czi Sti(π; f) ≥ 1 8 zd−αβ X j=1 i X l=1 tl − tl−1 zd exp \u0010 −32D2 ϕz−(2β+d)ti−1 \u0011 ≥ c⋆ zd−αβ X j=1 i X l=1 tl − tl−1 zd = c⋆ · ti zαβ =    c⋆ · ti t αβ 2β+d i−1 , i > 1 c⋆t1, i = 1 , for some c⋆ > 0. Step 4: Combining bounds together. Combining the previous arguments together leads to the con- clusion that sup (f, 1 2 )∈F(α,β) RT (π; f) ≥ max 1≤i≤M sup f∈Czi Rti(π; f) ≥ ( 1 D) 1+α α max 1≤i≤M t − 1 α i \" sup f∈Czi Sti(π; f) # 1+α α ≳ max \u001a t1, t2 tγ 1 , ..., T tγ M−1 \u001b (8) ≥ ˜DT 1−γ 1−γM . This finishes the proof. 3.2 Implications on design of the optimal M-batch policy As we have mentioned, the proof of the lower bound, i.e., Theorem 1 facilitates the design of optimal M-batch policy. 7 Algorithm 1 Batched successive elimination with dynamic binning (BaSEDB) Input: Batch size M, grid Γ = {ti}M i=0, split factors {gi}M−1 i=0 . L ← B1 for C ∈ L do IC = I for i = 1, ..., M − 1 do for t = ti−1 + 1, ..., ti do C ← L(Xt) Pull an arm from IC in a round-robin way. if t = ti then Update L and {IC}C∈L by Algorithm 2 (L, {IC}C∈L, i, gi). for t = tM−1 + 1, ..., T do C ← L(Xt) Pull any arm from IC. Grid selection. First, the lower bound of the whole horizon is reduced to the worst-case regret over a specific batch; see (3). Consequently, we need to design the grid Γ = (t0, t1, t2, . . . , tM−1, tM) such that the total regret is evenly distributed across batches. More concretely, in view of the lower bound (8), one needs to set t1 ≍ ti tγ i−1 ≍ T 1−γ 1−γM for 2 ≤ i ≤ M. Dynamic binning. In addition, in the proof of the lower bound, for each different batch i, we use different families of hard reward instances, parameterized by the number of bins zi = ⌈t1/(2β+d) i−1 ⌉. In other words, from the lower bound perspective, the granularity (i.e., the bin width 1/zi) at which we investigate the mean reward function depends crucially on the grid points {ti}: the larger the grid point ti, the finer the granularity. This key observation motivates us to consider the batched successive elimination with dynamic binning algorithm to be introduced below. 4 Batched successive elimination with dynamic binning In this section, we present the batched successive elimination with dynamic binning policy (BaSEDB) that nearly attains the minimax lower bound, up to log factors; see Algorithm 1. On a high level, Algorithm 1 gradually partitions the covariate space X into smaller hypercubes (i.e., bins) throughout the batches based on a list of carefully chosen cube widths, and reduces the nonparametric bandit in each cube to a bandit problem without covariates. A tree-based interpretation. The process is best illustrated with the notion of a tree T of depth M; see Figure 1. Each layer of of the tree T is a set of bins that form a regular partition of X using hypercubes with equal widths. And the common width of the bins Bi in layer i is dictated by a list {gi}M−1 i=0 of split factors. More precisely, we let wi := ( i−1 Y l=0 gl)−1 (9) be the width of the cubes in the i-th layer Bi for i ≥ 1, and w0 = 1. In other words, Bi contains all the cubes Ci,v = {x ∈ X : (vj − 1)wi ≤ xj < vjwi, 1 ≤ j ≤ d}, where v = (v1, v2, . . . , vd) ∈ [ 1 wi ]d. As a result, there are in total ( 1 wi )d bins in Bi. 8 Algorithm 2 Tree growing subroutine Input: Active nodes L, active arm sets {IC}C∈L, batch number i, split factor gi. L′ ← {} for C ∈ L do if |IC| = 1 then L′ ← L′ ∪ {C} Proceed to next C in the iteration. ¯Y max C,i ← maxk∈IC ¯Y (k) C,i for k ∈ IC do if ¯Y max C,i − ¯Y (k) C,i > U(mC,i, T, C) then IC ← IC − {k} if |IC| > 1 then IC′ ← IC for C′ ∈ child(C, gi) L′ ← L′ ∪ child(C, gi) else L′ ← L′ ∪ {C} Return L′ Algorithm 1 proceeds in batches and maintains two key objects: (1) a list L of active bins, and (2) the corresponding active arms IC for each C ∈ L; see Figure 1 for an example. Specifically, prior to the game (i.e., prior to the first batch), L is set to be B1, all bins in layer 1, and IC = {1, −1} for all C ∈ L. Within this batch, the statistician tries the arms in IC equally likely for all bins in L. Then at the end of the batch, given the revealed rewards in this batch, we update the active arms IC for each C ∈ L via successive elimination. If no arm were eliminated from IC, this suggests that the current bin is not fine enough for the statistician to tell the difference between the two arms. As a result, she splits the bin C ∈ L into its children child(C) in T . All the child nodes will be included in L, while the parent C stops being active (i.e., C is removed from L). The whole process repeats in a batch fashion. 1 When to eliminate arms? Now we zoom in on the elimination process described in Algorithm 2. The basic idea follows from successive elimination in the bandit literature [16, 39, 21]: the statistician eliminates an arm from IC if she expects the arm to be suboptimal in the bin C given the rewards collected in C. Specifically, for any node C ∈ T , define U(τ, T, C) := 4 r log(2T|C|d) τ , where |C| denotes the width of the bin. Let mC,i := Pti t=ti−1+1 1{Xt ∈ C} be the number of times we observe contexts from C in batch i. We then define for k ∈ {1, −1} that ¯Y (k) C,i := Pti t=ti−1+1 Yt · 1{Xt ∈ C, At = k} Pti t=ti−1+1 1{Xt ∈ C, At = k} , which is the empirical mean reward of arm k in node C during the i-th batch. It is easy to check that ¯Y (k) C,i has expectation ¯f (k) C given by ¯f (k) C := E[f (k)(X) | X ∈ C] = 1 PX(C) Z C f (k)(x)dPX(x). 1For the final batch M, the split factor gM−1 = 1 by default because there is no need to further partition the nodes for estimation. 9 [0, 1] [0, 1 4) [0, 1 12) [ 1 12, 1 6) [ 1 6, 1 4) [ 1 4, 1 2) [ 1 2, 3 4) [ 3 4, 1] [ 3 4, 5 6) [ 5 6, 11 12) [ 11 12, 1] Figure 1: An example of the tree growing process for d = 1, M = 3, G = {4, 3, 1}. The root node is at depth 0. For the first batch, the 4 nodes located at depth 1 of the tree were used. Both [ 1 4, 1 2) and [ 1 2, 3 4) only had one active arm remaining so they were not further split and remained in the set of active nodes (green). Meanwhile, |I[0, 1 4 )| = |I[ 3 4 ,1]| = 2 so each of them was split into 3 smaller nodes, and both nodes were marked as inactive (red). For the second batch, all the green nodes were actively used but arm elimination was performed at the end of batch 2 only for nodes located at depth 2 (the green nodes at depth 1 already have 1 active arm remaining so there is no need to eliminate again). Similarly, we define the average optimal reward in bin C to be ¯f ⋆ C := 1 PX(C) Z C f ⋆(x)dPX(x). The elimination threshold U(mC,i, T, C) is chosen such that an arm k with ¯f ⋆ C − ¯f (k) C ≫ |C|β is eliminated with high probability at the end of batch i. Therefore, when |IC| > 1, the remaining arms are statistically indistinguishable from each other, so C is split into smaller nodes to estimate those arms more accurately using samples from future batches. On the other hand, when |IC| = 1, the remaining arm is optimal in C with high probability—a consequence of the smoothness condition, and it will be exploited in the later batches. Grid Γ and split factors {gi}M−1 i=0 . As one can see, the split factor gi controls how many children a node at layer i can have and its appropriate choice is crucial for obtaining small regret. Intuitively, gi should be selected in a way such that a node Ci+1 with width wi can fully leverage the number of samples allocated to it during the (i + 1)-th batch. With these goals in mind, we design the grid Γ = {ti} and split factors {gi} as follows. Recall that γ = β(1+α) 2β+d . We set b = Θ \u0010 T 1−γ 1−γM \u0011 . The split factors are chosen according to g0 = ⌊b 1 2β+d ⌋, and gi = ⌊gγ i−1⌋, i = 1, ..., M − 2. (10) In addition, the grid is chosen such that ti − ti−1 = ⌊liw−(2β+d) i log(Twd i )⌋, 1 ≤ i ≤ M − 1, (11) 10 where li > 0 is a constant to be specified later. It is easy to check that with these choices, we have t1 ≍ T 1−γ 1−γM , and ti = ⌊b(ti−1)γ⌋, for i = 2, ..., M. In particular, we set b properly to make tM = T. Indeed, these choices taken together meet the expectation laid out in Section 3.2: we need to choose the grid and the split factors appropriately so that (1) the total regret spreads out across different batches, and (2) the granularity becomes finer as we move further to later batches. Connections and differences with ABSE in [39]. In appearance, BaSEDB (Algorithm 1) looks quite similar to the Adaptively Binned Successive Elimination (ABSE) proposed in [39]. However, we would like to emphasize several fundamental differences. First, the motivations for the algorithms are completely different. [39] designs ABSE to adapt to the unknown margin condition α, while our focus is to tackle the batch constraint. In fact, without the batch constraints, if α is known, adaptive binning is not needed to achieve the optimal regret [39]. This is certainly not the case in the batched setting. Fixing the number of bins used across different batches is suboptimal because one can construct instances that cause the regret incurred during a certain batch to explode. We will expand on this phenomenon in Section 4.3. Secondly, the algorithm in [39] partitions a bin into a fixed number 2d of smaller ones once the original bin is unable to distinguish the remaining arms. In this way, the algorithm can adapt to the difference in the local difficulty of the problem. In comparison, one of our main contributions is to carefully design the list of varying split factors that allows the new cubes to maximally utilize the number of samples allocated to it during the next batch. 4.1 Regret guarantees Now we are ready to present the regret performance of BaSEDB (Algorithm 1). Theorem 2. Suppose that αβ ≤ 1. Fix any constant D1 > 0 and suppose that M ≤ D1 log T. Equipped with the grid and split factors list that satisfy (11) and (10), the policy ˆπ given by Algorithm 1 obeys E[RT (ˆπ)] ≤ ˜C(log T)2 · T 1−γ 1−γM , where ˜C > 0 is a constant independent of T and M. See Section 5 for the proof. While Theorem 2 requires M ≲ log T, we see from the corollary below that it is in fact sufficient to show the optimality of Algorithm 1. Corollary 1. As long as M ≥ D2 log log(T), where D2 depends on γ = β(1+α) 2β+d , Algorithm 1 achieves E[RT (ˆπ)] ≤ ˜C(log T)2 · T 1−γ, where ˜C > 0 is a constant independent of T and M. Theorem 2, together with Corollary 1 and Theorem 1 establish the fundamental limits of batch learning for the nonparametric bandits with covariates, as well as the optimality of BaSEDB, up to logarithmic factors. To see this, when M ≲ log log(T), the upper bound in Theorem 2 matches the lower bound in Theorem 1, apart from log factors. On the other end, when M ≳ log log(T), Algorithm 1, while splitting the horizon into M batches, achieves the optimal regret (up to log factors) for the setting without the batch constraint [39]. It is evident that Algorithm 1 is optimal in this case. 4.2 Numerical experiments In this section, we provide some experiments on the empirical performance of Algorithm 1. We set T = 50000, d = β = 1, α = 0.2. We let PX be the uniform distribution on [0, 1]. Denote qj = (j − 1/2)/4 and 11 Figure 2: Regret vs. batch budget M. Cj = [qj − 1/8, qj + 1/8] for 1 ≤ j ≤ 4. For the mean reward functions, we choose f (1), f (−1) : [0, 1] → R such that f (1)(x) = 1 2 + 4 X j=1 ωjφj(x), f (−1)(x) = 1 2, where ω′ js are sampled i.i.d. from Rad( 1 2), φj(x) = 1 4ϕ(8(x − qj))1{x ∈ Cj} and ϕ(x) = (1 − |x|)1{|x| ≤ 1}. We let Y (k) ∼ Bernoulli(f (k)(x)). To illustrate the performance of Algorithm 1, we compare it with the Binned Successive Elimination (BSE) policy from [39], which is shown to be minimax optimal in the fully online case. Figure 2 shows the regret of Algorithm 1 under different batch budegts. One can see that it is sufficient to have M = 5 batches to achieve the fully online efficiency. 4.3 Failure of static binning We have seen the power of dynamic binning in solving batched nonparametric bandits by establishing its rate- optimality in minimizing regret. Now we turn to a complimentary but intriguing question: is it necessary to use dynamic binning to achieve optimal regret under the batch constraint? To formally address this question, we investigate the performance of successive elimination with static binning, i.e., Algorithm 1 with g0 = g, and g1 = g2 = · · · gM−2 = 1. Although static binning works when M is large (e.g., a single choice of g attains the optimal regret [48, 39] in the fully online setting), we show that it must fail when M is small. To bring the failure mode of static binning into focus, we consider the simplest scenario when M = 3, and α = β = d = 1. Note that the successive elimination with static binning algorithm is parameterized by the grid choice Γ = {t0 = 0, t1, t2, t3 = T} and the fixed number g of bins. The following theorem formalizes the failure of static binning in achieving optimal regret when M = 3. Theorem 3. Consider M = 3, and α = β = d = 1. For any choice of 1 ≤ t1 < t2 ≤ T − 1, and any choice of g, there exists a nonparametric bandit instance in F(1, 1) such that the resulting successive elimination with static binning algorithm ˆπstatic satisfies E[RT (ˆπstatic)] ≥ ˜C1T 9 19 +κ, for some κ, ˜C1 > 0 that are independent of T. Here T 9 19 is the optimal regret achieved by BaSEDB—an successive elimination algorithm with dynamic binning. While the formal proof is deferred to Section 6, we would like to immediately point out the intuition under- lying the failure of static binning. Necessary choice of grid Γ. It is evident from the proof of the minimax lower bound (Theorem 1) that one needs to set t1 ≍ T 9/19, and t2 ≍ T 15/19. Otherwise, the inequality (8) guarantees the worst-case regret of ˆπstatic exceeds the optimal one T 9 19 . Consequently, we can focus on the algorithm with t1 ≍ T 9/19, t2 ≍ T 15/19, and only consider the design choice g. 12 x 1/g 1/z δ/2 Figure 3: Instance with g > z. Each bin B produced by ˆπstatic has width 1/g. x 1/z 1/g δ/2 Figure 4: Instance with g < z. Each bin B produced by ˆπstatic has width 1/g. Why fixed g fails. As a baseline for comparison, recall that in the optimal algorithm with dynamic binning, we set g0 ≍ T 3/19, and g0g1 ≍ T 5/19 so that the worst case regret in three batches are all on the order of T 9 19 . In view of this, we split the choice of g into three cases. • Suppose that g ≫ T 3/19. In this case, we can construct an instance such that the reward difference only appears on an interval with length 1/z ≫ 1/g; see Figure 3. In other words, the static binning is finer than that in the reward instance. As a result, the number of pulls in the smaller bin (used by the algorithm) in the first batch is not sufficient to tell the two arms apart, that is with constant probability, arm elimination will not happen after the first batch. This necessarily yields the blowup of the regret in the second batch. • Suppose that g ≪ T 3/19. In this case, we can construct an instance such that the reward difference only appears on an interval with length 1/z ≪ 1/g; see Figure 4. In other words, the static binning is coarser than that in the reward instance. Since the aggregated reward difference on the larger bin is so small, the number of pulls in the larger bin (used by the algorithm) in the first batch is still not sufficient to result in successful arm elimination. Again, the regret on the second batch blows up. • Suppose that g ≍ T 3/19. Since this choices matches g0 used in the optimal dynamic binning algorithm, there is no reward instance that can blow up the regret in the first two batches. Nevertheless, since g ≪ g0g1 ≍ T 5/19, one can construct the instance similar to the previous case (i.e., Figure 4) such that the regret on the third batch blows up. 5 Regret analysis for BaSEDB Our proof of the regret upper bound is inspired by the framework developed in [39]. Our setting presents additional technical difficulty due to the batch constraint. We begin with introducing some useful notations. Recall the tree growing process described in section 4, where we have defined a tree T of depth M. The root (depth 0) of the tree is the whole space X. In depth 1, X has gd 0 children, each of which is a bin of width 1/g0. For each bin in depth 1, it has gd 1 children, each of which is a bin of width 1/(g0g1). These children form the depth 2 nodes of the tree T . We form the tree recursively until depth M. 13 For a bin C ∈ T , we define its parent by p(C) = {C′ ∈ T : C ∈ child(C′)}. Moreover, we let p1(C) = p(C) and define pk(C) = p(pk−1(C)) for k ≥ 2 recursively. In all, we denote by P(C) = {C′ ∈ T : C′ = pk(C) for some k ≥ 1} all the ancestors of the bin C. We also define Lt to be the set of active bins at time t, with the dummy case L0 = {X}. Clearly, for 1 ≤ t ≤ t1, one has L1 = B1, where B1 are all the bins in the first layer. 5.1 Two clean events The regret analysis relies on two clean events. First, fix a batch i ≥ 1, and recall Lti−1+1 is the set of active bins at time ti−1 + 1. We denote the random number of pulls for a bin C ∈ Lti−1+1 within batch i to be mC,i := ti X t=ti−1+1 1{Xt ∈ C}. Clearly, it has expectation m⋆ C,i = E[mC,i] = (ti − ti−1)PX(X ∈ C). The first clean event claims that mC,i concentrates well around its expectation m⋆ C,i uniformly over all C ∈ T . We denote this event by E. Lemma 2. Suppose that M ≤ D1 log(T) for some constant D1 > 0. With probability at least 1 − 1/T, for all 1 ≤ i ≤ M, and C ∈ Lti−1+1, we have 1 2m⋆ C,i ≤ mC,i ≤ 3 2m⋆ C,i. See Section 5.5.1 for the proof. Since M ≤ D1 log(T) by assumption, we can apply Lemma 2 to obtain E[RT (ˆπ)1(Ec)] ≤ TP(Ec) = 1. Therefore, in the remaining proof, we condition on E and focus on bounding E[RT (ˆπ)1(E)]. The second clean event is on the elimination process. Since we use successive elimination in each bin, it is natural to expect that the optimal arm in each bin is not eliminated during the process. To mathematically specify this event, we need a few notations. For each bin C ∈ Li, let I′ C be the set of remaining arms at the end of batch i, i.e., after Algorithm 2 is invoked. Define ¯ IC = \u001a k ∈ {1, −1} : sup x∈C f ⋆(x) − f (k)(x) ≤ c1|C|β \u001b , IC = \u001a k ∈ {1, −1} : sup x∈C f ⋆(x) − f (k)(x) ≤ c0|C|β \u001b , where c0 = 2Ldβ/2 + 1 and c1 = 8c0. Clearly, we have IC ⊆ ¯ IC. Define a good event AC = {IC ⊆ I′ C ⊆ ¯ IC}, which is the event that the remaining arms in C have gaps of correct order. In addition, define GC = ∩C′∈P(C)AC′. Recall Bi is the set of bins C with |C| = (Qi−1 l=0 gl)−1 = wi for i ≥ 1. Lemma 3. For any 1 ≤ i ≤ M − 1 and C ∈ Bi, we have P(E ∩ GC ∩ Ac C) ≤ 4m⋆ C,i T|C|d . In words, Lemma 3 guarantees that AC happens with high probability if E holds and AC′ holds for all the ancestors C’ of C. See Section 5.5.2 for the proof. 14 5.2 Regret decomposition In this section, we decompose the regret into three terms. First, for a bin C, we define rlive T (C) := T X t=1 \u0010 f ⋆(Xt) − f (πt(Xt))(Xt) \u0011 1(Xt ∈ C)1(C ∈ Lt). In addition, define Jt := ∪s≤tLs to be the set of bins that have been live up until time t. Correspondingly we define rborn T (C) := T X t=1 \u0010 f ⋆(Xt) − f (πt(Xt))(Xt) \u0011 1(Xt ∈ C)1(C ∈ Jt). It is clear from the definition that for any C ∈ T , one has rborn T (C) = rlive T (C) + X C′∈child(C) rborn T (C′) = rborn T (C)1(Ac C) + rlive T (C)1(AC) + X C′∈child(C) rborn T (C′)1(AC). Applying this relation recursively leads to the following regret decomposition: RT (π) = rborn T (X) = rlive T (X) | {z } =0 + X C′∈child(X) rborn T (C′) = X 1≤i<M       X C∈Bi rborn T (C)1(GC ∩ Ac C) | {z } =:Ui + X C∈Bi rlive T (C)1(GC ∩ AC) | {z } =:Vi       + X C∈BM rlive T (C)1(GC), where the second equality arises from the fact that rlive T (X) = 0. Indeed, X /∈ Lt for any 1 ≤ t ≤ T. 5.3 Controlling three terms In what follows, we control Vi, Ui and the last batch separately. 5.3.1 Controlling Vi Fix some 1 ≤ i ≤ M − 1, and some bin C ∈ Bi. On the event GC we have I ′ p(C) ⊆ ¯Ip(C), that is, for any k ∈ I ′ p(C), sup x∈p(C) f ⋆(x) − f (k)(x) ≤ c1|p(C)|β. This implies that for any x ∈ C, and k ∈ I ′ p(C), \u0010 f ⋆(x) − f (k)(x) \u0011 1{GC} ≤ c1|p(C)|β1(0 < \f\f\ff (1)(x) − f (−1)(x) \f\f\f ≤ c1|p(C)|β). (12) As a result, we obtain E[rlive T (C)1(GC ∩ AC)] = E \" T X t=1 \u0010 f ⋆(Xt) − f (πt(Xt))(Xt) \u0011 1(Xt ∈ C)1(C ∈ Lt)1(GC ∩ AC) # 15 (i) ≤ E \" T X t=1 c1|p(C)|β1(0 < \f\f\ff (1)(Xt) − f (−1)(Xt) \f\f\f ≤ c1|p(C)|β)1(Xt ∈ C, C ∈ Lt)1(GC ∩ AC) # (ii) ≤ c1|p(C)|βE   ti X t=ti−1+1 1(0 < \f\f\ff (1)(Xt) − f (−1)(Xt) \f\f\f ≤ c1|p(C)|β, Xt ∈ C)1(GC ∩ AC)   (iii) ≤ c1|p(C)|β ti X t=ti−1+1 P(0 < \f\f\ff (1)(Xt) − f (−1)(Xt) \f\f\f ≤ c1|p(C)|β, Xt ∈ C) = c1|p(C)|β(ti − ti−1)P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C). Here, step (i) uses relation (12), and the fact that πt(Xt) ∈ I ′ p(C) when Xt ∈ C. For step (ii), if C is split, then it is no longer live, so the live regret incurred on the remaining batches is zero. On the other hand, if C is not split, then |I′ C| = 1. Without loss of generality, assume that arm −1 is eliminated. Conditioned on AC, this means −1 /∈ IC and there exists x0 ∈ C such that f (1)(x0) − f (−1)(x0) > c0|C|β. By the smoothness condition, having a gap at least c0|C|β on a single point in C implies f (1)(x) − f (−1)(x) > |C|β for all x ∈ C. Therefore, arm 1 which is the remaining one is the optimal arm for all x ∈ C and would not incur any regret further. The third inequality holds since 1(GC ∩ AC) ≤ 1. Taking the sum over all bins in Bi and using the fact that |p(C)| = wi−1, we obtain X C∈Bi E[rlive T (C)1(GC ∩ AC)] ≤ X C∈Bi c1wβ i−1(ti − ti−1)P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C) = c1wβ i−1(ti − ti−1) X C∈Bi P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1, X ∈ C). (13) Note that X C∈Bi P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1, X ∈ C) = P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1) ≤ D0 · h c1wβ i−1 iα , (14) where the last inequality follows from the margin condition. Combining relations (14) and (13), we reach X C∈Bi E[rlive T (C)1(GC ∩ AC)] ≤ (ti − ti−1) · [c1wβ i−1]1+α · D0. 5.3.2 Controlling Ui Fix some 1 ≤ i ≤ M − 1, and some bin C ∈ Bi. Again, using the definition of GC, we obtain E[rborn T (C)1(GC ∩ Ac C)] = E \" T X t=1 \u0010 f ⋆(Xt) − f (πt(Xt))(Xt) \u0011 1(Xt ∈ C)1(C ∈ Jt)1(GC ∩ Ac C) # ≤ E \" T X t=1 c1|p(C)|β1(0 < \f\f\ff (1)(Xt) − f (−1)(Xt) \f\f\f ≤ c1|p(C)|β)1(Xt ∈ C, C ∈ Jt)1(GC ∩ Ac C) # ≤ c1|p(C)|βTP(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C)P(GC ∩ Ac C). Apply Lemma 3 to see that E[rborn T (C)1(GC ∩ Ac C)] ≤ c1|p(C)|βTP(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C) 4m⋆ C,i T|C|d 16 = c1wβ i−1P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1, X ∈ C)4(ti − ti−1)PX(X ∈ C) |C|d ≤ 4¯cc1wβ i−1P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1, X ∈ C)(ti − ti−1), where we use the fact that PX(X ∈ C) ≤ ¯c|C|d in the second inequality. Summing over all bins in Bi, we obtain X C∈Bi E[rborn T (C)1(GC ∩ Ac C)] ≤ 4¯cc1wβ i−1(ti − ti−1) X C∈Bi P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1wβ i−1, X ∈ C) ≤ 4¯cc1wβ i−1(ti − ti−1)D0 · h c1wβ i−1 iα = 4D0¯c(ti − ti−1)[c1wβ i−1]1+α, where the second inequality reuses the bound in (14). 5.3.3 Last Batch For C ∈ BM, one can similarly obtain E[rlive T (C)1(GC)] ≤ c1|p(C)|β(T − tM−1)P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C). Consequently, summing over C ∈ BM yields X C∈BM E[rlive T (C)1(GC)] ≤ X C∈BM c1|p(C)|β(T − tM−1)P(0 < \f\f\ff (1)(X) − f (−1)(X) \f\f\f ≤ c1|p(C)|β, X ∈ C) ≤ c1wβ M−1(T − tM−1)D0 · h c1wβ M−1 iα = D0(T − tM−1)[c1wβ M−1]1+α. 5.4 Putting things together In sum, the total regret is bounded by E[RT (π)] ≤ c   t1 + M−1 X i=2 (ti − ti−1) · wβ+αβ i−1 + (T − tM−1)wβ+αβ M−1 ! , where c is a constant that depends on (α, β, D, L). Recall that wi = (Qi−1 l=0 gl)−1, and the choices for the batch size and the split factors (11)-(10). We then obtain t1 ≲ T 1−γ 1−γM log T, (ti − ti−1) · wβ+αβ i−1 ≲ T 1−γ 1−γM log T, for 2 ≤ i ≤ M − 1, (T − tM−1)wβ+αβ M−1 ≤ Twβ+αβ M−1 ≲ T 1−γ 1−γM log T. The proof is finished by combining the above three bounds. 5.5 Proofs for the clean events We are left with proving that the two clean events happen with high probability. 17 5.5.1 Proof of Lemma 2 Fix the batch index i, and a node C in layer-i of the tree T . By relation (11), we have m⋆ C,i = (ti − ti−1)PX(X ∈ C) ≍ |C|−(2β+d) log(T|C|d)PX(X ∈ C) ? |C|−2β ≥ g2β 0 ≍ (T 1−γ 1−γM · 2β 2β+d ), where the last step uses the fact that PX(X ∈ C) ≥ c|C|d. Therefore, m⋆ C,i ≥ 3 4 log(2T 2) for all i and C, as long as T is sufficiently large. This allows us to invoke Chernoff’s bound to obtain that with probability at most 1/T 2 \f\f\f\f Xti t=ti−1+1 1{Xt ∈ C} − m⋆ C,i \f\f\f\f ≥ q 3 log(2T 2)m⋆ C,i. Denote Ec = {∃1 ≤ i ≤ M, C ∈ Lti−1+1 such that | Pti t=ti−1+1 1{Xt ∈ C} − m⋆ C,i |≥ q 3 log(2T 2)m⋆ C,i}. Applying union bound to reach P(Ec) ≤ X C∈T 1 T 2 (i) ≤ 1 T 2  M X i=1 ( i−1 Y l=0 gl)d ! (ii) ≤ 1 T 2 · M · ( M−1 Y l=0 gl)d, where step (i) sums over all possible nodes of T across batches, and step (ii) is due to (Qi−1 l=0 gl)d ≤ (QM−1 l=0 gl)d for any 1 ≤ i ≤ M. Since gM−1 = 1, we further obtain P(Ec) ≤ 1 T 2 · M · ( M−2 Y l=0 gl)d (iii) ≤ 1 T 2 · M · t d 2β+d M−1 (iv) ≤ D1 1 T 2 · log T · T d 2β+d ≤ 1 T , where step (iii) invokes relation (11), and step (iv) uses the assumption M ≤ D1 log T. This completes the proof. 5.5.2 Proof of Lemma 3 To simplify notation, for any event F, we define PGC(F) = P(E ∩ GC ∩ F). Let D1 C be the event that an arm k ∈ IC is eliminated at the end of batch i, and D2 C be the event that an arm k /∈ ¯ IC is not eliminated at the end of batch i. Consequently, we have PGC(Ac C) = PGC(D1 C) + PGC((D1 C)c ∩ D2 C). Recall U(τ, T, C) = 4 q log(2T |C|d) τ . By relation (11), we can write m⋆ C,i = (ti − ti−1)PX(X ∈ C) = li|C|−(2β+d) log(T|C|d)PX(X ∈ C), where li > 0 is a constant chosen such that U(2m⋆ C,i, T, C) = 2c0|C|β. Under E, we have U(mC,i, T, C) ≤ 4c0|C|β because mC,i ≥ 1 2m⋆ C,i. 1. Upper bounding PGC(D1 C): when D1 C occurs, an arm k ∈ IC is eliminated by some k′ ∈ I′ p(C) at the end of batch i. This means ¯Y (k′) C,i − ¯Y (k) C,i > U(mC,i, T, C). Meanwhile, ¯f (k′) C − ¯f (k) C ≤ ¯f ⋆ C − ¯f (k) C (i) ≤ c0|C|β ≤ 1 2U(2m⋆ C,i, T, C), where step (i) uses the definition of IC. Consequently, | ¯Y (k′) C,i − ¯f (k′) C | ≤ U(mC,i, T, C)/4 and | ¯Y (k) C,i − ¯f (k) C | ≤ U(mC,i, T, C)/4 cannot hold simultaneously. Otherwise, this would contradict with ¯Y (k′) C,i − ¯Y (k) C,i > U(mC,i, T, C) because mC,i ≤ 2m⋆ C,i under E. Therefore, PGC(D1 C) ≤ P \u001a ∃k ∈ I′ p(C), mC,i ≤ 2m⋆ C,i : | ¯Y (k) C,i − ¯f (k) C | ≥ 1 4U(mC,i, T, C) \u001b . 18 2. Upper bounding PGC((D1 C)c ∩ D2 C): when (D1 C)c ∩ D2 C happens, no arm in IC is eliminated while some k /∈ ¯ IC remains in the active arm set. By definition, there exists x(k) such that f ⋆(x(k)) − f (k)(x(k)) > 8c0|C|β. Let η(k) be any arm that satisfies f ⋆(x(k)) = f (η(k))(x(k)), and one can easily verify η(k) ∈ IC. Since k is not eliminated, we have ¯Y (η(k)) C,i − ¯Y (k) C,i ≤ U(mC,i, T, C). On the other hand, ¯f (η(k)) C (iii) ≥ f (η(k))(x(k)) − c0|C|β ≥ f (k)(x(k)) + 8c0|C|β − c0|C|β = f (k)(x(k)) + 7c0|C|β (iv) ≥ ¯f (k) C + 6c0|C|β ≥ ¯f (k) C + 3 2U(mC,i, T, C), (15) where steps (iii) and (iv) use Lemma 5. Inequality (15) together with the fact that ¯Y (η(k)) C,i − ¯Y (k) C,i ≤ U(mC,i, T, C) imply | ¯Y (k0) C,i − ¯f (k0) C | ≥ U(mC,i, T, C)/4 for either k0 = k or k0 = η(k). Consequently, PGC((D1 C)c ∩ D2 C) ≤ P \u001a ∃k ∈ I′ p(C), mC,i ≤ 2m⋆ C,i : | ¯Y (k) C,i − ¯f (k) C | ≥ 1 4U(mC,i, T, C) \u001b . Combining the two parts we obtain PGC(Ac C) = PGC(D1 C) + PGC((D1 C)c ∩ D2 C) ≤ 2 · P \u001a ∃k ∈ I′ p(C), mC,i ≤ 2m⋆ C,i : | ¯Y (k) C,i − ¯f (k) C | ≥ 1 4U(mC,i, T, C) \u001b ≤ 4m⋆ C,i T|C|d , where the last inequality applies Lemma 4. 5.5.3 Auxiliary lemmas Lemma 4. For any 1 ≤ i ≤ M − 1 and C ∈ Bi, one has P \u001a ∃k ∈ I′ p(C), mC,i ≤ 2m⋆ C,i : | ¯Y (k) C,i − ¯f (k) C | ≥ 1 4U(mC,i, T, C) \u001b ≤ 2m⋆ C,i T|C|d . Proof. Recall in Algorithm 1 we pull each arm in a round-robin fashion within a bin during batch i. Fix τ > 0. Let ¯Y (k) τ = Pτ j=1 Y (k) j /τ where Y (k) j ’s are i.i.d. random variables with Y (k) j ∈ [0, 1] and E[Y (k) j ] = ¯f (k) C . By Hoeffding’s inequality, with probability 1/(T|C|d), we have | ¯Y (k) τ − ¯f (k) C | ≥ r log(2T|C|d) 2τ . Applying union bound to get P ( ∃k ∈ Ip(C), 0 ≤ τ ≤ m⋆ C,i : | ¯Y (k) τ − ¯f (k) C | ≥ r log(2T|C|d) 2τ ) ≤ 2m⋆ C,i T|C|d , which completes the proof. Lemma 5. Fix k ∈ {1, −1} and C ∈ T , for any x ∈ C, one has | ¯f (k) C − f (k)(x)| ≤ c0|C|β, where c0 = 2Ldβ/2 + 1. 19 Proof. For notation simplicity, we write f for f (k) in the following proof. By definition, | ¯fC − f(x)| = | 1 P(C) Z C (f(y) − f(x))dP(y)| ≤ 1 P(C) Z C |f(y) − f(x)|dP(y) ≤ 1 P(C) Z C L∥x − y∥β 2dP(y), where the first inequality uses the triangle inequality, and the second inequality is due to the smoothness condition. Since x ∈ C, we further have | ¯fC − f(x)| ≤ 1 P(C) Z C L∥x − y∥β 2dP(y) ≤ 1 P(C) Z C Ldβ/2|C|βdP(y) ≤ c0|C|β. This completes the proof. 6 Proof of suboptimality of static binning As we argued after the statement of Theorem 3, one needs to set t1 ≍ T 9/19, and t2 ≍ T 15/19. Therefore, throughout the proof, we assume this is true and only focus on the number g of bins. To construct a hard instance, we partition [0, 1] into z bins with equal width. Denote the bins by Cj for j = 1, ..., z, and let qj be the center of Cj. Define a function ϕ : [0, 1] 7→ R as ϕ(x) = (1 − |x|)1{|x| ≤ 1}. Correspondingly define a function φj : [0, 1] 7→ R as φj(x) = Dϕz−1ϕ(2z(x − qj))1{x ∈ Cj}, where Dϕ = min(2−1L, 1/4). Define a function f : [0, 1] 7→ R: f(x) = 1 2 + φ1(x). The problem instance of interest is v = (f (1)(x) = f(x), f (−1)(x) = 1 2). It is easy to verify v ∈ F(1, 1). Throughout the proof, we condition on the event E specified by Lemma 2, which says the number of samples allocated to a bin concentrates well around its expectation. We will show even under this good event, there exists a choice of z that makes successive elimination fail to remove the suboptimal arms at the end of a batch with constant probability. 6.1 A helper lemma We begin with presenting a helper lemma that will be used extensively in the later part of the proof. The claim is intuitive: if the sample size is small, it is not sufficient to tell apart two Bernoulli distributions with similar means. Then, in our context, arm elimination will not occur. Lemma 6. Assume mB,i ≤ 2m⋆ B,i. For any B ⊆ [0, 1] and i ∈ {1, 2}. If ¯f (1) B − ¯f (−1) B ≤ δ ≤ 1/pm⋆ B,i for some δ > 0 , then P \u0010 ¯Y (1) B,i − ¯Y (−1) B,i > U(mB,i, T, B) \u0011 ≤ ti T . Proof. Fix 0 < τ ≤ m⋆ B,i. Let ¯Y (k) τ = Pτ l=1 Y (k) l /τ where Y (k) l ’s are i.i.d. random variables with Y (k) l ∈ [0, 1] and E[Y (k) l ] = ¯f (k) B for k ∈ {1, −1}. Recall U(τ, T, B) = 4 q log(2T |B|) τ 2. Then, P \u0010 ¯Y (1) τ − ¯Y (−1) τ > U(2τ, T, B) \u0011 (i) ≤ P   ¯Y (1) τ − ¯Y (−1) τ > δ + r log(2T/g) 2τ ! 2We remark the constant 4 is not essential for the proof to work. For any c > 0 , c log(2T|B|) = log((2T|B|)c) so the final success probability is still tiny as long as T is sufficiently large. 20 (ii) ≤ P   ¯Y (1) τ − ¯Y (−1) τ > ¯f (1) B − ¯f (−1) B + r log(2T/g) 2τ ! (iii) ≤ g T , where step (i) is because δ ≤ 1/pm⋆ B,i ≤ 1/√τ, step (ii) is due to ¯f (1) B − ¯f (−1) B ≤ δ, and step (iii) uses Hoeffding’s inequality. Applying union bound to get P \u0010 ∃0 < τ ≤ m⋆ B,i : ¯Y (1) τ − ¯Y (−1) τ > U(2τ, T, B) \u0011 ≤ m⋆ B,ig T ≤ ti T . This finishes the proof. 6.2 Three failure cases for g Fix some small constant ε > 0 to be specified later. From now on, we use ˆπ to denote ˆπstatic for simplicity. We split the proof into three cases: (1) g ≥ T 3/19+ε; (2) g ≤ T 3/19−ε; (3) and g ∈ (T 3/19−ε, T 3/19+ε). Case 1: g ≥ T 3/19+ε. Set z = T 3/19−ε/2. Assume without loss of generality that g = H ·z for some H ≥ 4; see Figure 3 for an illustration of the instance. Suppose C1 = ∪H l=1Bl, where Bl’s are the bins produced by ˆπ that lie in C1. It is clear that E[RT (ˆπ)] (i) ≥ E \" t2 X t=t1+1 \u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011# (ii) = E \" t2 X t=t1+1 \u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011 1{Xt ∈ C1} # (iii) ≥ t2 X t=t1+1 3H/4 X l=H/4 E h\u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011 1{Xt ∈ Bl} i , (16) where step (i) is because the total regret is greater than the regret incurred during the second batch, step (ii) uses the fact that under the instance v, the mean rewards of the two arms differ only in C1, and step (iii) arises since C1 = ∪H l=1Bl. Now we turn to lower bounding E \u0002\u0000f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0001 1{Xt ∈ Bl} \u0003 for each H/4 ≤ l ≤ 3H/4. Consider any such Bl. We drop the subscripts and write B instead for simplicity. By the design of v, we have ¯f (1) B − ¯f (−1) B ≤ Dϕz−1 = δ, which obeys Dϕz−1 ≤ 1/pm⋆ B,1—a consequence of the choice of z. Additionally, we have mB,1 ≤ 2m⋆ B,1 under E. Therefore, we can invoke Lemma 6 to obtain P \u0010 ¯Y (1) B,1 − ¯Y (−1) B,1 > U(mB,1, T, B) \u0011 ≤ t1 T ≤ 1 2. In words, with probability exceeding 1/2, no elimination will happen for the bin B. As a result, we obtain E[RT (ˆπ)] ≥ t2 X t=t1+1 3H/4 X l=H/4 E h\u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011 1{Xt ∈ Bl} i ≳ H · t2 g · z−1 ≍ t2 z2 = T 9 19 +ϵ, where we have used the choice of z. So Theorem (3) holds with κ = ϵ. 21 Case 2: g ≤ T 3/19−ε. Set z = T 3/19−ε/8. We have g < z and there exists H > 1 such that z = H · g; see Figure 4 for an illustration of the instance. Let B be the bin produced by ˆπ such that C1 ⊂ B. By the design of v, we have ¯f (1) B − ¯f (−1) B ≤ 1 H (1/2 + Dϕz−1) + (1 − 1 H )1 2 − 1 2 = Dϕz−1 H . Let δ = Dϕz−1 H , we have δ ≤ 1/pm⋆ B,1 due to our choice of z. Additionally, we have mB,1 ≤ 2m⋆ B,1 under E. Therefore, we can invoke Lemma 6 to obtain P \u0010 ¯Y (1) B,1 − ¯Y (−1) B,1 > U(mB,1, T, B) \u0011 ≤ t1 T ≤ 1 2. Thus, with probability exceeding 1/2, the suboptimal arm is not eliminated in B. Similar to the previous case, we obtain E[RT (ˆπ)] ≥ E \" t2 X t=t1+1 \u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011# = E \" t2 X t=t1+1 \u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011 1{Xt ∈ C1} # ? t2 z2 = T 9 19 + ϵ 4 . So Theorem (3) holds with κ = ϵ/4. Case 3: g ∈ (T 3/19−ε, T 3/19+ε). Set z ≍ T 1/4. We then have g < z, as long as ε ≤ 1/19. And there exists H > 1 such that z = H · g; see Figure 4 for an illustration of the instance. Let B be the bin produced by ˆπ such that C1 ⊂ B. By the design of v, we have ¯f (1) B − ¯f (−1) B ≤ 1 H (1/2 + Dϕz−1) + (1 − 1 H )1 2 − 1 2 = Dϕz−1 H . Let δ = Dϕz−1 H , we have δ ≤ 1/pm⋆ B,1 due to our choice of z. Additionally, we have mB,1 ≤ 2m⋆ B,1 under E. Therefore, we can invoke Lemma 6 to obtain P \u0010 ¯Y (1) B,1 − ¯Y (−1) B,1 > U(mB,1, T, B) \u0011 ≤ t1 T ≤ 1 4. This means with probability at least 3/4, arm elimination does not occur in B after the first batch. Moreover, since δ ≤ 1/pm⋆ B,2 by the choice of z, and mB,2 ≤ 2m⋆ B,2 under E, we can apply Lemma 6 again to get P \u0010 ¯Y (1) B,2 − ¯Y (−1) B,2 > U(mB,2, T, B) \u0011 ≤ t2 T ≤ 1 4. In all, with probability at least 1/2, arm elimination does not occur in B after the second batch. Similar to before, we reach the conclusion that E[RT (ˆπ)] ≥ E \" T X t=t2+1 \u0010 f ⋆(Xt) − f ˆπt(Xt)(Xt) \u0011# = E \" T X t=t2+1 (f ⋆(Xt) − f ˆπt(Xt)(Xt))1{Xt ∈ C1} # ≳ T z2 = T 1 2 . We see that Theorem (3) holds with κ = 1/38. 22 7 Discussion In this paper, we characterize the fundamental limits of batch learning in nonparametric contextual bandits. In particular, our optimal batch learning algorithm (i.e., Algorithm 1) is able to match the optimal regret in the fully online setting with only O(log log T) policy updates. Our work open a few interesting avenues to explore in the future. Extensions to multiple arms. With slight modification, our algorithm works for nonparametric contex- tual bandits with more than two arms. However, it remains unclear what the fundamental limits of batch learning are in this multi-armed case. Improving the log factor. Comparing the upper and lower bounds, it is evident that Algorithm 1 is near-optimal up to log factors. It is certainly interesting to improve this log factor, either by strengthening the lower bound, or making the upper bound more efficient. Adapting to smoothness and margin parameters. In practice, we do not always know the smoothness and the margin parameters. Can one develop a batch learning algorithm that can adapt to these unknown parameters? This question is intriguing because in the fully online setting, a similar adaptively binned successive elimination algorithm was proposed in [39] with the sole purpose of adapting to the margin parameter.3 Acknowledgements CM is partially supported by the National Science Foundation via grant DMS-2311127. References [1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic bandits. Advances in neural information processing systems, 24, 2011. [2] Sakshi Arya and Yuhong Yang. Randomized allocation with nonparametric estimation for contextual multi-armed bandits with delayed rewards. Statistics & Probability Letters, 164:108818, 2020. [3] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals of Statistics, 35(2):608–633, 2007. [4] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learn- ing Research, 3(Nov):397–422, 2002. [5] Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low switching cost. Advances in Neural Information Processing Systems, 32, 2019. [6] Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Opera- tions Research, 68(1):276–294, 2020. [7] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for con- textual bandits. Management Science, 67(3):1329–1349, 2021. [8] Dimitris Bertsimas and Adam J Mersereau. A learning approach for interactive marketing to a customer segment. Operations Research, 55(6):1120–1135, 2007. [9] Moise Blanchard, Steve Hanneke, and Patrick Jaillet. Non-stationary contextual bandits and universal learning. arXiv preprint arXiv:2302.07186, 2023. 3We emphasize again that if adaptivity to the margin parameter is not needed, then static binning suffices for the online setting. 23 [10] Changxiao Cai, T Tony Cai, and Hongzhe Li. Transfer learning for contextual multi-armed bandits. arXiv preprint arXiv:2211.12612, 2022. [11] T Tony Cai and Hongming Pu. Stochastic continuum-armed bandits with additive models: Minimax regrets and adaptive algorithm. The Annals of Statistics, 50(4):2179–2204, 2022. [12] Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other adaptive adversaries. Advances in Neural Information Processing Systems, 26, 2013. [13] Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1097–1105, 2014. [14] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural information processing systems, 24, 2011. [15] Stephen E Chick and Noah Gans. Economic analysis of simulation selection problems. Management Science, 55(3):421–437, 2009. [16] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stop- ping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine learning research, 7(6):1079–1105, 2006. [17] Jianqing Fan, Zhaoran Wang, Zhuoran Yang, and Chenlu Ye. Provably efficient high-dimensional bandit learning with batched feedbacks. arXiv preprint arXiv:2311.13180, 2023. [18] Yasong Feng, Zengfeng Huang, and Tianyu Wang. Lipschitz bandits with batched feedback. Advances in Neural Information Processing Systems, 35:19836–19848, 2022. [19] Manegueu Anne Gael, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits with arm-dependent delays. In International Conference on Machine Learning, pages 3348–3356. PMLR, 2020. [20] Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear markov decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021. [21] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem. Advances in Neural Information Processing Systems, 32, 2019. [22] Alexander Goldenshluger and Assaf Zeevi. Woodroofe’s one-armed bandit problem revisited. The Annals of Applied Probability, 19(4):1603–1633, 2009. [23] Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic Systems, 3(1):230–261, 2013. [24] Melody Guan and Heinrich Jiang. Nonparametric stochastic contextual bandits. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [25] Yonatan Gur, Ahmadreza Momeni, and Stefan Wager. Smoothness-adaptive contextual bandits. Oper- ations Research, 70(6):3198–3216, 2022. [26] Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu Ye. Se- quential batch learning in finite-action linear contextual bandits. arXiv preprint arXiv:2004.06321, 2020. [27] Yichun Hu, Nathan Kallus, and Xiaojie Mao. Smooth contextual bandits: Bridging the parametric and nondifferentiable regret regimes. Operations Research, 70(6):3261–3281, 2022. [28] Cem Kalkanli and Ayfer Ozgur. Batched thompson sampling. Advances in Neural Information Process- ing Systems, 34:29984–29994, 2021. 24 [29] Amin Karbasi, Vahab Mirrokni, and Mohammad Shadravan. Parallelizing thompson sampling. Advances in Neural Information Processing Systems, 34:10535–10548, 2021. [30] Edward S Kim, Roy S Herbst, Ignacio I Wistuba, J Jack Lee, George R Blumenschein Jr, Anne Tsao, David J Stewart, Marshall E Hicks, Jeremy Erasmus Jr, Sanjay Gupta, et al. The battle trial: person- alizing therapy for lung cancer. Cancer discovery, 1(1):44–53, 2011. [31] Aniket Kittur, Ed H Chi, and Bongwon Suh. Crowdsourcing user studies with mechanical turk. In Proceedings of the SIGCHI conference on human factors in computing systems, pages 453–456, 2008. [32] Anders Bredahl Kock and Martin Thyrsgaard. Optimal sequential treatment allocation. arXiv preprint arXiv:1705.09952, 2017. [33] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual ban- dits with continuous actions: Smoothing, zooming, and adapting. The Journal of Machine Learning Research, 21(1):5402–5446, 2020. [34] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020. [35] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to person- alized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670, 2010. [36] Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits. In Confer- ence on Learning Theory, pages 1463–1492. PMLR, 2018. [37] Tyler Lu, Dávid Pál, and Martin Pál. Showing relevant ads via context multi-armed bandits. In Proceedings of AISTATS, 2009. [38] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics, 27(6):1808–1829, 1999. [39] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. Ann. Statist., 41(2):693–721, 2013. [40] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems. Ann. Statist., 44(2):660–681, 2016. [41] Wei Qian, Ching-Kang Ing, and Ji Liu. Adaptive algorithm for multi-armed bandit problem with high-dimensional covariates. Journal of the American Statistical Association, pages 1–13, 2023. [42] Wei Qian and Yuhong Yang. Kernel estimation and model combination in a bandit problem with covariates. Journal of Machine Learning Research, 17(149), 2016. [43] Wei Qian and Yuhong Yang. Randomized allocation with arm elimination in a bandit problem with covariates. Electronic Journal of Statistics, 10(1):242–270, 2016. [44] Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog (t) switching cost. In International Conference on Machine Learning, pages 18031–18061. PMLR, 2022. [45] Henry Reeve, Joe Mellor, and Gavin Brown. The k-nearest neighbour ucb algorithm for multi-armed bandits with covariates. In Algorithmic Learning Theory, pages 725–752. PMLR, 2018. [46] Zhimei Ren and Zhengyuan Zhou. Dynamic batch learning in high-dimensional sparse linear contextual bandits. Management Science, 2023. [47] Zhimei Ren, Zhengyuan Zhou, and Jayant R Kalagnanam. Batched learning in generalized linear contextual bandits with general decision sets. IEEE Control Systems Letters, 6:37–42, 2020. [48] Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. arXiv preprint arXiv:1003.1630, 2010. 25 [49] Herbert E. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society, 58:527–535, 1952. [50] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising using multi-armed bandit experiments. Marketing Science, 36(4):500–522, 2017. [51] Joe Suk and Samory Kpotufe. Tracking most significant shifts in nonparametric contextual bandits. arXiv preprint arXiv:2307.05341, 2023. [52] Joseph Suk and Samory Kpotufe. Self-tuning bandits over unknown covariate-shifts. In Algorithmic Learning Theory, pages 1114–1156. PMLR, 2021. [53] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health. In Mobile Health, pages 495–517. Springer, 2017. [54] Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics, 32(1):135–166, 2004. [55] Claire Vernade, Olivier Cappé, and Vianney Perchet. Stochastic bandit models for delayed conversions. arXiv preprint arXiv:1706.09186, 2017. [56] Chi-Hua Wang and Guang Cheng. Online batch decision-making with high-dimensional covariates. In International Conference on Artificial Intelligence and Statistics, pages 3848–3857. PMLR, 2020. [57] Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Provably efficient reinforcement learning with linear function approximation under adaptivity constraints. Advances in Neural Information Processing Systems, 34:13524–13536, 2021. [58] Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American Statistical Association, 74(368):799–806, 1979. [59] Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi-armed bandit problem with covariates. Ann. Statist., 30(1):100–121, 2002. [60] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in neural information processing systems, 33:9818–9829, 2020. [61] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via reference-advantage decomposition. Advances in Neural Information Processing Systems, 33:15198– 15207, 2020. [62] Zhijin Zhou, Yingfei Wang, Hamed Mamani, and David G Coffey. How do tumor cytogenetics in- form cancer treatments? dynamic risk stratification and precision medicine using multi-armed bandits. Dynamic Risk Stratification and Precision Medicine Using Multi-armed Bandits (June 17, 2019). 26 "
}