{
    "optim": "Batched Nonparametric Contextual Bandits\nRong Jiang1 and Cong Ma2\n1Committee on Computational and Applied Mathematics, University of Chicago\n2Department of Statistics, University of Chicago\nFebruary 28, 2024\nAbstract\nWe study nonparametric contextual bandits under batch constraints, where the expected reward for\neach action is modeled as a smooth function of covariates, and the policy updates are made at the\nend of each batch of observations.\nWe establish a minimax regret lower bound for this setting and\npropose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret\n(up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins,\ncarefully aligning their widths with the batch size. We also show the suboptimality of static binning\nunder batch constraints, highlighting the necessity of dynamic binning. Additionally, our results suggest\nthat a nearly constant number of policy updates can attain optimal regret in the fully online setting.\n1\nIntroduction\nRecent years have witnessed substantial progress in the field of sequential decision making under uncertainty.\nEspecially noteworthy are the advancements in personalized decision making, where the decision maker uses\nside-information to make customized decision for a user. The contextual bandit framework has been widely\nadopted to model such problems because of its capability and elegance [35, 53, 6]. In this framework, one\ninteracts with an environment for a number of rounds: at each round, one is given a context, picks an action,\nand receives a reward. One can update the action-assignment policy based on previous observations and\nthe goal is to maximize the expected cumulative rewards. For example, in online news recommendation,\na recommendation algorithm selects an article for each newly arrived user based on the user’s contextual\ninformation, and observes whether the user clicks the article or not. The goal is to try to maximize the\nnumber of clicks received.\nApart from news recommendation, contextual bandits have found numerous\napplications in other fields such as clinical trials, personalized medicine, and online advertising [30, 62, 13].\nAt the core of designing a contextual bandit algorithm is deciding how to update the policy based on\nprior observations. A standard metric of performance for bandit algorithms is regret, which is the expected\ndifference between the cumulative rewards obtained by an oracle who knows the optimal action for every\ncontext and that obtained by the actual algorithm under consideration. Many existing regret optimal bandit\nalgorithms require a policy update per observation (unit) [4, 1, 39, 34]. At a first glance, such frequent policy\nupdates are needed so that the algorithm can quickly learn the optimal action under each context and reduce\nregret. However, this kind of algorithm ignores an important concern in the practice of sequential decision\nmaking—the batch constraint.\nIn many real world scenarios, the data often arrive in batches: the statistician can only observe the\noutcomes of the policy at the end of a batch, and then decides what to do for the next batch. For example,\nthis batch constraint is ubiquitous in clinical trials: statisticians need to divide the participants into batches,\ndetermine a treatment allocation policy before the batch starts, and then observe all the outcomes at the\nend of the batch [49]. Policy updates are made per batch instead of per unit. In fact, it is infeasible to\napply unit-wise policy update in this case because observing the effect of a treatment takes time and if one\nwaits for the result before deciding how to treat the next patient, the entire experiment will take too long to\ncomplete when the number of participants is huge. The batch constraint also appears in areas such as online\n1\narXiv:2402.17732v1  [math.ST]  27 Feb 2024\nmarketing, crowdsourcing, and simulations [8, 50, 31, 15]. Clearly, the batch constraint presents additional\nchallenges to online learning. Indeed, from an information perspective, the statistician’s information set is\nlargely restricted since she can only observe all the responses at the end of a batch. The following questions\nnaturally arise:\nGiven a batch budget M and a total number of T rounds, how should the statistician determine the size\nof each batch, and how should she update the policy after each batch? Can the statistician design batch\nlearning algorithms that achieve regret performances on par with the fully online setting using as few policy\nupdates as possible?\n1.1\nMain contributions\nIn this work, we address the aforementioned questions under a classical framework for personalized decision\nmaking—nonparametric contextual bandits [48, 39]. In this framework, the expected reward associated with\neach treatment (or arm in the language of bandits) is modeled as a nonparametric smooth function of the\ncovariates [59]. In the fully online setup, seminal works [48, 39] establish the minimax optimal regret bounds\nfor the nonparametric contextual bandits. Nevertheless, under the more challenging setting with the batch\nconstraint, the fundamental limits for nonparametric bandits remain unknown. Our paper aims to bridge\nthis gap. More concretely, we make the following three novel contributions:\n• First, we establish a minimax regret lower bound for the nonparametric bandits with the batch con-\nstraint M. Our proof relies on a simple but useful insight that the worst-case regret over the entire\nhorizon is greater than the worst-case regret over the first i batches for all 1 ≤ i ≤ M. To fully exploit\nthis insight, for each different batch number i, we construct different families of hard instances to target\nthis batch, leading to a maximal regret over this batch.\n• In addition, we demonstrate that the aforementioned lower bound is tight by providing a matching\nupper bound (up to log factors). Specifically, we design a novel algorithm—Batched Successive Elimi-\nnation with Dynamic Binning (BaSEDB)—for the nonparametric bandits with batch constraints. BaSEDB\nprogressively splits the covariate space into smaller bins whose widths are carefully selected to align\nwell with the corresponding batch size. The delicate interplay between the batch size and the bin width\nis crucial for obtaining the optimal regret in the batch setting.\n• On the other hand, we show the suboptimality of static binning under the batch constraint by proving\nan algorithm-specific lower bound. Unlike the fully online setting where policies that use a fixed number\nof bins can attain the optimal regret [39], our lower bound indicates that batched successive elimination\nwith static binning is strictly suboptimal. This highlights the necessity of dynamic binning in some\nsense under the batch setting, which is uncommon in classical nonparametric estimation.\nIt is also worth mentioning that an immediate consequence of our results is that M ≳ log log T number of\nbatches suffices to achieve the optimal regret in the fully online setting. In other words, we can use a nearly\nconstant number of policy updates in practice to achieve the optimal regret obtained by policies that require\none update per round.\n1.2\nRelated work\nNonparametric contextual bandits.\n[58] introduced the mathematical framework of contextual bandit.\nThe theory of contextual bandits in the fully online setting has been continuously developed in the past few\ndecades. On one hand, [4, 1, 23, 6, 7, 41] obtained learning guarantees for linear contextual bandits in\nboth low and high dimensional settings. On the other hand, [59] introduced the nonparametric approach\nto model the mean reward function. [48] proved a minimax lower bound on the regret of nonparametric\nbandit and developed an upper-confidence-bound (UCB) based policy to achieve a near-optimal rate. [39]\nimproved this result and proposed the Adaptively Binned Successive Elimination (ABSE) policy that can\nalso adapt to the unknown margin parameter. Further insights in this nonparametric setting were developed\nin subsequent works [42, 43, 45, 24, 27, 52, 25, 10, 51, 9]. The smoothness assumption is also adopted in\nanother line of work [37, 36, 33, 11] on the continuum-armed bandit problems. However in contrast to what\nwe study, the reward is assumed to be a Lipschitz function of the action, and the covariates are not taken\ninto considerations.\n2\nBatch learning.\nThe batch constraint has received increasing attention in recent years. [40, 21] considered\nthe multi-armed bandit problem under the batch setting and showed that O(log log T) batches are adequate\nin achieving the rate-optimal regret, compared to the fully online setting. [26, 47] extended batch learning to\nthe (generalized) linear contextual bandits and [46, 56, 17] further studied the setting with high-dimensional\ncovariates. [29, 28] established batch learning guarantees for the Thompson sampling algorithm. [18] consid-\nered Lipschitz continuum-armed bandit problem with the batch constraint. Inference for batched bandits was\nconsidered in [60]. A concept related to batch learning in literature is called delayed feedback [14, 13, 55, 19].\nThese works consider the setting where rewards are observed with delay and analyze effects of delay on the\nregret. [32, 2] studied delayed feedback in nonparametric bandits and the key difference to batch learning is\nthat the batch size is given, whereas in our case, it is a design choice by the statistician. Batch learning’s\nfocus is different to that of delayed feedback in the sense that the former gives the decision maker discretion\nto choose the batch size which makes it possible to approximate the optimal standard online regret with a\nsmall number of batches. Finally, the notion switching cost is intimately related to the batch constraint.\n[12] studied online learning with low switching cost and obtained minimax optimal regret with O(log log T)\nbatches. [5, 61, 20, 57, 44] developed regret guarantees with low switching cost for reinforcement learning.\nLow switching cost can be interpreted as infrequent policy updates, but it does not require the learner to\ndivide the samples into batches with feedback only becoming available at the end of a batch.\n2\nProblem setup\nWe begin by introducing the problem setup for nonparametric bandits with the batch constraint.\nA two-arm nonparametric bandit with horizon T ≥ 1 is specified by a sequence of independent and\nidentically distributed random vectors\n(Xt, Y (1)\nt\n, Y (−1)\nt\n),\nfor t = 1, 2, . . . , T,\n(1)\nwhere Xt is sampled from a distribution PX. Throughout the paper, we assume that Xt ∈ X := [0, 1]d, and\nPX has a density (w.r.t. the Lebesgue measure) that is bounded below and above by some constants c, ¯c > 0,\nrespectively. For k ∈ {1, −1} and t ≥ 1, we assume that Y (k)\nt\n∈ [0, 1] and that\nE[Y (k)\nt\n| Xt] = f (k)(Xt).\nHere f (k) is the unknown mean reward function for the arm k.\nWithout the batch constraint, the game of nonparametric bandits plays sequentially. At each step t, the\nstatistician observes the context Xt, and pulls an action At ∈ {1, −1} according to a rule πt : X 7→ {1, −1}.\nThen she receives the corresponding reward Y (At)\nt\n. In this case, the rule πt for selecting the action at time\nt is allowed to depend on all the observations strictly anterior to t.\nIn an M-batch game, the statistician is asked to divide the horizon [1 : T] into M disjoint batches\n[1 : t1], [t1 + 1 : t2], . . . , [tM−1 + 1, T]. In contrast to the case without the batch constraint, only the rewards\nassociated with timesteps prior to the current batch are observed and available for making decisions for the\ncurrent batch. More formally, an M-batch policy is composed of a pair (Γ, π), where Γ = {t0,t1,...,tM} is a\npartition of the entire time horizon T that satisfies 0 = t0 < t1 < ... < tM−1 < tM = T, and π = {πt}T\nt=1\nis a sequence of random functions πt : X 7→ {1, −1}. Let Γ(t) be the batch index for the time t, i.e., Γ(t)\nis the unique integer such that tΓ(t)−1 < t ≤ tΓ(t). Then at time t, the available information for πt is only\n{Xl}t\nl=1 ∪ {Y (Al)\nl\n}Γ(t)−1\nl=1\n, which we denote by Ft. The statistician’s policy πt at time t is allowed to depend\non Ft.\nThe goal of the statistician is to design an M-batch policy (Γ, π) that can compete with an oracle that has\nperfect knowledge (i.e., the law of (Xt, Y (1)\nt\n, Y (−1)\nt\n)) of the environment. Formally, we define the cumulative\nregret as\nRT (π) := E\n\" T\nX\nt=1\n\u0010\nf ⋆(Xt) − f (πt(Xt))(Xt)\n\u0011#\n,\n(2)\nwhere f ⋆(x) := maxk∈{1,−1} f (k)(x) is the maximum mean reward one could obtain on the context x. Note\nhere we omit the dependence on Γ for simplicity.\n3\n2.1\nAssumptions\nWe adopt two standard assumptions in the nonparametric bandits literature [48, 39]. The first assumption\nis on the smoothness of the mean reward functions.\nAssumption 1 (Smoothness). We assume that the reward function for each arm is (β, L)-smooth, that is,\nthere exist β ∈ (0, 1] and L > 0 such that for k ∈ {1, −1},\n|f (k)(x) − f (k)(x′)| ≤ L∥x − x′∥β\n2\nholds for all x, x′ ∈ X.\nThe second assumption is about the separation between the two reward functions.\nAssumption 2 (Margin). We assume that the reward functions satisfy the margin condition with parameter\nα > 0, that is there exist δ0 ∈ (0, 1) and D0 > 0 such that\nPX\n\u0010\n0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ δ\n\u0011\n≤ D0δα\nholds for all δ ∈ [0, δ0].\nAssumption 2 is related to the margin condition in classification [38, 54, 3] and is introduced to bandits in\n[22, 48, 39]. The margin parameter affects the complexity of the problem. Intuitively, a small α, say α ≈ 0,\nmeans the two mean functions are entangled with each other in many regions and hence it is challenging to\ndistinguish them; a large α, on the other hand, means the two reward functions are mostly well-separated.\nFrom now on, we use F(α, β) to denote the class of nonparametric bandit instances (i.e., distributions\nover (1)) that satisfy Assumptions 1-2.\nRemark 1. Throughout the paper, we assume that αβ ≤ 1. By proposition 2.1 from [48], when αβ > 1,\none of the arms will dominate the other one for the entire covariate space. The instance is reduced to a\nmulti-armed bandit without covariates which is not the interest of the current paper. Therefore, we focus on\nthe case αβ ≤ 1 hereafter.\n3\nFundamental limits of batched nonparametric bandits\nSomewhat unconventionally, we start with stating a minimax lower bound, as well as its proof, for regret\nminimization in batched nonparametric contextual bandits. As we will soon see, the proof of the lower\nbound is extremely instrumental in our development of an optimal M-batch policy (Γ, π), to be detailed in\nSection 4.\nRecall that F(α, β) denotes the class of nonparametric bandit instances (i.e., distributions over (1)) that\nobey Assumptions 1-2. We have the following minimax lower bound for any M-batch policy, in which we\ndefine\nγ := β(1 + α)\n2β + d\n∈ (0, 1).\nTheorem 1. Suppose that αβ ≤ 1, and assume that PX is the uniform distribution on X = [0, 1]d. For any\nM-batch policy (Γ, π), there exists a nonparametric bandit instance in F(α, β) such that the regret of (Γ, π)\non this instance is lower bounded by\nE[RT (π)] ≥ ˜DT\n1−γ\n1−γM ,\nwhere ˜D > 0 is a constant independent of T and M.\nSee Section 3.1 for the proof of this lower bound.\nAs a sanity check, one sees that as M increases, the lower bound decreases. This is intuitive, as the\npolicy is more powerful as M increases. As a result, the problem of batched nonparametric bandits becomes\neasier.\n4\n3.1\nProof of Theorem 1\nLet (Γ, π) be the M-batch policy under consideration, with\nΓ = {t0 = 0, t1, t2, . . . , tM = T}.\nThroughout this proof, we consider Bernoulli reward distributions, that is Y (1)\nt\n, Y (−1)\nt\nare Bernoulli random\nvariables with mean f (1)(Xt), and f (−1)(Xt), respectively. In addition, we fix f (−1)(x) = 1\n2. Let f be the\nmean reward function of the first arm. To make the dependence on the reward instance clear, we write the\ncumulative regret up to time n as Rn(π; f).\nOur proof relies on a simple observation: the worst-case regret over [T] is larger than the worst-case\nregret over the first i batches. Formally, we have\nsup\n(f, 1\n2 )∈F(α,β)\nRT (π; f) ≥ max\n1≤i≤M\nsup\n(f, 1\n2 )∈F(α,β)\nRti(π; f).\n(3)\nThough simple, this observation lends us freedom on choosing different families of instances in F(α, β)\ntargeting different batch indices i.\nOur proof consists of four steps. In Step 1, we reduce bounding the regret of a policy to lower bounding\nits inferior sampling rate to be defined. In Step 2, we detail the choice of different families of instances for\neach different batch index i. Then in Step 3, we apply an Assouad-type of argument to lower bound the\naverage inferior sampling rate of the family of hard instances. Lastly in Step 4, we combine the arguments\nto complete the proof.\nStep 1: Relating regret to inferior sampling rate.\nGiven an M-batch policy, we define its inferior\nsampling rate at time n on an instance (f, 1\n2) to be\nSn(π; f) := E\n\" n\nX\nt=1\n1{πt(Xt) ̸= π⋆(Xt), f(Xt) ̸= 1\n2}\n#\n.\nIn words, Sn(π; f) counts the number of times π selects the strictly suboptimal arm up to time n. Thanks\nto the following lemma, we can reduce lower bounding the regret to the inferior sampling rate.\nLemma 1 (Lemma 3.1 in [48]). Suppose that (f, 1\n2) ∈ F(α, β). Then for any 1 ≤ n ≤ T, we have\nSn(π; f) ≤ Dn\n1\n1+α Rn(π; f)\nα\n1+α ,\nfor some constant D > 0.\nAs an immediate consequence of the above lemma, we obtain\nsup\n(f, 1\n2 )∈F(α,β)\nRT (π; f) ≥ max\n1≤i≤M\nsup\n(f, 1\n2 )∈F(α,β)\n( 1\nD)\n1+α\nα t\n− 1\ni α\n(Sti(π; f))\n1+α\nα\n= ( 1\nD)\n1+α\nα\nmax\n1≤i≤M t\n− 1\nα\ni\n\"\nsup\n(f, 1\n2 )∈F(α,β)\nSti(π; f)\n# 1+α\nα\n.\nFrom now on, we focus on lower bounding sup(f, 1\n2 )∈F(α,β) Sti(π; f).\nStep 2: Introducing the family of reward instances for ti.\nOur construction of the family of hard\ninstances is adapted from [48]. Define z1 = 1, and zi = ⌈(ti−11/(2β+d)⌉ for i = 2, 3, . . . , M. Henceforth, we\nwill fix some i and write zi as z. We partition [0, 1]d into zd bins with equal width. Denote the bins by Cj\nfor j = 1, ..., zd, and let qj be the center of Cj.\nDefine a set of binary sequences Ωs := {±1}s, with s := ⌈zd−αβ⌉. For each ω ∈ Ωs we define a function\nfω : [0, 1]d 7→ R:\nfω(x) = 1\n2 +\ns\nX\nj=1\nωjφj(x),\n5\nwhere φj(x) = Dϕz−βϕ(2z(x−qj))1{x ∈ Cj} with ϕ(x) = (1−∥x∥∞)β1{∥x∥∞ ≤ 1}, and Dϕ = min(2−βL, 1/4).\nIn all, we consider the family of reward instances\nCz :=\n\u001a\nf (1)(x) = fω(x), f (−1)(x) = 1\n2 | ω ∈ Ωs\n\u001b\n.\nWith slight abuse of notation, we also use Cz to denote {fω : ω ∈ Ωs}. It is straightforward to check that\nCz ⊆ F(α, β).\nStep 3: Lower bounding the inferior sampling rate.\nFix some i ∈ [M], and consider z = zi. Since\nCz ⊆ F(α, β), we have\nsup\n(f, 1\n2 )∈F(α,β)\nSti(π; f) ≥ sup\nf∈Cz\nSti(π; f).\nUsing the definitions of Cz and Sti(π; f), we have\nsup\nf∈Cz\nSti(π; f) = sup\nω∈Ωs\nEπ,fω\n\" ti\nX\nt=1\n1{πt(Xt) ̸= sign(fω(Xt) − 1\n2), fω(Xt) ̸= 1\n2}\n#\n≥ 1\n2s\nX\nω∈Ωs\nEπ,fω\n\" ti\nX\nt=1\n1{πt(Xt) ̸= sign(fω(Xt) − 1\n2), fω(Xt) ̸= 1\n2}\n#\n.\nSince fω(x) = 1\n2 for x /∈ ∪j=1,...sCj, we further obtain\nsup\nf∈Cz\nSti(π; f) ≥ 1\n2s\nX\nω∈Ωs\nti\nX\nt=1\ns\nX\nj=1\nEt\nπ,fω [1{πt(Xt) ̸= ωj, Xt ∈ Cj}] .\n(4)\nHere we use Pt\nπ,fω to denote the joint distribution of {Xl}t\nl=1 ∪ {Y πl(Xl)\nl\n}Γ(t)−1\nl=1\n, where Γ(t) is the batch\nindex for t, i.e., the unique integer such that tΓ(t)−1 < t ≤ tΓ(t). We use Et\nπ,fω to denote the corresponding\nexpectation. Expand the right hand side of (4) to see that\nsup\nf∈Cz\nSti(π; f) ≥ 1\n2s\ns\nX\nj=1\nti\nX\nt=1\nX\nω[−j]∈Ωs−1\nX\nh∈{±1}\nEt\nπ,fωh\n[−j]\n[1{πt(Xt) ̸= h, Xt ∈ Cj}]\n|\n{z\n}\nWj,t,ω[−j]\n,\n(5)\nwhere ωh\n[−j] is the same as ω except for the j-th entry being h. Note that here we use the fact that for fωh\n[−j],\nthe optimal arm in the bin Cj is h. We then relate Wj,t,ω[−j] to a binary testing error,\nWj,t,ω[−j] = 1\nzd\nX\nh∈{±1}\nPt\nπ,fωh\n[−j]\n(πt(Xt) ̸= h | Xt ∈ Cj)\n≥\n1\n4zd exp\n\u0014\n−KL(Pt\nπ,fω−1\n[−j]\n, Pt\nπ,fω1\n[−j]\n)\n\u0015\n,\n(6)\nwhere the second step invokes Le Cam’s method. Under the batch setting, at time t, the available information\nis only up to tΓ(t)−1. Consequently, the KL divergence KL(Pt\nπ,fω−1\n[−j]\n, Pt\nπ,fω1\n[−j]\n) can be controlled by\nKL(Pt−1\nπ,fω−1\n[−j]\n, Pt−1\nπ,fω1\n[−j]\n)\n(i)\n≤ 8Eπ,fω−1\n[−j] [\ntΓ(t)−1\nX\nt=1\n(fω−1\n[−j](Xt) − fω1\n[−j](Xt))21{πt(Xt) = 1}]\n(ii)\n≤ 32D2\nϕz−2βEπ,fω−1\n[−j] [\ntΓ(t)−1\nX\nt=1\n1{πt(Xt) = 1, Xt ∈ Cj}]\n6\n(iii)\n= 32D2\nϕz−(2β+d)\ntΓ(t)−1\nX\nt=1\nPt\nπ,fω−1\n[−j]\n(πt(Xt) = 1 | Xt ∈ Cj)\n(iv)\n≤ 32D2\nϕz−(2β+d)tΓ(t)−1.\n(7)\nHere, step (i) uses the standard decomposition of KL divergence and Bernoulli reward structure; step (ii) is\ndue to the definition of fω; step (iii) uses P(Xt ∈ Cj) = 1/zd, and step (iv) arises from Pt\nπ,fω−1\n[−j]\n(πt(Xt) =\n1 | Xt ∈ Cj) ≤ 1 for any 1 ≤ t ≤ T. Combining (5), (6), and (7), we arrive at\nsup\nf∈Cz\nSti(π; f) ≥ 1\n8\ns\nX\nj=1\nti\nX\nt=1\n1\nzd exp\n\u0010\n−32D2\nϕz−(2β+d)tΓ(t)−1\n\u0011\n≥ 1\n8\nzd−αβ\nX\nj=1\ni\nX\nl=1\ntl − tl−1\nzd\nexp\n\u0010\n−32D2\nϕz−(2β+d)tl−1\n\u0011\n≥ 1\n8\nzd−αβ\nX\nj=1\ni\nX\nl=1\ntl − tl−1\nzd\nexp\n\u0010\n−32D2\nϕz−(2β+d)ti−1\n\u0011\n,\nwhere the second line uses the fact that s = ⌈zd−αβ⌉, and the last inequality holds since tl−1 ≤ ti−1 for all\n1 ≤ l ≤ i. Now recall that z = zi = ⌈(ti−1)1/(2β+d)⌉ for i ≥ 1, and z = 1 for i = 1. We can continue the\nlower bound to see that\nsup\nf∈Czi\nSti(π; f) ≥ 1\n8\nzd−αβ\nX\nj=1\ni\nX\nl=1\ntl − tl−1\nzd\nexp\n\u0010\n−32D2\nϕz−(2β+d)ti−1\n\u0011\n≥ c⋆\nzd−αβ\nX\nj=1\ni\nX\nl=1\ntl − tl−1\nzd\n= c⋆ · ti\nzαβ =\n\n\n\nc⋆ ·\nti\nt\nαβ\n2β+d\ni−1\n,\ni > 1\nc⋆t1,\ni = 1\n,\nfor some c⋆ > 0.\nStep 4: Combining bounds together.\nCombining the previous arguments together leads to the con-\nclusion that\nsup\n(f, 1\n2 )∈F(α,β)\nRT (π; f) ≥ max\n1≤i≤M sup\nf∈Czi\nRti(π; f)\n≥ ( 1\nD)\n1+α\nα\nmax\n1≤i≤M t\n− 1\nα\ni\n\"\nsup\nf∈Czi\nSti(π; f)\n# 1+α\nα\n≳ max\n\u001a\nt1, t2\ntγ\n1\n, ...,\nT\ntγ\nM−1\n\u001b\n(8)\n≥ ˜DT\n1−γ\n1−γM .\nThis finishes the proof.\n3.2\nImplications on design of the optimal M-batch policy\nAs we have mentioned, the proof of the lower bound, i.e., Theorem 1 facilitates the design of optimal M-batch\npolicy.\n7\nAlgorithm 1 Batched successive elimination with dynamic binning (BaSEDB)\nInput: Batch size M, grid Γ = {ti}M\ni=0, split factors {gi}M−1\ni=0 .\nL ← B1\nfor C ∈ L do\nIC = I\nfor i = 1, ..., M − 1 do\nfor t = ti−1 + 1, ..., ti do\nC ← L(Xt)\nPull an arm from IC in a round-robin way.\nif t = ti then\nUpdate L and {IC}C∈L by Algorithm 2 (L, {IC}C∈L, i, gi).\nfor t = tM−1 + 1, ..., T do\nC ← L(Xt)\nPull any arm from IC.\nGrid selection.\nFirst, the lower bound of the whole horizon is reduced to the worst-case regret over a\nspecific batch; see (3). Consequently, we need to design the grid Γ = (t0, t1, t2, . . . , tM−1, tM) such that the\ntotal regret is evenly distributed across batches. More concretely, in view of the lower bound (8), one needs\nto set t1 ≍\nti\ntγ\ni−1 ≍ T\n1−γ\n1−γM for 2 ≤ i ≤ M.\nDynamic binning.\nIn addition, in the proof of the lower bound, for each different batch i, we use different\nfamilies of hard reward instances, parameterized by the number of bins zi = ⌈t1/(2β+d)\ni−1\n⌉. In other words,\nfrom the lower bound perspective, the granularity (i.e., the bin width 1/zi) at which we investigate the\nmean reward function depends crucially on the grid points {ti}: the larger the grid point ti, the finer the\ngranularity. This key observation motivates us to consider the batched successive elimination with dynamic\nbinning algorithm to be introduced below.\n4\nBatched successive elimination with dynamic binning\nIn this section, we present the batched successive elimination with dynamic binning policy (BaSEDB) that\nnearly attains the minimax lower bound, up to log factors; see Algorithm 1. On a high level, Algorithm 1\ngradually partitions the covariate space X into smaller hypercubes (i.e., bins) throughout the batches based\non a list of carefully chosen cube widths, and reduces the nonparametric bandit in each cube to a bandit\nproblem without covariates.\nA tree-based interpretation.\nThe process is best illustrated with the notion of a tree T of depth M;\nsee Figure 1. Each layer of of the tree T is a set of bins that form a regular partition of X using hypercubes\nwith equal widths. And the common width of the bins Bi in layer i is dictated by a list {gi}M−1\ni=0\nof split\nfactors. More precisely, we let\nwi := (\ni−1\nY\nl=0\ngl)−1\n(9)\nbe the width of the cubes in the i-th layer Bi for i ≥ 1, and w0 = 1. In other words, Bi contains all the cubes\nCi,v = {x ∈ X : (vj − 1)wi ≤ xj < vjwi, 1 ≤ j ≤ d},\nwhere v = (v1, v2, . . . , vd) ∈ [ 1\nwi ]d. As a result, there are in total ( 1\nwi )d bins in Bi.\n8\nAlgorithm 2 Tree growing subroutine\nInput: Active nodes L, active arm sets {IC}C∈L, batch number i, split factor gi.\nL′ ← {}\nfor C ∈ L do\nif |IC| = 1 then\nL′ ← L′ ∪ {C}\nProceed to next C in the iteration.\n¯Y max\nC,i\n← maxk∈IC ¯Y (k)\nC,i\nfor k ∈ IC do\nif ¯Y max\nC,i\n− ¯Y (k)\nC,i > U(mC,i, T, C) then IC ← IC − {k}\nif |IC| > 1 then\nIC′ ← IC for C′ ∈ child(C, gi)\nL′ ← L′ ∪ child(C, gi)\nelse\nL′ ← L′ ∪ {C}\nReturn L′\nAlgorithm 1 proceeds in batches and maintains two key objects: (1) a list L of active bins, and (2) the\ncorresponding active arms IC for each C ∈ L; see Figure 1 for an example. Specifically, prior to the game\n(i.e., prior to the first batch), L is set to be B1, all bins in layer 1, and IC = {1, −1} for all C ∈ L. Within\nthis batch, the statistician tries the arms in IC equally likely for all bins in L. Then at the end of the\nbatch, given the revealed rewards in this batch, we update the active arms IC for each C ∈ L via successive\nelimination. If no arm were eliminated from IC, this suggests that the current bin is not fine enough for the\nstatistician to tell the difference between the two arms. As a result, she splits the bin C ∈ L into its children\nchild(C) in T . All the child nodes will be included in L, while the parent C stops being active (i.e., C is\nremoved from L). The whole process repeats in a batch fashion. 1\nWhen to eliminate arms?\nNow we zoom in on the elimination process described in Algorithm 2. The\nbasic idea follows from successive elimination in the bandit literature [16, 39, 21]: the statistician eliminates\nan arm from IC if she expects the arm to be suboptimal in the bin C given the rewards collected in C.\nSpecifically, for any node C ∈ T , define\nU(τ, T, C) := 4\nr\nlog(2T|C|d)\nτ\n,\nwhere |C| denotes the width of the bin. Let mC,i := Pti\nt=ti−1+1 1{Xt ∈ C} be the number of times we\nobserve contexts from C in batch i. We then define for k ∈ {1, −1} that\n¯Y (k)\nC,i :=\nPti\nt=ti−1+1 Yt · 1{Xt ∈ C, At = k}\nPti\nt=ti−1+1 1{Xt ∈ C, At = k}\n,\nwhich is the empirical mean reward of arm k in node C during the i-th batch. It is easy to check that ¯Y (k)\nC,i\nhas expectation ¯f (k)\nC\ngiven by\n¯f (k)\nC\n:= E[f (k)(X) | X ∈ C] =\n1\nPX(C)\nZ\nC\nf (k)(x)dPX(x).\n1For the final batch M, the split factor gM−1 = 1 by default because there is no need to further partition the nodes for\nestimation.\n9\n[0, 1]\n[0, 1\n4)\n[0, 1\n12)\n[ 1\n12, 1\n6)\n[ 1\n6, 1\n4)\n[ 1\n4, 1\n2)\n[ 1\n2, 3\n4)\n[ 3\n4, 1]\n[ 3\n4, 5\n6)\n[ 5\n6, 11\n12)\n[ 11\n12, 1]\nFigure 1: An example of the tree growing process for d = 1, M = 3, G = {4, 3, 1}. The root node is at depth\n0. For the first batch, the 4 nodes located at depth 1 of the tree were used. Both [ 1\n4, 1\n2) and [ 1\n2, 3\n4) only\nhad one active arm remaining so they were not further split and remained in the set of active nodes (green).\nMeanwhile, |I[0, 1\n4 )| = |I[ 3\n4 ,1]| = 2 so each of them was split into 3 smaller nodes, and both nodes were\nmarked as inactive (red). For the second batch, all the green nodes were actively used but arm elimination\nwas performed at the end of batch 2 only for nodes located at depth 2 (the green nodes at depth 1 already\nhave 1 active arm remaining so there is no need to eliminate again).\nSimilarly, we define the average optimal reward in bin C to be\n¯f ⋆\nC :=\n1\nPX(C)\nZ\nC\nf ⋆(x)dPX(x).\nThe elimination threshold U(mC,i, T, C) is chosen such that an arm k with ¯f ⋆\nC − ¯f (k)\nC\n≫ |C|β is eliminated\nwith high probability at the end of batch i. Therefore, when |IC| > 1, the remaining arms are statistically\nindistinguishable from each other, so C is split into smaller nodes to estimate those arms more accurately\nusing samples from future batches. On the other hand, when |IC| = 1, the remaining arm is optimal in\nC with high probability—a consequence of the smoothness condition, and it will be exploited in the later\nbatches.\nGrid Γ and split factors {gi}M−1\ni=0 .\nAs one can see, the split factor gi controls how many children a node\nat layer i can have and its appropriate choice is crucial for obtaining small regret. Intuitively, gi should be\nselected in a way such that a node Ci+1 with width wi can fully leverage the number of samples allocated to\nit during the (i + 1)-th batch. With these goals in mind, we design the grid Γ = {ti} and split factors {gi}\nas follows. Recall that γ = β(1+α)\n2β+d . We set\nb = Θ\n\u0010\nT\n1−γ\n1−γM \u0011\n.\nThe split factors are chosen according to\ng0 = ⌊b\n1\n2β+d ⌋,\nand\ngi = ⌊gγ\ni−1⌋, i = 1, ..., M − 2.\n(10)\nIn addition, the grid is chosen such that\nti − ti−1 = ⌊liw−(2β+d)\ni\nlog(Twd\ni )⌋, 1 ≤ i ≤ M − 1,\n(11)\n10\nwhere li > 0 is a constant to be specified later. It is easy to check that with these choices, we have\nt1 ≍ T\n1−γ\n1−γM ,\nand\nti = ⌊b(ti−1)γ⌋,\nfor i = 2, ..., M.\nIn particular, we set b properly to make tM = T. Indeed, these choices taken together meet the expectation\nlaid out in Section 3.2: we need to choose the grid and the split factors appropriately so that (1) the total\nregret spreads out across different batches, and (2) the granularity becomes finer as we move further to later\nbatches.\nConnections and differences with ABSE in [39].\nIn appearance, BaSEDB (Algorithm 1) looks quite\nsimilar to the Adaptively Binned Successive Elimination (ABSE) proposed in [39].\nHowever, we would\nlike to emphasize several fundamental differences. First, the motivations for the algorithms are completely\ndifferent. [39] designs ABSE to adapt to the unknown margin condition α, while our focus is to tackle the\nbatch constraint. In fact, without the batch constraints, if α is known, adaptive binning is not needed to\nachieve the optimal regret [39]. This is certainly not the case in the batched setting. Fixing the number of\nbins used across different batches is suboptimal because one can construct instances that cause the regret\nincurred during a certain batch to explode. We will expand on this phenomenon in Section 4.3. Secondly,\nthe algorithm in [39] partitions a bin into a fixed number 2d of smaller ones once the original bin is unable to\ndistinguish the remaining arms. In this way, the algorithm can adapt to the difference in the local difficulty\nof the problem. In comparison, one of our main contributions is to carefully design the list of varying split\nfactors that allows the new cubes to maximally utilize the number of samples allocated to it during the next\nbatch.\n4.1\nRegret guarantees\nNow we are ready to present the regret performance of BaSEDB (Algorithm 1).\nTheorem 2. Suppose that αβ ≤ 1. Fix any constant D1 > 0 and suppose that M ≤ D1 log T. Equipped\nwith the grid and split factors list that satisfy (11) and (10), the policy ˆπ given by Algorithm 1 obeys\nE[RT (ˆπ)] ≤ ˜C(log T)2 · T\n1−γ\n1−γM ,\nwhere ˜C > 0 is a constant independent of T and M.\nSee Section 5 for the proof.\nWhile Theorem 2 requires M ≲ log T, we see from the corollary below that it is in fact sufficient to show\nthe optimality of Algorithm 1.\nCorollary 1. As long as M ≥ D2 log log(T), where D2 depends on γ = β(1+α)\n2β+d , Algorithm 1 achieves\nE[RT (ˆπ)] ≤ ˜C(log T)2 · T 1−γ,\nwhere ˜C > 0 is a constant independent of T and M.\nTheorem 2, together with Corollary 1 and Theorem 1 establish the fundamental limits of batch learning\nfor the nonparametric bandits with covariates, as well as the optimality of BaSEDB, up to logarithmic factors.\nTo see this, when M ≲ log log(T), the upper bound in Theorem 2 matches the lower bound in Theorem 1,\napart from log factors. On the other end, when M ≳ log log(T), Algorithm 1, while splitting the horizon\ninto M batches, achieves the optimal regret (up to log factors) for the setting without the batch constraint\n[39]. It is evident that Algorithm 1 is optimal in this case.\n4.2\nNumerical experiments\nIn this section, we provide some experiments on the empirical performance of Algorithm 1. We set T =\n50000, d = β = 1, α = 0.2. We let PX be the uniform distribution on [0, 1]. Denote qj = (j − 1/2)/4 and\n11\nFigure 2: Regret vs. batch budget M.\nCj = [qj − 1/8, qj + 1/8] for 1 ≤ j ≤ 4. For the mean reward functions, we choose f (1), f (−1) : [0, 1] → R\nsuch that\nf (1)(x) = 1\n2 +\n4\nX\nj=1\nωjφj(x),\nf (−1)(x) = 1\n2,\nwhere ω′\njs are sampled i.i.d. from Rad( 1\n2), φj(x) = 1\n4ϕ(8(x − qj))1{x ∈ Cj} and ϕ(x) = (1 − |x|)1{|x| ≤ 1}.\nWe let Y (k) ∼ Bernoulli(f (k)(x)). To illustrate the performance of Algorithm 1, we compare it with the\nBinned Successive Elimination (BSE) policy from [39], which is shown to be minimax optimal in the fully\nonline case. Figure 2 shows the regret of Algorithm 1 under different batch budegts. One can see that it is\nsufficient to have M = 5 batches to achieve the fully online efficiency.\n4.3\nFailure of static binning\nWe have seen the power of dynamic binning in solving batched nonparametric bandits by establishing its rate-\noptimality in minimizing regret. Now we turn to a complimentary but intriguing question: is it necessary\nto use dynamic binning to achieve optimal regret under the batch constraint? To formally address this\nquestion, we investigate the performance of successive elimination with static binning, i.e., Algorithm 1 with\ng0 = g, and g1 = g2 = · · · gM−2 = 1. Although static binning works when M is large (e.g., a single choice of\ng attains the optimal regret [48, 39] in the fully online setting), we show that it must fail when M is small.\nTo bring the failure mode of static binning into focus, we consider the simplest scenario when M = 3,\nand α = β = d = 1. Note that the successive elimination with static binning algorithm is parameterized by\nthe grid choice Γ = {t0 = 0, t1, t2, t3 = T} and the fixed number g of bins. The following theorem formalizes\nthe failure of static binning in achieving optimal regret when M = 3.\nTheorem 3. Consider M = 3, and α = β = d = 1. For any choice of 1 ≤ t1 < t2 ≤ T − 1, and any choice\nof g, there exists a nonparametric bandit instance in F(1, 1) such that the resulting successive elimination\nwith static binning algorithm ˆπstatic satisfies\nE[RT (ˆπstatic)] ≥ ˜C1T\n9\n19 +κ,\nfor some κ, ˜C1 > 0 that are independent of T. Here T\n9\n19 is the optimal regret achieved by BaSEDB—an\nsuccessive elimination algorithm with dynamic binning.\nWhile the formal proof is deferred to Section 6, we would like to immediately point out the intuition under-\nlying the failure of static binning.\nNecessary choice of grid Γ.\nIt is evident from the proof of the minimax lower bound (Theorem 1)\nthat one needs to set t1 ≍ T 9/19, and t2 ≍ T 15/19. Otherwise, the inequality (8) guarantees the worst-case\nregret of ˆπstatic exceeds the optimal one T\n9\n19 . Consequently, we can focus on the algorithm with t1 ≍ T 9/19,\nt2 ≍ T 15/19, and only consider the design choice g.\n12\nx\n1/g\n1/z\nδ/2\nFigure 3: Instance with g > z. Each bin B produced by ˆπstatic has width 1/g.\nx\n1/z\n1/g\nδ/2\nFigure 4: Instance with g < z. Each bin B produced by ˆπstatic has width 1/g.\nWhy fixed g fails.\nAs a baseline for comparison, recall that in the optimal algorithm with dynamic\nbinning, we set g0 ≍ T 3/19, and g0g1 ≍ T 5/19 so that the worst case regret in three batches are all on the\norder of T\n9\n19 . In view of this, we split the choice of g into three cases.\n• Suppose that g ≫ T 3/19. In this case, we can construct an instance such that the reward difference\nonly appears on an interval with length 1/z ≫ 1/g; see Figure 3. In other words, the static binning\nis finer than that in the reward instance. As a result, the number of pulls in the smaller bin (used\nby the algorithm) in the first batch is not sufficient to tell the two arms apart, that is with constant\nprobability, arm elimination will not happen after the first batch. This necessarily yields the blowup\nof the regret in the second batch.\n• Suppose that g ≪ T 3/19. In this case, we can construct an instance such that the reward difference\nonly appears on an interval with length 1/z ≪ 1/g; see Figure 4. In other words, the static binning\nis coarser than that in the reward instance. Since the aggregated reward difference on the larger bin\nis so small, the number of pulls in the larger bin (used by the algorithm) in the first batch is still not\nsufficient to result in successful arm elimination. Again, the regret on the second batch blows up.\n• Suppose that g ≍ T 3/19. Since this choices matches g0 used in the optimal dynamic binning algorithm,\nthere is no reward instance that can blow up the regret in the first two batches. Nevertheless, since\ng ≪ g0g1 ≍ T 5/19, one can construct the instance similar to the previous case (i.e., Figure 4) such that\nthe regret on the third batch blows up.\n5\nRegret analysis for BaSEDB\nOur proof of the regret upper bound is inspired by the framework developed in [39]. Our setting presents\nadditional technical difficulty due to the batch constraint.\nWe begin with introducing some useful notations. Recall the tree growing process described in section 4,\nwhere we have defined a tree T of depth M. The root (depth 0) of the tree is the whole space X. In depth\n1, X has gd\n0 children, each of which is a bin of width 1/g0. For each bin in depth 1, it has gd\n1 children, each\nof which is a bin of width 1/(g0g1). These children form the depth 2 nodes of the tree T . We form the tree\nrecursively until depth M.\n13\nFor a bin C ∈ T , we define its parent by p(C) = {C′ ∈ T : C ∈ child(C′)}. Moreover, we let p1(C) = p(C)\nand define pk(C) = p(pk−1(C)) for k ≥ 2 recursively.\nIn all, we denote by P(C) = {C′ ∈ T : C′ =\npk(C) for some k ≥ 1} all the ancestors of the bin C.\nWe also define Lt to be the set of active bins at time t, with the dummy case L0 = {X}. Clearly, for\n1 ≤ t ≤ t1, one has L1 = B1, where B1 are all the bins in the first layer.\n5.1\nTwo clean events\nThe regret analysis relies on two clean events. First, fix a batch i ≥ 1, and recall Lti−1+1 is the set of active\nbins at time ti−1 + 1. We denote the random number of pulls for a bin C ∈ Lti−1+1 within batch i to be\nmC,i :=\nti\nX\nt=ti−1+1\n1{Xt ∈ C}.\nClearly, it has expectation\nm⋆\nC,i = E[mC,i] = (ti − ti−1)PX(X ∈ C).\nThe first clean event claims that mC,i concentrates well around its expectation m⋆\nC,i uniformly over all C ∈ T .\nWe denote this event by E.\nLemma 2. Suppose that M ≤ D1 log(T) for some constant D1 > 0. With probability at least 1 − 1/T, for\nall 1 ≤ i ≤ M, and C ∈ Lti−1+1, we have\n1\n2m⋆\nC,i ≤ mC,i ≤ 3\n2m⋆\nC,i.\nSee Section 5.5.1 for the proof.\nSince M ≤ D1 log(T) by assumption, we can apply Lemma 2 to obtain\nE[RT (ˆπ)1(Ec)] ≤ TP(Ec) = 1.\nTherefore, in the remaining proof, we condition on E and focus on bounding E[RT (ˆπ)1(E)].\nThe second clean event is on the elimination process. Since we use successive elimination in each bin, it is\nnatural to expect that the optimal arm in each bin is not eliminated during the process. To mathematically\nspecify this event, we need a few notations.\nFor each bin C ∈ Li, let I′\nC be the set of remaining arms at the end of batch i, i.e., after Algorithm 2 is\ninvoked. Define\n¯\nIC =\n\u001a\nk ∈ {1, −1} : sup\nx∈C\nf ⋆(x) − f (k)(x) ≤ c1|C|β\n\u001b\n,\nIC =\n\u001a\nk ∈ {1, −1} : sup\nx∈C\nf ⋆(x) − f (k)(x) ≤ c0|C|β\n\u001b\n,\nwhere c0 = 2Ldβ/2 + 1 and c1 = 8c0. Clearly, we have\nIC ⊆ ¯\nIC.\nDefine a good event AC = {IC ⊆ I′\nC ⊆ ¯\nIC}, which is the event that the remaining arms in C have gaps of\ncorrect order. In addition, define GC = ∩C′∈P(C)AC′. Recall Bi is the set of bins C with |C| = (Qi−1\nl=0 gl)−1 =\nwi for i ≥ 1.\nLemma 3. For any 1 ≤ i ≤ M − 1 and C ∈ Bi, we have\nP(E ∩ GC ∩ Ac\nC) ≤\n4m⋆\nC,i\nT|C|d .\nIn words, Lemma 3 guarantees that AC happens with high probability if E holds and AC′ holds for all the\nancestors C’ of C. See Section 5.5.2 for the proof.\n14\n5.2\nRegret decomposition\nIn this section, we decompose the regret into three terms. First, for a bin C, we define\nrlive\nT\n(C) :=\nT\nX\nt=1\n\u0010\nf ⋆(Xt) − f (πt(Xt))(Xt)\n\u0011\n1(Xt ∈ C)1(C ∈ Lt).\nIn addition, define Jt := ∪s≤tLs to be the set of bins that have been live up until time t. Correspondingly\nwe define\nrborn\nT\n(C) :=\nT\nX\nt=1\n\u0010\nf ⋆(Xt) − f (πt(Xt))(Xt)\n\u0011\n1(Xt ∈ C)1(C ∈ Jt).\nIt is clear from the definition that for any C ∈ T , one has\nrborn\nT\n(C) = rlive\nT\n(C) +\nX\nC′∈child(C)\nrborn\nT\n(C′)\n= rborn\nT\n(C)1(Ac\nC) + rlive\nT\n(C)1(AC) +\nX\nC′∈child(C)\nrborn\nT\n(C′)1(AC).\nApplying this relation recursively leads to the following regret decomposition:\nRT (π) = rborn\nT\n(X)\n= rlive\nT\n(X)\n| {z }\n=0\n+\nX\nC′∈child(X)\nrborn\nT\n(C′)\n=\nX\n1≤i<M\n\n\n\n\n\n\nX\nC∈Bi\nrborn\nT\n(C)1(GC ∩ Ac\nC)\n|\n{z\n}\n=:Ui\n+\nX\nC∈Bi\nrlive\nT\n(C)1(GC ∩ AC)\n|\n{z\n}\n=:Vi\n\n\n\n\n\n\n+\nX\nC∈BM\nrlive\nT\n(C)1(GC),\nwhere the second equality arises from the fact that rlive\nT\n(X) = 0. Indeed, X /∈ Lt for any 1 ≤ t ≤ T.\n5.3\nControlling three terms\nIn what follows, we control Vi, Ui and the last batch separately.\n5.3.1\nControlling Vi\nFix some 1 ≤ i ≤ M − 1, and some bin C ∈ Bi. On the event GC we have I\n′\np(C) ⊆ ¯Ip(C), that is, for any\nk ∈ I\n′\np(C),\nsup\nx∈p(C)\nf ⋆(x) − f (k)(x) ≤ c1|p(C)|β.\nThis implies that for any x ∈ C, and k ∈ I\n′\np(C),\n\u0010\nf ⋆(x) − f (k)(x)\n\u0011\n1{GC} ≤ c1|p(C)|β1(0 <\n\f\f\ff (1)(x) − f (−1)(x)\n\f\f\f ≤ c1|p(C)|β).\n(12)\nAs a result, we obtain\nE[rlive\nT\n(C)1(GC ∩ AC)] = E\n\" T\nX\nt=1\n\u0010\nf ⋆(Xt) − f (πt(Xt))(Xt)\n\u0011\n1(Xt ∈ C)1(C ∈ Lt)1(GC ∩ AC)\n#\n15\n(i)\n≤ E\n\" T\nX\nt=1\nc1|p(C)|β1(0 <\n\f\f\ff (1)(Xt) − f (−1)(Xt)\n\f\f\f ≤ c1|p(C)|β)1(Xt ∈ C, C ∈ Lt)1(GC ∩ AC)\n#\n(ii)\n≤ c1|p(C)|βE\n\n\nti\nX\nt=ti−1+1\n1(0 <\n\f\f\ff (1)(Xt) − f (−1)(Xt)\n\f\f\f ≤ c1|p(C)|β, Xt ∈ C)1(GC ∩ AC)\n\n\n(iii)\n≤ c1|p(C)|β\nti\nX\nt=ti−1+1\nP(0 <\n\f\f\ff (1)(Xt) − f (−1)(Xt)\n\f\f\f ≤ c1|p(C)|β, Xt ∈ C)\n= c1|p(C)|β(ti − ti−1)P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C).\nHere, step (i) uses relation (12), and the fact that πt(Xt) ∈ I\n′\np(C) when Xt ∈ C. For step (ii), if C is split,\nthen it is no longer live, so the live regret incurred on the remaining batches is zero. On the other hand, if\nC is not split, then |I′\nC| = 1. Without loss of generality, assume that arm −1 is eliminated. Conditioned\non AC, this means −1 /∈ IC and there exists x0 ∈ C such that f (1)(x0) − f (−1)(x0) > c0|C|β. By the\nsmoothness condition, having a gap at least c0|C|β on a single point in C implies f (1)(x) − f (−1)(x) > |C|β\nfor all x ∈ C. Therefore, arm 1 which is the remaining one is the optimal arm for all x ∈ C and would not\nincur any regret further. The third inequality holds since 1(GC ∩ AC) ≤ 1.\nTaking the sum over all bins in Bi and using the fact that |p(C)| = wi−1, we obtain\nX\nC∈Bi\nE[rlive\nT\n(C)1(GC ∩ AC)] ≤\nX\nC∈Bi\nc1wβ\ni−1(ti − ti−1)P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C)\n= c1wβ\ni−1(ti − ti−1)\nX\nC∈Bi\nP(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1, X ∈ C).\n(13)\nNote that\nX\nC∈Bi\nP(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1, X ∈ C) = P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1)\n≤ D0 ·\nh\nc1wβ\ni−1\niα\n,\n(14)\nwhere the last inequality follows from the margin condition. Combining relations (14) and (13), we reach\nX\nC∈Bi\nE[rlive\nT\n(C)1(GC ∩ AC)] ≤ (ti − ti−1) · [c1wβ\ni−1]1+α · D0.\n5.3.2\nControlling Ui\nFix some 1 ≤ i ≤ M − 1, and some bin C ∈ Bi. Again, using the definition of GC, we obtain\nE[rborn\nT\n(C)1(GC ∩ Ac\nC)] = E\n\" T\nX\nt=1\n\u0010\nf ⋆(Xt) − f (πt(Xt))(Xt)\n\u0011\n1(Xt ∈ C)1(C ∈ Jt)1(GC ∩ Ac\nC)\n#\n≤ E\n\" T\nX\nt=1\nc1|p(C)|β1(0 <\n\f\f\ff (1)(Xt) − f (−1)(Xt)\n\f\f\f ≤ c1|p(C)|β)1(Xt ∈ C, C ∈ Jt)1(GC ∩ Ac\nC)\n#\n≤ c1|p(C)|βTP(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C)P(GC ∩ Ac\nC).\nApply Lemma 3 to see that\nE[rborn\nT\n(C)1(GC ∩ Ac\nC)] ≤ c1|p(C)|βTP(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C)\n4m⋆\nC,i\nT|C|d\n16\n= c1wβ\ni−1P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1, X ∈ C)4(ti − ti−1)PX(X ∈ C)\n|C|d\n≤ 4¯cc1wβ\ni−1P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1, X ∈ C)(ti − ti−1),\nwhere we use the fact that PX(X ∈ C) ≤ ¯c|C|d in the second inequality. Summing over all bins in Bi, we\nobtain\nX\nC∈Bi\nE[rborn\nT\n(C)1(GC ∩ Ac\nC)] ≤ 4¯cc1wβ\ni−1(ti − ti−1)\nX\nC∈Bi\nP(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1wβ\ni−1, X ∈ C)\n≤ 4¯cc1wβ\ni−1(ti − ti−1)D0 ·\nh\nc1wβ\ni−1\niα\n= 4D0¯c(ti − ti−1)[c1wβ\ni−1]1+α,\nwhere the second inequality reuses the bound in (14).\n5.3.3\nLast Batch\nFor C ∈ BM, one can similarly obtain\nE[rlive\nT\n(C)1(GC)] ≤ c1|p(C)|β(T − tM−1)P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C).\nConsequently, summing over C ∈ BM yields\nX\nC∈BM\nE[rlive\nT\n(C)1(GC)] ≤\nX\nC∈BM\nc1|p(C)|β(T − tM−1)P(0 <\n\f\f\ff (1)(X) − f (−1)(X)\n\f\f\f ≤ c1|p(C)|β, X ∈ C)\n≤ c1wβ\nM−1(T − tM−1)D0 ·\nh\nc1wβ\nM−1\niα\n= D0(T − tM−1)[c1wβ\nM−1]1+α.\n5.4\nPutting things together\nIn sum, the total regret is bounded by\nE[RT (π)] ≤ c\n \nt1 +\nM−1\nX\ni=2\n(ti − ti−1) · wβ+αβ\ni−1\n+ (T − tM−1)wβ+αβ\nM−1\n!\n,\nwhere c is a constant that depends on (α, β, D, L). Recall that wi = (Qi−1\nl=0 gl)−1, and the choices for the\nbatch size and the split factors (11)-(10). We then obtain\nt1 ≲ T\n1−γ\n1−γM log T,\n(ti − ti−1) · wβ+αβ\ni−1\n≲ T\n1−γ\n1−γM log T,\nfor 2 ≤ i ≤ M − 1,\n(T − tM−1)wβ+αβ\nM−1 ≤ Twβ+αβ\nM−1 ≲ T\n1−γ\n1−γM log T.\nThe proof is finished by combining the above three bounds.\n5.5\nProofs for the clean events\nWe are left with proving that the two clean events happen with high probability.\n17\n5.5.1\nProof of Lemma 2\nFix the batch index i, and a node C in layer-i of the tree T . By relation (11), we have\nm⋆\nC,i = (ti − ti−1)PX(X ∈ C)\n≍ |C|−(2β+d) log(T|C|d)PX(X ∈ C)\n? |C|−2β ≥ g2β\n0\n≍ (T\n1−γ\n1−γM ·\n2β\n2β+d ),\nwhere the last step uses the fact that PX(X ∈ C) ≥ c|C|d. Therefore, m⋆\nC,i ≥ 3\n4 log(2T 2) for all i and C, as\nlong as T is sufficiently large. This allows us to invoke Chernoff’s bound to obtain that with probability at\nmost 1/T 2\n\f\f\f\f\nXti\nt=ti−1+1 1{Xt ∈ C} − m⋆\nC,i\n\f\f\f\f ≥\nq\n3 log(2T 2)m⋆\nC,i.\nDenote Ec = {∃1 ≤ i ≤ M, C ∈ Lti−1+1 such that | Pti\nt=ti−1+1 1{Xt ∈ C} − m⋆\nC,i |≥\nq\n3 log(2T 2)m⋆\nC,i}.\nApplying union bound to reach\nP(Ec) ≤\nX\nC∈T\n1\nT 2\n(i)\n≤ 1\nT 2\n M\nX\ni=1\n(\ni−1\nY\nl=0\ngl)d\n!\n(ii)\n≤\n1\nT 2 · M · (\nM−1\nY\nl=0\ngl)d,\nwhere step (i) sums over all possible nodes of T across batches, and step (ii) is due to (Qi−1\nl=0 gl)d ≤ (QM−1\nl=0\ngl)d\nfor any 1 ≤ i ≤ M. Since gM−1 = 1, we further obtain\nP(Ec) ≤ 1\nT 2 · M · (\nM−2\nY\nl=0\ngl)d (iii)\n≤\n1\nT 2 · M · t\nd\n2β+d\nM−1\n(iv)\n≤ D1\n1\nT 2 · log T · T\nd\n2β+d ≤ 1\nT ,\nwhere step (iii) invokes relation (11), and step (iv) uses the assumption M ≤ D1 log T. This completes the\nproof.\n5.5.2\nProof of Lemma 3\nTo simplify notation, for any event F, we define PGC(F) = P(E ∩ GC ∩ F).\nLet D1\nC be the event that an arm k ∈ IC is eliminated at the end of batch i, and D2\nC be the event that\nan arm k /∈ ¯\nIC is not eliminated at the end of batch i. Consequently, we have\nPGC(Ac\nC) = PGC(D1\nC) + PGC((D1\nC)c ∩ D2\nC).\nRecall U(τ, T, C) = 4\nq\nlog(2T |C|d)\nτ\n. By relation (11), we can write\nm⋆\nC,i = (ti − ti−1)PX(X ∈ C)\n= li|C|−(2β+d) log(T|C|d)PX(X ∈ C),\nwhere li > 0 is a constant chosen such that U(2m⋆\nC,i, T, C) = 2c0|C|β. Under E, we have U(mC,i, T, C) ≤\n4c0|C|β because mC,i ≥ 1\n2m⋆\nC,i.\n1. Upper bounding PGC(D1\nC): when D1\nC occurs, an arm k ∈ IC is eliminated by some k′ ∈ I′\np(C) at the\nend of batch i. This means ¯Y (k′)\nC,i − ¯Y (k)\nC,i > U(mC,i, T, C). Meanwhile,\n¯f (k′)\nC\n− ¯f (k)\nC\n≤ ¯f ⋆\nC − ¯f (k)\nC\n(i)\n≤ c0|C|β ≤ 1\n2U(2m⋆\nC,i, T, C),\nwhere step (i) uses the definition of IC. Consequently, | ¯Y (k′)\nC,i − ¯f (k′)\nC\n| ≤ U(mC,i, T, C)/4 and | ¯Y (k)\nC,i −\n¯f (k)\nC | ≤ U(mC,i, T, C)/4 cannot hold simultaneously. Otherwise, this would contradict with ¯Y (k′)\nC,i −\n¯Y (k)\nC,i > U(mC,i, T, C) because mC,i ≤ 2m⋆\nC,i under E. Therefore,\nPGC(D1\nC) ≤ P\n\u001a\n∃k ∈ I′\np(C), mC,i ≤ 2m⋆\nC,i : | ¯Y (k)\nC,i − ¯f (k)\nC | ≥ 1\n4U(mC,i, T, C)\n\u001b\n.\n18\n2. Upper bounding PGC((D1\nC)c ∩ D2\nC): when (D1\nC)c ∩ D2\nC happens, no arm in IC is eliminated while some\nk /∈ ¯\nIC remains in the active arm set. By definition, there exists x(k) such that f ⋆(x(k)) − f (k)(x(k)) > 8c0|C|β.\nLet η(k) be any arm that satisfies f ⋆(x(k)) = f (η(k))(x(k)), and one can easily verify η(k) ∈ IC. Since\nk is not eliminated, we have ¯Y (η(k))\nC,i\n− ¯Y (k)\nC,i ≤ U(mC,i, T, C). On the other hand,\n¯f (η(k))\nC\n(iii)\n≥ f (η(k))(x(k)) − c0|C|β\n≥ f (k)(x(k)) + 8c0|C|β − c0|C|β\n= f (k)(x(k)) + 7c0|C|β\n(iv)\n≥ ¯f (k)\nC\n+ 6c0|C|β ≥ ¯f (k)\nC\n+ 3\n2U(mC,i, T, C),\n(15)\nwhere steps (iii) and (iv) use Lemma 5. Inequality (15) together with the fact that ¯Y (η(k))\nC,i\n− ¯Y (k)\nC,i ≤\nU(mC,i, T, C) imply | ¯Y (k0)\nC,i\n− ¯f (k0)\nC\n| ≥ U(mC,i, T, C)/4 for either k0 = k or k0 = η(k). Consequently,\nPGC((D1\nC)c ∩ D2\nC) ≤ P\n\u001a\n∃k ∈ I′\np(C), mC,i ≤ 2m⋆\nC,i : | ¯Y (k)\nC,i − ¯f (k)\nC | ≥ 1\n4U(mC,i, T, C)\n\u001b\n.\nCombining the two parts we obtain\nPGC(Ac\nC) = PGC(D1\nC) + PGC((D1\nC)c ∩ D2\nC)\n≤ 2 · P\n\u001a\n∃k ∈ I′\np(C), mC,i ≤ 2m⋆\nC,i : | ¯Y (k)\nC,i − ¯f (k)\nC | ≥ 1\n4U(mC,i, T, C)\n\u001b\n≤\n4m⋆\nC,i\nT|C|d ,\nwhere the last inequality applies Lemma 4.\n5.5.3\nAuxiliary lemmas\nLemma 4. For any 1 ≤ i ≤ M − 1 and C ∈ Bi, one has\nP\n\u001a\n∃k ∈ I′\np(C), mC,i ≤ 2m⋆\nC,i : | ¯Y (k)\nC,i − ¯f (k)\nC | ≥ 1\n4U(mC,i, T, C)\n\u001b\n≤\n2m⋆\nC,i\nT|C|d .\nProof. Recall in Algorithm 1 we pull each arm in a round-robin fashion within a bin during batch i. Fix\nτ > 0. Let ¯Y (k)\nτ\n= Pτ\nj=1 Y (k)\nj\n/τ where Y (k)\nj\n’s are i.i.d. random variables with Y (k)\nj\n∈ [0, 1] and E[Y (k)\nj\n] = ¯f (k)\nC .\nBy Hoeffding’s inequality, with probability 1/(T|C|d), we have\n| ¯Y (k)\nτ\n− ¯f (k)\nC | ≥\nr\nlog(2T|C|d)\n2τ\n.\nApplying union bound to get\nP\n(\n∃k ∈ Ip(C), 0 ≤ τ ≤ m⋆\nC,i : | ¯Y (k)\nτ\n− ¯f (k)\nC | ≥\nr\nlog(2T|C|d)\n2τ\n)\n≤\n2m⋆\nC,i\nT|C|d ,\nwhich completes the proof.\nLemma 5. Fix k ∈ {1, −1} and C ∈ T , for any x ∈ C, one has\n| ¯f (k)\nC\n− f (k)(x)| ≤ c0|C|β,\nwhere c0 = 2Ldβ/2 + 1.\n19\nProof. For notation simplicity, we write f for f (k) in the following proof. By definition,\n| ¯fC − f(x)| = |\n1\nP(C)\nZ\nC\n(f(y) − f(x))dP(y)|\n≤\n1\nP(C)\nZ\nC\n|f(y) − f(x)|dP(y)\n≤\n1\nP(C)\nZ\nC\nL∥x − y∥β\n2dP(y),\nwhere the first inequality uses the triangle inequality, and the second inequality is due to the smoothness\ncondition. Since x ∈ C, we further have\n| ¯fC − f(x)| ≤\n1\nP(C)\nZ\nC\nL∥x − y∥β\n2dP(y)\n≤\n1\nP(C)\nZ\nC\nLdβ/2|C|βdP(y)\n≤ c0|C|β.\nThis completes the proof.\n6\nProof of suboptimality of static binning\nAs we argued after the statement of Theorem 3, one needs to set t1 ≍ T 9/19, and t2 ≍ T 15/19. Therefore,\nthroughout the proof, we assume this is true and only focus on the number g of bins.\nTo construct a hard instance, we partition [0, 1] into z bins with equal width. Denote the bins by Cj for\nj = 1, ..., z, and let qj be the center of Cj. Define a function ϕ : [0, 1] 7→ R as ϕ(x) = (1 − |x|)1{|x| ≤ 1}.\nCorrespondingly define a function φj : [0, 1] 7→ R as φj(x) = Dϕz−1ϕ(2z(x − qj))1{x ∈ Cj}, where Dϕ =\nmin(2−1L, 1/4). Define a function f : [0, 1] 7→ R:\nf(x) = 1\n2 + φ1(x).\nThe problem instance of interest is v = (f (1)(x) = f(x), f (−1)(x) = 1\n2). It is easy to verify v ∈ F(1, 1).\nThroughout the proof, we condition on the event E specified by Lemma 2, which says the number of samples\nallocated to a bin concentrates well around its expectation. We will show even under this good event, there\nexists a choice of z that makes successive elimination fail to remove the suboptimal arms at the end of a\nbatch with constant probability.\n6.1\nA helper lemma\nWe begin with presenting a helper lemma that will be used extensively in the later part of the proof. The\nclaim is intuitive: if the sample size is small, it is not sufficient to tell apart two Bernoulli distributions with\nsimilar means. Then, in our context, arm elimination will not occur.\nLemma 6. Assume mB,i ≤ 2m⋆\nB,i. For any B ⊆ [0, 1] and i ∈ {1, 2}. If ¯f (1)\nB\n− ¯f (−1)\nB\n≤ δ ≤ 1/pm⋆\nB,i for\nsome δ > 0 , then\nP\n\u0010\n¯Y (1)\nB,i − ¯Y (−1)\nB,i\n> U(mB,i, T, B)\n\u0011\n≤ ti\nT .\nProof. Fix 0 < τ ≤ m⋆\nB,i. Let ¯Y (k)\nτ\n= Pτ\nl=1 Y (k)\nl\n/τ where Y (k)\nl\n’s are i.i.d. random variables with Y (k)\nl\n∈ [0, 1]\nand E[Y (k)\nl\n] = ¯f (k)\nB\nfor k ∈ {1, −1}. Recall U(τ, T, B) = 4\nq\nlog(2T |B|)\nτ\n2. Then,\nP\n\u0010\n¯Y (1)\nτ\n− ¯Y (−1)\nτ\n> U(2τ, T, B)\n\u0011 (i)\n≤ P\n \n¯Y (1)\nτ\n− ¯Y (−1)\nτ\n> δ +\nr\nlog(2T/g)\n2τ\n!\n2We remark the constant 4 is not essential for the proof to work. For any c > 0 , c log(2T|B|) = log((2T|B|)c) so the final\nsuccess probability is still tiny as long as T is sufficiently large.\n20\n(ii)\n≤ P\n \n¯Y (1)\nτ\n− ¯Y (−1)\nτ\n> ¯f (1)\nB − ¯f (−1)\nB\n+\nr\nlog(2T/g)\n2τ\n!\n(iii)\n≤ g\nT ,\nwhere step (i) is because δ ≤ 1/pm⋆\nB,i ≤ 1/√τ, step (ii) is due to ¯f (1)\nB\n− ¯f (−1)\nB\n≤ δ, and step (iii) uses\nHoeffding’s inequality. Applying union bound to get\nP\n\u0010\n∃0 < τ ≤ m⋆\nB,i : ¯Y (1)\nτ\n− ¯Y (−1)\nτ\n> U(2τ, T, B)\n\u0011\n≤\nm⋆\nB,ig\nT\n≤ ti\nT .\nThis finishes the proof.\n6.2\nThree failure cases for g\nFix some small constant ε > 0 to be specified later. From now on, we use ˆπ to denote ˆπstatic for simplicity.\nWe split the proof into three cases: (1) g ≥ T 3/19+ε; (2) g ≤ T 3/19−ε; (3) and g ∈ (T 3/19−ε, T 3/19+ε).\nCase 1: g ≥ T 3/19+ε.\nSet z = T 3/19−ε/2. Assume without loss of generality that g = H ·z for some H ≥ 4;\nsee Figure 3 for an illustration of the instance. Suppose C1 = ∪H\nl=1Bl, where Bl’s are the bins produced by\nˆπ that lie in C1. It is clear that\nE[RT (ˆπ)]\n(i)\n≥ E\n\"\nt2\nX\nt=t1+1\n\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011#\n(ii)\n= E\n\"\nt2\nX\nt=t1+1\n\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011\n1{Xt ∈ C1}\n#\n(iii)\n≥\nt2\nX\nt=t1+1\n3H/4\nX\nl=H/4\nE\nh\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011\n1{Xt ∈ Bl}\ni\n,\n(16)\nwhere step (i) is because the total regret is greater than the regret incurred during the second batch, step\n(ii) uses the fact that under the instance v, the mean rewards of the two arms differ only in C1, and step\n(iii) arises since C1 = ∪H\nl=1Bl. Now we turn to lower bounding E\n\u0002\u0000f ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0001\n1{Xt ∈ Bl}\n\u0003\nfor\neach H/4 ≤ l ≤ 3H/4.\nConsider any such Bl. We drop the subscripts and write B instead for simplicity. By the design of v,\nwe have ¯f (1)\nB\n− ¯f (−1)\nB\n≤ Dϕz−1 = δ, which obeys Dϕz−1 ≤ 1/pm⋆\nB,1—a consequence of the choice of z.\nAdditionally, we have mB,1 ≤ 2m⋆\nB,1 under E. Therefore, we can invoke Lemma 6 to obtain\nP\n\u0010\n¯Y (1)\nB,1 − ¯Y (−1)\nB,1\n> U(mB,1, T, B)\n\u0011\n≤ t1\nT ≤ 1\n2.\nIn words, with probability exceeding 1/2, no elimination will happen for the bin B. As a result, we obtain\nE[RT (ˆπ)] ≥\nt2\nX\nt=t1+1\n3H/4\nX\nl=H/4\nE\nh\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011\n1{Xt ∈ Bl}\ni\n≳ H · t2\ng · z−1 ≍ t2\nz2 = T\n9\n19 +ϵ,\nwhere we have used the choice of z. So Theorem (3) holds with κ = ϵ.\n21\nCase 2: g ≤ T 3/19−ε.\nSet z = T 3/19−ε/8. We have g < z and there exists H > 1 such that z = H · g;\nsee Figure 4 for an illustration of the instance. Let B be the bin produced by ˆπ such that C1 ⊂ B. By the\ndesign of v, we have\n¯f (1)\nB − ¯f (−1)\nB\n≤ 1\nH (1/2 + Dϕz−1) + (1 − 1\nH )1\n2 − 1\n2 = Dϕz−1\nH\n.\nLet δ = Dϕz−1\nH\n, we have δ ≤ 1/pm⋆\nB,1 due to our choice of z. Additionally, we have mB,1 ≤ 2m⋆\nB,1 under\nE. Therefore, we can invoke Lemma 6 to obtain\nP\n\u0010\n¯Y (1)\nB,1 − ¯Y (−1)\nB,1\n> U(mB,1, T, B)\n\u0011\n≤ t1\nT ≤ 1\n2.\nThus, with probability exceeding 1/2, the suboptimal arm is not eliminated in B. Similar to the previous\ncase, we obtain\nE[RT (ˆπ)] ≥ E\n\"\nt2\nX\nt=t1+1\n\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011#\n= E\n\"\nt2\nX\nt=t1+1\n\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011\n1{Xt ∈ C1}\n#\n? t2\nz2 = T\n9\n19 + ϵ\n4 .\nSo Theorem (3) holds with κ = ϵ/4.\nCase 3: g ∈ (T 3/19−ε, T 3/19+ε).\nSet z ≍ T 1/4. We then have g < z, as long as ε ≤ 1/19. And there exists\nH > 1 such that z = H · g; see Figure 4 for an illustration of the instance. Let B be the bin produced by ˆπ\nsuch that C1 ⊂ B. By the design of v, we have\n¯f (1)\nB − ¯f (−1)\nB\n≤ 1\nH (1/2 + Dϕz−1) + (1 − 1\nH )1\n2 − 1\n2 = Dϕz−1\nH\n.\nLet δ = Dϕz−1\nH\n, we have δ ≤ 1/pm⋆\nB,1 due to our choice of z. Additionally, we have mB,1 ≤ 2m⋆\nB,1 under\nE. Therefore, we can invoke Lemma 6 to obtain\nP\n\u0010\n¯Y (1)\nB,1 − ¯Y (−1)\nB,1\n> U(mB,1, T, B)\n\u0011\n≤ t1\nT ≤ 1\n4.\nThis means with probability at least 3/4, arm elimination does not occur in B after the first batch. Moreover,\nsince δ ≤ 1/pm⋆\nB,2 by the choice of z, and mB,2 ≤ 2m⋆\nB,2 under E, we can apply Lemma 6 again to get\nP\n\u0010\n¯Y (1)\nB,2 − ¯Y (−1)\nB,2\n> U(mB,2, T, B)\n\u0011\n≤ t2\nT ≤ 1\n4.\nIn all, with probability at least 1/2, arm elimination does not occur in B after the second batch. Similar to\nbefore, we reach the conclusion that\nE[RT (ˆπ)] ≥ E\n\"\nT\nX\nt=t2+1\n\u0010\nf ⋆(Xt) − f ˆπt(Xt)(Xt)\n\u0011#\n= E\n\"\nT\nX\nt=t2+1\n(f ⋆(Xt) − f ˆπt(Xt)(Xt))1{Xt ∈ C1}\n#\n≳ T\nz2 = T\n1\n2 .\nWe see that Theorem (3) holds with κ = 1/38.\n22\n7\nDiscussion\nIn this paper, we characterize the fundamental limits of batch learning in nonparametric contextual bandits.\nIn particular, our optimal batch learning algorithm (i.e., Algorithm 1) is able to match the optimal regret\nin the fully online setting with only O(log log T) policy updates. Our work open a few interesting avenues\nto explore in the future.\nExtensions to multiple arms.\nWith slight modification, our algorithm works for nonparametric contex-\ntual bandits with more than two arms. However, it remains unclear what the fundamental limits of batch\nlearning are in this multi-armed case.\nImproving the log factor.\nComparing the upper and lower bounds, it is evident that Algorithm 1 is\nnear-optimal up to log factors. It is certainly interesting to improve this log factor, either by strengthening\nthe lower bound, or making the upper bound more efficient.\nAdapting to smoothness and margin parameters.\nIn practice, we do not always know the smoothness\nand the margin parameters. Can one develop a batch learning algorithm that can adapt to these unknown\nparameters?\nThis question is intriguing because in the fully online setting, a similar adaptively binned\nsuccessive elimination algorithm was proposed in [39] with the sole purpose of adapting to the margin\nparameter.3\nAcknowledgements\nCM is partially supported by the National Science Foundation via grant DMS-2311127.\nReferences\n[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic\nbandits. Advances in neural information processing systems, 24, 2011.\n[2] Sakshi Arya and Yuhong Yang. Randomized allocation with nonparametric estimation for contextual\nmulti-armed bandits with delayed rewards. Statistics & Probability Letters, 164:108818, 2020.\n[3] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals\nof Statistics, 35(2):608–633, 2007.\n[4] Peter Auer. Using confidence bounds for exploitation-exploration trade-offs. Journal of Machine Learn-\ning Research, 3(Nov):397–422, 2002.\n[5] Yu Bai, Tengyang Xie, Nan Jiang, and Yu-Xiang Wang. Provably efficient q-learning with low switching\ncost. Advances in Neural Information Processing Systems, 32, 2019.\n[6] Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Opera-\ntions Research, 68(1):276–294, 2020.\n[7] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for con-\ntextual bandits. Management Science, 67(3):1329–1349, 2021.\n[8] Dimitris Bertsimas and Adam J Mersereau. A learning approach for interactive marketing to a customer\nsegment. Operations Research, 55(6):1120–1135, 2007.\n[9] Moise Blanchard, Steve Hanneke, and Patrick Jaillet. Non-stationary contextual bandits and universal\nlearning. arXiv preprint arXiv:2302.07186, 2023.\n3We emphasize again that if adaptivity to the margin parameter is not needed, then static binning suffices for the online\nsetting.\n23\n[10] Changxiao Cai, T Tony Cai, and Hongzhe Li. Transfer learning for contextual multi-armed bandits.\narXiv preprint arXiv:2211.12612, 2022.\n[11] T Tony Cai and Hongming Pu. Stochastic continuum-armed bandits with additive models: Minimax\nregrets and adaptive algorithm. The Annals of Statistics, 50(4):2179–2204, 2022.\n[12] Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other\nadaptive adversaries. Advances in Neural Information Processing Systems, 26, 2013.\n[13] Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the 20th ACM\nSIGKDD international conference on Knowledge discovery and data mining, pages 1097–1105, 2014.\n[14] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural\ninformation processing systems, 24, 2011.\n[15] Stephen E Chick and Noah Gans. Economic analysis of simulation selection problems. Management\nScience, 55(3):421–437, 2009.\n[16] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stop-\nping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine\nlearning research, 7(6):1079–1105, 2006.\n[17] Jianqing Fan, Zhaoran Wang, Zhuoran Yang, and Chenlu Ye. Provably efficient high-dimensional bandit\nlearning with batched feedbacks. arXiv preprint arXiv:2311.13180, 2023.\n[18] Yasong Feng, Zengfeng Huang, and Tianyu Wang. Lipschitz bandits with batched feedback. Advances\nin Neural Information Processing Systems, 35:19836–19848, 2022.\n[19] Manegueu Anne Gael, Claire Vernade, Alexandra Carpentier, and Michal Valko. Stochastic bandits with\narm-dependent delays. In International Conference on Machine Learning, pages 3348–3356. PMLR,\n2020.\n[20] Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear markov\ndecision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021.\n[21] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.\nAdvances in Neural Information Processing Systems, 32, 2019.\n[22] Alexander Goldenshluger and Assaf Zeevi. Woodroofe’s one-armed bandit problem revisited. The Annals\nof Applied Probability, 19(4):1603–1633, 2009.\n[23] Alexander Goldenshluger and Assaf Zeevi.\nA linear response bandit problem.\nStochastic Systems,\n3(1):230–261, 2013.\n[24] Melody Guan and Heinrich Jiang. Nonparametric stochastic contextual bandits. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 32, 2018.\n[25] Yonatan Gur, Ahmadreza Momeni, and Stefan Wager. Smoothness-adaptive contextual bandits. Oper-\nations Research, 70(6):3198–3216, 2022.\n[26] Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu Ye. Se-\nquential batch learning in finite-action linear contextual bandits.\narXiv preprint arXiv:2004.06321,\n2020.\n[27] Yichun Hu, Nathan Kallus, and Xiaojie Mao. Smooth contextual bandits: Bridging the parametric and\nnondifferentiable regret regimes. Operations Research, 70(6):3261–3281, 2022.\n[28] Cem Kalkanli and Ayfer Ozgur. Batched thompson sampling. Advances in Neural Information Process-\ning Systems, 34:29984–29994, 2021.\n24\n[29] Amin Karbasi, Vahab Mirrokni, and Mohammad Shadravan. Parallelizing thompson sampling. Advances\nin Neural Information Processing Systems, 34:10535–10548, 2021.\n[30] Edward S Kim, Roy S Herbst, Ignacio I Wistuba, J Jack Lee, George R Blumenschein Jr, Anne Tsao,\nDavid J Stewart, Marshall E Hicks, Jeremy Erasmus Jr, Sanjay Gupta, et al. The battle trial: person-\nalizing therapy for lung cancer. Cancer discovery, 1(1):44–53, 2011.\n[31] Aniket Kittur, Ed H Chi, and Bongwon Suh. Crowdsourcing user studies with mechanical turk. In\nProceedings of the SIGCHI conference on human factors in computing systems, pages 453–456, 2008.\n[32] Anders Bredahl Kock and Martin Thyrsgaard. Optimal sequential treatment allocation. arXiv preprint\narXiv:1705.09952, 2017.\n[33] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual ban-\ndits with continuous actions: Smoothing, zooming, and adapting. The Journal of Machine Learning\nResearch, 21(1):5402–5446, 2020.\n[34] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.\n[35] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to person-\nalized news article recommendation. In Proceedings of the 19th international conference on World wide\nweb, pages 661–670, 2010.\n[36] Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits. In Confer-\nence on Learning Theory, pages 1463–1492. PMLR, 2018.\n[37] Tyler Lu, Dávid Pál, and Martin Pál.\nShowing relevant ads via context multi-armed bandits.\nIn\nProceedings of AISTATS, 2009.\n[38] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics,\n27(6):1808–1829, 1999.\n[39] Vianney Perchet and Philippe Rigollet. The multi-armed bandit problem with covariates. Ann. Statist.,\n41(2):693–721, 2013.\n[40] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems.\nAnn. Statist., 44(2):660–681, 2016.\n[41] Wei Qian, Ching-Kang Ing, and Ji Liu.\nAdaptive algorithm for multi-armed bandit problem with\nhigh-dimensional covariates. Journal of the American Statistical Association, pages 1–13, 2023.\n[42] Wei Qian and Yuhong Yang.\nKernel estimation and model combination in a bandit problem with\ncovariates. Journal of Machine Learning Research, 17(149), 2016.\n[43] Wei Qian and Yuhong Yang. Randomized allocation with arm elimination in a bandit problem with\ncovariates. Electronic Journal of Statistics, 10(1):242–270, 2016.\n[44] Dan Qiao, Ming Yin, Ming Min, and Yu-Xiang Wang. Sample-efficient reinforcement learning with loglog\n(t) switching cost. In International Conference on Machine Learning, pages 18031–18061. PMLR, 2022.\n[45] Henry Reeve, Joe Mellor, and Gavin Brown. The k-nearest neighbour ucb algorithm for multi-armed\nbandits with covariates. In Algorithmic Learning Theory, pages 725–752. PMLR, 2018.\n[46] Zhimei Ren and Zhengyuan Zhou. Dynamic batch learning in high-dimensional sparse linear contextual\nbandits. Management Science, 2023.\n[47] Zhimei Ren, Zhengyuan Zhou, and Jayant R Kalagnanam.\nBatched learning in generalized linear\ncontextual bandits with general decision sets. IEEE Control Systems Letters, 6:37–42, 2020.\n[48] Philippe Rigollet and Assaf Zeevi.\nNonparametric bandits with covariates.\narXiv preprint\narXiv:1003.1630, 2010.\n25\n[49] Herbert E. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American\nMathematical Society, 58:527–535, 1952.\n[50] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising\nusing multi-armed bandit experiments. Marketing Science, 36(4):500–522, 2017.\n[51] Joe Suk and Samory Kpotufe. Tracking most significant shifts in nonparametric contextual bandits.\narXiv preprint arXiv:2307.05341, 2023.\n[52] Joseph Suk and Samory Kpotufe. Self-tuning bandits over unknown covariate-shifts. In Algorithmic\nLearning Theory, pages 1114–1156. PMLR, 2021.\n[53] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health.\nIn Mobile Health, pages 495–517. Springer, 2017.\n[54] Alexander B Tsybakov. Optimal aggregation of classifiers in statistical learning. The Annals of Statistics,\n32(1):135–166, 2004.\n[55] Claire Vernade, Olivier Cappé, and Vianney Perchet. Stochastic bandit models for delayed conversions.\narXiv preprint arXiv:1706.09186, 2017.\n[56] Chi-Hua Wang and Guang Cheng. Online batch decision-making with high-dimensional covariates. In\nInternational Conference on Artificial Intelligence and Statistics, pages 3848–3857. PMLR, 2020.\n[57] Tianhao Wang, Dongruo Zhou, and Quanquan Gu.\nProvably efficient reinforcement learning with\nlinear function approximation under adaptivity constraints. Advances in Neural Information Processing\nSystems, 34:13524–13536, 2021.\n[58] Michael Woodroofe. A one-armed bandit problem with a concomitant variable. Journal of the American\nStatistical Association, 74(368):799–806, 1979.\n[59] Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi-armed\nbandit problem with covariates. Ann. Statist., 30(1):100–121, 2002.\n[60] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in neural\ninformation processing systems, 33:9818–9829, 2020.\n[61] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via\nreference-advantage decomposition.\nAdvances in Neural Information Processing Systems, 33:15198–\n15207, 2020.\n[62] Zhijin Zhou, Yingfei Wang, Hamed Mamani, and David G Coffey.\nHow do tumor cytogenetics in-\nform cancer treatments? dynamic risk stratification and precision medicine using multi-armed bandits.\nDynamic Risk Stratification and Precision Medicine Using Multi-armed Bandits (June 17, 2019).\n26\n"
}