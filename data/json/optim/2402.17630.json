{
    "optim": "Fine-Grained Natural Language Inference Based\nFaithfulness Evaluation for Diverse Summarisation Tasks\nHuajian Zhang∗\nYumo Xu†\nLaura Perez-Beltrachini\nILCC, School of Informatics\nUniversity of Edinburgh\nhuajian.zhang.21@gmail.com, {yumo.xu,lperez}@ed.ac.uk\nAbstract\nWe study existing approaches to leverage off-\nthe-shelf Natural Language Inference (NLI)\nmodels for the evaluation of summary faithful-\nness and argue that these are sub-optimal due\nto the granularity level considered for premises\nand hypotheses. That is, the smaller content\nunit considered as hypothesis is a sentence and\npremises are made up of a fixed number of doc-\nument sentences. We propose a novel approach,\nnamely INFUSE, that uses a variable premise\nsize and simplifies summary sentences into\nshorter hypotheses. Departing from previous\nstudies which focus on single short document\nsummarisation, we analyse NLI based faithful-\nness evaluation for diverse summarisation tasks.\nWe introduce DiverSumm, a new benchmark\ncomprising long form summarisation (long doc-\numents and summaries) and diverse summari-\nsation tasks (e.g., meeting and multi-document\nsummarisation). In experiments, INFUSE ob-\ntains superior performance across the different\nsummarisation tasks. 1\n1\nIntroduction\nCurrent state-of-the-art summarisation systems are\nable to generate fluent summaries; however, their\ninability to generate factually consistent summaries\nremains a significant constraint in their real-world\napplications. As a result, the assessment of sum-\nmary faithfulness, i.e., the degree to which a sum-\nmary accurately represents the content of the input\ndocument, has recently received much research at-\ntention. This evaluation is key to assess progress\nin abstractive summarisation (Gehrmann et al.,\n2021, 2023). Existing research focuses on devel-\noping models to detect unfaithful summary con-\ntent (Kryscinski et al. 2020; Scialom et al. 2021;\n∗Part of the work done for his MSc thesis at the University\nof Edinburgh.\n†Work done while at the University of Edinburgh.\n1Our code and data are available at https://github.\ncom/HJZnlp/infuse\nRibeiro et al. 2022; inter alia) as well as the meta-\nevaluation of these models with better benchmarks\n(Chen et al., 2021; Honovich et al., 2022; Durmus\net al., 2022).\nOne way increasingly adopted to assess sum-\nmary faithfulness is to use off-the-shelf Natural\nLanguage Inference (NLI; MacCartney and Man-\nning 2009) models to determine whether a sum-\nmary is entailed by the source document. NLI mod-\nels determine the semantic relationship between a\npair of texts: the premise and hypothesis. If the\nhypothesis can be inferred from the premise, it is\nsaid to be entailed by the premise. However, ex-\nisting NLI models are mainly trained on relatively\nshort texts from existing datasets Bowman et al.\n2015; Williams et al. 2018. Examples in these\ndatasets often represent inference cases over a sin-\ngle content unit (e.g., the example at the bottom\nof Figure 1 where inference is about the transmis-\nsion event). This raises the question of how to\napply them to produce entailment judgements for\ndocument-summary pairs consisting of multiple\nsentences aggregating several content units (e.g.,\nthe summary sentence MSS in Figure 1 aggregates\ncontent about the company launching a legal ac-\ntion, a strike event, and the consequences of the\nstrike). Producing an entailment judgement for a\nsummary sentence with several content units is a\nmore complex entailment reasoning task.\nTaking summary sentences as hypotheses, exist-\ning approaches try to either identify a document\nsentence that acts as the premise leading to the\nhighest possible entailment score (sentence-level\nNLI, (Laban et al., 2022; Nie et al., 2020)) or di-\nrectly measure entailment by taking the entire doc-\nument as premise (document-level NLI, (Maynez\net al., 2020; Honovich et al., 2022; Dziri et al.,\n2022)). However, due to content aggregation hap-\npening in summarisation, one document sentence\nwill not contain enough content to entail a sum-\nmary sentence. In Figure 1, none of the docu-\narXiv:2402.17630v1  [cs.CL]  27 Feb 2024\nD\n|= MSS\n|= SS1\n|= SS2\n1\nLufthansa lost an appeal to a Frankfurt labour court, but is making a further legal challenge that could go late\ninto Tuesday evening.\n0.37\n7.81\n0.06\n2\nThe pilots’ strike, called over a pay dispute, will affect around 100,000 passengers, Lufthansa said.\n0.61\n0.69\n1.74\n3\nThe industrial action is part of a long-running pay dispute at Lufthansa.\n0.18\n0.74\n0.06\n4\nThe pilots’ union Vereinigung Cockpit (VC) has organised 14 strikes since April 2014.\n0.07\n0.11\n0.10\n5\nShort and medium-haul flights from Germany will be affected from 00:01 to 23:59 local time (23:01-22:59\nGMT).\n0.09\n0.14\n0.06\n6\nFlights by Lufthansa’s other airlines including Eurowings, Swiss, Austrian Airlines, Air Dolomiti and\nBrussels Airlines are not affected by the strike, the airline said.\n0.11\n0.22\n0.12\n7\nPay talks between the Vereinigung union and the German airline broke down earlier this month, and Lufthansa\nsaid the union had \"consistently rejected the offer\" of mediation.\n0.20\n0.41\n0.06\n8\nThe union is calling for a 3.7% pay rise for 5,400 pilots dating back to 2012.\n0.14\n0.42\n0.05\n9\nLufthansa, which is facing increasing competition from budget rivals, offered a 2.5% increase over the six\nyears until 2019.\n0.12\n0.22\n0.11\n10\nMeanwhile, a separate dispute with cabin crew at Lufthansa’s low-cost subsidiary, Eurowings, led it to cancel\nmore than 60 flights on Tuesday.\n0.27\n0.47\n0.32\nMSS\nGerman airline Lufthansa has launched a fresh legal challenge against a strike by its pilots, which could lead to the cancellation of\nmore than 1,000 flights.\nSS\nGerman airline Lufthansa has launched a fresh legal challenge against a strike by its pilots.\nThe strike could lead to the cancellation of more than 1,000 flights.\nAt 8:34, the Boston Center controller received a third trans-\nmission from American 11\n|=\nThe Boston Center controller got a third transmission from\nAmerican 11.\nFigure 1: Example of input Document (D) and Model-generated Summary Sentence (MSS) from the AggreFact\n(Tang et al., 2023) benchmark on the XSum (Narayan et al., 2018) dataset. The example is considered unfaithful by\nthe annotators. Simplified Summary (SS) is the generated summary after automatic sentence splitting. The cyan\ncoulored text spans in the input document highlight those document content units that support the corresponding\ncyan spans in the summary. Red spans in the summary indicate content that is not supported by the input document.\nThe |= MSS and |= SSi columns show entailment scores assigned by an off-the-shelf NLI model to document\nsentences acting as premises and either MSS or SSi sentences as hypotheses. The table in the bottom shows an\nexample of entailment relation from the MNLI dataset (Williams et al., 2018). Entailment scores are computed by\nthe NLI model introduced in Section 4 and normalised for better reading.\nment sentences alone can entail the complex sum-\nmary sentence MSS aggregating several content\nunits. On the other hand, taking the entire docu-\nment as premise will perform poorly on long input\ndocuments (Schuster et al., 2022)). Recent work\nachieves promising results by first selecting an en-\ntailing context (context-level NLI, (Nie et al., 2019;\nSchuster et al., 2022; Kamoi et al., 2023)). That\nis, borrowing insights from information retrieval,\nthese approaches carry out an initial step of doc-\nument sentence retrieval to build a short context;\nand then perform NLI with the retrieved context as\na premise. Specifically, in the retrieval step, given\na summary sentence as hypothesis, document sen-\ntences are individually scored by an NLI model and\nranked and the top k thereof constitute the premise\n(e.g., for the MSS in Figure 1, the 2nd, 1st, and\n10th would be selected as premise if k = 3).\nIn this work we argue that existing NLI-based\napproaches do not operate at the right level of gran-\nularity (Nenkova et al., 2007); even context-level\nNLI approaches. Summary sentences may con-\nvey several content units (Nenkova et al., 2007)\npartly overlapping with different document sen-\ntences. This renders the retrieval step of document\nsentences based on NLI scores less accurate (e.g.,\neach document sentence in Figure 1 weakly entails\nthe complex summary sentence MSS). In addition,\nsummary sentences may aggregate content from\ndifferent numbers of document sentences which\nmakes it less accurate to have an entailing con-\ntext with a fixed k number of document sentences\n(e.g., in Figure 1, SS1 is entailed by two document\nsentences while SS2 requires only one document\nsentence to show that its content is not derived\nfrom the document).2 Finally, a fine-grained assess-\nment of summary faithfulness brings interpretabil-\nity, which hugely facilitates manual inspection of\nmodel-generated summaries.\nWe propose INFUSE, a faithfulness evaluation\napproach that INcrementally reasons over a docu-\nment so as to arrive at a FaithfUlnesS Estimation of\nits summary. It aims at retrieving the best possible\ncontext to assess the faithfulness of each summary\nsentence (and in turn the entire summary), i.e., a\ncontext with the minimal and most relevant set of\ndocument sentences. Our incremental reasoning\napproach approximates this via successive expan-\nsions of the context adding document sentences\nand evaluating whether the hypothesis is entailed\nby it. Our approach further decomposes summary\nsentences for their faithfulness analysis. It does\n2Note that more than 1,000 flights is not supported by the\nexplicit facts stated in the input document.\nthis via sentence simplification. That is, it splits\nlong summary sentences (e.g., MSS sentence in\nFigure 1) into a set of shorter ones conveying the\nsame content units (e.g., SS1 and SS1 in Figure 1).\nMost of previous work focuses on the meta-\nevaluation of NLI-based approaches on single docu-\nment news summarisation (Laban et al., 2022; Tang\net al., 2023). Thus, the question of how NLI-based\nevaluation works on diverse summarisation tasks\nis left unanswered. Hence, to widen the spectrum\nof NLI-based meta-evaluation (Gehrmann et al.,\n2021), we analyse the performance of NLI-based\nfaithfulness evaluation approaches on long doc-\nument summarisation with diverse domains and\ngenres (Cohan et al., 2018; Huang et al., 2021;\nZhong et al., 2021; Adams et al., 2023) and multi-\ndocument summarisation (Fabbri et al., 2019). We\ncollect human annotated model-generated sum-\nmaries from previous work on these tasks (Koh\net al., 2022; Adams et al., 2023; Chen et al., 2023).\nWe call this new set the DiverSumm benchmark.\nWe study existing NLI-based approaches on Ag-\ngreFact (Tang et al., 2023), a benchmark for the\nmeta-evaluation of single document summarisation,\nand DiverSumm. INFUSE achieves the best perfor-\nmance in these benchmarks. We find that the choice\nof an adequate level of granularity for the premise\nand hypothesis leads to more accurate entailment\njudgements when using off-the-shelf NLI models.\nOn summaries of extractive nature, retrieving a\nsmall relevant set of document sentences suffices.\nMoreover, our results show that this is crucial for\nsummarisation tasks with long input documents.\nSummary sentence splitting helps to obtain better\nperformance in all summarisation tasks.\n2\nFaithfulness Annotated Data for\nDifferent Summarisation Tasks\nFollowing previous work, we study faithfulness\nevaluation on two single document summarisation\ntasks, namely CNNDM (Nallapati et al., 2016) and\nXSum (Narayan et al., 2018). For this, we take\nthe latest introduced faithfulness benchmark, Ag-\ngreFact (Tang et al., 2023). It consists of a collec-\ntion of document and model-generated summary\npairs where summaries are annotated with faith-\nfulness judgements by human judges.\nThat is,\neach example in the benchmark is a triple (doc-\nument, generated-summary, faithful/unfaithful la-\nbel). AggreFact includes five annotated sets from\nthe earlier SummaC (Laban et al., 2022) bench-\nmark. These are XSumFaith (Maynez et al., 2020),\nFactCC (Kryscinski et al., 2020), SummEval (Fab-\nbri et al., 2021), FRANK (Pagnoni et al., 2021),\nand Polytope (Huang et al., 2020). In addition, Ag-\ngreFact includes four sets, namely QAGS (Wang\net al., 2020) (referred as Wang’20 in the bench-\nmark), CLIFF (Cao and Wang, 2021a), GOYAL’21\n(Goyal and Durrett, 2021) and CAO’22 (Cao et al.,\n2022). AggreFact organises the annotated data into\ntwo major sets per summarisation task, CNNDM\nand XSum, herein we name them CNNDMAG and\nXSumAG. See Appendix A for details on the faith-\nfulness annotation scheme of each dataset and the\nstandarisation criteria applied to derive AggreFact.\nDiverSumm a New Benchmark\nTo study the\nperformance of NLI-based faithfulness evaluation\non diverse summarisation tasks, we propose a new\nbenchmark, namely DiverSumm. It incorporates\nmodel generated summaries with human annota-\ntions about faithfulness from previous work (Koh\net al., 2022; Adams et al., 2023; Chen et al., 2023).\nWe follow (Laban et al., 2022) to standardise sum-\nmary annotations into faithful/unfaithful labels. We\ndiscuss the summarisation task and characteristics\nof the annotated sets below.\nChemSumm (Adams et al., 2023) embodies\nthe task of scientific long-form summarisation\nin the chemistry domain. Derived from aca-\ndemic journals, each input document contains\nsection headers and associated paragraphs for\nall sections from the introduction up to the\nconclusion, and abstracts constitute the refer-\nence summaries.\nMultiNews (Fabbri et al., 2019) is a large-\nscale multi-document news summarisation\ndataset with the number of input documents\nper example ranging from 2 to 6 and reference\nsummaries written by editors.\nQMSUM (Zhong et al., 2021) is a query-\nbased multi-domain meeting summarisation\ndataset. It consists of meeting transcripts and\nqueries associated with their corresponding\nabstractive summaries.\nArXiv (Cohan et al., 2018) is a long scientific\npaper summarisation dataset collected from\nArXiv covering a wide range of topics. The\nmain content up to the conclusion section of a\npaper is regarded as the document and the cor-\nresponding abstract section as the summary.\nSummarisation Task\nDoc.Tok\nSum.Sent\nSum.Tok\nCov\nDens\nSummarisers\nXSum (Tang et al., 2023)\n360.54\n1.01\n20.09\n0.55\n0.99\nOLD-EXFORMER, T5, BART, PEGASUS\nCNNDM (Tang et al., 2023)\n518.85\n2.72\n52.21\n0.80\n10.40\nOLD-EXFORMER, T5, BART, PEGASUS\nChemSumm (Adams et al., 2023)\n4612.40\n7.36\n172.79\n0.91\n10.89\nLongT5, PRIMERA\nQMSUM (Zhong et al., 2021)\n1138.73\n3.04\n65.22\n0.69\n5.13\nGPT-3.5, UniSumm, PEGASUS\nArXiv (Cohan et al., 2018)\n4406.99\n6.18\n149.70\n0.89\n9.59\nPEGASUS, BART\nGovReport (Huang et al., 2021)\n2008.16\n15.07\n391.22\n0.86\n12.76\nPEGASUS, BART\nMultiNews (Fabbri et al., 2019)\n669.20\n6.81\n152.20\n0.82\n14.19\nGPT-3.5, UniSumm, PEGASUS\nTable 1: Statistics on AggreFact (test split) and DiverSumm per summarisation task. Document length in average\nnumber of tokens (Doc.Tok), summary length in average number of sentences (Sum.Sent) and tokens (Sum.Tok),\nand extractive metrics (Grusky et al., 2018) Density (Dens) and Coverage (Cov). Models generating summaries are\nLongT5 (Guo et al., 2022), PRIMERA (Xiao et al., 2022), GPT-3.5 (text-davinci-002) (Brown et al., 2020a; Ouyang\net al., 2022), UniSumm (Chen et al., 2023), PEGASUS (Zhang et al., 2020), BART (Lewis et al., 2020), and T5\n(Raffel et al., 2020). As grouped by Tang et al. (2023), OLD-EXFORMER denotes older models (See et al., 2017;\nGehrmann et al., 2018; Liu and Lapata, 2019; Radford et al., 2019) .\nGovReport (Huang et al., 2021) pairs long\nreports from government research agencies,\nincluding the Congressional Research Service\nand U.S. Government Accountability Office,\nwith expert-written abstractive summaries.\nEach summary in QMSUM and MultiNews was\nlabeled using a 5-point Likert scale in terms of flu-\nency, coherence, consistency, and relevance (Chen\net al., 2023). We use the consistency criterion and\nlabel summaries as faithful if the score in consis-\ntency is 5, otherwise unfaithful. In ChemSumm,\narXiv, and GovReport, summaries are annotated\nwith a numerical number between 0 (inconsistent)\nand 1 (consistent) (Koh et al., 2022; Adams et al.,\n2023). We take summaries as faithful if the major-\nity of the annotators labeled the summary as 1.\nDiverSumm contains 563 test instances with a\ntotal of 4686 summary sentences of which 3138\nhave sentence level annotations. Table 1 shows rel-\nevant statistics about the benchmarks. Documents\nand summaries are longer in DiverSumm. Gener-\nated summaries for XSum and QMSUM are more\nabstractive (i.e., smaller coverage and density).\nError types\nSome subsets in AggreFact and Di-\nverSumm, namely FRANK, ArXiv, and GovReport,\ncontain sentence level and detailed error annota-\ntions for unfaithful summaries.3 We exploit these\nannotations to analyse the performance of both the\nstudied approaches and the NLI model on detecting\ndifferent types of faithfulness errors. Concretely,\nunfaithful summaries are annotated with the fol-\nlowing error types (Pagnoni et al., 2021). Relation\nError (PreE) is when the predicate in a summary\n3After manual inspection of the human annotations, we\nfiltered out some examples in ArXiv and GovReport with a\nmismatch between the sentence and summary level annotation.\nFigure 2: Statistics for the number of fused document\nsentences (the pie charts) and their distances (the blue\nvertical bars) on XSum and CNNDM (AggreFact) and\nGovReport and ChemSum (DiverSumm).\nsentence is inconsistent with respect to the doc-\nument. Entity Error (EntE) is when the primary\narguments of the predicate are incorrect. Circum-\nstance Error (CircE) is when the predicate’s circum-\nstantial information (i.e., name or time) is wrong.\nCo-reference error (CorefE) is when there is a pro-\nnoun or reference with an incorrect or non-existing\nantecedent. Discourse Link Error (LinkE) is when\nmultiple sentences are incorrectly linked. Out of\nArticle Error (OutE) is when the piece of summary\ncontains information not present in the document.\nGrammatical Error(GramE) indicates the existence\nof unreadable sentences due to grammatical errors.\n2.1\nThe Value of Adequate Premise and\nHypothesis Granularity\nWe analyse document-summary pairs in the Ag-\ngreFact and DiverSumm benchmarks to uncover\nthe rational of why adequate premise and hypothe-\nsis granularity brings value into the evaluation of\nsummary faithfulness (Nie et al., 2019; Schuster\net al., 2022; Kamoi et al., 2023).\nWe examine the number of document sentences\naggregated into a summary sentence via a greedy\nselection algorithm that maximizes document-\nsummary token overlap (Lebanoff et al., 2019).\nAs shown in Figure 2, 18-48% of summary sen-\ntences fuse more than one document sentence and\nat least 50% of the cases are not within a 5-sentence\nwindow.\nIn particular, in GovReport 64% and\nChemSumm 71% of the times the fused document\nsentences are in a 15 sentences or more window\nsize. This renders sentence- and paragraph-level\npremises not ideal due to low recall. We show\nsentence fusion statistics for the other datasets in\nFigure 4, Appendix A.\nAn alternative to improve recall would be via\nincreasing premise size. However, NLI models are\ntypically trained on short premise-hypothesis exam-\nples with a premise average length ranging on 16-\n80 tokens for widely used datasets and a hypotheses\nlength of 9-19 (Schuster et al., 2022). It is challeng-\ning for such models to generalise to document-level\npremises (average length is 439 in AggreFact and\n2566 in DiverSumm). Previous work has shown\nthat the performance of faithfulness evaluation\nconsistently drops with longer premises (Schus-\nter et al., 2022). We next describe our approach\nwith premises of variable size (i.e, variable num-\nber of document sentences) and shorter hypotheses\n(i.e., simplified summary sentences).\n3\nINFUSE\nWe denote a document as D = {dm}M\nm=1 and\na summary as S = {sn}N\nn=1 where dm and sn\nare sentences.\nFor a given summary sentence\nsn as the hypothesis, we aim to retrieve a re-\nlated context R(n), R(n) ⊆ D, to act as the\npremise and estimate whether sn can be entailed\nby R(n) (and, therefore, D) according to an NLI\nmodel θ. We assume that θ predicts one of the\n{entailment, neutral, contradict} labels for a given\npremise-hypothesis pair. Summary sentence faith-\nfulness estimates, given by θ(entailment|·), are\nthen aggregated into summary faithfulness scores\nwith mean pooling.\nIncremental Reasoning\nGiven an NLI model θ,\nwe construct a matrix E of entailment scores via\nsentence level inference between document sen-\ntences dm and each summary sentence sn. We\nderive from E entailment ranked lists of docu-\nment sentences ˆD(n) associated to each summary\nsentence sn. We then incrementally select sen-\ntences from ˆD(n) in a top-down fashion to retrieve\na context R(n) for sn. Starting from an empty\ncontext R(n)\n0 , at each step i, we remove the top\nsentence from ˆD(n) and incorporate it into the cur-\nrent context to obtain a new context R(n)\ni\n. We\nthen stop adding sentences to the context when\nthe local minimum of the neutral class probabil-\nity, ui,n = θ(neutral|R(n)\ni\n, sn), is reached, i.e.,\nui,n ≥ ui−1,n.\nIntuitively, decreasing neutral\nscores signal shifts in the perceived entailment rela-\ntionship from context R(n)\ni−1 to R(n)\ni\n(i.e., candidate\npremises) and sn (the hypothesis) leaning towards\neither entailment or contradiction. We stop when\nthere is an increase in the neutral score. At this\nstopping point, the entailment score between the\npremise given by context R(n)\ni\nand summary sen-\ntence sn as hypothesis is taken as the final faithful-\nness estimation for sn.\nAlgorithm 1 Summary sentence entailment estima-\ntion in INFUSE.\nInput: NLI model θ, pair (D, sn).\nOutput: R(n)\ni−1, ei−1,n premise and entailment\nscore for sn.\n1: for dm in D do\n2:\nem,n, um,n, cm,n = θ(dm, sn)\n3:\nen,m, un,m, cn,m = θ(sn, dm)\n4:\n# entailment e, neutral n, contradiction c\n5:\nˆEdm,sn = em,n + en,m\n6: end for\n7: ˆD(n) = rank( ˆEd1:M, sn)\n8: R(n)\n0\n= ∅, n0,n = 0\n9: for ˆdi in ˆD(n) do\n10:\nadd ˆdi to R(n)\ni\n11:\nei,n, ui,n, ci,n = θ(R(n)\ni\n, sn)\n12:\nif ui,n ≥ ui−1,n then\n13:\nstop and return R(n)\ni−1, ei−1,n\n14:\nend if\n15: end for\nReversed Reasoning\nIn some cases, the content\nexpressed in a document sentence dm will only en-\ntail part of a summary sentence sn (see example in\nTable 8 -bottom- of the Appendix). Thus, such dm\nwill have a low sentence level entailment score in E\ndespite dm really providing evidence for a part of\nsn. Because summaries will contain extracted doc-\nument fragments or paraphrases thereof, one way\nto improve entailment scores for such document\nsentences dm is to reverse the direction in which\nsentence level NLI is applied. That is, we take the\nsummary sentence sn as premise and the document\nsentence dm as hypothesis. We add reversed en-\ntailment scores to those on E and obtain a new\nre-weighted matrix ˆE which is adopted to perform\nincremental context retrieval. Algorithm 1 sum-\nmarises INFUSE steps to estimate the entailment\nscore of a given summary sentence with respect to\nits corresponding input document.\nSub-sentence Reasoning\nDifferent document\nsentences dm will entail different parts of a sum-\nmary sentence sn (see document sentence fusion in\nFigure 2). In addition, those document sentences\ndm may contain irrelevant content for sn. Thus,\nsentence level scores in E as well as final context\nlevel entailment scores for sn will be noisy (i.e.,\nmore chances of having neutral class high scores).\nShorter summary sentences with finer-grained con-\ntent units will yield more accurate contexts and\nentailment estimations. Figure 1 illustrates the dif-\nference in entailment scores in E when computed\non the original summary sentence (MSS) and when\ncomputed on its sub-sentences (SS1 and SS2). In\nthis work, we propose to simplify each summary\nsentence by splitting it into multiple sub-sentences.\n4\nExperimental Setup\nWe study NLI-based faithfulness evaluation ap-\nproaches on AggreFact (Tang et al., 2023) and\nDiverSumm (Section 2). We adopt an ALBERT-\nxlarge (Lan et al., 2020) model optimized on MNLI\n(Williams et al., 2018) and VitaminC (Schuster\net al., 2021) as our NLI model θ. MNLI covers\nten distinct genres and styles of written and spo-\nken English data. It aims to support a broader\nunderstanding and analysis of NLI across different\ngenres and domains. VitaminC is synthetically cre-\nated from Wikipedia sentences. Claim sentences\nare associated with contrastive evidence, i.e., one\nsentence that supports the claim and another one\nthat does not. On MNLI (VitaminC) premises are\n13.23 (43.03) tokens long in average and hypothe-\nses 13.23 (27.57).\nWe fine-tune a T5-large (Raffel et al., 2020)\nmodel for sentence splitting. We use this model to\nsimplify sentences in model generated summaries.\nWe manually inspect several samples of split sen-\ntences and find that the performance is reasonable.\nDetails about our sentence splitting model, exam-\nples, and statistics about the percentage of sentence\nsplits are presented in Appendix B.\nWe compare INFUSE with a widely adopted\napproach which considers the entire document as\na premise, we refer to it as FULLDOC. In prac-\ntice, it divides the input document into chunks\nthat fit the input size of the NLI model, com-\nputes chunks scores and takes the average thereof.\nWe also compare with SUMMACZS (Laban et al.,\n2022), a sentence-level method which assumes\neach summary sentence is supported by one doc-\nument sentence, and takes the one with the top\nentailment score as context.\nSUMMACCONV in-\ntroduces a convolutional layer trained on a sub-\nset of FactCC (Kryscinski et al., 2020) to aggre-\ngate the score given by an NLI model to each\n{entailment, neutral, contradict} label into a final\nscore. For a fair comparison with the other models,\nwe remove specific constraints used in the origi-\nnal implementation of SUMMAC variants (see Ap-\npendix B). SENTLI (Schuster et al., 2022) retrieves\na context with a fixed number k of document sen-\ntences. Its context includes document sentences\nwith top entailment and top contradiction scores.\nFollowing (Schuster et al., 2022), we set the value\nof k = 5. We show performance with other values\nof k in Figure 6 in Appendix E. INFUSESUB is our\nvariant with sub-sentence reasoning (i.e., summary\nsentence simplification). For this variant, to better\nmimic the process of label standardisation as de-\nscribed in Section 2, we use the min(·) operator to\naggregate the entailment scores from sub-sentences\ninto a sentence score.\n5\nResults\n5.1\nFaithfulness Evaluation\nFollowing Laban et al. (2022), we adopt ROC-\nAUC (Bradley, 1997) which depicts classification\nperformance with varied thresholds as our eval-\nuation metric. Results on AggreFact and Diver-\nSumm are shown in Table 2.4 INFUSE and IN-\nFUSESUB exhibit superior performance than previ-\nous approaches overall summarisation tasks. FULL-\nDOC exhibits the lowest performance, this con-\nfirms results from previous meta-evaluations (La-\nban et al., 2022; Schuster et al., 2022) and extends\nthe observations to the summarisation tasks in our\n4To determine the statistical significance of performance\ndifferences, we randomly re-sample 70% of the test instances\n100 times and evaluate the models on these sets. We use the\npairwise t-test to assess whether there is a significant differ-\nence between models.\nXSMAG CNDAG CSM QMS AXV GOV MNW AVG\nFULLDOC\n72.77\n64.40\n50.15 37.12 62.78 79.19 44.76 58.74\nSUMMACCONV\n67.76\n72.14\n53.14 51.13 61.22 65.34 53.05 60.54\nSUMMACZS\n70.29\n74.54\n54.41 48.21 69.44 79.37 50.17 63.78\nSENTLI\n73.61\n75.83\n50.13 47.56 64.49 79.68 46.61 62.56\nINFUSE\n73.42\n76.21\n54.11 52.16 71.38 80.45 53.16 65.84\nINFUSESUB\n73.21\n73.34\n59.26 53.20 73.89 80.05 49.37 66.05\nTable 2: Results for all summarisation tasks in AggreFact and DiverSumm. For AggreFact, we report the average\nresults for XSum (XSM; 5 datasets) and CNN/DM (CND; 7 datasets), respectively; dataset-level performance can\nbe found in Appendix D. CSM, MNW, QMS, AXV, and GOR refer to ChemSum, MultiNews, QMSUM, ArXiv, and\nGovReport respectively. We highlight highest scores and scores significantly different from FULLDOC, SUMMAC\nvariants, and SENTLI approaches (at p < .05).\nDiverSumm benchmark. SUMMACCONV, trained on\nspecific evaluation data, does not generalise well\nacross the different summarisation tasks. Thus, our\nmain comparison variant is SUMMACZS.\nAs for the role of sub-sentence reasoning, we ob-\nserve that INFUSESUB works better on ChemSumm,\nQMSUM, and ArXiv where summary sentences are\ncomplex and informative (see sentence fusion in\nFigure 2) and more abstract (Table 1). This further\nvalidates the positive findings from claim verifica-\ntion tasks (Kamoi et al., 2023) for text summari-\nsation. On the other hand, sub-sentence reasoning\nis less effective on CNNDMAG, GovReport and\nMultiNews which consist of more extractive sum-\nmaries (Table 1). In CNNDMAG segmenting short\nsentences may only introduce noise. This partially\nsupports Glover et al. (2022) who draw a negative\nconclusion on the effectiveness of sub-sentence\nevaluation based on CNNDM. We note that the\nnature of the data underlying evaluation bench-\nmarks should be further emphasized to delimit the\nscope of conclusions drawn. For GovReport and\nMultiNews, with the most extractive summaries\n(Table 1), we found that after splitting the relation\nbetween summary sub-sentences and document\nsentences becomes mostly one-to-one and thus ap-\nproaches taking one document sentence become\nmore effective (see results for existing approaches\nwith sub-sentence hypotheses in Appendix C).\nOn XSumAG, retrieval approaches, INFUSE and\nSENTLI, work very closely to the document-level\napproach FULLDOC. The success of the document-\nlevel approach lies on the fact that summaries in\nXSumAG are highly abstractive (Table 1) and re-\nquire reasoning over multiple document sentences;\nand input documents are short. Indeed, for highly\nabstractive summarisation tasks such as XSumAG\nor QMSUM it would make sense to build a struc-\ntured premise with document sub-sentence content,\nconnecting discourse information, explicit world\nknowledge, and intermediate inferences made ex-\nplicit (Dalvi et al., 2021).\nResults in Table 2 show that the variable premise\nsize of INFUSE leads to better performance across\nthe board. In Appendix E, we show performance\ncurves for INFUSE versus INFUSE−k, a version\nwith different fixed premise sizes, to further illus-\ntrate this. We report statistics about the number\nof document sentences retrieved by INFUSE and\nINFUSESUB in Table 4. These show the inherent\nvariability in document sentence fusion happening\nwithin summary sentences (see approximation of\nthis in Figure 2).\nThe reversed entailment direction in the retrieval\nstep acts as a re-weighting scheme that takes ad-\nvantage of paraphrased content and favors shorter\ndocument sentences (i.e., fewer content units than\nthose appearing in summary sentences). We pro-\nvide performance curves on the effect of reversed\nreasoning comparing INFUSE with a version IN-\nFUSE−reversed in Appendix E; and case studies\nin Appendix G .\nOn DiverSumm, ROC-AUC scores are consider-\nably lower than those obtained on AggreFact across\nthe board. We attribute this to the summarisation\ntasks been more complex and recent models that\ngenerated the summaries more powerful. The low-\nest scores are on ChemSumm and QMSUM, we\nattribute this to the shift in vocabulary and genre\nin these tasks. The following lowest scoring task\nis MultiNews, we attribute this to the redundancy\nfound in multi-document input. The fixed context\nof SENTLI will only include redundant sentences.\n5.2\nPerformance on Different Error Types\nWe look into the performance of the studied NLI-\nbased approaches with respect to unfaithfulness\nerrors discussed in Section 2.\nWe focus on\nArXiv, GovReport, and FRANK which contain\nfine-grained error annotations at sentence level. We\nconsider each summary sentence in these subsets\nto be labelled with the error types that the major-\nity of annotators agreed upon. We analyse the\ndistribution of entailment scores for FULLDOC,\nSENTLI, INFUSE, and INFUSESUB on summary\nsentences (i.e., without aggregation into a final sum-\nmary score). We show these in Figure 3.5\nBefore looking into specific error types we anal-\nyse the range of scores the approaches assign to\nfaithful sentences. Figure 3.a shows that FULL-\nDOC tends to predict rather low entailment scores,\nclose to zero, for most of faithful cases. This ex-\nplains the lower ROC-AUC in Table 2. Using the\nentire document leverages noise when computing\nentailment if the input documents are long. After\nall, NLI models are trained to use the entire premise\nto yield a judgement and not to distinguish those rel-\nevant from irrelevant premise parts. Context-level\napproaches produce higher scores. INFUSE and\nINFUSESUB produce more extreme scores.\nOn EntE error types, Figure 3.f shows that IN-\nFUSE assigns close to zero scores to more EntE\ncases. It works slightly better than INFUSESUB and\nwe attribute this to the fact that INFUSESUB may\nintroduce some noise when splitting sentences. In\ncontrast, SENTLI assigns entailment scores in the\nrange of [0.4,0.6] where also many faithful cases\nfall on. Figure 7.a/.b in Appendix F shows a similar\ntrend for PredE and CircE error types.\nAs for discourse level errors, on LinkE error\ntypes, Figure 3.c, INFUSE works better. After\nmanual inspection, we attribute this to the fact that\nnone of the incorrectly fused document sentences\ncontributes to a high entailment score. Thus, they\nwill not be retrieved as an entailing premise. On\nCorefE errors, Figure 3.d, we can see that all ap-\nproaches have poor performance assigning rela-\ntively high entailment scores. Note that the set of\nthese error types is rather small.\nFinally, on the OutE error type, Figure 3.e, IN-\nFUSE is better over INFUSESUB and SENTLI.\nWe attribute this to the fact that in cases of this\nerror type there is no document information that\ncan support nor contradict the summary sentence;\nthus, INFUSE will take the minimum number of\ndocument sentences (potentially only one) failing\nto entail the summary with OutE. Grammar errors,\nGramE in Figure 3.f, seem difficult to detect, which\n5Note that for none of the approaches, we have tuned a\nfaithful/unfaithful decision threshold; however, we compare\nfaithful/unfaithful distributions and analyse performance at\nextreme scores 1/0 in the [0,1] interval.\nmakes sense for NLI-based approaches.\nWe observed that in some error types IN-\nFUSE (and INFUSESUB) assigns extremely high\n(∼ 1) scores to some cases. We manually examine\na sample thereof and find that in most cases sum-\nmary sentences have a high lexical overlap with\ndocument sentences and vary either on few tokens\nor word order. Thus, the NLI model is biased to rely\non extractive cues (McKenna et al., 2023; Verma\net al., 2023). Table 10 in Appendix F shows exam-\nples of error types correctly (∼ 0) and incorrectly\n(∼ 1) evaluated by INFUSE .\n6\nRelated work\nSome NLI-based approaches directly train docu-\nment level NLI models (Yin et al., 2021; Utama\net al., 2022). Others leverage off-the-shelf NLI\nmodels (Nie et al., 2019, 2020; Laban et al., 2022;\nSchuster et al., 2022; Kamoi et al., 2023; Steen\net al., 2023). The former requires the construction\nof synthetic training data. In this paper, we study\nthe latter type of approaches. These do not require\nadditional data nor training resources.\n(Nie et al., 2020; Laban et al., 2022) select a\nsingle sentence as premise while (Nie et al., 2019;\nSchuster et al., 2022; Kamoi et al., 2023) select\na fixed number of document sentences, the same\nfor all summary sentences. Our approach selects a\nvariable number of document sentences as premise\nfor each summary sentence. Recently, (Chen and\nEger, 2023) conduct an empirical analysis of how\nto use the three directions in which entailment can\nbe computed (entailment direction implication, re-\nverse implication, and bi-implication). However,\n(Chen and Eger, 2023) directly use the scores from\nthese directions in a single pass using the entire doc-\nument as premise. In contrast, we apply reversed\nreasoning only to re-weight document sentences\nin the context retrieval step. (Steen et al., 2023)\npropose a document-level approach with data aug-\nmentation to adapt NLI models to task specific\nscenarios such as dialogue. Furthermore, they en-\nsemble a number of calls to the NLI model via\nMonte-Carlo dropout to cope with domain shift.\nThese ideas are orthogonal to our work and would\nmake sense to use them in combination.\nThe value of fine-grained assessment of sum-\nmary content has been highlighted in earlier\nwork on summarisation evaluation (Marcu, 2001;\nVoorhees, 2004; van Halteren and Teufel, 2003;\nTeufel and van Halteren, 2004; Nenkova et al.,\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n200\n400\n600\n800\n1000\n1200\nCount\n(a) Faithful\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n20\n40\n60\n80\n100\nCount\n(b) EntE\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\n15.0\n17.5\n20.0\nCount\n(c) LinkE\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nCount\n(d) CorefE\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n25\n50\n75\n100\n125\n150\n175\n200\nCount\n(e) OutE\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n10\n20\n30\n40\n50\nCount\n(f) GramE\nFigure 3: Distribution of entailment scores on faithful summary sentences and unfaithful ones encompassing\ndifferent error types for ArXiv, GovReport and FRANK sets. The x-axe corresponds to the NLI-based approach.\nThat is, FULLDOC in red, SENTLI in green, INFUSE in cyan, and INFUSESUB in purple. The y-axe corresponds\nto the entailment scores (i.e., values ranging in [0,1]), and the z-axe corresponds to the count of instances.\n2007; Gao et al., 2019; Shapira et al., 2019). This\nresearch highlights that summary sentences aggre-\ngate several content units and judgements should\nbe initially provided for these before yielding a con-\nclusion at summary level. However its focus is on\nthe evaluation of content relevance. Recent work\nin the context of summary faithfulness evaluation\nassesses faithfulness of summary predicates and\narguments (Goyal and Durrett, 2021). Conciliating\nwith our results, they also show that fine-grained\nevaluation is beneficial. However, their approach is\nnot based on NLI; and requires syntactic analysis of\nsummary sentences and task specific human anno-\ntated data to train a classifier. Our approach is more\ngeneric and builds on existing resources. Contem-\nporary with our work, (Min et al., 2023) propose\nthe evaluation of Large Language Models (LLMs)\ngenerated biographies via their decomposition into\nsmaller content units (i.e., atomic facts). Their ap-\nproach is applied to factual descriptive generation.\nIn contrast, we evaluate hallucination detection in a\nvariety of summarisation tasks. For long dialogue\nsummarisation, (Lattimer et al., 2023) propose to\ndecompose the input into chunks, INFUSE could\nbe combined with a coarse chunk selection step.\nFiner-grained evaluation has also shown posi-\ntive results in the related task of claim verification\n(Chen et al., 2022; Kamoi et al., 2023). However,\nin the same way as current factuality evaluation\non LLM generated text (Min et al., 2023; Man-\nakul et al., 2023), they address more open-ended\ngeneration tasks where no ground truth input is as-\nsumed; their information source is either retrieved\nor parametric. We focus on NLI-based faithfulness\nevaluation from given input documents.\n7\nConclusions\nWe study existing NLI-based faithfulness evalua-\ntion approaches and propose a new one, INFUSE,\nthat works at finer-grained granularity levels for\ncomputing document-summary entailment judge-\nments. Our study shows that lower granularity via\npremises with variable size and summary sentence\nsplitting is key to achieve more accurate entailment\njudgements when using off-the-shelf NLI models.\nWe also introduce a new benchmark for long form\ninput and diverse summarisation tasks. Experimen-\ntal results show that INFUSE achieves superior\nperformance on evaluating faithfulness for diverse\nsummarisation tasks.\nLimitations\nINFUSE’s stopping criteria can fall into a local min-\nimum. In Table 4 (see Appendix E), we show the\naverage number of document sentences retrieved\nby INFUSE. It is evident that INFUSE incremen-\ntal context retrieval extracts more document sen-\ntences on XSumAG than on CNNDMAG. This\naligns with our analysis in Section 2.1 and the fact\nthat summaries in XSumAG are more abstractive\nthan those in CNNDMAG. However, it might still\nnot be enough, especially in XSumAG, where some\nsummary sentences indeed require more document\nsentences to form an entailing context. As a result,\nINFUSE performance is comparable to SENTLI\nwhich manually sets a fixed number of document\nsentences to be retrieved. This limitation can be\novercome by introducing a hyper-parameter to the\nstopping criterion (Section 3); for example, to stop\nexpanding the context when the neutral probability\nincreases only by a large margin. The stopping\ncriterion we adopt is simple but enough to show\nthat it is possible to improve faithfulness evaluation\nperformance when using off-the-shelf NLI models\nby allowing a variable premise size.\nAnother limitation of INFUSE is that it requires\nadditional calls to the NLI model. In Table 5, we\nshow for all the compared approaches the compu-\ntation cost of evaluating one summary sentence\nversus the achieved average performance. The re-\nversed reasoning re-weighting in INFUSE dou-\nbles the computation cost when compared with\nSENTLI and SUMMAC. However, in practice, it\nwould be possible to decrease the number of calls\nby using some heuristics to flag when it is neces-\nsary (or not). For instance, when the entailment\nscore is above some threshold the reversed direc-\ntion is not analysed; or the decision could be based\non whether the summary sentence fuses more than\none document sentence which can be computed\nbased on a cheap metric such as ROUGE. The\nautomatic stopping criteria requires a number of\nadditional calls given by the expected number of re-\ntrieved document sentences kavg taken as premise.\nThe complexity of inference for a context-level\napproach with a fixed number of retrieved sen-\ntences k, i.e., INFUSE-k or SENTLI, assuming\na standard transformer, is O(k2) whereas for IN-\nFUSE it is in O(k3\navg). If kavg is small enough and\nthere is variability in the number of retrieved docu-\nment sentences, which is the case in the analysed\nsummarisation tasks (see Table 4 in Appendix E),\nINFUSE can be competitive in terms of running\ntimes. Summary sentence splitting also adds an\nextra overhead; however, it will decrease summary\nsentence fusion of document sentences, i.e., fewer\ncases will need reversed reasoning and kavg will be\nsmaller. In terms of performance, the contribution\nof reversed reasoning and dynamic stopping can\nbe seen in Figure 6 in Appendix E. Although grid\nsearch for k will give the best possible k, this k\nvalue will be the same for all summary sentences\n(within a summary and within a dataset). In con-\ntrast, dynamic stopping lets each summary sen-\ntence be analysed with a different k value. Figure 6\nshows that INFUSE with dynamic stopping is bet-\nter than INFUSE-k for different values of k.\nAcknowledgements\nWe thank our reviewers for their constructive feed-\nback. We are grateful to Griffin Adams, Huan Koh,\nYulong Chen, and Yang Liu for making available\nthe annotated datasets that we include in Diver-\nSumm. We also thank Mirella Lapata for useful\nfeedback. We gratefully acknowledge the support\nof the UK Engineering and Physical Sciences Re-\nsearch Council (grant EP/L016427/1).\nReferences\nGriffin Adams, Bichlien Nguyen, Jake Smith, Yingce\nXia, Shufang Xie, Anna Ostropolets, Budhaditya\nDeb, Yuan-Jyue Chen, Tristan Naumann, and\nNoémie Elhadad. 2023. What are the desired charac-\nteristics of calibration sets? identifying correlates on\nlong form scientific summarization. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 10520–10542, Toronto, Canada. Association\nfor Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAndrew P Bradley. 1997. The use of the area under\nthe roc curve in the evaluation of machine learning\nalgorithms. Pattern recognition, 30(7):1145–1159.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020a.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020b.\nLanguage models are few-shot learners.\nIn Ad-\nvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMeng Cao, Yue Dong, and Jackie Cheung. 2022. Hal-\nlucinated but factual! inspecting the factuality of\nhallucinations in abstractive summarization. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3340–3354, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nShuyang Cao and Lu Wang. 2021a. CLIFF: contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 6633–6649. Association for Computa-\ntional Linguistics.\nShuyang Cao and Lu Wang. 2021b. CLIFF: Contrastive\nlearning for improving faithfulness and factuality in\nabstractive summarization. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6633–6649, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nJifan Chen, Aniruddh Sriram, Eunsol Choi, and Greg\nDurrett. 2022. Generating literal and implied sub-\nquestions to fact-check complex claims. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing, pages 3495–3516,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nYanran Chen and Steffen Eger. 2023. MENLI: Robust\nEvaluation Metrics from Natural Language Inference.\nTransactions of the Association for Computational\nLinguistics, 11:804–825.\nYiran Chen, Pengfei Liu, and Xipeng Qiu. 2021.\nAre factuality checkers reliable? adversarial meta-\nevaluation of factuality in summarization. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2021, pages 2082–2095, Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYulong Chen, Yang Liu, Ruochen Xu, Ziyi Yang, Chen-\nguang Zhu, Michael Zeng, and Yue Zhang. 2023.\nUniSumm and SummZoo: Unified model and diverse\nbenchmark for few-shot summarization. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12833–12855, Toronto, Canada. Association\nfor Computational Linguistics.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nProceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 2 (Short Papers), pages 615–621, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7358–7370, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nRodolfo Delmonte, Antonella Bristot, Marco Aldo Pic-\ncolino Boniforti, and Sara Tonelli. 2007. Entailment\nand anaphora resolution in RTE3. In Proceedings of\nthe ACL-PASCAL Workshop on Textual Entailment\nand Paraphrasing, pages 48–53, Prague. Association\nfor Computational Linguistics.\nEsin Durmus, Faisal Ladhak, and Tatsunori Hashimoto.\n2022. Spurious correlations in reference-free evalu-\nation of text generation. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1443–\n1454, Dublin, Ireland. Association for Computational\nLinguistics.\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David\nReitter. 2022. Evaluating attribution in dialogue sys-\ntems: The BEGIN benchmark. Transactions of the\nAssociation for Computational Linguistics, 10:1066–\n1083.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1074–1084, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAlexander R. Fabbri, Wojciech Kry´sci´nski, Bryan Mc-\nCann, Caiming Xiong, Richard Socher, and Dragomir\nRadev. 2021. SummEval: Re-evaluating summariza-\ntion evaluation. Transactions of the Association for\nComputational Linguistics, 9:391–409.\nYanjun Gao, Chen Sun, and Rebecca J. Passonneau.\n2019. Automated pyramid summarization evaluation.\nIn Proceedings of the 23rd Conference on Computa-\ntional Natural Language Learning (CoNLL), pages\n404–418, Hong Kong, China. Association for Com-\nputational Linguistics.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal,\nPawan\nSasanka\nAmmanamanchi,\nAnuoluwapo\nAremu,\nAntoine\nBosselut,\nKhy-\nathi Raghavi Chandu, Miruna-Adriana Clinciu,\nDipanjan Das, Kaustubh Dhole, Wanyu Du, Esin\nDurmus, Ondˇrej Dušek, Chris Chinenye Emezue,\nVarun\nGangal,\nCristina\nGarbacea,\nTatsunori\nHashimoto, Yufang Hou, Yacine Jernite, Harsh Jham-\ntani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv\nKumar, Faisal Ladhak, Aman Madaan, Mounica\nMaddela, Khyati Mahajan, Saad Mahamood, Bod-\nhisattwa Prasad Majumder, Pedro Henrique Martins,\nAngelina McMillan-Major, Simon Mille, Emiel van\nMiltenburg, Moin Nadeem, Shashi Narayan, Vitaly\nNikolaev, Andre Niyongabo Rubungo, Salomey\nOsei,\nAnkur\nParikh,\nLaura\nPerez-Beltrachini,\nNiranjan Ramesh Rao, Vikas Raunak, Juan Diego\nRodriguez,\nSashank\nSanthanam,\nJoão\nSedoc,\nThibault Sellam, Samira Shaikh, Anastasia Shimo-\nrina, Marco Antonio Sobrevilla Cabezudo, Hendrik\nStrobelt, Nishant Subramani, Wei Xu, Diyi Yang,\nAkhila Yerukola, and Jiawei Zhou. 2021.\nThe\nGEM benchmark:\nNatural language generation,\nits evaluation and metrics. In Proceedings of the\n1st Workshop on Natural Language Generation,\nEvaluation, and Metrics (GEM 2021), pages 96–120,\nOnline. Association for Computational Linguistics.\nSebastian Gehrmann, Elizabeth Clark, and Thibault Sel-\nlam. 2023. Repairing the cracked foundation: A sur-\nvey of obstacles in evaluation practices for generated\ntext. J. Artif. Int. Res., 77.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109, Brussels, Belgium. Association for Com-\nputational Linguistics.\nJohn Glover, Federico Fancellu, Vasudevan Jagan-\nnathan, Matthew R. Gormley, and Thomas Schaaf.\n2022. Revisiting text decomposition methods for\nNLI-based factuality scoring of summaries. In Pro-\nceedings of the 2nd Workshop on Natural Language\nGeneration, Evaluation, and Metrics (GEM), pages\n97–105, Abu Dhabi, United Arab Emirates (Hybrid).\nAssociation for Computational Linguistics.\nTanya Goyal and Greg Durrett. 2021. Annotating and\nmodeling fine-grained factuality in summarization.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1449–1462, Online. Association for Computa-\ntional Linguistics.\nMax Grusky, Mor Naaman, and Yoav Artzi. 2018.\nNewsroom: A dataset of 1.3 million summaries with\ndiverse extractive strategies. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 708–719, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2022. LongT5: Efficient text-to-text transformer for\nlong sequences. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 724–\n736, Seattle, United States. Association for Compu-\ntational Linguistics.\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai\nTaitelbaum, Doron Kukliansy, Vered Cohen, Thomas\nScialom, Idan Szpektor, Avinatan Hassidim, and\nYossi Matias. 2022. TRUE: Re-evaluating factual\nconsistency evaluation. In Proceedings of the Second\nDialDoc Workshop on Document-grounded Dialogue\nand Conversational Question Answering, pages 161–\n175, Dublin, Ireland. Association for Computational\nLinguistics.\nDandan Huang, Leyang Cui, Sen Yang, Guangsheng\nBao, Kun Wang, Jun Xie, and Yue Zhang. 2020.\nWhat have we achieved on text summarization? In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 446–469, Online. Association for Computa-\ntional Linguistics.\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\nJi, and Lu Wang. 2021. Efficient attentions for long\ndocument summarization. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1419–1436, Online.\nAssociation for Computational Linguistics.\nRyo Kamoi, Tanya Goyal, Juan Diego Rodriguez, and\nGreg Durrett. 2023. WiCE: Real-world entailment\nfor claims in Wikipedia. In Proceedings of the 2023\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 7561–7583, Singapore. As-\nsociation for Computational Linguistics.\nHuan Yee Koh, Jiaxin Ju, He Zhang, Ming Liu, and\nShirui Pan. 2022. How far are we from robust long\nabstractive summarization? In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2682–2698, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nWojciech Kryscinski, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2020. Evaluating the factual\nconsistency of abstractive text summarization. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9332–9346, Online. Association for Computa-\ntional Linguistics.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-visiting NLI-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nBarrett Martin Lattimer, Patrick Chen, Xinyuan Zhang,\nand Yi Yang. 2023. Fast and accurate factual in-\nconsistency detection over long documents. In Pro-\nceedings of the 2023 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2023,\nSingapore, December 6-10, 2023, pages 1691–1703.\nAssociation for Computational Linguistics.\nLogan Lebanoff, Kaiqiang Song, Franck Dernoncourt,\nDoo Soon Kim, Seokhwan Kim, Walter Chang, and\nFei Liu. 2019. Scoring sentence singletons and pairs\nfor abstractive summarization. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 2175–2189, Florence, Italy.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nYang Liu and Mirella Lapata. 2019. Text summariza-\ntion with pretrained encoders. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 3730–3740, Hong Kong,\nChina. Association for Computational Linguistics.\nBill MacCartney and Christopher D. Manning. 2009.\nAn extended model of natural logic. In Proceed-\nings of the Eight International Conference on Com-\nputational Semantics, pages 140–156, Tilburg, The\nNetherlands. Association for Computational Linguis-\ntics.\nPotsawee Manakul, Adian Liusie, and Mark J. F. Gales.\n2023. Selfcheckgpt: Zero-resource black-box hal-\nlucination detection for generative large language\nmodels. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Process-\ning, EMNLP 2023, Singapore, December 6-10, 2023,\npages 9004–9017. Association for Computational\nLinguistics.\nDaniel Marcu. 2001. Discourse-based summarization\nin duc-2001.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nNick McKenna, Tianyi Li, Liang Cheng, Moham-\nmad Javad Hosseini, Mark Johnson, and Mark Steed-\nman. 2023. Sources of hallucination by large lan-\nguage models on inference tasks. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2023, Singapore, December 6-10, 2023, pages 2758–\n2774. Association for Computational Linguistics.\nSewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis,\nWen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle-\nmoyer, and Hannaneh Hajishirzi. 2023. FActScore:\nFine-grained atomic evaluation of factual precision\nin long form text generation. In Proceedings of the\n2023 Conference on Empirical Methods in Natural\nLanguage Processing, pages 12076–12100, Singa-\npore. Association for Computational Linguistics.\nRamesh Nallapati, Bowen Zhou, Cicero dos Santos,\nÇaglar Gulçehre, and Bing Xiang. 2016. Abstrac-\ntive text summarization using sequence-to-sequence\nRNNs and beyond.\nIn Proceedings of the 20th\nSIGNLL Conference on Computational Natural Lan-\nguage Learning, pages 280–290, Berlin, Germany.\nAssociation for Computational Linguistics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don’t give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1797–1807, Brussels, Bel-\ngium. Association for Computational Linguistics.\nAni Nenkova, Rebecca Passonneau, and Kathleen McK-\neown. 2007. The pyramid method: Incorporating\nhuman content selection variation in summarization\nevaluation.\nACM Trans. Speech Lang. Process.,\n4(2):4–es.\nYixin Nie, Haonan Chen, and Mohit Bansal. 2019.\nCombining fact extraction and verification with\nneural semantic matching networks. Proceedings\nof the AAAI Conference on Artificial Intelligence,\n33(01):6859–6866.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In Advances in Neural\nInformation Processing Systems 35: Annual Confer-\nence on Neural Information Processing Systems 2022,\nNeurIPS 2022, New Orleans, LA, USA, November 28\n- December 9, 2022.\nArtidoro Pagnoni, Vidhisha Balachandran, and Yulia\nTsvetkov. 2021. Understanding factuality in abstrac-\ntive summarization with FRANK: A benchmark for\nfactuality metrics. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2021, Online, June\n6-11, 2021, pages 4812–4829. Association for Com-\nputational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and\nChristopher D. Manning. 2020. Stanza: A Python\nnatural language processing toolkit for many human\nlanguages. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics:\nSystem Demonstrations.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nLeonardo F. R. Ribeiro, Mengwen Liu, Iryna Gurevych,\nMarkus Dreyer, and Mohit Bansal. 2022. FactGraph:\nEvaluating factuality in summarization with semantic\ngraph representations. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3238–3253, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nTal Schuster, Sihao Chen, Senaka Buthpitiya, Alex\nFabrikant, and Donald Metzler. 2022. Stretching\nsentence-pair NLI models to reason over long doc-\numents and clusters. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n394–412, Abu Dhabi, United Arab Emirates. Associ-\nation for Computational Linguistics.\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021.\nGet your vitamin C! robust fact verification with\ncontrastive evidence. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 624–643, Online. As-\nsociation for Computational Linguistics.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. QuestEval: Summariza-\ntion asks for fact-based evaluation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6594–6604, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073–\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nOri Shapira, David Gabay, Yang Gao, Hadar Ronen, Ra-\nmakanth Pasunuru, Mohit Bansal, Yael Amsterdamer,\nand Ido Dagan. 2019. Crowdsourcing lightweight\npyramids for manual summary evaluation. In Pro-\nceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long and Short Papers), pages 682–687, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJulius Steen, Juri Opitz, Anette Frank, and Katja Mark-\nert. 2023. With a little push, NLI models can robustly\nand efficiently predict faithfulness. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 2: Short Papers),\npages 914–924, Toronto, Canada. Association for\nComputational Linguistics.\nLiyan Tang, Tanya Goyal, Alex Fabbri, Philippe La-\nban, Jiacheng Xu, Semih Yavuz, Wojciech Kryscin-\nski, Justin Rousseau, and Greg Durrett. 2023. Un-\nderstanding factual errors in summarization: Errors,\nsummarizers, datasets, error detectors. In Proceed-\nings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 11626–11644, Toronto, Canada. Association\nfor Computational Linguistics.\nSimone Teufel and Hans van Halteren. 2004. Evaluat-\ning information content by factoid analysis: Human\nannotation and stability. In Proceedings of the 2004\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 419–426, Barcelona, Spain.\nAssociation for Computational Linguistics.\nPrasetya Utama, Joshua Bambrick, Nafise Moosavi,\nand Iryna Gurevych. 2022. Falsesum: Generating\ndocument-level NLI examples for recognizing fac-\ntual inconsistency in summarization. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2763–2776,\nSeattle, United States. Association for Computational\nLinguistics.\nHans van Halteren and Simone Teufel. 2003. Examin-\ning the consensus between human summaries: initial\nexperiments with factoid analysis. In Proceedings of\nthe HLT-NAACL 03 Text Summarization Workshop,\npages 57–64.\nDhruv Verma, Yash Kumar Lal, Shreyashee Sinha, Ben-\njamin Van Durme, and Adam Poliak. 2023. Evalu-\nating paraphrastic robustness in textual entailment\nmodels. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 880–892, Toronto,\nCanada. Association for Computational Linguistics.\nEllen Voorhees. 2004. Overview of the trec 2003 ques-\ntion answering track.\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020.\nAsking and answering questions to evaluate the fac-\ntual consistency of summaries. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5008–5020, Online. Asso-\nciation for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nWen Xiao, Iz Beltagy, Giuseppe Carenini, and Arman\nCohan. 2022. PRIMERA: Pyramid-based masked\nsentence pre-training for multi-document summariza-\ntion. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 5245–5263, Dublin,\nIreland. Association for Computational Linguistics.\nWenpeng Yin, Dragomir Radev, and Caiming Xiong.\n2021. DocNLI: A large-scale dataset for document-\nlevel natural language inference.\nIn Findings of\nthe Association for Computational Linguistics: ACL-\nIJCNLP 2021, pages 4913–4922, Online. Association\nfor Computational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter\nLiu. 2020. PEGASUS: Pre-training with extracted\ngap-sentences for abstractive summarization. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, volume 119 of Proceedings of\nMachine Learning Research, pages 11328–11339.\nPMLR.\nMing Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia\nMutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli\nCelikyilmaz, Yang Liu, Xipeng Qiu, and Dragomir\nRadev. 2021. QMSum: A new benchmark for query-\nbased multi-domain meeting summarization. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n5905–5921, Online. Association for Computational\nLinguistics.\nMultinews\nQMSUM\nArXiv\n \n71.0%\n \n22.3%\n \n6.8%\n28%\n17%\n32%\n23%\n \n66.2%\n \n28.7%\n \n5.1%\n36%\n17%\n21%\n26%\n \n87.3%\n \n11.7%\n \n1.0%\n46%\n13%\n16%\n25%\n>15 sentences\n<15 sentences\n<10 sentences\n<5 sentences\n1 sentence\n2 sentences\n>2 sentences\nFigure 4: Statistics for the number of fused document\nsentences (the pie charts) and their distances (the blue\nvertical bars) on qmsum, multinews, and arxiv (Diver-\nSumm).\nA\nAdditional Dataset Details\nAnnotated Sets in AggreFact\nFactCC by\nKryscinski et al. (2020) and SummEval Fabbri\net al. (2021) are annotated at summary level.\nFactCC uses a binary consistency label (consis-\ntent/inconsistent). SummEval uses a 5-point Likert\nscale where only a score of 5 is treated as correct\nwhile the rest are considered incorrect.\nThe annotation of Wang’20 Wang et al. (2020)\nand FRANK Pagnoni et al. (2021) operates at sen-\ntence level. Wang’20 employs a binary consistency\nlabel (consistent/inconsistent). A summary is la-\nbelled as faithful (consistent) if all of its sentences\nare labelled as consistent. The annotation scheme\nin FRANK highlights faithfulness error types (see\nError Types in Section 2) in summary sentences.\nSummaries in FRANK are considered to be faithful\nif none of their sentences are annotated with errors.\nPolytope (Huang et al., 2020), XSumFaith\nMaynez et al. (2020) and Goyal’21 Goyal and Dur-\nrett (2021) are annotated at span level. Polytope\nidentifies various error types such as addition, omis-\nsion, and intrinsic inaccuracies. The annotation of\nXSumFaith revolves around error types like intrin-\nsic and extrinsic. Goyal’21 classifies the error types\ninto intrinsic, extrinsic × entity, event, noun phrase.\nSummaries devoid of these errors are marked as\nfaithful.\nCLIFF Cao and Wang (2021b) is annotated at\nword level and its annotation scheme accounts for\ninstinct/extrinsic hallucinations and lack of world\nknowledge. Cao’22 by Cao et al. (2022) anno-\ntates entities and categorizes incorrect entities into\nfactual/non-factual/instinct hallucinations. Sum-\n57.5\n32.5\n7.6\n2.4\nChemSum\n62.7\n32.3\n4.1\n0.8\nQMSUM\n45.5\n32.8\n16.0\n5.8\nMultinews\n54.7\n32.5\n8.9\n3.9\nArXiv\n50.1\n30.7\n13.6\n5.6\nGovReport\n75.9\n19.5\n3.6\n1.0\nXSUMAG\n76.5\n19.8\n3.3\n0.5\nCNNDMAG\nLabels\nNo Split\n2 Sentences\n3 Sentences\nMore than 3 sentences\nFigure 5: Distribution of number of splits occurring in summary sentences.\nSentence\nSub-sentences\nHeritage auctions offered the gray jacket featuring\na black zigzag applique\nHeritage auctions offered the gray jacket.\nThe gray jacket featured a black zigzag applique.\nS.t. Mirren have signed striker Jeremy Clarkson\non a season-long loan from Dundee.\nS.t. Mirren have signed striker Jeremy Clarkson.\nThe striker is on a season-long loan from Dundee.\nChange is a problem for many disabled people.\nChange is a problem for many disabled people.\nTable 3: Examples of original sentences and their rewritten sentences for sub-sentence reasoning.\nmaries devoid of these errors are marked as faith-\nful.\nFor details on the annotation process, we refer\nthe reader to Aggrefact (Tang et al., 2023).\nLicense\nNo license is found for AggreFact, Gov-\nReport and ChemSumm. ArXiv is under Apache-\n2.0 license. QMSUM and MultiNews are under\nMIT License. We ensure that the data was used\nsolely for academic purposes, which aligns with the\nintended use of these datasets. For data safety, con-\ntent filtering was conducted when the creators built\nthe original datasets. It is not avoidable that some\ndocuments can contain uncomfortable content, in-\ncluding news coverage of crimes and wars. For the\nmodel-generated summaries annotated with human\njudgements collected from (Chen et al., 2023; Koh\net al., 2022; Adams et al., 2023) to create Diver-\nSumm, we download some sets from their corre-\nsponding online download link and make others\ndirectly facilitated by the authors available in our\ngithub.6 We obtained permission from the authors\nfor their use and encourage citation of the sets’\ncorresponding work upon their future use within\nDiverSumm. We use these annotated sets only for\nresearch purposes.\nB\nTraining Configurations\nModels\nWe\nuse\nthe\npublicly-available\nhttps://huggingface.co/tals/\nalbert-xlarge-vitaminc-mnli NLI model. We\nuse the tokenizer from Stanza (Qi et al., 2020).\n6https://huggingface.co/datasets/griffin/\nChemSum, https://github.com/huankoh/How-Far-are-We-from-\nRobust-Long-Abstractive-Summarization.\nModels\nXSMAG\nCNDAG\nCSM\nQMS\nAXV\nGOV\nMNW\nINFUSE\n2.66±1.67 1.79±1.04 2.50 ±4.76 2.55 ±1.48 4.89 ±9.25 2.11±1.21 1.98 ±1.25\nINFUSESUB 2.40±1.57 1.22±1.75 2.12 ±3.79 2.41 ±1.43 4.09 ±8.26 1.92 ±1.11 1.76 ±1.09\nTable 4: Average number of retrieved document sentences and standard deviation for INFUSE and INFUSESUB on\nAggreFact and DiverSumm.\nApproach\nAUC Nb. calls to NLI\nFULLDOC\n58.74\n1\nSUMMACCONV 60.54\nM+C\nSUMMACZS\n63.78\nM\nSENTLI\n62.56\nM+1\nINFUSE-k\n65.01\n2M+1\nINFUSE\n65.84\n2M+kavg+1\nTable 5:\nPerformance / Computation trade-off. We\nreport the AUC versus the number of calls to the NLI\nmodel. M is the number of document sentences. kavg is\nthe expected number of retrieved document sentences\nwhich can entail the summary sentence. C is the call to\nthe convolution layer.\nFigure 6: Performance over retrieval size k. We report\nthe average ROC-AUC on AggreFact and DiverSumm.\nOriginally SUMMACZS uses the combination of\nentailment - contradiction which was found to per-\nform better (Laban et al., 2022). However, we\nfind that in both AggreFact and DiverSumm, by\ntaking only the entailment score SUMMACZS ob-\ntains a much better performance. Thus, we only\nuse entailment scores. In addition, the implementa-\ntion of SUMMAC ignores those document sentences\nwith less than 10 tokens and only considers the\nfirst 100 sentences of the document. We remove\nsuch constraints for a fair comparison. In addition,\nSUMMAC obtains better performance without these\nconstraints.\nSentence Splitting\nKamoi et al. (2023) propose\na dataset, namely WiCE, including original claim\nsentences paired with their decomposition (split)\ninto more than one sentence generated by GPT-3\n(Brown et al., 2020b). We leverage such parallel\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n5\n10\n15\n20\n25\n30\n35\n40\nCount\n(a) CircE\n0.0\n0.2\n0.4\n0.6\n0.8\nScore\nFULL DOC\nSENTLI\nINFUSE\nINFUSESUB\n0\n5\n10\n15\n20\n25\nCount\n(b) PredE\nFigure 7: Distribution of entailment scores on correct\nand different error types for arXiv and GovReport from\nDiverSumm. The x-axe corresponds to the NLI-based\napproach. That is, FULLDOC in red, SENTLI in green,\nINFUSE in cyan, and INFUSESUB in purple. The y-\naxe corresponds to the entailment scores (i.e., values\nranging in [0,1]), and the z-axe corresponds to the count\nof instances.\ndata to train a sentence splitting model for sub-\nsentence reasoning based on T5-large (Raffel et al.,\n2020). We fine-tune T5-large for 5 epochs with a\nbatch size of 32 and a learning rate 5e-4. We force\nthe length of the output to be within [3, 128]. We\nshow a few sentence splitting examples in Table\n3. Figure 5 shows the distribution of the number\nof splits that summary sentences had. We train the\nmodel on an A6000 GPU and each epoch costs 90\nseconds. The inference time is around 8 sample per\nsecond.\nXSMAG\nCNDAG\nCSM\nQMS\nAXV\nGOV\nMNW\nAVG\nCONTEXT\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nSENT\nSUB\nFULLDOC\n72.77 73.63 64.40 63.68 50.15 58.72 37.12 39.76 62.78 62.46 79.19 77.69 44.76 46.72 58.74 60.38\nSUMMACCONV 67.76 65.77 72.14 70.84 53.14 51.10 51.13 54.42 61.22 44.26 65.34 81.58 53.05 56.27 60.54 60.61\nSUMMACZS\n70.29 66.67 74.54 74.98 54.41 57.32 48.21 51.42 69.44 67.26 79.37 81.09 50.17 54.20 63.78 64.71\nSENTLI\n73.61 71.45 75.83 74.66 50.13 55.69 47.56 51.88 64.49 76.35 79.68 77.65 46.61 43.61 62.56 64.47\nINFUSE\n73.42 73.21 76.21 73.34 54.11 59.26 52.16 53.20 71.38 73.89 80.45 80.05 53.16 49.37 65.84 66.05\nTable 6: Results for all summarisation tasks in AggreFact and DiverSumm combined with summary sentence\nsplitting (SUB column). For AggreFact, we report the average results for XSum (XSM; 5 datasets) and CNN/DM\n(CND; 7 datasets), respectively; dataset-level performance can be found in Appendix D. CSM, MNW, QMS, AXV,\nand GOV refer to ChemSumm, MultiNews, QMSUM, ArXiv, and GovReport respectively. We highlight highest\nscores and scores significantly different from FULLDOC, all SUMMAC variants and SENTLI models (at p < .05,\nusing pairwise t-test). We additionally highlight in olive improved scores for existing approaches when combined\nwith summary sentence splitting.\nC\nSummary Sentence Splitting is\nBeneficial for All Approaches\nTable 6 shows additional results when we com-\nbine the proposed summary sentence splitting step\nwith the different approaches to build a premise.\nWe can see that sub-sentence (SUB column in Ta-\nble 6) brings improvements across all of them (as\ndiscussed before with the exception of CNN/DM).\nSub-sentence evaluation brings improvements for\nsentence-level premises such as SUMMAC in par-\nticular for the version that relies on a convolutional\nneural network trained to map the distribution of\nentailment scores to correct/incorrect judgements.\nAfter splitting there is less content fusion from\ndocument sentences and more feasible to judge\nentailment with one document sentence.\nFor ArXiv and CSM context-level works bet-\nter indicating that neither one sentence nor the en-\ntire document provide adequate context even after\nsummary sentence splitting. For XSum, the most\nabstractive dataset with short input documents,\nthe document-level (FULLDOC) and context-level\n(INFUSE and SENTLI) premises work well. For\nthis dataset sentence-level approaches (SUMMAC)\neven with sentence splitting are not enough. Over-\nall, INFUSE and INFUSESUB perform the best, this\nshows that the variable context allows to account\nfor different levels of document sentence fusion.\nD\nDataset-Level Performance on\nAggreFact\nWe show detailed results for AggreFact in Table\n7. Statistical significance of INFUSE w.r.t. to the\nother best performing approaches are computed as\ndescribed in Section 5.1. Overall, there is no sig-\nnificant difference among INFUSE , SENTLI, and\nFULLDOC on XSumAG and CNNDMAG. Inter-\nestingly, the models exhibit different performance\nwithin subsets of the tasks. INFUSE is significantly\nbetter on Wand’20, CLIFF, and FactCC.\nE\nPerformance per Premise Sizes\nFigure 6 shows the evaluation performance (ROC-\nAUC) for different premise sizes k (i.e., number of\ndocument sentences). It includes SENTLI , a vari-\nant of INFUSE with a fixed retrieval size (INFUSE-\nk), and INFUSE without reverse reasoning. As can\nbe seen, reversed reasoning helps to produce better\nentailment judgements as there is a performance\ndegradation when we remove it from INFUSE. In-\ncremental reasoning allows INFUSE to determine\nwhen to stop automatically, removing the require-\nment of additional data for optimizing the retrieval\nsize k which has a substantial impact on model\nperformance.\nTable 4 shows the average premise size, in num-\nber of document sentences, at which INFUSE and\nINFUSESUB work. We can see that there is con-\nsiderable variability in the number of retrieved sen-\ntences within and across tasks. This further sup-\nports the difference in performance between IN-\nFUSE and INFUSE-k.\nF\nPerformance per Error Types\nFigure 7 shows two additional graphs for CircE\nand PredE error types. Similarly to EntE (Sec-\ntion 5.2), INFUSE and INFUSE perform better\nthan SENTLI which assigns scores mainly in the\n[0.4, 0.6] interval. INFUSE performs better than\nINFUSESUB. We show examples of cases correctly\n(∼ 0) and incorrectly (∼ 1) judged by INFUSE in\nTable 10.\nModels\nXSum Test\nCNN/DM Test\nWang’20 Cao’22 XSF Goyal’21 CLF AVG FCC Wang’20 SEV PTP\nFRK Goyal’21 CLF AVG\nFULLDOC\n64.62\n71.50 75.76\n74.70\n77.34 72.78 75.63\n84.09\n74.07 76.69 65.26\n4.17\n70.90 64.40\nSUMMACCONV\n69.59\n69.71 70.03\n56.40\n73.09 67.76 92.22\n76.67\n85.48 81.67 76.72\n25.00\n67.20 72.14\nSUMMACZS\n73.77\n67.27 72.73\n61.58\n76.11 70.29 93.72\n80.94\n87.57 88.57 77.22\n25.00\n68.81 74.54\nSENTLI\n72.80\n70.57 69.46\n74.88\n80.34 73.61 92.26\n80.04\n87.75 92.82 79.92\n20.83\n77.23 75.83\nINFUSE\n76.41\n67.73 74.01\n71.43\n77.51 73.42 94.99\n80.21\n88.65 92.84 79.48\n20.83\n76.45 76.21\nINFUSESUB\n73.76\n69.92 74.69\n66.34\n81.36 73.21 92.73\n78.66\n87.76 83.68 77.76\n16.67\n72.82 72.87\nTable 7: Dataset-level performance on AggreFact. For XSum Test, XSF and CLF refer to XSumFaith and CLIFF,\nrespectively. PTP, FCC, SEV and FRK refer to Polytope, FactCC, SummEval and FRANK, respectively. We\nhighlight highest scores and scores significantly different from FULLDOC, all SUMMAC and SENTLI models (at\np < .05, using pairwise t-test).\nG\nCase Studies\nSentence Fusion\nTo illustrate how sentence fu-\nsion renders difficult the assessment of entailment\nby current sentence-level NLI models, we provide\ntwo representative examples from the faithfulness\nevaluation benchmarks in Table 8.\n0\n20\n40\n60\n80\n100\nROUGE-2 Recall\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nEntailment Score\nFigure 8: ROUGE-2 Recall versus Entailment Score\non summary sentences labelled as unfaithful from the\nArXiv and GovReport datasets.\n2\n4\n6\n8\n10\nRetrieved Document Sentences k\n30\n32\n34\n36\n38\nAverage Entailment Score\n29.02\n32.34\n33.4\n34.82\n35.44\n36.51 36.63 36.77\n37.36\n36.95\nFigure 9: Average entailment score for summary sen-\ntences labelled as unfaithful from the ArXiv and Gov-\nReport datasets. Premise size, in number of retrieved\ndocument sentences, ranges from 1 to 10.\nThe first example, taken from FactCC (Kryscin-\nski et al., 2020), shows a summary that is simply a\nshort version of one document sentence. In these\ncases, a sentence-level NLI evaluator (Laban et al.,\n2022; Nie et al., 2020) would capture the relation\nand assign a high entailment score.\nIn contrast, the second example from CAO’22\n(Cao et al., 2022) is more complex: the content\nconveyed in the summary sentence fuses content\nincluded in multiple document sentences. In this\nsituation, three possibilities arise. First, if the sum-\nmary sentence is more informative than a document\nsentence and the part that overlaps is a paraphrase,\nit can be captured by applying NLI in reversed di-\nrection (i.e., summary-to-document, MSS |= DS\ncolumn in Table 8). Examples of this scenario are\ntext segments highlighted in cyan. Second, if none\nof the inference directions (neither document-to-\nsummary, DS |= MSS column in Table 8, nor MSS\n|= DS) achieve a high entailment score individu-\nally, the combined score may still be relatively high\nallowing the bidirectional method in INFUSE to\ncapture such cases as illustrated by the example\nin green. Third, a content unit in a complex and\ninformative summary sentence is entailed by a con-\ntent unit in a complex document sentence they only\noverlap on this content unit. It is possible that the\nmethod will fail in these cases, as the sentence\nsegments in violet illustrate.\nHigh Reversed Reasoning Scores\nTable 9 shows\nexamples of document and summary sentence in-\nference applied in both the standard and reversed\ndirection (lines 2 and 3 in Algorithm 1). These ex-\namples are taken from summaries annotated as (cor-\nrect) faithful. In particular, these show cases where\nthe reversed direction yields high entailment scores.\nThese are cases where the summary sentence is pro-\nDS |= MSS\nMSS |= DS\nD\nSao Paulo, Brazil (CNN)Brazilian supermodel Gisele Bundchen sashayed down\nthe catwalk at Sao Paulo Fashion Week on Wednesday night in an emotional\nfarewell to the runway.\n.003\n.003\nBundchen announced over the weekend that she would be retiring from the\ncatwalk, though not the fashion industry.\n.004\n.003\nThe 34-year-old, who is married to New England Patriots quarterback Tom Brady\nand has two children, has said she wants to spend more time with her family.\n.001\n.001\nOn Wednesday night, Brady had a front-row seat at what was hailed as a historic\nmoment in Brazil’s fashion world.\n.006\n.003\nBundchen wrote about her fashion career on her Instagram account: \"I am\ngrateful that at 14, I was given the opportunity to start this journey.\n.996\n.002\nToday after 20 years in the industry, it is a privilege to be doing my last fashion\nshow by choice and yet still be working in other facets of the business.\"\n.002\n.001\nMSS\nbundchen wrote about her fashion career on her instagram account.\nD\nDavid Lipton, second in command at the IMF, outlined some of these risks in a\nspeech to the National Association for Business Economics in Washington on\nTuesday.\n.018\n.001\n\"The IMF’s latest reading of the global economy shows once again a weakening\nbaseline,\" he said.\n.103\n.006\n\"We are clearly at a delicate juncture.\"\n.020\n.212\nThe comments come after weaker-than-expected trade figures from China show-\ning that exports plunged by a quarter from a year ago.\n.004\n.001\nThe IMF has already said it is likely it will downgrade its current forecast of\n3.4% for global growth when it next releases its economic predictions in April.\n.050\n.020\nThe dismal picture is one that has on-going ramifications for businesses and\nindustries that bet on China’s growth story.\n.002\n.003\nRead more from Karishma:\n.002\n.004\nWhy a story about bulk shipping matters.\n.002\n.019\nMSS\nThe head of the International Monetary Fund (IMF) has warned that the global economy is \"at a delicate\njuncture\" and that the outlook for global growth is \"deteriorating\".\nTable 8: We show input Document (D), Model-generated Summary Sentence (MSS), and DS |= MSS (Document\nSentence -DS- to summary sentence reasoning) and MSS |= DS (reversed reasoning) scores. We highlight content\nsegments in summary sentences and their corresponding document evidence in violet, cyan and green. The example\nin the top part is from FactCC (Kryscinski et al., 2020) in CNNDMAG and the second is from CAO’22 (Cao et al.,\n2022) in XSUMAG. Both labelled as faithful (correct) summaries.\nviding more details due to sentence fusion. For in-\nstance, in the third example, the summary sentence\nis adding extra information (taken from other docu-\nment sentences) about Paulo Duarte being Burkina\nFaso’s coach. Note that in some cases sentences\ncontain pronouns and thus they should not lead to\nhigh entailment scores because the referent is un-\nknown (Delmonte et al., 2007). However, the NLI\nmodel is biased because of the premise-hypothesis\nlength and token overlap (McKenna et al., 2023;\nVerma et al., 2023).\n∼ 0 and ∼ 1 Entailment Scores on Different\nError Types\nTable 10 shows examples of IN-\nFUSE working on FRANK, ArXiv, and GovRe-\nport (Section 5.2). The top part of the table includes\ncases where INFUSE successfully assigns close to\nzero scores to unfaithful cases per error type and the\nbottom part illustrates those scenarios where it fails\nto identify the error. On manual inspection, we find\nthat in many cases these failures are related to high\nlexical overlap and premise-hypothesis length bias\n(McKenna et al., 2023; Verma et al., 2023). Fig-\nure 8 and Figure 9 show this trend for all unfaithful\nsentences in the ArXiv and GovReport subsets. We\nobserve a similar trend in all datasets in AggreFact\nand DiverSumm but only these two datasets have\nsentence level annotation.\nIn Figure 8, we analyse entailment scores for\npremise-hypothesis pairs in relation to their lexical\nDS\nMSS\nDS |= MSS\nMSS |= DS\nHe resigned from his post in order to make\nthis appearance.\nA police chief resigned from his post to ap-\npear on bbc question time.\n.003\n.938\nWe will be making no appeal.\nWigan warriors will not appeal against the\neight-game ban given to ben flower for\npunching st helens prop lance hohaia.\n.004\n.930\n\"I am confident they can recover in time,\"\nDuarte insisted.\nBurkina faso coach paulo duarte says he\nis confident his players will be fit for next\nmonth’s africa cup of nations.\n.013\n.388\nIn a statement the company said the blaze\nhad affected an estimated 1,000-2,000\ntonnes of recycled wood chip.\nFirefighters are continuing to tackle a blaze\nat a wood chip recycling plant in Bridgend\ncounty which has destroyed up to 2,000\ntonnes of wood chip.\n.019\n.067\nDecisions about which people, groups, or\nevents to memorialize are made by many dif-\nferent entities, including Congress, federal\nagencies, state and local governments, and\nprivate citizens, among others.\nDecisions about which people, groups (or\nevents), and which places to memorialize,\nare made by many different entities, in-\ncluding Congress, federal agencies, state\nand local governments, and private citizens,\namong others.\n.091\n.980\nNOAA has defined natural infrastructure and\nnature-based infrastructure in NOAA Ad-\nministrative Order (NAO) 216-117: NOAA\nNational Habitat Policy.\nNOAA’s National Habitat Policy (NAO 216-\n117) directs the agency to protect, maintain,\nand restore ocean, coastal, and Great Lakes\necosystems by \"applying natural and natural\ninfrastructure,\" among other activities.\n.007\n.685\nThis report considers the extent of federal\ninvolvement in memorials located outside\nthe District of Columbia (Washington, DC).\nThis report considers the extent of federal\ninvolvement in national memorials located\noutside the District of Columbia (Washing-\nton, DC).\n.166\n.981\nIn the United States, there are hundreds, and\npossibly thousands, of memorials to various\nindividuals, groups, and events.\nIn the United States, there are hundreds, and\npossibly thousands, of memorials to various\nindividuals, group, and historical events.\n.224\n.989\nTable 9: Examples of reversed reasoning with high entailment scores. Document Sentence (DS), Model-generated\nSummmary Sentence (MSS), document to summary entailment (DS |= MSS), and reverse direction (MSS |= DS).\nAll examples are from summaries in the DiverSumm benchmark labelled as faithful (correct).\noverlap.7 We compute lexical overlap as ROUGE-\n2 Recall in order to capture phrase information. As\ncan be seen, on the left-bottom corner, a high per-\ncentage of pairs with low ROUGE-2 Recall obtain\na low entailment score. Another cluster of pairs\nis on the right-top corner where pairs with high\nlexical overlap get high entailment scores. This\nbehaviour of NLI models will undermine evalua-\ntion of summary faithfulness when summaries are\nabstract or have a high token overlap but differ in\nfew words that change the content conveyed in the\ninput document (e.g., negation). Figure 9 shows av-\nerage entailment scores in relation to premise size.\nThat is, we compute average entailment scores for\npremise-hypothesis pairs setting the premise to the\ntop k ranked document sentences; k takes values\nfrom 1 to 10. We can see that longer premises ob-\ntain higher entailment scores despite the fact that\n7Note that by premise we mean the premise selected by\nINFUSE.\nthey include document sentences further below in\nthe rank.\nDRS\nMSS\nError\nType\nEntailment Scores ∼ 0\nCosts for Group B benefits and administration are financed\nby the one-time appropriation of $4.6 billion provided in the\nZadroga Reauthorization Act of 2015.\nCosts for Group B benefits and administrative ex-\npenses were financed by a one-time appropriation\nof $3.\nEntE\nJan 2006 - Government proposes nuclear as part of future\nenergy mixMar 2013 - Construction of Hinkley Point approve-\ndOct 2013 - UK government agrees £92.50 per megawatt-hour\nwill be paid for electricity produced at the Somerset site -\naround double the current market rate at the timeOct 2015 -\nEDF signs investment agreement with China General Nuclear\nPower Corporation (CGN)July 2016 - EDF board approves\nfinal investment decision, but the UK Government postpones\na final decision on the project until autumn.\nThe government has given the go-ahead for a new\nnuclear power plant at a former nuclear plant in\nsomerset.\nPredE\nThe VCF was reauthorized in 2015 and, if not reauthorized in\nthe 116 th Congress, will sunset on December 18, 2020.\nThe MTF was reauthorized in 2015 and, if not reau-\nthorized, the current iteration will sunset on June\n18, 2017.\nCircE\nWhile men caregivers may face some of these risks, the effects\nof caregiving for women are compounded by lower average\nlifetime earnings and a longer life expectancy than men. As a\nresult, women caregivers are at an increased risk of outliving\ntheir savings.\nWomen caregivers were more likely than men care-\ngivers to be employed and to have higher levels\nof earnings, but women caregivers were also more\nlikely to work part-time and have lower levels of\nemployment and have less income.\nLinkE\nIn our December 2018 report, we found that TSA provides\npipeline operators with voluntary security guidelines that op-\nerators can implement to enhance the security of their pipeline\nfacilities.\nThe Transportation Security Administration (TSAO)\nprovides pipeline operators with voluntary security\nguidelines that operators can implement to enhance\nthe security of their pipeline facilities.\nCorefE\nSince fiscal year 2008, the United States has allocated about $3\nbillion for assistance for Mexico under the Mérida Initiative.\nYou asked us to review issues related to Mérida Initiative\nimplementation and objectives.\nSince fiscal year 2008, the United State has allo-\ncated about $3 billion for assistance for Mexico\nunder the Civil Standards Initiative.\nOutE\nIn July 2016, OMB issued an updated Circular No. A-123,\nManagement’s Responsibility for Enterprise Risk Manage-\nment and Internal Control, which requires executive agencies\nto implement enterprise risk management (ERM) in their man-\nagement practices. Since the July 2016 update to OMB Cir-\ncular No. A-123 required agencies to implement ERM, the\nAir Force has been leveraging and relying on its existing risk\nmanagement practices.\nIn July 2016, OMB issued an updated Circular No A.\nB, Management’s Responsibility for Enterprise Risk\nManagement and Internal Control, which requires\nexecutive [incomplete sentence]\nGramE\nEntailment Scores ∼ 1\nPractitioners and decisionmakers have been using the term\nnature-based infrastructure and supporting nature-based in-\nfrastructure features since at least the late 2000s (although\nthese types of features have been assigned various names over\ntime)\nPractitioners and decisionmakers have been using\nthe term nature-by-nature-infrastructure since at\nleast the late 2000s, although these types have been\nassigned various names over time.\nEntE\nMemorials with \"medium\" federal involvement typically either\nare located on federal land but do not receive federal funding,\nor are located on nonfederal land but receive assistance from a\nfederal agency.\nMemorials for purposes of \"medium\" involvement\nare either located on nonfederal land but do not\nreceive federal funding, or are located in federal\nland but receive federal assistance from a federal\nagency.\nPredE\nBut he now faces at least a year at a militant rehabilitation\ncentre in Kuwait, according to the terms of the release. The\nKuwaiti government had pushed hard for the release of all\nKuwaiti detainees at Guantanamo.\nA former guantanamo bay detainee has been re-\nleased from kuwait.\nCircE\nThe value of the 15 State projects in our sample is about $88\nmillion, and the value of the five USAID projects in our sample\nis about $107 million. Because State/INL implemented about\n90 percent of Mérida Initiative projects during this period, we\nchose a larger State/INL sample than a USAID sample.\nState/INL and USAID have implemented about 90\npercent of MérIDA Initiative projects.\nLinkE\nAdministrators of the ACT test took the decision just hours\nbefore some 5,500 students were due to sit it. The other\nentrance exam - the SAT - was cancelled in South Korea in\n2013 because some of the questions were leaked.\nA number of students have been barred from taking\npart in a test test test in south korea.\nCorefE\nBut Prof Peter Godfrey-Smith said the unique study, based\non 53 hours of footage and published on Friday in the journal\nCurrent Biology, provided a novel perspective on octopus\nbehaviour.\"[An aggressive] octopus will turn very dark, stand\nin a way that accentuates its size and it will often seek to stand\non a higher spot,\" Prof Godfrey-Smith, who co-authored the\nreport, said.\nOne of the world’s most aggressive octopuses ap-\npears to show signs of aggressive behaviour, a study\nsuggests.\nOutE\nNo systematic law or set of regulations governs the establish-\nment of memorials outside Washington, DC.\nNo systematic law or set of regulations governs the\nestablishment of memorialses outside Washington,\nD.C.\nGramE\nTable 10: Examples of unfaithful summaries per error type which correctly obtain low scores by INFUSE (top\nblock) and incorrectly high scores (bottom block). We indicate the document sentences retrieved by INFUSE (DRS),\nthe Model-generated Summary Sentence (MSS), and Error Type according to (Koh et al., 2022).\n"
}