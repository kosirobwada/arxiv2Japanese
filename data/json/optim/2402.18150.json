{
    "optim": "Unsupervised Information Refinement Training of Large Language Models\nfor Retrieval-Augmented Generation\nShicheng Xu∗1,2\nLiang Pang1†\nMo Yu2†\nFandong Meng2\nHuawei Shen1\nXueqi Cheng1\nJie Zhou2\n1CAS Key Laboratory of AI Security, Institute of Computing Technology, CAS\n2Pattern Recognition Center, WeChat AI\nxushicheng21s@ict.ac.cn\npangliang@ict.ac.cn\nmoyumyu@global.tencent.com\nAbstract\nRetrieval-augmented generation (RAG) en-\nhances large language models (LLMs) by incor-\nporating additional information from retrieval.\nHowever, studies have shown that LLMs still\nface challenges in effectively using the re-\ntrieved information, even ignoring it or being\nmisled by it. The key reason is that the training\nof LLMs does not clearly make LLMs learn\nhow to utilize input retrieved texts with var-\nied quality. In this paper, we propose a novel\nperspective that considers the role of LLMs in\nRAG as “Information Refiner”, which means\nthat regardless of correctness, completeness, or\nusefulness of retrieved texts, LLMs can consis-\ntently integrate knowledge within the retrieved\ntexts and model parameters to generate the texts\nthat are more concise, accurate, and complete\nthan the retrieved texts. To this end, we pro-\npose an information refinement training method\nnamed INFO1-RAG that optimizes LLMs for\nRAG in an unsupervised manner. INFO-RAG\nis low-cost and general across various tasks.\nExtensive experiments on zero-shot prediction\nof 11 datasets in diverse tasks including Ques-\ntion Answering, Slot-Filling, Language Mod-\neling, Dialogue, and Code Generation show\nthat INFO-RAG improves the performance\nof LLaMA2 by an average of 9.39% relative\npoints. INFO-RAG also shows advantages in\nin-context learning and robustness of RAG.\n1\nIntroduction\nRetrieval-augmented generation (RAG) is a popu-\nlar framework in modern NLP systems that equips\nneural with retrieved information for text gener-\nation like open-domain question answering, dia-\nlogue (Lewis et al., 2020; Guu et al., 2020) etc.\nRecently, RAG has been applied to large language\nmodels (LLMs) to provide additional knowledge\n∗Work done during the Tencent Rhino-bird Research Elite\nProgram at WeChat.\n†Corresponding authors.\n1Recursive acronym for InFO reFinement Objective.\nWhat was OpenAI founded, where is its headquarters located, and what models has it developed?\n… (complex context) OpenAI was\nestablished in 2015, …\n(complex\ncontext) OpenAI is headquartered\nin San Francisco, …\n(complex\ncontext) … OpenAI has developed\nGPT-3, DALL-E, CLIP, etc. …\n(complex context)\nScenario 1: \nAll knowledge for the question \nis already in the retrieved texts\n… (complex context) OpenAI was\nestablished in 2021, …\n(complex\ncontext)\nThe\nheadquartered\nof\nOpenAI is in Seattle, … (complex\ncontext) …\nScenario 2: \nRetrieved texts are incomplete \nor incorrect for the question.\n… (complex context) OpenAI\nis an artificial intelligence (AI)\nresearch\nlab\nfocused\non\nadvancing AI capabilities while\nalso addressing safety concerns\nand ethical considerations …\n(complex context)\nScenario 3: \nRetrieved texts are relevant but \ndo not have any answer.\nLLMs’\nOpenAI was established in 2015.\nOpenAI is headquartered in San\nFrancisco. OpenAI has developed\nGPT-3, DALL-E, CLIP, etc.\nRetrieval\nOpenAI was established in 2015.\nOpenAI is headquartered in San\nFrancisco. OpenAI has developed\nGPT-3, DALL-E, CLIP, etc.\nOpenAI was established in 2015.\nOpenAI is headquartered in San\nFrancisco. OpenAI has developed\nGPT-3, DALL-E, CLIP, etc.\nKnowledge \nKnowledge \nKnowledge \nLLMs’\nLLMs’\nMore concise\nMore concise, accurate, and complete More concise, accurate, and complete\nPositive Information Gain\nRefinement\nRefinement\nRefinement\nFigure 1: We consider the role of LLMs in RAG as\n“Information Refiner” that can generate more concise,\naccurate, and complete texts than the input retrieved\ntexts. In this way, LLM can consistently make RAG\nsystem produce positive information gain.\nand mitigate issues such as hallucination (Peng\net al., 2023; Shi et al., 2023; Ren et al., 2023).\nDespite the improved performance of retrieval\nmodels, the internet continues to be inundated with\nfake news, rumors, and fragmented, noisy informa-\ntion, posing challenges for retrieval models to reli-\nably identify and shield against such content (Sun\net al., 2022; Thakur et al., 2021). Consequently, not\nall retrieved texts are beneficial, necessitating that\nLLMs determine how to judiciously utilize them.\nHowever, pre-training tasks do not explicitly en-\nable LLMs to learn how to utilize the retrieved texts\nwith varied quality for generation. For a question\nand its retrieved texts as input sequence, RAG aims\nto minimize the negative log-likelihood (NLL) of\nsub-sequence (question and generated answer) by\nreferring to the retrieved texts. However, main-\nstream pre-training for LLMs with decoder-only\narchitecture is language modeling based on the pre-\nfix (Radford et al., 2018; Touvron et al., 2023a),\nthe training objective aims to minimize the neg-\native log-likelihood (NLL) of the entire input se-\nquence (retrieved texts, question, and generated\nanswer) (Mikolov et al., 2012). This gap causes\narXiv:2402.18150v1  [cs.CL]  28 Feb 2024\nLLMs to only regard the input retrieved texts as\na part of the prefix for language modeling rather\nthan additional reference, which leads to the fol-\nlowing problems. Firstly, for the long and complex\nretrieved texts, LLMs struggle to extract the correct\nanswers (Deng et al., 2023) accurately. Secondly,\nin situations where the retrieved texts cannot ad-\ndress the task, LLMs lack the capability to integrate\nthe knowledge within model parameters with the\nretrieved texts to generate improved texts. Thirdly,\nLLMs are susceptible to incorrect and noisy infor-\nmation in retrieved texts, posing a risk of being\nmisled (Chen et al., 2023; Yoran et al., 2023).\nTo solve above problems, some previous meth-\nods explore strategies for how or when to perform\nretrieval for LLMs by prompt techniques (Press\net al., 2023; Khattab et al., 2022; Xu et al., 2023;\nAsai et al., 2023). However, prompt cannot materi-\nally change the ability of LLMs to utilize retrieved\ntexts because model parameters are not updated for\nthis ability. Some methods fine-tune LLMs on the\nconstructed RAG data for a specific task such as\nQA (Yoran et al., 2023; Yu et al., 2023). However,\nunder the trend that LLMs are regarded as founda-\ntion models for various tasks in zero-shot setting,\nfine-tuning LLMs only on a few tasks make LLMs\nlimited to the RAG of training tasks and lose their\ngeneralizability. Because catastrophic forgetting\nstill exists in supervised fine-tuning of LLMs (Luo\net al., 2023). Although constructing data for a large\nnumber of tasks can alleviate this, it is hard to de-\nsign the data in various RAG tasks and requires\nhigh data annotation costs. Our paper aims to fun-\ndamentally improve the ability of LLMs to utilize\nretrieved texts while preserving the generalizability\nof LLMs for various RAG tasks in zero-shot setting,\nwhich is orthogonal to prompt techniques and can\nbe combined with them to get better performance.\nIn this paper, considering that LLMs have a cer-\ntain ability to use their own knowledge to examine\ninformation (Dhuliawala et al., 2023), we intro-\nduce a novel perspective to reassess the role of\nLLMs in RAG. Specifically, we propose consider-\ning LLMs as “Information Refiner”. The key idea\nbehind this is to continue training the pre-trained\nLLMs with an Information Refinement objective\nthat regardless of the correctness, completeness,\nor usefulness of the input retrieved texts, LLMs\ncan consistently integrate knowledge within the re-\ntrieved texts and model parameters to generate the\ntexts that are more concise, accurate, and complete\nthan the retrieved texts (Figure 1). We term this\nprocess “Positive Information Gain”. This enables\nLLMs to extract correct information from complex\ntexts as well as resist and rectify retrieved erro-\nneous information and noise, thereby improving\nthe information bottleneck of the RAG and allow-\ning the knowledge capacity of RAG to approximate\nthe combined knowledge of IR and LLMs.\nWe make the information refinement training\nwork in a completely unsupervised manner, such\nthat it is easy to obtain large-scale training data and\nmaintain the generalizability of the trained LLMs\nthat can be used in various RAG tasks in zero-shot\nsetting. Specifically, we propose an unsupervised\ntraining method named INFO-RAG. INFO-RAG\nclassifies the retrieved texts into three scenarios\n(shown in Figure 1) and proposes the unsupervised\ntraining task for each scenario. For the first sce-\nnario that all knowledge for the question is already\nin the retrieved texts, LLMs need to accurately ex-\ntract relevant knowledge from complex retrieved\ntexts and generate more concise texts. For the sec-\nond scenario that retrieved texts are incomplete or\nincorrect for the question, LLMs need to combine\nthe knowledge within model parameters to verify\nthe retrieved texts, correct the wrong knowledge,\nand complete the missing knowledge. For the third\nscenario that retrieved texts are relevant but do not\nhave any answer, LLMs need to find the knowledge\nwithin model parameters based on relevant context\nto generate correct answers. We mix the above\nthree tasks to train INFO-RAG unsupervisedly.\nMain contributions of this paper are as follows:\n(1) We introduce a novel perspective to reassess\nthe role of LLMs in the RAG system that considers\nLLMs as “Information Refiner” that can produce\npositive information gain in RAG scenarios.\n(2) We propose an unsupervised training method\nnamed INFO-RAG that enables LLMs to perform\ninformation refinement in RAG. INFO-RAG is\nlow-cost and general for various RAG tasks.\n(3) Extensive experiments show INFO-RAG en-\nhances the zero-shot RAG of LLaMA2 across\nQuestion Answering, Slot-Filling, Language Mod-\neling, Dialog, and Code Generation. INFO-RAG\nalso shows advantages in in-context learning and\nrobustness of RAG. Code will be released at https:\n//github.com/xsc1234/INFO-RAG/.\n2\nRelated Work\nRetrieval Augmented Generation\nRetrieval\naugmented generation (RAG) aims to provide addi-\ntional knowledge for language models by retrieving\ninformation from external databases (Lewis et al.,\n2020; Guu et al., 2020; Borgeaud et al., 2022; Izac-\nard et al., 2022). RAG makes the text generated\nby LLM more accurate and credible, and is widely\nused in Open-domain QA (Karpukhin et al., 2020;\nTrivedi et al., 2022a), dialogue (Cai et al., 2018,\n2019) and Code Generation (Parvez et al., 2021).\nRecently, RAG has also been widely applied in\nLLMs (Peng et al., 2023; Shi et al., 2023; Ren\net al., 2023). The form of RAG in LLMs is using\nthe retrieved texts as contexts (Ram et al., 2023).\nSome studies have noted that noise in retrieved\ntexts will interfere with the performance of the lan-\nguage model or even mislead it (Xu et al., 2023;\nWang et al., 2023; Chen et al., 2023; Xu et al.,\n2024). These works try to solve this problem from\nthe interactive framework between IR and LM,\nwhile our work points out a more essential view.\nThat is, previous studies on RAG do not define the\nrole of LLMs in RAG clearly. Our paper introduces\na novel perspective to reassess the role of LLMs\nin RAG that considers LLMs as “Information Re-\nfiner”.\nUnsupervised Learning of RAG\nUnsupervised\nlearning of RAG can be divided into the training\nof retrievers and language models. As for retriev-\ners, REALM (Guu et al., 2020) proposes using\nmasked language modeling to pre-train a knowl-\nedge retriever. REPLUG (Shi et al., 2023) trains the\nretriever according to the feedback from black-box\nLM. As for language models, RETRO (Borgeaud\net al., 2022) improves language models by retriev-\ning tokens. Atlas proposes pretext tasks to jointly\ntrain the retriever and language model. However,\nthese two methods focus on the model of encoder-\ndecoder architecture, which is inconsistent with the\ncurrent mainstream LLMs based on decoder-only.\nPrevious unsupervised training methods do not\nconsider the specific role that language models\nshould play in RAG. In this paper, we focus on\ntraining language model as an “Information Re-\nfiner” that can further improve the information bot-\ntleneck of RAG and be robust to retrieved texts.\n3\nOur INFO-RAG\nThis section introduces our INFO-RAG, an unsu-\npervised training method to enable LLMs to per-\nform information refinement in RAG. Firstly, we\nsummarize the retrieved texts in RAG into three\nscenarios and define the positive information gain\nfor each scenario. Secondly, we construct sam-\nple pairs in which the output has information gain\ncompared to the input for these three scenarios and\ndesign three training tasks. Thirdly, we train LLMs\nunder our designed tasks on the unsupervised sam-\nples. Unsupervised training makes INFO-RAG\nlow-cost and general for RAG in various tasks.\n3.1\nPositive Information Gain in RAG\nIn this paper, we introduce a novel perspective to re-\nassess the role of LLMs in RAG that LLMs should\nbe the “Information Refiner” that can produce “Pos-\nitive Information Gain” in the information flow of\nRAG. This section details the scenarios of retrieved\ntexts and defines specific information gain LLMs\nshould produce in each scenario.\nScenario 1. The first scenario is that all knowl-\nedge for the question is already in the retrieved\ntexts. Even if the correct knowledge already exists\nin the retrieved texts, complex and lengthy retrieved\ntexts are not conducive for users to directly obtain\nthe knowledge. Therefore, the positive informa-\ntion gain in this scenario means that LLMs extract\ncorrect knowledge as much as possible while re-\nmoving irrelevant information, thereby generating\nmore direct and concise texts for users.\nScenario 2. The second scenario is that although\nthe retrieved texts contain some usable knowledge,\nthey still contain some incomplete or incorrect\nknowledge. This scenario is very common, espe-\ncially with the current proliferation of fake news,\nmisinformation, and fragmented knowledge on the\nInternet. There has been study proving that noise\nand erroneous knowledge in retrieved texts greatly\nmislead the generation of LLMs (Xu et al., 2023).\nThe positive information gain in this scenario is\nthat LLMs can exploit the knowledge within their\nparameters to verify the knowledge in the retrieved\ntexts. Utilize accurate knowledge, rectify incorrect\nknowledge, and complete missing knowledge\nScenario 3. The third scenario is that the re-\ntrieved texts do not have any answer that can used\nto solve the question. This scenario means that the\nquestion is very difficult or the knowledge involved\nis very long-tail for information retrieval systems.\nEven in this case, the retrieval model’s ability to\nmodel semantics allows it to provide texts that are\nsemantically related to the question (Karpukhin\net al., 2020). Therefore, the positive information\ngain in this scenario is that LLMs can stimulate\nthe knowledge within their parameters based on\nsemantically relevant context to solve the question.\nTarget\nWikipedia Document \nIntercept k consecutive sentences\nSimulated Retrieved Texts\nInitial Sentence Set\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\n𝑠𝑠𝑙𝑙\n𝑡𝑡\n…\n𝑠𝑠𝑙𝑙\n𝑝𝑝\n…\nPrefix\nTarget\n(a) Data Collection\n(b) Data Construction\n(c) Unsupervised multi-task learning\n…\nPrefix\n…\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\nRandomly masking or replac-\nement  on informative tokens\n…\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\nEliminate 𝑠𝑠𝑙𝑙 \n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\nKeep\n𝑠𝑠𝑙𝑙\n𝑝𝑝\n…\nLLM\n𝑠𝑠𝑙𝑙\n𝑡𝑡\n…\n𝑠𝑠𝑙𝑙\n𝑝𝑝\n…\nLLM\n𝑠𝑠𝑙𝑙\n𝑡𝑡\n…\n𝑠𝑠𝑙𝑙\n𝑝𝑝\n…\nLLM\n𝑠𝑠𝑙𝑙\n𝑡𝑡\n…\n𝑆𝑆\n𝑆𝑆𝑆\n𝑆𝑆 − {𝑠𝑠𝑙𝑙}\n𝑝𝑝 𝑠𝑠𝑙𝑙\n𝑡𝑡 = 𝑝𝑝𝜃𝜃([𝑆𝑆; 𝑠𝑠𝑙𝑙\n𝑝𝑝])\n𝑝𝑝 𝑠𝑠𝑙𝑙\n𝑡𝑡 = 𝑝𝑝𝜃𝜃([𝑆𝑆𝑆; 𝑠𝑠𝑙𝑙\n𝑝𝑝])\n𝑝𝑝 𝑠𝑠𝑙𝑙\n𝑡𝑡 = 𝑝𝑝𝜃𝜃([𝑆𝑆 − {𝑠𝑠𝑙𝑙}; 𝑠𝑠𝑙𝑙\n𝑝𝑝])\nSelect and Copy\nContextual Stimulation\nCorrect and Complete\nScenario 1\nScenario 2\nScenario 3\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\n𝑆𝑆\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\n𝑆𝑆\n…\n…\n𝑠𝑠1\n𝑠𝑠2\n𝑠𝑠𝑘𝑘\n…\n𝑠𝑠𝑙𝑙\n…\n…\n𝑆𝑆\n…\n𝓡𝓡(𝑠𝑠𝑙𝑙\n𝑝𝑝)\n𝑆𝑆\nFigure 2: Overview of our INFO-RAG. Each sample is only processed for a single scenario to avoid data leakage.\n3.2\nUnsupervised Learning\nThis section introduces unsupervised learning in\nINFO-RAG. We construct the input-output pairs\nthat satisfy the information gain in the above three\nscenarios on Wikipedia. We continue to train pre-\ntrained LLMs on the constructed data to perform\ninformation refinement in the form of next token\nprediction in prefix language modeling, which is\ngeneral for various tasks. Pipeline is in Figure 2.\n3.2.1\nData Collection\nThe data construction is performed on English\nWikipedia. Specifically, for each document d in\nWikipedia, we intercept k consecutive sentences\nfrom d and get the sentence set S = [s1, s2, ..., sk].\nOur method randomly selects sl from S and uses it\nas the object for language modeling. The first 1\n3 to\n2\n3 of the tokens of sl are randomly intercepted as\nthe prefix (sp\nl ) and the other tokens of sl are used\nas the prediction target (st\nl). We also perform the\nprocess (Section 3.2.2) on sentence set S so that it\ncan be used to simulate the retrieved texts R(sp\nl )\nfor prefix sp\nl in three scenarios for conditioning the\ngeneration of st\nl. Then, we can get an unsupervised\ntraining sample for prefix language modeling that\npredicts st\nl given the prefix sp\nl and the retrieved\ntexts R(sp\nl ). This can be formulated as:\np(st\nl) = pθ([R(sp\nl ); sp\nl ]),\n(1)\nθ are parameters of LLMs, [R(sp\nl ); sp\nl ] is the con-\ncatenation of R(sp\nl ) and sp\nl by a special token.\n3.2.2\nData Construction and Training Tasks\nThis section details our data construction and train-\ning tasks for three scenarios in Section 3.1.\nFor Scenario 1 that needs LLMs to extract the\ncorrect knowledge from the complex texts, we\npropose the training task named Select and Copy.\nSpecifically, given the sentence set S for a sample,\nSelect and Copy directly uses all sentences in S as\nretrieved texts for conditioning LLMs to predict st\nl\nfor the given prefix sp\nl . This can be formulated as:\np(st\nl) = pθ([S; sp\nl ]).\n(2)\nIn Select and Copy, sl (both sp\nl and st\nl) has been\ncontained in the retrieved texts S, this needs LLMs\nto select the texts matching the prefix sp\nl from the\ncomplex retrieved texts S and directly copy the\ntarget st\nl for generation. The information gain be-\ntween st\nl and input retrieved texts S is that st\nl is\nmore concise to be used as the postfix of sp\nl .\nFor Scenario 2 that needs LLMs to verify the\nknowledge in the retrieved texts, utilize accurate\nknowledge, rectify incorrect knowledge, and com-\nplete missing knowledge. We propose the training\ntask named Correct and Complete. Given a sen-\ntence set S, firstly, this task uses the stability of\nword distribution between layers to get informa-\ntive tokens. The intention for this is that the more\nunstable the word distribution of the token among\nthe topmost layers is, the more it indicates that the\ntoken is an informative token (Chuang et al., 2023).\nSpecifically, for each sentence si in S, our method\nobtains the next word distribution of the a-th token\ns[a]\ni\ngiven prefix s<a\ni\nof si in each layer of LLM as:\ndj(s[a]\ni |s<a\ni ) = softmax(WH[a]\nj ),\n(3)\nin which j indicates the j-th layer of LLMs, H[a]\nj\n∈\nRh is the hidden states for token s[a]\ni\nin the j-th\nlayer, W ∈ Rh×v is the vocabulary head that maps\nthe hidden states H[a]\nj\nto the word distribution with\nvocabulary size v. Then, for the LLM with N lay-\ners, our method uses Jensen-Shannon Divergence\n(JSD) to measure the differences in word distribu-\ntion between layers and gets the word distribution\nstability of token s[a]\ni\nas:\nO[a]\ni\n= arg max\nj∈J JSD(dN(s[a]\ni |s<a\ni )||dj(s[a]\ni |s<a\ni )),\n(4)\nin which J is the set of candidate layers (0-th to\nN\n2 -th layers), dN(s[a]\ni |s<a\ni ) is the word distribu-\ntion of the last layer. This design aims to find\nthe layer with the largest word distribution differ-\nence between the last layer and use the JSD of the\ntwo as the word distribution stability of the token\ns[a]\ni\n(Chuang et al., 2023). For each token of si, we\nobtain its word distribution stability in parallel and\nget the set of word distribution stability for si as:\nOi = {O[0]\ni , O[1]\ni , ..., O[n]\ni }.\n(5)\nWe choose the tokens corresponding to the top 50%\nof the elements in Oi as informative tokens within\nthe sentence si. Subsequently, we apply a specific\npercentage (30%) of random masking and replace-\nment to these tokens. For the randomly selected\ntoken, we replace it with [MASK] with a 50% prob-\nability to simulate the incomplete knowledge, and\nrandomly replace it with another token with a 40%\nprobability to simulate the incorrect knowledge,\nwhile keeping it unchanged with a 10% probability\nto simulate the correct knowledge. We do the above\npipeline for each sentence in the set S and get the\nprocessed set S′. RAG in Correct and Complete\ncan be formulated as:\np(st\nl) = pθ([S′; sp\nl ]).\n(6)\nIn Correct and Complete, broken sl with noise is\nin S′, LLMs need to extract, correct, and complete\nthe knowledge in sl from S′ to generate st\nl.\nFor Scenario 3 that needs LLMs to find answers\nfrom their knowledge based on relevant texts in\ncontext. We propose the training task named Con-\ntextual Stimulation. Contextual Stimulation elim-\ninates sl (both sp\nl and st\nl) from the set S and uses\nthe remaining sentences as retrieved tests for gen-\neration, which can be formulated as:\np(st\nl) = pθ([S − {sl}; sp\nl ]).\n(7)\nIn Contextual Stimulation, each sentence in re-\ntrieved texts S − {sl} is semantically relevant to\nsp\nl but cannot help LLMs to directly generate st\nl.\nLLMs need to be stimulated by relevant informa-\ntion to generate st\nl based on their own knowledge.\n3.2.3\nTraining Strategy\nAfter the data construction for three training tasks,\nwe mix them for multi-task training. Specifically,\nwe use LoRA (Hu et al., 2021) to train the pre-\ntrained LLMs on the mixed dataset of three tasks.\nThree tasks are trained alternately in batches. Since\nSelect and Copy is relatively simple for LLMs,\nit only accounts for 20% of the batches, while\nCorrect and Complete and Contextual Stimulation\neach account for 40% of the batches. Using LoRA\nnot only reduces training costs but also makes our\nmethod plug-and-play. The trained LoRA parame-\nters are loaded when LLMs need to perform RAG\nand unloaded when RAG is not needed.\n4\nExperiments\n4.1\nDatasets and Evaluation Metrics\nTo demonstrate the generality of our unsupervised\ntraining method, we evaluate the performance of\nINFO-RAG on eleven datasets across seven tasks.\nOpen-domain Question Answering Open-domain\nQA is a typical knowledge-intensive task that can\ndirectly evaluate the knowledge of LLMs. We use\nNatural Questions (Kwiatkowski et al., 2019) (NQ)\nand WebQuestions (Berant et al., 2013) (WebQ)\nas the datasets. We use cover Exact Match (EM)\nto determine whether the ground truth exactly ap-\npears in the output and the accuracy is used as the\nevaluation metric, following (Schick et al., 2023)\nSoft Filling Soft filling requires LLMs to output\nthe object entities for the input subject entity and\nrelation. We use two knowledge-intensive datasets\nincluding Zero Shot RE (Levy et al., 2017) (ZS)\nand T-REx (Elsahar et al., 2018). We use the same\nevaluation metric as Open-domain QA.\nLong-Form Question Answering Compared with\nopen-domain QA, LFQA is the QA task whose\nground truth answer is a relatively long text. We\nuse ELI5 (Fan et al., 2019), a knowledge-intensive\ndataset for LFQA. We use ROUGE-L as the evalu-\nation metric (Petroni et al., 2020).\nDialogue Dialogue in our experiment focuses\non the factual knowledge.\nWe use Wizard\nof Wikipedia (Dinan et al., 2018) (WoW), a\nknowledge-powered dialogue dataset whose con-\nversation is grounded with knowledge. We use F1\nas the evaluation metric (Petroni et al., 2020).\nLanguage Modeling We use WikiText-103 (Mer-\nity, 2016), a popular dataset for language modeling.\nWe use ROUGE-L as the evaluation metric.\nSoft-Filling\nODQA\nMulti-Hop QA\nLFQA\nDialog\nLM\nCode Gen\nOverall\nAccuracy\nAccuracy\nAccuracy\nROUGE\nF1\nROUGE\nCodeBLEU\nT-REx\nZS\nNQ\nWebQ Hotpot Musique\nElI5\nWow\nWikiText Python Java\nLLaMA-2-7B\n55.60 54.08 46.82 43.52\n39.40\n25.95\n15.18\n7.85\n60.77\n21.44\n22.99\n35.78\n+ INFO-RAG\n65.91 57.01 45.74 44.68\n46.56\n30.19\n17.18\n9.09\n62.91\n26.75\n32.06\n39.83\nLLaMA-2-7B-chat\n60.63 55.03 49.42 46.72\n50.03\n42.69\n27.81\n10.21\n60.26\n22.46\n23.90\n40.83\n+ INFO-RAG\n65.77 58.32 53.93 49.13\n52.01\n44.45\n28.15\n10.49\n63.24\n27.25\n28.79\n43.78\nLLaMA-2-13B\n60.08 50.77 47.40 44.62\n42.12\n25.78\n14.80\n7.04\n62.20\n21.52\n29.16\n36.86\n+ INFO-RAG\n62.80 55.63 47.82 45.42\n51.48\n35.02\n17.48\n7.20\n64.14\n29.00\n35.50\n41.04\nLLaMA-2-13B-chat 62.53 56.81 50.36 45.47\n61.23\n47.06\n27.07\n11.19\n60.52\n22.34\n30.96\n43.23\n+ INFO-RAG\n65.39 59.05 54.04 51.07\n61.91\n47.93\n27.24\n11.38\n63.92\n31.98\n38.12\n46.55\nTable 1: Overall performance on retrieval-augmented generation on 11 datasets across 7 tasks in zero-shot setting.\nMulti-Hop Question Answering Multi-hop QA\nmeasures the ability of LLMs to perform combined\nreasoning on multiple knowledge. We use Hot-\npotQA (Yang et al., 2018) and Musique (Trivedi\net al., 2022b) for this task. We use the same evalua-\ntion metric as Open-domain QA.\nCode Generation Code generation aims to gener-\nate the code for the given natural language. We use\nJava and Python in CodeXGLUE (Iyer et al., 2018)\nfor this task. We use CodeBLEU (Ren et al., 2020)\nas the evaluation metric.\n4.2\nExperimental Settings\nLLMs in our paper include LLaMA-2-7B, 13B and\ntheir chat version (Touvron et al., 2023b). We use\nLoRA to fine-tune these pre-trained LLMs on four\nA100 GPUs with the learning rate of 1e-5, per-gpu\nbatch size of 4 (for 7B) and 2 (for 13B) for 5K\nsteps. As for the training data, we intercept 15\nconsecutive sentences for each example.\nAs for the retrieval model and retrieval database,\nfor Open-domain QA, Soft Filling and Lan-\nguage Modeling, we use ColBERTv2 (Santhanam\net al., 2022), a late-interaction model with ex-\ncellent generalization ability as the retriever,\nand use Wikipedia consisting of 21,015,324 pas-\nsages (Karpukhin et al., 2020) as retrieval database.\nFor Code Generation, we SCODE-R (Parvez et al.,\n2021) as code retriever and use deduplicated source\ncodes in CodeSearchNET (Husain et al., 2019) as\nretrievel database. For all the above tasks, we give\nTop-5 retrieved passages to each example. For\nLFQA, Dialog, and Multi-Hop QA, we use the list\nof contextual passages provided in the datasets as\nthe retrieved list (distractor setting).\n4.3\nExperimental Results\nMain Results (Zero-Shot Setting)\nExperimen-\ntal results in Table 1 show the improvement (the\naverage is 9.39%) of our method on the utilization\nof retrieved knowledge from four aspects.\n(1) Short and Direct Knowledge.\nOur method\ncan significantly improve the RAG performance\nof LLaMA on ODQA and Slot-Filling tasks. The\nanswer in ODQA and Slot-Filling is short and di-\nrect, it can directly reflect the ability of LLMs to\nutilize the knowledge in retrieved texts.\n(2) Reasoning on Multiple Knowledge. Our INFO-\nRAG has advantages in cross-passage reasoning on\nmultiple knowledge of retrieval lists. Questions in\nboth HotpotQA and Musique are complex and need\nmultiple knowledge from different passages. These\nquestions not only require LLMs to extract correct\nknowledge from the retrieved passage list but also\nto combine the knowledge of different passages in\nthe list for reasoning to give the final answer.\n(3) Long and Complex Knowledge. Our INFO-\nRAG can improve the RAG performance of\nLLaMA on LFQA, Dialogue and Language Model-\ning. These tasks require LLaMA to output long and\ncomplex texts grounded with intensive knowledge.\n(4) Code Knowledge. Our INFO-RAG can also\nimprove the RAG performance of LLaMA on Code\nGeneration. This further demonstrates the cross-\ntask generality of INFO-RAG. Our method is only\ntrained on natural language but can also show ad-\nvantages in programming language tasks, which\ndemonstrates that INFO-RAG successfully enables\nLLMs to learn how to exploit the retrieved informa-\ntion rather than just fitting the data. Unsupervised\nand prefix language modeling training paradigms\nmake INFO-RAG general in various tasks.\nT-REx\nZS\nNQ\nWebQ\nhas-ans. replace no-ans. has-ans. replace no-ans. has-ans. replace no-ans. has-ans. replace no-ans.\nLLaMA-2-7B\n67.19\n38.37\n6.49\n64.41\n12.78\n2.44\n65.54\n16.91\n3.41\n60.64\n25.68\n7.90\n+ INFO-RAG\n79.80\n41.79\n7.04\n68.10\n13.55\n3.26\n64.43\n22.68\n4.70\n62.70\n26.48\n8.96\nLLaMA-2-7B-chat\n73.79\n40.56\n4.87\n66.71\n14.19\n1.63\n68.72\n20.81\n4.50\n66.86\n28.63\n5.62\n+ INFO-RAG\n80.01\n42.92\n5.42\n69.64\n15.02\n2.65\n70.99\n23.14\n5.62\n68.73\n29.74\n9.12\nLLaMA-2-13B\n72.26\n39.47\n7.76\n60.14\n19.71\n4.69\n65.94\n18.45\n4.42\n62.09\n26.63\n9.27\n+ INFO-RAG\n75.80\n44.08\n8.48\n65.94\n23.21\n4.90\n64.98\n27.60\n8.02\n63.51\n28.24\n9.88\nLLaMA-2-13B-chat 75.96\n43.79\n5.59\n67.03\n16.58\n1.42\n69.37\n30.72\n6.16\n65.07\n31.88\n5.47\n+ INFO-RAG\n79.25\n48.59\n6.67\n70.26\n25.02\n3.87\n73.73\n33.85\n8.39\n70.59\n37.48\n11.25\nTable 2: Experimental results on three scenarios. “has-ans.” is the first scenario that correct answers are in retrieved\ntexts. “replace” is the second scenario that correct answers are randomly replaced with other phrases to simulate the\nincorrect and incomplete knowledge. “no-ans.” is the third scenario that retrieval cannot find any answers.\nResults on In-context Learning for RAG\nBe-\nsides, our INFO-RAG allows further improve-\nment cooperating with in-context learning (ICL).\nICL (Brown et al., 2020) works by prepending a\nfew examples of the target task before the query,\nwhich helps LLMs understand the task.\nHow-\never, ICL may not always help in the RAG setting,\nmainly due to the confusion between the retrieved\ntexts of the query and the few-shot examples. As\nshown in Table 3, LLaMA-2 cannot further im-\nprove the RAG performance from ICL, even some-\ntimes hurt by the few-shot examples while INFO-\nRAG can further improve RAG by ICL. This is\nmainly because INFO-RAG enables LLaMA to\nunderstand the task form of RAG, thereby better\nlearning the general task pattern from ICL exam-\nples. In this experiment, we construct the ICL\nexample consisting of a query, a relevant passage,\nand an answer. For a fair comparison, we need to\nensure that the performance of our method and the\nbaseline are close in non-ICL setting. Therefore,\nwe select queries for which the baseline gives the\nsame answer as our method (both correct or both\nincorrect) and evaluate the ICL performance on\nthese queries.\nEnhancing Previous SOTA in Open-Retrieval\nSetting\nWe further show that our INFO-RAG\ncan cooperate well with the recent prompting tech-\nniques that perform multi-step reasoning to com-\nbine with retrieval to solve questions (Xu et al.,\n2023; Khattab et al., 2022; Press et al., 2023; Yao\net al., 2022). To make a fair comparison, we follow\nSearChain (Xu et al., 2023) that runs on Multi-Hop\nQA and Slot-Filling in open-retrieval setting that\nretrieves passages from the full Wikipedia in each\nreasoning step. SearChain and other baselines use\nLLaMA-2-13B-chat as the backbone. Then, we\nData\nModel\nNumber of Examples in ICL\n0\n2\n4\n8\n12\n16\nNQ\nLLaMA-2\n43.36 23.34 16.60 39.22 44.32 43.00\n+INFO-RAG 43.36 44.35 45.88 44.45 47.75 46.25\nWebQ LLaMA-2\n43.20 18.36 9.40 36.71 44.80 44.81\n+INFO-RAG 43.20 48.03 49.82 48.25 47.86 47.29\nT-REx LLaMA-2\n59.83 47.05 49.11 56.51 55.23 56.31\n+INFO-RAG 59.83 63.08 63.45 63.54 63.57 63.38\nZS\nLLaMA-2\n52.41 42.71 37.05 50.40 50.20 51.01\n+INFO-RAG 52.41 56.53 60.37 59.86 59.75 59.85\nTable 3: RAG performance changes with number of\nexamples in In-context learning.\nMulti-Hop QA\nSlot-Filling\nHotpotQA\nMusique\nT-REx\nzsRE\nPrevious SOTA\n28.19\n10.03\n63.10\n57.09\nSearChain\n31.21\n11.27\n64.58\n58.91\n+ INFO-RAG\n33.04\n12.10\n66.95\n60.72\nTable 4: Enhancement to the state-of-the-art RAG frame-\nwork. Previous SOTA includes DSP, Self-Ask, React.\nperform SearChain based on LLaMA-2-13B-chat\ntrained by INFO-RAG to show the enhancement to\nSearChain by INFO-RAG. Results in Table 4 show\nthat INFO-RAG can make SearChain achieve bet-\nter performance. This provides additional support\nthat our unsupervised INFO training fundamentally\nimproves the RAG performance of LLMs.\n4.4\nAnalysis\nFine-grained Analysis for Three Scenarios\nAs\nshown in Table 2, our INFO-RAG is effective in all\nthree RAG scenarios and shows better robustness\nto incorrect, incomplete, and noisy retrieved texts.\nWe propose corresponding unsupervised training\ntasks for the three scenarios of RAG. This section\nintroduces the fine-grained analysis for each sce-\nT-REx\nZS\nNQ\nWebQ Hotpot Musique\nElI5\nWow\nWikiText Python\nJava\nOverall\nLLaMA-2 w/o RAG\n35.60\n10.99 32.67\n39.13\n29.16\n5.83\n26.05 10.71\n41.80\n20.67\n25.87\n25.32\nLLaMA-2 w/ RAG\n62.53\n56.81 50.36\n45.47\n61.23\n47.06\n27.07 11.19\n60.52\n22.34\n30.96\n43.23\n+ training on wiki\n62.55\n56.79 49.23\n45.05\n61.00\n46.95\n26.31 11.05\n60.84\n22.05\n30.28\n42.92\n+ INFO-RAG\n65.39\n59.05 54.04\n51.07\n61.91\n47.93\n27.24 11.38\n63.92\n31.98\n38.12\n46.55\nTable 5: Analysis on the best-performed model LLaMA-2-13B-chat.\nMethod\nNQ\noriginal has-ans. replace no-ans.\nBaseline\n50.36\n69.37\n30.72\n6.16\nS1: Select and Copy\n48.77\n69.59\n25.40\n0.11\nS2: Correct and Complete 51.59\n70.42\n32.71\n4.48\nS3: Contextual Stimulation 52.75\n72.50\n31.77\n8.86\nS2&S3\n53.73\n73.01\n32.50\n9.01\nINFO-RAG (S1& S2&S3) 54.04\n73.73\n33.85\n8.39\nTable 6: Effects of three training tasks.\nnario. For Scenario 1, we use cover EM to select\nthose samples that already contain the correct an-\nswers in the retrieval list. For Scenario 2, we ran-\ndomly replace the correct answers in the retrieved\ntexts with another phrase with the same properties.\nFor Scenario 3, we use cover EM to select those\nsamples that retrieved texts do not contain any cor-\nrect answers. We count the accuracy of LLaMA on\nsamples of these three scenarios respectively. Ques-\ntions in the third scenario are more difficult than\nin the second scenario because retrieval models\ncannot find anything to solve them. Table 2 indi-\ncates that our method shows advantages in each\nscenario and is more robust regardless of whether\nthe retrieved texts contain the correct answer.\nAblation Study\nWe conduct ablation study to\nexplore the effects of the following factors.\n(1) Additional Training on Wikipedia. We study\nwhether our improvement is from helping the\nmodel to achieve information refinement, or simply\nbecause of additional training on Wikipedia. To\nthis end, we train LLaMA-2 on Wikipedia with\nstandard language modeling objective, by setting\nthe same hyperparameters as our INFO-RAG. The\nresults in Table 5 show that this baseline leads to no\nimprovement over the backbone LLaMA-2, con-\nfirming the effectiveness of our training method\nrather than additional training on Wikipedia.\n(2) Training tasks. We perform three training tasks\nproposed in INFO-RAG separately on original data\nand data constructed for each scenario to explore\ntheir effects respectively. Table 6 shows that both\nS2 and S3 have gains in their scenarios. Although\nDatasets\nMethod\nMax ∆\nMax ∆\nMax ∆\nratio\nposition\nnumber\nNQ\nLLaMA-2\n-51.94%\n-16.18%\n-25.43%\n+ INFO-RAG\n-43.48%\n-15.80%\n-17.25%\nWebQ\nLLaMA-2\n-50.57%\n-5.63%\n-22.13%\n+ INFO-RAG\n-45.48%\n-8.72%\n-11.91%\nT-REx\nLLaMA-2\n-46.57%\n-9.45%\n-5.95%\n+ INFO-RAG\n-44.38%\n-8.61%\n-2.99%\nZS\nLLaMA-2\n-59.25%\n-13.40%\n-12.37%\n+ INFO-RAG\n-50.08%\n-11.11%\n-11.43%\nTable 7: Maximum relative performance change caused\nby changes in retrieval results.\nS1 has negative effects when performed alone, it\ncan achieve the best results when trained together\nwith S2 and S3. This is mainly because S1 alone\nis so simple that causes LLM to overfit the data.\nAdding S2 and S3 allows LLM to learn the task\nparadigm of information refinement, making LLM\nbetter extract the correct answer for Scenario 1.\nRobustness to Retrieval Results\nTable 7 shows\nINFO-RAG is more robust to changes in retrieval\nresults including the ratio and position of positive\npassages and number of retrieved passages. More\ndetails can be found in Section A of Appendix.\nAvoid Catastrophic Forgetting Experiment on\nMMLU (Hendrycks et al., 2020) without RAG\nshows that INFO-RAG performs very close to the\noriginal LLaMA-2 (7B: 45.0 vs. 45.3; 13B: 54.3 vs.\n54.8), which indicates that INFO-RAG enhances\nRAG while avoiding catastrophic forgetting. More\ndetails can be found in Section A.4 of Appendix.\n5\nConclusion\nThis paper proposes a novel perspective to reassess\nthe role of LLMs in RAG that considers LLMs as\n“Information Refiner”. This means that regardless\nof the correctness, completeness, or usefulness of\nthe retrieved texts, LLMs can consistently integrate\nknowledge within model parameters and the re-\ntrieved texts to generate texts that are more concise,\naccurate, and complete. To achieve it, we propose\nan information refinement training method named\nINFO-RAG in an unsupervised manner, which is\nlow-cost and general across various tasks. Exten-\nsive experiments across 11 datasets of 7 tasks in\nzero-shot setting show that INFO-RAG improves\nthe performance of LLMs for RAG. INFO-RAG\nalso shows advantages in ICL and robustness of\nRAG and can be combined with the SOTA RAG\nframework to further improve its performance.\nLimitations\nThis paper aims to enable LLMs to perform infor-\nmation refinement in RAG by unsupervised train-\ning, so as to accurately extract correct information\nand avoid the interference of incorrect information.\nThe main limitation of this paper is that due to\nthe lack of computing resources, we only conduct\nexperiments on models with 7B and 13B param-\neter sizes. In the future, we consider using more\ncomputing resources to explore the performance of\nmodels with larger parameter sizes.\nEthics Statement\nAfter careful consideration, we believe that our pa-\nper does not introduce additional ethical concerns.\nWe declare that our work complies with the ACL\nEthics Policy.\nReferences\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and\nHannaneh Hajishirzi. 2023. Self-rag: Learning to\nretrieve, generate, and critique through self-reflection.\narXiv preprint arXiv:2310.11511.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy\nLiang. 2013. Semantic parsing on freebase from\nquestion-answer pairs. In Proceedings of the EMNLP\n2013, pages 1533–1544.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nTom B. Brown, Benjamin Mann, Nick Ryder, et al. 2020.\nLanguage models are few-shot learners.\nDeng Cai, Yan Wang, Victoria Bi, Zhaopeng Tu, Xi-\naojiang Liu, Wai Lam, and Shuming Shi. 2018.\nSkeleton-to-response: Dialogue generation guided by\nretrieval memory. arXiv preprint arXiv:1809.05296.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\naojiang Liu, and Shuming Shi. 2019.\nRetrieval-\nguided dialogue response generation via a matching-\nto-generation framework.\nIn Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866–1875.\nJiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.\n2023.\nBenchmarking large language models in\nretrieval-augmented generation.\narXiv preprint\narXiv:2309.01431.\nYung-Sung Chuang, Yujia Xie, Hongyin Luo, Yoon\nKim, James Glass, and Pengcheng He. 2023. Dola:\nDecoding by contrasting layers improves factu-\nality in large language models.\narXiv preprint\narXiv:2309.03883.\nJingcheng Deng, Liang Pang, Huawei Shen, and Xueqi\nCheng. 2023. Regavae: A retrieval-augmented gaus-\nsian mixture variational auto-encoder for language\nmodeling. arXiv preprint arXiv:2310.10567.\nShehzaad Dhuliawala, Mojtaba Komeili, Jing Xu,\nRoberta Raileanu, Xian Li, Asli Celikyilmaz, and\nJason Weston. 2023. Chain-of-verification reduces\nhallucination in large language models.\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela\nFan, Michael Auli, and Jason Weston. 2018. Wizard\nof wikipedia: Knowledge-powered conversational\nagents. arXiv preprint arXiv:1811.01241.\nHady Elsahar, Pavlos Vougiouklis, Arslen Remaci,\nChristophe Gravier, Jonathon Hare, Frederique Lafor-\nest, and Elena Simperl. 2018. T-rex: A large scale\nalignment of natural language with knowledge base\ntriples. In Proceedings of LREC 2018.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. Eli5:\nLong form question answering.\narXiv preprint\narXiv:1907.09190.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International confer-\nence on machine learning, pages 3929–3938. PMLR.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2020. Measuring massive multitask language under-\nstanding. arXiv preprint arXiv:2009.03300.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021.\nLora: Low-rank adap-\ntation of large language models.\narXiv preprint\narXiv:2106.09685.\nHamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis\nAllamanis, and Marc Brockschmidt. 2019. Code-\nsearchnet challenge: Evaluating the state of semantic\ncode search. arXiv preprint arXiv:1909.09436.\nSrinivasan Iyer, Ioannis Konstas, Alvin Cheung, and\nLuke Zettlemoyer. 2018.\nMapping language to\ncode in programmatic context.\narXiv preprint\narXiv:1808.09588.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nOmar Khattab,\nKeshav Santhanam,\nXiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022.\nDemonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp.\narXiv preprint\narXiv:2212.14024.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics, 7:453–\n466.\nOmer Levy, Minjoon Seo, Eunsol Choi, and Luke\nZettlemoyer. 2017.\nZero-shot relation extrac-\ntion via reading comprehension.\narXiv preprint\narXiv:1706.04115.\nPatrick Lewis,\nEthan Perez,\nAleksandra Piktus,\net al. 2020.\nRetrieval-augmented generation for\nknowledge-intensive nlp tasks. Advances in Neural\nInformation Processing Systems, 33:9459–9474.\nYun Luo, Zhen Yang, Fandong Meng, Yafu Li, Jie\nZhou, and Yue Zhang. 2023. An empirical study\nof catastrophic forgetting in large language mod-\nels during continual fine-tuning.\narXiv preprint\narXiv:2308.08747.\nStephen Merity. 2016. The wikitext long term depen-\ndency language modeling dataset. Salesforce Meta-\nmind, 9.\nTomáš Mikolov et al. 2012. Statistical language models\nbased on neural networks. Presentation at Google,\nMountain View, 2nd April, 80(26).\nMd Rizwan Parvez, Wasi Uddin Ahmad, Saikat\nChakraborty, Baishakhi Ray, and Kai-Wei Chang.\n2021. Retrieval augmented code generation and sum-\nmarization. arXiv preprint arXiv:2108.11601.\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng,\nYujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou\nYu, Weizhu Chen, et al. 2023. Check your facts and\ntry again: Improving large language models with\nexternal knowledge and automated feedback. arXiv\npreprint arXiv:2302.12813.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Mail-\nlard, et al. 2020.\nKilt: a benchmark for knowl-\nedge intensive language tasks.\narXiv preprint\narXiv:2009.02252.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nOri Ram, Yoav Levine, Itay Dalmedigos, Dor Muhlgay,\nAmnon Shashua, Kevin Leyton-Brown, and Yoav\nShoham. 2023. In-context retrieval-augmented lan-\nguage models. arXiv preprint arXiv:2302.00083.\nRuiyang Ren, Yuhao Wang, Yingqi Qu, Wayne Xin\nZhao, Jing Liu, Hao Tian, Hua Wu, Ji-Rong Wen,\nand Haifeng Wang. 2023.\nInvestigating the fac-\ntual knowledge boundary of large language mod-\nels with retrieval augmentation.\narXiv preprint\narXiv:2307.11019.\nShuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie Liu,\nDuyu Tang, Neel Sundaresan, Ming Zhou, Ambrosio\nBlanco, and Shuai Ma. 2020. Codebleu: a method\nfor automatic evaluation of code synthesis. arXiv\npreprint arXiv:2009.10297.\nKeshav Santhanam, Omar Khattab, Jon Saad-Falcon,\nChristopher Potts, and Matei Zaharia. 2022. Col-\nbertv2:\nEffective\nand\nefficient\nretrieval\nvia\nlightweight late interaction.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models.\narXiv\npreprint arXiv:2301.12652.\nTiening Sun, Zhong Qian, Sujun Dong, Peifeng Li, and\nQiaoming Zhu. 2022. Rumor detection on social\nmedia with graph adversarial contrastive learning. In\nProceedings of the WWW 2022, pages 2789–2797.\nNandan Thakur, Nils Reimers, Andreas Rücklé, Ab-\nhishek Srivastava, and Iryna Gurevych. 2021. Beir:\nA heterogenous benchmark for zero-shot evalua-\ntion of information retrieval models. arXiv preprint\narXiv:2104.08663.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022a.\nInterleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions.\narXiv\npreprint arXiv:2212.10509.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022b.\nMusique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539–554.\nYile Wang, Peng Li, Maosong Sun, and Yang Liu.\n2023.\nSelf-knowledge guided retrieval augmen-\ntation for large language models.\narXiv preprint\narXiv:2310.05002.\nShicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng,\nand Tat-seng Chua. 2023. Search-in-the-chain: To-\nwards the accurate, credible and traceable content\ngeneration for complex knowledge-intensive tasks.\narXiv preprint arXiv:2304.14732.\nShicheng Xu, Liang Pang, Jun Xu, Huawei Shen, and\nXueqi Cheng. 2024. List-aware reranking-truncation\njoint model for search and retrieval-augmented gen-\neration. arXiv preprint arXiv:2402.02764.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nOri Yoran, Tomer Wolfson, Ori Ram, and Jonathan\nBerant. 2023. Making retrieval-augmented language\nmodels robust to irrelevant context. arXiv preprint\narXiv:2310.01558.\nWenhao Yu, Hongming Zhang, Xiaoman Pan, Kaixin\nMa, Hongwei Wang, and Dong Yu. 2023. Chain-of-\nnote: Enhancing robustness in retrieval-augmented\nlanguage models.\nA\nMore Analysis\nA.1\nRobustness to ratio of Positive Passages\nOur INFO-RAG improves the robustness of RAG\nperformance to retrieval performance. The perfor-\nmance of the retriever greatly affects the perfor-\nmance of LLM in RAG (Chen et al., 2023). We\nexplore this in this section. Specifically, we sim-\nulate changes in retrieval performance by varying\nthe ratio of positive and negative passages in the\nretrieved list and report the RAG performance with\ndifferent ratios. Table 8 shows INFO-RAG per-\nforms better when the ratio is low and the perfor-\nmance is more stable than baseline when the ratio\nchanges from 100% to 0% (Max ∆). The model in\nthis experiment is LLaMA-2-13B-chat.\nA.2\nRobustness to Positive Passage Position\nExperimental results in Table 9 show that our\nINFO-RAG consistently outperforms the baseline\n(LLaMA-2) regardless of where the positive pas-\nsage (passage contains the correct answers) appears\nin the retrieved list. Specifically, we mix positive\nand negative passages in a ratio of 1:9 to simu-\nlate the retrieved passage list, vary the position of\nthe positive passage in the retrieved list from 0\nto 9, and evaluate the corresponding RAG perfor-\nmance respectively. The model in this experiment\nis LLaMA-2-13B-chat. Experimental results show\nthat our INFO-RAG not only outperforms the base-\nline at every position but also achieves more stable\nperformance varying with the position (Max ∆).\nA.3\nRobustness to Number of Retrieved\nPassages\nExperimental results in Table 10 show that our\nINFO-RAG consistently outperforms the baseline\nwith the different number of retrieved passages\n(from 1 to 10) and is robust to the change of the\nnumber. In this experiment, we use LLaMA-2-\n13B-chat as the base model, change the number of\nretrieved passages from 1 to 10, and evaluate the\ncorresponding performance.\nA.4\nPerformance on MMLU\nExperimental results on MMLU benchmark in the\nsetting without RAG shown in Table 11 show that\nour INFO-RAG significantly improves the perfor-\nmance of LLMs in RAG, while still maintaining\nits versatility and avoiding catastrophic forgetting.\nMMLU is a benchmark that measures massive mul-\ntitask language understanding ability of LLMs. It\nData\nModel\nratio of Positive Passages\nMax ∆\n100%\n80%\n60%\n40%\n20%\n0%\nNQ\nLLaMA-2\n88.11\n82.71\n80.81\n77.62\n69.73\n42.35\n-51.94%\n+ INFO-RAG\n90.31\n83.72\n81.72\n79.72\n71.52\n51.04\n-43.48%\nWebQ\nLLaMA-2\n79.41\n75.43\n71.63\n65.53\n63.39\n39.25\n-50.57%\n+ INFO-RAG\n83.66\n76.23\n74.23\n69.05\n65.74\n45.61\n-45.48%\nT-REx\nLLaMA-2\n80.01\n70.05\n71.52\n68.53\n66.23\n42.75\n-46.57%\n+ INFO-RAG\n83.52\n73.22\n74.93\n72.32\n70.12\n46.45\n-44.38%\nZS\nLLaMA-2\n69.52\n65.48\n63.81\n60.95\n57.14\n28.33\n-59.25%\n+ INFO-RAG\n72.50\n72.62\n67.62\n67.86\n60.48\n36.19\n-50.08%\nTable 8: RAG performance changes with the ratio of positive passages (randomly select 500 samples).\nDatasets\nMethod\nPosition of Positive Passage\nMax ∆\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\nNQ\nLLaMA-2\n54.94\n48.05\n46.05\n46.45\n46.35\n48.30\n48.35\n47.15\n51.64\n50.44\n-16.18%\n+ INFO-RAG\n63.23\n58.34\n54.54\n54.44\n53.54\n53.24\n53.84\n54.44\n53.34\n53.34\n-15.80%\nWebQ\nLLaMA-2\n66.13\n63.21\n62.54\n62.68\n64.01\n62.41\n63.21\n64.54\n63.87\n64.14\n-5.63%\n+ INFO-RAG\n71.58\n68.39\n66.26\n65.34\n67.19\n65.73\n65.73\n65.81\n65.54\n66.72\n-8.72%\nT-REx\nLLaMA-2\n64.43\n60.13\n58.34\n60.23\n58.54\n59.14\n59.74\n60.53\n63.53\n63.23\n-9.45%\n+ INFO-RAG\n70.72\n66.23\n64.93\n65.23\n65.43\n64.83\n66.03\n67.23\n64.63\n66.83\n-8.61%\nZS\nLLaMA-2\n63.04\n59.04\n54.59\n55.03\n55.17\n57.15\n56.42\n57.89\n58.04\n59.47\n-13.40%\n+ INFO-RAG\n66.42\n63.33\n59.04\n60.23\n61.42\n61.66\n60.00\n61.19\n60.23\n62.14\n-11.11%\nTable 9: RAG performance changes with the position of positive passage (randomly select 500 samples).\ncovers 57 subjects across STEM, the humanities,\nthe social sciences, and more. It ranges in difficulty\nfrom an elementary level to an advanced profes-\nsional level, and it tests both world knowledge and\nproblem-solving ability (Hendrycks et al., 2020).\nExperiments show that our INFO-RAG performs\nvery close to the original LLaMA-2 on MMLU,\nwhich shows that our INFO-RAG does not damage\nthe basic language understanding ability of LLMs.\nThis is mainly because the prefix language model-\ning training paradigm of our method is consistent\nwith the pre-training task of LLMs. The difference\nis that in the training of prefix language modeling,\nour method learns to perform information refine-\nment that utilizes the retrieved texts for the next\ntoken prediction.\nDatasets\nMethod\nNumber of Retrieved Passages\nMax ∆\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNQ\nLLaMA-2\n38.80\n43.21\n46.62\n47.84\n48.61\n49.42\n52.03\n50.23\n50.40\n50.20\n-25.43%\n+ INFO-RAG\n45.18\n46.80\n51.44\n51.23\n51.00\n53.21\n54.03\n53.44\n53.82\n54.60\n-17.25%\nWebQ\nLLaMA-2\n40.22\n43.63\n48.20\n46.61\n48.32\n49.11\n49.40\n50.22\n51.65\n50.43\n-22.13%\n+ INFO-RAG\n50.21\n53.84\n54.41\n55.07\n55.25\n55.27\n57.00\n55.45\n56.62\n56.03\n-11.91%\nT-REx\nLLaMA-2\n66.20\n63.45\n67.22\n64.45\n64.43\n65.40\n64.41\n65.22\n63.22\n65.01\n-5.95%\n+ INFO-RAG\n66.25\n66.03\n66.31\n65.80\n67.23\n67.22\n66.65\n67.83\n67.03\n67.40\n-2.99%\nZS\nLLaMA-2\n49.25\n50.01\n52.38\n54.09\n56.12\n56.20\n56.13\n56.05\n55.95\n56.11\n-12.37%\n+ INFO-RAG\n53.17\n54.08\n56.35\n58.01\n59.45\n59.12\n59.40\n58.55\n60.03\n59.08\n-11.43%\nTable 10: RAG performance changes with the number of retrieved passages (randomly select 500 samples).\nHumanities\nSTEM\nSocial Sciences\nOther\nAverage\nLLaMA-2-7B w/o RAG\n42.9\n36.4\n51.2\n52.2\n45.3\n+ INFO-RAG w/o RAG\n42.8\n36.1\n50.8\n52.0\n45.0\nLLaMA-2-13B w/o RAG\n52.8\n44.1\n62.6\n61.1\n54.8\n+ INFO-RAG w/o RAG\n52.5\n43.7\n62.1\n60.9\n54.3\nTable 11: Performance on MMLU in the setting without retrieval-augmented generation.\n"
}