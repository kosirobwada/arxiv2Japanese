{
    "optim": "Auto-Encoding Bayesian Inverse Games\nXinjie Liu†∗\nLasse Peters‡∗\nJavier Alonso-Mora‡\nUfuk Topcu†\nDavid Fridovich-Keil†\n†The University of Texas at Austin\n‡Delft University of Technology\n{ xinjie-liu, utopcu, dfk }@utexas.edu,\n{ l.peters, j.alonsomora }@tudelft.nl\nAbstract—When multiple agents interact in a common environ-\nment, each agent’s actions impact others’ future decisions, and\nnoncooperative dynamic games naturally capture this coupling.\nIn interactive motion planning, however, agents typically do\nnot have access to a complete model of the game, e.g., due to\nunknown objectives of other players. Therefore, we consider the\ninverse game problem, in which some properties of the game\nare unknown a priori and must be inferred from observations.\nExisting maximum likelihood estimation (MLE) approaches to\nsolve inverse games provide only point estimates of unknown\nparameters without quantifying uncertainty, and perform poorly\nwhen many parameter values explain the observed behavior. To\naddress these limitations, we take a Bayesian perspective and\nconstruct posterior distributions of game parameters. To render\ninference tractable, we employ a variational autoencoder (VAE)\nwith an embedded differentiable game solver. This structured\nVAE can be trained from an unlabeled dataset of observed\ninteractions, naturally handles continuous, multi-modal distri-\nbutions, and supports efficient sampling from the inferred pos-\nteriors without computing game solutions at runtime. Extensive\nevaluations in simulated driving scenarios demonstrate that the\nproposed approach successfully learns the prior and posterior\ngame parameter distributions, provides more accurate objective\nestimates than MLE baselines, and facilitates safer and more\nefficient game-theoretic motion planning. 1\nI. INTRODUCTION\nAutonomous robots often need to interact with other agents\nto operate seamlessly in real-world environments. For exam-\nple, in the scenario depicted in Fig. 1, a robot encounters\na human driver while navigating an intersection. In such\nsettings, coupling effects between agents significantly com-\nplicate decision-making: if the human acts assertively and\ndrives straight toward its goal, the robot will be forced to\nbrake to avoid a collision. Hence, the agents compete to\noptimize their individual objectives. Dynamic noncooperative\ngame theory [1] provides an expressive framework to model\nsuch interaction among rational, self-interested agents.\nIn many scenarios, however, a robot must handle interac-\ntions under incomplete information, e.g., without knowing the\ngoal position of the human driver in Fig. 1. To this end, a\nrecent line of work [2–11] proposes to solve inverse dynamic\ngames to infer the unknown parameters of a game—such as\nother agents’ objectives—from observed interactions.\nA common approach to solve inverse dynamic games uses\nmaximum likelihood estimation (MLE) to find the most likely\nparameters given observed behavior [3–11]. However, MLE\nsolutions provide only a point estimate without any uncertainty\nquantification and can perform poorly in scenarios where\n∗Equal contribution\n1https://xinjie-liu.github.io/projects/bayesian-inverse-games\nposterior\nsamples\npredictions\nmade by\nthe robot\nhuman\nobservations\ntrue human plan\nunknown\ntrue human\ngoal\nrobot goal\nrobot\nFig. 1: A robot interacting with a human driver whose goal\nposition is unknown. We embed a differentiable game solver in\na structured variational autoencoder to infer the distribution of\nthe human’s objectives based on observations of their behavior.\nmany parameter values explain the observations [10], yielding\noverconfident, unsafe motion plans [12].\nBayesian formulations of inverse games mitigate these lim-\nitations of MLE methods by inferring a posterior distribution,\ni.e., belief, over the unknown parameters [13, 14]. Knowledge\nof this distribution allows the robot to account for uncertainty\nand generate safer yet efficient plans [12, 14–16].\nUnfortunately, exact Bayesian inference is typically in-\ntractable in dynamic games, especially when dynamics are\nnonlinear. Prior work [14] alleviates this challenge by using\nan unscented Kalman filter (UKF) for approximate Bayesian\ninference. However, that approach is limited to unimodal\nuncertainty models and demands solving multiple games per\nbelief update, thereby posing a computational challenge.\nThe main contribution of this work is a framework for\ntractable Bayesian inference of posterior distributions over un-\nknown parameters in dynamic games. To this end, we approx-\nimate exact Bayesian inference with a structured variational\nautoencoder (VAE). During training, this VAE embeds a differ-\nentiable game solver to facilitate unsupervised learning from\nan unlabeled dataset of observed interactions. At runtime, the\nproposed approach can generate samples from the predicted\nposterior without solving additional games. As a result, our\napproach naturally captures continuous, multi-modal beliefs,\nand addresses the limitations of MLE inverse games without\nresorting to the simplifications or computational complexity of\nexisting Bayesian methods.\nThrough extensive evaluations of our method in two simu-\narXiv:2402.08902v2  [cs.RO]  16 Feb 2024\nlated driving scenarios, we support the following key claims.\nThe proposed framework (i) learns the underlying prior game\nparameter distribution from unlabeled interactions, and (ii)\ncaptures potential multi-modality of game parameters. Our ap-\nproach (iii) is uncertainty-aware, predicting narrow unimodal\nbeliefs when observations clearly reveal the intentions of other\nagents, and beliefs that are closer to the learned prior in\ncase of uninformative observations. The proposed framework\n(iv) provides more accurate inference performance than MLE\ninverse games by effectively leveraging the learned prior\ninformation, especially in settings where multiple parameter\nvalues explain the observed behavior. As a result, our approach\n(v) enables safer downstream robot plans than MLE methods.\nII. RELATED WORK\nThis section provides an overview of the literature on\ndynamic game theory, focusing on both forward games (Sec-\ntion II-A) and inverse games (Section II-B).\nA. Forward Dynamic Games\nThis work focuses on noncooperative games where agents\nhave partially conflicting but not completely adversarial goals\nand make sequential decisions over time [1]. Since we assume\nthat agents take actions simultaneously without leader-follower\nhierarchy and we consider coupling between agents’ decisions\nthrough both objectives and constraints, our focus is on\ngeneralized Nash equilibrium problems (GNEPs).\nGNEPs are challenging coupled mathematical optimization\nproblems. Nonetheless, recent years have witnessed an ad-\nvancement of efficient computation methods for GNEPs with\nsmooth objectives and constraints. Due to the computational\nchallenges involved in solving such problems under feedback\ninformation structure [17], most works aim to find open-loop\nNash equilibria (OLNE) [1] instead, where players choose\ntheir action sequence—an open-loop strategy—at once. A\nsubstantial body of work employs the iterated best response\nalgorithm to find a GNEP by iteratively solving single-\nagent optimization problems [18, 19, 19–22]. More recently,\nmethods based on sequential quadratic approximations have\nbeen proposed [23, 24], aiming to speed up convergence\nby updating all players’ strategies simultaneously at each\niteration. Finally, since the first-order necessary conditions of\nopen-loop GNEPs take the form of a mixed complementarity\nproblem (MCP) [25], several works [8, 12] solve generalized\nNash equilibria (GNE) using established MCP solvers [26, 27].\nThis work builds on the latter approach.\nB. Inverse Dynamic Games\nInverse games study the problem of inferring unknown game\nparameters, e.g. of objective functions, from observations\nof agents’ behavior [2]. In recent years, several approaches\nhave extended single-agent inverse optimal control (IOC) and\ninverse reinforcement learning (IRL) techniques to multi-agent\ninteractive settings. For instance, the approaches of [5, 6] min-\nimize the residual of agents’ first-order necessary conditions,\ngiven full state-control observations, in order to infer unknown\nobjective parameters. Inga et al. [4] extends this approach to\nmaximum-entropy settings.\nRecent work [9] proposes to maximize observation likeli-\nhood while enforcing the Karush–Kuhn–Tucker (KKT) con-\nditions of OLNE as constraints. This approach only requires\npartial-state observations and can cope with noise-corrupted\ndata. Approaches [8, 10] propose an extension of the MLE\napproach [9] to inverse feedback and open-loop games with\ninequality constraints via differentiable forward game solvers.\nTo amortize the computation of MLE, works [8, 28] demon-\nstrate integration with neural network (NN) components.\nIn general, MLE solutions can be understood as point\nestimates of Bayesian posteriors, assuming a uniform prior [29,\nCh.4]. When multiple parameter values explain the observa-\ntions equally well, this simplifying assumption can result in\nill-posed problems—causing MLE inverse games to recover\npotentially inaccurate estimates [10]. Moreover, in the con-\ntext of motion planning, the use of point estimates without\nawareness of uncertainty can result in unsafe plans [12, 16].\nTo address these issues, several recent works take a\nBayesian view on inverse games [13, 14], aiming to infer a\nposterior distribution while factoring in prior knowledge. Since\nexact Bayesian inference is intractable in these problems, the\nbelief update may be approximated via a particle filter [13].\nHowever, this approach requires solving a large number of\nequilibrium problems online to maintain the belief distribution,\nposing a significant computational burden. A sigma-point ap-\nproximation [14] reduces the number of required samples but\nlimits the estimator to unimodal uncertainty models. Recently,\nDiehl et al. [30] trained a NN that embeds differentiable\noptimization to infer a categorical distribution over intents.\nThat approach, however, fixes the number of modes a priori,\nrequires training data labeled with ground-truth intents, and is\nlimited to the class of potential games.\nTo overcome the limitations of MLE approaches while\navoiding the intractability of exact Bayesian inference, we\npropose to approximate the posterior via a VAE [31] that\nembeds a differentiable game solver [8] during training. The\nproposed approach can be trained from an unlabeled dataset\nof observed interactions, naturally handles continuous, multi-\nmodal distributions, and does not require computation of game\nsolutions at runtime to sample from the posterior.\nIII. PRELIMINARIES: GENERALIZED NASH GAME\nIn this work, we consider strategic interactions of self-\ninterested rational agents in the framework of generalized\nNash equilibrium problems (GNEPs). In this framework, each\nof N agents seeks to unilaterally minimize their respective cost\nwhile being conscious of the fact that their “opponents”, too,\nact in their own best interest. We define a parametric N-player\nGNEP as N coupled, constrained optimization problems,\nSi\nθ(τ ¬i) := arg min\nτi\nJi\nθ(τ i, τ ¬i)\n(1a)\ns.t.\ngi\nθ(τ i, τ ¬i) ≥ 0,\n(1b)\nwhere θ ∈ Rp denotes a parameter vector whose role will\nbecome clear below, and player i ∈ [N] := {1, . . . , N}\nhas cost function Ji\nθ and private constraints gi\nθ. Furthermore,\nobserve that the costs and constraints of player i depend not\nonly on their own strategy τ i ∈ Rmi, but also on the strategy\nof all other players, τ ¬i ∈ R\nP\nj∈[N]\\{i} mj.\nGeneralized Nash Equilibria. For a given parameter θ, the\nsolution of a GNEP is an equilibrium strategy profile τ ∗ :=\n(τ 1∗, . . . , τ N∗) so that each agent’s strategy is a best response\nto the others’, i.e.,\nτ i∗ ∈ Si\nθ(τ ¬i∗), ∀i ∈ [N].\n(2)\nIntuitively, at a GNE, no player can further reduce their cost\nby unilaterally adopting another feasible strategy.\nExample: Online Game-Theoretic Motion Planning. This\nwork focuses on applying games to online motion planning in\ninteraction with other agents, such as the intersection scenario\nshown in Fig. 1. In this context, the strategy of agent i,\nτi, represents a trajectory—i.e., a sequence of states and\ninputs extended over a finite horizon—which is recomputed\nin a receding-horizon fashion. This paradigm results in the\ngame-theoretic equivalent of model-predictive control (MPC):\nmodel-predictive game-play (MPGP). The construction a tra-\njectory game largely follows the procedure used in single-\nagent trajectory optimization: gi\nθ(·) encodes input and state\nconstraints including those enforcing dynamic feasibility, col-\nlision avoidance, and road geometry; and Ji\nθ(·) encodes the\nith agent’s objective such as reference tracking. Adopting\na convention in which index 1 refers to the ego agent, the\nequilibrium solution of the game then serves two purposes si-\nmultaneously: the opponents’ solution, τ ¬1∗, serves as a game-\ntheoretic prediction of their behavior while the ego agent’s\nsolution, τ 1∗, provides the corresponding best response.\nThe Role of Game Parameters θ. A key difference between\ntrajectory games and single-agent trajectory optimization is the\nrequirement to provide the costs and constraints of all agents.\nIn practice, a robot may have insufficient knowledge of their\nopponents’ intents, dynamics, or state to instantiate a complete\ngame-theoretic model. This aspect motivates the parameterized\nformulation of the game above: for the remainder of this\nmanuscript, θ will capture the unknown aspects of the game.\nIn the context of game-theoretic motion planning, e.g. to\nmodel agents navigating an intersection, θ typically includes\naspects of opponents’ preferences such as their unknown\ndesired lane or preferred velocity. For conciseness, we denote\ngame (1) compactly via the parametric tuple of problem data\nΓ(θ) := ({Ji\nθ, gi\nθ}i∈[N]). Next, we discuss how to infer these\nparameters online from observed interactions.\nIV. FORMALIZING BAYESIAN INVERSE GAMES\nThis section presents our main contribution: a framework for\ninferring unknown game parameters θ based on observations y.\nThis problem is commonly referred to as an inverse game [2].\nSeveral prior works on inverse games [9, 10, 32] seek\nto find game parameters that directly maximize observation\nlikelihood. However, this MLE formulation of inverse games\nunaugmented model\ngame\nsolution\nobservation\nlatent posterior surrogate\ndiﬀerentiable\ngame solver\ngame\nparameters\nencoder\nNN\ndecoder\nNN\nauxiliary\nvariable\naugmentation\nFig. 2: Overview of a structured VAE for generative Bayesian\ninverse games. Top (left to right): decoder pipeline. Bottom\n(right to left): variational inference process via an encoder.\n(i) only provides a point estimate of the unknown game pa-\nrameters θ, thereby precluding the consideration of uncertainty\nin downstream tasks such as motion planning; and (ii) fails to\nprovide reasonable parameter estimates when observations are\nuninformative, as we shall also demonstrate in Section V.\nA. A Bayesian View on Inverse Games\nIn order to address the limitations of the MLE inverse\ngames, we consider a Bayesian formulation of inverse games,\nand seek to construct the belief distribution\nb(θ) = p(θ | y) = p(y | θ)p(θ)\np(y)\n.\n(3)\nIn contrast to MLE inverse games, this formulation provides a\nfull posterior distribution over the unknown game parameters\nθ and factors in prior knowledge p(θ).\nObservation Model p(y | θ). In autonomous online oper-\nation of a robot, y represents the (partial) observations of\nother players’ recent trajectories—e.g., from a fixed lag buffer\nas shown in orange in Fig. 1. Like prior works on inverse\ngames [9, 10, 14], we assume that, given the unobserved\ntrue trajectory of the human, y is Gaussian-distributed; i.e.,\np(y | τ) = N(y | µy(τ), Σy(τ)). Assuming that the underly-\ning trajectory is the solution of a game with known structure Γ\nbut unknown parameters θ, we express the observation model\nas p(y | θ) = p(y | TΓ(θ)), where TΓ denotes a game solver\nthat returns a solution τ ∗ of the game Γ(θ).\nChallenges of Bayesian Inverse Games. While the Bayesian\nformulation of inverse games in Eq. (3) is conceptually\nstraightforward, it poses several challenges:\n(i) The prior p(θ) is typically unavailable and instead must\nbe learned from data.\n(ii) The computation of the normalizing constant, p(y) =\nR\np(y | θ)p(θ)dθ, is intractable in practice due to the\nmarginalization of θ.\n(iii) Both the prior, p(θ), and posterior p(θ | y) are in general\nnon-Gaussian or even multi-modal and are therefore dif-\nficult to represent explicitly in terms of their probability\ndensity function (PDF).\nPrior work [14] partially mitigates these challenges by using\nan UKF for approximate Bayesian inference, but that approach\nis limited to unimodal uncertainty models and requires solving\nmultiple games for a single belief update, thereby posing a\ncomputational challenge.\nFortunately, as we shall demonstrate in Section V, many\npractical applications of inverse games do not require an\nexplicit evaluation of the belief PDF, b(θ). Instead, a gen-\nerative model of the belief—i.e., one that allows drawing\nsamples θ ∼ b(θ)—often suffices. Throughout this section,\nwe demonstrate how to learn such a generative model from\nan unlabeled dataset D = {yk | yk ∼ p(y), ∀k ∈ [K]} of\nindependent and identically distributed observed interactions.\nB. Augmentation to Yield a Generative Model\nTo obtain a generative model of the belief b(θ), we aug-\nment our Bayesian model as summarized in the top half\nof Fig. 2. The goal of this augmentation is to obtain a\nmodel structure that lends itself to approximate inference\nin the framework of variational autoencoders (VAEs) [31].\nSpecifically, we introduce an auxiliary random variable z with\nknown standard-normal prior p(z) = N(z), and a deter-\nministic decoder dϕ with unknown parameters ϕ. Following\nthe generative process outlined in Fig. 2 from left to right,\nthe decoder maps z to the game parameters θ and hence\nwe have pϕ(θ | z) = δ\n\u0000dϕ(z) − θ\n\u0001\n, where δ denotes the\nDirac delta function. Next, the game solver TΓ produces the\nGNE solution τ ∗ which determines the observation likelihood\np(y | τ). In terms of probability distributions, dϕ induces\nthe conditional pϕ(y | z) =\nR\np(y | θ)pϕ(θ | z)dθ, the\nprior pϕ(θ) =\nR\npϕ(θ | z)p(z)dz, and the data distribution\npϕ(y) =\nR\np(y | θ)pϕ(θ)dθ.\nC. Auto-Encoding Bayesian Inverse Games\nIn the augmented model of Section IV-B, knowledge of ϕ\nand the latent posterior pϕ(z | y) = pϕ(y | z)p(z)/pϕ(y)\nsuffices to specify generative models for the prior pϕ(θ) and\nposterior pϕ(θ | y). That is, by propagating samples from the\nlatent prior p(z) through dϕ, we implicitly generate samples\nfrom the prior pϕ(θ). Similarly, by propagating samples from\nthe latent posterior pϕ(z | y) through dϕ, we implicitly\ngenerate samples from the posterior pϕ(θ | y). We thus have\nconverted the problem of (generative) Bayesian inverse games\nto estimation of ϕ and inference of pϕ(z | y).\nRecognizing the similarity of the generative process from z\nto y outlined in Fig. 2 to the decoding process in a VAE,\nwe seek to approximate the latent posterior pϕ(z | y) with\na Gaussian qψ(z\n| y) = N(z\n| eψ(y)). Here, in the\nlanguage of VAEs, eψ takes the role of an encoder that maps\nobservation y to the mean µz(y) and covariance matrix Σz(y)\nof the Gaussian posterior approximation; cf. bottom of Fig. 2.\nThe key difference between our pipeline and a conventional\nVAE is the special structure of the generative process from\nlatent z to observation y. Within this process, the decoder\nnetwork dϕ is composed with the game solver TΓ to produce an\nequilibrium strategy that parameterizes the observation model.\nBuilding on the approach of Liu et al. [8], we can recover\nthe gradient of the game solver TΓ with respect to the game\nparameters θ. It is this gradient information from the game-\ntheoretic “layer” in the decoding pipeline that induces an\ninterpretable structure on the output of the decoder-NN dϕ,\nforcing it to predict the hidden game parameters θ.\nRemark. To generate samples from the estimated posterior\nqψ,ϕ(θ | y) =\nR\npϕ(θ | z)qψ(z | y)dz, we do not need to\nevaluate the game solver TΓ: posterior sampling involves only\nthe evaluation of NNs dϕ and eψ; cf. y → θ in Fig. 2.\nNext, we discuss the training of the decoder and encoder\nnetworks dϕ and eψ of our structured VAE.\nD. Training the Structured Variational Autoencoder\nBelow, we outline the process for optimizing model param-\neters in our specific setting. For a general discussion of VAEs\nand variational inference (VI), refer to Murphy [33].\nFitting a Prior to D. First, we discuss the identification of\nthe prior parameters ϕ. Specifically, we seek to choose these\nparameters so that the data distribution induced by ϕ, i.e.,\npϕ(y) =\nR\npϕ(y | z)p(z)dθ, closely matches the unknown\ntrue data distribution p(y). For this purpose, we measure\ncloseness between distributions using the Kullback–Leibler\n(KL) divergence\nDKL (p ∥ q) := Ex∼p(x) [log p(x) − log q(x)] .\n(4)\nThe key properties of this divergence metric, DKL (p ∥ q) ≥ 0\nand DKL (p ∥ q) = 0\n⇐⇒\np = q, allow us to cast the\nestimation of ϕ as an optimization problem:\nϕ∗\n∈ arg min\nϕ\nDKL (p(y) ∥ pϕ(y))\n(5a)\n= arg min\nϕ\n− Ey∼p(y)\n\u0002\nlog Ez∼p(z) [pϕ(y | z)]\n\u0003\n. (5b)\nWith the prior recovered, we turn to the approximation of the\nposterior pϕ(θ | y).\nVariational Belief Inference. We can find the closest sur-\nrogate qψ(z | y) of pϕ(z | y) by minimizing the expected\nKL divergence between the two distributions over the data\ndistribution y ∼ p(y) using the framework of VI:\nψ∗ ∈ arg min\nψ Ey∼p(y) [DKL (qψ(z | y) ∥ pϕ(z | y))]\n(6a)\n= arg min\nψ E y∼p(y)\nz∼qψ(z|y)\n[ℓ(ψ, ϕ, y, z)] ,\n(6b)\nwhere\nℓ(ψ, ϕ, y, z) := log qψ(z | y)\n|\n{z\n}\nN (z|eψ(y))\n− log\npϕ(y | z)\n|\n{z\n}\nN (y|(TΓ◦dϕ)(z))\n− log p(z)\n|{z}\nN (z)\n.\nWe have therefore outlined, in theory, the methodology to\ndetermine all parameters of the pipeline from the unlabeled\ndataset D.\nConsiderations for Practical Realization. To solve Eqs. (5)\nand (6) in practice, we must overcome two main challenges:\nfirst, the loss landscapes are highly nonlinear due to the\nnonlinear transformations eψ, dϕ and TΓ; and second, the\nexpected values cannot be computed in closed form. These\nchallenges can be addressed by taking a stochastic first-order\noptimization approach, such as stochastic gradient descent\n(SGD), thereby limiting the search to a local optimum and\napproximating the gradients of the expected values via Monte\nCarlo sampling. To facilitate SGD, we seek to construct\nunbiased gradient estimators of the objectives of Eq. (5b)\nand Eq. (6b) from gradients computed at individual samples.\nAn unbiased gradient estimator is straightforward to define\nwhen the following conditions hold: (C1) any expectations\nappear on the outside, and (C2) the sampling distribution of the\nexpectations are independent of the variable of differentiation.\nThe objective of the optimization problem in Eq. (6b) takes\nthe form\nL(q, ϕ) := E y∼p(y)\nz∼q(z|y)\n[ℓ(ψ, ϕ, y, z)] ,\n(7)\nwhich clearly satisfies C1. Similarly, it is easy to verify that\nthe objective of Eq. (5b) can be identified as L(pϕ(z | y), ·).\nUnfortunately, this latter insight is not immediately actionable\nsince pϕ(z | y) is not readily available. However, if instead we\nuse our surrogate model from Eq. (6)—which, by construction\nclosely matches pϕ(z | y)—we can cast the estimation of the\nprior and posterior model as a joint optimization of L:\n˜ψ∗, ˜ϕ∗ ∈ arg min\nψ,ϕ L(qψ, ϕ).\n(8)\nWhen\neψ\nand\ndϕ\nare\nsufficiently\nexpressive\nto\nallow\nDKL (qψ(z | y) ∥ pϕ(z | y)) = 0, then this reformulation is\nexact; i.e., ˜ϕ∗ and ˜ψ∗ are also minimizers of the original\nproblems Eq. (5) and Eq. (6), respectively. In practice, a\nperfect match of distributions is not typically achieved and\n˜ϕ∗ and ˜ψ∗ are biased. Nonetheless, the scalability enabled by\nthis reformulation has been demonstrated to enable generative\nmodeling of complex distributions, including those of real-\nworld images [34]. Finally, to also satisfy C2 for the objective\nof Eq. (8), we apply the well-established “reparameterization\ntrick” to cast the inner expectation over a sampling distribution\nindependent of ψ. That is, we write\nL(qψ, ϕ) = Ey∼p(y)\nϵ∼N\n\u0002\nℓ(ψ, ϕ, y, rqψ(ϵ, y))\n\u0003\n,\n(9)\nwhere the inner expectation is taken over ϵ that has multi-\nvariate standard normal distribution, and rqψ(·, y) defines a\nbijection from ϵ to z so that z ∼ qψ(· | y). Since we model the\nlatent posterior as Gaussian—i.e., qψ(z | y) = N(z | eψ(y))—\nthe reparameterization map is rqψ(ϵ, y) := µz(y) + Lz(y)ϵ,\nwhere LzL⊤\nz = Σz is a Cholesky decomposition of the latent\ncovariance. Observe that Eq. (9) only involves Gaussian PDFs,\nand all terms can be easily evaluated in closed form.\nStochastic Optimization. As in SGD-based training of con-\nventional VAEs, at iteration k, we sample yk ∼ D, and\nencode yk into the parameters of the latent distribution via eψk.\nFrom the latent distribution we then sample zk, and evaluate\nthe unbiased gradient estimators ∇ψℓ(ψ, ϕk, yk, zk)|ψ=ψk and\n∇ϕℓ(ψk, ϕ, yk, zk)|ϕ=ϕk of L at the current parameter iterates,\nψk and ϕk. Due to our model’s special structure, the evalu-\nation of these gradient estimators thereby also involves the\ndifferentiation of the game solver TΓ.\nV. EXPERIMENTS\nTo assess the proposed approach, we evaluate its online\ninference capabilities and its efficacy in downstream motion\nplanning tasks in two simulated driving scenarios.\nA. Two-Player Intersection Game\nFirst, we evaluate the proposed framework for downstream\nmotion planning tasks in the intersection scenario depicted\nin Fig. 1. This experiment is designed to validate hypotheses:\n• H1 (Inference Accuracy). Our method provides more\naccurate inference performance than MLE by leveraging\nthe learned prior information, especially in settings where\nmultiple human objectives explain the observations.\n• H2 (Multi-modality). Our approach predicts posterior\ndistributions that capture the multi-modality of agents’\nobjectives and behavior.\n• H3 (Planning Safety). Bayesian inverse game solutions\nenable safer robot plans over the MLE methods.\n1) Experiment Setup: In the test scenario, shown in Fig. 3,\nthe red robot must navigate an intersection, while interact-\ning with the green human whose goal position is initially\nunknown. In each simulated interaction, the human’s goal\nis randomly sampled from a Gaussian mixture distribution\nwith equal probabilities for two mixture components: one for\nturning left, and one for going straight.\nWe model the agents’ dynamics as kinematic bicycles. The\nstate at time step t includes position, longitudinal velocity,\nand orientation, i.e., xi\nt = (pi\nx,t, pi\ny,t, vi\nt, ξi\nt), and the control\ncomprises acceleration and steering angle, i.e., ui\nt = (ai\nt, ηi\nt).\nWe assign player index 1 to the robot and index 2 to the\nhuman. Over a planning horizon of\nT = 15 time steps,\neach player i seeks to minimize a cost function that encodes\nincentives for reaching the goal, reducing control effort, and\navoiding collision:\nJi\nθ =\nT −1\nX\nt=1\n∥pi\nt+1 − pi\ngoal∥2\n2 + 0.1∥ui\nt∥2\n2\n+ 400 max(0, dmin − ∥pi\nt+1 − p¬i\nt+1∥2)3,\n(10)\nwhere pi\nt = (pi\nx,t, pi\ny,t) denotes agent i’s position at time step t,\npi\ngoal = (pi\ngoal,x, pi\ngoal,y) denotes their goal position, and dmin\ndenotes a preferred minimum distance to other agents. The\nunknown parameter θ inferred by the robot contains the two-\ndimensional goal position p2\ngoal of the human.\nWe train the structured VAE from Section IV-C on a dataset\nof observations from 560 closed-loop interaction episodes\nobtained by solving dynamic games with the opponent’s\nground truth goals sampled from the Gaussian mixture de-\nscribed above. The 560 closed-loop interactions are sliced\ninto 34600 15-step observations y. Partial-state observations\nconsist of the agents’ positions and orientations. We employ\nfully-connected feedforward NNs with two 128-dimensional\nand 80-dimensional hidden layers as the encoder model eψ\nand decoder model dϕ, respectively. The latent variable z\nFig. 3: Qualitative behavior of B-PinE (ours) vs. R-MLE. In the bottom row, the size of the green stars increases with time.\nFig. 4: Negative observation log-likelihood − log p(y | p2\ngoal)\nfor varying human goal positions p2\ngoal at two time steps of\nthe R-MLE trial in Fig. 3.\nis 16-dimensional. We train the VAE with Adam [35] for 100\nepochs, which took 14 hours of wall-clock time.\n2) Baselines: We evaluate the following methods to control\nthe robot interacting with a human of unknown intent:\n• Ground truth (GT): This planner generates robot plans\nby solving games with access to ground truth opponent\nobjectives. We include this oracle baseline to provide a\nreference upper-bound on planner performance.\n• Bayesian inverse game (ours) + planning in expec-\ntation (B-PinE): This planner solves multi-hypothesis\ngames [12] in which the robot minimizes the expected\ncost Eθ∼qψ,ϕ(θ|y)\n\u0002\nJ1\nθ\n\u0003\nunder the posterior distributions\npredicted by our structured VAE based on new observa-\ntions at each time step.\n• Bayesian inverse game (ours) + maximum a posteriori\n(MAP) planning (B-MAP): Instead of using the full\nposteriors from our framework, this approach constructs\nMAP estimates ˆθMAP ∈ arg maxθ qψ,ϕ(θ | y) from the\nposteriors and solves the game Γ(ˆθMAP).\n• Randomly initialized MLE planning (R-MLE): This\nbaseline [8] solves the game Γ(ˆθMLE), where ˆθMLE ∈\narg maxθ p(y | θ). The MLE problems are solved online\nvia gradient descent based on new observations at each\ntime step as in [8]. The initial guess for optimization is\nsampled uniformly from a rectangular region covering all\npotential ground truth goal positions.\n• Bayesian prior initialized MLE planning (BP-MLE):\nInstead of uniformly sampling heuristic initial guesses as\nin R-MLE, this baseline solves for the MLE with initial\nguesses from the Bayesian prior learned by our approach.\n• Static Bayesian prior planning (St-BP): This baseline\nsamples ˆθ from the learned Bayesian prior and uses the\nsample as a fixed human objective estimate to solve Γ(ˆθ).\nThis baseline is designed as an ablation study of the effect\nof online objective inference.\nFor those planners that utilize our VAE for inference, we\ntake 1000 samples at each time step to approximate the\ndistribution of human objectives, which takes ≈7 ms. For B-\nPinE, we cluster the posterior samples into two groups to be\ncompatible with the multi-hypothesis game solver from [12].\n3) Qualitative Behavior: Figure 3 illustrates the qualitative\nbehavior of B-PinE and R-MLE.\nB-PinE. The top row of Fig. 3 shows that our approach\ninitially generates a bimodal belief, capturing the distribution\nFig. 5: Quantitative results of S1: (a) Minimum distance between agents in each trial. (b) Robot costs (with GT costs subtracted).\nFig. 6: Quantitative results of S2 and S3: (a) Minimum distance between agents in each trial of S2. (b-c) Robot costs of S2-3\n(with GT costs subtracted).\nof potential opponent goals. In the face of this uncertainty,\nthe planner initially computes a more conservative trajectory\nto remain safe. As the human approaches the intersection and\nreveals its intent, the left-turning mode gains probability mass\nuntil the computed belief eventually collapses to a unimodal\ndistribution. Throughout the interaction, the predictions gen-\nerated by the multi-hypothesis game accurately cover the true\nhuman plan, allowing safe and efficient interaction.\nR-MLE. As shown in the bottom row of Fig. 3, the R-MLE\nbaseline initially estimates that the human will go straight. The\ntrue human goal only becomes clear later in the interaction\nand the over-confident plans derived from these poor point\nestimates eventually lead to a collision.\nObservability Issues of MLE. To illustrate the underlying\nissues that caused the poor performance of the R-MLE base-\nline discussed above, Fig. 4 shows the negative observation\nlog-likelihood for two time steps of the interaction. As can be\nseen, a large region in the game parameter space explains the\nobserved behavior well, and the baseline incorrectly concludes\nthat the opponent’s goal is always in front of their current\nposition; cf. top of Fig. 3. In contrast, by leveraging the learned\nprior distribution, our approach predicts objective samples\nthat capture potential opponent goals well even when the\nobservation is uninformative; cf. bottom of Fig. 3. In summary,\nthese results validate hypotheses H1 and H2.\n4) Quantitative Analysis: To quantify the performance of\nall six planners, we run a Monte Carlo study of 1500 trials.\nIn each trial, we randomly sample the robot’s initial position\nalong the lane from a uniform distribution such that the result-\ning ground truth interaction covers a spectrum of behaviors,\nranging from the robot entering the intersection first to the\nhuman entering the intersection first. We use the minimum\ndistance between players in each trial to measure safety and\nthe robot’s cost as a metric for interaction efficiency.\nWe group the trials into three settings based on the ground-\ntruth behavior of the agents: (S1) The human turns left and\npasses the intersection first. In this setting, a robot recognizing\nthe human’s intent should yield, whereas blindly optimizing\nthe goal-reaching objective will likely lead to unsafe inter-\naction. (S2) The human turns left, but the robot reaches\nthe intersection first. In this setting, we expect the effect of\ninaccurate goal estimates to be less pronounced than in S1\nsince the robot passes the human on the right irrespective of\ntheir true intent. (S3) The human drives straight. Safety is\ntrivially achieved in this setting.\nResults, S1. Figure 5(a) shows that methods using our frame-\nwork achieve better planning safety than other baselines,\nclosely matching the ground truth in this metric. Using the\nminimum distance between agents among all ground truth\ntrials as a collision threshold, the collision rates of the\nmethods are 0.0% (B-PinE), 0.78% (B-MAP), 17.05% (R-\nMLE), 16.28% (BP-MLE), and 17.83% (St-BP), respectively.\nMoreover, the improved safety of the planners using Bayesian\ninference does not come at the cost of reduced planning\nefficiency. As shown in\nFig. 5(b-c), B-PinE and B-MAP\nconsistently achieve low interaction costs.\nResults, S2. Figure 6(a) shows that all the approaches except\nfor the St-BP baseline approximately achieve ground truth\nsafety in this less challenging setting. While the gap between\napproaches is less pronounced, we still find improved planning\nsafety for our methods over the baselines. Taking the same\ncollision distance threshold as in S1, the collision rates of\nthe methods are 0.86% (B-PinE), 2.24% (B-MAP), 7.59%\n(R-MLE), 6.03% (BP-MLE), and 7.59% (St-BP), respec-\ntively. B-MAP gives less efficient performance in this setting.\nCompared with B-PinE, B-MAP only uses point estimates\nof the unknown goals and commits to over-confident and\nmore aggressive behavior. We observe that this aggressiveness\nresults in coordination issues when the two agents enter the\nintersection nearly simultaneously, causing them to acceler-\nate simultaneously and then brake together. Conversely, the\nuncertainty-aware B-PinE variant of our method does not\nexperience coordination failures, highlighting the advantage of\nincorporating the inferred distribution during planning. Lastly,\nSt-BP exhibits the poorest performance in S2, stressing the\nimportance of online objective inference.\nResults, S3. In Fig. 6(d), B-MAP achieves the highest effi-\nciency, while B-PinE produces several trials with increased\ncosts due to more conservative planning. Again, the St-BP\nbaseline exhibits the poorest performance.\nSummary. These quantitative results show that our Bayesian\ninverse game approach improves motion planning safety over\nthe MLE baselines, validating hypothesis\nH3. Between B-\nPinE and B-MAP, we observe improved safety in S1-2 and\nefficiency in S2.\nB. Two-Player Highway Game\nThis experiment is designed to provide more detailed\nqualitative results on distributions inferred by the proposed\napproach and validate the following hypotheses:\n• H4 (Bayesian Prior). The proposed approach learns the\nunderlying multi-modal prior objective distribution.\n• H5 (Uncertainty Awareness). The proposed approach\ncomputes a narrow, unimodal belief when the unknown\nobjective is clearly observable and computes a wide,\nmulti-modal belief that is closer to the prior in case of\nuninformative observations.\nWe consider a two-player highway driving scenario, in which\nan ego robot drives in front of a human opponent and is\ntasked to infer the human’s desired driving speed. The rear\nvehicle is responsible for decelerating and avoiding collisions,\nand the front vehicle always drives at their desired speed. To\nsimplify the visualization of beliefs and thereby illustrate H4-\n5, we reduce the setting to a single spatial dimension and\nmodel one-dimensional uncertainty: we limit agents to drive\nin a single lane and model the agents’ dynamics as double\nintegrators with longitudinal position and velocity as states\nand acceleration as controls; furthermore, we employ a VAE\nwith the same hidden layers as in Section V-A but only one-\ndimensional latent space. The VAE takes a 15-step observation\nof the two players’ velocities as input for inference.\nWe collect a dataset of 20000 observations to train a VAE. In\neach trial, the ego agent’s reference velocity is sampled from a\nuniform distribution from 0 m s−1 to the maximum velocity of\n20 m s−1; the opponent’s desired velocity is sampled from a bi-\nmodal Gaussian mixture distribution shown in grey in Fig. 7(a)\nwith two unit-variance mixture components at means of 30%\nand 70% of the maximum velocity.\n(a) Prior distributions.\n(b) Posterior distributions.\nFig. 7: (a) Learned and ground truth priors for the opponent’s\nobjective. (b) Inferred objective posterior distributions.\nFigure 7(a) shows that the learned prior objective distribu-\ntion captures the underlying training set distribution closely\nand thereby validates our hypothesis H4. Figure 7(b) shows\nbeliefs inferred by our approach for selected trials. The\nproposed Bayesian inverse game approach recovers a narrow\ndistribution with low uncertainty when the opponent’s desired\nspeed is clearly observable, e.g., when the front vehicle drives\nfaster so that the rear vehicle can drive at their desired speed\n(Fig. 7, subfigures 1, 3). The rear driver’s objective becomes\nunobservable when they wish to drive fast but are blocked\nby the car in front. In this case, our approach infers a wide,\nmulti-modal distribution with high uncertainty that is closer\nto the prior distribution (Fig. 7, subfigures 2, 4). These results\nvalidate hypothesis H5.\nVI. CONCLUSION & FUTURE WORK\nWe presented an approach for Bayesian inference in dy-\nnamic games, which can tractably infer posterior distribu-\ntions over unknown game parameters. The core computational\nenabler of this technique is a VAE that approximates the\ntrue posterior by embedding a differentiable game solver [8]\nduring training. This structured VAE can be trained from an\nunlabeled dataset of observed interactions, naturally handles\ncontinuous, multi-modal distributions, and supports efficient\nsampling from the inferred posteriors without solving games\nat runtime.\nIn two simulated driving scenarios, we thoroughly assessed\nour method’s capability for online inference and its efficacy in\nmotion planning tasks. The findings show that our approach\nsuccessfully learns prior game parameter distributions and\nutilizes this knowledge for inference of multi-modal posterior\ndistributions. Compared to MLE inverse game approaches, our\nmethod provides more accurate game parameter estimates,\nfacilitating safer and more efficient game-theoretic motion\nplanning.\nThis work explored the combination of the proposed in-\nference pipeline with two game-theoretic planners, both of\nwhich showed promising results compared to non-Bayesian\nbaselines. Future work should investigate the combination with\nother sophisticated planning frameworks, including applica-\ntions of active information gathering and signaling of ego\nagent intents to other agents in the environment.\nREFERENCES\n[1] Tamer Bas¸ar and Geert Jan Olsder. Dynamic Noncoop-\nerative Game Theory. Society for Industrial and Applied\nMathematics (SIAM), 2. edition, 1999.\n[2] Kevin Waugh, Brian D Ziebart, and J Andrew Bagnell.\nComputational rationalization: The inverse equilibrium\nproblem. arXiv preprint arXiv:1308.3506, 2013.\n[3] Florian K¨opf, Jairo Inga, Simon Rothfuß, Michael Flad,\nand S¨oren Hohmann. Inverse reinforcement learning for\nidentification in linear-quadratic dynamic games. IFAC-\nPapersOnLine, 50(1):14902–14908, 2017.\n[4] Jairo Inga, Esther Bischoff, Florian K¨opf, and S¨oren\nHohmann. Inverse dynamic games based on maximum\nentropy inverse reinforcement learning. arXiv preprint\narXiv:1911.07503, 2019.\n[5] Chaitanya Awasthi and Andrew Lamperski.\nInverse\ndifferential games with mixed inequality constraints. In\nProc. of the IEEE American Control Conference (ACC),\n2020.\n[6] Simon Rothfuß, Jairo Inga, Florian K¨opf, Michael Flad,\nand S¨oren Hohmann. Inverse optimal control for iden-\ntification in non-cooperative differential games. IFAC-\nPapersOnLine, 50(1):14909–14915, 2017. ISSN 2405-\n8963. doi: https://doi.org/10.1016/j.ifacol.2017.08.2538.\n[7] Tianyu Qiu and David Fridovich-Keil. Identifying oc-\ncluded agents in dynamic games with noise-corrupted\nobservations. arXiv preprint arXiv:2303.09744, 2023.\n[8] Xinjie Liu, Lasse Peters, and Javier Alonso-Mora. Learn-\ning to play trajectory games against opponents with\nunknown objectives.\nIEEE Robotics and Automation\nLetters, 8(7):4139–4146, 2023. doi: 10.1109/LRA.2023.\n3280809.\n[9] Lasse Peters, Vicenc Rubies-Royo, Claire J. Tomlin,\nLaura Ferranti, Javier Alonso-Mora, Cyrill Stachniss,\nand David Fridovich-Keil. Online and offline learning\nof player objectives from partial observations in dy-\nnamic games.\nIn Intl. Journal of Robotics Research\n(IJRR), 2023. URL https://journals.sagepub.com/doi/pdf/\n10.1177/02783649231182453.\n[10] Jingqi Li, Chih-Yuan Chiu, Lasse Peters, Somayeh So-\njoudi, Claire Tomlin, and David Fridovich-Keil.\nCost\ninference for feedback dynamic games from noisy partial\nstate observations and incomplete trajectories.\narXiv\npreprint arXiv:2301.01398, 2023.\n[11] Negar Mehr, Mingyu Wang, Maulik Bhatt, and Mac\nSchwager.\nMaximum-entropy multi-agent dynamic\ngames: Forward and inverse solutions. IEEE Trans. on\nRobotics (TRO), 39(3):1801–1815, 2023. doi: 10.1109/\nTRO.2022.3232300.\n[12] Lasse Peters, Andrea Bajcsy, Chih-Yuan Chiu, David\nFridovich-Keil, Forrest Laine, Laura Ferranti, and Javier\nAlonso-Mora. Contingency games for multi-agent inter-\naction. IEEE Robotics and Automation Letters (RA-L),\n2024.\n[13] Lasse Peters. Accommodating intention uncertainty in\ngeneral-sum games for human-robot interaction. Master’s\nthesis, Hamburg University of Technology, 2020.\n[14] Simon Le Cleac’h, Mac Schwager, and Zachary Manch-\nester. LUCIDGames: Online unscented inverse dynamic\ngames for adaptive trajectory prediction and planning.\nIEEE Robotics and Automation Letters (RA-L), 6(3):\n5485–5492, 2021.\n[15] Wilko Schwarting, Alyssa Pierson, Sertac Karaman, and\nDaniela Rus. Stochastic dynamic games in belief space.\nIEEE Transactions on Robotics, 37(6):2157–2172, 2021.\n[16] Haimin Hu and Jaime F Fisac.\nActive uncertainty\nreduction for human-robot interaction: An implicit dual\ncontrol approach.\nIn Intl. Workshop on the Algorith-\nmic Foundations of Robotics (WAFR), pages 385–401.\nSpringer, 2022.\n[17] Forrest Laine, David Fridovich-Keil, Chih-Yuan Chiu,\nand Claire Tomlin.\nThe computation of approximate\ngeneralized feedback Nash equilibria.\narXiv preprint\narXiv:2101.02900, 2021.\n[18] Grady Williams, Brian Goldfain, Paul Drews, James M\nRehg, and Evangelos A Theodorou.\nBest response\nmodel predictive control for agile interactions between\nautonomous ground vehicles. In 2018 IEEE International\nConference on Robotics and Automation (ICRA), 2018.\n[19] Riccardo Spica, Eric Cristofalo, Zijian Wang, Eduardo\nMontijano, and Mac Schwager. A real-time game the-\noretic planner for autonomous two-player drone racing.\nIEEE Trans. on Robotics (TRO), 36(5):1389–1403, 2020.\n[20] Zijian Wang, Riccardo Spica, and Mac Schwager. Game\ntheoretic motion planning for multi-robot racing.\nIn\nDistributed Autonomous Robotic Systems: The 14th In-\nternational Symposium, pages 225–238. Springer, 2019.\n[21] Mingyu Wang, Zijian Wang, John Talbot, J. Christian\nGerdes, and Mac Schwager. Game-theoretic planning for\nself-driving cars in multivehicle competitive scenarios.\nIEEE Transactions on Robotics, 37(4):1313–1325, 2021.\ndoi: 10.1109/TRO.2020.3047521.\n[22] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora,\nSertac Karaman, and Daniela Rus.\nSocial behavior\nfor autonomous vehicles. Proceedings of the National\nAcademy of Sciences, 116(50):24972–24978, 2019.\n[23] Le Cleac’h, Mac Schwager, and Zachary Manchester.\nALGAMES: A fast augmented lagrangian solver for\nconstrained dynamic games. Autonomous Robots, 46(1):\n201–215, 2022.\n[24] Edward L Zhu and Francesco Borrelli.\nA sequential\nquadratic programming approach to the solution of open-\nloop generalized nash equilibria. In 2023 IEEE Interna-\ntional Conference on Robotics and Automation (ICRA),\npages 3211–3217. IEEE, 2023.\n[25] Francisco\nFacchinei\nand\nJong-Shi\nPang.\nFinite-\nDimensional Variational Inequalities and Complementar-\nity Problems. Springer Verlag, 2003.\n[26] Stephen C Billups, Steven P Dirkse, and Michael C\nFerris. A comparison of large scale mixed complemen-\ntarity problem solvers. Computational Optimization and\nApplications, 7(1):3–25, 1997.\n[27] Steven P Dirkse and Michael C Ferris.\nThe PATH\nsolver: A nommonotone stabilization scheme for mixed\ncomplementarity problems.\nOptimization methods and\nsoftware, 5(2):123–156, 1995.\n[28] Philipp Geiger and Christoph-Nikolas Straehle. Learning\ngame-theoretic models of multiagent trajectories using\nimplicit layers. In Proc. of the Conference on Advance-\nments of Artificial Intelligence (AAAI), volume 35, 2021.\n[29] Kevin P. Murphy. Probabilistic Machine Learning: An\nintroduction. MIT Press, 2022. URL probml.ai.\n[30] Christopher Diehl, Tobias Klosek, Martin Krueger, Nils\nMurzyn, Timo Osterburg, and Torsten Bertram. Energy-\nbased potential games for joint motion forecasting and\ncontrol. In Conference on Robot Learning, pages 3112–\n3141. PMLR, 2023.\n[31] Diederik P Kingma and Max Welling.\nAuto-encoding\nvariational bayes. arXiv preprint arXiv:1312.6114, 2013.\n[32] Stefan Clarke, Gabriele Dragotto, Jaime Fern´andez Fisac,\nand Bartolomeo Stellato. Learning rationality in potential\ngames. arXiv preprint arXiv:2303.11188, 2023.\n[33] Kevin P Murphy. Probabilistic machine learning: Ad-\nvanced topics. MIT press, 2023.\n[34] Ishaan\nGulrajani,\nKundan\nKumar,\nFaruk\nAhmed,\nAdrien Ali Taiga, Francesco Visin, David Vazquez, and\nAaron Courville. Pixelvae: A latent variable model for\nnatural images. In International Conference on Learning\nRepresentations, 2016.\n[35] DP Kingma. Adam: a method for stochastic optimization.\nIn Int Conf Learn Represent, 2014.\n"
}