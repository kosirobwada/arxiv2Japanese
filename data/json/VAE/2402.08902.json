{
    "optim": "Auto-Encoding Bayesian Inverse Games Xinjie Liu†∗ Lasse Peters‡∗ Javier Alonso-Mora‡ Ufuk Topcu† David Fridovich-Keil† †The University of Texas at Austin ‡Delft University of Technology { xinjie-liu, utopcu, dfk }@utexas.edu, { l.peters, j.alonsomora }@tudelft.nl Abstract—When multiple agents interact in a common environ- ment, each agent’s actions impact others’ future decisions, and noncooperative dynamic games naturally capture this coupling. In interactive motion planning, however, agents typically do not have access to a complete model of the game, e.g., due to unknown objectives of other players. Therefore, we consider the inverse game problem, in which some properties of the game are unknown a priori and must be inferred from observations. Existing maximum likelihood estimation (MLE) approaches to solve inverse games provide only point estimates of unknown parameters without quantifying uncertainty, and perform poorly when many parameter values explain the observed behavior. To address these limitations, we take a Bayesian perspective and construct posterior distributions of game parameters. To render inference tractable, we employ a variational autoencoder (VAE) with an embedded differentiable game solver. This structured VAE can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, multi-modal distri- butions, and supports efficient sampling from the inferred pos- teriors without computing game solutions at runtime. Extensive evaluations in simulated driving scenarios demonstrate that the proposed approach successfully learns the prior and posterior game parameter distributions, provides more accurate objective estimates than MLE baselines, and facilitates safer and more efficient game-theoretic motion planning. 1 I. INTRODUCTION Autonomous robots often need to interact with other agents to operate seamlessly in real-world environments. For exam- ple, in the scenario depicted in Fig. 1, a robot encounters a human driver while navigating an intersection. In such settings, coupling effects between agents significantly com- plicate decision-making: if the human acts assertively and drives straight toward its goal, the robot will be forced to brake to avoid a collision. Hence, the agents compete to optimize their individual objectives. Dynamic noncooperative game theory [1] provides an expressive framework to model such interaction among rational, self-interested agents. In many scenarios, however, a robot must handle interac- tions under incomplete information, e.g., without knowing the goal position of the human driver in Fig. 1. To this end, a recent line of work [2–11] proposes to solve inverse dynamic games to infer the unknown parameters of a game—such as other agents’ objectives—from observed interactions. A common approach to solve inverse dynamic games uses maximum likelihood estimation (MLE) to find the most likely parameters given observed behavior [3–11]. However, MLE solutions provide only a point estimate without any uncertainty quantification and can perform poorly in scenarios where ∗Equal contribution 1https://xinjie-liu.github.io/projects/bayesian-inverse-games posterior samples predictions made by the robot human observations true human plan unknown true human goal robot goal robot Fig. 1: A robot interacting with a human driver whose goal position is unknown. We embed a differentiable game solver in a structured variational autoencoder to infer the distribution of the human’s objectives based on observations of their behavior. many parameter values explain the observations [10], yielding overconfident, unsafe motion plans [12]. Bayesian formulations of inverse games mitigate these lim- itations of MLE methods by inferring a posterior distribution, i.e., belief, over the unknown parameters [13, 14]. Knowledge of this distribution allows the robot to account for uncertainty and generate safer yet efficient plans [12, 14–16]. Unfortunately, exact Bayesian inference is typically in- tractable in dynamic games, especially when dynamics are nonlinear. Prior work [14] alleviates this challenge by using an unscented Kalman filter (UKF) for approximate Bayesian inference. However, that approach is limited to unimodal uncertainty models and demands solving multiple games per belief update, thereby posing a computational challenge. The main contribution of this work is a framework for tractable Bayesian inference of posterior distributions over un- known parameters in dynamic games. To this end, we approx- imate exact Bayesian inference with a structured variational autoencoder (VAE). During training, this VAE embeds a differ- entiable game solver to facilitate unsupervised learning from an unlabeled dataset of observed interactions. At runtime, the proposed approach can generate samples from the predicted posterior without solving additional games. As a result, our approach naturally captures continuous, multi-modal beliefs, and addresses the limitations of MLE inverse games without resorting to the simplifications or computational complexity of existing Bayesian methods. Through extensive evaluations of our method in two simu- arXiv:2402.08902v2  [cs.RO]  16 Feb 2024 lated driving scenarios, we support the following key claims. The proposed framework (i) learns the underlying prior game parameter distribution from unlabeled interactions, and (ii) captures potential multi-modality of game parameters. Our ap- proach (iii) is uncertainty-aware, predicting narrow unimodal beliefs when observations clearly reveal the intentions of other agents, and beliefs that are closer to the learned prior in case of uninformative observations. The proposed framework (iv) provides more accurate inference performance than MLE inverse games by effectively leveraging the learned prior information, especially in settings where multiple parameter values explain the observed behavior. As a result, our approach (v) enables safer downstream robot plans than MLE methods. II. RELATED WORK This section provides an overview of the literature on dynamic game theory, focusing on both forward games (Sec- tion II-A) and inverse games (Section II-B). A. Forward Dynamic Games This work focuses on noncooperative games where agents have partially conflicting but not completely adversarial goals and make sequential decisions over time [1]. Since we assume that agents take actions simultaneously without leader-follower hierarchy and we consider coupling between agents’ decisions through both objectives and constraints, our focus is on generalized Nash equilibrium problems (GNEPs). GNEPs are challenging coupled mathematical optimization problems. Nonetheless, recent years have witnessed an ad- vancement of efficient computation methods for GNEPs with smooth objectives and constraints. Due to the computational challenges involved in solving such problems under feedback information structure [17], most works aim to find open-loop Nash equilibria (OLNE) [1] instead, where players choose their action sequence—an open-loop strategy—at once. A substantial body of work employs the iterated best response algorithm to find a GNEP by iteratively solving single- agent optimization problems [18, 19, 19–22]. More recently, methods based on sequential quadratic approximations have been proposed [23, 24], aiming to speed up convergence by updating all players’ strategies simultaneously at each iteration. Finally, since the first-order necessary conditions of open-loop GNEPs take the form of a mixed complementarity problem (MCP) [25], several works [8, 12] solve generalized Nash equilibria (GNE) using established MCP solvers [26, 27]. This work builds on the latter approach. B. Inverse Dynamic Games Inverse games study the problem of inferring unknown game parameters, e.g. of objective functions, from observations of agents’ behavior [2]. In recent years, several approaches have extended single-agent inverse optimal control (IOC) and inverse reinforcement learning (IRL) techniques to multi-agent interactive settings. For instance, the approaches of [5, 6] min- imize the residual of agents’ first-order necessary conditions, given full state-control observations, in order to infer unknown objective parameters. Inga et al. [4] extends this approach to maximum-entropy settings. Recent work [9] proposes to maximize observation likeli- hood while enforcing the Karush–Kuhn–Tucker (KKT) con- ditions of OLNE as constraints. This approach only requires partial-state observations and can cope with noise-corrupted data. Approaches [8, 10] propose an extension of the MLE approach [9] to inverse feedback and open-loop games with inequality constraints via differentiable forward game solvers. To amortize the computation of MLE, works [8, 28] demon- strate integration with neural network (NN) components. In general, MLE solutions can be understood as point estimates of Bayesian posteriors, assuming a uniform prior [29, Ch.4]. When multiple parameter values explain the observa- tions equally well, this simplifying assumption can result in ill-posed problems—causing MLE inverse games to recover potentially inaccurate estimates [10]. Moreover, in the con- text of motion planning, the use of point estimates without awareness of uncertainty can result in unsafe plans [12, 16]. To address these issues, several recent works take a Bayesian view on inverse games [13, 14], aiming to infer a posterior distribution while factoring in prior knowledge. Since exact Bayesian inference is intractable in these problems, the belief update may be approximated via a particle filter [13]. However, this approach requires solving a large number of equilibrium problems online to maintain the belief distribution, posing a significant computational burden. A sigma-point ap- proximation [14] reduces the number of required samples but limits the estimator to unimodal uncertainty models. Recently, Diehl et al. [30] trained a NN that embeds differentiable optimization to infer a categorical distribution over intents. That approach, however, fixes the number of modes a priori, requires training data labeled with ground-truth intents, and is limited to the class of potential games. To overcome the limitations of MLE approaches while avoiding the intractability of exact Bayesian inference, we propose to approximate the posterior via a VAE [31] that embeds a differentiable game solver [8] during training. The proposed approach can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, multi- modal distributions, and does not require computation of game solutions at runtime to sample from the posterior. III. PRELIMINARIES: GENERALIZED NASH GAME In this work, we consider strategic interactions of self- interested rational agents in the framework of generalized Nash equilibrium problems (GNEPs). In this framework, each of N agents seeks to unilaterally minimize their respective cost while being conscious of the fact that their “opponents”, too, act in their own best interest. We define a parametric N-player GNEP as N coupled, constrained optimization problems, Si θ(τ ¬i) := arg min τi Ji θ(τ i, τ ¬i) (1a) s.t. gi θ(τ i, τ ¬i) ≥ 0, (1b) where θ ∈ Rp denotes a parameter vector whose role will become clear below, and player i ∈ [N] := {1, . . . , N} has cost function Ji θ and private constraints gi θ. Furthermore, observe that the costs and constraints of player i depend not only on their own strategy τ i ∈ Rmi, but also on the strategy of all other players, τ ¬i ∈ R P j∈[N]\\{i} mj. Generalized Nash Equilibria. For a given parameter θ, the solution of a GNEP is an equilibrium strategy profile τ ∗ := (τ 1∗, . . . , τ N∗) so that each agent’s strategy is a best response to the others’, i.e., τ i∗ ∈ Si θ(τ ¬i∗), ∀i ∈ [N]. (2) Intuitively, at a GNE, no player can further reduce their cost by unilaterally adopting another feasible strategy. Example: Online Game-Theoretic Motion Planning. This work focuses on applying games to online motion planning in interaction with other agents, such as the intersection scenario shown in Fig. 1. In this context, the strategy of agent i, τi, represents a trajectory—i.e., a sequence of states and inputs extended over a finite horizon—which is recomputed in a receding-horizon fashion. This paradigm results in the game-theoretic equivalent of model-predictive control (MPC): model-predictive game-play (MPGP). The construction a tra- jectory game largely follows the procedure used in single- agent trajectory optimization: gi θ(·) encodes input and state constraints including those enforcing dynamic feasibility, col- lision avoidance, and road geometry; and Ji θ(·) encodes the ith agent’s objective such as reference tracking. Adopting a convention in which index 1 refers to the ego agent, the equilibrium solution of the game then serves two purposes si- multaneously: the opponents’ solution, τ ¬1∗, serves as a game- theoretic prediction of their behavior while the ego agent’s solution, τ 1∗, provides the corresponding best response. The Role of Game Parameters θ. A key difference between trajectory games and single-agent trajectory optimization is the requirement to provide the costs and constraints of all agents. In practice, a robot may have insufficient knowledge of their opponents’ intents, dynamics, or state to instantiate a complete game-theoretic model. This aspect motivates the parameterized formulation of the game above: for the remainder of this manuscript, θ will capture the unknown aspects of the game. In the context of game-theoretic motion planning, e.g. to model agents navigating an intersection, θ typically includes aspects of opponents’ preferences such as their unknown desired lane or preferred velocity. For conciseness, we denote game (1) compactly via the parametric tuple of problem data Γ(θ) := ({Ji θ, gi θ}i∈[N]). Next, we discuss how to infer these parameters online from observed interactions. IV. FORMALIZING BAYESIAN INVERSE GAMES This section presents our main contribution: a framework for inferring unknown game parameters θ based on observations y. This problem is commonly referred to as an inverse game [2]. Several prior works on inverse games [9, 10, 32] seek to find game parameters that directly maximize observation likelihood. However, this MLE formulation of inverse games unaugmented model game solution observation latent posterior surrogate diﬀerentiable game solver game parameters encoder NN decoder NN auxiliary variable augmentation Fig. 2: Overview of a structured VAE for generative Bayesian inverse games. Top (left to right): decoder pipeline. Bottom (right to left): variational inference process via an encoder. (i) only provides a point estimate of the unknown game pa- rameters θ, thereby precluding the consideration of uncertainty in downstream tasks such as motion planning; and (ii) fails to provide reasonable parameter estimates when observations are uninformative, as we shall also demonstrate in Section V. A. A Bayesian View on Inverse Games In order to address the limitations of the MLE inverse games, we consider a Bayesian formulation of inverse games, and seek to construct the belief distribution b(θ) = p(θ | y) = p(y | θ)p(θ) p(y) . (3) In contrast to MLE inverse games, this formulation provides a full posterior distribution over the unknown game parameters θ and factors in prior knowledge p(θ). Observation Model p(y | θ). In autonomous online oper- ation of a robot, y represents the (partial) observations of other players’ recent trajectories—e.g., from a fixed lag buffer as shown in orange in Fig. 1. Like prior works on inverse games [9, 10, 14], we assume that, given the unobserved true trajectory of the human, y is Gaussian-distributed; i.e., p(y | τ) = N(y | µy(τ), Σy(τ)). Assuming that the underly- ing trajectory is the solution of a game with known structure Γ but unknown parameters θ, we express the observation model as p(y | θ) = p(y | TΓ(θ)), where TΓ denotes a game solver that returns a solution τ ∗ of the game Γ(θ). Challenges of Bayesian Inverse Games. While the Bayesian formulation of inverse games in Eq. (3) is conceptually straightforward, it poses several challenges: (i) The prior p(θ) is typically unavailable and instead must be learned from data. (ii) The computation of the normalizing constant, p(y) = R p(y | θ)p(θ)dθ, is intractable in practice due to the marginalization of θ. (iii) Both the prior, p(θ), and posterior p(θ | y) are in general non-Gaussian or even multi-modal and are therefore dif- ficult to represent explicitly in terms of their probability density function (PDF). Prior work [14] partially mitigates these challenges by using an UKF for approximate Bayesian inference, but that approach is limited to unimodal uncertainty models and requires solving multiple games for a single belief update, thereby posing a computational challenge. Fortunately, as we shall demonstrate in Section V, many practical applications of inverse games do not require an explicit evaluation of the belief PDF, b(θ). Instead, a gen- erative model of the belief—i.e., one that allows drawing samples θ ∼ b(θ)—often suffices. Throughout this section, we demonstrate how to learn such a generative model from an unlabeled dataset D = {yk | yk ∼ p(y), ∀k ∈ [K]} of independent and identically distributed observed interactions. B. Augmentation to Yield a Generative Model To obtain a generative model of the belief b(θ), we aug- ment our Bayesian model as summarized in the top half of Fig. 2. The goal of this augmentation is to obtain a model structure that lends itself to approximate inference in the framework of variational autoencoders (VAEs) [31]. Specifically, we introduce an auxiliary random variable z with known standard-normal prior p(z) = N(z), and a deter- ministic decoder dϕ with unknown parameters ϕ. Following the generative process outlined in Fig. 2 from left to right, the decoder maps z to the game parameters θ and hence we have pϕ(θ | z) = δ \u0000dϕ(z) − θ \u0001 , where δ denotes the Dirac delta function. Next, the game solver TΓ produces the GNE solution τ ∗ which determines the observation likelihood p(y | τ). In terms of probability distributions, dϕ induces the conditional pϕ(y | z) = R p(y | θ)pϕ(θ | z)dθ, the prior pϕ(θ) = R pϕ(θ | z)p(z)dz, and the data distribution pϕ(y) = R p(y | θ)pϕ(θ)dθ. C. Auto-Encoding Bayesian Inverse Games In the augmented model of Section IV-B, knowledge of ϕ and the latent posterior pϕ(z | y) = pϕ(y | z)p(z)/pϕ(y) suffices to specify generative models for the prior pϕ(θ) and posterior pϕ(θ | y). That is, by propagating samples from the latent prior p(z) through dϕ, we implicitly generate samples from the prior pϕ(θ). Similarly, by propagating samples from the latent posterior pϕ(z | y) through dϕ, we implicitly generate samples from the posterior pϕ(θ | y). We thus have converted the problem of (generative) Bayesian inverse games to estimation of ϕ and inference of pϕ(z | y). Recognizing the similarity of the generative process from z to y outlined in Fig. 2 to the decoding process in a VAE, we seek to approximate the latent posterior pϕ(z | y) with a Gaussian qψ(z | y) = N(z | eψ(y)). Here, in the language of VAEs, eψ takes the role of an encoder that maps observation y to the mean µz(y) and covariance matrix Σz(y) of the Gaussian posterior approximation; cf. bottom of Fig. 2. The key difference between our pipeline and a conventional VAE is the special structure of the generative process from latent z to observation y. Within this process, the decoder network dϕ is composed with the game solver TΓ to produce an equilibrium strategy that parameterizes the observation model. Building on the approach of Liu et al. [8], we can recover the gradient of the game solver TΓ with respect to the game parameters θ. It is this gradient information from the game- theoretic “layer” in the decoding pipeline that induces an interpretable structure on the output of the decoder-NN dϕ, forcing it to predict the hidden game parameters θ. Remark. To generate samples from the estimated posterior qψ,ϕ(θ | y) = R pϕ(θ | z)qψ(z | y)dz, we do not need to evaluate the game solver TΓ: posterior sampling involves only the evaluation of NNs dϕ and eψ; cf. y → θ in Fig. 2. Next, we discuss the training of the decoder and encoder networks dϕ and eψ of our structured VAE. D. Training the Structured Variational Autoencoder Below, we outline the process for optimizing model param- eters in our specific setting. For a general discussion of VAEs and variational inference (VI), refer to Murphy [33]. Fitting a Prior to D. First, we discuss the identification of the prior parameters ϕ. Specifically, we seek to choose these parameters so that the data distribution induced by ϕ, i.e., pϕ(y) = R pϕ(y | z)p(z)dθ, closely matches the unknown true data distribution p(y). For this purpose, we measure closeness between distributions using the Kullback–Leibler (KL) divergence DKL (p ∥ q) := Ex∼p(x) [log p(x) − log q(x)] . (4) The key properties of this divergence metric, DKL (p ∥ q) ≥ 0 and DKL (p ∥ q) = 0 ⇐⇒ p = q, allow us to cast the estimation of ϕ as an optimization problem: ϕ∗ ∈ arg min ϕ DKL (p(y) ∥ pϕ(y)) (5a) = arg min ϕ − Ey∼p(y) \u0002 log Ez∼p(z) [pϕ(y | z)] \u0003 . (5b) With the prior recovered, we turn to the approximation of the posterior pϕ(θ | y). Variational Belief Inference. We can find the closest sur- rogate qψ(z | y) of pϕ(z | y) by minimizing the expected KL divergence between the two distributions over the data distribution y ∼ p(y) using the framework of VI: ψ∗ ∈ arg min ψ Ey∼p(y) [DKL (qψ(z | y) ∥ pϕ(z | y))] (6a) = arg min ψ E y∼p(y) z∼qψ(z|y) [ℓ(ψ, ϕ, y, z)] , (6b) where ℓ(ψ, ϕ, y, z) := log qψ(z | y) | {z } N (z|eψ(y)) − log pϕ(y | z) | {z } N (y|(TΓ◦dϕ)(z)) − log p(z) |{z} N (z) . We have therefore outlined, in theory, the methodology to determine all parameters of the pipeline from the unlabeled dataset D. Considerations for Practical Realization. To solve Eqs. (5) and (6) in practice, we must overcome two main challenges: first, the loss landscapes are highly nonlinear due to the nonlinear transformations eψ, dϕ and TΓ; and second, the expected values cannot be computed in closed form. These challenges can be addressed by taking a stochastic first-order optimization approach, such as stochastic gradient descent (SGD), thereby limiting the search to a local optimum and approximating the gradients of the expected values via Monte Carlo sampling. To facilitate SGD, we seek to construct unbiased gradient estimators of the objectives of Eq. (5b) and Eq. (6b) from gradients computed at individual samples. An unbiased gradient estimator is straightforward to define when the following conditions hold: (C1) any expectations appear on the outside, and (C2) the sampling distribution of the expectations are independent of the variable of differentiation. The objective of the optimization problem in Eq. (6b) takes the form L(q, ϕ) := E y∼p(y) z∼q(z|y) [ℓ(ψ, ϕ, y, z)] , (7) which clearly satisfies C1. Similarly, it is easy to verify that the objective of Eq. (5b) can be identified as L(pϕ(z | y), ·). Unfortunately, this latter insight is not immediately actionable since pϕ(z | y) is not readily available. However, if instead we use our surrogate model from Eq. (6)—which, by construction closely matches pϕ(z | y)—we can cast the estimation of the prior and posterior model as a joint optimization of L: ˜ψ∗, ˜ϕ∗ ∈ arg min ψ,ϕ L(qψ, ϕ). (8) When eψ and dϕ are sufficiently expressive to allow DKL (qψ(z | y) ∥ pϕ(z | y)) = 0, then this reformulation is exact; i.e., ˜ϕ∗ and ˜ψ∗ are also minimizers of the original problems Eq. (5) and Eq. (6), respectively. In practice, a perfect match of distributions is not typically achieved and ˜ϕ∗ and ˜ψ∗ are biased. Nonetheless, the scalability enabled by this reformulation has been demonstrated to enable generative modeling of complex distributions, including those of real- world images [34]. Finally, to also satisfy C2 for the objective of Eq. (8), we apply the well-established “reparameterization trick” to cast the inner expectation over a sampling distribution independent of ψ. That is, we write L(qψ, ϕ) = Ey∼p(y) ϵ∼N \u0002 ℓ(ψ, ϕ, y, rqψ(ϵ, y)) \u0003 , (9) where the inner expectation is taken over ϵ that has multi- variate standard normal distribution, and rqψ(·, y) defines a bijection from ϵ to z so that z ∼ qψ(· | y). Since we model the latent posterior as Gaussian—i.e., qψ(z | y) = N(z | eψ(y))— the reparameterization map is rqψ(ϵ, y) := µz(y) + Lz(y)ϵ, where LzL⊤ z = Σz is a Cholesky decomposition of the latent covariance. Observe that Eq. (9) only involves Gaussian PDFs, and all terms can be easily evaluated in closed form. Stochastic Optimization. As in SGD-based training of con- ventional VAEs, at iteration k, we sample yk ∼ D, and encode yk into the parameters of the latent distribution via eψk. From the latent distribution we then sample zk, and evaluate the unbiased gradient estimators ∇ψℓ(ψ, ϕk, yk, zk)|ψ=ψk and ∇ϕℓ(ψk, ϕ, yk, zk)|ϕ=ϕk of L at the current parameter iterates, ψk and ϕk. Due to our model’s special structure, the evalu- ation of these gradient estimators thereby also involves the differentiation of the game solver TΓ. V. EXPERIMENTS To assess the proposed approach, we evaluate its online inference capabilities and its efficacy in downstream motion planning tasks in two simulated driving scenarios. A. Two-Player Intersection Game First, we evaluate the proposed framework for downstream motion planning tasks in the intersection scenario depicted in Fig. 1. This experiment is designed to validate hypotheses: • H1 (Inference Accuracy). Our method provides more accurate inference performance than MLE by leveraging the learned prior information, especially in settings where multiple human objectives explain the observations. • H2 (Multi-modality). Our approach predicts posterior distributions that capture the multi-modality of agents’ objectives and behavior. • H3 (Planning Safety). Bayesian inverse game solutions enable safer robot plans over the MLE methods. 1) Experiment Setup: In the test scenario, shown in Fig. 3, the red robot must navigate an intersection, while interact- ing with the green human whose goal position is initially unknown. In each simulated interaction, the human’s goal is randomly sampled from a Gaussian mixture distribution with equal probabilities for two mixture components: one for turning left, and one for going straight. We model the agents’ dynamics as kinematic bicycles. The state at time step t includes position, longitudinal velocity, and orientation, i.e., xi t = (pi x,t, pi y,t, vi t, ξi t), and the control comprises acceleration and steering angle, i.e., ui t = (ai t, ηi t). We assign player index 1 to the robot and index 2 to the human. Over a planning horizon of T = 15 time steps, each player i seeks to minimize a cost function that encodes incentives for reaching the goal, reducing control effort, and avoiding collision: Ji θ = T −1 X t=1 ∥pi t+1 − pi goal∥2 2 + 0.1∥ui t∥2 2 + 400 max(0, dmin − ∥pi t+1 − p¬i t+1∥2)3, (10) where pi t = (pi x,t, pi y,t) denotes agent i’s position at time step t, pi goal = (pi goal,x, pi goal,y) denotes their goal position, and dmin denotes a preferred minimum distance to other agents. The unknown parameter θ inferred by the robot contains the two- dimensional goal position p2 goal of the human. We train the structured VAE from Section IV-C on a dataset of observations from 560 closed-loop interaction episodes obtained by solving dynamic games with the opponent’s ground truth goals sampled from the Gaussian mixture de- scribed above. The 560 closed-loop interactions are sliced into 34600 15-step observations y. Partial-state observations consist of the agents’ positions and orientations. We employ fully-connected feedforward NNs with two 128-dimensional and 80-dimensional hidden layers as the encoder model eψ and decoder model dϕ, respectively. The latent variable z Fig. 3: Qualitative behavior of B-PinE (ours) vs. R-MLE. In the bottom row, the size of the green stars increases with time. Fig. 4: Negative observation log-likelihood − log p(y | p2 goal) for varying human goal positions p2 goal at two time steps of the R-MLE trial in Fig. 3. is 16-dimensional. We train the VAE with Adam [35] for 100 epochs, which took 14 hours of wall-clock time. 2) Baselines: We evaluate the following methods to control the robot interacting with a human of unknown intent: • Ground truth (GT): This planner generates robot plans by solving games with access to ground truth opponent objectives. We include this oracle baseline to provide a reference upper-bound on planner performance. • Bayesian inverse game (ours) + planning in expec- tation (B-PinE): This planner solves multi-hypothesis games [12] in which the robot minimizes the expected cost Eθ∼qψ,ϕ(θ|y) \u0002 J1 θ \u0003 under the posterior distributions predicted by our structured VAE based on new observa- tions at each time step. • Bayesian inverse game (ours) + maximum a posteriori (MAP) planning (B-MAP): Instead of using the full posteriors from our framework, this approach constructs MAP estimates ˆθMAP ∈ arg maxθ qψ,ϕ(θ | y) from the posteriors and solves the game Γ(ˆθMAP). • Randomly initialized MLE planning (R-MLE): This baseline [8] solves the game Γ(ˆθMLE), where ˆθMLE ∈ arg maxθ p(y | θ). The MLE problems are solved online via gradient descent based on new observations at each time step as in [8]. The initial guess for optimization is sampled uniformly from a rectangular region covering all potential ground truth goal positions. • Bayesian prior initialized MLE planning (BP-MLE): Instead of uniformly sampling heuristic initial guesses as in R-MLE, this baseline solves for the MLE with initial guesses from the Bayesian prior learned by our approach. • Static Bayesian prior planning (St-BP): This baseline samples ˆθ from the learned Bayesian prior and uses the sample as a fixed human objective estimate to solve Γ(ˆθ). This baseline is designed as an ablation study of the effect of online objective inference. For those planners that utilize our VAE for inference, we take 1000 samples at each time step to approximate the distribution of human objectives, which takes ≈7 ms. For B- PinE, we cluster the posterior samples into two groups to be compatible with the multi-hypothesis game solver from [12]. 3) Qualitative Behavior: Figure 3 illustrates the qualitative behavior of B-PinE and R-MLE. B-PinE. The top row of Fig. 3 shows that our approach initially generates a bimodal belief, capturing the distribution Fig. 5: Quantitative results of S1: (a) Minimum distance between agents in each trial. (b) Robot costs (with GT costs subtracted). Fig. 6: Quantitative results of S2 and S3: (a) Minimum distance between agents in each trial of S2. (b-c) Robot costs of S2-3 (with GT costs subtracted). of potential opponent goals. In the face of this uncertainty, the planner initially computes a more conservative trajectory to remain safe. As the human approaches the intersection and reveals its intent, the left-turning mode gains probability mass until the computed belief eventually collapses to a unimodal distribution. Throughout the interaction, the predictions gen- erated by the multi-hypothesis game accurately cover the true human plan, allowing safe and efficient interaction. R-MLE. As shown in the bottom row of Fig. 3, the R-MLE baseline initially estimates that the human will go straight. The true human goal only becomes clear later in the interaction and the over-confident plans derived from these poor point estimates eventually lead to a collision. Observability Issues of MLE. To illustrate the underlying issues that caused the poor performance of the R-MLE base- line discussed above, Fig. 4 shows the negative observation log-likelihood for two time steps of the interaction. As can be seen, a large region in the game parameter space explains the observed behavior well, and the baseline incorrectly concludes that the opponent’s goal is always in front of their current position; cf. top of Fig. 3. In contrast, by leveraging the learned prior distribution, our approach predicts objective samples that capture potential opponent goals well even when the observation is uninformative; cf. bottom of Fig. 3. In summary, these results validate hypotheses H1 and H2. 4) Quantitative Analysis: To quantify the performance of all six planners, we run a Monte Carlo study of 1500 trials. In each trial, we randomly sample the robot’s initial position along the lane from a uniform distribution such that the result- ing ground truth interaction covers a spectrum of behaviors, ranging from the robot entering the intersection first to the human entering the intersection first. We use the minimum distance between players in each trial to measure safety and the robot’s cost as a metric for interaction efficiency. We group the trials into three settings based on the ground- truth behavior of the agents: (S1) The human turns left and passes the intersection first. In this setting, a robot recognizing the human’s intent should yield, whereas blindly optimizing the goal-reaching objective will likely lead to unsafe inter- action. (S2) The human turns left, but the robot reaches the intersection first. In this setting, we expect the effect of inaccurate goal estimates to be less pronounced than in S1 since the robot passes the human on the right irrespective of their true intent. (S3) The human drives straight. Safety is trivially achieved in this setting. Results, S1. Figure 5(a) shows that methods using our frame- work achieve better planning safety than other baselines, closely matching the ground truth in this metric. Using the minimum distance between agents among all ground truth trials as a collision threshold, the collision rates of the methods are 0.0% (B-PinE), 0.78% (B-MAP), 17.05% (R- MLE), 16.28% (BP-MLE), and 17.83% (St-BP), respectively. Moreover, the improved safety of the planners using Bayesian inference does not come at the cost of reduced planning efficiency. As shown in Fig. 5(b-c), B-PinE and B-MAP consistently achieve low interaction costs. Results, S2. Figure 6(a) shows that all the approaches except for the St-BP baseline approximately achieve ground truth safety in this less challenging setting. While the gap between approaches is less pronounced, we still find improved planning safety for our methods over the baselines. Taking the same collision distance threshold as in S1, the collision rates of the methods are 0.86% (B-PinE), 2.24% (B-MAP), 7.59% (R-MLE), 6.03% (BP-MLE), and 7.59% (St-BP), respec- tively. B-MAP gives less efficient performance in this setting. Compared with B-PinE, B-MAP only uses point estimates of the unknown goals and commits to over-confident and more aggressive behavior. We observe that this aggressiveness results in coordination issues when the two agents enter the intersection nearly simultaneously, causing them to acceler- ate simultaneously and then brake together. Conversely, the uncertainty-aware B-PinE variant of our method does not experience coordination failures, highlighting the advantage of incorporating the inferred distribution during planning. Lastly, St-BP exhibits the poorest performance in S2, stressing the importance of online objective inference. Results, S3. In Fig. 6(d), B-MAP achieves the highest effi- ciency, while B-PinE produces several trials with increased costs due to more conservative planning. Again, the St-BP baseline exhibits the poorest performance. Summary. These quantitative results show that our Bayesian inverse game approach improves motion planning safety over the MLE baselines, validating hypothesis H3. Between B- PinE and B-MAP, we observe improved safety in S1-2 and efficiency in S2. B. Two-Player Highway Game This experiment is designed to provide more detailed qualitative results on distributions inferred by the proposed approach and validate the following hypotheses: • H4 (Bayesian Prior). The proposed approach learns the underlying multi-modal prior objective distribution. • H5 (Uncertainty Awareness). The proposed approach computes a narrow, unimodal belief when the unknown objective is clearly observable and computes a wide, multi-modal belief that is closer to the prior in case of uninformative observations. We consider a two-player highway driving scenario, in which an ego robot drives in front of a human opponent and is tasked to infer the human’s desired driving speed. The rear vehicle is responsible for decelerating and avoiding collisions, and the front vehicle always drives at their desired speed. To simplify the visualization of beliefs and thereby illustrate H4- 5, we reduce the setting to a single spatial dimension and model one-dimensional uncertainty: we limit agents to drive in a single lane and model the agents’ dynamics as double integrators with longitudinal position and velocity as states and acceleration as controls; furthermore, we employ a VAE with the same hidden layers as in Section V-A but only one- dimensional latent space. The VAE takes a 15-step observation of the two players’ velocities as input for inference. We collect a dataset of 20000 observations to train a VAE. In each trial, the ego agent’s reference velocity is sampled from a uniform distribution from 0 m s−1 to the maximum velocity of 20 m s−1; the opponent’s desired velocity is sampled from a bi- modal Gaussian mixture distribution shown in grey in Fig. 7(a) with two unit-variance mixture components at means of 30% and 70% of the maximum velocity. (a) Prior distributions. (b) Posterior distributions. Fig. 7: (a) Learned and ground truth priors for the opponent’s objective. (b) Inferred objective posterior distributions. Figure 7(a) shows that the learned prior objective distribu- tion captures the underlying training set distribution closely and thereby validates our hypothesis H4. Figure 7(b) shows beliefs inferred by our approach for selected trials. The proposed Bayesian inverse game approach recovers a narrow distribution with low uncertainty when the opponent’s desired speed is clearly observable, e.g., when the front vehicle drives faster so that the rear vehicle can drive at their desired speed (Fig. 7, subfigures 1, 3). The rear driver’s objective becomes unobservable when they wish to drive fast but are blocked by the car in front. In this case, our approach infers a wide, multi-modal distribution with high uncertainty that is closer to the prior distribution (Fig. 7, subfigures 2, 4). These results validate hypothesis H5. VI. CONCLUSION & FUTURE WORK We presented an approach for Bayesian inference in dy- namic games, which can tractably infer posterior distribu- tions over unknown game parameters. The core computational enabler of this technique is a VAE that approximates the true posterior by embedding a differentiable game solver [8] during training. This structured VAE can be trained from an unlabeled dataset of observed interactions, naturally handles continuous, multi-modal distributions, and supports efficient sampling from the inferred posteriors without solving games at runtime. In two simulated driving scenarios, we thoroughly assessed our method’s capability for online inference and its efficacy in motion planning tasks. The findings show that our approach successfully learns prior game parameter distributions and utilizes this knowledge for inference of multi-modal posterior distributions. Compared to MLE inverse game approaches, our method provides more accurate game parameter estimates, facilitating safer and more efficient game-theoretic motion planning. This work explored the combination of the proposed in- ference pipeline with two game-theoretic planners, both of which showed promising results compared to non-Bayesian baselines. Future work should investigate the combination with other sophisticated planning frameworks, including applica- tions of active information gathering and signaling of ego agent intents to other agents in the environment. REFERENCES [1] Tamer Bas¸ar and Geert Jan Olsder. Dynamic Noncoop- erative Game Theory. Society for Industrial and Applied Mathematics (SIAM), 2. edition, 1999. [2] Kevin Waugh, Brian D Ziebart, and J Andrew Bagnell. Computational rationalization: The inverse equilibrium problem. arXiv preprint arXiv:1308.3506, 2013. [3] Florian K¨opf, Jairo Inga, Simon Rothfuß, Michael Flad, and S¨oren Hohmann. Inverse reinforcement learning for identification in linear-quadratic dynamic games. IFAC- PapersOnLine, 50(1):14902–14908, 2017. [4] Jairo Inga, Esther Bischoff, Florian K¨opf, and S¨oren Hohmann. Inverse dynamic games based on maximum entropy inverse reinforcement learning. arXiv preprint arXiv:1911.07503, 2019. [5] Chaitanya Awasthi and Andrew Lamperski. Inverse differential games with mixed inequality constraints. In Proc. of the IEEE American Control Conference (ACC), 2020. [6] Simon Rothfuß, Jairo Inga, Florian K¨opf, Michael Flad, and S¨oren Hohmann. Inverse optimal control for iden- tification in non-cooperative differential games. IFAC- PapersOnLine, 50(1):14909–14915, 2017. ISSN 2405- 8963. doi: https://doi.org/10.1016/j.ifacol.2017.08.2538. [7] Tianyu Qiu and David Fridovich-Keil. Identifying oc- cluded agents in dynamic games with noise-corrupted observations. arXiv preprint arXiv:2303.09744, 2023. [8] Xinjie Liu, Lasse Peters, and Javier Alonso-Mora. Learn- ing to play trajectory games against opponents with unknown objectives. IEEE Robotics and Automation Letters, 8(7):4139–4146, 2023. doi: 10.1109/LRA.2023. 3280809. [9] Lasse Peters, Vicenc Rubies-Royo, Claire J. Tomlin, Laura Ferranti, Javier Alonso-Mora, Cyrill Stachniss, and David Fridovich-Keil. Online and offline learning of player objectives from partial observations in dy- namic games. In Intl. Journal of Robotics Research (IJRR), 2023. URL https://journals.sagepub.com/doi/pdf/ 10.1177/02783649231182453. [10] Jingqi Li, Chih-Yuan Chiu, Lasse Peters, Somayeh So- joudi, Claire Tomlin, and David Fridovich-Keil. Cost inference for feedback dynamic games from noisy partial state observations and incomplete trajectories. arXiv preprint arXiv:2301.01398, 2023. [11] Negar Mehr, Mingyu Wang, Maulik Bhatt, and Mac Schwager. Maximum-entropy multi-agent dynamic games: Forward and inverse solutions. IEEE Trans. on Robotics (TRO), 39(3):1801–1815, 2023. doi: 10.1109/ TRO.2022.3232300. [12] Lasse Peters, Andrea Bajcsy, Chih-Yuan Chiu, David Fridovich-Keil, Forrest Laine, Laura Ferranti, and Javier Alonso-Mora. Contingency games for multi-agent inter- action. IEEE Robotics and Automation Letters (RA-L), 2024. [13] Lasse Peters. Accommodating intention uncertainty in general-sum games for human-robot interaction. Master’s thesis, Hamburg University of Technology, 2020. [14] Simon Le Cleac’h, Mac Schwager, and Zachary Manch- ester. LUCIDGames: Online unscented inverse dynamic games for adaptive trajectory prediction and planning. IEEE Robotics and Automation Letters (RA-L), 6(3): 5485–5492, 2021. [15] Wilko Schwarting, Alyssa Pierson, Sertac Karaman, and Daniela Rus. Stochastic dynamic games in belief space. IEEE Transactions on Robotics, 37(6):2157–2172, 2021. [16] Haimin Hu and Jaime F Fisac. Active uncertainty reduction for human-robot interaction: An implicit dual control approach. In Intl. Workshop on the Algorith- mic Foundations of Robotics (WAFR), pages 385–401. Springer, 2022. [17] Forrest Laine, David Fridovich-Keil, Chih-Yuan Chiu, and Claire Tomlin. The computation of approximate generalized feedback Nash equilibria. arXiv preprint arXiv:2101.02900, 2021. [18] Grady Williams, Brian Goldfain, Paul Drews, James M Rehg, and Evangelos A Theodorou. Best response model predictive control for agile interactions between autonomous ground vehicles. In 2018 IEEE International Conference on Robotics and Automation (ICRA), 2018. [19] Riccardo Spica, Eric Cristofalo, Zijian Wang, Eduardo Montijano, and Mac Schwager. A real-time game the- oretic planner for autonomous two-player drone racing. IEEE Trans. on Robotics (TRO), 36(5):1389–1403, 2020. [20] Zijian Wang, Riccardo Spica, and Mac Schwager. Game theoretic motion planning for multi-robot racing. In Distributed Autonomous Robotic Systems: The 14th In- ternational Symposium, pages 225–238. Springer, 2019. [21] Mingyu Wang, Zijian Wang, John Talbot, J. Christian Gerdes, and Mac Schwager. Game-theoretic planning for self-driving cars in multivehicle competitive scenarios. IEEE Transactions on Robotics, 37(4):1313–1325, 2021. doi: 10.1109/TRO.2020.3047521. [22] Wilko Schwarting, Alyssa Pierson, Javier Alonso-Mora, Sertac Karaman, and Daniela Rus. Social behavior for autonomous vehicles. Proceedings of the National Academy of Sciences, 116(50):24972–24978, 2019. [23] Le Cleac’h, Mac Schwager, and Zachary Manchester. ALGAMES: A fast augmented lagrangian solver for constrained dynamic games. Autonomous Robots, 46(1): 201–215, 2022. [24] Edward L Zhu and Francesco Borrelli. A sequential quadratic programming approach to the solution of open- loop generalized nash equilibria. In 2023 IEEE Interna- tional Conference on Robotics and Automation (ICRA), pages 3211–3217. IEEE, 2023. [25] Francisco Facchinei and Jong-Shi Pang. Finite- Dimensional Variational Inequalities and Complementar- ity Problems. Springer Verlag, 2003. [26] Stephen C Billups, Steven P Dirkse, and Michael C Ferris. A comparison of large scale mixed complemen- tarity problem solvers. Computational Optimization and Applications, 7(1):3–25, 1997. [27] Steven P Dirkse and Michael C Ferris. The PATH solver: A nommonotone stabilization scheme for mixed complementarity problems. Optimization methods and software, 5(2):123–156, 1995. [28] Philipp Geiger and Christoph-Nikolas Straehle. Learning game-theoretic models of multiagent trajectories using implicit layers. In Proc. of the Conference on Advance- ments of Artificial Intelligence (AAAI), volume 35, 2021. [29] Kevin P. Murphy. Probabilistic Machine Learning: An introduction. MIT Press, 2022. URL probml.ai. [30] Christopher Diehl, Tobias Klosek, Martin Krueger, Nils Murzyn, Timo Osterburg, and Torsten Bertram. Energy- based potential games for joint motion forecasting and control. In Conference on Robot Learning, pages 3112– 3141. PMLR, 2023. [31] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [32] Stefan Clarke, Gabriele Dragotto, Jaime Fern´andez Fisac, and Bartolomeo Stellato. Learning rationality in potential games. arXiv preprint arXiv:2303.11188, 2023. [33] Kevin P Murphy. Probabilistic machine learning: Ad- vanced topics. MIT press, 2023. [34] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David Vazquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. In International Conference on Learning Representations, 2016. [35] DP Kingma. Adam: a method for stochastic optimization. In Int Conf Learn Represent, 2014. "
}