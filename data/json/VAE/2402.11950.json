{
    "optim": " \n \nA novel molecule generative model of VAE \ncombined with Transformer \n \n \nYasuhiro Yoshikai1, *      Tadahaya Mizuno2,*, † \n \nShumpei Nemoto1      Hiroyuki Kusuhara1 \n \n \n \n \n \n \n \n \n \n \n \nAbstract \n \n \nKeywords: molecule generation, Transformer, VAE \n \n1 Introduction \nMachine learning has advanced research in various fields of science, and drug discovery is one of \nthem. In low molecular drug discovery, it is said that there can be as many as 10\u0003\u0004 possible drug \ncandidate molecules1, while only ~10\u0006 of them are recognized and registered in current databases2–4. \nAccessing this large number of unexplored molecules is expected to be useful for discovering new \ndrug seeds. Therefore, deep learning has been actively used to generate novel molecules in recent \nyears. In particular, the variational autoencoder (VAE)5 architecture is one of the most popular \nmodels since its study by Gomez-Bombarelli et al.6. This architecture consists of an encoder and \ndecoder, where the encoder learns to convert the input data into a latent representation, and the \ndecoder learns to restore the original input from the latent representation. In the generation phase, the \nRecently, molecule generation using deep learning has been actively investigated in drug \ndiscovery. In this field, Transformer and VAE are widely used as powerful models, but they \nare rarely used in combination due to structural and performance mismatch of them. This \nstudy proposes a model that combines these two models through structural and parameter \noptimization in handling diverse molecules. The proposed model shows comparable \nperformance to existing models in generating molecules, and showed by far superior \nperformance in generating molecules with unseen structures. In addition, the proposed \nmodel successfully predicted molecular properties using the latent representation of VAE.  \nAblation studies suggested the advantage of VAE over other generative models like \nlanguage model in generating novel molecules, and that the molecules can be described by \n~32 dimensional variables, much smaller than existing descriptors and models. This study is \nexpected to provide a virtual chemical library containing a wide variety of compounds for \nvirtual screening and to enable efficient screening. \n1 Laboratory of Molecular Pharmacokinetics, Graduate School \nof Pharmaceutical Sciences, The University of Tokyo, 7-3-1 \nHongo, Bunkyo, Tokyo, Japan \n2 Laboratory of Molecular Pharmacokinetics, Graduate School \nof Pharmaceutical Sciences, The University of Tokyo, 7-3-1 \nHongo, Bunkyo, Tokyo, Japan, tadahaya@gmail.com \n†Author to whom correspondence should be addressed. \n*These authors contributed equally. \ndecoder generates new data from randomly generated latent representations. In molecule generation, \nit is difficult to use the raw compound structure as the input and the output of the model, and their \nstring representation of them are used in many cases. For example, Gomez-Bombarelli et al.6 \nrepresented the structure of a molecule as a simplified molecular input line entry system (SMILES), \nand processed them with an encoder and decoder of a recurrent neural network (RNN), a model used \nin the field of natural language processing (NLP), to learn and represent the distribution of molecules. \nElsewhere, Kusner et al.7 used representation of molecules as a sequence of SMILES generation \nrules.  \n In these models, RNN models in NLP are used to encode and decode molecules that are expressed as \nstrings. On the other hand, the Transformer model8 is now prevalent in NLP as a highly powerful model \napplicable to various tasks. While the original Transformer model has shown high performance in \nsentence translation, the Transformer models pretrained with large language corpora, such as BERT9 or \nGPT10, have shown high performance in various tasks such as sentence summarization or conversation \nresponse generation. Therefore, it is natural to expect that introduction of the Transformer into VAE as its \nencoder and decoder will lead to a higher generative performance. In practice, however, the Transformer \nmodel is rarely combined with VAE11 possibly for two reasons. The Transformer model receives and \noutputs a matrix of variable length called memories, whereas VAE architecture requires the encoder and \ndecoder to transform inputs into a distribution of fixed-dimensional latent variables and restore the input \nfrom it, and natural form of Transformer does not fit VAE. Additionally, since Transformer is a powerful \nmodel, using such a powerful model as the backbone of VAE is prone to posterior collapse12, where the \ndecoder ignores the latent variables when generating the output. When posterior collapse occurs, the \nencoder always outputs the prior distribution of \u0007 (\b(\u0007)), whereas the decoder always outputs the \ndistribution of input data \u000b (\b(\u000b)) regardless of the latent representation. In this case, the distribution \b(\u000b) \ngenerated by the decoder is what the observed data follows itself, and the model becomes the language \nmodel when it generates variable-length data one by one, such as RNN and Transformer.  \n In this paper, we propose methods to overcome these two challenges and utilize Transformer as the \nencoder and decoder of VAE to handle molecules. First, to provide Transformer with a latent variable \nlayer of constant dimension, multiple pooling processes are applied to memory, and in the decoder, the \nlatent representation is added to the embedding of each token during decoding, following previous \nresearch in NLP13. To prevent posterior collapse when handling diverse molecules, a regularization term to \nmaximize the variational lower bound of the mutual information between the VAE input \u000b and the latent \nrepresentation \u0007 was added to the loss function according to Alemi et al.14. The developed VAE model \nwith Transformer as its backbone showed competitive or better performance than existing models, \nespecially in generating novel molecules or scaffolds that do not exist in the learned training set. The \neffect of regularizing mutual information was evaluated, and it was shown that the model succeeded in \nbalancing the decoding and the regularization of the latent space. The latent variables of VAE also showed \ncompetitive performance in the prediction of various molecular properties. Furthermore, the attention \nintensity of the encoder was visualized to show the important structures for generating latent \nrepresentations. \n \n2 Related Work \n2.1 VAE-based molecule generation \nVAE is one of the most prevalent models used for generative tasks of deep learning, and has also been \napplied to de novo drug design. The VAE model is trained with an unlabeled dataset and assumes the \nprobability distribution of the observed data \u000b, supposing that the distribution of \u000b is determined by \nanother latent variable \u0007. The model consists of an encoder and decoder, and the encoder receives the \nobserved data \u000b and predicts the posterior distribution of \u0007, whereas the decoder restores the original data \n\u000b from latent variables \u0007 sampled from the predicted distribution. Most of the studies on molecular VAE \nrepresent molecules as strings called SMILES, while some studies on VAE for molecular generation deal \nwith graph representation of molecules as input and output 15,16. The founder of the SMILES-based \nstrategy was Gómez-Bombarelli et al. 6, who introduced VAE to de novo drug design, using SMILES as \nthe input and the output of VAE and RNN, which can process strings of variable length, as backbones of \nthe encoder and decoder. This strategy has been widely accepted and various applications have been \nstudied 17. For example, Lim et al.18 designed a conditional variational autoencoder of molecules, and \ngenerated molecules which satisfied conditions of drug likeliness. Chenthamarakshan et al.19 also \nachieved the controlled generation of molecules and designed drug candidates for COVID-19. Samanta et \nal.20 applied the latent representations of molecules in VAE to measure molecular similarities. Kusner et \nal.7 focused on grammatical constraint of SMILES to maintain the high validity of the generated \nmolecules and trained VAE with grammatical parse tree of SMILES. \n \n2.2 Molecule generation with Transformer \nTransformer has now become one of the most widespread architectures in deep learning. Although this \nmodel was originally designed for translation of sentences, it has recently been applied to many tasks \nrelated to natural language processing (NLP), and has achieved high performance9,10. These prominent \nresults have encouraged scientists to adopt Transformer to SMILES strings to generate molecules for de \nnovo drug design. However, most of these studies were based on language model rather than VAE. A \nlanguage model is trained to simply predict the probability distribution of the token at each position from \nthe previous sequence, regardless of the latent variables. Bagal et al.21 developed a language model of \nSMILES conditioned on molecular properties based on the Transformer decoder, and succeeded in \ngenerating molecules which satisfy multiple molecular properties or scaffold conditions. Hong et al.22 \ndesigned a combined model of Transformer and feed-forward network and trained it as a language model. \nThe model not only showed competitive performance on molecular generation, but also generated some \ndrug candidates for COVID-19. Wang et al.23 distilled a Transformer-based language model into RNN and \ntrained the distilled model to generate molecules on multiple constraints using reinforcement learning. \nCofala et al.24 invented an autoregressive method to generate molecular graphs directly using the attention \nstructure of Transformer. \n \n2.3 Combined approach of VAE and Transformer \nDespite the popularity of VAE and Transformer in the field of de novo drug design, Transformer-based \nVAE have not been frequently studied11. One of the possible reasons is that while VAE requires a latent \nbottleneck layer of a fixed dimension, the decoder of Transformer cannot output a sentence from the small \nfixed-dimensional latent vector of VAE because it requires memory, the encoded information of the input \nof variable length from the encoder of the input. Kim et al.25 sampled multiple latent vectors from the \nposterior distribution estimated by the encoder and used them as the memory of Transformer. Dollar et \nal.11 designed a Transformer VAE model whose latent variables from the encoder were low-dimensional \nmemory of fixed length, and the model deconvoluted it to the model size at the decoding part. In the field \nof NLP, Fang et al.13 compared several methods to decode a sentence from a latent vector of fixed length \nusing Transformer, and demonstrated that adding the vector to the embedded input of the decoder showed \nthe best performance. Our model is based on this best architecture for chemical language models. \n \n3 Method \n3.1 Model architecture \nFigure 1 illustrates the structure of the model. The model was fed with randomized SMILES of molecules \nand trained to decode canonical SMILES of them. To make the latent representation contain more \nessential structural information, different representations of molecules (randomized and canonical \nSMILES) were used at the input and the output as in neural machine translation, inspired by Winter R. et \nal26. The embedded randomized SMILES was added with sinusoidal positional encoding and then inputted \ninto the Transformer Encoder. We followed the vanilla Transformer8 when setting hyperparameters \n(Supplementary Table 1) with some exceptions: there were 8 attention and feed-forward layers in the \nencoder, and the pre-LN structure27 was adopted, which means layer normalization was placed before the \nattention / feed-forward layer and residual connection. The mean and maximum of the memory (the output \nof the encoder) were pooled and concatenated with the memory corresponding to the initial token. The \nmean and variance of the posterior distribution of latent variables were estimated from the pooled memory. \nThe reparametrized latent variables were added to the embedded input of the decoder. The decoder has the \nsame structure and hyperparameters as the encoder. We used teacher forcing 28 during the training, and \ncalculated loss function as follows: \n \n\f = \u000e\u000f\u0010(\u0011|\u0013)\u0014\u0015\u0016\u0017 \u0018\u0019(\u0013|\u0011)\u001a + \u001c\u001d\u001e\u001f  \u000f\u0010(\u0011|\u0013)||\u0018\u0019(\u0011)! \nwhere \u0013 is the sampled SMILES, \u0011 is the latent variable, \u0019 and \u0010 are the parameters to model the \nencoder(\u000f) and decoder(\u0018).  \u001c is \" in usual VAE, but we set \u001c = #. #\" to prevent posterior collapse \nbased on optimization in handling molecules. Alemi et al.14 proved that reducing the coefficient of the \n\u001d\u001e\u001f is equal to adding normalization to maximize the variational lower bound of mutual information \nbetween the input and the latent variables to the loss function. \n \n3.2 Training of the Transformer VAE \nDataset \nWe used two different datasets to train the Transformer VAE: Molecular Sets (MOSES)29 and ZINC-152. \nMOSES dataset is prepared to evaluate the ability to generate drug-like molecules, and contains \napproximately 1.9M molecules extracted from ZINC clean leads datasets 2, and its molecules satisfy \nmultiple conditions as a drug candidate, such as having proper molecular weight or logP. MOSES have \n1.6M molecules in the training set, 176k molecules in the test set and 176k molecules in the testSF set. \nMolecules in the testSF set have scaffolds which do not appear in the training or test set and are used to \nmeasure the ability of the model to generate novel scaffolds. Canonical and randomized SMILES of \nmolecules in the training set were generated as the input and target of the model. Here, one molecule can \nbe translated into multiple SMILES, and canonical SMILES was chosen from them with a fixed protocol, \nwhereas randomized SMILES was randomly chosen from possible SMILES by renumbering atoms in \nmolecules30,31. These SMILES were tokenized with the vocabulary shown in Supplementary Table 2, \nwhich consists of element symbols and characters representing bonds. To iterate the evaluation frequently, \n10,000 molecules were randomly sampled from the MOSES test set, and used to evaluate the \nreconstruction ability of the model. \nZINC-15 is a larger dataset which contains about 1 billion commercially available molecules. This \ndataset was used to train the VAE to learn and generate broader range of molecules. We sampled \napproximately 30M molecules from ZINC-15 in a stratified manner with respect to the length of their \nSMILES32. Molecules with other than organic atoms (H, B, C, N, O, F, P, S, Cl, Br and I) or those with \nSMILES longer than 120 letters were removed. Random/canonical SMILES were generated and tokenized \nin the same way as MOSES dataset. About 9,000 molecules were sampled as the test set, and the \nremaining molecules were used for training. All of these processes were conducted using RDKit module \nversion 2023.03.133. \nLearning procedure \nWe set batch size to 128, and one epoch of training set of MOSES and ZINC-15 amounted to about 12k \nand 240k steps, respectively. Warm up scheduler was adopted as in the case of vanilla Transformer8. The \ndetailed hyperparameters are listed in Supplementary Table 1. The model was trained for 250k steps. \n \n3.3 Evaluation of VAE \nPerformance as a generative model \n30,000 latent vectors were randomly generated following prior distribution %(0, '), and SMILES were \ngenerated from them with the decoder by 4 beam search. For the molecules generated from the model \ntrained with MOSES datasets, we measured the metrics provided by the dataset 29, and compared them \nwith the previously reported scores of other models. Valid is ratio of valid SMILES in all generated \nSMILES. unique@1000 is ratio of SMILES which appear only once in 1000 valid generated SMILES. \nunique@10000 is the same uniqueness in 10000 valid generated SMILES. FCD (Fréchet ChemNet \nDistance) is difference of generated molecules to test set in terms of distribution of molecules in the last \nlayer activations of ChemNet. SNN (nearest neighbor similarity) is average similarity of each generated \nmolecule to its nearest molecule in test set. Frag (fragment similarity) is cosine similarity of frequency of \nfragments between generated molecules and test set. Scaf (scaffold similarity) is cosine similarity of \nfrequency of scaffolds between generated molecules and test set. FCD, SNN, Frag and Scaf metrics were \ncalculated for both test and testSF set. IntDiv (internal diversity) is average Tanimoto distance of ECFP \n(R=2) of generated molecules. Filters is ratio of valid unique molecules which satisfied conditions used to \nfilter training & test set. Novelty is ratio of valid unique molecules which does not appear in training set. \nAs for the model trained with ZINC-15, only Valid, unique@1000, unique@10000, IntDiv and Novelty \nagainst the training set of the generated molecules were measured because the other metrics29 depend on \nthe training and test set. Training and generation of molecules were conducted 3 times in the baseline \ncondition and once in the ablation studies. \n \nTransition of reconstruction and regularization \nThe reconstruction ability and normalization of the latent space during training were measured. \nRandomized SMILES of test set molecules were encoded and decoded to their canonical SMILES, and \ntheir complete reconstruction accuracy (perfect accuracy) and character-wise reconstruction accuracy \n(partial accuracy)34 were calculated. Greedy decoding was used for reconstruction. \nThe distribution of estimated mean and variance of the posterior distribution of the latent variables were \ncalculated at step 10000, 50000, 100000, 250000. Our model yields mean (() and log variance (\u0015\u0016\u0017()*)) \nof the latent variables, and the distributions of them for all elements and all molecules in the test set were \ncalculated. \n \nMolecular property prediction from latent variables \nWe used the property datasets of molecules in MoleculeNet 35 with the DeepChem module 36 (summarized \nin Supplementary Table 3). We filtered the molecules in the dataset in the same way as the training set. \nAdditionally, we removed the salts of the molecules because the molecules in the training set (both \nMOSES and ZINC-15) do not contain salts. We substituted the mean of all predictions to the prediction on \nexcluded molecules. The recommended splitting algorithms in MoleculeNet and DeepChem were used, \nand XGBoost37 was used to predict the properties of molecules from the means of latent variables \nestimated from the encoder of the model fed with randomized SMILES. The hyperparameters of XGBoost \nwere optimized by Bayesian optimization with optuna38 by 100 trials in the ranges shown in \nSupplementary Table 4.  ECFP39 and CDDD26 were used as the baseline descriptors, and Uni-Mol40 was \nused as the baseline model. Note that Uni-Mol is a fine-tuning model, whereas others, including our \nmodel, are descriptor generation methods. \n \n3.3 Visualization of attention \nWe computed and visualized the allocation of attention to each molecule in the model trained with \nMOSES dataset. At each layer of the encoder, we visualized attention to each atom averaged for all tokens \nin SMILES and all heads. Note that we did not study about the decoder because some attention \ncombinations were masked.  \nAttention to tokens subordinate to one atom (like ‘[’, ‘]’, and ‘@’) was added to the attention to that atom, \nwhile attention to tokens which cannot be attributed to a certain atom (like ‘-’ or ‘=’ representing bonds) \nwas ignored. \n \n4 Results and Discussion \n4.1 Generative and reconstruction performance of Transformer VAE \nThe Transformer VAE model was trained with the MOSES train dataset, and 30,000 SMILES were \ngenerated by the trained decoder from random latent variables following the prior distribution. Table 1 \nshows the property of the generated molecules averaged for 3 trials compared to those from other \ngenerative models trained with the MOSES dataset. The results showed that while the Transformer VAE \nmodel maintains high validity and uniqueness, our model achieved a high ability to generate molecules \nwhich do not exist in the training set. The high novelty and uniqueness imply that the current model \nsucceeded in encompassing far more molecules in the latent space, suggesting that the chemical space \ngenerated by the model can contain more drug seeds. The model also showed high scaffold similarity with \nthe testSF set, which have scaffolds not seen in the train set. This means that our model has a greater \nability to generate novel scaffolds, which can be the reason for the high novelty of the generated \nmolecules. This feature of the proposed model suggests the possibility of further study to screen novel \ndrug candidates in the latent space using docking simulator or its emulator 41. \nReconstruction performance of the molecules in the test set were also measured, and perfect accuracy \n(ratio of completely matched predictions) and partial accuracy (character-wise correspondence of the \npredicted strings, see methods for both metrics) and reached nearly 90% and 99%, respectively (Table 2). \nThis indicates that about 90% of molecules were reversibly mapped to the latent space, and it is suggested \nthat the model can also be used for generation of molecular descriptor, and that the tasks related to the \ndistribution of molecules can be performed in the latent space before decoding, such as molecule \noptimization, active learning and definition of applicability domain of machine learning models.  \nDistribution of the predicted mean and variance of the latent variables for all elements of latent \nvariables and all molecules in test set and all molecules in the test were shown in Figure 2, and it shows \nthat most of the elements in the latent variables follow normal distribution %(0, ') (same as prior \ndistribution), while a few elements have low variance. This suggests that the structural information of the \ninput is held by only a few elements, while most of the elements have no information. We conducted \nfurther experiment to confirm this in section 4.2.4 \n \n4.2 Ablation studies \n  4.2.1 Training length \nWe investigated the learning progress and generative performance of the Transformer VAE model during \ntraining, conducting a total of 3 trials. One of the trials was prolonged to 1M steps to observe the \nsaturation of the performance. The results indicated that, over the course of training, the Transformer VAE \nmodel gradually learned to generate molecules similar to the training set, while the ability to generate \nnovel scaffolds (measured by scaffold similarity to the testSF set) slightly decreased (Figure 3a). The \nvalidity, a metric indicating the property of generating valid chemical structures, consistently remained \nhigh during the learning progress. These findings suggest that adjusting the length of training is crucial, \ndepending on whether the goal is to generate novel or similar molecules to the learned ones. In terms of \nreconstruction performance, partial accuracy saturated at early steps, while the convergence of perfect \naccuracy required longer training, consistent with a previous study (Figure 3b)42. The transition of the \ndistribution of the latent space showed gradual convergence to the prior probability distribution (+ =\n0, -. = 1), indicating that the model successfully balanced the reconstruction of inputs and regularization \nof the latent space during training (Figure 3c and 3d). \n \n  4.2.2 Weight of regularization on latent variables \nThe proposed Transformer VAE model adjusted to the ratio of /01 loss to reconstruction loss (defined as \n2) to 0.01. Here this ratio was varied from 0 (no regularization) to 1 (normal VAE) to investigate the effect \nof 2. When 2 is 0, the latent variables are not regularized. In this scenario, the latent variables to be \ndecoded into SMILES were generated from a normal distribution, where the distribution has the same \nmean and variance as the latent variables of the sampled training set. The results showed that the validity \nand scaffold similarity to the test set of the generated molecules improved as 2 increased (except when 2 \nis 1), while novelty and scaffold similarity to the testSF set deteriorated (Figure 4a). These findings \nsuggest that a larger 2 encourages the model to generate molecules close to the learned chemical space, \nwhile smaller 2 values result in the generation of more novel molecules. Large 2 can degrade the \nreconstruction performance of the model, but regularization of latent variables has little effect on \nreconstruction when 2 is small (~0.001), compared to the performance of the model without regularization \n(2=0) (Figure 4b). The distribution of mean and variance of the latent variables indicated that the \ndistribution becomes closer to the standard normal distribution as 2 is increased, while a few elements \ncontinue to hold molecular information with low variance, except when 2 is 1 (Figure 4c and 4d). When \n2 =1 and the model returns to a normal VAE, it is suggested that posterior collapse occurs, resulting in the \ncomplete correspondence of the distribution of the latent space and prior distribution, and low novelty of \ngenerated molecules. \n \n  4.2.3 Comparison with other generative model architecture \nTo assess the impact of the VAE in the model, a comparison was made with two different architectures \nutilizing Transformer (Figure 5). The first model, named no-reparametrization, simply decodes SMILES \nfrom latent variables generated by the encoder without reparametrization by the normal distribution. This \nmodel resembles the VAE model when 2 = 0. The second model is a generative language model, which \nlearns the distribution of each token in SMILES conditioned by the preceding tokens. This model learns \nthe distribution of molecules similar to a collapsed VAE model. For the language model, SMILES strings \nwere generated not by beam search but by sampling each token following the predicted distribution. This \ngeneration method was also applied to the VAE model for comparison. The results indicated that the VAE \nmodel outperformed the other models in generating novel molecules with novel scaffolds (Table 3). This \noutcome suggests the advantage of VAE in generating novel data compared to a language model, which is \ncurrently prevalent in generative tasks. \n \n  4.2.4 Dimension of latent variables \nFigure 2 suggests that the structural information of molecules is primarily encoded by a few elements of \nlatent variables with low variance, while most of the elements follow the prior distribution. To explore this \nfurther, the dimension of latent variables was varied, and the generative and reconstruction performance \nwas compared. The results revealed that generative performance remained consistent until the dimension \nwas decreased to 16, and all metrics, except scaffold similarity to the test set, were unaffected when 2 was \ndecreased to 8 (Figure 6a). Reconstruction accuracy also remained at a consistent level when the \ndimension was 16 or larger (Figure 6b and 6c). These findings indicate that the structural information of \nmolecules and the diversity of the MOSES dataset can be adequately represented by 16-dimensional latent \nvariables. This dimension is considerably shorter than commonly used molecular descriptors such as \nECFP (~2048 dimensions) or MACCS (166 dimensions), as well as deep-learning-based descriptors like \nCDDD26. This suggests the possibility of using more lightweight machine learning models in tasks related \nto molecules. The required dimension of latent variables may depend on the dataset to be learned, as \ndiscussed in section 4.4. \n \n4.4 Training with ZINC-15 dataset \nWe have been training Transformer VAE with the MOSES dataset which satisfies rigid conditions \nregarding drug likeliness, but a model trained on a larger dataset could generate various and novel \nmolecules that contribute to the discovery of new drug seeds. To examine the ability of the proposed \nmodel to generate novel molecules from a larger dataset, we trained the model using the ZINC-15 dataset, \nwhich contains approximately 1 billion molecules. The model was trained with the same hyperparameters \nas the training with the MOSES dataset and successfully converged without posterior collapse \n(Supplementary Table 5,  Figure 7a and 7b). The generated molecules showed high uniqueness, validity \nand novelty (Table 4) when generating SMILES.  \nWhen varying the dimension of the latent variables, the reconstruction performance did not change \nwhen the dimension was 32 or larger (Figure 7c). However, unlike the model trained by the MOSES \ndataset, perfect accuracy decreased when the dimension was 16. This result suggests that the molecules in \nthe ZINC-15 dataset are more diverse and require a higher dimension (~32) for adequate description, \nalthough the required dimension is still smaller than commonly used descriptors. This also suggests that \nthe diversity of dataset needs to be considered to decide proper model size. \n \n4.5 Molecular property prediction from latent variables \nThe latent variables encoded by the model are expected to contain structural information of molecules, \nand can be used as molecular descriptors like those of other chemical language models 6,26. We therefore \nattempted to predict the molecular property and activity data provided by MoleculeNet from the mean of \nthe latent variables estimated by the encoder (before reparameterization). Figure 8 shows the prediction \nperformance compared with existing descriptors. The latent variables as a descriptor showed competitive \nperformance against existing molecular descriptors and a model in predicting most of the molecular \nproperties. These results indicate the capacity of the model to learn joint distribution of the output and \nmolecular properties, and to generate molecules conditioned by molecular properties. Notably, each \nvariable is derived from normal distribution, which would provide us the advantage of facilitating the \nconsideration and definition of applicability domain when latent representations are used as descriptors for \nmolecular property prediction. \n \n4.6 Visualization of attention \nFinally, we visualized attention weights to each token to clarify important substructures that were strongly \nattended by the other tokens. Figure 9 shows the weight of the attention targeted to each token, averaged \nfor all source tokens of attention and all heads. Note that only attention in the 1st, 4th, and 8th (last) layers is \nshown owing to space restrictions. The results showed that rare atoms or substructures received relatively \nlarge amount of attention, and suggested that such information is emphasized in the latent variables. Thus, \none of the advantages of this Transformer-based model is that atoms of importance can be visualized when \ngenerating latent variables, and this method can contribute to improving the accountability of chemical \nlanguage models, particularly in settings where the models learn various chemical structures in pretraining \n(not end-to-end purpose). \n \n5 Conclusion \nThe contributions of this study are as follows: \n \n1. We have developed a Transformer-VAE model that learns diverse chemical structures without \nencountering posterior collapse, the notorious problem associated with VAEs. \n2. The proposed model has demonstrated state-of-the-art performance in generating unseen molecules. \n3. Ablation study implied the advantage of VAE in generating novel molecules, compared to other \narchitectures such as a language model. \n4. We showed that the molecules in MOSES or ZINC-15 dataset can be described by variables of only 16 \nto 32 elements and such dataset dependency would highlight the importance of dataset-specific \nconsiderations in model design. \n5. The latent representation of the VAE showed competitive performance in predicting molecular \nproperties, suggesting the capacity for fast screening by the association of molecular properties with latent \nrepresentation. \n6. Attention weight in VAE encoder was visualized to show important structure in generating latent \nrepresentation of molecules by Transformer VAE model. \n \nThe novelty of this model is that it combines Transformer with VAE in the field of molecule generation. \nThe model structure and hyperparameters were optimized to improve the performance of molecule \ngeneration. The molecules generated by the model can be searched for numerous novel drug seeds by \nvirtual screening. Recently, there has been a growing demand for Virtual Chemical Libraries (VCL), \nparticularly in light of advancements in virtual screening methodologies. Our proposed model offers the \ncapability to easily generate a customized VCL from Gaussian random numbers and has the potential to \naccess unseen chemical spaces. \nA major limitation of our model is the lack of consideration of synthesis difficulty and in vivo stability. \nThis limitation can potentially be addressed by integrating models that evaluate synthesis complexity or \nimplementing filtering strategies as a virtual screening pipeline43,44. Thus, we believe that our model \ncontributes to the foundational aspects of drug discovery through the use of deep generative models, \nparticularly in handling diverse chemical structures. \n \nAuthor Contribution \nYasuhiro Yoshikai: Methodology, Software, Investigation, Writing – Original Draft, Visualization. \nTadahaya Mizuno: Conceptualization, Resources, Supervision, Project administration, Writing – Original \nDraft, Writing – Review & Editing, Funding acquisition. \nShumpei Nemoto: Methodology, Software \nHiroyuki Kusuhara: Writing – Review \n \nConflicts of Interest \nThe authors declare that they have no conflicts of interest. \n \nAvailability \nCode and models are available at https://github.com/mizuno-group/TransformerVAE. \n \nAcknowledgement \nWe thank all those who contributed to the construction of the following data sets employed in the present \nstudy such as ZINC-15 and MoleculeNet. This work was supported by AMED under Grant Number \nJP22mk0101250h and 23ak0101199h0001. \n \nReferences \n1. \nPolishchuk, P. G., Madzhidov, T. I. & Varnek, A. Estimation of the size of \ndrug-like chemical space based on GDB-17 data. J Comput Aided Mol Des 27, \n675–679 (2013). \n2. \nSterling, T. & Irwin, J. J. ZINC 15–ligand discovery for everyone. J Chem Inf \nModel 55, 2324–2337 (2015). \n3. \nKim, S. et al. PubChem substance and compound databases. Nucleic Acids Res \n44, D1202–D1213 (2016). \n4. \nMendez, D. et al. ChEMBL: towards direct deposition of bioassay data. \nNucleic Acids Res 47, D930–D940 (2019). \n5. \nKingma, D. P. & Welling, M. Auto-encoding variational bayes. Preprint at \nhttps://arxiv.org/abs/1312.6114 (2013). \n6. \nGómez-Bombarelli, R. et al. Automatic Chemical Design Using a Data-Driven \nContinuous Representation of Molecules. ACS Cent Sci 4, 268–276 (2018). \n7. \nKusner, M. J., Paige, B. & Miguel Hernández-Lobato, J. Grammar Variational \nAutoencoder. http://opensmiles.org/spec/open-smiles-2-grammar.html (2017). \n8. \nVaswani, A. et al. Attention Is All You Need. in Advances in Neural \nInformation Processing Systems (2017). \n9. \nDevlin, J., Chang, M.-W., Lee, K., Kristina, T. & Language, A. I. BERT: Pre-\ntraining of Deep Bidirectional Transformers for Language. Preprint at \nhttps://arxiv.org/abs/1810.04805 (2018). \n10. \nOpenai, A. R., Openai, K. N., Openai, T. S. & Openai, I. S. Improving \nLanguage \nUnderstanding \nby \nGenerative \nPre-Training. \nhttps://gluebenchmark.com/leaderboard. \n11. \nDollar, O., Joshi, N., Beck, D. A. C. & Pfaendtner, J. Attention-based \ngenerative models for: De novo molecular design. Chem Sci 12, 8362–8372 \n(2021). \n12. \nBowman, S. R. et al. Generating Sentences from a Continuous Space. (2015). \n13. \nFang, L. et al. Transformer-based Conditional Variational Autoencoder for \nControllable Story Generation. (2021). \n14. \nAlemi, A. A. et al. Fixing a Broken ELBO. (2017). \n15. \nJin, W., Barzilay, R. & Jaakkola, T. Junction Tree Variational Autoencoder for \nMolecular Graph Generation. (2018). \n16. \nLiu, Q., Allamanis, M., Brockschmidt, M. & Gaunt, A. L. Constrained Graph \nVariational Autoencoders for Molecule Design. \n17. \nDeep Learning for Molecular Design-a Review of the State of the Art. \n18. \nLim, J., Ryu, S., Kim, J. W. & Kim, W. Y. Molecular generative model based \non conditional variational autoencoder for de novo molecular design. J \nCheminform 10, (2018). \n19. \nChenthamarakshan, V. et al. CogMol: Target-Specific and Selective Drug \nDesign for COVID-19 Using Deep Generative Models. \n20. \nSamanta, S., O’Hagan, S., Swainston, N., Roberts, T. J. & Kell, D. B. VAE-\nSim: A novel molecular similarity measure based on a variational autoencoder. \nMolecules 25, (2020). \n21. \nBagal, V., Aggarwal, R., Vinod, P. K. & Priyakumar, U. D. MolGPT: molecular \ngeneration using a transformer-decoder model. J Chem Inf Model 62, 2064–\n2076 (2021). \n22. \nHong, Y.-B., Lee, K.-J., Heo, D. & Choi, H. Molecule Generation for Drug \nDiscovery \nwith \nNew \nTransformer \nArchitecture. \nPreprint \nat \nhttps://ssrn.com/abstract=4195528 (2022). \n23. \nWang, J. et al. Multi-constraint molecular generation based on conditional \ntransformer, knowledge distillation and reinforcement learning. Nat Mach \nIntell 3, 914–922 (2021). \n24. \nCofala, T. & Kramer, O. Transformers for Molecular Graph Generation. \n25. \nKim, H., Na, J. & Lee, W. B. Generative chemical transformer: neural machine \nlearning of molecular geometric structures from chemical language via \nattention. J Chem Inf Model 61, 5804–5814 (2021). \n26. \nWinter, R., Montanari, F., Noé, F. & Clevert, D.-A. Learning continuous and \ndata-driven \nmolecular \ndescriptors \nby \ntranslating \nequivalent \nchemical \nrepresentations. Chem Sci 10, 1692–1701 (2019). \n27. \nXiong, R. et al. On Layer Normalization in the Transformer Architecture. \n(2020). \n28. \nWilliams, R. J. & Zipser, D. A Learning Algorithm for Continually Running \nFully \nRecurrent \nNeural \nNetworks. \nhttp://direct.mit.edu/neco/article-\npdf/1/2/270/811849/neco.1989.1.2.270.pdf. \n29. \nPolykovskiy, D. et al. Molecular Sets (MOSES): A Benchmarking Platform for \nMolecular Generation Models. Front Pharmacol 11, (2020). \n30. \nIrwin, R., Dimitriadis, S., He, J. & Bjerrum, E. J. Chemformer: A pre-trained \ntransformer for computational chemistry. Mach Learn Sci Technol 3, 015022 \n(2022). \n31. \nBjerrum, E. J. & Sattarov, B. Improving chemical autoencoder latent space and \nmolecular de novo generation diversity with heteroencoders. Biomolecules 8, \n(2018). \n32. \nYoshikai, Y., Mizuno, T., Nemoto, S. & Kusuhara, H. Difficulty in Learning \nChirality for Transformer Fed with SMILES. \n33. \nRDKit. \n34. \nNemoto, S., Mizuno, T. & Kusuhara, H. Investigation of chemical structure \nrecognition by encoder–decoder models in learning progress. J Cheminform 15, \n(2023). \n35. \nWu, Z. et al. MoleculeNet: A benchmark for molecular machine learning. \nChem Sci 9, 513–530 (2018). \n36. \nRamsundar, B. MOLECULAR MACHINE LEARNING WITH DEEPCHEM. \nhttp://purl.stanford.edu/js264hd4826 (2018). \n37. \nChen, T. & Guestrin, C. XGBoost: A Scalable Tree Boosting System. (2016) \ndoi:10.1145/2939672.2939785. \n38. \nAkiba, T., Sano, S., Yanase, T., Ohta, T. & Koyama, M. Optuna: A next-\ngeneration hyperparameter optimization framework. in Proceedings of the 25th \nACM SIGKDD international conference on knowledge discovery & data mining \n2623–2631 (2019). \n39. \nRogers, D. & Hahn, M. Extended-connectivity fingerprints. J Chem Inf Model \n50, 742–754 (2010). \n40. \nZhou, G. et al. Uni-Mol: a universal 3D molecular representation learning \nframework. in International Conference on Learning Representations (2023). \n41. \nGentile, F. et al. Deep Docking: A Deep Learning Platform for Augmentation \nof Structure Based Drug Discovery. ACS Cent Sci 6, 939–949 (2020). \n42. \nYoshikai, Y., Mizuno, T., Nemoto, S. & Kusuhara, H. Difficulty in chirality \nrecognition for Transformer architectures learning chemical structures from \nstring representations. Nat Commun 15, 1197 (2024). \n43. \nBrenk, R. et al. Lessons learnt from assembling screening libraries for drug \ndiscovery for neglected diseases. ChemMedChem 3, 435–444 (2008). \n44. \nBaell, J. B. & Holloway, G. A. New substructure filters for removal of pan \nassay interference compounds (PAINS) from screening libraries and for their \nexclusion in bioassays. J Med Chem 53, 2719–2740 (2010). \n  \n \n \nFigures and Tables \n \nFigure 1. Structure of Transformer VAE model \nVisual summary of the model proposed in this paper. Filled shapes represent variables, and white shapes \nrepresent layers of process. Randomized SMILES was inputted into the encoder, and distribution of latent \nvariables \u0007 were estimated from the pooled memory. Latent variables were added to the embedding of \ncanonical SMILES to be decoded. Teacher forcing was used in the decoder during training, while beam \nsearch was used to generate new molecules. \n \n \n \nFigure 2. Distribution of the predicted mean and variance of latent variables \n(a, b) Distribution of mean and variance of latent variables estimated by the VAE encoder, for all elements \nand all molecules in test set at the end of the training for 3 training trials. Bin length was 0.02 for both \nmean and variance, and frequency indicates the number of elements in each bin. \n \n \n \nFigure 3. Effect of training steps \n(a) Temporal change of generative performance of the model evaluated by the metrics provided by the \ndataset. See Method section for detailed explanation of each metrics. The training was conducted for \n1000000 steps, and the performance was evaluated at various training steps. (b) Temporal change of \nperfect/partial accuracy for 1000000 steps. Perfect accuracy is the ratio of SMILES in training set which \nthe model was able to decode completely, and partial accuracy is the average ratio of the decoded tokens \nwhich matched the original. Greedy decode was used to generate prediction. (c, d) Distribution of mean \nand variance of latent variables estimated by the VAE encoder, for all elements and all molecules in test \nset at different training steps. Bin length was 0.02 for both mean and variance, and frequency indicates the \nnumber of elements in each bin. \n \n \n \nFigure 4. Effect of weight on \u001d\u001e\u001f \n(a) Generative performance of the model evaluated by the metrics provided by the dataset at the end of the \ntraining with different weight of loss on /01 to reconstruction loss (defined as 2). See Method section for \ndetailed explanation of each metrics. (b) Perfect accuracy / partial accuracy of the model trained with \ndifferent 2. Perfect accuracy is the ratio of SMILES in training set which the model was able to decode \ncompletely, and Partial accuracy is the average ratio of the decoded tokens which matched the original. \nGreedy decode was used to generate prediction. (c, d) Distribution of mean and variance of latent \nvariables estimated by the VAE encoder, for all elements and all molecules in test set at the end of the \ntraining with different 2. Bin length was 0.02 for both mean and variance, and frequency indicates the \nnumber of elements in each bin. \n \n \n \nFigure 5. The structure of the model compared with VAE \nStructure of two Transformer-based models used as baselines of generative performance. No-\nreparametrization model has similar structure as VAE, but it decodes SMILES from the latent variables \noutputted from the encoder without reparametrization, and generates molecules from randomly generated \nlatent variables following normal distribution with the same mean and variance of randomly sampled \ntraining set. Language model estimates the distribution of each token conditioned by the preceding tokens, \nand it generates SMILES by sampling tokens one by one following the estimated distribution. \n \n \n \nFigure 6. Effect of the dimension of the latent variables \n(a) Generative performance of the model evaluated by the metrics provided by the dataset at the end of the \ntraining with different dimension of latent variables. See Method section for detailed explanation of each \nmetrics. (b) Perfect accuracy / partial accuracy with different dimension of latent variables. Perfect \naccuracy is the ratio of SMILES in training set which the model was able to decode completely, and \nPartial accuracy is the average ratio of the decoded tokens which matched the original. Greedy decode \nwas used to generate prediction. (c, d) Distribution of mean and variance of latent variables estimated by \nthe VAE encoder, for all elements and all molecules in test set at the end of the training with different \ndimension of latent variables. Bin length was 0.02 for both mean and variance, and frequency indicates \nthe number of elements in each bin. \n \n \n \nFigure 7. Distribution of latent variables when the model was trained with ZINC-15 dataset \n(a, b) Distribution of mean and variance of latent variables estimated by the VAE encoder, for all \ndimensions and all molecules in test set at the end of the training with ZINC-15 dataset. Bin length was \n0.02 for both mean and variance, and frequency indicates the number of elements in each bin. \n \n \n \n \nFigure 8.  Performance of molecular property prediction by latent variables  \nPerformance of molecular property prediction by mean of latent variables estimated by the encoder from \nthe input randomized SMILES. The metrics of the prediction performance were decided based on \nMoleculeNet. ECFP and CDDD was used as baseline descriptor of molecules, and Uni-Mol was used as \nbaseline model. For all descriptors, XGBoost was used as predictor and hyperparameters were optimized \nby Bayesian optimization with optuna. Prediction was conducted and evaluated for 5 folds splitted by the \nmethods recommended by MoleculeNet. \n \n \n \n \nfigure 9.  Visualized weight of attention on each atom. \nWeights of attention to each atom averaged for all sources of attention in several layers of the encoder. \nAttention to tokens which are not element symbols but belong to certain atoms is incorporated into those \natoms, while attention to tokens representing bonds or other grammatical information are ignored. \nNonlinear colormap is used to highlight the difference of weights. \n \n \n \nTable 1. Generative performance of the Transformer VAE model, compared to baseline models \nin MolecularSets \n(￪)/ (￬) means higher/lower value is better.  \nOur model\nHMM\nNGram\nCombinatorial\nCharRNN\nAAE\nVAE\nJTN-VAE\nLatentGAN\nValid(↑)\n0.8761±0.0035\n0.0760±0.0322\n0.2376±0.0025\n1.0000±0.0000\n0.9748±0.0264\n0.9368±0.0341\n0.9767±0.0012\n1.0000±0.0000\n0.8966±0.0029\nunique@1000(↑)\n1.0000±0.0000\n0.6230±0.1224\n0.9740±0.0108\n0.9983±0.0015\n1.0000±0.0000\n1.0000±0.0000\n1.0000±0.0000\n1.0000±0.0000\n1.0000±0.0000\nunique@10000(↑)\n1.0000±0.0001\n0.5671±0.1424\n0.9217±0.0019\n0.9909±0.0009\n0.9994±0.0003\n0.9973±0.0020\n0.9984±0.0005\n0.9996±0.0003\nTest\n1.3380±0.0735\n24.4661±2.5251\n5.5069±0.1027\n4.2375±0.0370\n0.0732±0.0247\n0.5555±0.2033\n0.0990±0.0125\n0.2968±0.0087\nTestSF\n1.7098±0.0740\n25.4312±2.5599\n6.2306±0.0966\n4.5113±0.0274\n0.5204±0.0379\n1.0572±0.2375\n0.5670±0.0338\n0.8281±0.0117\nTest\n0.4803±0.0025\n0.3876±0.0107\n0.5209±0.0010\n0.4514±0.0003\n0.6015±0.0206\n0.6081±0.0043\n0.6257±0.0005\n0.5477±0.0076\n0.5371±0.0004\nTestSF\n0.4658±0.0026\n0.3795±0.0107\n0.4997±0.0005\n0.4388±0.0002\n0.5649±0.0142\n0.5677±0.0045\n0.5783±0.0008\n0.5194±0.0070\n0.5132±0.0002\nTest\n0.9927±0.0016\n0.5754±0.1224\n0.9846±0.0012\n0.9912±0.0004\n0.9998±0.0002\n0.9910±0.0051\n0.9994±0.0001\n0.9986±0.0004\nTestSF\n0.9898±0.0018\n0.5681±0.1218\n0.9815±0.0012\n0.9904±0.0003\n0.9983±0.0003\n0.9985±0.0003\n0.9947±0.0002\n0.9972±0.0007\nTest\n0.7159±0.0192\n0.2065±0.0481\n0.5302±0.0163\n0.4445±0.0056\n0.9242±0.0058\n0.9386±0.0021\n0.8964±0.0039\n0.8867±0.0009\nTestSF 0.1893±0.0081\n0.0490±0.0180\n0.0977±0.0142\n0.0865±0.0027\n0.1101±0.0081\n0.0588±0.0095\n0.1009±0.0105\n0.1072±0.0098\nIntDiv(↑)\n0.8531±0.0013\n0.8466±0.0403\n0.8738±0.0002\n0.8732±0.0002\n0.8557±0.0031\n0.8558±0.0004\n0.8551±0.0034\n0.8565±0.0007\nFilters(↑)\n0.9706±0.0005\n0.9024±0.0489\n0.9582±0.0010\n0.9557±0.0018\n0.9943±0.0034\n0.9960±0.0006\n0.9970±0.0002\n0.9760±0.0016\n0.9735±0.0006\nNovelty(↑)\n0.9911±0.0005\n0.9994±0.0010\n0.9694±0.0010\n0.9878±0.0008\n0.8419±0.0509\n0.7931±0.0285\n0.6949±0.0069\n0.9143±0.0058\n0.9498±0.0006\nFCD(↓)\nSNN(↑)\nFrag(↑)\nScaf(↑)\nTable 2. Reconstruction performance of the model in 3 training trials \n \n  \nPerfect accuracy \nPartial accuracy \ntrial #1 \n0.8927  \n0.9571  \ntrial #2 \n0.9067  \n0.9621  \ntrial #3 \n0.9065  \n0.9632  \nmean±std \n0.9020±0.0080 \n0.9608±0.0032 \n \n \n \nTable 3. Generative performance of the Transformer VAE model, compared to other \narchitectures \n \n \n \n \nVAE\n(beam search)\nVAE\n(token sampling)\nno-\nreparametrization\nlanguage\nmodel\nValid(↑)\n0.8747\n0.7735\n0.9960\nunique@1000(↑)\n1.0000\n1.0000\n0.9990\n1.0000\nunique@10000(↑)\n1.0000\n1.0000\n0.9977\n0.9980\nTest\n1.4128\n6.1512\n0.0712\nTestSF\n1.7952\n1.8560\n6.2894\n0.5459\nTest\n0.4774\n0.4741\n0.4936\n0.6417\nTestSF\n0.4628\n0.4603\n0.4811\n0.5871\nTest\n0.9911\n0.9903\n0.9611\n0.9998\nTestSF\n0.9880\n0.9577\n0.9982\nTest\n0.6939\n0.3726\n0.9394\nTestSF\n0.1863\n0.1914\n0.1235\n0.0321\nIntDiv(↑)\n0.8544\n0.8547\n0.8571\nFilters(↑)\n0.9700\n0.9618\n0.9990\nNovelty(↑)\n0.9906\n0.9916\n0.9942\n0.4800\nFCD(↓)\nSNN(↑)\nFrag(↑)\nScaf(↑)\nTable 4. Scores of molecules generated by Transformer VAE model trained with ZINC-15 \ndataset. \n(￪)/ (￬) means higher/lower value is better. \n \nMetric \nScore \nValid(↑) \n0.9010  \nunique@1000(↑) \n1.0000  \nunique@10000(↑) \n1.0000  \nIntDiv(↑) \n0.8518  \nNovelty(↑) \n0.9675  \n \n \n \nSupplementary Information for \n“A novel molecule generative model of VAE \ncombined with Transformer” \nSupplementary Tables \nSupplementary Table 1. Hyperparameters of the Transformer VAE model and training \n \nParameter \nValue \nn_layer \n8 \nd_model \n512 \ndim_feedforward \n2048 \ndropout \n0 \nscheduler \nwarmup \nscheduler8 \n warmup step \n4000 \n max learning rate \n0.001 \n \n \n \nSupplementary Table 2. Tokens in Transformer VAE model \n \nSpecial tokens \n<s>, </s>, <pad> \nNormal tokens \n0 1 2 3 4 5 6 7 8 9 ( ) [ ] : = @ \n@@ + / . - # % b c n o s p H B C \nN O S P F Cl Br I \n \n \n \nSupplementary Table 3. Datasets used for molecular property prediction \nNumber of valid molecules is the number of SMILES which successfully generated molecules, and \norganic molecules are those which contains only organic atoms. \n \nDataset \nTask \nSplitting \nMetric \n# of molecules \nTotal \nValid \nOrganic \nESOL \nRegression \nscaffold \nRMSE \n1128 \n1128 \n1128 \nFreeSolv \nRegression \nrandom \nRMSE \n642 \n642 \n642 \nLipo \nRegression \nscaffold \nRMSE \n4200 \n4200 \n4198 \nBACE \nClassification \nscaffold \nAUROC \n1513 \n1513 \n1513 \nBBBP \nClassification \nscaffold \nAUROC \n2050 \n2039 \n2039 \nClinTox \nClassification \nrandom \nAUROC \n1484 \n1478 \n1456 \n \n \n \n \nSupplementary Table 4. Search range of hyperparameters of XGBoost \nSee documentation of XGBoost module (https://xgboost.readthedocs.io/en/stable) for \nexplanation of each parameter. \n \n  \nMinimum \nMaximum \neta \n1.0-8 \n10.0  \ngamma \n1.0-8 \n10.0  \nmax_depth \n3 \n15 \nmin_child_weight \n1 \n10 \nmax_delta_step \n1 \n10 \nsubsample \n0.6 \n1.0  \ncolsample_bytree \n0.6 \n1.0  \nlambda \n1.0-8 \n0.1 \nalpha \n1.0-8 \n1.0  \n \n \n \nSupplementary Table 5. Reconstruction performance of the model trained with ZINC-15 \ndataset \n \nPerfect accuracy \nPartial accuracy \n0.8775  \n0.9760  \n \n \n"
}