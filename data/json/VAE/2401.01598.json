{
    "optim": "Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning Zitong Huang1 Ze Chen2 Zhixing Chen1 Erjin Zhou2 Xinxing Xu3 Rick Siow Mong Goh3 Yong Liu3 Chunmei Feng3Q Wangmeng Zuo1Q 1Harbin Institute of Technology 2MEGVII Technology 3IHPC, A*STAR zitonghuang99@gmail.com, chenze@megvii.com, robotic.zxchen@gmail.com, zej@megvii.com, fengcm.ai@gmail.com, wmzuo@hit.edu.cn Abstract Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes based on very limited training data without forgetting the old ones encountered. Existing studies solely relied on pure visual networks, while in this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP) and propose a sim- ple yet effective framework, named Learning Prompt with Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP for zero-shot evaluation can sub- stantially outperform the most influential methods. Then, prompt tuning technique is involved to further improve its adaptation ability, allowing the model to continually cap- ture specific knowledge from each session. To prevent the learnable prompt from forgetting old knowledge in the new session, we propose a pseudo-feature replay approach. Specifically, we preserve the old knowledge of each class by maintaining a feature-level Gaussian distribution with a diagonal covariance matrix, which is estimated by the image features of training images and synthesized features generated from a VAE. When progressing to a new ses- sion, pseudo-features are sampled from old-class distribu- tions combined with training images of the current session to optimize the prompt, thus enabling the model to learn new knowledge while retaining old knowledge. Experi- ments on three prevalent benchmarks, i.e., CIFAR100, mini- ImageNet, CUB-200, and two more challenging bench- marks, i.e. SUN-397 and CUB-200∗ proposed in this paper showcase the superiority of LP-DiF, achieving new state- of-the-art (SOTA) in FSCIL. Code is publicly available at https://github.com/1170300714/LP-DiF. 1. Introduction Class-Incremental Learning (CIL) [10, 42, 59] faces challenges in data-scarce real-world applications, e.g., face Average Accuracy (%) TOPIC [37] CEC [53] F2M [33] FACT [56] GKEAL [61] BDF [55] CLIP [29] CLIP+BDF  Ours  Joint-LP (UB)  40 50 60 70 80 90 100 39.64 57.75 57.89 59.88 60.47 94.81 93.76 77.57 84.63 61.42 FSCIL with pure vision model FSCIL with V-L model Ours Joint training (UB) Figure 1. Comparison of FSCIL methods in terms of Average Ac- curacy (%) on the test set of mini-ImageNet benchmark [32] under 5-shot setting. Red-highlighted bars indicate SOTA vision- based models (e.g., CNN [17]), while orange highlights show V-L pretrained models enhancing FSCIL, significantly outper- forming those vision-based counterparts. Our method, marked in green, achieves 93.76%, surpassing CLIP+BDF by 9.13%, and comparable to the theoretical upper bound (UB) that highlights in blue achieved through learning prompt in joint-training manner. recognition systems [56] and smart photo albums [37]. This has led to the emergence of Few-Shot CIL (FSCIL) [39], where models adapt to new classes with limited training data, showcasing their relevance and flexibility in data- scarce scenarios. In FSCIL, with only a few samples for each incremen- tal task, the main challenge is not just avoiding catas- trophic forgetting of previous knowledge [36, 37, 39] but 1 arXiv:2401.01598v1  [cs.CV]  3 Jan 2024 also facilitating plasticity from limited data. Existing stud- ies usually address this by first pre-training a classifier on a base set with numerous images for a robust founda- tion [21, 33, 37, 54, 56–58, 61]. Subsequent adaptations, e.g., knowledge distillation [56], class relationship model- ing [37, 54], and specific optimization [33], are then applied to the sparse incremental session data to boost performance while maintaining previously acquired knowledge. This work diverges from approaches that solely rely on visual networks [17], opting instead to leverage the capa- bilities of a Vision-Language (V-L) pretrained model, i.e., CLIP [29, 60], to develop a few-shot incremental learner. Comparing with existing state-of-the-art techniques (see the Red-highlighted bars in Fig. 1), we observed that by simply crafting the manual prompt “A photo of a [CLS]” as textual input and performing zero-shot evaluation on the widely used FSCIL benchmark, mini-ImageNet [32] test set, CLIP (refer to the orange CLIP bar in Fig. 1) substan- tially outperforms all these SOTA methods, with a notable 16.15% performance boost over BiDistFSCIL (BDF) [56]. This finding indicates that the generalization abilities of V- L pretrained models are highly beneficial for FSCIL, e.g., naturally mitigating the plasticity issues caused by limited training samples. Further, from Fig. 1, simply replacing the existing backbones of current SOTA methods with a pre- trained image encoder, initializing and learning the classi- fier with the corresponding text encoding of manual prompt can further enhance performance (7.16% gain to CLIP) but still lag behind the UB (9.13% lower than the UB) . There- fore, how to derive an efficient and lightweight prompt for FSCIL continues to be a compelling challenge. Based on the above preliminary results, this paper pro- poses a simple yet effective FSCIL framework by learning a lightweight prompt built upon the V-L pre-trained models. Unlike CLIP, as well as simply integrating CLIP with exist- ing methods (refer to the orange bar in Fig. 1, we resort to improving prompt tuning [60] for meeting the requirements of FSCIL. Specifically, for session t, we take the prompt in session t − 1 for initialization, combine it with [CLS] to create the full-text input for each class, and then optimize learnable prompt with training data. To prevent the learnable prompt from forgetting prior knowledge in a new session, we also propose a pseudo- feature replay technique. Specifically, observing that the image features extracted by the image encoder of CILP for each class seem to follow a Gaussian distribution (refer to Fig. 3), we attempt to estimate its mean vector and diag- onal covariance matrix (i.e. parameters of Gaussian distri- bution) to fit the training data of each class. To this end, a VAE [24, 47] comprised of the V-L model and lightweight MLPs are proposed to synthesize features based on the few training samples and text information, permitting the us- age of real image features as well as synthesized features to estimate Gaussian distribution parameters more accurately. When the model trains on a new session, pseudo-image fea- tures from the old-class distributions are sampled as old- knowledge replay to constrain the optimization direction of the prompt, avoiding learning towards catastrophic for- getting. The results in Fig. 1 showcase that our approach improves zero-shot evaluation for CLIP by 16.19% and for CLIP+BDF by 9.13%. Notably, our method is merely 1.05% lower than the upper bound (Joint-LP, i.e., learning prompt on training data of each session jointly). In a nutshell, the main contributions of this paper are summarized as follows: 1) We empirically show that pretrained V-L models, e.g. CLIP, are beneficial for FSCIL due to its considerable generalization ability, inspiring us to propose a simple yet effective V-L based FSCIL method named LP-DiF. 2) We adopt prompt tuning for allowing the model to con- tinually capture specific knowledge of each session, and present a feature replay technique to prevent catastrophic forgetting. By constructing feature-level Gaussian distri- bution for each class, pseudo feature replay can be com- bined with training images of current session to learn new knowledge while retaining old knowledge. 3) Extensive evaluations and comparisons on three prevalent FSCIL benchmarks (CIFAR-100, CUB-200 and mini- ImageNet) and two proposed more challenging bench- marks (SUN-397 and CUB-200∗) show the superiority of our methods in comparison to state-of-the-arts. 2. Related Work Few-Shot Class-Incremental Learning. The few-shot class-incremental learning methods (FSCIL) aims to train a model in a class-incremental manner [10, 59] with only a few samples for each new tasks [39]. Existing stud- ies can be categorized into four families, i.e., dynamic network-based methods, meta-learning-based methods, fea- ture space-based methods, and replay-based methods. In specific, dynamic network structure [15, 37, 49, 50] is proposed to adaptive learn the new knowledge. Meta learning-based methods [9, 18, 28, 53, 61, 63] employ a session sampling scheme, simulating the incremental learn- ing process during evaluation. feature space-based meth- ods [2, 8, 23, 55, 57, 58, 62], focus on mapping the original image into a condensed feature space while preserving its essential attributes. Replay-based methods [7, 11, 26] retain or produce significant data from prior tasks to be reintro- duced in the ongoing task. While these methods have shown commendable performance, all those studies are based on feature extractors and classifiers built from deep networks trained in the base session. Due to the scarcity of incre- mental class samples, the feature representation ability is limited. In contrast, we propose to construct an incremental learner on a VL pre-trained model [29, 60] that offers in- 2 herent merits for FSCIL, i.e., endowing the image encoder with powerful feature representation abilities. Replay-based Incremental Learning. The replay-based approach in incremental learning leverages knowledge from previous tasks to mitigate catastrophic forgetting in mod- els [1, 3–6, 16, 19, 20, 31, 34]. A basic data replay ap- proach involves retaining a concise exemplar set, capturing essential samples from prior tasks [4, 5], then, the classifica- tion model is trained on the combination of exemplars and the data of the current task. Different from directly storing the real instances, several following works [16, 19, 22, 34] leveraged a generative model [14, 24] for generating data from previous tasks. Compared to methods based on real image replay, pseudo replay reduces storage needs by elim- inating the requirement for exemplars and enriches the di- versity of samples from previous tasks. Yet, the overhead of training the image generator and dynamically produc- ing pseudo images introduces additional computational de- mands and prolongs training time. Instead of retaining an image generator, we represent the feature representation for each class using a Gaussian distribution, utilizing it to sam- ple pseudo-features for rehearsing prior knowledge. More- over, drawing samples from this distribution is computa- tionally efficient, offering our method an effective way for handling prevent catastrophic forgetting. Incremental Learning via Pre-trained Model. Recent studies have explored constructing incremental learners us- ing pre-trained models [13, 35, 38, 43–46, 52]. The core idea of these methods is to leverage a pre-trained backbone, e.g., ViT [12], for robust image feature extraction, while only fine-tuning a selected set of parameters to adapt to new tasks. For example, L2P [46] employs a fixed pre-trained ViT as its backbone and sustains a dynamic prompt pool with various sets of adaptable prompts. Some following works [35, 43] built upon this concept, applying it to the VL pretrained model [29], leveraging linguistic knowledge to bolster classification performance. The above studies un- derscore the significant advantages of using pretrained mod- els to boost performance in standard CIL scenarios. As for FSCIL, we inherit the advantages of pretrained models in CIL, and further maintain a Gaussian distribution at the fea- ture level for each class to mitigate catastrophic forgetting. 3. Proposed Method Problem Formulation. The purpose of FSCIL is to con- tinually learn knowledge of new classes from few samples, while simultaneously preventing the model from forgetting knowledge of old classes. Formally, a model is trained by a sequence of training data DTrain = {D(t) Train}T t=0 contin- ually, where D(t) Train = {(xi, yi)}N (t) i=0 denotes the training set of session (task) t. xi is a training image with corre- sponding class label yi ∈ C(t), where C(t) denotes the class space of D(t) Train. For different sessions, the class spaces are non-overlapping, i.e. ∀t1, t2 ∈ {0, 1, . . . , T} and t1 ̸= t2, C(t1) ∩ C(t2) = ∅. Typically, D(0) Train of the first session (i.e. t = 0), which is usually referred to as the base ses- sion, contains a substantial amount of training data. While D(t) Train(t > 0) of the incremental sessions only contains few training sample, organized as the N-Way K-shot for- mat, i.e., N classes in each incremental session with each class comprising K training images. Following the formu- lation of standard class-incremental learning, in session t, only D(t) Train and an optional memory buffer used to store the old knowledge (e.g. exemplar) can be accessed. After fin- ishing training on D(t) Train, the model is evaluated on a test set D(t) Test, the class space of which is union of all the classes encountered so far, i.e. C(0) ∪ C(1) · · · ∪ C(t). In this section, we propose a FSCIL method based on the V-L pretrained model, e.g. CLIP [29]. We assume that the class names are accessible during the training and testing of each session. Formally, CLIP contains an image encoder EImg(x) and a text encoder ETxt(p), which are pretrained jointly with a huge amount of image-text pairs in contrastive learning manner. An image x is fed into the image encoder, obtaining the corresponding L2-normalized feature f. p is a text token which is obtained by tokenizing a sentence like “A photo of a [CLS].”, where [CLS] represents a certain class name. We replace [CLS] by each class name respectively and obtain a set of text tokens {pc}C c=1, where C denotes the total number of classes encountered so far. Then, {pc}C c=1 are fed into the text encoder, obtaining the corresponding L2-normalized text feature {gc}C c=1. Finally, the prediction score of class c is computed by: p(y = c|x) = exp(⟨f, gc⟩/τ) PC j=1 exp(⟨f, gj⟩/τ) , (1) where ⟨·, ·⟩ denotes the cosine similarity of the two features and τ is the temperature parameter. 3.1. Approach Overview Although CLIP has demonstrated its superior perfor- mance on FSCIL in Fig. 1, using hand-crafted prompt is sub-optimal for transfer the knowledge to each incremen- tal session. So we replace the hand-crafted prompt with a set of learnable vectors V = {[V]l}L l=1 [60], where [V]l (l ∈ {1, . . . , L}) denotes one learnable vector, and L is the number of vectors. Hence, the expression for the text prompt is modified to: p(V) = [V]1[V]2 . . . [V]L[CLS], (2) To learn V on DTrain, an intuitive approach is to sequentially tune the prompt using training data from each incremen- tal session to continually acquire new knowledge. Specif- 3 Text  Encoder Image  Encoder 𝑧 = ෥𝝈 ∙ 𝒩 𝟎, 𝐈 + ෥𝝁 VAE   Encoder MLP MLP Bias [CLS] + ℒr VAE  Decoder (෥𝝁, ෥𝝈𝟐) ℒKL (a) Training VAE Image  Encoder [CLS] 𝐱 (𝑡−1) … … 𝐳~𝒩 𝟎, 𝐈 Noises 𝒩(𝝁𝑐, 𝝈𝑐2) VAE Decoder Estimate  Distribution dog car bird chairbanana cow human horse Old-class Knowledge Save to (b) Preserving Old-class Knowledge  Image Feature Text Feature Frozen Trainable Sampled Feature Gaussian Distribution Randomly Sample a  Pseudo Feature ℒ𝑛 ℒ𝑜 Supervise New-class Loss Old-class Loss apple New Class car,dog Old Class Synthesized Feature (c) Prompt Tuning with Replaying on Feature Distribution [𝐕VAE] 𝑁𝑐 𝑀 𝑀 Image  Encoder [CLS] 𝐕 1 𝐕 𝐿 … Prompts car，dog, cat … apple … 𝐱 (𝑡) 𝐟  𝐠car 𝐠dog 𝐠cat 𝐠apple 𝒩(𝝁car, 𝝈car 2 ) Text  Encoder 𝒩(𝝁dog,𝝈dog 2 ) … … … ℒn ℒo Old-class Knowledge Similarity Scores dog car bird chairbanana cow human horse 𝑦 (𝑡) {𝑐car, 𝑐dog} Randomly Select  B Old Classes B=2 መ𝐟car መ𝐟dog Figure 2. Overview of our proposed LP-DiF. (a) In each session, we first train a VAE [24, 47] comprised of the V-L model and lightweight components, i.e., MLPs and learnable prompt, based on few training data and textual information of this session. (b) We preserve the knowledge of each class by estimating their feature-level statistical distribution. The mean vector and diagonal covariance matrix of the distribution are estimated by both the features of real images and the synthesized features from trained VAE. (c) Prompt is trained jointly with the combination of the real image of the current session and the pseudo-features sampled from old-class distributions. … … 𝑐1: 𝑐2: dim 1 dim 2 dim D Value Density Value Density Value Density Value Density Density Value Density Value Figure 3. Histogram visualization of the statistical distribution of image features. We take the image features with different di- mensions (dim) of classes c1 and c2 as example selected from the mini-ImageNet [32] benchmark by the image encoder of CLIP (ViT-B/16) [29]. Each sub-figure shows the distribution with his- togram of corresponding random variable Zcd, where c and d de- notes the index of class and feature dimension respectively. Obvi- ously, 1) each dimension of the image features per class approx- imates Gaussian distribution; 2) distributions of same dimension vary in different classes, e.g., Zc11 vs. Zc21 . ically, at the beginning of session 0, we initialize V ran- domly; while for each following session t (t > 0), we use the V trained in the previous session (e.g. session t − 1) to initialize the V for current session. In a certain session t, given a pair of training sample (xi, yi) from D(t) Train, prompt is optimized on by minimizing Ln: Ln = − log exp(⟨fi, ETxt(pyi(V))⟩/τ) P| St s=0 C(s)| c=1 exp(⟨fi, ETxt(pc(V))⟩/τ) , (3) where fi denotes the L2-normalized image feature of xi, and pc(V) denotes the prompt corresponding to class c. However, using only the D(t) Train to optimize the prompt in session t will inevitably lead to catastrophic forgetting. Ide- ally, learning prompt with all training data from both pre- vious and current sessions (e.g. St s=0 D(s) Train) can address this issue, but this is not allowed under the protocol of FS- CIL. Therefore, this paper adopts a compromise solution, proposing to record old knowledge by maintaining statis- tical distributions of old classes instead of directly storing origin images. We setup a feature-level Gaussian distribu- tion to represent each old class, which is represented by a mean vector and a diagonal covariance matrix. We name it the old-class distribution. The mean vector and diago- nal covariance matrix of the old-class distribution are es- timated jointly from the features of real images as well as synthetic features generated by a VAE decoder. When learning the prompt in a new session, we randomly sample 4 features based on the statistical distribution of old classes to replay old knowledge. Then, the sampled features of old classes and the real features of new classes will jointly op- timize the prompt, thereby learning new knowledge while also replaying old knowledge. In the following, We will in- troduce how to obtain the old-class distribution in Sec 3.2, and how to learn prompt in Sec 3.3. 3.2. Estimation of Old-Class Distribution In each session t, we should estimate the feature-level statistical distribution for each class of D(t) Train. Given a certain class label c ∈ C(t) , the corresponding training images {xi}Nc i=1 are fed into the image encoder EImg(x) to obtain their L2-normalized features {fi}Nc i=1, where Nc denotes the number of training images of class c, fi = [fi1, fi2, . . . , fiD]T and D is the feature dimension (e.g. D = 512 for ViT-B/16). Intuitively, we assume that the features of class c follow a multivariate distribution N(µc, Σc), where µc ∈ RD denotes the mean vector and Σc ∈ RD×D ≥0 denotes the covariance matrix. As shown in Fig. 3, we observe that each dimension of these features of each class approximates a Gaussian distribution, and distri- butions of same dimension vary in different classes. Thus, each dimension of the feature can be treated as indepen- dently distributed, and the covariance matrix Σc can be sim- plified to a diagonal matrix and be represented by a vector σ2 c = [σ2 c1, σ2 c2, . . . , σ2 cD]T , which is diagonal values of Σc. We use random variable Zcd to represent the d-th dimen- sion of feature, following a specific Gaussian distribution N(µcd, σ2 cd), where µcd denotes the mean value of the d-th dimension. Then, Zc = [Zc1, Zc2, . . . , ZcD] represents the random variable of the whole feature following N(µc, σ2 c). Our goal is to estimate the µc and σ2 c for each class. For each class, simply using only {fi}Nc i=1 to estimate the µc and σ2 c may be inadequate due to the scarcity of the data. To tackle with this problem, we utilize a VAE [24, 47] com- prised of the V-L models and lightweight MLPs, leveraging the few training data and textual information to synthesize more image features, thereby benefiting the estimation of the distribution. As shown in Fig. 2 (a), in VAE Encoder, an image feature f is fed into a MLP, encoded to a latent code z, of which distribution is assumed to be a prior N(0, I): LKL = KL(N(˜µ, ˜σ2)||N(0, I)), (4) where KL represents the Kullback-Leibler divergence. In VAE Decoder, z is fed to another MLP and obtain the bias r, which is added to a set of learnable prompt VVAE = {[VVAE]l}L l=1: VVAE(z) = {[VVAE]l + r}L l=1, (5) Then, VVAE(z) concatenating with the class name [CLS] corresponding to f is fed into the text encoder, obtaining the reconstruct feature ˜f then calculating the reconstruct loss Lr: Lr = ∥f − ˜f∥2. (6) Finally, the total loss LVAE of training the VAE is: LVAE = LKL + λrLr, (7) where λr represents the coefficient of Lr. Using both the features synthesized by the VAE and the real image features, we estimate µc and σ2 c. As shown in Fig. 2 (b), for a specific class c, M noise vectors z ∼ N(0, I) and corresponding class name are input into the VAE Decoder, obtaining M synthesized features {˜fj}M j=1. Then, µc and σ2 c = [σ2 c1, σ2 c2, . . . , σ2 cD]T are estimated by: µc = 1 Nc + M ( Nc X i=1 fi + M X j=1 ˜fj), (8) σ2 cd = 1 (Nc + M) − 1( Nc X i=1 (fid−µcd)2+ M X j=1 ( ˜fjd−µcd)2). (9) 3.3. Learning Prompt with Feature Replay At session t, we learn prompt with D(t) Train as well as the distributions of old classes preserved in previous sessions. 1) When t = 0, i.e., the first session, we just follow the approach in Sec.3.1, randomly initializing V and learning them with D(0) Train by Ln (Eq. (3)). 2) When t > 0, V are initialized from trained weights in ses- sion t−1. For the new knowledge of D(t) Train, we adopt Ln. For the old knowledge of previous sessions, we randomly sample pseudo image features of old classes from their corresponding distributions. As shown in Fig.2 (c), for each selected training image in one batch xi, we first ran- domly select B old classes: {cb}B b=1 and cb ∈ ∪t−1 s C(s). Then, for each selected class cb, we randomly sample a pseudo feature ˆfcb from its feature distribution ˆfcb ∼ N(µcb, σ2 cb). These sampled features with their corre- sponding class labels are used to calculate the loss Lo: Lo = − B X b=1 log exp(⟨ˆfcb, ETxt(pcb(V))⟩/τ) P| St s=0 C(s)| c=1 exp(⟨ˆfcb, ETxt(pc(V))⟩/τ) . (10) Finally, the prompt is optimized by minimizing the loss: LLP = ( Ln if t = 0, Ln + λoLo if t > 0, (11) where λo represents the tradeoff coefficient. 5 Table 1. Comparison on mini-ImageNet, where the rows filled with gray indicate the existing SOTA FSCIL methods. “Avg” represents the average accuracy of all sessions; the higher the value, the better performance. “PD” represents the Performance Drop rate; the lower the value, the better performance. Methods Accuracy in each session (%) ↑ Avg ↑ PD ↓ 0 1 2 3 4 5 6 7 8 TOPIC [37] 61.31 50.09 45.17 41.16 37.48 35.52 32.19 29.46 24.42 39.64 36.89 CEC [54] 72.00 66.83 62.97 59.43 56.70 53.73 51.19 49.24 47.63 57.75 24.37 F2M [33] 72.05 67.47 63.16 59.70 56.71 53.77 51.11 49.21 47.84 57.89 24.21 Replay [27] 71.84 67.12 63.21 59.77 57.01 53.95 51.55 49.52 48.21 58.02 23.63 MetaFSCIL [9] 72.04 67.94 63.77 60.29 57.58 55.16 52.90 50.79 49.19 58.85 22.85 GKEAL [62] 73.59 68.90 65.33 62.29 59.39 56.70 54.20 52.59 51.31 60.48 22.28 FACT [57] 72.56 69.63 66.38 62.77 60.60 57.33 54.34 52.16 50.49 60.70 22.07 C-FSCIL [18] 76.40 71.14 66.46 63.29 60.42 57.46 54.78 53.11 51.41 61.59 14.99 BiDistFSCIL [56] 74.65 70.70 66.81 63.63 61.36 58.14 55.59 54.23 53.39 62.06 21.26 FCIL [15] 76.34 71.40 67.10 64.08 61.30 58.51 55.72 54.08 52.76 62.37 23.58 SAVC [36] 81.12 76.14 72.43 68.92 66.48 62.95 59.92 58.39 57.11 67.05 24.01 NC-FSCIL [51] 84.02 76.80 72.00 67.83 66.35 64.04 61.46 59.54 58.31 67.82 25.71 CLIP (Baseline) [29] 80.01 79.16 78.89 77.97 77.44 76.83 76.32 76.02 75.45 77.57 4.56 LP-DiF (Ours) 96.34 96.14 94.62 94.37 94.06 93.44 92.21 92.29 91.68 93.76 4.66 4. Experiments 4.1. Experimental Setup Datasets. Following the mainstream benchmark set- tings [56], we conduct experiments on three datasets, i.e., CIFAR-100 [25], mini-ImageNet [32] and CUB-200 [41], to evaluate our LP-DiF. Additionally, this paper also pro- poses two more challenging benchmarks for FSCIL, i.e., SUN-397 [48] and CUB-200∗. For SUN-397 [48], it has about twice the number of classes and incremental sessions than the aforementioned three benchmarks. We evaluate our method on this benchmark to reveal whether it works in scenarios with more classes and more incremental ses- sions. CUB-200∗ is a variant of CUB-200 but excluding the base session, is used to evaluate whether our method works in scenarios without the base session. Refer to Suppl. for the details of all selected benchmarks. Implementation Details. All experiments are conducted with PyTorch on 8× NVIDIA RTX 2080Ti GPUs. We leverage the ViT-B/16 as the image encoder and adopt SGD with 0.9 momentum to optimize the prompts. The learning rate is initialized by 0.002. For the base session, the batch size is set to 64 and the training epoch is set to 200, As for each incremental session, the batch size and the train- ing epochs are set to 25, 100, respectively. The VAE com- ponent is enabled only for incremental sessions. For the hyper-parameters, M is set to 10; B and λo are set to 8 and 2, respectively; L is set to 16 following Zhou et al. [60]; λr is set to 1 following Wang et al. [47]. 4.2. Main Results Comparison with State-of-The-Arts. We summarize the results of competing methods on mini-ImageNet in Table 1. Clearly, employing CLIP (baseline) [29, 38] for zero-shot Table 2. Comparison with Upper bound on the three common benchmarks in terms of Avg. Methods CIFAR-100 mini-ImageNet CUB-200 LP-DiF (Ours) 75.12 93.76 74.00 Joint-LP 76.22 94.81 75.42 (a) CIFAR-100 (b) CUB-200 Figure 4. Accuracy curves of our LP-DiF and comparison with existing SOTA FSCIL methods on (a) CIFAR100 and (b) CUB200. (a) SUN-397 (b) CUB-200* Figure 5. Accuracy curves of our LP-DiF and comparison with counterparts on (a) SUN-397 and (b) CUB200*. evaluation alone outperforms all existing FSCIL methods by a large margin in terms of accuracy in each session and Average Accuracy (Avg). Naturally, it achieves a no- 6 Table 3. Ablation studies of our LP-DiF on mini-ImageNet. LP and OCD denote learning prompts and old-class distribution, respectively. RF and SF denote the real features of training images and synthesized features generated by VAE, respectively. CLIP LP OCD Accuracy in each session (%) ↑ Avg ↑ PD ↓ RF SF 0 1 2 3 4 5 6 7 8 ✓ 80.01 79.16 78.89 77.97 77.44 76.83 76.32 76.02 75.45 77.57 4.56 ✓ ✓ 96.34 94.28 92.83 89.93 88.39 86.10 85.49 85.70 84.76 89.31 11.58 ✓ ✓ ✓ 96.34 96.14 94.01 94.27 93.23 93.07 91.34 91.17 90.76 93.37 5.46 ✓ ✓ ✓ 96.34 96.14 93.79 92.48 91.25 90.94 90.15 89.41 89.27 92.23 7.07 ✓ ✓ ✓ ✓ 96.34 96.14 94.62 94.37 94.06 93.44 92.21 92.29 91.68 93.76 4.66 Table 4. Comparison with other replay approaches on CUB-200 in terms of Avg. The results of “Randomly selection” are reports over 5 runs with mean and standard deviations. iCaRL† means applying the replay technique proposed in iCaRL to CLIP + LP. Methods Ne Storage Space Avg CLIP + LP - - 69.36 Randomly selection 1 18.32 ± 0.37 MB 69.95 ± 0.56 2 37.31 ± 0.99 MB 71.16 ± 0.24 3 55.95 ± 1.01 MB 72.44 ± 0.09 4 74.33 ± 0.70 MB 73.64 ± 0.16 iCaRL† [30] 1 18.54 MB 70.81 2 38.39 MB 71.68 3 55.40 MB 72.86 4 74.68 MB 73.95 LP-DiF (Ours) - 0.78 MB 74.00 tably lower Performance Drop rate (PD). Our LP-DiF fur- ther achieves 16.19% (77.57% → 93.76%) gains than the CLIP in terms of Avg, and shows comparable PD perfor- mance, i.e., 4.66% vs. 4.56%. As for the existing SOTA methods, e.g., NC-FSCIL [51], which presents the best Avg among all the SOTA methods, LP-DiF gains 25.94% im- provements, i.e., 67.82% → 93.76%. Comparing with C- FSCIL [18], which presents the best PD. among the com- peting methods, LP-DiF gains 10.33% improvements, i.e., 14.99% → 4.66%. Fig. 4 shows the performance curves on CIFAR-100 and CUB-200. One can observe that LP-DiF consistently outperforms existing SOTA methods in all ses- sions for all benchmarks. These results clearly illustrate the superiority of our LP-DiF. Comparison with Upper Bound. Assuming that the train- ing set from each previous session is available, we can jointly train the prompts using these sets, thereby avoiding the issue of forgetting old information. In class-incremental learning, the above-mentioned setting can be considered as an upper bound, and serve as a reference for evaluating FS- CIL method. Thus, we compare our LP-DiF with the upper bound (i.e. Joint-LP). As shown in Table 2, across the three benchmarks, the performance of our method is very close to the upper bound in terms of Avg, with the largest gap being only 1.42% on CUB-200. The results indicate that our LP- DiF is highly effective in preventing catastrophic forgetting. More Challenging Benchmarks. On the three widely used benchmarks, the performance of our LP-DiF closely approaches the upper bound. To further assess our LP- DiF, we provide two more challenging benchmarks: SUN- 397 and CUB-200* (refer to suppl. for details). For each challenging benchmark, we compare our LP-DiF with three distinct approaches: zero-shot evaluation using CLIP (baseline), Joint-LP (upper bound), and BiDistFSCIL [56] (SOTA open-source method). The corresponding perfor- mance curves are depicted in Fig. 5. Overall, on both SUN- 397 and CUB-200*, our LP-DiF method 1) significantly surpasses both CLIP and BiDistFSCIL, and 2) attains per- formance levels that are very close to the respective upper bounds. The results show that our LP-DiF remains very ef- fective on these challenging situations, including those with a larger number of classes and extended session lengths, e.g., 397 classes across 21 sessions in SUN-397, as well as in those without a base session, exemplified by CUB-200*. 4.3. Ablation Study Analysis of Key Components. Our proposed method in- volves prompt tuning on CLIP to adapt the knowledge from each incremental session. It also constructs feature-level distributions to preserve old knowledge, thereby achiev- ing resistance to catastrophic forgetting. To investigate the effect of the key components in our method, i.e., CLIP, prompt learning (LP), the distribution estimated by real fea- tures (RF) of training images, and the synthesized features (SF), we summarized the performance of each component on mini-ImageNet in Table 3. As illustrated, employing the LP technique noticeably improves performance across each session, ultimately resulting in a superior 11.74% perfor- mance, i.e., from 77.57% → 89.31% in terms of average performance (refer to the second row of Table 3). How- ever, solely implementing LP causes higher PD than CLIP, e.g., 4.56% → 11.58%, due to the forgetting of old knowl- edge during learning in new sessions. Additionally, as men- tioned in Sec.3.2, the old-class distribution (OCD) is effec- tive in tackling the forgetting problem. Note that the distri- bution is estimated by real features (RF) and the synthesized features (SF) generated by VAE. So we conducted separate evaluations to assess the effect of these two types of “fea- tures”. Concretely, using only RF to estimate the old-class 7 (b) Analysis of M (a) Comparison with LC CIFAR-100 mini-ImageNet CUB-200 50 60 70 80 90 Avg (%) (c) Analysis of     and 2 4 6 8 10 0.25 0.50 1.00 2.00 4.00 Figure 6. Ablation studies of our LP-DiF. (a) Comparison with the method of incorporating a Linear Classifier (LC) into a pre-trained image encoder for training. (b) Analysis of M. (c) Analysis of B and λo in terms of Avg on CUB-200. distribution can improve the Avg by 4.11%, i.e., 89.31% → 93.42%, and reduce the PD. by 6.12%, i.e., 11.58% → 5.46%, (see the third row of Table 3). Using only SF can also improve Avg and reduce PD, however, its effectiveness is marginally inferior to using only RF (refer to the fourth row of Table 3). Finally, using both types of “features” can further improve the performance, which surpasses the out- comes achieved by using either RF or SF alone (see the last row of Table 3). Thus, although LP can enable the model to effectively capture the knowledge of each session and im- prove performance, it still leads to catastrophic forgetting, while OCD can effectively prevent this issue. Analysis of M. As mentioned in Sec. 3.2, M is the number of synthetic features which participate in estimating µc and σc of old-class distribution. The value of M will affect the accuracy of the estimated distribution, thus influencing the performance of the model. Therefore, we test the effect of M on the performance. Fig. 6 (b) shows the results on the three widely used benchmarks. Clearly, when M increases from 0, the Avg gradually improves. Nonetheless, when M continues to increase, the Avg decreases slightly, possibly ascribing to that too many synthesized features can cause the estimated distribution to overly skew towards the distri- bution of the synthesized features. In summary, M = 10 (on mini-ImageNet) or M = 15 (on CIFAR-100 and CUB- 200) are the best choices for performance. Analysis of B and λo. Here we investigate the effect of the two hyper-parameters, i.e., B, the number of selected old classes involved in Lo, and λo, the tradeoff coefficient in Lo. The results on CUB-200 are shown with a mixed matrix of these two hyper-parameters in Fig 6 (c). Obvi- ously, keeping λo fixed, as B increases, the Avg improves gradually. Keeping B fixed and tuning λo shows a similar tendency with the above setting. It achieves the best Avg when B = 8 and λo = 2 on CUB-200. Then, when the values of B and λo are too large, e.g. B = 10 and λo = 4, there is a slight performance drop. These may be because too small values of B and λo lead to insufficient represen- tation of old knowledge, while too large values may cause the model to overly emphasize old knowledge. Learning Prompt vs. Linear Classifier. This study uti- lizes prompt tuning to tailor CLIP to the specific knowl- edge of each session. Another straightforward and intuitive strategy involves incorporating a linear classifier with the image encoder, which is initialized using the text encoding of handcrafted prompts. So we conduct additional experi- ments: 1) Refining the linear classifier (LC) solely with the training set accessible in the current session; 2) Extending the first approach by integrating the old-class distribution for feature replay (LC + OCD); 3) Jointly training the linear classifier with the complete training set from each session (Joint-LC). As shown in Fig 6 (a), the Avg of LC is notably lower than that of LP across three wide benchmarks in terms of Avg. The incorporation of OCD with LC (denoted as LC + OCD) enhances performance beyond LC alone, highlight- ing OCD’s effectiveness in mitigating catastrophic forget- ting. Nevertheless, the combined LC + OCD is still inferior to LP + OCD. In a joint training scenario, the performance of Joint-LC continues to be inferior to Joint-LP. The results suggest that the strategy of learning prompts offers more merits for FSCIL than that of learning a linear classifier. Old-Class Distribution vs. Image Exemplar. To further validate its efficacy in avoiding catastrophic forgetting, we compare our method with other replay-based approaches tailored for learning prompts, i.e., 1) randomly selecting Ne images of per old class as exemplars; 2) adopting the replay strategy in iCaRL [30], specifically choosing Ne im- ages for each old class based on the proximity to the mean feature. In addition, we execute the random selection ap- proach five times, each with a different random seed, to re- duce the uncertainty. The average results with necessary storage space for replay on CUB-200 are shown in Table 4, where CLIP + LP indicates the baseline without replay of old classes. Obviously, our method exhibits the best perfor- mance and lowest storage space in comparison to the two counterparts under various Ne. Especially, compared with iCaRL† under Ne = 4, LP-DiF shows a comparable per- formance while only requiring about 0.01% storage space. This underscores that our pseudo-feature replay technique can effectively combat catastrophic forgetting under condi- tions of light storage overhead. 8 5. Conclusion In this paper, we studied the FSCIL problem by in- troducing V-L pretrained model, and proposed Learning Prompt with Distribution-based Feature replay (LP-DiF). Specifically, prompt tuning is involved to adaptively capture the knowledge of each session. To alleviate catastrophic forgetting, we established a feature-level distribution for each class, which is estimated by both real features of training images and synthesized features generated by a VAE decoder. Then, pseudo features are sampled from old-class distributions, and combined with the training set of current session to train the prompts jointly. Extensive experiments demonstrate that our LP- DiF achieves the new state-of-the-art in the FSCIL task. References [1] Aishwarya Agarwal, Biplab Banerjee, Fabio Cuzzolin, and Subhasis Chaudhuri. Semantics-driven generative replay for few-shot class incremental learning. In Proceedings of the 30th ACM International Conference on Multimedia, pages 5246–5254, 2022. 3 [2] Afra Feyza Aky¨urek, Ekin Aky¨urek, Derry Tanti Wijaya, and Jacob Andreas. Subspace regularizers for few-shot class in- cremental learning. arXiv preprint arXiv:2110.07059, 2021. 2 [3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben- gio. Gradient based sample selection for online continual learning. Advances in neural information processing sys- tems, 32, 2019. 3 [4] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha, and Jonghyun Choi. Rainbow memory: Continual learn- ing with a memory of diverse samples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8218–8227, 2021. 3 [5] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan- than, and Philip HS Torr. Riemannian walk for incremen- tal learning: Understanding forgetting and intransigence. In Proceedings of the European conference on computer vision (ECCV), pages 532–547, 2018. 3 [6] Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip Torr, and David Lopez-Paz. Using hindsight to anchor past knowledge in continual learning. In Proceedings of the AAAI conference on artificial intelligence, pages 6993–7001, 2021. 3 [7] Ali Cheraghian, Shafin Rahman, Pengfei Fang, Soumava Kumar Roy, Lars Petersson, and Mehrtash Harandi. Semantic-aware knowledge distillation for few- shot class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2534–2543, 2021. 2 [8] Ali Cheraghian, Shafin Rahman, Sameera Ramasinghe, Pengfei Fang, Christian Simon, Lars Petersson, and Mehrtash Harandi. Synthesized feature based few-shot class- incremental learning on a mixture of subspaces. In Proceed- ings of the IEEE/CVF international conference on computer vision, pages 8661–8670, 2021. 2 [9] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao Yu, and Jin Tang. Metafscil: A meta-learning approach for few-shot class incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14166–14175, 2022. 2, 6, 13 [10] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah Parisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and Tinne Tuytelaars. A continual learning survey: Defying for- getting in classification tasks. IEEE transactions on pattern analysis and machine intelligence, 44(7):3366–3385, 2021. 1, 2 [11] Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan Chang, Xing Wei, and Yihong Gong. Few-shot class- incremental learning via relation knowledge distillation. In Proceedings of the AAAI Conference on Artificial Intelli- gence, pages 1255–1263, 2021. 2 [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, et al. An image is worth 16x16 words: Trans- formers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 3 [13] Arthur Douillard, Alexandre Ram´e, Guillaume Couairon, and Matthieu Cord. Dytox: Transformers for continual learning with dynamic token expansion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9285–9295, 2022. 3 [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. 3 [15] Ziqi Gu, Chunyan Xu, Jian Yang, and Zhen Cui. Few-shot continual infomax learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 19224– 19233, 2023. 2, 6, 13 [16] Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen. Exemplar-supported generative reproduction for class incre- mental learning. In BMVC, page 98, 2018. 3 [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceed- ings of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016. 1, 2 [18] Michael Hersche, Geethan Karunaratne, Giovanni Cheru- bini, Luca Benini, Abu Sebastian, and Abbas Rahimi. Con- strained few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pat- tern Recognition, pages 9057–9067, 2022. 2, 6, 7, 13 [19] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zheng- wei Tao, Jinwen Ma, Dongyan Zhao, and Rui Yan. Overcom- ing catastrophic forgetting for continual learning via model adaptation. In International conference on learning repre- sentations, 2018. 3 [20] David Isele and Akansel Cosgun. Selective experience re- play for lifelong learning. In Proceedings of the AAAI Con- ference on Artificial Intelligence, 2018. 3 [21] Zhong Ji, Zhishen Hou, Xiyao Liu, Yanwei Pang, and Xue- long Li. Memorizing complementation network for few- 9 shot class-incremental learning. IEEE Transactions on Im- age Processing, 32:937–948, 2023. 2 [22] Jian Jiang, Edoardo Cetin, and Oya Celiktutan. Ib-drr- incremental learning with information-back discrete repre- sentation replay. In Proceedings of the IEEE/CVF Confer- ence on Computer Vision and Pattern Recognition, pages 3533–3542, 2021. 3 [23] Do-Yeon Kim, Dong-Jun Han, Jun Seo, and Jaekyun Moon. Warping the space: Weight space rotation for class- incremental few-shot learning. In The Eleventh International Conference on Learning Representations, 2022. 2 [24] Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3, 4, 5 [25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 6, 12 [26] Anna Kukleva, Hilde Kuehne, and Bernt Schiele. Gener- alized and incremental few-shot learning by explicit learn- ing and calibration without forgetting. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9020–9029, 2021. 2 [27] Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu, Jun Chen, and Jin Tang. Few-shot class-incremental learning via entropy-regularized data-free replay. In European Con- ference on Computer Vision, pages 146–162. Springer, 2022. 6, 13 [28] Pratik Mazumder, Pravendra Singh, and Piyush Rai. Few- shot lifelong learning. In Proceedings of the AAAI Confer- ence on Artificial Intelligence, pages 2337–2345, 2021. 2 [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2, 3, 4, 6, 13 [30] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. icarl: Incremental classifier and representation learning. In Proceedings of the IEEE con- ference on Computer Vision and Pattern Recognition, pages 2001–2010, 2017. 7, 8 [31] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil- licrap, and Gregory Wayne. Experience replay for continual learning. Advances in Neural Information Processing Sys- tems, 32, 2019. 3 [32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211–252, 2015. 1, 2, 4, 6, 12 [33] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming Zhan, and Xiao-Ming Wu. Overcoming catastrophic for- getting in incremental few-shot learning by finding flat min- ima. Advances in neural information processing systems, 34: 6747–6761, 2021. 2, 6, 13 [34] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. Continual learning with deep generative replay. Advances in neural information processing systems, 30, 2017. 3 [35] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola Cascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar Panda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin- ual decomposed attention-based prompting for rehearsal-free continual learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 11909–11919, 2023. 3 [36] Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, Li Yuan, and Yonghong Tian. Learning with fantasy: Semantic-aware virtual contrastive constraint for few-shot class-incremental learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24183– 24192, 2023. 1, 6, 13 [37] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin Dong, Xing Wei, and Yihong Gong. Few-shot class- incremental learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 12183–12192, 2020. 1, 2, 6, 13 [38] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad Khan. Clip model is an efficient continual learner. arXiv preprint arXiv:2210.03114, 2022. 3, 6 [39] Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Xin Ning, and Prayag Tiwari. A survey on few-shot class-incremental learning. arXiv preprint arXiv:2304.08130, 2023. 1, 2 [40] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Ad- vances in neural information processing systems, 29, 2016. 12 [41] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per- ona, and Serge Belongie. The caltech-ucsd birds-200-2011 dataset. 2011. 6, 12 [42] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A comprehensive survey of continual learning: Theory, method and application. arXiv preprint arXiv:2302.00487, 2023. 1 [43] Runqi Wang, Xiaoyue Duan, Guoliang Kang, Jianzhuang Liu, Shaohui Lin, Songcen Xu, Jinhu L¨u, and Baochang Zhang. Attriclip: A non-incremental learner for incremental knowledge learning. In Proceedings of the IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, pages 3654–3663, 2023. 3, 14 [44] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts learning with pre-trained transformers: An occam’s razor for domain incremental learning. Advances in Neural Informa- tion Processing Systems, 35:5682–5695, 2022. [45] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun, Han Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin- cent Perot, Jennifer Dy, et al. Dualprompt: Complementary prompting for rehearsal-free continual learning. In European Conference on Computer Vision, pages 631–648. Springer, 2022. 14 [46] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang, Ruoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer Dy, and Tomas Pfister. Learning to prompt for continual learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 139–149, 2022. 3, 14 [47] Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang, and Tieniu Tan. Improving zero-shot generalization for clip 10 with synthesized prompts. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3032– 3042, 2023. 2, 4, 5, 6 [48] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer so- ciety conference on computer vision and pattern recognition, pages 3485–3492. IEEE, 2010. 6, 12 [49] Boyu Yang, Mingbao Lin, Binghao Liu, Mengying Fu, Chang Liu, Rongrong Ji, and Qixiang Ye. Learnable expansion-and-compression network for few-shot class- incremental learning. arXiv preprint arXiv:2104.02281, 2021. 2 [50] Boyu Yang, Mingbao Lin, Yunxiao Zhang, Binghao Liu, Xi- aodan Liang, Rongrong Ji, and Qixiang Ye. Dynamic sup- port network for few-shot class incremental learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(3):2945–2951, 2022. 2 [51] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao. Neural collapse inspired feature- classifier alignment for few-shot class-incremental learning. In The Eleventh International Conference on Learning Rep- resentations, 2022. 6, 7, 13 [52] Yang Yang, Zhiying Cui, Junjie Xu, Changhong Zhong, Wei- Shi Zheng, and Ruixuan Wang. Continual learning with bayesian model based on a fixed pre-trained feature extrac- tor. Visual Intelligence, 1(1):5, 2023. 3 [53] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip Torr, and Dacheng Tao. Neural collapse inspired feature- classifier alignment for few-shot class incremental learning. arXiv preprint arXiv:2302.03004, 2023. 2 [54] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, and Yinghui Xu. Few-shot incremental learning with contin- ually evolved classifiers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12455–12464, 2021. 2, 6, 13, 14 [55] Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu, and Xi Li. Mgsvf: Multi-grained slow vs. fast framework for few-shot class-incremental learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021. 2 [56] Linglan Zhao, Jing Lu, Yunlu Xu, Zhanzhan Cheng, Dashan Guo, Yi Niu, and Xiangzhong Fang. Few-shot class- incremental learning via class-aware bilateral distillation. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 11838–11847, 2023. 1, 2, 6, 7, 12, 13, 14 [57] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shil- iang Pu, and De-Chuan Zhan. Forward compatible few-shot class-incremental learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9046–9056, 2022. 2, 6, 13, 14 [58] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu, and De-Chuan Zhan. Few-shot class-incremental learning by sampling multi-phase tasks. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022. 2 [59] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De- Chuan Zhan, and Ziwei Liu. Deep class-incremental learn- ing: A survey. arXiv preprint arXiv:2302.03648, 2023. 1, 2, 14 [60] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. In- ternational Journal of Computer Vision, 130(9):2337–2348, 2022. 2, 3, 6 [61] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun Zha. Self-promoted prototype refinement for few-shot class- incremental learning. In Proceedings of the IEEE/CVF con- ference on computer vision and pattern recognition, pages 6801–6810, 2021. 2 [62] Huiping Zhuang, Zhenyu Weng, Run He, Zhiping Lin, and Ziqian Zeng. Gkeal: Gaussian kernel embedded analytic learning for few-shot class incremental task. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7746–7755, 2023. 2, 6, 13 [63] Yixiong Zou, Shanghang Zhang, Yuhua Li, and Ruixuan Li. Margin-based few-shot class-incremental learning with class-level overfitting mitigation. Advances in neural infor- mation processing systems, 35:27267–27279, 2022. 2 11 Learning Prompt with Distribution-Based Feature Replay for Few-Shot Class-Incremental Learning Appendix Contents The following items are included in the appendix: • Details of Selected Benchmarks in Sec. 6. • Quantitative Comparison with SOTAs on CUB-200 and CIFAR-100 in Sec. 7. • Comparison with Pre-trained Model-based Standard CIL Methods in Sec. 8. • Decomposing the Performance of the Base Class and the Incremental Class in Sec. 9. • Analysis of Various Shot Numbers in Sec. 10. 6. Details of Benchmarks Following the mainstream benchmark settings [56], we conduct our experiments on three datasets, i.e., CIFAR- 100 [25], mini-ImageNet [40] and CUB-200 [41], to evalu- ate our LP-DiF. • CIFAR-100 dataset consists of 100 classes, each of which contains 50, 000 training images. Following the previous study [56], there are 60 classes in the base session, and the remaining classes will be divided into 8 incremen- tal sessions, with each incremental session comprising 5 classes. • CUB-200 is a fine-grained classification dataset contain- ing 200 bird species with about 6, 000 training images. Following the previous study [56], there are 100 classes in the base session, and the remaining classes will be di- vided into 10 incremental sessions, with each incremental session comprising 10 classes. • mini-ImageNet is a smaller part of ImageNet [32], which has 50, 000 training images from 100 chosen classes. Fol- lowing the previous study [56], there are 60 classes in the base session, and the remaining classes will be divided into 8 incremental sessions, with each incremental ses- sion comprising 5 classes. Additionally, this paper also proposes two more chal- lenging benchmarks for FSCIL, i.e., SUN-397 [48] and CUB-200∗: • SUN-397 is a large-scale scene understanding dataset containing 397 distinct scene classes with about 76, 000 training images. We select 197 classes for the base ses- Table 5. Details of selected benchmarks. The first three lines are commonly used benchmarks, while the last two lines are the more challenging benchmarks proposed in this paper. |CAll|, |CBase| and |CInc| denotes the total number of classes, the number of classes in base session, and the number of classes in each incremental session respectively. #Base and #Inc denote the number of base sessions and the incremental session respectively. Shot denotes the number of training images of each incremental session. ∗ rep- resents a variant version. Dataset |CAll| |CBase| |CInc| #Base #Inc Shot CIFAR-100 [25] 100 60 5 1 8 5 mini-ImageNet [32] 100 60 5 1 8 5 CUB-200 [41] 200 100 10 1 10 5 SUN-397 [48] 397 197 10 1 20 5 CUB-200* [41] 200 0 10 0 20 5 sion; the remaining classes will be split into 20 incremen- tal sessions, with each incremental session comprising 10 classes. We evaluate our method on this benchmark to re- veal whether it is effective in scenarios with more classes and more incremental sessions. • CUB-200∗ is a variant of CUB-200 but excludes the base session. We evenly divide the total 200 classes into 20 in- cremental sessions, with each session containing 10 cate- gories. Following the previous study [56], there are 100 classes in the base session, and the remaining classes will be divided into 10 incremental sessions, with each incre- mental session comprising 10 classes. We use it to eval- uate whether our method works in scenarios without the base session. For the above five benchmarks, the classes in the base ses- sion have a large amount of training data, while each class in every incremental session has only 5 training images. Tab. 5 summarizes the details of each selected benchmark. 7. Quantitative comparison with SOTAs on CUB-200 and CIFAR-100 We have shown the accuracy comparison curves with the state-of-the-art methods on CIFAR-100 and CUB-200 in Fig. 4 of our main paper. Here, we present details compar- ison results with specific values on these two benchmarks, shown in Tab. 6 and Tab. 7. Overall, the performance of our LP-DiF can be summarized in three points. 1) There are significant improvements compared to CLIP (baseline) in terms of Avg (i.e., 15.91% and 4.26% improvements on 12 Table 6. Comparison with state-of-the-art FSCIL methods on CUB-200, where the rows filled with gray indicate the existing SOTA FSCIL methods. “Avg” represents the average accuracy of all sessions; the higher the value, the better performance. “PD” represents the Performance Drop rate; the lower the value, the better performance. Methods Accuracy in each session (%) ↑ Avg ↑ PD ↓ 0 1 2 3 4 5 6 7 8 9 10 TOPIC [37] 68.68 62.49 54.81 49.99 45.25 41.40 38.35 35.36 32.22 28.31 26.26 43.92 42.42 CEC [54] 75.85 71.94 68.50 63.50 62.43 58.27 57.73 55.81 54.83 53.52 52.28 61.33 23.57 Replay [27] 75.90 72.14 68.64 63.76 62.58 59.11 57.82 55.89 54.92 53.58 52.39 61.52 23.51 MetaFSCIL [9] 75.90 72.41 68.78 64.78 62.96 59.99 58.30 56.85 54.78 53.82 52.64 61.93 23.26 FACT [57] 75.90 73.23 70.84 66.13 65.56 62.15 61.74 59.83 58.41 57.89 56.94 64.42 18.96 FCIL [15] 78.70 75.12 70.10 66.26 66.51 64.01 62.69 61.00 60.36 59.45 58.48 65.70 20.22 GKEAL [62] 78.88 75.62 72.32 68.62 67.23 64.26 62.98 61.89 60.20 59.21 58.67 66.35 20.21 NC-FSCIL [51] 80.45 75.98 72.30 70.28 68.17 65.16 64.43 63.25 60.66 60.01 59.44 67.28 21.01 BiDistFSCIL [56] 79.12 75.37 72.80 69.05 67.53 65.12 64.00 63.51 61.87 61.47 60.93 67.34 18.19 SAVC [36] 81.85 77.92 74.95 70.21 69.96 67.02 66.16 65.30 63.84 63.15 62.50 69.35 19.35 F2M [33] 81.07 78.16 75.57 72.89 70.86 68.17 67.01 65.26 63.36 61.76 60.26 69.49 20.81 CLIP (Baseline) [29] 65.54 62.91 61.54 57.75 57.88 57.89 56.62 55.40 54.20 54.23 55.06 58.09 10.48 LP-DiF (Ours) 83.94 80.59 79.17 74.30 73.89 73.44 71.60 70.81 69.08 68.74 68.53 74.00 15.41 Joint-LP (Upper bound) 83.94 80.83 79.43 77.06 76.35 74.89 73.66 72.79 71.84 72.06 71.88 75.88 12.06 Table 7. Comparison with state-of-the-art FSCIL methods on CIFAR-100, where the rows filled with gray indicate the existing SOTA FSCIL methods. “Avg” represents the average accuracy of all sessions; the higher the value, the better performance. “PD” represents the Performance Drop rate; the lower the value, the better performance. Methods Accuracy in each session (%) ↑ Avg ↑ PD ↓ 0 1 2 3 4 5 6 7 8 TOPIC [37] 64.10 55.88 47.07 45.16 40.11 36.38 33.96 31.55 29.37 42.62 34.73 F2M [33] 64.71 62.05 59.01 55.58 52.55 49.96 48.08 46.28 44.67 53.65 20.04 CEC [54] 73.07 68.88 65.26 61.19 58.09 55.57 53.22 51.34 49.14 59.53 23.93 Replay [27] 74.40 70.20 66.54 62.51 59.71 56.58 54.52 52.39 50.14 60.78 24.26 MetaFSCIL [9] 74.50 70.10 66.84 62.77 59.48 56.52 54.36 52.56 49.97 60.79 24.53 GKEAL [62] 74.01 70.45 67.01 63.08 60.01 57.30 55.50 53.39 51.40 61.35 22.61 C-FSCIL [18] 77.47 72.40 67.47 63.25 59.84 56.95 54.42 52.47 50.47 61.64 27.00 FCIL [15] 77.12 72.42 68.31 64.47 61.18 58.17 56.06 54.19 52.02 62.66 25.10 FACT [57] 78.22 72.40 68.57 64.73 61.40 58.57 56.30 53.83 51.72 62.86 26.50 SAVC [36] 78.77 73.31 69.31 64.93 61.70 59.25 57.13 55.19 53.12 63.63 25.65 BiDistFSCIL [56] 79.45 75.38 71.84 67.95 64.96 61.95 60.16 57.67 55.88 66.14 23.57 NC-FSCIL [51] 82.52 76.82 73.34 69.68 66.19 62.85 60.96 59.02 56.11 67.50 26.41 CLIP (Baseline) [29] 74.44 72.96 72.21 70.49 70.18 70.00 69.81 69.23 68.37 70.86 6.07 LP-DiF (Ours) 80.23 77.75 76.78 74.62 74.03 73.87 73.84 72.96 72.02 75.12 8.21 Joint-LP (Upper bound) 80.23 79.85 78.63 76.13 75.31 74.67 74.24 73.58 73.35 76.22 6.88 CUB-200 and CIFAR-100 respectively). 2) Compared to existing SOTA methods, LP-DiF achieves a higher Avg and lower PD. 3) Both Avg and PD of LP-DiF are very close to that of Joint-LP (upper bound), i.e., lower only 1.88% Avg and 3.35% PD on CUB-200, 1.10% Avg and 0.81% PD on CIFAR-100. These results clearly illustrate the ef- fectiveness of our LP-DiF. 13 Table 8. Comparison with standard CIL methods based on pre- trained models on the three common benchmarks in terms of Avg. ‡ indicates our reproduction. Methods CIFAR-100 mini-ImageNet CUB-200 BiDistFSCIL [56] 66.14 62.04 67.34 L2P‡ [46] 61.77 75.68 56.95 DualPrompt‡ [45] 63.50 76.61 62.32 AttriCLIP‡ [43] 59.24 81.74 47.81 LP-DiF (Ours) 75.12 93.76 74.00 Accuracy (%) 71.1 33.9 45.9 73.9 40.5 52.3 72.7 49.5 58.9 77.4 59.4 67.2 Base Class Incremental Class Harmonic Mean Figure 7. Decomposing the performance of the base class and the incremental class. The performance is evaluated by the model from the last session on CUB-200. 8. Comparison with Pre-trained Models-based Standard CIL Methods To further demonstrate the superiority of our method, we compare it with several recent standard CIL [59] meth- ods and variant it to pre-trained model-based methods: L2P [46], DualPrompt [45] and AttriCLIP [43]. We repro- duce these three approaches on CIFAR-100, CUB-200, and mini-ImageNet and evaluate them under FSCIL protocol re- spectively. As shown in Tab. 8, our LP-DiF outperforms these methods by a large margin in terms of Avg across all three benchmarks. We also find that these methods based on pre-trained models underperform BiDistFSCIL [56] on CIFAR-100 and CUB-200. This indicates that these meth- ods are not advantageous for the FSCIL setting, further un- derscoring the effectiveness and significance of our method. 9. Decomposing the Performance of the Base Class and the Incremental Class Following previous studies [54, 56, 57], in this section, we decompose the accuracy, respectively analyzing the ef- fectiveness of our LP-DiF for the classes in the base session (i.e., base class) and for the classes in incremental sessions (a) 2-Shot (b) 5-Shot (c) 10-Shot (d) 15-Shot Figure 8. Comparison with BiDistFSCIL (SOTA FSCIL method) and Joint-LP (Upper bound) under various shot numbers of in- cremental classes on CUB-200. (i.e., incremental class), to evaluate if our method performs well on both base and incremental classes. We report the comparison results in terms of individual accuracy of base and novel classes, as well as their harmonic mean, in the last session on CUB-200. Fig. 7 shows that our LP-DiF outper- forms the second best method on base class (i.e., FACT) by 3.5%, while outperforms the second best method on in- cremental class (i.e., BiDistFSCIL) by 9.9%. Finally, the superior harmonic mean demonstrates our achievement of an enhanced balance between base and novel classes. 10. Analysis Various Shot Numbers In the main paper, we have compared the performance of our LP-DiF with SOTA methods and the Joint-LP (up- per bound) under 5-shot for each incremental class. To fur- ther demonstrate the superiority of our approach, we con- ducted experiments under various shot numbers of incre- mental classes. Fig. 8 show the comparison results with BiDistFSCIL [56] and Joint-LP on CUB-200 under (a) 2- shot, (b) 5-shot (default in main paper), (c) 10-shot and (d) 15-shot. Obviously, across all the shot number settings, our LP-DiF consistently outperforms BiDistFSCIL signifi- cantly, and its performance is very close to the upper bound. This result demonstrates that, regardless of the shot num- bers of incremental classes, our LP-DiF presents satisfac- tory performance and the ability to resist catastrophic for- getting. 14 "
}