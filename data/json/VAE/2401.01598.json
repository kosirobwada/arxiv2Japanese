{
    "optim": "Learning Prompt with Distribution-Based Feature Replay\nfor Few-Shot Class-Incremental Learning\nZitong Huang1\nZe Chen2\nZhixing Chen1\nErjin Zhou2\nXinxing Xu3\nRick Siow Mong Goh3\nYong Liu3\nChunmei Feng3Q\nWangmeng Zuo1Q\n1Harbin Institute of Technology\n2MEGVII Technology\n3IHPC, A*STAR\nzitonghuang99@gmail.com, chenze@megvii.com, robotic.zxchen@gmail.com,\nzej@megvii.com, fengcm.ai@gmail.com, wmzuo@hit.edu.cn\nAbstract\nFew-shot Class-Incremental Learning (FSCIL) aims to\ncontinuously learn new classes based on very limited\ntraining data without forgetting the old ones encountered.\nExisting studies solely relied on pure visual networks,\nwhile in this paper we solved FSCIL by leveraging the\nVision-Language model (e.g., CLIP) and propose a sim-\nple yet effective framework, named Learning Prompt with\nDistribution-based Feature Replay (LP-DiF). We observe\nthat simply using CLIP for zero-shot evaluation can sub-\nstantially outperform the most influential methods. Then,\nprompt tuning technique is involved to further improve its\nadaptation ability, allowing the model to continually cap-\nture specific knowledge from each session.\nTo prevent\nthe learnable prompt from forgetting old knowledge in the\nnew session, we propose a pseudo-feature replay approach.\nSpecifically, we preserve the old knowledge of each class\nby maintaining a feature-level Gaussian distribution with\na diagonal covariance matrix, which is estimated by the\nimage features of training images and synthesized features\ngenerated from a VAE. When progressing to a new ses-\nsion, pseudo-features are sampled from old-class distribu-\ntions combined with training images of the current session\nto optimize the prompt, thus enabling the model to learn\nnew knowledge while retaining old knowledge.\nExperi-\nments on three prevalent benchmarks, i.e., CIFAR100, mini-\nImageNet, CUB-200, and two more challenging bench-\nmarks, i.e. SUN-397 and CUB-200∗ proposed in this paper\nshowcase the superiority of LP-DiF, achieving new state-\nof-the-art (SOTA) in FSCIL. Code is publicly available at\nhttps://github.com/1170300714/LP-DiF.\n1. Introduction\nClass-Incremental Learning (CIL) [10, 42, 59] faces\nchallenges in data-scarce real-world applications, e.g., face\nAverage Accuracy (%)\nTOPIC [37]\nCEC [53]\nF2M [33]\nFACT [56]\nGKEAL [61]\nBDF [55]\nCLIP [29]\nCLIP+BDF \nOurs \nJoint-LP\n(UB) \n40\n50\n60\n70\n80\n90\n100\n39.64\n57.75\n57.89\n59.88\n60.47\n94.81\n93.76\n77.57\n84.63\n61.42\nFSCIL with pure vision model\nFSCIL with V-L model\nOurs\nJoint training (UB)\nFigure 1. Comparison of FSCIL methods in terms of Average Ac-\ncuracy (%) on the test set of mini-ImageNet benchmark [32]\nunder 5-shot setting. Red-highlighted bars indicate SOTA vision-\nbased models (e.g., CNN [17]), while orange highlights show\nV-L pretrained models enhancing FSCIL, significantly outper-\nforming those vision-based counterparts. Our method, marked in\ngreen, achieves 93.76%, surpassing CLIP+BDF by 9.13%, and\ncomparable to the theoretical upper bound (UB) that highlights in\nblue achieved through learning prompt in joint-training manner.\nrecognition systems [56] and smart photo albums [37]. This\nhas led to the emergence of Few-Shot CIL (FSCIL) [39],\nwhere models adapt to new classes with limited training\ndata, showcasing their relevance and flexibility in data-\nscarce scenarios.\nIn FSCIL, with only a few samples for each incremen-\ntal task, the main challenge is not just avoiding catas-\ntrophic forgetting of previous knowledge [36, 37, 39] but\n1\narXiv:2401.01598v1  [cs.CV]  3 Jan 2024\nalso facilitating plasticity from limited data. Existing stud-\nies usually address this by first pre-training a classifier\non a base set with numerous images for a robust founda-\ntion [21, 33, 37, 54, 56–58, 61]. Subsequent adaptations,\ne.g., knowledge distillation [56], class relationship model-\ning [37, 54], and specific optimization [33], are then applied\nto the sparse incremental session data to boost performance\nwhile maintaining previously acquired knowledge.\nThis work diverges from approaches that solely rely on\nvisual networks [17], opting instead to leverage the capa-\nbilities of a Vision-Language (V-L) pretrained model, i.e.,\nCLIP [29, 60], to develop a few-shot incremental learner.\nComparing with existing state-of-the-art techniques (see the\nRed-highlighted bars in Fig. 1), we observed that by simply\ncrafting the manual prompt “A photo of a [CLS]” as\ntextual input and performing zero-shot evaluation on the\nwidely used FSCIL benchmark, mini-ImageNet [32] test\nset, CLIP (refer to the orange CLIP bar in Fig. 1) substan-\ntially outperforms all these SOTA methods, with a notable\n16.15% performance boost over BiDistFSCIL (BDF) [56].\nThis finding indicates that the generalization abilities of V-\nL pretrained models are highly beneficial for FSCIL, e.g.,\nnaturally mitigating the plasticity issues caused by limited\ntraining samples. Further, from Fig. 1, simply replacing the\nexisting backbones of current SOTA methods with a pre-\ntrained image encoder, initializing and learning the classi-\nfier with the corresponding text encoding of manual prompt\ncan further enhance performance (7.16% gain to CLIP) but\nstill lag behind the UB (9.13% lower than the UB) . There-\nfore, how to derive an efficient and lightweight prompt for\nFSCIL continues to be a compelling challenge.\nBased on the above preliminary results, this paper pro-\nposes a simple yet effective FSCIL framework by learning\na lightweight prompt built upon the V-L pre-trained models.\nUnlike CLIP, as well as simply integrating CLIP with exist-\ning methods (refer to the orange bar in Fig. 1, we resort to\nimproving prompt tuning [60] for meeting the requirements\nof FSCIL. Specifically, for session t, we take the prompt in\nsession t − 1 for initialization, combine it with [CLS] to\ncreate the full-text input for each class, and then optimize\nlearnable prompt with training data.\nTo prevent the learnable prompt from forgetting prior\nknowledge in a new session, we also propose a pseudo-\nfeature replay technique. Specifically, observing that the\nimage features extracted by the image encoder of CILP for\neach class seem to follow a Gaussian distribution (refer to\nFig. 3), we attempt to estimate its mean vector and diag-\nonal covariance matrix (i.e. parameters of Gaussian distri-\nbution) to fit the training data of each class. To this end, a\nVAE [24, 47] comprised of the V-L model and lightweight\nMLPs are proposed to synthesize features based on the few\ntraining samples and text information, permitting the us-\nage of real image features as well as synthesized features to\nestimate Gaussian distribution parameters more accurately.\nWhen the model trains on a new session, pseudo-image fea-\ntures from the old-class distributions are sampled as old-\nknowledge replay to constrain the optimization direction\nof the prompt, avoiding learning towards catastrophic for-\ngetting. The results in Fig. 1 showcase that our approach\nimproves zero-shot evaluation for CLIP by 16.19% and\nfor CLIP+BDF by 9.13%. Notably, our method is merely\n1.05% lower than the upper bound (Joint-LP, i.e., learning\nprompt on training data of each session jointly).\nIn a nutshell, the main contributions of this paper are\nsummarized as follows:\n1) We empirically show that pretrained V-L models, e.g.\nCLIP, are beneficial for FSCIL due to its considerable\ngeneralization ability, inspiring us to propose a simple yet\neffective V-L based FSCIL method named LP-DiF.\n2) We adopt prompt tuning for allowing the model to con-\ntinually capture specific knowledge of each session, and\npresent a feature replay technique to prevent catastrophic\nforgetting. By constructing feature-level Gaussian distri-\nbution for each class, pseudo feature replay can be com-\nbined with training images of current session to learn new\nknowledge while retaining old knowledge.\n3) Extensive evaluations and comparisons on three prevalent\nFSCIL benchmarks (CIFAR-100, CUB-200 and mini-\nImageNet) and two proposed more challenging bench-\nmarks (SUN-397 and CUB-200∗) show the superiority of\nour methods in comparison to state-of-the-arts.\n2. Related Work\nFew-Shot Class-Incremental Learning.\nThe few-shot\nclass-incremental learning methods (FSCIL) aims to train\na model in a class-incremental manner [10, 59] with only\na few samples for each new tasks [39].\nExisting stud-\nies can be categorized into four families, i.e., dynamic\nnetwork-based methods, meta-learning-based methods, fea-\nture space-based methods, and replay-based methods. In\nspecific, dynamic network structure [15, 37, 49, 50] is\nproposed to adaptive learn the new knowledge.\nMeta\nlearning-based methods [9, 18, 28, 53, 61, 63] employ a\nsession sampling scheme, simulating the incremental learn-\ning process during evaluation. feature space-based meth-\nods [2, 8, 23, 55, 57, 58, 62], focus on mapping the original\nimage into a condensed feature space while preserving its\nessential attributes. Replay-based methods [7, 11, 26] retain\nor produce significant data from prior tasks to be reintro-\nduced in the ongoing task. While these methods have shown\ncommendable performance, all those studies are based on\nfeature extractors and classifiers built from deep networks\ntrained in the base session. Due to the scarcity of incre-\nmental class samples, the feature representation ability is\nlimited. In contrast, we propose to construct an incremental\nlearner on a VL pre-trained model [29, 60] that offers in-\n2\nherent merits for FSCIL, i.e., endowing the image encoder\nwith powerful feature representation abilities.\nReplay-based Incremental Learning. The replay-based\napproach in incremental learning leverages knowledge from\nprevious tasks to mitigate catastrophic forgetting in mod-\nels [1, 3–6, 16, 19, 20, 31, 34]. A basic data replay ap-\nproach involves retaining a concise exemplar set, capturing\nessential samples from prior tasks [4, 5], then, the classifica-\ntion model is trained on the combination of exemplars and\nthe data of the current task. Different from directly storing\nthe real instances, several following works [16, 19, 22, 34]\nleveraged a generative model [14, 24] for generating data\nfrom previous tasks. Compared to methods based on real\nimage replay, pseudo replay reduces storage needs by elim-\ninating the requirement for exemplars and enriches the di-\nversity of samples from previous tasks. Yet, the overhead\nof training the image generator and dynamically produc-\ning pseudo images introduces additional computational de-\nmands and prolongs training time. Instead of retaining an\nimage generator, we represent the feature representation for\neach class using a Gaussian distribution, utilizing it to sam-\nple pseudo-features for rehearsing prior knowledge. More-\nover, drawing samples from this distribution is computa-\ntionally efficient, offering our method an effective way for\nhandling prevent catastrophic forgetting.\nIncremental Learning via Pre-trained Model.\nRecent\nstudies have explored constructing incremental learners us-\ning pre-trained models [13, 35, 38, 43–46, 52]. The core\nidea of these methods is to leverage a pre-trained backbone,\ne.g., ViT [12], for robust image feature extraction, while\nonly fine-tuning a selected set of parameters to adapt to new\ntasks. For example, L2P [46] employs a fixed pre-trained\nViT as its backbone and sustains a dynamic prompt pool\nwith various sets of adaptable prompts. Some following\nworks [35, 43] built upon this concept, applying it to the\nVL pretrained model [29], leveraging linguistic knowledge\nto bolster classification performance. The above studies un-\nderscore the significant advantages of using pretrained mod-\nels to boost performance in standard CIL scenarios. As for\nFSCIL, we inherit the advantages of pretrained models in\nCIL, and further maintain a Gaussian distribution at the fea-\nture level for each class to mitigate catastrophic forgetting.\n3. Proposed Method\nProblem Formulation. The purpose of FSCIL is to con-\ntinually learn knowledge of new classes from few samples,\nwhile simultaneously preventing the model from forgetting\nknowledge of old classes. Formally, a model is trained by\na sequence of training data DTrain = {D(t)\nTrain}T\nt=0 contin-\nually, where D(t)\nTrain = {(xi, yi)}N (t)\ni=0 denotes the training\nset of session (task) t. xi is a training image with corre-\nsponding class label yi ∈ C(t), where C(t) denotes the class\nspace of D(t)\nTrain. For different sessions, the class spaces are\nnon-overlapping, i.e. ∀t1, t2 ∈ {0, 1, . . . , T} and t1 ̸= t2,\nC(t1) ∩ C(t2) = ∅. Typically, D(0)\nTrain of the first session\n(i.e. t = 0), which is usually referred to as the base ses-\nsion, contains a substantial amount of training data. While\nD(t)\nTrain(t > 0) of the incremental sessions only contains\nfew training sample, organized as the N-Way K-shot for-\nmat, i.e., N classes in each incremental session with each\nclass comprising K training images. Following the formu-\nlation of standard class-incremental learning, in session t,\nonly D(t)\nTrain and an optional memory buffer used to store the\nold knowledge (e.g. exemplar) can be accessed. After fin-\nishing training on D(t)\nTrain, the model is evaluated on a test\nset D(t)\nTest, the class space of which is union of all the classes\nencountered so far, i.e. C(0) ∪ C(1) · · · ∪ C(t).\nIn this section, we propose a FSCIL method based on the\nV-L pretrained model, e.g. CLIP [29]. We assume that the\nclass names are accessible during the training and testing\nof each session. Formally, CLIP contains an image encoder\nEImg(x) and a text encoder ETxt(p), which are pretrained\njointly with a huge amount of image-text pairs in contrastive\nlearning manner. An image x is fed into the image encoder,\nobtaining the corresponding L2-normalized feature f.\np\nis a text token which is obtained by tokenizing a sentence\nlike “A photo of a [CLS].”, where [CLS] represents\na certain class name. We replace [CLS] by each class name\nrespectively and obtain a set of text tokens {pc}C\nc=1, where\nC denotes the total number of classes encountered so far.\nThen, {pc}C\nc=1 are fed into the text encoder, obtaining the\ncorresponding L2-normalized text feature {gc}C\nc=1. Finally,\nthe prediction score of class c is computed by:\np(y = c|x) =\nexp(⟨f, gc⟩/τ)\nPC\nj=1 exp(⟨f, gj⟩/τ)\n,\n(1)\nwhere ⟨·, ·⟩ denotes the cosine similarity of the two features\nand τ is the temperature parameter.\n3.1. Approach Overview\nAlthough CLIP has demonstrated its superior perfor-\nmance on FSCIL in Fig. 1, using hand-crafted prompt is\nsub-optimal for transfer the knowledge to each incremen-\ntal session. So we replace the hand-crafted prompt with a\nset of learnable vectors V = {[V]l}L\nl=1 [60], where [V]l\n(l ∈ {1, . . . , L}) denotes one learnable vector, and L is\nthe number of vectors. Hence, the expression for the text\nprompt is modified to:\np(V) = [V]1[V]2 . . . [V]L[CLS],\n(2)\nTo learn V on DTrain, an intuitive approach is to sequentially\ntune the prompt using training data from each incremen-\ntal session to continually acquire new knowledge. Specif-\n3\nText \nEncoder\nImage \nEncoder\n𝑧 = ෥𝝈 ∙ 𝒩 𝟎, 𝐈 + ෥𝝁\nVAE   Encoder\nMLP\nMLP\nBias\n[CLS]\n+\nℒr\nVAE  Decoder\n(෥𝝁, ෥𝝈𝟐)\nℒKL\n(a) Training VAE\nImage \nEncoder\n[CLS]\n𝐱 (𝑡−1)\n…\n…\n𝐳~𝒩 𝟎, 𝐈\nNoises\n𝒩(𝝁𝑐, 𝝈𝑐2)\nVAE\nDecoder\nEstimate \nDistribution\ndog\ncar\nbird\nchairbanana\ncow human\nhorse\nOld-class Knowledge\nSave to\n(b) Preserving Old-class Knowledge \nImage Feature\nText Feature\nFrozen\nTrainable\nSampled Feature\nGaussian Distribution\nRandomly Sample a \nPseudo Feature\nℒ𝑛\nℒ𝑜\nSupervise\nNew-class Loss\nOld-class Loss\napple New Class\ncar,dog Old Class\nSynthesized Feature\n(c) Prompt Tuning with Replaying on Feature Distribution\n[𝐕VAE]\n𝑁𝑐\n𝑀\n𝑀\nImage \nEncoder\n[CLS]\n𝐕 1\n𝐕 𝐿\n…\nPrompts\ncar，dog, cat\n… apple\n…\n𝐱 (𝑡)\n𝐟 \n𝐠car 𝐠dog 𝐠cat\n𝐠apple\n𝒩(𝝁car, 𝝈car\n2 )\nText \nEncoder\n𝒩(𝝁dog,𝝈dog\n2\n)\n…\n…\n…\nℒn\nℒo\nOld-class Knowledge\nSimilarity Scores\ndog\ncar\nbird\nchairbanana\ncow human\nhorse\n𝑦 (𝑡)\n{𝑐car, 𝑐dog}\nRandomly Select \nB Old Classes\nB=2\nመ𝐟car\nመ𝐟dog\nFigure 2. Overview of our proposed LP-DiF. (a) In each session, we first train a VAE [24, 47] comprised of the V-L model and lightweight\ncomponents, i.e., MLPs and learnable prompt, based on few training data and textual information of this session. (b) We preserve the\nknowledge of each class by estimating their feature-level statistical distribution. The mean vector and diagonal covariance matrix of the\ndistribution are estimated by both the features of real images and the synthesized features from trained VAE. (c) Prompt is trained jointly\nwith the combination of the real image of the current session and the pseudo-features sampled from old-class distributions.\n…\n…\n𝑐1:\n𝑐2:\ndim 1\ndim 2\ndim D\nValue\nDensity\nValue\nDensity\nValue\nDensity\nValue\nDensity\nDensity\nValue\nDensity\nValue\nFigure 3. Histogram visualization of the statistical distribution\nof image features. We take the image features with different di-\nmensions (dim) of classes c1 and c2 as example selected from\nthe mini-ImageNet [32] benchmark by the image encoder of CLIP\n(ViT-B/16) [29]. Each sub-figure shows the distribution with his-\ntogram of corresponding random variable Zcd, where c and d de-\nnotes the index of class and feature dimension respectively. Obvi-\nously, 1) each dimension of the image features per class approx-\nimates Gaussian distribution; 2) distributions of same dimension\nvary in different classes, e.g., Zc11 vs. Zc21 .\nically, at the beginning of session 0, we initialize V ran-\ndomly; while for each following session t (t > 0), we use\nthe V trained in the previous session (e.g. session t − 1) to\ninitialize the V for current session. In a certain session t,\ngiven a pair of training sample (xi, yi) from D(t)\nTrain, prompt\nis optimized on by minimizing Ln:\nLn = − log\nexp(⟨fi, ETxt(pyi(V))⟩/τ)\nP|\nSt\ns=0 C(s)|\nc=1\nexp(⟨fi, ETxt(pc(V))⟩/τ)\n,\n(3)\nwhere fi denotes the L2-normalized image feature of xi,\nand pc(V) denotes the prompt corresponding to class c.\nHowever, using only the D(t)\nTrain to optimize the prompt in\nsession t will inevitably lead to catastrophic forgetting. Ide-\nally, learning prompt with all training data from both pre-\nvious and current sessions (e.g. St\ns=0 D(s)\nTrain) can address\nthis issue, but this is not allowed under the protocol of FS-\nCIL. Therefore, this paper adopts a compromise solution,\nproposing to record old knowledge by maintaining statis-\ntical distributions of old classes instead of directly storing\norigin images. We setup a feature-level Gaussian distribu-\ntion to represent each old class, which is represented by a\nmean vector and a diagonal covariance matrix. We name\nit the old-class distribution. The mean vector and diago-\nnal covariance matrix of the old-class distribution are es-\ntimated jointly from the features of real images as well\nas synthetic features generated by a VAE decoder. When\nlearning the prompt in a new session, we randomly sample\n4\nfeatures based on the statistical distribution of old classes\nto replay old knowledge. Then, the sampled features of old\nclasses and the real features of new classes will jointly op-\ntimize the prompt, thereby learning new knowledge while\nalso replaying old knowledge. In the following, We will in-\ntroduce how to obtain the old-class distribution in Sec 3.2,\nand how to learn prompt in Sec 3.3.\n3.2. Estimation of Old-Class Distribution\nIn each session t, we should estimate the feature-level\nstatistical distribution for each class of D(t)\nTrain.\nGiven a\ncertain class label c ∈ C(t) , the corresponding training\nimages {xi}Nc\ni=1 are fed into the image encoder EImg(x)\nto obtain their L2-normalized features {fi}Nc\ni=1, where Nc\ndenotes the number of training images of class c, fi =\n[fi1, fi2, . . . , fiD]T and D is the feature dimension (e.g.\nD = 512 for ViT-B/16).\nIntuitively, we assume that\nthe features of class c follow a multivariate distribution\nN(µc, Σc), where µc ∈ RD denotes the mean vector and\nΣc ∈ RD×D\n≥0\ndenotes the covariance matrix. As shown in\nFig. 3, we observe that each dimension of these features of\neach class approximates a Gaussian distribution, and distri-\nbutions of same dimension vary in different classes. Thus,\neach dimension of the feature can be treated as indepen-\ndently distributed, and the covariance matrix Σc can be sim-\nplified to a diagonal matrix and be represented by a vector\nσ2\nc = [σ2\nc1, σ2\nc2, . . . , σ2\ncD]T , which is diagonal values of Σc.\nWe use random variable Zcd to represent the d-th dimen-\nsion of feature, following a specific Gaussian distribution\nN(µcd, σ2\ncd), where µcd denotes the mean value of the d-th\ndimension. Then, Zc = [Zc1, Zc2, . . . , ZcD] represents the\nrandom variable of the whole feature following N(µc, σ2\nc).\nOur goal is to estimate the µc and σ2\nc for each class.\nFor each class, simply using only {fi}Nc\ni=1 to estimate the\nµc and σ2\nc may be inadequate due to the scarcity of the data.\nTo tackle with this problem, we utilize a VAE [24, 47] com-\nprised of the V-L models and lightweight MLPs, leveraging\nthe few training data and textual information to synthesize\nmore image features, thereby benefiting the estimation of\nthe distribution. As shown in Fig. 2 (a), in VAE Encoder, an\nimage feature f is fed into a MLP, encoded to a latent code\nz, of which distribution is assumed to be a prior N(0, I):\nLKL = KL(N(˜µ, ˜σ2)||N(0, I)),\n(4)\nwhere KL represents the Kullback-Leibler divergence. In\nVAE Decoder, z is fed to another MLP and obtain the bias\nr, which is added to a set of learnable prompt VVAE =\n{[VVAE]l}L\nl=1:\nVVAE(z) = {[VVAE]l + r}L\nl=1,\n(5)\nThen, VVAE(z) concatenating with the class name [CLS]\ncorresponding to f is fed into the text encoder, obtaining the\nreconstruct feature ˜f then calculating the reconstruct loss\nLr:\nLr = ∥f − ˜f∥2.\n(6)\nFinally, the total loss LVAE of training the VAE is:\nLVAE = LKL + λrLr,\n(7)\nwhere λr represents the coefficient of Lr.\nUsing both the features synthesized by the VAE and the\nreal image features, we estimate µc and σ2\nc.\nAs shown\nin Fig. 2 (b), for a specific class c, M noise vectors z ∼\nN(0, I) and corresponding class name are input into the\nVAE Decoder, obtaining M synthesized features {˜fj}M\nj=1.\nThen, µc and σ2\nc = [σ2\nc1, σ2\nc2, . . . , σ2\ncD]T are estimated by:\nµc =\n1\nNc + M (\nNc\nX\ni=1\nfi +\nM\nX\nj=1\n˜fj),\n(8)\nσ2\ncd =\n1\n(Nc + M) − 1(\nNc\nX\ni=1\n(fid−µcd)2+\nM\nX\nj=1\n( ˜fjd−µcd)2). (9)\n3.3. Learning Prompt with Feature Replay\nAt session t, we learn prompt with D(t)\nTrain as well as the\ndistributions of old classes preserved in previous sessions.\n1) When t = 0, i.e., the first session, we just follow the\napproach in Sec.3.1, randomly initializing V and learning\nthem with D(0)\nTrain by Ln (Eq. (3)).\n2) When t > 0, V are initialized from trained weights in ses-\nsion t−1. For the new knowledge of D(t)\nTrain, we adopt Ln.\nFor the old knowledge of previous sessions, we randomly\nsample pseudo image features of old classes from their\ncorresponding distributions. As shown in Fig.2 (c), for\neach selected training image in one batch xi, we first ran-\ndomly select B old classes: {cb}B\nb=1 and cb ∈ ∪t−1\ns\nC(s).\nThen, for each selected class cb, we randomly sample\na pseudo feature ˆfcb from its feature distribution ˆfcb ∼\nN(µcb, σ2\ncb). These sampled features with their corre-\nsponding class labels are used to calculate the loss Lo:\nLo = −\nB\nX\nb=1\nlog\nexp(⟨ˆfcb, ETxt(pcb(V))⟩/τ)\nP|\nSt\ns=0 C(s)|\nc=1\nexp(⟨ˆfcb, ETxt(pc(V))⟩/τ)\n.\n(10)\nFinally, the prompt is optimized by minimizing the loss:\nLLP =\n(\nLn\nif t = 0,\nLn + λoLo\nif t > 0,\n(11)\nwhere λo represents the tradeoff coefficient.\n5\nTable 1. Comparison on mini-ImageNet, where the rows filled with gray indicate the existing SOTA FSCIL methods. “Avg” represents\nthe average accuracy of all sessions; the higher the value, the better performance. “PD” represents the Performance Drop rate; the lower\nthe value, the better performance.\nMethods\nAccuracy in each session (%) ↑\nAvg ↑\nPD ↓\n0\n1\n2\n3\n4\n5\n6\n7\n8\nTOPIC [37]\n61.31\n50.09\n45.17\n41.16\n37.48\n35.52\n32.19\n29.46\n24.42\n39.64\n36.89\nCEC [54]\n72.00\n66.83\n62.97\n59.43\n56.70\n53.73\n51.19\n49.24\n47.63\n57.75\n24.37\nF2M [33]\n72.05\n67.47\n63.16\n59.70\n56.71\n53.77\n51.11\n49.21\n47.84\n57.89\n24.21\nReplay [27]\n71.84\n67.12\n63.21\n59.77\n57.01\n53.95\n51.55\n49.52\n48.21\n58.02\n23.63\nMetaFSCIL [9]\n72.04\n67.94\n63.77\n60.29\n57.58\n55.16\n52.90\n50.79\n49.19\n58.85\n22.85\nGKEAL [62]\n73.59\n68.90\n65.33\n62.29\n59.39\n56.70\n54.20\n52.59\n51.31\n60.48\n22.28\nFACT [57]\n72.56\n69.63\n66.38\n62.77\n60.60\n57.33\n54.34\n52.16\n50.49\n60.70\n22.07\nC-FSCIL [18]\n76.40\n71.14\n66.46\n63.29\n60.42\n57.46\n54.78\n53.11\n51.41\n61.59\n14.99\nBiDistFSCIL [56]\n74.65\n70.70\n66.81\n63.63\n61.36\n58.14\n55.59\n54.23\n53.39\n62.06\n21.26\nFCIL [15]\n76.34\n71.40\n67.10\n64.08\n61.30\n58.51\n55.72\n54.08\n52.76\n62.37\n23.58\nSAVC [36]\n81.12\n76.14\n72.43\n68.92\n66.48\n62.95\n59.92\n58.39\n57.11\n67.05\n24.01\nNC-FSCIL [51]\n84.02\n76.80\n72.00\n67.83\n66.35\n64.04\n61.46\n59.54\n58.31\n67.82\n25.71\nCLIP (Baseline) [29]\n80.01\n79.16\n78.89\n77.97\n77.44\n76.83\n76.32\n76.02\n75.45\n77.57\n4.56\nLP-DiF (Ours)\n96.34\n96.14\n94.62\n94.37\n94.06\n93.44\n92.21\n92.29\n91.68\n93.76\n4.66\n4. Experiments\n4.1. Experimental Setup\nDatasets.\nFollowing the mainstream benchmark set-\ntings [56], we conduct experiments on three datasets, i.e.,\nCIFAR-100 [25], mini-ImageNet [32] and CUB-200 [41],\nto evaluate our LP-DiF. Additionally, this paper also pro-\nposes two more challenging benchmarks for FSCIL, i.e.,\nSUN-397 [48] and CUB-200∗. For SUN-397 [48], it has\nabout twice the number of classes and incremental sessions\nthan the aforementioned three benchmarks. We evaluate\nour method on this benchmark to reveal whether it works\nin scenarios with more classes and more incremental ses-\nsions. CUB-200∗ is a variant of CUB-200 but excluding the\nbase session, is used to evaluate whether our method works\nin scenarios without the base session. Refer to Suppl. for\nthe details of all selected benchmarks.\nImplementation Details. All experiments are conducted\nwith PyTorch on 8× NVIDIA RTX 2080Ti GPUs.\nWe\nleverage the ViT-B/16 as the image encoder and adopt SGD\nwith 0.9 momentum to optimize the prompts. The learning\nrate is initialized by 0.002. For the base session, the batch\nsize is set to 64 and the training epoch is set to 200, As\nfor each incremental session, the batch size and the train-\ning epochs are set to 25, 100, respectively. The VAE com-\nponent is enabled only for incremental sessions. For the\nhyper-parameters, M is set to 10; B and λo are set to 8 and\n2, respectively; L is set to 16 following Zhou et al. [60]; λr\nis set to 1 following Wang et al. [47].\n4.2. Main Results\nComparison with State-of-The-Arts. We summarize the\nresults of competing methods on mini-ImageNet in Table 1.\nClearly, employing CLIP (baseline) [29, 38] for zero-shot\nTable 2. Comparison with Upper bound on the three common\nbenchmarks in terms of Avg.\nMethods\nCIFAR-100\nmini-ImageNet\nCUB-200\nLP-DiF (Ours)\n75.12\n93.76\n74.00\nJoint-LP\n76.22\n94.81\n75.42\n(a) CIFAR-100\n(b) CUB-200\nFigure 4.\nAccuracy curves of our LP-DiF and comparison\nwith existing SOTA FSCIL methods on (a) CIFAR100 and (b)\nCUB200.\n(a) SUN-397\n(b) CUB-200*\nFigure 5. Accuracy curves of our LP-DiF and comparison with\ncounterparts on (a) SUN-397 and (b) CUB200*.\nevaluation alone outperforms all existing FSCIL methods\nby a large margin in terms of accuracy in each session\nand Average Accuracy (Avg). Naturally, it achieves a no-\n6\nTable 3. Ablation studies of our LP-DiF on mini-ImageNet. LP and OCD denote learning prompts and old-class distribution, respectively.\nRF and SF denote the real features of training images and synthesized features generated by VAE, respectively.\nCLIP\nLP\nOCD\nAccuracy in each session (%) ↑\nAvg ↑\nPD ↓\nRF\nSF\n0\n1\n2\n3\n4\n5\n6\n7\n8\n✓\n80.01\n79.16\n78.89\n77.97\n77.44\n76.83\n76.32\n76.02\n75.45\n77.57\n4.56\n✓\n✓\n96.34\n94.28\n92.83\n89.93\n88.39\n86.10\n85.49\n85.70\n84.76\n89.31\n11.58\n✓\n✓\n✓\n96.34\n96.14\n94.01\n94.27\n93.23\n93.07\n91.34\n91.17\n90.76\n93.37\n5.46\n✓\n✓\n✓\n96.34\n96.14\n93.79\n92.48\n91.25\n90.94\n90.15\n89.41\n89.27\n92.23\n7.07\n✓\n✓\n✓\n✓\n96.34\n96.14\n94.62\n94.37\n94.06\n93.44\n92.21\n92.29\n91.68\n93.76\n4.66\nTable 4. Comparison with other replay approaches on CUB-200\nin terms of Avg. The results of “Randomly selection” are reports\nover 5 runs with mean and standard deviations. iCaRL† means\napplying the replay technique proposed in iCaRL to CLIP + LP.\nMethods\nNe\nStorage Space\nAvg\nCLIP + LP\n-\n-\n69.36\nRandomly selection\n1\n18.32 ± 0.37 MB\n69.95 ± 0.56\n2\n37.31 ± 0.99 MB\n71.16 ± 0.24\n3\n55.95 ± 1.01 MB\n72.44 ± 0.09\n4\n74.33 ± 0.70 MB\n73.64 ± 0.16\niCaRL† [30]\n1\n18.54 MB\n70.81\n2\n38.39 MB\n71.68\n3\n55.40 MB\n72.86\n4\n74.68 MB\n73.95\nLP-DiF (Ours)\n-\n0.78 MB\n74.00\ntably lower Performance Drop rate (PD). Our LP-DiF fur-\nther achieves 16.19% (77.57% → 93.76%) gains than the\nCLIP in terms of Avg, and shows comparable PD perfor-\nmance, i.e., 4.66% vs. 4.56%. As for the existing SOTA\nmethods, e.g., NC-FSCIL [51], which presents the best Avg\namong all the SOTA methods, LP-DiF gains 25.94% im-\nprovements, i.e., 67.82% → 93.76%. Comparing with C-\nFSCIL [18], which presents the best PD. among the com-\npeting methods, LP-DiF gains 10.33% improvements, i.e.,\n14.99% → 4.66%. Fig. 4 shows the performance curves on\nCIFAR-100 and CUB-200. One can observe that LP-DiF\nconsistently outperforms existing SOTA methods in all ses-\nsions for all benchmarks. These results clearly illustrate the\nsuperiority of our LP-DiF.\nComparison with Upper Bound. Assuming that the train-\ning set from each previous session is available, we can\njointly train the prompts using these sets, thereby avoiding\nthe issue of forgetting old information. In class-incremental\nlearning, the above-mentioned setting can be considered as\nan upper bound, and serve as a reference for evaluating FS-\nCIL method. Thus, we compare our LP-DiF with the upper\nbound (i.e. Joint-LP). As shown in Table 2, across the three\nbenchmarks, the performance of our method is very close to\nthe upper bound in terms of Avg, with the largest gap being\nonly 1.42% on CUB-200. The results indicate that our LP-\nDiF is highly effective in preventing catastrophic forgetting.\nMore Challenging Benchmarks.\nOn the three widely\nused benchmarks, the performance of our LP-DiF closely\napproaches the upper bound.\nTo further assess our LP-\nDiF, we provide two more challenging benchmarks: SUN-\n397 and CUB-200* (refer to suppl.\nfor details).\nFor\neach challenging benchmark, we compare our LP-DiF with\nthree distinct approaches: zero-shot evaluation using CLIP\n(baseline), Joint-LP (upper bound), and BiDistFSCIL [56]\n(SOTA open-source method). The corresponding perfor-\nmance curves are depicted in Fig. 5. Overall, on both SUN-\n397 and CUB-200*, our LP-DiF method 1) significantly\nsurpasses both CLIP and BiDistFSCIL, and 2) attains per-\nformance levels that are very close to the respective upper\nbounds. The results show that our LP-DiF remains very ef-\nfective on these challenging situations, including those with\na larger number of classes and extended session lengths,\ne.g., 397 classes across 21 sessions in SUN-397, as well as\nin those without a base session, exemplified by CUB-200*.\n4.3. Ablation Study\nAnalysis of Key Components. Our proposed method in-\nvolves prompt tuning on CLIP to adapt the knowledge from\neach incremental session. It also constructs feature-level\ndistributions to preserve old knowledge, thereby achiev-\ning resistance to catastrophic forgetting. To investigate the\neffect of the key components in our method, i.e., CLIP,\nprompt learning (LP), the distribution estimated by real fea-\ntures (RF) of training images, and the synthesized features\n(SF), we summarized the performance of each component\non mini-ImageNet in Table 3. As illustrated, employing the\nLP technique noticeably improves performance across each\nsession, ultimately resulting in a superior 11.74% perfor-\nmance, i.e., from 77.57% → 89.31% in terms of average\nperformance (refer to the second row of Table 3). How-\never, solely implementing LP causes higher PD than CLIP,\ne.g., 4.56% → 11.58%, due to the forgetting of old knowl-\nedge during learning in new sessions. Additionally, as men-\ntioned in Sec.3.2, the old-class distribution (OCD) is effec-\ntive in tackling the forgetting problem. Note that the distri-\nbution is estimated by real features (RF) and the synthesized\nfeatures (SF) generated by VAE. So we conducted separate\nevaluations to assess the effect of these two types of “fea-\ntures”. Concretely, using only RF to estimate the old-class\n7\n(b) Analysis of M\n(a) Comparison with LC\nCIFAR-100\nmini-ImageNet\nCUB-200\n50\n60\n70\n80\n90\nAvg (%)\n(c) Analysis of     and\n2\n4\n6\n8\n10\n0.25\n0.50\n1.00\n2.00\n4.00\nFigure 6. Ablation studies of our LP-DiF. (a) Comparison with the method of incorporating a Linear Classifier (LC) into a pre-trained\nimage encoder for training. (b) Analysis of M. (c) Analysis of B and λo in terms of Avg on CUB-200.\ndistribution can improve the Avg by 4.11%, i.e., 89.31%\n→ 93.42%, and reduce the PD. by 6.12%, i.e., 11.58% →\n5.46%, (see the third row of Table 3). Using only SF can\nalso improve Avg and reduce PD, however, its effectiveness\nis marginally inferior to using only RF (refer to the fourth\nrow of Table 3). Finally, using both types of “features” can\nfurther improve the performance, which surpasses the out-\ncomes achieved by using either RF or SF alone (see the last\nrow of Table 3). Thus, although LP can enable the model to\neffectively capture the knowledge of each session and im-\nprove performance, it still leads to catastrophic forgetting,\nwhile OCD can effectively prevent this issue.\nAnalysis of M. As mentioned in Sec. 3.2, M is the number\nof synthetic features which participate in estimating µc and\nσc of old-class distribution. The value of M will affect the\naccuracy of the estimated distribution, thus influencing the\nperformance of the model. Therefore, we test the effect of\nM on the performance. Fig. 6 (b) shows the results on the\nthree widely used benchmarks. Clearly, when M increases\nfrom 0, the Avg gradually improves. Nonetheless, when M\ncontinues to increase, the Avg decreases slightly, possibly\nascribing to that too many synthesized features can cause\nthe estimated distribution to overly skew towards the distri-\nbution of the synthesized features. In summary, M = 10\n(on mini-ImageNet) or M = 15 (on CIFAR-100 and CUB-\n200) are the best choices for performance.\nAnalysis of B and λo. Here we investigate the effect of\nthe two hyper-parameters, i.e., B, the number of selected\nold classes involved in Lo, and λo, the tradeoff coefficient\nin Lo. The results on CUB-200 are shown with a mixed\nmatrix of these two hyper-parameters in Fig 6 (c). Obvi-\nously, keeping λo fixed, as B increases, the Avg improves\ngradually. Keeping B fixed and tuning λo shows a similar\ntendency with the above setting. It achieves the best Avg\nwhen B = 8 and λo = 2 on CUB-200. Then, when the\nvalues of B and λo are too large, e.g. B = 10 and λo = 4,\nthere is a slight performance drop. These may be because\ntoo small values of B and λo lead to insufficient represen-\ntation of old knowledge, while too large values may cause\nthe model to overly emphasize old knowledge.\nLearning Prompt vs. Linear Classifier. This study uti-\nlizes prompt tuning to tailor CLIP to the specific knowl-\nedge of each session. Another straightforward and intuitive\nstrategy involves incorporating a linear classifier with the\nimage encoder, which is initialized using the text encoding\nof handcrafted prompts. So we conduct additional experi-\nments: 1) Refining the linear classifier (LC) solely with the\ntraining set accessible in the current session; 2) Extending\nthe first approach by integrating the old-class distribution\nfor feature replay (LC + OCD); 3) Jointly training the linear\nclassifier with the complete training set from each session\n(Joint-LC). As shown in Fig 6 (a), the Avg of LC is notably\nlower than that of LP across three wide benchmarks in terms\nof Avg. The incorporation of OCD with LC (denoted as LC\n+ OCD) enhances performance beyond LC alone, highlight-\ning OCD’s effectiveness in mitigating catastrophic forget-\nting. Nevertheless, the combined LC + OCD is still inferior\nto LP + OCD. In a joint training scenario, the performance\nof Joint-LC continues to be inferior to Joint-LP. The results\nsuggest that the strategy of learning prompts offers more\nmerits for FSCIL than that of learning a linear classifier.\nOld-Class Distribution vs. Image Exemplar. To further\nvalidate its efficacy in avoiding catastrophic forgetting, we\ncompare our method with other replay-based approaches\ntailored for learning prompts, i.e., 1) randomly selecting\nNe images of per old class as exemplars; 2) adopting the\nreplay strategy in iCaRL [30], specifically choosing Ne im-\nages for each old class based on the proximity to the mean\nfeature. In addition, we execute the random selection ap-\nproach five times, each with a different random seed, to re-\nduce the uncertainty. The average results with necessary\nstorage space for replay on CUB-200 are shown in Table 4,\nwhere CLIP + LP indicates the baseline without replay of\nold classes. Obviously, our method exhibits the best perfor-\nmance and lowest storage space in comparison to the two\ncounterparts under various Ne. Especially, compared with\niCaRL† under Ne = 4, LP-DiF shows a comparable per-\nformance while only requiring about 0.01% storage space.\nThis underscores that our pseudo-feature replay technique\ncan effectively combat catastrophic forgetting under condi-\ntions of light storage overhead.\n8\n5. Conclusion\nIn this paper, we studied the FSCIL problem by in-\ntroducing V-L pretrained model, and proposed Learning\nPrompt with Distribution-based Feature replay (LP-DiF).\nSpecifically,\nprompt tuning is involved to adaptively\ncapture the knowledge of each session.\nTo alleviate\ncatastrophic forgetting,\nwe established a feature-level\ndistribution for each class, which is estimated by both\nreal features of training images and synthesized features\ngenerated by a VAE decoder. Then, pseudo features are\nsampled from old-class distributions, and combined with\nthe training set of current session to train the prompts\njointly.\nExtensive experiments demonstrate that our LP-\nDiF achieves the new state-of-the-art in the FSCIL task.\nReferences\n[1] Aishwarya Agarwal, Biplab Banerjee, Fabio Cuzzolin, and\nSubhasis Chaudhuri. Semantics-driven generative replay for\nfew-shot class incremental learning. In Proceedings of the\n30th ACM International Conference on Multimedia, pages\n5246–5254, 2022. 3\n[2] Afra Feyza Aky¨urek, Ekin Aky¨urek, Derry Tanti Wijaya, and\nJacob Andreas. Subspace regularizers for few-shot class in-\ncremental learning. arXiv preprint arXiv:2110.07059, 2021.\n2\n[3] Rahaf Aljundi, Min Lin, Baptiste Goujaud, and Yoshua Ben-\ngio. Gradient based sample selection for online continual\nlearning.\nAdvances in neural information processing sys-\ntems, 32, 2019. 3\n[4] Jihwan Bang, Heesu Kim, YoungJoon Yoo, Jung-Woo Ha,\nand Jonghyun Choi.\nRainbow memory: Continual learn-\ning with a memory of diverse samples. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8218–8227, 2021. 3\n[5] Arslan Chaudhry, Puneet K Dokania, Thalaiyasingam Ajan-\nthan, and Philip HS Torr. Riemannian walk for incremen-\ntal learning: Understanding forgetting and intransigence. In\nProceedings of the European conference on computer vision\n(ECCV), pages 532–547, 2018. 3\n[6] Arslan Chaudhry, Albert Gordo, Puneet Dokania, Philip\nTorr, and David Lopez-Paz. Using hindsight to anchor past\nknowledge in continual learning.\nIn Proceedings of the\nAAAI conference on artificial intelligence, pages 6993–7001,\n2021. 3\n[7] Ali\nCheraghian,\nShafin\nRahman,\nPengfei\nFang,\nSoumava Kumar Roy,\nLars Petersson,\nand Mehrtash\nHarandi.\nSemantic-aware knowledge distillation for few-\nshot class-incremental learning.\nIn Proceedings of the\nIEEE/CVF conference on computer vision and pattern\nrecognition, pages 2534–2543, 2021. 2\n[8] Ali Cheraghian, Shafin Rahman, Sameera Ramasinghe,\nPengfei Fang,\nChristian Simon,\nLars Petersson,\nand\nMehrtash Harandi. Synthesized feature based few-shot class-\nincremental learning on a mixture of subspaces. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 8661–8670, 2021. 2\n[9] Zhixiang Chi, Li Gu, Huan Liu, Yang Wang, Yuanhao\nYu, and Jin Tang.\nMetafscil: A meta-learning approach\nfor few-shot class incremental learning. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 14166–14175, 2022. 2, 6, 13\n[10] Matthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Aleˇs Leonardis, Gregory Slabaugh, and\nTinne Tuytelaars. A continual learning survey: Defying for-\ngetting in classification tasks. IEEE transactions on pattern\nanalysis and machine intelligence, 44(7):3366–3385, 2021.\n1, 2\n[11] Songlin Dong, Xiaopeng Hong, Xiaoyu Tao, Xinyuan\nChang, Xing Wei, and Yihong Gong.\nFew-shot class-\nincremental learning via relation knowledge distillation. In\nProceedings of the AAAI Conference on Artificial Intelli-\ngence, pages 1255–1263, 2021. 2\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020. 3\n[13] Arthur Douillard, Alexandre Ram´e, Guillaume Couairon,\nand Matthieu Cord.\nDytox: Transformers for continual\nlearning with dynamic token expansion. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 9285–9295, 2022. 3\n[14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. Advances in\nneural information processing systems, 27, 2014. 3\n[15] Ziqi Gu, Chunyan Xu, Jian Yang, and Zhen Cui. Few-shot\ncontinual infomax learning. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 19224–\n19233, 2023. 2, 6, 13\n[16] Chen He, Ruiping Wang, Shiguang Shan, and Xilin Chen.\nExemplar-supported generative reproduction for class incre-\nmental learning. In BMVC, page 98, 2018. 3\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770–778, 2016. 1, 2\n[18] Michael Hersche, Geethan Karunaratne, Giovanni Cheru-\nbini, Luca Benini, Abu Sebastian, and Abbas Rahimi. Con-\nstrained few-shot class-incremental learning. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 9057–9067, 2022. 2, 6, 7, 13\n[19] Wenpeng Hu, Zhou Lin, Bing Liu, Chongyang Tao, Zheng-\nwei Tao, Jinwen Ma, Dongyan Zhao, and Rui Yan. Overcom-\ning catastrophic forgetting for continual learning via model\nadaptation. In International conference on learning repre-\nsentations, 2018. 3\n[20] David Isele and Akansel Cosgun. Selective experience re-\nplay for lifelong learning. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, 2018. 3\n[21] Zhong Ji, Zhishen Hou, Xiyao Liu, Yanwei Pang, and Xue-\nlong Li.\nMemorizing complementation network for few-\n9\nshot class-incremental learning. IEEE Transactions on Im-\nage Processing, 32:937–948, 2023. 2\n[22] Jian Jiang, Edoardo Cetin, and Oya Celiktutan.\nIb-drr-\nincremental learning with information-back discrete repre-\nsentation replay. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, pages\n3533–3542, 2021. 3\n[23] Do-Yeon Kim, Dong-Jun Han, Jun Seo, and Jaekyun\nMoon. Warping the space: Weight space rotation for class-\nincremental few-shot learning. In The Eleventh International\nConference on Learning Representations, 2022. 2\n[24] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 2, 3, 4,\n5\n[25] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 6, 12\n[26] Anna Kukleva, Hilde Kuehne, and Bernt Schiele.\nGener-\nalized and incremental few-shot learning by explicit learn-\ning and calibration without forgetting.\nIn Proceedings of\nthe IEEE/CVF international conference on computer vision,\npages 9020–9029, 2021. 2\n[27] Huan Liu, Li Gu, Zhixiang Chi, Yang Wang, Yuanhao Yu,\nJun Chen, and Jin Tang. Few-shot class-incremental learning\nvia entropy-regularized data-free replay. In European Con-\nference on Computer Vision, pages 146–162. Springer, 2022.\n6, 13\n[28] Pratik Mazumder, Pravendra Singh, and Piyush Rai. Few-\nshot lifelong learning. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, pages 2337–2345, 2021. 2\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2, 3, 4, 6, 13\n[30] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. icarl: Incremental classifier\nand representation learning. In Proceedings of the IEEE con-\nference on Computer Vision and Pattern Recognition, pages\n2001–2010, 2017. 7, 8\n[31] David Rolnick, Arun Ahuja, Jonathan Schwarz, Timothy Lil-\nlicrap, and Gregory Wayne. Experience replay for continual\nlearning. Advances in Neural Information Processing Sys-\ntems, 32, 2019. 3\n[32] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, et al.\nImagenet large\nscale visual recognition challenge. International journal of\ncomputer vision, 115:211–252, 2015. 1, 2, 4, 6, 12\n[33] Guangyuan Shi, Jiaxin Chen, Wenlong Zhang, Li-Ming\nZhan, and Xiao-Ming Wu.\nOvercoming catastrophic for-\ngetting in incremental few-shot learning by finding flat min-\nima. Advances in neural information processing systems, 34:\n6747–6761, 2021. 2, 6, 13\n[34] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim.\nContinual learning with deep generative replay. Advances in\nneural information processing systems, 30, 2017. 3\n[35] James Seale Smith, Leonid Karlinsky, Vyshnavi Gutta, Paola\nCascante-Bonilla, Donghyun Kim, Assaf Arbelle, Rameswar\nPanda, Rogerio Feris, and Zsolt Kira. Coda-prompt: Contin-\nual decomposed attention-based prompting for rehearsal-free\ncontinual learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n11909–11919, 2023. 3\n[36] Zeyin Song, Yifan Zhao, Yujun Shi, Peixi Peng, Li Yuan,\nand Yonghong Tian. Learning with fantasy: Semantic-aware\nvirtual contrastive constraint for few-shot class-incremental\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 24183–\n24192, 2023. 1, 6, 13\n[37] Xiaoyu Tao, Xiaopeng Hong, Xinyuan Chang, Songlin\nDong, Xing Wei, and Yihong Gong.\nFew-shot class-\nincremental learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n12183–12192, 2020. 1, 2, 6, 13\n[38] Vishal Thengane, Salman Khan, Munawar Hayat, and Fahad\nKhan. Clip model is an efficient continual learner. arXiv\npreprint arXiv:2210.03114, 2022. 3, 6\n[39] Songsong Tian, Lusi Li, Weijun Li, Hang Ran, Xin Ning,\nand Prayag Tiwari. A survey on few-shot class-incremental\nlearning. arXiv preprint arXiv:2304.08130, 2023. 1, 2\n[40] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan\nWierstra, et al. Matching networks for one shot learning. Ad-\nvances in neural information processing systems, 29, 2016.\n12\n[41] Catherine Wah, Steve Branson, Peter Welinder, Pietro Per-\nona, and Serge Belongie. The caltech-ucsd birds-200-2011\ndataset. 2011. 6, 12\n[42] Liyuan Wang, Xingxing Zhang, Hang Su, and Jun Zhu. A\ncomprehensive survey of continual learning: Theory, method\nand application. arXiv preprint arXiv:2302.00487, 2023. 1\n[43] Runqi Wang, Xiaoyue Duan, Guoliang Kang, Jianzhuang\nLiu, Shaohui Lin, Songcen Xu, Jinhu L¨u, and Baochang\nZhang. Attriclip: A non-incremental learner for incremental\nknowledge learning. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pages\n3654–3663, 2023. 3, 14\n[44] Yabin Wang, Zhiwu Huang, and Xiaopeng Hong. S-prompts\nlearning with pre-trained transformers: An occam’s razor for\ndomain incremental learning. Advances in Neural Informa-\ntion Processing Systems, 35:5682–5695, 2022.\n[45] Zifeng Wang, Zizhao Zhang, Sayna Ebrahimi, Ruoxi Sun,\nHan Zhang, Chen-Yu Lee, Xiaoqi Ren, Guolong Su, Vin-\ncent Perot, Jennifer Dy, et al. Dualprompt: Complementary\nprompting for rehearsal-free continual learning. In European\nConference on Computer Vision, pages 631–648. Springer,\n2022. 14\n[46] Zifeng Wang, Zizhao Zhang, Chen-Yu Lee, Han Zhang,\nRuoxi Sun, Xiaoqi Ren, Guolong Su, Vincent Perot, Jennifer\nDy, and Tomas Pfister.\nLearning to prompt for continual\nlearning. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 139–149,\n2022. 3, 14\n[47] Zhengbo Wang, Jian Liang, Ran He, Nan Xu, Zilei Wang,\nand Tieniu Tan. Improving zero-shot generalization for clip\n10\nwith synthesized prompts. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 3032–\n3042, 2023. 2, 4, 5, 6\n[48] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva,\nand Antonio Torralba.\nSun database: Large-scale scene\nrecognition from abbey to zoo. In 2010 IEEE computer so-\nciety conference on computer vision and pattern recognition,\npages 3485–3492. IEEE, 2010. 6, 12\n[49] Boyu Yang, Mingbao Lin, Binghao Liu, Mengying Fu,\nChang Liu, Rongrong Ji, and Qixiang Ye.\nLearnable\nexpansion-and-compression network for few-shot class-\nincremental learning.\narXiv preprint arXiv:2104.02281,\n2021. 2\n[50] Boyu Yang, Mingbao Lin, Yunxiao Zhang, Binghao Liu, Xi-\naodan Liang, Rongrong Ji, and Qixiang Ye. Dynamic sup-\nport network for few-shot class incremental learning. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n45(3):2945–2951, 2022. 2\n[51] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip\nTorr, and Dacheng Tao.\nNeural collapse inspired feature-\nclassifier alignment for few-shot class-incremental learning.\nIn The Eleventh International Conference on Learning Rep-\nresentations, 2022. 6, 7, 13\n[52] Yang Yang, Zhiying Cui, Junjie Xu, Changhong Zhong, Wei-\nShi Zheng, and Ruixuan Wang.\nContinual learning with\nbayesian model based on a fixed pre-trained feature extrac-\ntor. Visual Intelligence, 1(1):5, 2023. 3\n[53] Yibo Yang, Haobo Yuan, Xiangtai Li, Zhouchen Lin, Philip\nTorr, and Dacheng Tao.\nNeural collapse inspired feature-\nclassifier alignment for few-shot class incremental learning.\narXiv preprint arXiv:2302.03004, 2023. 2\n[54] Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan,\nand Yinghui Xu. Few-shot incremental learning with contin-\nually evolved classifiers. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 12455–12464, 2021. 2, 6, 13, 14\n[55] Hanbin Zhao, Yongjian Fu, Mintong Kang, Qi Tian, Fei Wu,\nand Xi Li. Mgsvf: Multi-grained slow vs. fast framework for\nfew-shot class-incremental learning. IEEE Transactions on\nPattern Analysis and Machine Intelligence, 2021. 2\n[56] Linglan Zhao, Jing Lu, Yunlu Xu, Zhanzhan Cheng, Dashan\nGuo, Yi Niu, and Xiangzhong Fang.\nFew-shot class-\nincremental learning via class-aware bilateral distillation. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 11838–11847, 2023. 1,\n2, 6, 7, 12, 13, 14\n[57] Da-Wei Zhou, Fu-Yun Wang, Han-Jia Ye, Liang Ma, Shil-\niang Pu, and De-Chuan Zhan. Forward compatible few-shot\nclass-incremental learning. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition,\npages 9046–9056, 2022. 2, 6, 13, 14\n[58] Da-Wei Zhou, Han-Jia Ye, Liang Ma, Di Xie, Shiliang Pu,\nand De-Chuan Zhan. Few-shot class-incremental learning by\nsampling multi-phase tasks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2022. 2\n[59] Da-Wei Zhou, Qi-Wei Wang, Zhi-Hong Qi, Han-Jia Ye, De-\nChuan Zhan, and Ziwei Liu. Deep class-incremental learn-\ning: A survey. arXiv preprint arXiv:2302.03648, 2023. 1, 2,\n14\n[60] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei\nLiu. Learning to prompt for vision-language models. In-\nternational Journal of Computer Vision, 130(9):2337–2348,\n2022. 2, 3, 6\n[61] Kai Zhu, Yang Cao, Wei Zhai, Jie Cheng, and Zheng-Jun\nZha. Self-promoted prototype refinement for few-shot class-\nincremental learning. In Proceedings of the IEEE/CVF con-\nference on computer vision and pattern recognition, pages\n6801–6810, 2021. 2\n[62] Huiping Zhuang, Zhenyu Weng, Run He, Zhiping Lin, and\nZiqian Zeng.\nGkeal: Gaussian kernel embedded analytic\nlearning for few-shot class incremental task.\nIn Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 7746–7755, 2023. 2, 6, 13\n[63] Yixiong Zou, Shanghang Zhang, Yuhua Li, and Ruixuan\nLi. Margin-based few-shot class-incremental learning with\nclass-level overfitting mitigation. Advances in neural infor-\nmation processing systems, 35:27267–27279, 2022. 2\n11\nLearning Prompt with Distribution-Based Feature Replay\nfor Few-Shot Class-Incremental Learning\nAppendix\nContents\nThe following items are included in the appendix:\n• Details of Selected Benchmarks in Sec. 6.\n• Quantitative Comparison with SOTAs on CUB-200 and\nCIFAR-100 in Sec. 7.\n• Comparison with Pre-trained Model-based Standard CIL\nMethods in Sec. 8.\n• Decomposing the Performance of the Base Class and the\nIncremental Class in Sec. 9.\n• Analysis of Various Shot Numbers in Sec. 10.\n6. Details of Benchmarks\nFollowing the mainstream benchmark settings [56], we\nconduct our experiments on three datasets, i.e., CIFAR-\n100 [25], mini-ImageNet [40] and CUB-200 [41], to evalu-\nate our LP-DiF.\n• CIFAR-100 dataset consists of 100 classes, each of which\ncontains 50, 000 training images. Following the previous\nstudy [56], there are 60 classes in the base session, and\nthe remaining classes will be divided into 8 incremen-\ntal sessions, with each incremental session comprising 5\nclasses.\n• CUB-200 is a fine-grained classification dataset contain-\ning 200 bird species with about 6, 000 training images.\nFollowing the previous study [56], there are 100 classes\nin the base session, and the remaining classes will be di-\nvided into 10 incremental sessions, with each incremental\nsession comprising 10 classes.\n• mini-ImageNet is a smaller part of ImageNet [32], which\nhas 50, 000 training images from 100 chosen classes. Fol-\nlowing the previous study [56], there are 60 classes in the\nbase session, and the remaining classes will be divided\ninto 8 incremental sessions, with each incremental ses-\nsion comprising 5 classes.\nAdditionally, this paper also proposes two more chal-\nlenging benchmarks for FSCIL, i.e., SUN-397 [48] and\nCUB-200∗:\n• SUN-397 is a large-scale scene understanding dataset\ncontaining 397 distinct scene classes with about 76, 000\ntraining images. We select 197 classes for the base ses-\nTable 5. Details of selected benchmarks. The first three lines are\ncommonly used benchmarks, while the last two lines are the more\nchallenging benchmarks proposed in this paper. |CAll|, |CBase| and\n|CInc| denotes the total number of classes, the number of classes\nin base session, and the number of classes in each incremental\nsession respectively. #Base and #Inc denote the number of base\nsessions and the incremental session respectively. Shot denotes\nthe number of training images of each incremental session. ∗ rep-\nresents a variant version.\nDataset\n|CAll|\n|CBase|\n|CInc|\n#Base\n#Inc\nShot\nCIFAR-100 [25]\n100\n60\n5\n1\n8\n5\nmini-ImageNet [32]\n100\n60\n5\n1\n8\n5\nCUB-200 [41]\n200\n100\n10\n1\n10\n5\nSUN-397 [48]\n397\n197\n10\n1\n20\n5\nCUB-200* [41]\n200\n0\n10\n0\n20\n5\nsion; the remaining classes will be split into 20 incremen-\ntal sessions, with each incremental session comprising 10\nclasses. We evaluate our method on this benchmark to re-\nveal whether it is effective in scenarios with more classes\nand more incremental sessions.\n• CUB-200∗ is a variant of CUB-200 but excludes the base\nsession. We evenly divide the total 200 classes into 20 in-\ncremental sessions, with each session containing 10 cate-\ngories. Following the previous study [56], there are 100\nclasses in the base session, and the remaining classes will\nbe divided into 10 incremental sessions, with each incre-\nmental session comprising 10 classes. We use it to eval-\nuate whether our method works in scenarios without the\nbase session.\nFor the above five benchmarks, the classes in the base ses-\nsion have a large amount of training data, while each class in\nevery incremental session has only 5 training images. Tab. 5\nsummarizes the details of each selected benchmark.\n7. Quantitative comparison with SOTAs on\nCUB-200 and CIFAR-100\nWe have shown the accuracy comparison curves with the\nstate-of-the-art methods on CIFAR-100 and CUB-200 in\nFig. 4 of our main paper. Here, we present details compar-\nison results with specific values on these two benchmarks,\nshown in Tab. 6 and Tab. 7. Overall, the performance of\nour LP-DiF can be summarized in three points. 1) There\nare significant improvements compared to CLIP (baseline)\nin terms of Avg (i.e., 15.91% and 4.26% improvements on\n12\nTable 6. Comparison with state-of-the-art FSCIL methods on CUB-200, where the rows filled with gray indicate the existing SOTA\nFSCIL methods. “Avg” represents the average accuracy of all sessions; the higher the value, the better performance. “PD” represents the\nPerformance Drop rate; the lower the value, the better performance.\nMethods\nAccuracy in each session (%) ↑\nAvg ↑\nPD ↓\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTOPIC [37]\n68.68\n62.49\n54.81\n49.99\n45.25\n41.40\n38.35\n35.36\n32.22\n28.31\n26.26\n43.92\n42.42\nCEC [54]\n75.85\n71.94\n68.50\n63.50\n62.43\n58.27\n57.73\n55.81\n54.83\n53.52\n52.28\n61.33\n23.57\nReplay [27]\n75.90\n72.14\n68.64\n63.76\n62.58\n59.11\n57.82\n55.89\n54.92\n53.58\n52.39\n61.52\n23.51\nMetaFSCIL [9]\n75.90\n72.41\n68.78\n64.78\n62.96\n59.99\n58.30\n56.85\n54.78\n53.82\n52.64\n61.93\n23.26\nFACT [57]\n75.90\n73.23\n70.84\n66.13\n65.56\n62.15\n61.74\n59.83\n58.41\n57.89\n56.94\n64.42\n18.96\nFCIL [15]\n78.70\n75.12\n70.10\n66.26\n66.51\n64.01\n62.69\n61.00\n60.36\n59.45\n58.48\n65.70\n20.22\nGKEAL [62]\n78.88\n75.62\n72.32\n68.62\n67.23\n64.26\n62.98\n61.89\n60.20\n59.21\n58.67\n66.35\n20.21\nNC-FSCIL [51]\n80.45\n75.98\n72.30\n70.28\n68.17\n65.16\n64.43\n63.25\n60.66\n60.01\n59.44\n67.28\n21.01\nBiDistFSCIL [56]\n79.12\n75.37\n72.80\n69.05\n67.53\n65.12\n64.00\n63.51\n61.87\n61.47\n60.93\n67.34\n18.19\nSAVC [36]\n81.85\n77.92\n74.95\n70.21\n69.96\n67.02\n66.16\n65.30\n63.84\n63.15\n62.50\n69.35\n19.35\nF2M [33]\n81.07\n78.16\n75.57\n72.89\n70.86\n68.17\n67.01\n65.26\n63.36\n61.76\n60.26\n69.49\n20.81\nCLIP (Baseline) [29]\n65.54\n62.91\n61.54\n57.75\n57.88\n57.89\n56.62\n55.40\n54.20\n54.23\n55.06\n58.09\n10.48\nLP-DiF (Ours)\n83.94\n80.59\n79.17\n74.30\n73.89\n73.44\n71.60\n70.81\n69.08\n68.74\n68.53\n74.00\n15.41\nJoint-LP (Upper bound)\n83.94\n80.83\n79.43\n77.06\n76.35\n74.89\n73.66\n72.79\n71.84\n72.06\n71.88\n75.88\n12.06\nTable 7. Comparison with state-of-the-art FSCIL methods on CIFAR-100, where the rows filled with gray indicate the existing SOTA\nFSCIL methods. “Avg” represents the average accuracy of all sessions; the higher the value, the better performance. “PD” represents the\nPerformance Drop rate; the lower the value, the better performance.\nMethods\nAccuracy in each session (%) ↑\nAvg ↑\nPD ↓\n0\n1\n2\n3\n4\n5\n6\n7\n8\nTOPIC [37]\n64.10\n55.88\n47.07\n45.16\n40.11\n36.38\n33.96\n31.55\n29.37\n42.62\n34.73\nF2M [33]\n64.71\n62.05\n59.01\n55.58\n52.55\n49.96\n48.08\n46.28\n44.67\n53.65\n20.04\nCEC [54]\n73.07\n68.88\n65.26\n61.19\n58.09\n55.57\n53.22\n51.34\n49.14\n59.53\n23.93\nReplay [27]\n74.40\n70.20\n66.54\n62.51\n59.71\n56.58\n54.52\n52.39\n50.14\n60.78\n24.26\nMetaFSCIL [9]\n74.50\n70.10\n66.84\n62.77\n59.48\n56.52\n54.36\n52.56\n49.97\n60.79\n24.53\nGKEAL [62]\n74.01\n70.45\n67.01\n63.08\n60.01\n57.30\n55.50\n53.39\n51.40\n61.35\n22.61\nC-FSCIL [18]\n77.47\n72.40\n67.47\n63.25\n59.84\n56.95\n54.42\n52.47\n50.47\n61.64\n27.00\nFCIL [15]\n77.12\n72.42\n68.31\n64.47\n61.18\n58.17\n56.06\n54.19\n52.02\n62.66\n25.10\nFACT [57]\n78.22\n72.40\n68.57\n64.73\n61.40\n58.57\n56.30\n53.83\n51.72\n62.86\n26.50\nSAVC [36]\n78.77\n73.31\n69.31\n64.93\n61.70\n59.25\n57.13\n55.19\n53.12\n63.63\n25.65\nBiDistFSCIL [56]\n79.45\n75.38\n71.84\n67.95\n64.96\n61.95\n60.16\n57.67\n55.88\n66.14\n23.57\nNC-FSCIL [51]\n82.52\n76.82\n73.34\n69.68\n66.19\n62.85\n60.96\n59.02\n56.11\n67.50\n26.41\nCLIP (Baseline) [29]\n74.44\n72.96\n72.21\n70.49\n70.18\n70.00\n69.81\n69.23\n68.37\n70.86\n6.07\nLP-DiF (Ours)\n80.23\n77.75\n76.78\n74.62\n74.03\n73.87\n73.84\n72.96\n72.02\n75.12\n8.21\nJoint-LP (Upper bound)\n80.23\n79.85\n78.63\n76.13\n75.31\n74.67\n74.24\n73.58\n73.35\n76.22\n6.88\nCUB-200 and CIFAR-100 respectively). 2) Compared to\nexisting SOTA methods, LP-DiF achieves a higher Avg and\nlower PD. 3) Both Avg and PD of LP-DiF are very close\nto that of Joint-LP (upper bound), i.e., lower only 1.88%\nAvg and 3.35% PD on CUB-200, 1.10% Avg and 0.81%\nPD on CIFAR-100. These results clearly illustrate the ef-\nfectiveness of our LP-DiF.\n13\nTable 8. Comparison with standard CIL methods based on pre-\ntrained models on the three common benchmarks in terms of Avg.\n‡ indicates our reproduction.\nMethods\nCIFAR-100\nmini-ImageNet\nCUB-200\nBiDistFSCIL [56]\n66.14\n62.04\n67.34\nL2P‡ [46]\n61.77\n75.68\n56.95\nDualPrompt‡ [45]\n63.50\n76.61\n62.32\nAttriCLIP‡ [43]\n59.24\n81.74\n47.81\nLP-DiF (Ours)\n75.12\n93.76\n74.00\nAccuracy (%)\n71.1\n33.9\n45.9\n73.9\n40.5\n52.3\n72.7\n49.5\n58.9\n77.4\n59.4\n67.2\nBase Class\nIncremental Class\nHarmonic Mean\nFigure 7. Decomposing the performance of the base class and the\nincremental class. The performance is evaluated by the model\nfrom the last session on CUB-200.\n8. Comparison with Pre-trained Models-based\nStandard CIL Methods\nTo further demonstrate the superiority of our method,\nwe compare it with several recent standard CIL [59] meth-\nods and variant it to pre-trained model-based methods:\nL2P [46], DualPrompt [45] and AttriCLIP [43]. We repro-\nduce these three approaches on CIFAR-100, CUB-200, and\nmini-ImageNet and evaluate them under FSCIL protocol re-\nspectively. As shown in Tab. 8, our LP-DiF outperforms\nthese methods by a large margin in terms of Avg across all\nthree benchmarks. We also find that these methods based\non pre-trained models underperform BiDistFSCIL [56] on\nCIFAR-100 and CUB-200. This indicates that these meth-\nods are not advantageous for the FSCIL setting, further un-\nderscoring the effectiveness and significance of our method.\n9. Decomposing the Performance of the Base\nClass and the Incremental Class\nFollowing previous studies [54, 56, 57], in this section,\nwe decompose the accuracy, respectively analyzing the ef-\nfectiveness of our LP-DiF for the classes in the base session\n(i.e., base class) and for the classes in incremental sessions\n(a) 2-Shot\n(b) 5-Shot\n(c) 10-Shot\n(d) 15-Shot\nFigure 8. Comparison with BiDistFSCIL (SOTA FSCIL method)\nand Joint-LP (Upper bound) under various shot numbers of in-\ncremental classes on CUB-200.\n(i.e., incremental class), to evaluate if our method performs\nwell on both base and incremental classes. We report the\ncomparison results in terms of individual accuracy of base\nand novel classes, as well as their harmonic mean, in the last\nsession on CUB-200. Fig. 7 shows that our LP-DiF outper-\nforms the second best method on base class (i.e., FACT)\nby 3.5%, while outperforms the second best method on in-\ncremental class (i.e., BiDistFSCIL) by 9.9%. Finally, the\nsuperior harmonic mean demonstrates our achievement of\nan enhanced balance between base and novel classes.\n10. Analysis Various Shot Numbers\nIn the main paper, we have compared the performance\nof our LP-DiF with SOTA methods and the Joint-LP (up-\nper bound) under 5-shot for each incremental class. To fur-\nther demonstrate the superiority of our approach, we con-\nducted experiments under various shot numbers of incre-\nmental classes. Fig. 8 show the comparison results with\nBiDistFSCIL [56] and Joint-LP on CUB-200 under (a) 2-\nshot, (b) 5-shot (default in main paper), (c) 10-shot and\n(d) 15-shot. Obviously, across all the shot number settings,\nour LP-DiF consistently outperforms BiDistFSCIL signifi-\ncantly, and its performance is very close to the upper bound.\nThis result demonstrates that, regardless of the shot num-\nbers of incremental classes, our LP-DiF presents satisfac-\ntory performance and the ability to resist catastrophic for-\ngetting.\n14\n"
}