{
    "optim": "ζ-QVAE: A Quantum Variational Autoencoder utilizing\nRegularized Mixed-state Latent Representations\nGaoyuan Wang1,2†\nJonathan Warrell 1,2†\nPrashant S. Emani1,2\nMark Gerstein1,2,3,4,5∗\n1 Program in Computational Biology and Bioinformatics,\n2 Department of Molecular Biophysics and Biochemistry,\n3 Department of Computer Science,\n4 Department of Statistics & Data Science,\n5 Department of Biomedical Informatics & Data Science,\nYale University, New Haven, Connecticut 06520, USA\n† These authors contributed equally to this work.\n*Corresponding author:\npi@gersteinlab.org\nAbstract\nA major challenge in near-term quantum computing is its application to large\nreal-world datasets due to scarce quantum hardware resources. One approach to\nenabling tractable quantum models for such datasets involves compressing the\noriginal data to manageable dimensions while still representing essential information\nfor downstream analysis. In classical machine learning, variational autoencoders\n(VAEs) facilitate efficient data compression, representation learning for subsequent\ntasks, and novel data generation. However, no model has been proposed that exactly\ncaptures all of these features for direct application to quantum data on quantum\ncomputers. Some existing quantum models for data compression lack regularization\nof latent representations, thus preventing direct use for generation and control\nof generalization. Others are hybrid models with only some internal quantum\ncomponents, impeding direct training on quantum data. To bridge this gap, we\npresent a fully quantum framework, ζ-QVAE, which encompasses all the capabilities\nof classical VAEs and can be directly applied for both classical and quantum\ndata compression. Our model utilizes regularized mixed states to attain optimal\nlatent representations. It accommodates various divergences for reconstruction and\nregularization. Furthermore, by accommodating mixed states at every stage, it can\nutilize the full-data density matrix and allow for a “global” training objective. Doing\nso, in turn, makes efficient optimization possible and has potential implications for\nprivate and federated learning. In addition to exploring the theoretical properties of\nζ-QVAE, we demonstrate its performance on representative genomics and synthetic\ndata. Our results consistently indicate that ζ-QVAE exhibits similar or better\nperformance compared to matched classical models.\n1\nIntroduction\nAutoencoders play an important role in current machine learning systems, enabling compression of\ndata, learning latent representations, and as generative models. Classical variational autoencoders\n(VAEs) provide a unified modeling framework which combines these strengths, and more recent\nclassical models have extended this framework to allow a trade-off between reconstruction and\n1\narXiv:2402.17749v1  [quant-ph]  27 Feb 2024\ninformation captured by the latent space [1], to maximize the coverage of the latent space and hence\navoid generating spurious patterns [2], and to incorporate more complex encoders and decoders [3, 4]\nIn the Noisy Intermediate-Scale Quantum (NISQ) era, quantum technologies are progressing rapidly,\nand classical machine learning methods are rapidly being generalized to operate in a quantum machine\nlearning setting. Yet, the limited availability of quantum hardware and restrictions on the number of\nqubits in actual quantum devices underscores the need to minimize quantum resource requirements.\nIn this work, we introduce a fully generalized quantum variational autoencoder (QVAE) framework,\nwhich answers the challenges above by allowing efficient quantum data compression. Our framework\npreserves or generalizes all the key features of classical VAE models, while directly operating on\nquantum data, to which classical compression methods cannot be directly applied. Notably, our\nproposed framework is valuable not just for quantum datasets but also for classical datasets due to\nthe following potential advantages: (1) Quantum superposition offers the inherent advantage of a\nmuch richer representation space than classical binary bits. This enables potentially more efficient\nrepresentations of data, crucial for compression into a compact latent space; (2) The entanglement\nof qubits can be utilized to capture intricate dependencies in the original data via the encoding into\nlatent states, which classical methods may be unable to represent efficiently; (3) Our framework\nemploys quantum probability in place of classical distributions; for instance, we replace the classical\nGaussian distributions typically used in VAEs with quantum mixed states.\nA large number of proposals have been made to provide quantum analogues of autoencoder models\n[5, 6, 7, 8, 9]. Mostly, such models learn a quantum circuit to directly maximize the reconstruction\nof input quantum states. However, this approach has several shortcomings. While such quantum\nautoencoder analogs are optimized for the reconstruction of quantum states, they do not include\na regularization term over the latent space, and hence cannot be used directly for generation and\ndo not explicitly control generalization. We note that such models are ‘variational’ in the sense\nthat their quantum circuits may be trained using an approximate Ansatz, which differs from the\napproximation of a prior distribution which induces the regularization term in a classical VAE\nobjective; we will thus refer to this type of model as a Quantum Autoencoder (QAE). Further, the\ntraining for such QAEs assumes a particular form for the reconstruction error (quantum fidelity), and\nhence cannot be directly generalized if other forms of objective are required. In the quantum context,\nmany different measures of similarity between quantum states have been proposed in addition to\nfidelity; the restriction to a particular similarity measure is thus undesirable. Another shortcoming\nof such QAEs is that the input, encoded and output states are all assumed to be pure states; this\nsacrifices a unique potential advantage of quantum models for handling large datasets in parallel\nand embedding information using mixed quantum states.\nAn alternative kind of model is a hybrid quantum-classical analogue, such as the Ref. [10], which is a\nclassical VAE with a quantum Boltzmann distribution imposed upon its latent variables. Although\nsuch hybrid models are trained as generative models, their objective is defined using a bound on the\nclassical log-likelihood (hence they are trained to generate and reconstruct classical data). They\ncannot therefore be trained directly on quantum inputs, and moreover sacrifice the potential virtues\nof handling data efficiently through mixed quantum states. Hence, none of the current models is\nable to integrate the advantages of mixed-state input and latent representations, flexible objectives\nand effective, fully quantum regularization. However, such capabilities are particularly important for\nscaling up quantum models to handle real-world datasets with large feature spaces by compressing\nthem down to dimensions feasible for NISQ quantum hardware.\nWe therefore introduce a gate-based quantum variational autoencoder framework with a training\nobjective that includes a latent space regularization term (and hence may be viewed as a generalized\nform of probabilistic generative models). We refer to our model as a ζ-QVAE, where ζ represents\nthe density matrix of the mixed-state latent representation in our model (analogous to the classical\n2\nlatent state, Z). The ζ−QVAE allows for regularized latent representations to exist as mixed states\nwithin the Bloch sphere, which further enriches the representation space, and potentially allows\nmore efficient data compression. In our framework, the encoder and decoder pairs are modeled as\nquantum operations (completely positive trace-preserving (CPTP) linear maps), to provide mixed-\nstate latent representations for quantum inputs (which may be mixed or pure states). Our model\nalso offers multiple divergence options for the reconstruction and regularization losses, since specific\nlosses are commonly used for particular applications of Quantum Machine Learning. Specifically,\nwe show how i) fidelity [11], frequently utilized in quantum state tomography [12], ii) quantum\nrelative entropy/quantum Jensen-Shannon divergence, significant in the field of quantum information\ntheory [13, 14, 15], and iii) quantum Wasserstein distance, applied in generative models [16], can\nbe integrated into our framework. Furthermore, we show that a quantum information-theoretic\nanalogue to the classical evidence lower bound (ELBO) exists for the regularized reconstruction loss\nwhen the quantum relative entropy is used.\nMoreover, we formulate both local and global versions of each divergence, which allow models to be\noptimized for reconstruction of individual data points or the dataset as a whole, respectively. In\nparticular, the quantum Wasserstein distance can be shown to give rise to equivalent optimal models\nunder both global and instance-based objectives, while the other global divergences were observed to\ngive similarly good performance under both on a real-world genomics dataset. The global version of\neach divergence allows efficient optimization by reducing the number of repeated quantum operations\nduring the training. It also has implications for private and federated learning, since it requires only\naggregate information about the dataset as opposed to individual data points. Further, we discuss\nsuitable application domains for the ζ-QVAE, as well as addressing the challenges associated with\nimplementing our framework on NISQ hardware.\n2\nTheoretical Framework\nWe assume that our data live in an input Hilbert space, X, over NX qubits, and that we wish\nto learn an encoder/decoder pair to compress our dataset to Hilbert space, Z, over NZ qubits\n(NZ ≤ NX). Our dataset consists of a finite set of N pure states, |ψ1⟩ ... |ψN⟩ (which may themselves\nbe generated from classical data-points, e.g. by amplitude encoding, or may be generated from an\nintrinsically quantum source). We use the following definitions for the input density matrices of\nindividual datapoints (indexed by i), and a global density matrix representing the entire dataset:\nρi\n=\n|ψi⟩ ⟨ψi|\nρglob\n=\n1\nN\nX\ni\nρi\n(1)\nWe note that our dataset may be considered a finite sample from a distribution across pure states\nover X, and hence ρglob may be considered a finite-sample approximation to an underlying data\ndistribution. Our goal is to learn quantum operations (completely positive trace-preserving (CPTP)\nlinear maps) (E, D) corresponding to an encoder and decoder respectively, where these have the\nrespective signatures E : D(X) → D(Z) and D : D(Z) → D(X) (here, D(X) denotes the set of\ndensity matrices over finite Hilbert space X; we note also that, due to circuit constraints, we may\nhave E ∈ SE and D ∈ SD, where SE and SD are subsets of CPTP linear maps having a predefined\nmaximum circuit complexity). Given an (E, D) pair, we define:\n3\nζi\n=\nE(ρi)\nσi\n=\nD(ζi)\n(2)\nwhere ζi and σi represent the latent and reconstructed state respectively associated with input state\nρi, and similarly, ζglob = E(ρglob) and σglob = D(ζglob). Further, we assume we have a predefined\n‘prior’ density matrix over the latent space, ζgen; below, we will take this to be the maximally mixed\nstate, ζgen = (1/2NZ)I2NZ . As in the classical case, this prior, ζgen, is transformed by the decoder\nto produce a generative approximation of the data distribution, σgen = D(ζgen). For concreteness,\nto define our encoder and decoder (see Fig. 1 and Fig. 2), we append NA auxiliary qubits to our\ninput Hilbert space X, and NT reference qubits along with NB auxiliary qubits to our latent Hilbert\nspace Z (NT = NX − NZ, where NT denotes the number of ‘trash’ qubits). Then, we can use the\nfollowing definitions:\nE(ρ)\n=\nTrNA+NT (U −1(ρ ⊗ |0NA⟩ ⟨0NA|)U)\nD(ζ)\n=\nTrNB(V −1(ζ ⊗ |0NB+NT ⟩ ⟨0NB+NT |)V )\n(3)\nwhere U and V are unitary matrix representations of the encoder and decoder circuits (E and D)\nrespectively, and TrN(.) denotes the trace over the final N qubits (where, in general, the qubits may\nbe ordered/indexed arbitrarily, although below we assume that the auxiliary qubits are ordered after\nthose in X and Z). In Appendix A (Prop. 1), we prove that setting NA = NB = NZ is sufficient to\nallow arbitrary pairs of quantum operations (E, D) to be learned (assuming no circuit complexity\nconstraints).\nGlobal Training Objective: To derive a training loss for the model above, we assume that we\nare interested in learning a model which minimizes a completely general loss L1(a, b) (the only\nassumptions being that it is non-negative and 0 iff a = b, but not necessarily symmetric, i.e., a\ndivergence) between the implicit generative model and the global data density matrix (note that we\nderive an alternative instance-based objective below); hence we seek to optimize:\nmin\nD L1(ρglob, σgen)\n(4)\nWe note that Eq. 4 involves only the decoder, D. In analogy with the classical VAE, to simultaneously\nlearn a representation of our data in the latent space, we introduce a variational density parameterized\nby our encoder E, which we assume (temporarily) to be expressive enough to fulfill the condition\nζglob = ζgen:\nmin\nD L1(ρglob, σgen) =\nmin\nE,D\ns.t.ζglob=ζgen\nL1(ρglob, σglob)\n(5)\nWe can reformulate Eq. 5 as a constrained optimization problem, introducing a second (regularization)\nloss L2 (with the same conditions as L1):\nmin\nE,D L1(ρglob, σglob)\nL2(ζglob, ζgen) ≤ ϵ\n(6)\n4\nTo account for the fact that our class of encoders may not allow L2(ζglob, ζgen) = 0 to be fulfilled,\nwe introduce the constant ϵ = minE(L2(ζglob, ζgen)) in Eq. 6. Finally, introducing the Lagrange\nmultiplier β ≥ 0, we derive our training objective F for a global input density matrix, ρglob:\nmin\nE,D\ns.t.L2(ζglob,ζgen)≤ϵ\nL1(ρglob, σglob)\n≥\nmax\nβ\nmin\nE,D Fglob(E, D, β)\nFglob(E, D, β)\n=\nL1(ρglob, σglob) + β(L2(ζglob, ζgen) − ϵ)\n(7)\nIn place of the maximization across β on the RHS (upper) of Eq. 7, we treat β as a hyperparameter\nwhen optimizing Fglob, and we disregard the constant −βϵ for the objectives used in the ζ-QVAE\n(as defined in Eq. 22 and Eq. 23). We also show, in Appendix A (Prop. 2), that when L1 and L2 are\nthe quantum relative entropy, β = 1 and ϵ = 0, that Fglob(E, D, β) forms an analogue of the classical\nEvidence Lower-Bound (ELBO), as in the classical VAE [17] (we note that this bound is distinct\nfrom the Q-ELBO bound in [10], since the Q-ELBO is a bound on the classical log-likelihood, while\nProp. 2 is a bound on the quantum relative entropy). The ζ-QVAE objective therefore optimizes\nthe original objective in Eq. 4 in the case that either L1 and L2 are the quantum relative entropy\nwith β = 1, or β = β∗, where β∗ is the optimum value of β in the RHS (upper) of Eq. 7.\nInstance-based Training Objective: In the above, Eq. 7 provides an objective for training (E, D)\nbased on the generation and reconstruction of the global data density matrix ρglob. However,\nwe may be interested in the reconstruction of individual data points, which is not explicitly\noptimized in Eq. 7. For this reason, we consider the following optimization problem for instance-level\ngeneration/reconstruction:\nmin\nD\nX\ni\nL1(ρi, σgen)\n(8)\nBy a similar argument to above, this leads to the following instance-level objective, Finst:\nmin\nE,D\ns.t.L2(ζi,ζgen)≤ϵ,∀i\nX\ni\nL1(ρi, σi)\n≥\nmax\nβ1...N min\nE,D Finst(E, D, β1...N)\nFinst(E, D, β1...N)\n=\nX\ni\n(L1(ρi, σi) + βi(L2(ζi, ζgen) − ϵ))\n(9)\nAs above, we treat the β’s as a hyperparameter, using a common β = β1 = ... = βN, and ignore the\nconstant terms −βiϵ. Here, ϵ = minE(maxi L2(ζi, ζgen)), and Eq. 9 forms a strict lower-bound on\nEq. 8 when ϵ = 0. In general, Fglob and Finst will lead to different optimization problems, and hence\ndifferent solutions for (E, D); however, in Appendix A (Prop. 3), we show that for certain losses, the\noptimization problems in Eq. 7 and Eq. 9 become equivalent.\nWe note finally that, if we have an auxiliary loss function L′\n1 for which:\nmin\nE,D\ns.t.D(E(ρ))=σ\nL′\n1(ρ, E, D) = L1(ρ, σ),\n(10)\n5\nwe may use the following alternative definitions of Fglob and Finst in Eq. 7 and Eq. 9:\nF ′\nglob(E, D, β)\n=\nL′\n1(ρglob, E, D) + β(L2(ζglob, ζgen) − ϵ)\nF ′\ninst(E, D, β1...N)\n=\nX\ni\n(L′\n1(ρi, E, D) + βi(L2(ζi, ζgen) − ϵ))\n(11)\nThis version of the bound will used below when L1 is the Wasserstein divergence.\n3\nModel\n3.1\nζ-QVAE architecture\nThe overall architecture of the proposed QVAE is given in Fig. 1 with an example of NX = 2 input\nqubits, a latent space of NZ = 1 qubit, and one auxiliary qubit (d1) in both the encoder and decoder\n(hence, NA = NB = 1). The encoder and decoder are defined by quantum circuits, with trainable\nparameters θe and θd respectively. The corresponding unitary matrices are denoted U(θe) and V (θd)\nrespectively. Additionally, the unitary matrix Ai performs the conversion from a classical source\nto a quantum representation for data-point i (e.g. using amplitude or angle embedding). Hence,\n|ψi⟩ = Ai |0⟩, and ρi = |ψi⟩ ⟨ψi|. After the embedding and encoder circuits have been applied to the\ninitial |0⟩ state, both the NA auxiliary qubits and the NT = NX − NZ trash qubits are discarded by\na partial trace operation, and the remaining qubit q1 is considered the latent state. The encoder E\nas a whole therefore has the form defined by Eq. 3. To reconstruct original information from the\nlatent state, NT + NB zero state qubits are added to the remaining qubits, and a final partial trace\nis performed across the auxiliary qubit d1; hence the decoder D is of the form in Eq. 3.\nIn Fig. 2, the encoder circuit U(θe) we used in this study is shown for one trainable layer i.e. Nl = 1\n(note that we use a data embedding circuit, Ai, to first project classical data to a quantum state, e.g.\nvia amplitude encoding; this is not formally part of the ζ-QVAE encoder, and can be removed if a\nquantum data source provides the input state). The Ansatz (marked in beige) was introduced by\n[18] and contains Rzz entangling gates and single qubit Ry rotations. The decoder circuit contains\nthe same Ansatz as the encoder.\n3.2\nTraining objectives\nReconstruction loss, L1: We provide here the explicit forms of all the divergences we consider for the\nreconstruction loss. As in Sec. 2, a divergence between two density matrices ρ and σ over the same\nHilbert space is a non-negative function, L(ρ, σ), which is zero iff ρ = σ, but unlike a metric, need\nnot be symmetric. For generality, we write all divergences below for arbitrary ρ and σ. However, we\nare particularly interested in the cases L(ρglob, σglob) and L(ρi, σi), denoting the divergence between\ninput and output density matrices for the global and instance level objectives respectively (see Eq. 7\nand Eq. 9). For these cases, ρi = |ψi⟩ ⟨ψi|, where |ψi⟩ is the state vector of the i-th input data-point,\nρglob = (1/N) P\ni ρi, σi = D(E(ρi)), and σglob = D(E(ρglob)), where E and D are the quantum\noperation representations of the encoder and decoder, as in Sec. 2. The particular losses we consider\nfor the reconstruction loss, L1(ρ, σ), are summarized below:\n• Fidelity loss:\nLf\n1(ρ, σ) = 1 −\n\u0012\nTr\nq√σρ√σ\n\u00132\n,\n(12)\n6\n(a)\n(b)\nFigure 1: (a) Illustration of the architecture and objective function of the ζ-QVAE; arrows represent\ntransformations between mixed states, ζgen is the maximally mixed state, and L1 and L2 are the\nreconstruction and regularization loss respectively. (b) ζ-QVAE overall circuit representation.\nq1 : |0⟩\nEmbedding Ai\nRzz(θ1)\nRzz(θ3)\nRy(θ4)\nq2 : |0⟩\nRzz(θ2)\nRy(θ5)\nd1 : |0⟩\nRy(θ6)\nFigure 2: Encoder circuit\nwhere, for a pure state ρ = |ψρ⟩ ⟨ψρ|, this reduces to: Lf\n1(ρ, σ) = ⟨ψρ| σ |ψρ⟩.\n• Quantum relative entropy (KLD):\nLkl\n1 (ρ, σ) = S(ρ|σ) = S(ρ, σ) − S(ρ) = − Tr(ρ log(σ)) − S(ρ),\n(13)\nwhere S(ρ) = − Tr(ρ log(ρ)) and S(ρ, σ) = − Tr(ρ log(σ)) .\n• Symmetric quantum relative entropy (JSD) [15]:\nLjsd\n1\n(ρ, σ)\n=\nS\n\u0010\nρ\n\f\f\f 1\n2 [ρ + σ]\n\u0011\n+ S\n\u0010\nσ\n\f\f\f 1\n2 [ρ + σ]\n\u0011\n.\n(14)\n7\n• Quantum Wasserstein-distance loss:\nLw\n1 (ρ, σ) =\nmin\nT :T (ρ)=σ Tr(π(ρ, T )C),\n(15)\nwhere T is a quantum operation, and:\nπ(ρ, T )\n:=\nX\ni\npi(T (|ei⟩ ⟨ei|)) ⊗ (|ei⟩ ⟨ei|)\n(16)\nwith |ei⟩ an orthogonal basis for ρ, hence ρ = P\ni pi |ei⟩ ⟨ei|, and C is defined as in [16]. As\ndiscussed in Sec. 2, we introduce the following auxiliary loss function in place of Lw\n1 for the\nreconstruction loss when using the Quantum Wasserstein-distance:\n(L′\n1)w(ρ, E, D) = Tr(π(ρ, D ◦ E)C),\n(17)\nClearly, we have:\nmin\nE,D\ns.t.D(E(ρ))=σ\n(L′\n1)w(ρ, E, D) = Lw\n1 (ρ, σ),\n(18)\nand so we can use the alternative form of the training objectives in Eq. 11 to optimize\nLw\n1 (ρ, σ).\nRegularization loss, L2: We write the regularization loss below in the general form L2(ζ, ζgen), i.e. a\ndivergence between a mixed-state latent representation ζ and the analog of the classical generative\n‘prior’ on the latent space, ζgen. As discussed in Sec. 2, we use ζgen = 1\nλI, where I is the identity\noperator and λ the dimension of the latent Hilbert space. This represents the maximally mixed\nstate, i.e. the quantum state with the maximal entropy. In principle, all the divergences above could\nbe used for the regularization loss, L2. However, we exclude the quantum Wasserstein loss, since\nthis would require us to minimize over an auxiliary circuit to find the lowest-cost transformation\nT between ζ and ζgen. We briefly summarize the remaining divergences used for L2, with the\nsimplifications induced by setting ζgen = 1\nλI.\n• Fidelity loss:\nLf\n2(ζ, ζgen) = 1 −\n\u0012\nTr\nqp\nζgenζ\np\nζgen\n\u00132\n.\n(19)\n• Quantum relative entropy (KLD):\nLkl\n2 (ζ, ζgen) = S(ζ, ζgen) − S(ζ) = Tr(ζ log(ζ)) − log(1/λ) = −S(ζ) + c,\n(20)\nwhere c = − log(1/λ).\n• Symmetric quantum relative entropy (JSD):\nLjsd\n2\n(ζ, ζgen)\n=\nS\n\u0010\nζ\n\f\f\f 1\n2 [ζ + ζgen]\n\u0011\n+ S\n\u0010\nζgen\n\f\f\f 1\n2 [ζ + ζgen]\n\u0011\n(21)\nOverall training objectives: For explicitness, we collect together the specific forms of the overall\nglobal and instance based training objectives used to train our model, based on Eq. 7 and Eq. 9\nrespectively:\n8\nLglob(θe, θd, β)\n=\nL1(ρglob, σglob) + βL2(ζglob, ζgen)\nLinst(θe, θd, β)\n=\nX\ni\n(L1(ρi, σi) + βL2(ζi, ζgen))\n(22)\nalong with the alternative forms used for the Wasserstein reconstruction loss based on Eq. 11 and\nEq. 17:\nL′\nglob(θe, θd, β)\n=\nL′\n1(ρglob, E(θe), D(θd)) + βL2(ζglob, ζgen)\nL′\ninst(θe, θd, β)\n=\nX\ni\n(L′\n1(ρi, E(θe), D(θd)) + βL2(ζi, ζgen)) .\n(23)\n3.3\nQSVC classifier\nIn addition to the ability of the ζ-QVAE to reconstruct the original states, we are also interested\nin how well the latent and reconstructed states belonging to different classes can be effectively\ndistinguished. In other words, we want to evaluate the classification performance on the latent and\nreconstructed states in comparison to the original input states. To evaluate this, we implemented a\nquantum kernel based classifier[18] with amplitude embedding.\nOur QSVC classifier is illustrated in Fig. 3 using an example with one trainable layer. We use the\nsame Ansatz, which includes alternating Rzz and Ry gates for the quantum kernel, as employed in\nthe encoder and decoder. The similarity kernel of our QSVC is obtained as the quantum fidelity\nbetween each data pair. Due to the nature of the gene expression data and the normalizations\nwe applied to our data for the amplitude embedding, we have observed a concentration of fidelity\nscores towards the higher end rather than being spread across the entire range from zero to one. To\naddress this, we introduced a scaling function\nf\n\u0010\n⟨vi|vj⟩\n\u0011\n= tan\n\u0010 π\n2.03 ⟨vi|vj⟩\n\u0011\nto enhance resolution within the densely populated region.\nq1 : |0⟩\nEmbedding Ai\nRzz(θ1)\nRzz(θ3)\nRy(θ4)\nQSV C\nq2 : |0⟩\nRzz(θ2)\nRy(θ5)\nq3 : |0⟩\nRy(θ6)\nFigure 3: The overall QSVC architecture.\n4\nExperiments\n4.1\nDataset\nWe test our model on a synthetic dataset that is designed to be compressible and a large and\nnoisy real-world gene expression dataset (including schizophrenia patients and controls) from the\nPsychENCODE project [19].\n9\n4.1.1\nPsychENCODE gene expression data\nIn this dataset, the schizophrenia status for patients and controls is given together with the quantile\nnormalized expression values of 16 selected genes, generated from RNAseq data from the prefrontal\ncortex of ∼ 1500 postmortem subjects from the PsychENCODE consortium [19]. These genes were\nselected from a panel of 555 genes, including pre-identified high-confidence schizophrenia genes and\ntranscription factors. The 16 genes selected were those found to have the highest variance across\npatients.\nTo allow for possible future applications of angle embedding on this dataset, we first conducted a\nglobal normalization on the data\n⃗xi → π × [⃗xi − min(X)]/[max(X) − min(X)],\nwhere X is the entire dataset matrix (including all feature vectors from all data points) and ⃗xi the\ngene expression vector of the i-th data point. We employed amplitude embedding in this study; we\ntherefore performed an additional per-data-point L2 normalization, which allowed us to process the\ndata as state vectors. We randomly picked equal number of cases (patients) and controls to create a\ntraining and test partition of size 695 and 298 respectively.\n4.1.2\nSwiss Roll synthetic dataset\nIn the interest of understanding the generalizability to different datasets, we run the ζ-QVAE and\nQSVC classifier on 1000 data points from the Swiss Roll dataset [20] as implemented in Python’s\nscikit-learn package. The Swiss Roll dataset involves a helically distributed sheet of 3-dimensional\npoints that can be compressed to a 2-dimensional manifold. To adapt the dataset to our context, we\ntake the 3-dimensional dataset and append 5 additional dimensions by adding Gaussian-distributed\nnoise terms (zero-centered, standard deviation = 0.2). We applied a per-data-point L2 normalization\nto make the inputs suitable for the quantum circuit. Furthermore, we set up a classification task by\ndesignating approximately half of the points as “cases” and the other half as “controls”; the task is\ndesigned to allow perfect classification along the 2-dimensional manifold, thus serving to evaluate\nhow well we capture the 2-dimensional manifold.\nFigure 4: The 3-dimensional Swiss Roll dataset. The colors indicate the labels for the classification\ntask.\n10\n4.2\nModel setup\nTo train the ζ-QVAE, we employed the COBYLA optimizer, utilizing a training duration of 60\nepochs with a patience setting of 20 epochs throughout the study. Additionally, we kept the number\nof layers Nl identical within both the encoder and decoder, as well as the count of auxiliary qubits in\nthe encoder and decoder (NA and NB). When running the ζ-QVAE with NT trash qubits, we select\nthe first NT qubits as trash qubits. All the performance results, including ζ-QVAE reconstruction\nrate and QSVC classification accuracies are averaged over five random initializations.\nWe determined the number of layers of the QSVC classifier Ncl based on its performance on the\ninput datasets. Notably, we observed that varying Ncl between one and three had negligible impact\nfor the datasets comprising 16 input features. Nevertheless, to account for potential larger input\nfeature dimensions, where a greater Ncl might be essential, we opted to set Ncl = 3 for the remainder\nof the study. Throughout this study, the test accuracy serves as the metric for the classification\nperformance.\n5\nResults\nIn this section, we begin by subsec. 5.1 offering an overview of different objective functions introduced\nin subsec. 3.2. Subsequently, we focus on the specific case of our model in which the negative fidelity\nserves as the reconstruction loss, complemented by the JSD as the regularization loss. We will refer\nto this specific objective function as Fid+JSD in the following. In subsec. 5.2 and subsec. 5.3, we\nprovide a thorough investigation of the impact of the model architecture on the regularization, and\nconsequently, on the quantum state reconstruction and downstream classification tasks using the\nFid+JSD objective function. In subsec. 5.4 we test our framework with the global level objective\nfunction. Then in subsec. 5.5, we test the ζ−QVAE on the synthetic dataset. Finally in subsec. 5.6,\nwe compare the ζ−QVAE with QAE and classical VAE models.\nIn our model, the architecture is controlled by several hyperparameters, including the number of\nlayers Nl in the encoder and decoder, the β-value and the number of auxiliary qubits in the encoder\nand decoder NA = NB. As we show, the impacts of these hyperparameters are not independent\nfrom each other. We evaluate the performance of each model using the fidelity reconstruction rate\nof the ζ-QVAE and the accuracy of the QSVC on the downstream classification tasks. The notation\nemployed in this section is as follows: f(NA, Nl) represents the fidelity reconstruction rate of a given\nmodel with NA auxiliary qubits and Nl layers. Similarly, l(NA, Nl) signifies the QSVC test accuracy\nusing the latent states of the corresponding model with NA auxiliary qubits and Nl layers as input,\nwhile r(NA, Nl) denotes the QSVC test accuracy using the reconstructed states as input.\n5.1\nObjective function choice:\nChoice of reconstruction loss We begin with the evaluation of the three different forms of recon-\nstruction loss - fidelity, Wasserstein and JSD - at β = 0, which implies that the regularization term\nis excluded from the objective function. The results are presented in Tab. 1.\nWe found that all three types of reconstruction loss behave qualitatively similarly at β = 0, which\ncan be briefly summarized as follows (further details are elaborated in subsec. 5.2 and subsec. 5.3):\n• In the case no auxiliary qubits are employed in the decoder, the reconstructed state is identical\nto the latent state (extended by reference |0⟩ trash-qubits) up to a unitary transformation\nand thus results in substantially the same fidelity-based quantum kernel matrix for QSVC.\nThus, the classification performance on the latent and reconstructed states is observed to\nbe effectively the same.\n11\nTable 1: Comparison of the three types of reconstruction losses at β = 0.\nfidelity\nwasserstein\nJSD\nf(0, 3)\n0.844 ± 0.043\n0.853 ± 0.033\n0.839 ± 0.037\nl(0, 3)\n0.647 ± 0.01\n0.652 ± 0.007\n0.652 ± 0.013\nr(0, 3)\n0.644 ± 0.01\n0.65 ± 0.005\n0.653 ± 0.012\nf(0, 2)\n0.898 ± 0.008\n0.882 ± 0.013\n0.875 ± 0.012\nl(0, 2)\n0.636 ± 0.008\n0.65 ± 0.009\n0.653 ± 0.011\nr(0, 2)\n0.637 ± 0.008\n0.65 ± 0.01\n0.654 ± 0.007\nf(0, 1)\n0.953 ± 0\n0.771 ± 0.223\n0.953 ± 0.0\nl(0, 1)\n0.606 ± 0.002\n0.62 ± 0.019\n0.605 ± 0.002\nr(0, 1)\n0.607 ± 0.003\n0.622 ± 0.023\n0.606 ± 0.005\nf(1, 3)\n0.742 ± 0.046\n0.695 ± 0.024\n0.686 ± 0.048\nl(1, 3)\n0.655 ± 0.003\n0.65 ± 0.008\n0.651 ± 0.005\nr(1, 3)\n0.627 ± 0.021\n0.607 ± 0.026\n0.63 ± 0.016\nf(1, 2)\n0.851 ± 0.029\n0.838 ± 0.026\n0.841 ± 0.03\nl(1, 2)\n0.653 ± 0.013\n0.657 ± 0.011\n0.653 ± 0.014\nr(1, 2)\n0.613 ± 0.019\n0.601 ± 0.023\n0.622 ± 0.019\nf(1, 1)\n0.886 ± 0.009\n0.894 ± 0.005\n0.9 ± 0.007\nl(1, 1)\n0.629 ± 0.018\n0.64 ± 0.027\n0.636 ± 0.023\nr(1, 1)\n0.574 ± 0.021\n0.573 ± 0.03\n0.571 ± 0.032\n• Regardless of the presence of auxiliary qubits, with an increasing number of layers, the\nfidelity reconstruction rate decreases while the test accuracy of classification tasks improves\nfor both latent and reconstructed states. This observation implies that increasing the\nnumber of layers may implicitly regularize the model, since the larger parameter search\nspace increases the difficulty for the COBYLA optimizer of finding solutions with high\nreconstruction fidelity.\n• For any given value of Nl, incorporating auxiliary qubits results in a lower fidelity recon-\nstruction rate compared to the case where no auxiliary qubits are used. However, the\nclassification accuracy on the latent states improves slightly while the classification accuracy\non the reconstructed states drops especially for smaller Nl.\nWe observe that, generally, the negative fidelity loss is able to achieve better reconstruction perfor-\nmance, while performing comparably to the other losses on classification tasks; we therefore use\nfidelity reconstruction loss in the following sections.\nChoice of regularization loss: Next, we examine the different choices of regularization loss choices at\ndifferent values of β. The results are shown in Tab. 2. Given the variations in overall scale among\nthe different forms of regularization loss, our focus shifts to slightly different ranges of β for each.\nRecalling the baseline results at β = 0 from Tab. 1, the performance of the fidelity reconstruction\nloss is as follows: f(0, 3) = 0.844 ± 0.043, l(0, 3) = 0.647 ± 0.01 and r(0, 3) = 0.644 ± 0.01.\nHere, we observe that KLD has the least favorable performance among the regularization loss options\nin terms of reconstruction rate. All three regularization loss options seem to be comparable in\nclassification test accuracy, although the model utilizing JSD performs is slightly better for the\noptimal β. Consequently, our focus in the next section is on the combination of negative fidelity\nreconstruction loss and JSD regularization loss.\n5.2\nUnderstanding the determinants of regularization and reconstruction in ζ−QVAE\nModels with appropriately tuned regularization, leading to an optimal degree of disentanglement in\ntheir latent representations, have been shown to outperform those lacking such adjustments due\n12\nTable 2: Negative fidelity reconstruction loss with three different regularization loss options.\nβ = 0.5\nβ = 1\nβ = 1.5\nβ = 2\nβ = 2.2\nβ = 2.5\nβ = 2.7\nf(0, 3)\n0.876 ± 0.025\n0.843 ± 0.02\n0.813 ± 0.017\n0.815 ± 0.013\n0.763 ± 0.043\n0.765 ± 0.038\n0.728 ± 0.035\nl(0, 3)\n0.651 ± 0.007\n0.651 ± 0.011\n0.654 ± 0.012\n0.653 ± 0.009\n0.658 ± 0.005\n0.669 ± 0.005\n0.661 ± 0.011\nr(0, 3)\n0.653 ± 0.005\n0.649 ± 0.009\n0.652 ± 0.01\n0.655 ± 0.01\n0.661 ± 0.005\n0.665 ± 0.003\n0.061 ± 0.011\n(a) JSD regularization loss\nβ = 0.5\nβ = 1\nβ = 1.5\nβ = 2\nβ = 2.2\nβ = 2.5\nf(0, 3)\n0.773 ± 0.039\n0.714 ± 0.037\n0.664 ± 0.064\n0.522 ± 0.085\n0.53 ± 0.076\n0.425 ± 0.068\nl(0, 3)\n0.654 ± 0.014\n0.653 ± 0.012\n0.652 ± 0.009\n0.653 ± 0.014\n0.656 ± 0.009\n0.661 ± 0.007\nr(0, 3)\n0.654 ± 0.011\n0.652 ± 0.012\n0.652 ± 0.008\n0.65 ± 0.014\n0.658 ± 0.014\n0.661 ± 0.007\n(b) KLD regularization loss\nβ = 0.5\nβ = 1\nβ = 1.1\nβ = 1.2\nβ = 1.5\nβ = 2\nβ = 2.5\nf(0, 3)\n0.863 ± 0.009\n0.864 ± 0.025\n0.847 ± 0.015\n0.849 ± 0.015\n0.834 ± 0.027\n0.829 ± 0.033\n0.798 ± 0.03\nl(0, 3)\n0.648 ± 0.007\n0.646 ± 0.015\n0.657 ± 0.008\n0.661 ± 0.01\n0.655 ± 0.005\n0.653 ± 0.008\n0.648 ± 0.019\nr(0, 3)\n0.649 ± 0.007\n0.646 ± 0.014\n0.658 ± 0.01\n0.658 ± 0.01\n0.657 ± 0.006\n0.656 ± 0.01\n0.651 ± 0.018\n(c) Negative fidelity regularization loss\nto their ability to capture the independent underlying latent factors effectively [1]. In this section,\nwe investigate how the degree of regularization (explicit and implicit, as discussed below) and the\nreconstruction rate of the ζ−QVAE are influenced by the interplay of several factors: the β-value,\nthe presence of auxiliary qubits and the circuit complexity. We impose different circuit complexity\nconstraints by varying the number of layers Nl in the encoder and decoder and studied a range of\nβ-values from zero to six, while considering all combinations with and without one auxiliary qubit.\nThe reconstruction ability of the model is estimated using the fidelity reconstruction rate. To\nquantify the regularization effect, we analyze the distribution of the latent states in the latent space\nby calculating the regularization loss. In addition, we take into account downstream classification\nperformance as an additional metric for the evaluation of the reconstruction rate and degree of\nregularization.\nWe noticed that the models with Nl = 1 and no auxiliary qubits often failed to converge at non-zero\nβ values, leading to the large standard deviation of f(0, 1) in Tab. 3. For example, among the five\nrandom initializations at β = 2, two exhibited a test fidelity reconstruction rate around 0.5 while the\nremaining three had a fidelity of approximately 0.88. Similarly, at β = 1, three had fidelity around\n0.5, and the remaining two showed a fidelity near 0.93. Further, for the models with Nl = 1 and\none auxiliary qubit, we found that for the range of β ∈ [0, 6], the reconstruction fidelity remained\nconstant within the error range as suggested by f(1, 1) in Tab. 3. Below we provide several key\nfindings based on results obtained for Nl = 2 and 3.\nRegularization is controlled by β-value, model complexity and number of auxiliary qubits: Varying β\nis the most direct way to adjust the degree of the regularization. In Fig. 5, the fidelity reconstruction\nrate (which is 1 for a perfect reconstruction) and regularization loss (where 0 implies stronger\nregularization / smaller regularization loss) are shown as a function of β. We see that for the entire\nrange of β considered in this study, higher β values lead to stronger regularization and worse fidelity\nreconstruction rates. One can also see in Fig. 5, that the regularization loss is smaller when using\none auxiliary qubit compared to the scenario without any auxiliary qubits, across all values of β. In\naddition, the model complexity controlled by Nl can also influence the degree of the regularization.\nAs we can see in Tab. 3 in the case where no auxiliary qubits are present, similar to our observation\nat β = 0, increasing the number of layers in the encoder and decoder also leads to a decreased\nreconstruction rate for β = 1 and 2, accompanied by improved classification performance. Increasing\n13\nTable 3: Fid+JSD objective function\nβ = 0\nβ = 1\nβ = 2\nf(0, 3)\n0.844 ± 0.043\n0.843 ± 0.02\n0.815 ± 0.013\nl(0, 3)\n0.647 ± 0.01\n0.651 ± 0.011\n0.653 ± 0.009\nr(0, 3)\n0.644 ± 0.01\n0.649 ± 0.009\n0.655 ± 0.01\nf(0, 2)\n0.898 ± 0.008\n0.883 ± 0.018\n0.832 ± 0.027\nl(0, 2)\n0.636 ± 0.008\n0.644 ± 0.009\n0.653 ± 0.009\nr(0, 2)\n0.637 ± 0.008\n0.644 ± 0.004\n0.655 ± 0.007\nf(0, 1)\n0.953 ± 0.0\n0.673 ± 0.212\n0.728 ± 0.186\nl(0, 1)\n0.606 ± 0.002\n0.646 ± 0.006\n0.653 ± 0.008\nr(0, 1)\n0.607 ± 0.003\n0.647 ± 0.003\n0.653 ± 0.01\nf(1, 3)\n0.742 ± 0.046\n0.649 ± 0.061\n0.561 ± 0.066\nl(1, 3)\n0.655 ± 0.003\n0.651 ± 0.011\n0.651 ± 0.016\nr(1, 3)\n0.627 ± 0.021\n0.613 ± 0.004\n0.605 ± 0.011\nf(1, 2)\n0.851 ± 0.029\n0.815 ± 0.03\n0.686 ± 0.044\nl(1, 2)\n0.653 ± 0.013\n0.638 ± 0.01\n0.644 ± 0.005\nr(1, 2)\n0.613 ± 0.019\n0.6 ± 0.027\n0.593 ± 0.024\nf(1, 1)\n0.886 ± 0.009\n0.887 ± 0.009\n0.887 ± 0.006\nl(1, 1)\n0.629 ± 0.018\n0.648 ± 0.005\n0.648 ± 0.012\nr(1, 1)\n0.574 ± 0.021\n0.588 ± 0.014\n0.562 ± 0.022\nthe number of layers or the number of auxiliary qubits thus has a similar effect to increasing β,\nresulting in a form of implicit regularization as noted above in subsec. 5.1. At β = 0, the effect of\nincreasing number of layers is more noticeable than at a higher value of β. The number of layers,\nauxiliary qubits and β can thus be viewed as jointly contributing to the regularization of the model.\nFigure 5: The two components of the objective function are plotted as a function of β for the case\nwith one trash qubit and Nl = 3.\n14\nReconstruction and regularization are strongly dependent in the absence of auxiliary qubits: In the\ncase where no auxiliary qubits are used, the reconstructed states are obtained from the latent states\nby a unitary (linear) operation. This means effectively, both the reconstruction constraints and the\nregularization constraints are imposed to the same space (since the latent states are mapped to a\nlinear subspace of the output space with the same intrinsic dimensionality). The reconstructed state\nthus inherits directly the same regularization as the latent state. This can be seen in the left panels\nin Fig. 6, where a lower reconstruction loss at smaller β can only be achieved by sacrificing the\nregularization loss, i.e. by allowing a higher regularization loss.\nReconstruction and regularization are substantially decoupled in the presence of auxiliary qubits:\nIn the presence of auxiliary qubits, we observed simultaneous decreasing curves for the regularization\nloss and the reconstruction loss in right panels of Fig. 6 (one should note that the reconstruction\nloss is the negative counterpart of the reconstruction fidelity shown in Fig. 5). This is because the\npresence of auxiliary qubits allows for non-unitary transformations from latent states to reconstructed\nstates, thus allowing a separate optimization of the regularization loss and reconstruction loss. This\nfeature of ζ−QVAE has no classical analogue and can be potentially utilized to devise a framework\nfor controlling the degree of coupling between latent and reconstructed states, thus enabling a flexible\ntrade-off between regularization and reconstruction rates.\nReconstructing the original state poses challenges in the presence of auxiliary qubits: As shown in\nTab. 3, the fidelity reconstruction rate is lower in the presence of one auxiliary qubit compared to\nits absence. In addition, we note a decrease in the classification performance on the reconstructed\nstates when one auxiliary qubit is used compared to when no auxiliary qubits are used. Whereas\nfor the latent states, the classification performance remains similar. The observed phenomenon\nmay be explained by the removal of the constraint imposed by the coupled reconstruction loss and\nregularization loss when employing one auxiliary qubit, which introduces a greater challenge to the\noptimization process of the model parameters[21]. This difficulty in optimization may be exacerbated\nby the Barren plateau effect intensified by the inclusion of an additional qubit [22].\n5.3\nOptimal representations for downstream classification tasks\nAn optimal degree of regularization exists for the downstream classification performance: In Fig. 7\n(a) and (b), we plot the downstream classification performance against β. We noticed that when no\nauxiliary qubits are used, an optimal range of β is associated with higher classification accuracy.\nFor the one trash qubit case, i.e. NT = 1, shown in (a), the optimal β is found to be around 2.5 for\nboth Nl = 2 and Nl = 3. We also note that the model using three layers achieved slightly higher\nclassification accuracy than the two-layer model. On the other hand, when one auxiliary qubit is\nused, the regularization seems to have no clear impact on the downstream classification performance\n(red and green points). Nevertheless, we cannot conclude less significant improvements cannot be\nidentified since there is a large range of uncertainty in the performance. In panel (b) where two\ntrash qubits are used, the optimal β occurs around 2 for the scenario without auxiliary qubits.\nAlthough it is still difficult to determine if there exists an optimal range of β in the scenarios with\none auxiliary qubit, we can see that the performance of the models with auxiliary qubits is slightly\nbetter than that without auxiliary qubits.\nIn panels (c) and (d), we plot the test accuracy directly against the regularization loss. For both\nNT = 1 and NT = 2 without auxiliary qubits, we see a clear optimal range of regularization loss\nat 0.6 (for NT = 1) and 0.11 (for NT = 2). For models with auxiliary qubits, we included also\nnegative βs, indicated by green points. This is motivated by the observation that, even for β = 0,\nthe regularization was already stronger than the optimal range observed for cases without auxiliary\nqubits case (blue points). As shown in (c), l(1, 3) (red points) improves slightly with increasing\nregularization loss in the NT = 1 case, and this trend persists with a slight improvement for negative\n15\n(a) Nl = 2, no auxiliary qubits\n(b) Nl = 2, one auxiliary qubit\n(c) Nl = 3, no auxiliary qubits\n(d) Nl = 3, one auxiliary qubit\nFigure 6: The optimization process of the reconstruction loss and regularization loss are shown\nseparately.\nIn the absence of auxiliary qubits, the two components of the objective function\nare mutually dependent. Utilizing auxiliary qubits allows for decoupling and thus simultaneous\nimprovement of both terms.\nβ values. In (d), The data suggests an upward trend as the regularization loss decreases. However,\nthis observed trend is less pronounced compared to models without auxiliary qubits and remains\nsuggestive rather than conclusive.\nRegularization is more advantageous for smaller latent space: For NT = 1, the latent space is\nan eight-dimensional Hilbert space formed by three qubits while for NT = 2, the latent space is\nfour-dimensional formed by two qubits. As shown in Fig. 7 (a) and (b), for the NT = 1 case, the\nregularization improves the classification performance by ≈ 4% while for NT = 2, the improvement\nis over 7.5%.\n16\nClassification performance on the latent states is similar to that on the input states: The classification\nperformance of the employed QSVC on the input states is 0.675 ± 0.003.1 For one trash qubit case,\ni.e. compressing to half of the original dimensionality, the best classification performance achieved\non the latent states is 0.669 ± 0.005 for β = 2.5, Nl = 3 and no auxiliary qubits. Notably, this is only\n0.9% lower than that achieved with the full original states. For two trash qubits, i.e. compressing\nto a quarter of the original dimensionality, we achieved a classification performance of 0.63 ± 0.015\nfor β = 6, Nl = 3 and one auxiliary qubit.\n5.4\nTraining using global objective\nRecall from Eq. 1, that the global state is defined as a mixed state over the entire input dataset. In\nthis section, we test the performance of the ζ-QVAE using the global density matrix. We consider\nonly the setup where negative fidelity serves as the reconstruction loss and JSD acts as regularization\nloss.\nIn this scenario, our quantum circuit is trained on a single global input state, while the model\nconstruction is identical to that of the instance-level model. Hence, through the training phase,\none single latent state and one output state are present. Following the completion of quantum\ncircuit training, each individual instance-level input data point will be fed through the optimized\nmodel. For each data point within the original dataset, the associated latent state and reconstructed\nstate are computed. Subsequently, calculations for the fidelity reconstruction rate calculation and\ndownstream classification tasks are executed on the instance-level input, latent and reconstructed\nstates.\nWe tested a range of βs on the global ζ−QVAE and the results are shown in Fig. 8. While the\nreconstruction rate is slightly lower for nearly all βs, the overall pattern of the curve with respect\nto β is very similar to that of the instance-level trained models. In the down-stream classification\ntasks, the QSVC test accuracy achieved on the latent and reconstructed states remains comparable\nfor ζ−QVAE models trained on both global and instance-level data. For l(0, 3), where an optimal β\nof 2.5 was observed for the instance-level trained models, the globally trained models exhibit an\noptimal β of three. Nevertheless, the disparity in performance falls within the error range.\n5.5\nApplication to the Swiss Roll dataset\nTo evaluate performance on the Swiss Roll dataset (Fig. 9), we considered the case where our\n8-dimensional input state is mapped to 3 qubits and the latent state is determined by 1 qubit. For\nthe ζ-QVAE, we set NT = 2, Nl = 3, β = {0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5}, and test the scenario\nwith zero auxiliary qubits to one with 1 auxiliary qubit added to the encoder and decoder. The\nresults yield similar conclusions to those of the gene expression dataset. The reconstruction fidelity\non the leave-out test set steadily decreases with an increase in β, irrespective of the number of\nauxiliary qubits (Fig. 9b). In contrast, the test accuracy in the classification task using the latent\nstate achieves a peak at β = 1.5 for the 0-auxiliary-qubits case, showing an improved test accuracy\nof 0.75 ± 0.04 relative to the accuracy of 0.60 ± 0.02 at β = 0. On the other hand, there is no clear\nbenefit of a non-zero β for the 1-auxiliary-qubit case, at least at the values screened here (Fig. 9c).\nThis may be due to the fact that the implicit regularization due to the inclusion of the auxiliary\nqubit is already quite strong at β = 0. Overall, the test accuracy remains reasonably high, with a\nmaximum of 0.75 ± 0.04 (based on both the latent and reconstructed states; test AUC = 0.82 ± 0.05)\n1We also tested a classical SVC with RBF kernel on the input states and obtained a classification\nperformance of 0.648 ± 0.016, lower than that of the QSVC. The error range in this case is obtained by\naveraging over various data partitions.\n17\n(a) NT = 1\n(b) NT = 2\n(c) NT = 1, including negative β in green\n(d) NT = 2\nFigure 7: (a) and (b): Classification performance is plotted against β. For both NT = 1 and 2, the\nno auxiliary qubit cases (orange and blue line) clearly show an optimal β with improved classification\nperformance, while in the one auxiliary qubit case the optimal β range is unclear. For NT = 2 using\nauxiliary qubits is advantageous compared to no auxiliary qubits. (c) and (d): Plots test accuracy\ndirectly against regularization loss to eliminate uncertainties caused by the intermediate parameter\nβ. For both NT = 1 and 2, while in the no auxiliary qubit case, there is clearly an optimal range for\nregularization loss, for the one auxiliary qubit case this is less clear.\n18\n(a) Fidelity reconstruction rate\n(b) Classification performance\nFigure 8: For NT = 1 and Nl = 3, we compare the globally trained with the instance-level trained\nmodel. The shaded areas represent the error range.\nfor 0 auxiliary qubits and 0.77 ± 0.01 (based on the latent state; test AUC = 0.82 ± 0.02) for 1\nauxiliary qubit.\n(a) Fidelity reconstruction rate\n(b) Classification performance\nFigure 9: NT = 2, Nl = 3. We evaluate the performance of the ζ-QVAE (a) and QSVC classifier on\nthe latent states (b) using the Swiss Roll dataset. The shaded areas represent the error range.\n5.6\nComparison to QAE and classical VAEs\nQuantum Autoencoder (QAE): At β = 0, ζ-QVAE without the regularization term and with a\nfidelity-based reconstruction loss is similar to the QAE introduced in Ref. [5] with the following\nminor differences: (1) The objective function to be maximized in Ref. [5], i.e. fidelity on the trash\nstate F(ρt, |0⟩), serves as an upper bound of the actual reconstruction fidelity F(ρi, D(E(ρi))), which\nwe optimize directly; (2) The decoder in QAE is the inverse of encoder, which is a special instance\n19\nof our decoder, whose parameters are independent from that of the encoder. Our results show that\nthe ζ-QVAE achieves improved classification performance at β > 0, suggesting that models with\nregularization offer advantages compared to the QAE.\nClassical VAEs: We compare two types of classical β-VAEs [1] to the ζ−QVAE. The first type has a\nsingle linear layer without an activation function for both the encoder and decoder. The second type\nis a two-layer β-VAE with 12 hidden nodes and a RELU activation function. We consider classical\nβ-VAEs with 8-, 4-, 2- and 1-dimensional latent spaces. Across these cases, we conducted tests\nover a wide range of β and presented the highest classification performance overall βs in Tab. 4.\nIt is important to note that in the classical β-VAE, each dimension in the latent space includes a\nmean and a variance, resulting in two degrees of freedom per dimension. Therefore, a four/two/one\nlatent dimensional classical VAE is comparable to three/two/one latent qubits in the ζ−QVAE,\nrespectively.\nOn the gene expression data, the fully quantum compression and classification scheme (QVAE+QSVC)\nreached a classification accuracy of 0.669 ± 0.005 using three latent qubits and 0.63 ± 0.015 with two\nlatent qubits, outperforming the fully classical compression and classification scheme (VAE+SVC).\nOn the synthetic Swiss Roll dataset, the ζ−QVAE with one latent qubit achieved a classification\naccuracy of 0.77±0.01, which is slightly higher than that of the classical β−VAE with one-dimensional\nlatent space.\nIn addition to the improved classification accuracy on the latent states, the number of parameters\nused by the ζ−QVAE is also much smaller than that of the classical VAE. For example, in the case\nof 16 input features, a single-layer classical β−VAE with a 4-dimensional latent space has 216 free\nparameters. In contrast, a 3-layer ζ−QVAE has only 60 free parameters for the same input features,\nbut benefits from the high dimensionality of the Hilbert space associated with the latent state. It\nis also possible that the entanglement between the qubits employed by the model contributes to a\nreduction in the number of parameters needed for encoding the original data.\nTable 4: Classification performance on the latent representations\ngene expression data\nSwiss Roll dataset\nlinear VAE\nstandard VAE\nlinear VAE\nstandard VAE\nlatent dim. = 8\n0.653 ± 0.027\n0.659 ± 0.01\n−\n−\nlatent dim. = 4\n0.643 ± 0.017\n0.646 ± 0.006\n−\n−\nlatent dim. = 2\n0.615 ± 0.01\n0.612 ± 0.019\n0.784 ± 0.003\n0.775 ± 0.006\nlatent dim. = 1\n−\n−\n0.708 ± 0.004\n0.768 ± 0.004\n6\nAdvantages of the ζ-QVAE framework\nIn general, the application domains of ζ−QVAE are not expected to differ significantly from those\nof classical VAEs. However, certain distinctive features of ζ−QVAE can offer specific advantages in\nselect applications.\nApplication to large-scale datasets. Our framework addresses key challenges in applying quantum\nmodels to fields involving large-scale datasets by allowing big datasets with large feature spaces to be\ncompressed into a smaller latent space while preserving essential information crucial for downstream\nanalyses, such as classification. This reduces the necessary (quantum) data storage capacity and\naddresses the limited availability of quantum hardware by allowing subsequent analysis to be carried\nout by quantum devices with a small number of qubits.\n20\nApplication to privacy-aware computation. Our formulation of global objectives holds potential for\nprivacy-preserving computation, as it potentially eliminates the need for access to all the original\ndata points during model training. Instead, only the global density matrix may be required, or\nalternatively samples may be provided from any equivalent quantum ensemble with the same density\nmatrix (for instance, the eigenvectors in the basis which the data density matrix diagonalizes,\nweighted by their eigenvalues). Given that mixed states are composed of classical mixtures of\npure states, which may not necessarily be orthogonal, it is possible for different sets of pure states\nto yield the same mixed state. As a consequence, the decomposition of a mixed state into an\nensemble of pure states is not unique. Consequently, if only the global mixed state density matrix\nis provided, individual-level data cannot be recovered. Moreover, the global objective also offers\npotential for application in federated learning. In this scenario, the sub-ensembles of each actor may\nbe transformed independently, as their density matrices can be combined additively to generate the\nfull data matrix.\nApplication to genomics studies. Combining the specific advantages of our framework, we are\nparticularly driven by potential applications in genomics studies. Genomics studies involve large-\nscale datasets that are diverse in data modalities and often contain sensitive information. Our\nframework addresses key challenges in the integration of quantum computing within such domains\nby: 1. enabling the compression of large-scale data into a compact representation, 2. offering flexible\nselection of problem-specific objectives for various data types, and 3. providing methods to conceal\nsensitive training data, for instance in scenarios involving individual-level genomics and clinical data.\n7\nImplementation on near-term quantum devices\nIt is important to note that implementing our framework on NISQ hardware presents challenges not\naddressed in this manuscript, such as implementing circuits to input amplitude-encoded state vectors\n[23], reading out latent mixed states with sufficient accuracy, and storage of the resulting density\nmatrices. Specifically for our framework, efficient methods are needed for the divergence calculations\nbetween pairs of quantum states in the objective function, and for quantum state tomography, which\nis required for latent state readout and storage. For the latter, while there are several generally\napplicable approaches based on matrix-state tomography [24], neural-network-based tomography\n[25, 26], or the efficient calculation of density matrix properties [12], the question remains of how\nwell these methods scale for the states learned by our model. Frameworks like ours, which primarily\nutilize mixed states, may encounter additional practical difficulties due to the large number of\nparameters required to fully characterize the state, which in turn would impact the number of state\nsamples needed for accurate readout of the states. In our current simulation-based implementation,\nthe latent states are represented by a small number of qubits, but scaling up could demand the\nincorporation of additional methods when applied to NISQ hardware.\n8\nDiscussion\nWe have introduced a novel fully quantum VAE architecture, named ζ-QVAE, which utilizes mixed-\nstate latent representation and provides a flexible framework in which a wide range of quantum\nreconstruction losses and regularizers can be combined in a unified way. Further, a theoretical\nanalysis can be given of the objective functions we introduce, which optimize quantum analogues\nof the variation bounds underlying the classical β-VAE and Wasserstein-AE. A notable feature of\nour framework is that mixed states are treated analogously to classical distributions, significantly\ngeneralizing previous QAE architectures. Our results show that our model outperforms classical\nand alternative QAE models with matched architectures on reconstruction and classification tasks.\nMoreover, we show that the ability to fine-tune the regularization of the latent states allows our\n21\nmodel to optimize its representations for down-stream classification tasks, and there is a complex\ninterplay between regularization and model architecture (including circuit complexity, latent space\ndimensionality and the inclusion of auxiliary qubits) in determining performance on downstream\ntasks. We further show that our model performs consistently well when trained using a global\nmixed-state to represent the data, as opposed to individual pure states per data point, thus indicating\npromising application potential in private and federated learning settings.\nWith such considerations in mind, we propose that our framework may be ideally suited to construct-\ning practical quantum models in application areas involving large-scale, heterogeneous and potentially\nprivacy-aware dataset such as genomics. In future work, we intend to further investigate how to\nutilize the observed interaction between model architecture, explicit and implicit regularization, and\ndownstream task performance from the point of view of representational complexity [27]. Further,\nwe intend to investigate how explicit privacy guarantees and federated versions of our approach\nmay be derived for training our model based on our global objective. Finally, we will investigate\nthe potential of our model to provide efficient compression of intrinsically quantum sources, and\nimplementations of our approach on quantum hardware.\nCode availability\nThe code to run ζ-QVAE is available at https://github.com/gaoyuanwang1976/QVAE.git.\nAcknowledgement\nWe acknowledge support from the NIH and from the AL Williams Professorship funds. We would\nalso like to thank Huan-hsin Tseng and Aram Harrow for valuable discussions.\nReferences\n[1]\nIrina Higgins et al. “beta-vae: Learning basic visual concepts with a constrained variational\nframework”. In: International conference on learning representations. 2016.\n[2]\nIlya Tolstikhin et al. “Wasserstein auto-encoders”. In: arXiv preprint arXiv:1711.01558 (2017).\n[3]\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. “U-net: Convolutional networks\nfor biomedical image segmentation”. In: Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9,\n2015, Proceedings, Part III 18. Springer. 2015, pp. 234–241.\n[4]\nJonathan Ho, Ajay Jain, and Pieter Abbeel. “Denoising diffusion probabilistic models”. In:\nAdvances in neural information processing systems 33 (2020), pp. 6840–6851.\n[5]\nJonathan Romero, Jonathan P Olson, and Alan Aspuru-Guzik. “Quantum autoencoders for\nefficient compression of quantum data”. In: Quantum Science and Technology 2.4 (2017),\np. 045001.\n[6]\nHailan Ma et al. “On compression rate of quantum autoencoders: Control design, numerical\nand experimental realization”. In: Automatica 147 (2023), p. 110659. issn: 0005-1098.\n[7]\nStefano Mangini et al. “Quantum neural network autoencoder and classifier applied to an\nindustrial case study”. In: Quantum Machine Intelligence 4.2 (June 2022). issn: 2524-4914.\n[8]\nCarlos Bravo-Prieto. “Quantum autoencoders with enhanced data encoding”. In: Machine\nLearning: Science and Technology 2.3 (July 2021), p. 035028.\n[9]\nPablo Rivas, Liang Zhao, and Javier Orduz. “Hybrid Quantum Variational Autoencoders for\nRepresentation Learning”. In: 2021 International Conference on Computational Science and\nComputational Intelligence (CSCI). IEEE, Dec. 2021.\n22\n[10]\nAmir Khoshaman et al. “Quantum variational autoencoder”. In: Quantum Science and Tech-\nnology 4.1 (2018), p. 014001.\n[11]\nRichard Jozsa. “Fidelity for Mixed Quantum States”. In: Journal of Modern Optics 41.12\n(1994), pp. 2315–2323.\n[12]\nHsin-Yuan Huang, Richard Kueng, and John Preskill.“Predicting many properties of a quantum\nsystem from very few measurements”. In: Nature Physics 16.10 (June 2020), pp. 1050–1057.\nissn: 1745-2481.\n[13]\nV. Vedral. “The role of relative entropy in quantum information theory”. In: Rev. Mod. Phys.\n74 (1 Mar. 2002), pp. 197–234.\n[14]\nHamza Fawzi and Omar Fawzi. “Efficient optimization of the quantum relative entropy”. In:\nJournal of Physics A: Mathematical and Theoretical 51.15 (2018), p. 154003.\n[15]\nA. P. Majtey, P. W. Lamberti, and D. P. Prato. “Jensen-Shannon divergence as a measure of\ndistinguishability between mixed quantum states”. In: Physical Review A 72.5 (Nov. 2005).\nissn: 1094-1622.\n[16]\nShouvanik Chakrabarti et al. “Quantum Wasserstein generative adversarial networks”. In:\nAdvances in Neural Information Processing Systems 32 (2019).\n[17]\nDiederik P Kingma and Max Welling. “Auto-encoding variational bayes”. In: arXiv preprint\narXiv:1312.6114 (2013).\n[18]\nSeth Lloyd et al. Quantum embeddings for machine learning. 2020. arXiv: 2001.03622 [quant-\nph].\n[19]\nDaifeng Wang et al. “Comprehensive functional genomic resource and integrative model for\nthe human brain”. In: Science 362.6420 (2018), eaat8464.\n[20]\nStephen Marsland. Machine Learning: An Algorithmic Perspective. 2nd. CRC Press, 2014.\nChap. 6.\n[21]\nMichael Ragone et al. Representation Theory for Geometric Quantum Machine Learning. 2023.\narXiv: 2210.07980 [quant-ph].\n[22]\nAndrew Arrasmith et al. “Effect of barren plateaus on gradient-free optimization”. In: Quantum\n5 (Oct. 2021), p. 558. issn: 2521-327X.\n[23]\nIsrael F. Araujo et al. “A divide-and-conquer algorithm for quantum state preparation”. In:\nScientific Reports 11.6329 (2021).\n[24]\nMarcus Cramer et al. “Efficient quantum state tomography”. In: Nature Communications 1.149\n(2010).\n[25]\nGiacomo Torlai et al. “Neural-network quantum state tomography”. In: Nature Physics 14 (5\n2018), pp. 447–450.\n[26]\nJuan Carrasquilla et al. “Reconstructing quantum states with generative models”. In: Nature\nMachine Intelligence 1 (3 2019), pp. 155–161.\n[27]\nMaria Schuld and Francesco Petruccione. Machine learning with quantum computers. Springer,\n2021.\n9\nAppendix A\nWe provide here further details and proofs regarding the theoretical properties of our framework.\nThe first relates to the number of qubits required to achieve arbitrary mappings in our encoder and\ndecoder:\n23\nProposition 1: Setting NA = NB = NZ is sufficient to allow arbitrary pairs of quantum operations\n(E, D) to be learned in our framework.\nProof: An arbitrary quantum channel may be represented by a Choi matrix of rank between 1 and\ndimX · dimY, where X and Y are the input and output Hilbert spaces of the channel respectively.\nFor E, the input and output spaces have dimension 2NX and 2NZ respectively; hence the maximum\nrank of the Choi matrix is 2NX · 2NZ = 2NX+NZ, corresponding to a system requiring NX + NZ\nqubits, which can be achieved by setting NA = NZ. For D, the input and output spaces are\nswapped, and so the maximum rank is again 2NX+NZ, leading to an identical setting for NB.\nSecond, we show that, as in the classical case, the regularized reconstruction loss objective\nwe use is also a lower-bound on the negative quantum relative entropy (the analogue of the\nclassical log-likelihood), when using the quantum relative entropy for both the reconstruction and\nregularization terms in our objective, and setting β = 1 and ϵ = 0.\nProposition 2: −S(ρglob|σgen) ≥ −S(ρglob|σglob) − S(ζglob|ζgen)\nProof:\nWe let ρglob = P\ni pi |vi⟩ ⟨vi|, ζgen = (1/2NZ) P\nj |wj⟩ ⟨wj| and ζglob = E(ρglob) =\nP\nj qj |wj⟩ ⟨wj|. Notice that we choose to express ζgen in the same basis as ζglob, which is possible,\nsince the former is the maximally mixed state, which diagonalizes in any basis. We can express the\nLHS of the proposition as:\n−S(ρglob|σgen)\n=\nTr{ρglob log σgen} + S(ρglob)\n=\nX\ni\npi Tr{|vi⟩ ⟨vi| log σgen} + S(ρglob)\n(24)\nTo derive the proposition, we will bound each of the summands Tr{|vi⟩ ⟨vi| log σgen}. We begin by\nobserving the following:\nTr{|vi⟩ ⟨vi| σgen}\n=\nEj∼Categ(1/2NZ )[Tr{|vi⟩ ⟨vi| D(|wj⟩ ⟨wj|)}]\n=\nEj∼Categ(q1...q2NZ )[Tr{|vi⟩ ⟨vi| D(|wj⟩ ⟨wj|)} · 2−NZ\nqj\n]\n=\nTr\n\u001a\n|vi⟩ ⟨vi| Ej∼Categ(q1...q2NZ )[D(|wj⟩ ⟨wj|) · 2−NZ\nqj\n]\n\u001b\n(25)\nHence, introducing logs and applying Jensen’s trace inequality (lines 2-3), we have:\n24\nTr{|vi⟩ ⟨vi| log σgen}\n= Tr\n\u001a\n|vi⟩ ⟨vi| log Ej∼Categ(q1...q2NZ )[D(|wj⟩ ⟨wj|) · 2−NZ\nqj\n]\n\u001b\n≥ Tr\n\u001a\n|vi⟩ ⟨vi| Ej∼Categ(q1...q2NZ )[log D(|wj⟩ ⟨wj|) · 2−NZ\nqj\n]\n\u001b\n= Tr{|vi⟩ ⟨vi| Ej∼Q[log D(|wj⟩ ⟨wj|)]} − Ej∼Q[log qj] + log 2−NZ\n= Tr{|vi⟩ ⟨vi| log σglob} + S(ζglob) − S(ζgen)\n(26)\nSubstituting Eq. 26 into Eq. 24 and summing across i, we thus have:\n−S(ρglob|σgen)\n≥\nX\ni\npi(Tr{|vi⟩ ⟨vi| log σglob} + S(ζglob) − S(ζgen)) + S(ρglob)\n=\n−S(ρglob|σglob) + S(ζglob) − S(ζgen)\n(27)\nand the proposition follows, since S(ζglob|ζgen) = S(ζgen) − S(ζglob).\nFinally, we show that our global and local objectives are equivalent for linear divergences in the\nfollowing sense:\nProposition 3: Our global and local objectives have identical minimizers for E and D, when they\ncan be expressed in the form given in Eq. 23, and L′\n1 and L2 are linear functions their first arguments.\nProof: We can express ρglob = (1/N) P\ni ρi, and ζglob = (1/N) P\ni E(ρi) = (1/N) P\ni ζi, where ρi\nare the pure states associated with each data-point, and ζi are the associated mixed-state latent\nrepresentations. Hence, if L′\n1 and L2 are linear in their first arguments, we have:\nL′\nglob(θe, θd, β)\n=\nL′\n1(ρglob, E(θe), D(θd)) + βL2(ζglob, ζgen)\n=\n(1/N)\nX\ni\nL′\n1(ρi, E(θe), D(θd)) + (1/N)\nX\ni\nβL2(ζi, ζgen)\n=\n(1/N)L′\ninst(θe, θd, β)\n(28)\nHence, the two objectives are equivalent up to the factor (1/N), leading to identical minimizers.\nIn particular, Prop. 3 implies that setting L′\n1 to the form given in Eq. 17 for the Quantum Wasserstein\nloss, and β = 0 (or setting L2 to the Quantum Wasserstein loss with respect to the ζgen), results in\nidentical global and local optimization problems.\n25\n"
}