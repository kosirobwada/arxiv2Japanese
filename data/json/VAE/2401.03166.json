{
    "optim": "Short-Time Fourier Transform for deblurring Variational\nAutoencoders\nVibhu Dalal\nSri Aurobindo International Centre of Education\nPuducherry, India\nAbstract\nVariational Autoencoders (VAEs) are powerful generative models, however their generated\nsamples are known to suffer from a characteristic blurriness, as compared to the outputs\nof alternative generating techniques. Extensive research efforts have been made to tackle\nthis problem, and several works have focused on modifying the reconstruction term of the\nevidence lower bound (ELBO). In particular, many have experimented with augmenting\nthe reconstruction loss with losses in the frequency domain. Such loss functions usually\nemploy the Fourier transform to explicitly penalise the lack of higher frequency components\nin the generated samples, which are responsible for sharp visual features. In this paper,\nwe explore the aspects of previous such approaches which aren’t well understood, and we\npropose an augmentation to the reconstruction term in response to them. Our reasoning\nleads us to use the short-time Fourier transform and to emphasise on local phase coherence\nbetween the input and output samples. We illustrate the potential of our proposed loss on\nthe MNIST dataset by providing both qualitative and quantitative results.\nKeywords: Generative modelling, Variational Autoencoders, Short-time Fourier trans-\nform\n1. Introduction\nGenerative modelling is a broad area of machine learning that aims to learn a data dis-\ntribution from a given set of samples, such that novel samples can be generated from the\nlearned distribution. With the advent of deep learning, deep generative models, which are\nneural networks trained to approximate complicated, high-dimensional probability distri-\nbutions, have soared in popularity (Ruthotto and Haber (2021), Bond-Taylor et al. (2021)).\nOne such generative modelling approach of particular interest is Variational Bayes, which\nis employed by the Variational Autoencoder (VAE) as introduced by Kingma and Welling\n(2013) to learn a latent variable model.\nVAEs belong to the families of probabilistic graphical models and have become increasingly\npopular due to their strong probabilistic foundations and latent variable model architec-\nture. They have become one of the major areas of research in generative modelling, and\nhave supplanted other autoencoder approaches. In terms of of architecture, they employ\nan encoder-decoder pair and use backpropagation to directly maximise the evidence lower\nbound (ELBO). The term ‘variational’ stems from the variational inference technique which\n1\narXiv:2401.03166v1  [eess.IV]  6 Jan 2024\nis used to regularise the latent space. But despite their popularity and achievements, the\nbehaviour of VAEs is still often far from satisfactory. Several theoretical and practical hin-\ndrances exist and represent active areas of research. The presence of blurriness in generated\nimages is one such drawback of VAEs.\nIn comparison with other generative techniques, VAEs are known to produce images with a\ncharacteristic blurriness. As explained by Bredell et al. (2022), this is mainly due to the for-\nmulation of the optimisation. VAEs are optimised by maximising the evidence lower bound\n(ELBO) Ez∼qϕ(z|x) log[pθ(x|z)]−DKL[qϕ(z|x)||p(z)] with respect to the network parameters\nθ and ϕ. This optimisation objective can equivalently be formulated as minimising the\nKullback-Leibler divergence between the original data distribution qD,ϕ(x, z) and pθ(x, z).\nNow due to the asymmetry of the Kullback-Leibler divergence, the model will be penalised\nheavily if the samples are likely under qD,ϕ(x, z) but not under pθ(x, z). A consequence of\nthis would be that pθ(x, z) will have a larger variance than the original data distribution,\nleading to generated samples being more diverse and potentially more blurry. Several at-\ntempts have been made to resolve this issue by manipulating the reconstruction term, and\none popular method is that of using a frequency loss function in the Fourier domain.\nTo devise a frequency loss function, the Fourier transforms of both the input and out-\nput images are first calculated and then a function is designed to penalise the differences\nin features between the two transforms. The loss function usually penalises the higher fre-\nquencies more since blurriness in an image is often associated with a lack of high-frequency\ncomponents in the frequency spectrum of the image.\nJiang et al. (2021) proposed a simple yet effective frequency loss function called the fo-\ncal frequency loss. They first map each spectrum coordinate value to a Euclidean vector\nin two-dimensional space using both the amplitude and phase information. The proposed\nloss is then defined as the scaled Euclidean distances of the corresponding vectors of input\nand output images. As another instance, Fuoli et al. (2021) first compute the L1 distances\nbetween the frequency amplitudes and phases of the input and output images, and then\nproceed to evaluate the frequency loss which is the sum of the frequency amplitude and\nphase losses. Some other instances of using frequency losses include the works of Bj¨ork\net al. (2022) and Bredell et al. (2022). In all the aforementioned instances, the frequency\nloss functions are used in combination with pixel-wise loss functions. And importantly, they\nall base themselves on the global frequency spectrum of the images.\nInspired by the findings of Wang and Simoncelli (2003), which illustrate that local frequency\ncomponents in the Fourier domain can be more relevant in the preservation of image infor-\nmation than the global frequency components, we propose a novel frequency loss function\nfor the Variational Autoencoder which bases itself on the local frequency amplitude and\nphase values of an image. The insights presented in Wronski (2021) and Jiang et al. (2021)\nfurther help us to frame our loss function. To the best of our knowledge, ours is the only\nsuch work which applies a local frequency loss as a supervision loss for the variational au-\ntoencoder.\n2\nWe use a small-scale VAE and test our proposed loss function on the MNIST dataset,\nand achieve a SSIM score of 0.492 and a PSNR score of 11.93, and show that our method’s\nperformance marginally beats the performances of alternate loss functions. We also provide\na qualitative analysis of the results in 2.\n2. Background\nAs explained in Fuoli et al. (2021), the main advantage of using a frequency loss function is\nthat it explicitly penalises the difference between the high-frequency components of input\nand output images, which promotes the retention of sharp features in the generated image.\nThis is opposed to pixel-wise loss functions such as the L1 or L2 loss functions which are\nknown to be insensitive to blurriness (Bredell et al. (2022)). However, as discussed in the\napproaches mentioned in the previous section, using a frequency loss function in combina-\ntion with a pixel-wise loss function is empirically shown to produce the best results.\nPrevious works that have used frequency loss functions in the training of Variational Au-\ntoencoders employed the global Fourier transform of the input and output images, which is\ngiven by:\nF(u, v) =\n1\n√\nHW\nH−1\nX\nh=0\nW−1\nX\nw=0\nf(h, w)e−i2π( uh\nH + vw\nW )\nwhere the input image is of size H × W, and f(h, w) is the value of the pixel at (h, w).\nAs seen in the previously mentioned works, such frequency loss functions involve the use of\nthe L1 or L2 functions to compute some form of dissimilarity between the global Fourier\ntransforms of the input and output images. As explored in Jiang et al. (2021), a weighting\nscheme is sometimes used to accentuate the differences in the higher frequency values of the\nimages, which corresponds to penalising the output image for not possessing the sharper\nfeatures of the input image. However, there are several details regarding the framing of\nthese loss functions which are not well understood.\nFirstly, there is no clear insight as to which component of the global Fourier transform\n– amplitude or phase – should be emphasised more in the formulation of the frequency loss\nfunction. The aforementioned approaches seem to provide equal emphasis to both compo-\nnents, which might not necessarily be the optimal strategy. Secondly, the advantage of the\nglobal Fourier transform over the local or short-time Fourier transform is not well under-\nstood, especially when it pertains to blurriness. These details might be filled by examining\nthe ideas explored in the works of Murray and Bex (2010) and Wang and Simoncelli (2003).\nMurray and Bex (2010) had previously questioned the traditional view that perceived blur\nin an image is due to reduced energy at high frequencies, and had emphasised the impor-\ntance of the phase of high spatial frequencies. Meanwhile, Wang and Simoncelli (2003)\nargue that the distortions of local phase contribute more to the perception of blurriness\nthan the widely noted loss of high-frequency energy. They also illustrate this qualitatively\nby showing that a sharp image with its high frequency energy reduced but local phase pre-\nserved appears much sharper than a blurred image with its high frequency energy corrected\n3\nbut local phase uncorrected, as shown in 1. In the context of training Variational Autoen-\ncoders, these findings would suggest that it might be advantageous to use the short-time\nFourier transform and accentuate the differences in the local phase values between the input\nand output images in order to emphasise local phase coherence.\nFigure 1: Illustration of the significance of local phase in blur perception.\n(a) original\nimage; (b) blurred image obtained by convolving with a circular-symmetric Gaussian low-\npass filter; (c) image (a) with high-frequency band energy reduced to match that of (b)’s;\n(d) image (b) with high-frequency band energy elevated to match that of (a)’s.\n3. Methodology\n3.1 Frequency loss function\nA β Variational Autoencoder is optimised by maximising the evidence lower bound (ELBO)\nwhich is given by:\nL(q) = Ez∼qϕ(z|x) log[pθ(x|z)] − βDKL[qϕ(z|x)||p(z)]\nThe first term is referred to as the reconstruction loss which ensures that a sample can be\nreconstructed from its latent representation, and the second term is the Kulbeck-Leibler di-\nvergence between the learned approximate posterior distribution and the prior distribution\np(z) which is usually N(0, 1).\n4\nIn order to explicitly minimise blur in the generated samples, we formulate the recon-\nstruction loss as a combination of a frequency loss and a pixel-wise loss. Inspired by the\nfindings discussed in the previous section, we formulate our frequency loss function as fol-\nlows.\nLet F(u, v, m0, n0) be the short-time Fourier transform of an M × N image, which is given\nby:\nF(u, v, m0, n0) =\nM−1\nX\nm=0\nN−1\nX\nn=0\nf(m, n) W(m − m0, n − n0)e−i[u(m−m0)+v(n−n0)]\nwhere W(x, y) is a real valued windowed function centered at (x, y) which is used to avoid\nedge induced artifacts. Inspired by Fuoli et al. (2021), we employ an L × L Hann window\nas the windowed function, which is given by:\nW(x, y) = 1\n4\n\u0012\n1 − cos 2πx\nL\n\u0013 \u0012\n1 − cos 2πy\nL\n\u0013\n, 0 ≤ x, y < L\nand 0 otherwise. Thus computing the short-time Fourier transform of an image amounts to\nsliding the Hann window over all possible (m0, n0) pairs and then computing the discrete\nFourier transform of the resulting image. If hl and hs are the side length and stride of the\nHann window respectively, then the output of the short-time Fourier transform algorithm\nperformed on an M × N image is a K × L × hl × hl matrix where\nK =\n\u0016M − hl\nhs\n\u0017\n+ 1,\nL =\n\u0016N − hl\nhs\n\u0017\n+ 1\nLet Fi and Fo denote the short-time Fourier transforms of the input and output images,\nand let Ai, Pi and Ao, Po denote their respective amplitude and phase components. The\nmatrices Ai and Ao contain the local amplitudes of the input and output images respectively,\nand the matrices Pi and Po contain their local phases. They are given by:\nAih,w = |Fih,w| =\nq\nR(Fih,w)2 + I(Fih,w)2\nPih,w = ∠Fih,w = atan2(I(Fih,w), R(Fih,w))\nNext, we use the L1 function to compute the dissimilarities between Ai and Ao, and Pi\nand Po. Moreover, we use a linearly increasing weighting scheme to emphasise the local\namplitudes and phases pertaining to higher frequencies, in order to explicitly tackle the\nblurriness in the generated image.\nAdditionally, inspired by the findings of Wang and\nSimoncelli (2003), we apply a higher relative weighting to the local phase loss by multiplying\nit with a tunable hyperparametre λ. The resulting frequency loss function becomes:\nLfreq =\nX\nj\n(λ|Poj − Pij| + |Aoj − Aij|)Wj, λ > 1\nwhere Wj is the corresponding weighting term meant to accentuate the losses of higher\nfrequencies.\n5\n3.2 Pixel-wise loss function\nWe experiment with a variety of pixel-wise loss functions such as the L1, L2, sigmoid cross\nentropy and the SSIM loss functions, and we find that the latter seems to work best with our\nproposed frequency loss function. The Structural Similarity Index (SSIM) is a metric for\nperceptual difference that compares two images on the bases of three features: luminance,\ncontrast and structure. The SSIM between two images is a value between -1 and 1, where\nthe former signifies that the given images are very different, and the latter signifies that\nthe images are very similar or the same. The reader may refer to Gilbert et al., Zhao et al.\n(2016) for more information on the SSIM.\nThe resulting reconstruction loss between the input sample Si and the output sample So\ncan now be written as\nLrecons = λfreqLfreq + SSIM(Si, So)\nwhere λfreq is a tunable hyperparametre.\nFinally, the complete loss function of the Variational Autoencoder can be written as:\nL = βDKL[qϕ(z|x)||p(z) + Lrecons\n(1)\n= βDKL[qϕ(z|x)||p(z) + λfreqLfreq + SSIM(Si, So)\n(2)\n4. Experiments and Results\nTo evaluate our model on a benchmark dataset, we used a fixed binarized version of the\nMNIST digit dataset defined by Larochelle and Murray [2011]. It consists of 60,000 train-\ning and 10,000 test images of handwritten digits (0 to 9) which are 28 × 28 pixels in size.\nThe code is written in Python and Tensorflow is used for training the models. We use a\nsimple VAE architecture similar to the one described in Lamberta (2021), which consists\nof 2 convolutional encoding and decoding layers that are connected through a multilayer\nperceptron which does the final compression to the latent variable size, which is 2, and\nthe initial scaling up thereafter. We choose a compact architecture due to our hardware\nlimitations. Details on the architecture can be found in the code that is publicly available\non Github.1\nFurthermore, the Adam optimizer is used with a linearly decreasing cyclical learning rate\nas described in Smith (2017). We set the minimum and maximum learning rates to 10−4\nand 10−3 respectively, and the step size to 2000. We train for 50 epochs with a batch size\nof 50. Additionally, we experiment with a range of short-time Fourier transform window\ndimensions and strides, and find that the best results are achieved with a 16 x 16 window\nand a stride of 4.\nWe used the peak-signal to noise ratio (PSNR) and the structural similarity measure index\n(SSIM) as metrics for comparing the reconstruction with its corresponding input sample.\n1. https://github.com/Vibhu04/Deblurring-Variational-Autoencoders-with-STFT\n6\nWe provide quantitative and qualitative assessments of 3 reconstruction loss functions: 1)\nmean squared error (MSE), 2) SSIM, and 3) the full frequency loss function defined above.\n(a) L2 loss\n(b) SSIM loss\n(c) DFT+SSIM\n(d) Ours\nFigure 2: Samples generated after training the VAE on the MNIST dataset. In (a) we see\nthe characteristic blurriness of VAEs by using MSE as the reconstruction loss term. In (b)\nwe already see an improvement over (a) but the digits still possess a degree of roughness\nand blur. In (c) we notice that both blurriness and roughness have reduced slightly.\n7\nReconstruction loss function\nPSNR\nSSIM\nL2\n10.20\n0.41\nL1\n10.13\n0.32\nSSIM\n11.38\n0.495\nDFT+SSIM\n11.51\n0.486\nOurs\n11.93\n0.492\nTable 1: We provide quantitative results on the MNIST dataset. It can be seen that our\nproposed loss function marginally outperforms the SSIM loss function, which is actually\none of its components.\n5. Conclusion\nIn this paper, we have proposed a reconstruction loss function which employs a frequency\nloss function in the Fourier domain and the SSIM function to minimise the characteristic\nblurriness in the generated samples of a VAE. We differentiate our approach from others\nby using the short-time Fourier transform instead of the global Fourier transform, and by\nemphasising more on local phase coherence than local amplitude differences. We also weigh\nthe differences in higher frequency components between the input and the corresponding\noutput more in order to explicitly encourage the generated samples to have sharper features.\nWe perform experiments on the MNIST dataset to compare the results of our proposed\nreconstruction loss with the results of other popular reconstruction losses, and we provide\nboth qualitative and quantitative evaluations.\nReferences\nSara Bj¨ork, Jonas Nordhaug Myhre, and Thomas Haugland Johansen.\nSimpler is bet-\nter: spectral regularization and up-sampling techniques for variational autoencoders. In\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP), pages 3778–3782. IEEE, 2022.\nSam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks.\nDeep generative\nmodelling: A comparative review of vaes, gans, normalizing flows, energy-based and\nautoregressive models. IEEE transactions on pattern analysis and machine intelligence,\n2021.\nGustav Bredell, Kyriakos Flouris, Krishna Chaitanya, Ertunc Erdil, and Ender Konukoglu.\nExplicitly minimizing the blur error of variational autoencoders. In Submission ICLR\n2023, 2022.\nDario Fuoli, Luc Van Gool, and Radu Timofte. Fourier space losses for efficient perceptual\nimage super-resolution. In Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 2360–2369, 2021.\nAndy Gilbert, Shai Messingher, and Anirudh Patel.\nNon-blind image deblurring using\nneural networks.\n8\nLiming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal frequency loss for image\nreconstruction and synthesis. In Proceedings of the IEEE/CVF International Conference\non Computer Vision, pages 13919–13929, 2021.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\nWilliam Chen Lamberta, Mark Daoust. Convolutional Variational Autoencoder, 2021. URL\nhttps://www.tensorflow.org/tutorials/generative/cvae.\nStephanie Murray and Peter J Bex. Perceived blur in naturally contoured images depends\non phase. Frontiers in psychology, 1:185, 2010.\nLars Ruthotto and Eldad Haber. An introduction to deep generative modeling. GAMM-\nMitteilungen, 44(2):e202100008, 2021.\nLeslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter\nconference on applications of computer vision (WACV), pages 464–472. IEEE, 2017.\nZhou Wang and Eero Simoncelli. Local phase coherence and the perception of blur. Advances\nin neural information processing systems, 16, 2003.\nBart\nWronski.\nComparing\nimages\nin\nfrequency\ndomain.\n“Spectral\nloss”\n–\ndoes\nit\nmake\nsense?,\n2021.\nURL\nhttps://bartwronski.com/2021/07/06/\ncomparing-images-in-frequency-domain-spectral-loss-does-it-make-sense/.\nHang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration\nwith neural networks. IEEE Transactions on computational imaging, 3(1):47–57, 2016.\n9\n"
}