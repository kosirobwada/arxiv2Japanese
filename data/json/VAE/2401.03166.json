{
    "optim": "Short-Time Fourier Transform for deblurring Variational Autoencoders Vibhu Dalal Sri Aurobindo International Centre of Education Puducherry, India Abstract Variational Autoencoders (VAEs) are powerful generative models, however their generated samples are known to suffer from a characteristic blurriness, as compared to the outputs of alternative generating techniques. Extensive research efforts have been made to tackle this problem, and several works have focused on modifying the reconstruction term of the evidence lower bound (ELBO). In particular, many have experimented with augmenting the reconstruction loss with losses in the frequency domain. Such loss functions usually employ the Fourier transform to explicitly penalise the lack of higher frequency components in the generated samples, which are responsible for sharp visual features. In this paper, we explore the aspects of previous such approaches which aren’t well understood, and we propose an augmentation to the reconstruction term in response to them. Our reasoning leads us to use the short-time Fourier transform and to emphasise on local phase coherence between the input and output samples. We illustrate the potential of our proposed loss on the MNIST dataset by providing both qualitative and quantitative results. Keywords: Generative modelling, Variational Autoencoders, Short-time Fourier trans- form 1. Introduction Generative modelling is a broad area of machine learning that aims to learn a data dis- tribution from a given set of samples, such that novel samples can be generated from the learned distribution. With the advent of deep learning, deep generative models, which are neural networks trained to approximate complicated, high-dimensional probability distri- butions, have soared in popularity (Ruthotto and Haber (2021), Bond-Taylor et al. (2021)). One such generative modelling approach of particular interest is Variational Bayes, which is employed by the Variational Autoencoder (VAE) as introduced by Kingma and Welling (2013) to learn a latent variable model. VAEs belong to the families of probabilistic graphical models and have become increasingly popular due to their strong probabilistic foundations and latent variable model architec- ture. They have become one of the major areas of research in generative modelling, and have supplanted other autoencoder approaches. In terms of of architecture, they employ an encoder-decoder pair and use backpropagation to directly maximise the evidence lower bound (ELBO). The term ‘variational’ stems from the variational inference technique which 1 arXiv:2401.03166v1  [eess.IV]  6 Jan 2024 is used to regularise the latent space. But despite their popularity and achievements, the behaviour of VAEs is still often far from satisfactory. Several theoretical and practical hin- drances exist and represent active areas of research. The presence of blurriness in generated images is one such drawback of VAEs. In comparison with other generative techniques, VAEs are known to produce images with a characteristic blurriness. As explained by Bredell et al. (2022), this is mainly due to the for- mulation of the optimisation. VAEs are optimised by maximising the evidence lower bound (ELBO) Ez∼qϕ(z|x) log[pθ(x|z)]−DKL[qϕ(z|x)||p(z)] with respect to the network parameters θ and ϕ. This optimisation objective can equivalently be formulated as minimising the Kullback-Leibler divergence between the original data distribution qD,ϕ(x, z) and pθ(x, z). Now due to the asymmetry of the Kullback-Leibler divergence, the model will be penalised heavily if the samples are likely under qD,ϕ(x, z) but not under pθ(x, z). A consequence of this would be that pθ(x, z) will have a larger variance than the original data distribution, leading to generated samples being more diverse and potentially more blurry. Several at- tempts have been made to resolve this issue by manipulating the reconstruction term, and one popular method is that of using a frequency loss function in the Fourier domain. To devise a frequency loss function, the Fourier transforms of both the input and out- put images are first calculated and then a function is designed to penalise the differences in features between the two transforms. The loss function usually penalises the higher fre- quencies more since blurriness in an image is often associated with a lack of high-frequency components in the frequency spectrum of the image. Jiang et al. (2021) proposed a simple yet effective frequency loss function called the fo- cal frequency loss. They first map each spectrum coordinate value to a Euclidean vector in two-dimensional space using both the amplitude and phase information. The proposed loss is then defined as the scaled Euclidean distances of the corresponding vectors of input and output images. As another instance, Fuoli et al. (2021) first compute the L1 distances between the frequency amplitudes and phases of the input and output images, and then proceed to evaluate the frequency loss which is the sum of the frequency amplitude and phase losses. Some other instances of using frequency losses include the works of Bj¨ork et al. (2022) and Bredell et al. (2022). In all the aforementioned instances, the frequency loss functions are used in combination with pixel-wise loss functions. And importantly, they all base themselves on the global frequency spectrum of the images. Inspired by the findings of Wang and Simoncelli (2003), which illustrate that local frequency components in the Fourier domain can be more relevant in the preservation of image infor- mation than the global frequency components, we propose a novel frequency loss function for the Variational Autoencoder which bases itself on the local frequency amplitude and phase values of an image. The insights presented in Wronski (2021) and Jiang et al. (2021) further help us to frame our loss function. To the best of our knowledge, ours is the only such work which applies a local frequency loss as a supervision loss for the variational au- toencoder. 2 We use a small-scale VAE and test our proposed loss function on the MNIST dataset, and achieve a SSIM score of 0.492 and a PSNR score of 11.93, and show that our method’s performance marginally beats the performances of alternate loss functions. We also provide a qualitative analysis of the results in 2. 2. Background As explained in Fuoli et al. (2021), the main advantage of using a frequency loss function is that it explicitly penalises the difference between the high-frequency components of input and output images, which promotes the retention of sharp features in the generated image. This is opposed to pixel-wise loss functions such as the L1 or L2 loss functions which are known to be insensitive to blurriness (Bredell et al. (2022)). However, as discussed in the approaches mentioned in the previous section, using a frequency loss function in combina- tion with a pixel-wise loss function is empirically shown to produce the best results. Previous works that have used frequency loss functions in the training of Variational Au- toencoders employed the global Fourier transform of the input and output images, which is given by: F(u, v) = 1 √ HW H−1 X h=0 W−1 X w=0 f(h, w)e−i2π( uh H + vw W ) where the input image is of size H × W, and f(h, w) is the value of the pixel at (h, w). As seen in the previously mentioned works, such frequency loss functions involve the use of the L1 or L2 functions to compute some form of dissimilarity between the global Fourier transforms of the input and output images. As explored in Jiang et al. (2021), a weighting scheme is sometimes used to accentuate the differences in the higher frequency values of the images, which corresponds to penalising the output image for not possessing the sharper features of the input image. However, there are several details regarding the framing of these loss functions which are not well understood. Firstly, there is no clear insight as to which component of the global Fourier transform – amplitude or phase – should be emphasised more in the formulation of the frequency loss function. The aforementioned approaches seem to provide equal emphasis to both compo- nents, which might not necessarily be the optimal strategy. Secondly, the advantage of the global Fourier transform over the local or short-time Fourier transform is not well under- stood, especially when it pertains to blurriness. These details might be filled by examining the ideas explored in the works of Murray and Bex (2010) and Wang and Simoncelli (2003). Murray and Bex (2010) had previously questioned the traditional view that perceived blur in an image is due to reduced energy at high frequencies, and had emphasised the impor- tance of the phase of high spatial frequencies. Meanwhile, Wang and Simoncelli (2003) argue that the distortions of local phase contribute more to the perception of blurriness than the widely noted loss of high-frequency energy. They also illustrate this qualitatively by showing that a sharp image with its high frequency energy reduced but local phase pre- served appears much sharper than a blurred image with its high frequency energy corrected 3 but local phase uncorrected, as shown in 1. In the context of training Variational Autoen- coders, these findings would suggest that it might be advantageous to use the short-time Fourier transform and accentuate the differences in the local phase values between the input and output images in order to emphasise local phase coherence. Figure 1: Illustration of the significance of local phase in blur perception. (a) original image; (b) blurred image obtained by convolving with a circular-symmetric Gaussian low- pass filter; (c) image (a) with high-frequency band energy reduced to match that of (b)’s; (d) image (b) with high-frequency band energy elevated to match that of (a)’s. 3. Methodology 3.1 Frequency loss function A β Variational Autoencoder is optimised by maximising the evidence lower bound (ELBO) which is given by: L(q) = Ez∼qϕ(z|x) log[pθ(x|z)] − βDKL[qϕ(z|x)||p(z)] The first term is referred to as the reconstruction loss which ensures that a sample can be reconstructed from its latent representation, and the second term is the Kulbeck-Leibler di- vergence between the learned approximate posterior distribution and the prior distribution p(z) which is usually N(0, 1). 4 In order to explicitly minimise blur in the generated samples, we formulate the recon- struction loss as a combination of a frequency loss and a pixel-wise loss. Inspired by the findings discussed in the previous section, we formulate our frequency loss function as fol- lows. Let F(u, v, m0, n0) be the short-time Fourier transform of an M × N image, which is given by: F(u, v, m0, n0) = M−1 X m=0 N−1 X n=0 f(m, n) W(m − m0, n − n0)e−i[u(m−m0)+v(n−n0)] where W(x, y) is a real valued windowed function centered at (x, y) which is used to avoid edge induced artifacts. Inspired by Fuoli et al. (2021), we employ an L × L Hann window as the windowed function, which is given by: W(x, y) = 1 4 \u0012 1 − cos 2πx L \u0013 \u0012 1 − cos 2πy L \u0013 , 0 ≤ x, y < L and 0 otherwise. Thus computing the short-time Fourier transform of an image amounts to sliding the Hann window over all possible (m0, n0) pairs and then computing the discrete Fourier transform of the resulting image. If hl and hs are the side length and stride of the Hann window respectively, then the output of the short-time Fourier transform algorithm performed on an M × N image is a K × L × hl × hl matrix where K = \u0016M − hl hs \u0017 + 1, L = \u0016N − hl hs \u0017 + 1 Let Fi and Fo denote the short-time Fourier transforms of the input and output images, and let Ai, Pi and Ao, Po denote their respective amplitude and phase components. The matrices Ai and Ao contain the local amplitudes of the input and output images respectively, and the matrices Pi and Po contain their local phases. They are given by: Aih,w = |Fih,w| = q R(Fih,w)2 + I(Fih,w)2 Pih,w = ∠Fih,w = atan2(I(Fih,w), R(Fih,w)) Next, we use the L1 function to compute the dissimilarities between Ai and Ao, and Pi and Po. Moreover, we use a linearly increasing weighting scheme to emphasise the local amplitudes and phases pertaining to higher frequencies, in order to explicitly tackle the blurriness in the generated image. Additionally, inspired by the findings of Wang and Simoncelli (2003), we apply a higher relative weighting to the local phase loss by multiplying it with a tunable hyperparametre λ. The resulting frequency loss function becomes: Lfreq = X j (λ|Poj − Pij| + |Aoj − Aij|)Wj, λ > 1 where Wj is the corresponding weighting term meant to accentuate the losses of higher frequencies. 5 3.2 Pixel-wise loss function We experiment with a variety of pixel-wise loss functions such as the L1, L2, sigmoid cross entropy and the SSIM loss functions, and we find that the latter seems to work best with our proposed frequency loss function. The Structural Similarity Index (SSIM) is a metric for perceptual difference that compares two images on the bases of three features: luminance, contrast and structure. The SSIM between two images is a value between -1 and 1, where the former signifies that the given images are very different, and the latter signifies that the images are very similar or the same. The reader may refer to Gilbert et al., Zhao et al. (2016) for more information on the SSIM. The resulting reconstruction loss between the input sample Si and the output sample So can now be written as Lrecons = λfreqLfreq + SSIM(Si, So) where λfreq is a tunable hyperparametre. Finally, the complete loss function of the Variational Autoencoder can be written as: L = βDKL[qϕ(z|x)||p(z) + Lrecons (1) = βDKL[qϕ(z|x)||p(z) + λfreqLfreq + SSIM(Si, So) (2) 4. Experiments and Results To evaluate our model on a benchmark dataset, we used a fixed binarized version of the MNIST digit dataset defined by Larochelle and Murray [2011]. It consists of 60,000 train- ing and 10,000 test images of handwritten digits (0 to 9) which are 28 × 28 pixels in size. The code is written in Python and Tensorflow is used for training the models. We use a simple VAE architecture similar to the one described in Lamberta (2021), which consists of 2 convolutional encoding and decoding layers that are connected through a multilayer perceptron which does the final compression to the latent variable size, which is 2, and the initial scaling up thereafter. We choose a compact architecture due to our hardware limitations. Details on the architecture can be found in the code that is publicly available on Github.1 Furthermore, the Adam optimizer is used with a linearly decreasing cyclical learning rate as described in Smith (2017). We set the minimum and maximum learning rates to 10−4 and 10−3 respectively, and the step size to 2000. We train for 50 epochs with a batch size of 50. Additionally, we experiment with a range of short-time Fourier transform window dimensions and strides, and find that the best results are achieved with a 16 x 16 window and a stride of 4. We used the peak-signal to noise ratio (PSNR) and the structural similarity measure index (SSIM) as metrics for comparing the reconstruction with its corresponding input sample. 1. https://github.com/Vibhu04/Deblurring-Variational-Autoencoders-with-STFT 6 We provide quantitative and qualitative assessments of 3 reconstruction loss functions: 1) mean squared error (MSE), 2) SSIM, and 3) the full frequency loss function defined above. (a) L2 loss (b) SSIM loss (c) DFT+SSIM (d) Ours Figure 2: Samples generated after training the VAE on the MNIST dataset. In (a) we see the characteristic blurriness of VAEs by using MSE as the reconstruction loss term. In (b) we already see an improvement over (a) but the digits still possess a degree of roughness and blur. In (c) we notice that both blurriness and roughness have reduced slightly. 7 Reconstruction loss function PSNR SSIM L2 10.20 0.41 L1 10.13 0.32 SSIM 11.38 0.495 DFT+SSIM 11.51 0.486 Ours 11.93 0.492 Table 1: We provide quantitative results on the MNIST dataset. It can be seen that our proposed loss function marginally outperforms the SSIM loss function, which is actually one of its components. 5. Conclusion In this paper, we have proposed a reconstruction loss function which employs a frequency loss function in the Fourier domain and the SSIM function to minimise the characteristic blurriness in the generated samples of a VAE. We differentiate our approach from others by using the short-time Fourier transform instead of the global Fourier transform, and by emphasising more on local phase coherence than local amplitude differences. We also weigh the differences in higher frequency components between the input and the corresponding output more in order to explicitly encourage the generated samples to have sharper features. We perform experiments on the MNIST dataset to compare the results of our proposed reconstruction loss with the results of other popular reconstruction losses, and we provide both qualitative and quantitative evaluations. References Sara Bj¨ork, Jonas Nordhaug Myhre, and Thomas Haugland Johansen. Simpler is bet- ter: spectral regularization and up-sampling techniques for variational autoencoders. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Pro- cessing (ICASSP), pages 3778–3782. IEEE, 2022. Sam Bond-Taylor, Adam Leach, Yang Long, and Chris G Willcocks. Deep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models. IEEE transactions on pattern analysis and machine intelligence, 2021. Gustav Bredell, Kyriakos Flouris, Krishna Chaitanya, Ertunc Erdil, and Ender Konukoglu. Explicitly minimizing the blur error of variational autoencoders. In Submission ICLR 2023, 2022. Dario Fuoli, Luc Van Gool, and Radu Timofte. Fourier space losses for efficient perceptual image super-resolution. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2360–2369, 2021. Andy Gilbert, Shai Messingher, and Anirudh Patel. Non-blind image deblurring using neural networks. 8 Liming Jiang, Bo Dai, Wayne Wu, and Chen Change Loy. Focal frequency loss for image reconstruction and synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13919–13929, 2021. Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. William Chen Lamberta, Mark Daoust. Convolutional Variational Autoencoder, 2021. URL https://www.tensorflow.org/tutorials/generative/cvae. Stephanie Murray and Peter J Bex. Perceived blur in naturally contoured images depends on phase. Frontiers in psychology, 1:185, 2010. Lars Ruthotto and Eldad Haber. An introduction to deep generative modeling. GAMM- Mitteilungen, 44(2):e202100008, 2021. Leslie N Smith. Cyclical learning rates for training neural networks. In 2017 IEEE winter conference on applications of computer vision (WACV), pages 464–472. IEEE, 2017. Zhou Wang and Eero Simoncelli. Local phase coherence and the perception of blur. Advances in neural information processing systems, 16, 2003. Bart Wronski. Comparing images in frequency domain. “Spectral loss” – does it make sense?, 2021. URL https://bartwronski.com/2021/07/06/ comparing-images-in-frequency-domain-spectral-loss-does-it-make-sense/. Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on computational imaging, 3(1):47–57, 2016. 9 "
}