{
    "optim": "Tighter Bounds on the Information Bottleneck with Application to Deep\nLearning\nNir Weingarten 1 Zohar Yakhini 1 Moshe Butman 1 Ran Gilad-Bachrach 2\nAbstract\nDeep Neural Nets (DNNs) learn latent representa-\ntions induced by their downstream task, objective\nfunction, and other parameters. The quality of\nthe learned representations impacts the DNN’s\ngeneralization ability and the coherence of the\nemerging latent space. The Information Bottle-\nneck (IB) provides a hypothetically optimal frame-\nwork for data modeling, yet it is often intractable.\nRecent efforts combined DNNs with the IB by\napplying VAE-inspired variational methods to\napproximate bounds on mutual information, re-\nsulting in improved robustness to adversarial at-\ntacks. This work introduces a new and tighter\nvariational bound for the IB, improving perfor-\nmance of previous IB-inspired DNNs. These ad-\nvancements strengthen the case for the IB and\nits variational approximations as a data modeling\nframework, and provide a simple method to sig-\nnificantly enhance the adversarial robustness of\nclassifier DNNs.\n1. Introduction\nIn recent years, Deep Neural Networks (DNNs) have gained\nprominence in various learning tasks, revolutionizing many\ncomputational fields with their ability to approximate com-\nplex functions. Despite the great achievements, it is still\npostulated that the current networks are prone to overfit\nthe training data (Ying, 2019), may be considerably uncal-\nibrated (Guo et al., 2017) and are susceptible to adversar-\nial attacks (Goodfellow et al., 2015). A question emerges\nregarding the extraction of an optimal representation for\nall data points from a restricted set of training examples.\nClassic information theory provides tools to optimize com-\npression and transmission of data, but it does not provide\nmethods to gauge the relevance of a compressed signal to its\n1Efi Arzi School of Computer Science, Reichman University,\nHerzliya, Israel 2Department of Biomedical Engineering, Tel-Aviv\nUniversity, Tel-Aviv, Israel. Correspondence to: Nir Weingarten\n<nir.weingarten@runi.ac.il>.\narXiv preprint, Copyright 2024 by the author(s).\ndownstream task. Methods such as rate-distortion (Blahut,\n1972) regard all information as equal, not taking into account\nwhich information is more relevant without constructing\ncomplex distortion functions. The Information Bottleneck\n(IB) (Tishby et al., 1999) resolves this limitation by defin-\ning mutual information between the learned representation\nand the downstream task as a universal distortion function.\nUnder this definition, an optimal rate-disotrion ratio can\nbe implicitly computed for a Lagrange multiplier β, con-\ntrolling the tradeoff between the desired rate and distortion.\nHowever, optimizing over the IB requires mutual informa-\ntion computations, which are tractable in discrete settings\nand for some specific continuous distributions. Adopting\nthe IB framework for DNNs requires computing mutual\ninformation for unknown distributions and has no analytic\nsolution. However, recent work approximated tractable\nupper bounds for the IB functional in DNN settings us-\ning variational approximations. Variational Auto Encoders\n(VAEs) (Kingma & Welling, 2014) use stochastic DNNs\nto approximate intractable distributions, as elaborated in\nSection 2.3. Similarly to VAEs, Alemi et al. (2017) pro-\nposed using stochastic DNNs as variational approximations\nof latent models, thus making possible the computations of\nupper bounds for mutual information between the DNN’s\ninput, output and latent representation. A proposed DNN\noptimization method called Deep Variational Information\nBottleneck (VIB) derives an upper bound for the IB ob-\njective and minimizes its approximation by fitting some\ntraining dataset. Optimizing classifier DNNs with the VIB\nobjective results in a slight decrease in test set accuracy\ncompared to deterministic DNNs, but yields a significant\nincrease in robustness to adversarial attacks.\nIn this study, we adopt the same information theoretic and\nvariational approach proposed in VIB. The work begins\nby deriving a new upper bound for the IB functional. We\nthen employ a tractable variational approximation for this\nbound, named VUB - ’Variational Upper Bound’ and show\nthat it is a tighter bound on the IB objective than VIB. We\nproceed to show empirical evidence that VUB substantially\nincreases test set accuracy over VIB while providing similar\nor superior robustness to adversarial attacks across several\nchallenging tasks and different modalities. Finally, we dis-\ncuss these effects in the context of previous work on the\n1\narXiv:2402.07639v1  [cs.LG]  12 Feb 2024\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nIB and on DNN regularization. The conclusion drawn is\nthat while increasing mutual information between encod-\ning and output does not necessarily improve classification,\nand while increasing encoding compression does not al-\nways enhance regularization (Amjad & Geiger, 2020), the\napplication of IB approximations as objectives to DNNs\nempirically improves regularization, suggesting better data\nmodeling. This notion contributes for the adaptation of the\nIB, and its variational approximations, as an objective for\nlearning tasks and as a theoretic framework to gauge and\nexplain data modeling.\nIn addition, we demonstrate that VUB can be easily adapted\nto any classifier DNN, including transformer based NLP\nclassifiers, to substantially increase robustness to adversarial\nattacks while only slightly decreasing, or in some cases even\nincreasing, test set accuracy.\n1.1. Preliminaries\nThe following literature review and derivations refer to in-\nformation theory and variational approximations. A prelim-\ninary mutual ground and notation is provided.\nWe denote random variables (RVs) with upper cased letters\nX, Y , and their realizations in lower case x, y. Denote dis-\ncrete Probability Mass Functions (PMFs) with an upper case\nP(x) and continuous Probability Density Functions (PDFs)\nwith a lower case p(x). Hat notation denotes empirical\nmeasurements.\nLet X, Y be two observed random variables with unknown\ndistributions p∗(x), p∗(y) that we aim to model. Assume\nX, Y are governed by some unknown underlying process\nwith a joint probability distribution p∗(x, y). We can attempt\nto approximate these distributions using a model pθ with pa-\nrameters θ such that for generative tasks pθ(x) ≈ p∗(x)\nand for discriminative tasks pθ(y|x) ≈ p∗(y|x), using\na dataset S = {(x1, y1), ..., (xN, yN)} to fit our model.\nOne can also assume the existence of an additional unob-\nserved RV Z ∼ p∗(z) that influences or generates the ob-\nserved RVs X, Y . Since Z is unobserved it is absent from\nthe dataset S and so cannot be modeled directly. Denote\nR\npθ(x|z)pθ(z)dz the marginal, pθ(z) the prior as it is not\nconditioned over any other RV, and pθ(z|x) the posterior\nfollowing Bayes’ rule.\nWhen modeling an unobserved variable of an unknown dis-\ntribution we encounter a problem as the marginal pθ(x) =\nR\npθ(x, z)dz doesn’t have an analytic solution. This in-\ntractability can be overcome by choosing some tractable\nparametric variational distribution qϕ(z|x) to approximate\nthe posterior pθ(z|x) such that qϕ(z|x) ≈ pθ(z|x), and esti-\nmate pθ(x, z) or pθ(x, z|y) by fitting the dataset S (Kingma\n& Welling, 2019).\nIn this work information theoretic functions share the same\nnotation for discrete and continuous settings. For brevity,\nwe will only present the continuous form:\nEntropy\nHp(X) = −\nR\np(x)log (p(x)) dx\nCross\nEntropy\nCE(p, q) = −\nR\np(x)log (q(x)) dx\nKL\nDivergence\nDKL\n\u0000p\n\f\f\f\fq\n\u0001\n=\nR\np(x)log\n\u0010\np(x)\nq(x)\n\u0011\ndx\nMutual\nInformation\nI(X; Y ) =\nR R\np(x, y)log\n\u0010\np(x,y)\np(x)p(y)\n\u0011\ndxdy\n2. Related work\n2.1. IB and its analytic solutions\nClassic information theory offers rate-distortion (Blahut,\n1972) to mitigate signal loss during compression. Rate\nbeing the signal’s compression measured by mutual infor-\nmation between input and output signals, and distortion a\nchosen task-specific function. The Information Bottleneck\n(IB) method (Tishby et al., 1999) extends rate-distortion by\nreplacing the tailored distortion functions with mutual in-\nformation between the learned representation and the down-\nstream task. Denote X the source signal, Z its encoding\nand Y the target signal for some specific task. Assum-\ning a latent variable model that follows the Markov chain\nZ ↔ X ↔ Y , we define some positive minimal threshold\nD for the desired distortion. We seek an optimal encoding\nZ :\nmin\nP (Z|X)I(X; Z) subject to I(Z; Y ) ≥ D. This con-\nstrained problem can be implicitly optimized by minimizing\nthe functional LP (Z|X) = I(Z; X) − βI(Z; Y ), the first\nterm being rate and the second distortion modulated by the\nLagrange multiplier β. The optimal solution is a function of\nβ and was named ’the information curve’ as illustrated in\nFigure 1. The IB can be interpreted as a method to learn a\nrepresentation that holds just enough information to satisfy\na desired task, while discarding all other available informa-\ntion, presumably providing a model with the least possible\ncomplexity.\nThe IB functional is only tractable when mutual informa-\ntion can be computed and was originally demonstrated for\nsoft clustering tasks over a discrete and known distribution\nP ∗(x, y). Chechik et al. (2003) extended the IB for gaus-\nsian distributions and Painsky & Tishby (2017) offered a\nlimited linear approximation of the IB for any distribution.\n2.2. IB and deep learning\nTishby & Zaslavsky (2015) proposed an IB interpretation\nof DNNs, regarding them as Markov cascades of interme-\n2\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nFigure 1. The information plane and curve: rate-distortion ratio\nover β. At β = 0 the representation is compressed but uninfor-\nmative (maximal compression), at β → ∞ the representation\nis informative but potentially overfitted (maximal information).\nAdapted from (Slonim, 2002).\ndiate representations between hidden layers. Under this\nframework, comparing the optimal and the achieved rate-\ndistortion ratios between DNN layers will indicate if a model\nis too complex or too simple for a given task and training\nset. Shwartz-Ziv & Tishby (2017) visualized and analyzed\nthe information plane behavior of DNNs over a toy prob-\nlem with a known joint distribution. Mutual information\nof the different layers was estimated and used to analyze\nthe training process. The learning process over Stochastic\nGradient Descent (SGD) exhibited two separate and sequen-\ntial behaviors: A short Empirical Error Minimization phase\n(ERM) characterized by a rapid decrease in distortion, fol-\nlowed by a long compression phase with an increase in rate\nuntil convergence to an optimal IB limit as demonstrated in\nFigure 2. Similar yet repetitive behavior was observed in\nthe current study, as elaborated in Section 4.2.1.\nAmjad & Geiger (2020) pointed out three flaws in the us-\nage of the IB functional as an objective for deterministic\nDNN classifiers: (1) When data X is absolutely continuous\nthe mutual information term I(X; Z) is infinite; (2) When\ndata X is discrete the IB functional is a piecewise constant\nfunction of the parameters, making it’s SGD optimization\ndifficult or impossible; (3) Equivalent representations might\nyield the same IB loss while one achieved better classifica-\ntion rate than the other. These discrepancies were attributed\nto mutual information’s invariance to invertible transfor-\nmations and to the absence of a decision function in the\nobjective.\nFigure 2. Information plane scatters of different DNN layers (col-\nors) in 50 randomized networks. From Shwartz-Ziv & Tishby\n(2017). Left are initial weights, Right are at 400 epochs. Our study\nreproduced similar yet repetitive behavior on complicated high\ndimensional tasks, as elaborated in Section 4.2.1 and in Figure 3.\n2.3. Variational approximations to the IB objective\nKingma & Welling (2014) introduced the Variational Auto\nEncoder (VAE) - a stochastic generative DNN. An unob-\nserved RV Z is assumed to generate evidence X and the\ntrue probability p∗(x) can be modeled using a paramet-\nric model over the marginal pθ(x) =\nR\npθ(x|z)pθ(z)dz.\nHowever, since the marginal is intractable a variational\napproximation qϕ(z|x) ≈ pθ(z|x) is proposed instead.\nThe log probability log (pθ(x)) is then developed in to\nthe tractable VAE loss comprised of the Evidence Lower\nBound (ELBO) and KL regularization terms:\nL\nELBO =\nEqϕ(z|x) [log (pθ(x|z))] − DKL\n\u0000qϕ(z|x)\n\f\f\f\fpθ(z)\n\u0001\n. qϕ(z|x)\nis modeled using a stochastic neural encoder having it’s final\nactivation used as parameters for the assumed variational\ndistribution (typically a spherical gaussian with parameters\nµ, Σ). Each forward pass emulates a stochastic realization\nz ∈ Z from these parameters by using the ’reparameter-\nization trick’: z = µ + ϵ · Σ for some unparameterized\nscalar ϵ ∼ N(0, 1) such that a backwards pass is possible.\nHiggins et al. (2017) later proposed the β-autoencoder, intro-\nducing a hyper parameter β over the KL term to control the\nregularization-reconstruction tradeoff. Alemi et al. (2018)\nfound that as the ELBO loss in VAEs depends solely on\nimage reconstruction it does not necessarily induce a better\nquality modeling of the marginal pθ(z), hence not neces-\nsarily a better representation learned. This gap is attributed\nto powerful decoders being overfitted, as will be further\ndiscussed in Section 5.\nAlemi et al. (2017) introduced the Variational Information\nBottleneck (VIB) as a variational approximation for an\nupper bound to the IB objective for classifier DNN opti-\nmization. Bounds for I(Z, Y ) and I(X, Z) are derived\nfrom the non negativity of KL divergence and are used to\nform an upper bound for the IB functional. This upper\n3\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nbound is approximated using variational approximations for\np∗(y|z), p∗(z) as done in VAEs. This approximation of an\nupper bound for the IB objective is empirically estimated\nas cross entropy and a beta scaled KL regularization term\nas in β-autoencoders, and is optimized over the training\ndata using Monte Carlo sampling and the reparameteriza-\ntion trick. VIB was evaluated over image classification tasks\nand, while causing a slight reduction in test set accuracy,\ngenerated substantial improvements in robustness to adver-\nsarial attacks.\nAdditional noteworthy contributions to this field have been\nmade in recent years by Achille & Soatto (2018); Wieczorek\n& Roth (2019); Fischer (2020) and others. However, a\ndetailed review of these works is beyond the scope of this\npaper.\n2.4. Non IB information theoretic regularization\nLabel smoothing (Szegedy et al., 2016) and entropy regular-\nization (Pereyra et al., 2017) both regularize classifier DNNs\nby increasing the entropy of their output. This is done either\ndirectly by inserting a scaled conditional entropy term to the\nloss function, −γ · H (pθ(y|x)), or by smoothing the train-\ning data labels. Applying both methods was demonstrated\nto improve test accuracy and model calibration on various\nchallenging classification tasks. In the current work a sim-\nilar conditional entropy term emerges from the derivation\nof the new upper bound for the IB objective, as shown in\nSection 3.\n3. From VIB to VUB\nThe VIB loss consists of a cross entropy term and a KL\nregularization term, as in VAE loss. The KL term is de-\nrived from a bound on the IB rate term I(X; Z), while the\ncross entropy term from a bound on the IB distortion term\nI(Z; Y ) = H(Y ) − H(Y |Z). When deriving the latter the\nentropy term H(Y ) is ignored as it is constant and does not\neffect optimization. We note that since Y is unknown any\noptimization over Z, including cross entropy, depends on\nour decoder model of Y . Following this logic, instead of\nomitting H(Y ) we replace it with a variational approxima-\ntion of the decoder entropy, which provides a lower bound.\n3.1. IB upper bound\nWe begin by establishing a new upper bound for the IB\nfunctional by bounding the mutual information terms, using\nthe same method as in VIB.\nConsider I(Z; X):\nI(Z; X) =\nZ Z\np∗(x, z)log (p∗(z|x)) dxdz\n−\nZ\np∗(z)log (p∗(z)) dz\n(1)\nFor\nany\nprobability\ndistribution\nr\nwe\nhave\nthat\nDKL\n\u0000p∗(z)\n\f\f\f\fr(z)\n\u0001\n≥ 0, it follows that:\nZ\np∗(z)log (p∗(z)) dz ≥\nZ\np∗(z)log (r(z)) dz\n(2)\nAnd so by Equation 2:\nI(Z; X) ≤\nZ Z\np∗(x)p∗(z|x)log\n\u0012p∗(z|x)\nr(z)\n\u0013\ndxdz (3)\nConsider I(Z; Y ):\nFor\nany\nprobability\ndistribution\nc\nwe\nhave\nthat\nDKL\n\u0000p∗(y|z)\n\f\f\f\fc(y|z)\n\u0001\n≥ 0, it follows that:\nZ\np∗(y|z)log (p∗(y|z)) dy ≥\nZ\np∗(y|z)log (c(y|z)) dy\n(4)\nAnd so by Equation 4:\nI(Z; Y ) =\nZ Z\np∗(y, z)log\n\u0012 p∗(y, z)\np∗(y)p∗(z)\n\u0013\ndydz\n≥\nZ Z\np∗(y|z)p∗(z)log\n\u0012c(y|z)\np∗(y)\n\u0013\ndydz\n=\nZ Z\np∗(y, z)log (c(y|z)) dydz + Hp∗(Y )\n(5)\nWe now diverge from the original VIB derivation by re-\nplacing Hp∗(Y ) with Hc(Y |Z) instead of omitting it. In\naddition, we limit the new term to make sure that the in-\nequality H(Y |Z) ≤ H(Y ) holds when computing entropy\nover the different distributions p∗ and c.\nI(Z; Y ) ≥\nZ Z\np∗(y, z)log (c(y|z)) dydz\n+min {Hp∗(Y ), Hc(Y |Z)}\n(6)\nWe further develop this term using the IB Markov chain\nZ ↔ X ↔ Y and total probability:\n4\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nI(Z; Y ) ≥\nZ Z Z\np∗(x)p∗(y|x)p∗(z|x)log (c(y|z)) dxdydz\n−min\n\u001a\nHp∗(Y ), −\nZ Z\nc(y, z)log (c(y|z)) dydz\n\u001b\n(7)\nFinally, we define a new upper bound for the IB functional\nnamed LUB by joining the bound on rate in Equation 3 with\nthe bound on distortion in Equation 7:\nLUB ≡\nZ Z\np∗(x)p∗(z|x)log\n\u0012p∗(z|x)\nr(z)\n\u0013\ndxdz\n−\nZ Z Z\np∗(x)p∗(y|x)p∗(z|x)log (c(y|z)) dxdydz\n+min\n\u001a\nHp∗(Y ), −\nZ Z\nc(y, z)log (c(y|z)) dydz\n\u001b\n(8)\nIt is easy to verify that the bound holds for all β ≥ 0 such\nthat LIB = β · I (Z; X) − I (Z; Y ).\n3.2. Variational approximation\nFollowing the same variational approach as in VIB, we\ndefine LV UB as a new tractable upper bound for the IB\nfunctional. Let p∗(x, y, z) be the unknown joint distribution,\ne(z|x) a variational encoder approximating p∗(z|x) and\nc(y|z) a variational classifier approximating p∗(y|z):\nLV UB ≡\nβ\nZ Z\np∗(x)e(z|x)log\n\u0012e(z|x)\nr(z)\n\u0013\ndxdz\n−\nZ Z Z\np∗(x)p∗(y|x)e(z|x)log (c(y|z)) dxdydz\n(9)\n−min\n(\nHp∗(Y ),\n−\nZ Z Z\np∗(x)e(z|x)c(y|z)log (c(y|z)) dxdydz\n)\n≥LIB\n3.3. Empirical estimation\nWe proceed to model VUB using DNNs and optimize it\nusing Monte Carlo sampling over some training dataset. Let\neϕ be a stochastic DNN encoder with parameters ϕ applying\nthe reparameterization trick such that eϕ(x) ∼ N(µ, Σ) and\nlet Cλ be a discrete classifier DNN parameterized by λ such\nthat Cλ(ˆz) ∼ Categorical.\nˆLV UB ≡\n1\nN\nN\nX\nn=1\n\"\nβ · DKL\n\u0000eϕ(xn)\n\f\f\f\fr(z)\n\u0001\n(10)\n−P ∗(yn) · log (Cλ (eϕ(xn)))\n−min\nn\nH( ˆY ), H (Cλ (eϕ(xn)))\no #\nAs in VIB and VAE, eϕ(x) and r(z) are computed as spher-\nical gaussians. eϕ(x) by using the first half of the encoder’s\noutput entries as µ and the second as the diagonal Σ, and\nr(z) by a standard normal gaussian.\n3.4. Interpretation\nSimilarly to the confidence penalty suggested by Pereyra\net al. (2017), the new derivation adds classifier regulariza-\ntion to the VIB objective. Regularizing the classifier might\nprevent it from overfitting, and is a possible remedy to the\ndiscrepancies in the ELBO loss observed by Alemi et al.\n(2018), as elaborated in Section 5.\nIn terms of tightness we have that VUB is a tighter theoreti-\ncal bound on the IB objective than VIB for any Y such that\nH(Y ) > 0, and a tighter empirical bound for all Y .\n4. Experiments\nWe follow the experimental setup proposed by Alemi et al.\n(2017), extending it to NLP tasks as well. Image classifi-\ncation models were trained on the ImageNet 2012 dataset\n(Deng et al., 2009) and text classification over the IMDB\nsentiment analysis dataset (Maas et al., 2011). For each\ndataset, a competitive pre-trained model (Vanilla model)\nwas evaluated and then used to encode embeddings. These\nembeddings were then used as a dataset for a new stochas-\ntic classifier net with either a VIB or a VUB loss function.\nStochastic classifiers consisted of two ReLU activated linear\nlayers of the same dimensions as the pre-trained model’s log-\nits (2048 for image and 768 for text classification), followed\nby reparameterization and a final softmax activated FC layer.\nLearning rate was 10−4 and decaying exponentially with a\nfactor of 0.97 every two epochs. Batch sizes were 32 for\nImageNet and 16 for IMDB. We used a single forward pass\nper sample for inference. Each model was trained and evalu-\nated 5 times per β value with consistent performance. Beta\nvalues of β = 10−i for i ∈ {1, 2, 3} were tested since pre-\nvious studies indicated this is the best range for VIB (Alemi\net al., 2017; 2018). Each model was evaluated using test set\naccuracy and robustness to various adversarial attacks over\n5\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nthe test set. For image classification we employed the untar-\ngeted Fast Gradient Sign (FGS) attack (Goodfellow et al.,\n2015) as well as the targeted CW L2 optimization attack\n(Carlini & Wagner, 2017), (Kaiwen, 2018). For text classifi-\ncation we used the untargeted Deep Word Bug attack (Gao\net al., 2018) as well as the untargeted PWWS attack (Ren\net al., 2019), (Morris et al., 2020). All models were trained\nusing an Nvidia RTX3080 GPU. Code to reconstruct the\nexperiments is available in the following github repository:\nhttps://github.com/hopl1t/vub.\n4.1. Image classification\nA pre-trained inceptionV3 (Szegedy et al., 2016) base model\nwas used and achieved a 77.21% accuracy on the ImageNet\n2012 validation set (Test set for ImageNet is unavailable).\nNote that inceptionV3 yields a slightly worse single shot\naccuracy than inceptionV2 (80.4%) when run in a single\nmodel and single crop setting, however we’ve used Incep-\ntionV3 over V2 for simplicity. Each model was trained for\n100 epochs. The entire validation set was used to measure\naccuracy and robustness to FGS attacks, while only 1%\nof it was used for CW attacks as they are computationally\nexpensive.\n4.1.1. EVALUATION AND ANALYSIS\nImage classification evaluation results are shown in Table 1,\nexamples of successful attacks are shown in Figures 4, 5.\nThe empirical results presented in Table 1 confirm that while\nVIB and VUB reduce performance on the validation set,\nthey substantially improve robustness to adversarial attacks.\nMoreover, these results demonstrate that VUB significantly\noutperforms VIB in terms of validation set accuracy while\nproviding competitive robustness to attacks. A comparison\nof the best VIB and VUB models further substantiates these\nfindings, with statistical significance confirmed by a p-value\nof less than 0.05 in a Wilcoxon rank sum test.\n4.2. Text classification\nA fine tuned BERT uncased (Devlin et al., 2019) base model\nwas used and achieved a 93.0% accuracy on the IMDB\nsentiment analysis test set. Each model was trained for 150\nepochs. The entire test set was used to measure accuracy,\nwhile only the first 200 entries in the test set were used for\nadversarial attacks as they are computationally expensive.\n4.2.1. EVALUATION AND ANALYSIS\nText classification evaluation results are shown in Table 2,\nexamples of successful attacks are shown in Figure 3. In\nthis modality VUB significantly outperforms VIB in both\ntest set accuracy and robustness to both attacks. Moreover,\nVUB also outperomed the original model in terms of test\nβ\nVal ↑\nFGS\nϵ=0.1 ↓\nFGS\nϵ=0.5 ↓\nCW↑\nVanilla model\n-\n77.2%\n68.9%\n67.7%\n788\nVIB models\n10−3\n73.7%\n±.1%\n59.5%\n±.2%\n63.9%\n±.2%\n3917\n±291\n10−2\n72.8%\n±.1%\n53.5%\n±.2%\n62.0%\n±.1%\n3318\n±293\n10−1\n72.1%\n±.01%\n58.4%\n±.1%\n62.0%\n±.1%\n3318\n±293\nVUB models\n10−3\n75.5%\n±.03%\n62.8%\n±.1%\n66.4%\n±.1%\n2666\n±140\n10−2\n75.0%\n±.05%\n57.6%\n±.2%\n64.3%\n±.1%\n1564\n±218\n10−1\n74.8%\n±0.09%\n57.9%\n±.5%\n64.8%\n±.5%\n3575\n±456\nTable 1. ImageNet evaluation scores for vanilla, VIB and VUB\nmodels, average over 5 runs with standard deviation. First column\nis performance on the ImageNet validation set (higher is better\n↑), second and third columns are the % of successful FGS attacks\nat ϵ = 0.1, 0.5 (lower is better ↓) and the fourth column is the\naverage L2 distance for a successful Carlini Wagner L2 targeted\nattack (higher is better ↑).\nFigure 3. Estimated information plane metrics per epoch for VUB\ntrained on the IMDB dataset with β = 0.001. I(Z; X) is approxi-\nmated by H(R) − H(Z|X) and\n1\nCE(Y ; ˆY ) is used as an analog for\nI(Z; Y ). The epochs have been grouped and color-coded in inter-\nvals of 30 epochs in the order: Orange (0-30), gray (30-60), yellow\n(60-90), green (90-120) and red (120-150). We notice recurring\npatterns of distortion reduction followed by rate increase, resem-\nbling the ERM and representation compression stages described\nby Shwartz-Ziv & Tishby (2017).\n6\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nFigure 4. Successful targeted CW attack examples. Images are\nperturbations of previously successfully classified instances from\nthe ImageNet validation set. The target label is ’Soccer ball’.\nAverage L2 distance required for a successful attack is shown on\nthe left. The higher the required L2 distance the greater the visible\nchange required to fool the model. Original and wrongly assigned\nlabels are listed at the top of each image. Mind the difference in\nnoticeable change as compared to the FGS perturbations presented\nin Figure 5, and between VIB and VUB perturbations.\nset accuracy. A comparison of the best VIB and VUB\nmodels further substantiates these findings, with statistical\nsignificance confirmed by a p-value of less than 0.05 in a\nWilcoxon rank sum test.\nIn addition to the above evaluation metrics, we also mea-\nsured approximated rate and distortion throughout training\nand plotted them on the information curve as shown in Fig-\nure 3. We notice recurring patterns of distortion reduction\nfollowed by rate increase, resembling the ERM and repre-\nsentation compression stages described by Shwartz-Ziv &\nTishby (2017).\n5. Discussion\nWhile providing a complete framework for optimal data\nmodeling, the IB, and it’s variational approximations, rely\non three assumptions: (1) It suffices to optimize the mutual\ninformation metric to optimize a model’s performance; (2)\nForgetting more information about the input while keep-\ning the same information about the output induces better\ngeneralization; (3) Mutual information between the input,\noutput and latent representation can be either computed\nor approximated to a desired level of accuracy. Our study\nstrengthens the argument for using the Information Bottle-\nβ\nTest↑\nDWB↓\nPWWS↓\nVanilla model\n-\n93.0%\n54.3%\n100%\nVIB models\n10−3\n91.0%\n±1.0%\n35.1%\n±4.4%\n41.6%\n±6.6%\n10−2\n90.8%\n±0.5%\n41.0%\n±4.8%\n62.9%\n±14.3%\n10−1\n89.4%\n±.9%\n90.0%\n±8.0%\n99.1%\n±0.9%\nVUB models\n10−3\n93.2%\n±.5%\n27.5%\n±2.0%\n28.4%\n±1.3%\n10−2\n92.6%\n±.8%\n30.8%\n±2.0%\n50.0%\n±4.8%\n10−1\n89.2%\n±2.0%\n99.2%\n±0.5%\n100%\n±0%\nTable 2. Evaluation for vanilla, VIB and VUB models, average\nover 5 runs with standard deviation over the IMDB dataset. First\ncolumn is performance on the test set (higher is better ↑), second\nis % of successful Deep Word Bug attacks (lower is better ↓) and\nthe third column is % of successful PWWS attacks (lower is better\n↓).\nText perturbed with DWB\ngnreat historical movie, will not allow a viewer to leave\nonce you begin to watch. View is presented differently\nthan displayed by most school books on this sSubject [...]\nText perturbed with PWWS\nthe acting , costumes , music , cinematography and sound\nare all astoundingdumbfounding given the production ’s\naustere locales .\nTable 3. Examples of successful DWB and PWWS perturbations\non a vanilla Bert model fine tuned over the IMDB dataset. The\noriginal input strings were perturbed such that inserted tokens\nare marked in underscored boldface and removed tokens in\nstrikethrough. Both examples were classified correctly as ’Positive\nsentiment’ before the attack and ’Negative sentiment’ afterwards.\n7\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nneck combined with variational approximations to obtain\nrobust models that can withstand adversarial attacks. By de-\nriving a tighter bound on the IB functional, we demonstrate\nit’s utility as the Variational Upper Bound (VUB) objective\nfor neural networks. We demonstrate that VUB outperforms\nthe Variational Information Bottleneck (VIB) in terms of\ntest accuracy while providing similar or superior robustness\nto adversarial attacks in challenging classification tasks of\ndifferent modalities, suggesting an improvement in data\nmodeling quality.\nComparing VIB and VUB we observe that both methods\npromote a disentangled latent space by using a stochastic\nfactorized prior, as suggested by Chen et al. (2018). In\naddition, both methods utilize KL regularization, enforcing\nclustering around a 0 mean which might increase latent\nsmoothness. These traits can make it difficult for minor\nperturbations to significantly alter latent semantics, making\nthe models more robust to attacks. In the case of VUB,\nthe enhanced results induced by classifier regularization\nnot only reinforce previous studies on the ELBO function,\nwhich suggest that overly powerful decoders diminish the\nquality of learned representations (Alemi et al., 2018), but\nalso align with the confidence penalty proposed by Pereyra\net al. (2017).\nIn addition, we observed that in many cases VIB achieves\nlower validation set cross entropy while VUB achieves sig-\nnificantly higher test set accuracy. We attribute this gap to\nthe VUB models becoming more calibrated, and we suggest\nthat practitioners also monitor validation set accuracy and\nrate-distortion ratio during training. These metrics may be\nmore informative indicators of model performance than val-\nidation set cross entropy alone, as validation cross entropy\ncould increase as models become more calibrated.\nWe made another interesting observation during our study\nregarding information plane behavior throughout the train-\ning process. While previous research has documented the\noccurrence of error minimization and representation com-\npression phases, our work revealed that these phases can\noccur in cycles throughout training. This finding is partic-\nularly noteworthy because previous studies observed this\nphenomenon in simple toy problems, whereas our research\ndemonstrated it in complex tasks of high dimensionality\nwith unknown distributions. This suggests that this informa-\ntion plane behavior is not limited to simplified scenarios but\nis a characteristic of the learning process in more challeng-\ning tasks as well.\nIn conclusion, while the IB and its variational approxima-\ntions do not provide a complete theoretical framework for\nDNN data modeling and regularization, they offer a strong,\nmeasurable and theoretically grounded approach. VUB is\npresented as a tractable and tighter upper bound of the IB\nfunctional that can be easily adapted to any classifier DNN,\nincluding transformer based text classifiers, to significantly\nincrease robustness to various adversarial attacks while in-\nflicting minimal decrease in test set performance, and in\nsome cases even increasing it.\nThis study opens many opportunities for further research.\nBesides further improvements to the upper bound, it is in-\ntriguing to use VUB in self-supervised learning and in gener-\native tasks. Other possible directions, including measuring\nmodel calibration as proposed by Achille & Soatto (2018)\nare left for future work.\nFigure 5. Successful untargeted FGS attack examples. Images are\nperturbations of previously successfully classified instances from\nthe ImageNet validation set. Perturbation magnitude is determined\nby the parameter ϵ shown on the left, the higher the more perturbed.\nOriginal and wrongly assigned labels are listed at the top of each\nimage. Notice the deterioration of image quality as ϵ increases.\n8\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nImpact\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAchille, A. and Soatto, S. Information dropout: Learning op-\ntimal representations through noisy computation. IEEE\nTrans. Pattern Anal. Mach. Intell., 40(12):2897–2905,\n2018.\nURL http://dblp.uni-trier.de/db/\njournals/pami/pami40.html#AchilleS18.\nAlemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep\nvariational information bottleneck. In Proceedings of the\nInternational Conference on Learning Representations\n(ICLR), Google Research, 2017.\nAlemi, A. A., Poole, B., Fischer, I., Dillon, J. V.,\nSaurous, R. A., and Murphy, K.\nFixing a bro-\nken elbo.\nIn Proceedings of Machine Learning\nResearch, volume 80, pp. 159–168, PMLR, 2018.\nURL http://dblp.uni-trier.de/db/conf/\nicml/icml2018.html#AlemiPFDS018.\nAmjad,\nR. A. and Geiger,\nB. C.\nLearning rep-\nresentations for neural network-based classification\nusing the information bottleneck principle.\nIEEE\nTrans. Pattern Anal. Mach. Intell., 42(9):2225–2239,\n2020.\nURL http://dblp.uni-trier.de/db/\njournals/pami/pami42.html#AmjadG20.\nBlahut, R. E.\nComputation of channel capacity and\nrate distortion function.\nIEEE Transactions on\nInformation Theory,\nIT-18:460–473,\n1972.\ndoi:\nhttps://ieeexplore.ieee.org/document/1054855.\nURL\nhttps://ieeexplore.ieee.org/document/\n1054855.\nCarlini, N. and Wagner, D. A. Towards evaluating the ro-\nbustness of neural networks. In IEEE Symposium on\nSecurity and Privacy, pp. 39–57. IEEE Computer So-\nciety, 2017. URL http://dblp.uni-trier.de/\ndb/conf/sp/sp2017.html#Carlini017.\nChechik, G. et al.\nGaussian information bottleneck.\nIn\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems, 2003.\nURL https://proceedings.\nneurips.cc/paper/2003/hash/\n7e05d6f828574fbc975a896b25bb011e-Abstract.\nhtml.\nChen,\nT. Q.,\nLi,\nX.,\nGrosse,\nR. B.,\nand Duve-\nnaud,\nD.\nIsolating sources of disentanglement\nin variational autoencoders.\nIn Proceedings of\nthe 32nd International Conference on Neural Infor-\nmation Processing Systems, pp. 2615–2625, 2018.\nURL http://dblp.uni-trier.de/db/conf/\nnips/nips2018.html#ChenLGD18.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248–255. Ieee, 2009.\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:\nPre-training of deep bidirectional transformers for lan-\nguage understanding. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pp. 4171–\n4186, Minneapolis, Minnesota, 2019.\nURL https:\n//www.aclweb.org/anthology/N19-1423.\nFischer,\nI.\nThe\nconditional\nentropy\nbottle-\nneck.\nEntropy,\n22(9):999,\n2020.\nURL\nhttp://dblp.uni-trier.de/db/journals/\nentropy/entropy22.html#Fischer20.\nGao, J., Lanchantin, J., Soffa, M. L., and Qi, Y. Black-box\ngeneration of adversarial text sequences to evade deep\nlearning classifiers. In 2018 IEEE Security and Privacy\nWorkshops (SPW), pp. 50–56. IEEE, 2018.\nGoodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and\nharnessing adversarial examples. In ICLR (Poster), 2015.\nURL http://dblp.uni-trier.de/db/conf/\niclr/iclr2015.html#GoodfellowSS14.\nGuo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On\ncalibration of modern neural networks. In International\nconference on machine learning, pp. 1321–1330. PMLR,\n2017.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework. In ICLR (Poster), 2017.\nKaiwen. pytorch-cw2, 2018. URL https://github.\ncom/kkew3/pytorch-cw2. GitHub repository.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In 2nd International Conference on Learning\nRepresentations, ICLR 2014, Banff, AB, Canada, April\n14-16, 2014, Conference Track Proceedings, 2014.\nKingma,\nD.\nP.\nand\nWelling,\nM.\nAn\nintroduc-\ntion\nto\nvariational\nautoencoders.\nFoundations\nand Trends in Machine Learning,\n12(4):307–392,\n2019.\nURL http://dblp.uni-trier.de/db/\njournals/ftml/ftml12.html#KingmaW19.\n9\nTighter Bounds on the Information Bottleneck with Application to Deep Learning\nMaas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and\nPotts, C. Learning word vectors for sentiment analysis.\nIn Proceedings of the 49th annual meeting of the asso-\nciation for computational linguistics: Human language\ntechnologies, pp. 142–150, 2011.\nMorris, J., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi,\nY. Textattack: A framework for adversarial attacks, data\naugmentation, and adversarial training in nlp. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing: System Demonstrations,\npp. 119–126, 2020.\nPainsky, A. and Tishby, N.\nGaussian lower bound\nfor the information bottleneck limit.\nJ. Mach.\nLearn.\nRes.,\n18:213:1–213:29,\n2017.\nURL\nhttp://dblp.uni-trier.de/db/journals/\njmlr/jmlr18.html#PainskyT17.\nPereyra, G., Tucker, G., Chorowski, J., Kaiser, L.,\nand\nHinton,\nG.\nE.\nRegularizing\nneural\nnet-\nworks by penalizing confident output distributions.\nIn Proceedings of the International Conference on\nLearning\nRepresentations,\nOpenReview.net,\n2017.\nURL http://dblp.uni-trier.de/db/conf/\niclr/iclr2017w.html#PereyraTCKH17.\nRen, S., Deng, Y., He, K., and Che, W. Generating nat-\nural language adversarial examples through probability\nweighted word saliency. In Proceedings of the 57th an-\nnual meeting of the association for computational linguis-\ntics, pp. 1085–1097, 2019.\nShwartz-Ziv, R. and Tishby, N. Opening the black box of\ndeep neural networks via information, 2017. URL http:\n//arxiv.org/abs/1703.00810. 19 pages, 8 fig-\nures.\nSlonim, N. The information bottleneck: Theory and ap-\nplications. PhD thesis, Hebrew University of Jerusalem\nJerusalem, Israel, 2002.\nSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna,\nZ. Rethinking the inception architecture for computer vi-\nsion. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 2818–2826, 2016.\nTishby, N. and Zaslavsky, N. Deep learning and the infor-\nmation bottleneck principle, 2015.\nTishby, N., Pereira, F. C., and Bialek, W. The information\nbottleneck method. In The 37th annual Allerton Con-\nference on Communication, Control, and Computing.,\nHebrew University, Jerusalem 91904, Israel, 1999.\nWieczorek, A. and Roth, V. On the difference between\nthe information bottleneck and the deep information\nbottleneck.\nCoRR, abs/1912.13480, 2019.\nURL\nhttp://dblp.uni-trier.de/db/journals/\ncorr/corr1912.html#abs-1912-13480.\nYing, X.\nAn overview of overfitting and its solu-\ntions.\nJournal of Physics: Conference Series, 1168\n(2):022022, feb 2019. doi: 10.1088/1742-6596/1168/2/\n022022. URL https://dx.doi.org/10.1088/\n1742-6596/1168/2/022022.\n10\n"
}