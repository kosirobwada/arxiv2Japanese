{
    "optim": "Tighter Bounds on the Information Bottleneck with Application to Deep Learning Nir Weingarten 1 Zohar Yakhini 1 Moshe Butman 1 Ran Gilad-Bachrach 2 Abstract Deep Neural Nets (DNNs) learn latent representa- tions induced by their downstream task, objective function, and other parameters. The quality of the learned representations impacts the DNN’s generalization ability and the coherence of the emerging latent space. The Information Bottle- neck (IB) provides a hypothetically optimal frame- work for data modeling, yet it is often intractable. Recent efforts combined DNNs with the IB by applying VAE-inspired variational methods to approximate bounds on mutual information, re- sulting in improved robustness to adversarial at- tacks. This work introduces a new and tighter variational bound for the IB, improving perfor- mance of previous IB-inspired DNNs. These ad- vancements strengthen the case for the IB and its variational approximations as a data modeling framework, and provide a simple method to sig- nificantly enhance the adversarial robustness of classifier DNNs. 1. Introduction In recent years, Deep Neural Networks (DNNs) have gained prominence in various learning tasks, revolutionizing many computational fields with their ability to approximate com- plex functions. Despite the great achievements, it is still postulated that the current networks are prone to overfit the training data (Ying, 2019), may be considerably uncal- ibrated (Guo et al., 2017) and are susceptible to adversar- ial attacks (Goodfellow et al., 2015). A question emerges regarding the extraction of an optimal representation for all data points from a restricted set of training examples. Classic information theory provides tools to optimize com- pression and transmission of data, but it does not provide methods to gauge the relevance of a compressed signal to its 1Efi Arzi School of Computer Science, Reichman University, Herzliya, Israel 2Department of Biomedical Engineering, Tel-Aviv University, Tel-Aviv, Israel. Correspondence to: Nir Weingarten <nir.weingarten@runi.ac.il>. arXiv preprint, Copyright 2024 by the author(s). downstream task. Methods such as rate-distortion (Blahut, 1972) regard all information as equal, not taking into account which information is more relevant without constructing complex distortion functions. The Information Bottleneck (IB) (Tishby et al., 1999) resolves this limitation by defin- ing mutual information between the learned representation and the downstream task as a universal distortion function. Under this definition, an optimal rate-disotrion ratio can be implicitly computed for a Lagrange multiplier β, con- trolling the tradeoff between the desired rate and distortion. However, optimizing over the IB requires mutual informa- tion computations, which are tractable in discrete settings and for some specific continuous distributions. Adopting the IB framework for DNNs requires computing mutual information for unknown distributions and has no analytic solution. However, recent work approximated tractable upper bounds for the IB functional in DNN settings us- ing variational approximations. Variational Auto Encoders (VAEs) (Kingma & Welling, 2014) use stochastic DNNs to approximate intractable distributions, as elaborated in Section 2.3. Similarly to VAEs, Alemi et al. (2017) pro- posed using stochastic DNNs as variational approximations of latent models, thus making possible the computations of upper bounds for mutual information between the DNN’s input, output and latent representation. A proposed DNN optimization method called Deep Variational Information Bottleneck (VIB) derives an upper bound for the IB ob- jective and minimizes its approximation by fitting some training dataset. Optimizing classifier DNNs with the VIB objective results in a slight decrease in test set accuracy compared to deterministic DNNs, but yields a significant increase in robustness to adversarial attacks. In this study, we adopt the same information theoretic and variational approach proposed in VIB. The work begins by deriving a new upper bound for the IB functional. We then employ a tractable variational approximation for this bound, named VUB - ’Variational Upper Bound’ and show that it is a tighter bound on the IB objective than VIB. We proceed to show empirical evidence that VUB substantially increases test set accuracy over VIB while providing similar or superior robustness to adversarial attacks across several challenging tasks and different modalities. Finally, we dis- cuss these effects in the context of previous work on the 1 arXiv:2402.07639v1  [cs.LG]  12 Feb 2024 Tighter Bounds on the Information Bottleneck with Application to Deep Learning IB and on DNN regularization. The conclusion drawn is that while increasing mutual information between encod- ing and output does not necessarily improve classification, and while increasing encoding compression does not al- ways enhance regularization (Amjad & Geiger, 2020), the application of IB approximations as objectives to DNNs empirically improves regularization, suggesting better data modeling. This notion contributes for the adaptation of the IB, and its variational approximations, as an objective for learning tasks and as a theoretic framework to gauge and explain data modeling. In addition, we demonstrate that VUB can be easily adapted to any classifier DNN, including transformer based NLP classifiers, to substantially increase robustness to adversarial attacks while only slightly decreasing, or in some cases even increasing, test set accuracy. 1.1. Preliminaries The following literature review and derivations refer to in- formation theory and variational approximations. A prelim- inary mutual ground and notation is provided. We denote random variables (RVs) with upper cased letters X, Y , and their realizations in lower case x, y. Denote dis- crete Probability Mass Functions (PMFs) with an upper case P(x) and continuous Probability Density Functions (PDFs) with a lower case p(x). Hat notation denotes empirical measurements. Let X, Y be two observed random variables with unknown distributions p∗(x), p∗(y) that we aim to model. Assume X, Y are governed by some unknown underlying process with a joint probability distribution p∗(x, y). We can attempt to approximate these distributions using a model pθ with pa- rameters θ such that for generative tasks pθ(x) ≈ p∗(x) and for discriminative tasks pθ(y|x) ≈ p∗(y|x), using a dataset S = {(x1, y1), ..., (xN, yN)} to fit our model. One can also assume the existence of an additional unob- served RV Z ∼ p∗(z) that influences or generates the ob- served RVs X, Y . Since Z is unobserved it is absent from the dataset S and so cannot be modeled directly. Denote R pθ(x|z)pθ(z)dz the marginal, pθ(z) the prior as it is not conditioned over any other RV, and pθ(z|x) the posterior following Bayes’ rule. When modeling an unobserved variable of an unknown dis- tribution we encounter a problem as the marginal pθ(x) = R pθ(x, z)dz doesn’t have an analytic solution. This in- tractability can be overcome by choosing some tractable parametric variational distribution qϕ(z|x) to approximate the posterior pθ(z|x) such that qϕ(z|x) ≈ pθ(z|x), and esti- mate pθ(x, z) or pθ(x, z|y) by fitting the dataset S (Kingma & Welling, 2019). In this work information theoretic functions share the same notation for discrete and continuous settings. For brevity, we will only present the continuous form: Entropy Hp(X) = − R p(x)log (p(x)) dx Cross Entropy CE(p, q) = − R p(x)log (q(x)) dx KL Divergence DKL \u0000p \f\f\f\fq \u0001 = R p(x)log \u0010 p(x) q(x) \u0011 dx Mutual Information I(X; Y ) = R R p(x, y)log \u0010 p(x,y) p(x)p(y) \u0011 dxdy 2. Related work 2.1. IB and its analytic solutions Classic information theory offers rate-distortion (Blahut, 1972) to mitigate signal loss during compression. Rate being the signal’s compression measured by mutual infor- mation between input and output signals, and distortion a chosen task-specific function. The Information Bottleneck (IB) method (Tishby et al., 1999) extends rate-distortion by replacing the tailored distortion functions with mutual in- formation between the learned representation and the down- stream task. Denote X the source signal, Z its encoding and Y the target signal for some specific task. Assum- ing a latent variable model that follows the Markov chain Z ↔ X ↔ Y , we define some positive minimal threshold D for the desired distortion. We seek an optimal encoding Z : min P (Z|X)I(X; Z) subject to I(Z; Y ) ≥ D. This con- strained problem can be implicitly optimized by minimizing the functional LP (Z|X) = I(Z; X) − βI(Z; Y ), the first term being rate and the second distortion modulated by the Lagrange multiplier β. The optimal solution is a function of β and was named ’the information curve’ as illustrated in Figure 1. The IB can be interpreted as a method to learn a representation that holds just enough information to satisfy a desired task, while discarding all other available informa- tion, presumably providing a model with the least possible complexity. The IB functional is only tractable when mutual informa- tion can be computed and was originally demonstrated for soft clustering tasks over a discrete and known distribution P ∗(x, y). Chechik et al. (2003) extended the IB for gaus- sian distributions and Painsky & Tishby (2017) offered a limited linear approximation of the IB for any distribution. 2.2. IB and deep learning Tishby & Zaslavsky (2015) proposed an IB interpretation of DNNs, regarding them as Markov cascades of interme- 2 Tighter Bounds on the Information Bottleneck with Application to Deep Learning Figure 1. The information plane and curve: rate-distortion ratio over β. At β = 0 the representation is compressed but uninfor- mative (maximal compression), at β → ∞ the representation is informative but potentially overfitted (maximal information). Adapted from (Slonim, 2002). diate representations between hidden layers. Under this framework, comparing the optimal and the achieved rate- distortion ratios between DNN layers will indicate if a model is too complex or too simple for a given task and training set. Shwartz-Ziv & Tishby (2017) visualized and analyzed the information plane behavior of DNNs over a toy prob- lem with a known joint distribution. Mutual information of the different layers was estimated and used to analyze the training process. The learning process over Stochastic Gradient Descent (SGD) exhibited two separate and sequen- tial behaviors: A short Empirical Error Minimization phase (ERM) characterized by a rapid decrease in distortion, fol- lowed by a long compression phase with an increase in rate until convergence to an optimal IB limit as demonstrated in Figure 2. Similar yet repetitive behavior was observed in the current study, as elaborated in Section 4.2.1. Amjad & Geiger (2020) pointed out three flaws in the us- age of the IB functional as an objective for deterministic DNN classifiers: (1) When data X is absolutely continuous the mutual information term I(X; Z) is infinite; (2) When data X is discrete the IB functional is a piecewise constant function of the parameters, making it’s SGD optimization difficult or impossible; (3) Equivalent representations might yield the same IB loss while one achieved better classifica- tion rate than the other. These discrepancies were attributed to mutual information’s invariance to invertible transfor- mations and to the absence of a decision function in the objective. Figure 2. Information plane scatters of different DNN layers (col- ors) in 50 randomized networks. From Shwartz-Ziv & Tishby (2017). Left are initial weights, Right are at 400 epochs. Our study reproduced similar yet repetitive behavior on complicated high dimensional tasks, as elaborated in Section 4.2.1 and in Figure 3. 2.3. Variational approximations to the IB objective Kingma & Welling (2014) introduced the Variational Auto Encoder (VAE) - a stochastic generative DNN. An unob- served RV Z is assumed to generate evidence X and the true probability p∗(x) can be modeled using a paramet- ric model over the marginal pθ(x) = R pθ(x|z)pθ(z)dz. However, since the marginal is intractable a variational approximation qϕ(z|x) ≈ pθ(z|x) is proposed instead. The log probability log (pθ(x)) is then developed in to the tractable VAE loss comprised of the Evidence Lower Bound (ELBO) and KL regularization terms: L ELBO = Eqϕ(z|x) [log (pθ(x|z))] − DKL \u0000qϕ(z|x) \f\f\f\fpθ(z) \u0001 . qϕ(z|x) is modeled using a stochastic neural encoder having it’s final activation used as parameters for the assumed variational distribution (typically a spherical gaussian with parameters µ, Σ). Each forward pass emulates a stochastic realization z ∈ Z from these parameters by using the ’reparameter- ization trick’: z = µ + ϵ · Σ for some unparameterized scalar ϵ ∼ N(0, 1) such that a backwards pass is possible. Higgins et al. (2017) later proposed the β-autoencoder, intro- ducing a hyper parameter β over the KL term to control the regularization-reconstruction tradeoff. Alemi et al. (2018) found that as the ELBO loss in VAEs depends solely on image reconstruction it does not necessarily induce a better quality modeling of the marginal pθ(z), hence not neces- sarily a better representation learned. This gap is attributed to powerful decoders being overfitted, as will be further discussed in Section 5. Alemi et al. (2017) introduced the Variational Information Bottleneck (VIB) as a variational approximation for an upper bound to the IB objective for classifier DNN opti- mization. Bounds for I(Z, Y ) and I(X, Z) are derived from the non negativity of KL divergence and are used to form an upper bound for the IB functional. This upper 3 Tighter Bounds on the Information Bottleneck with Application to Deep Learning bound is approximated using variational approximations for p∗(y|z), p∗(z) as done in VAEs. This approximation of an upper bound for the IB objective is empirically estimated as cross entropy and a beta scaled KL regularization term as in β-autoencoders, and is optimized over the training data using Monte Carlo sampling and the reparameteriza- tion trick. VIB was evaluated over image classification tasks and, while causing a slight reduction in test set accuracy, generated substantial improvements in robustness to adver- sarial attacks. Additional noteworthy contributions to this field have been made in recent years by Achille & Soatto (2018); Wieczorek & Roth (2019); Fischer (2020) and others. However, a detailed review of these works is beyond the scope of this paper. 2.4. Non IB information theoretic regularization Label smoothing (Szegedy et al., 2016) and entropy regular- ization (Pereyra et al., 2017) both regularize classifier DNNs by increasing the entropy of their output. This is done either directly by inserting a scaled conditional entropy term to the loss function, −γ · H (pθ(y|x)), or by smoothing the train- ing data labels. Applying both methods was demonstrated to improve test accuracy and model calibration on various challenging classification tasks. In the current work a sim- ilar conditional entropy term emerges from the derivation of the new upper bound for the IB objective, as shown in Section 3. 3. From VIB to VUB The VIB loss consists of a cross entropy term and a KL regularization term, as in VAE loss. The KL term is de- rived from a bound on the IB rate term I(X; Z), while the cross entropy term from a bound on the IB distortion term I(Z; Y ) = H(Y ) − H(Y |Z). When deriving the latter the entropy term H(Y ) is ignored as it is constant and does not effect optimization. We note that since Y is unknown any optimization over Z, including cross entropy, depends on our decoder model of Y . Following this logic, instead of omitting H(Y ) we replace it with a variational approxima- tion of the decoder entropy, which provides a lower bound. 3.1. IB upper bound We begin by establishing a new upper bound for the IB functional by bounding the mutual information terms, using the same method as in VIB. Consider I(Z; X): I(Z; X) = Z Z p∗(x, z)log (p∗(z|x)) dxdz − Z p∗(z)log (p∗(z)) dz (1) For any probability distribution r we have that DKL \u0000p∗(z) \f\f\f\fr(z) \u0001 ≥ 0, it follows that: Z p∗(z)log (p∗(z)) dz ≥ Z p∗(z)log (r(z)) dz (2) And so by Equation 2: I(Z; X) ≤ Z Z p∗(x)p∗(z|x)log \u0012p∗(z|x) r(z) \u0013 dxdz (3) Consider I(Z; Y ): For any probability distribution c we have that DKL \u0000p∗(y|z) \f\f\f\fc(y|z) \u0001 ≥ 0, it follows that: Z p∗(y|z)log (p∗(y|z)) dy ≥ Z p∗(y|z)log (c(y|z)) dy (4) And so by Equation 4: I(Z; Y ) = Z Z p∗(y, z)log \u0012 p∗(y, z) p∗(y)p∗(z) \u0013 dydz ≥ Z Z p∗(y|z)p∗(z)log \u0012c(y|z) p∗(y) \u0013 dydz = Z Z p∗(y, z)log (c(y|z)) dydz + Hp∗(Y ) (5) We now diverge from the original VIB derivation by re- placing Hp∗(Y ) with Hc(Y |Z) instead of omitting it. In addition, we limit the new term to make sure that the in- equality H(Y |Z) ≤ H(Y ) holds when computing entropy over the different distributions p∗ and c. I(Z; Y ) ≥ Z Z p∗(y, z)log (c(y|z)) dydz +min {Hp∗(Y ), Hc(Y |Z)} (6) We further develop this term using the IB Markov chain Z ↔ X ↔ Y and total probability: 4 Tighter Bounds on the Information Bottleneck with Application to Deep Learning I(Z; Y ) ≥ Z Z Z p∗(x)p∗(y|x)p∗(z|x)log (c(y|z)) dxdydz −min \u001a Hp∗(Y ), − Z Z c(y, z)log (c(y|z)) dydz \u001b (7) Finally, we define a new upper bound for the IB functional named LUB by joining the bound on rate in Equation 3 with the bound on distortion in Equation 7: LUB ≡ Z Z p∗(x)p∗(z|x)log \u0012p∗(z|x) r(z) \u0013 dxdz − Z Z Z p∗(x)p∗(y|x)p∗(z|x)log (c(y|z)) dxdydz +min \u001a Hp∗(Y ), − Z Z c(y, z)log (c(y|z)) dydz \u001b (8) It is easy to verify that the bound holds for all β ≥ 0 such that LIB = β · I (Z; X) − I (Z; Y ). 3.2. Variational approximation Following the same variational approach as in VIB, we define LV UB as a new tractable upper bound for the IB functional. Let p∗(x, y, z) be the unknown joint distribution, e(z|x) a variational encoder approximating p∗(z|x) and c(y|z) a variational classifier approximating p∗(y|z): LV UB ≡ β Z Z p∗(x)e(z|x)log \u0012e(z|x) r(z) \u0013 dxdz − Z Z Z p∗(x)p∗(y|x)e(z|x)log (c(y|z)) dxdydz (9) −min ( Hp∗(Y ), − Z Z Z p∗(x)e(z|x)c(y|z)log (c(y|z)) dxdydz ) ≥LIB 3.3. Empirical estimation We proceed to model VUB using DNNs and optimize it using Monte Carlo sampling over some training dataset. Let eϕ be a stochastic DNN encoder with parameters ϕ applying the reparameterization trick such that eϕ(x) ∼ N(µ, Σ) and let Cλ be a discrete classifier DNN parameterized by λ such that Cλ(ˆz) ∼ Categorical. ˆLV UB ≡ 1 N N X n=1 \" β · DKL \u0000eϕ(xn) \f\f\f\fr(z) \u0001 (10) −P ∗(yn) · log (Cλ (eϕ(xn))) −min n H( ˆY ), H (Cλ (eϕ(xn))) o # As in VIB and VAE, eϕ(x) and r(z) are computed as spher- ical gaussians. eϕ(x) by using the first half of the encoder’s output entries as µ and the second as the diagonal Σ, and r(z) by a standard normal gaussian. 3.4. Interpretation Similarly to the confidence penalty suggested by Pereyra et al. (2017), the new derivation adds classifier regulariza- tion to the VIB objective. Regularizing the classifier might prevent it from overfitting, and is a possible remedy to the discrepancies in the ELBO loss observed by Alemi et al. (2018), as elaborated in Section 5. In terms of tightness we have that VUB is a tighter theoreti- cal bound on the IB objective than VIB for any Y such that H(Y ) > 0, and a tighter empirical bound for all Y . 4. Experiments We follow the experimental setup proposed by Alemi et al. (2017), extending it to NLP tasks as well. Image classifi- cation models were trained on the ImageNet 2012 dataset (Deng et al., 2009) and text classification over the IMDB sentiment analysis dataset (Maas et al., 2011). For each dataset, a competitive pre-trained model (Vanilla model) was evaluated and then used to encode embeddings. These embeddings were then used as a dataset for a new stochas- tic classifier net with either a VIB or a VUB loss function. Stochastic classifiers consisted of two ReLU activated linear layers of the same dimensions as the pre-trained model’s log- its (2048 for image and 768 for text classification), followed by reparameterization and a final softmax activated FC layer. Learning rate was 10−4 and decaying exponentially with a factor of 0.97 every two epochs. Batch sizes were 32 for ImageNet and 16 for IMDB. We used a single forward pass per sample for inference. Each model was trained and evalu- ated 5 times per β value with consistent performance. Beta values of β = 10−i for i ∈ {1, 2, 3} were tested since pre- vious studies indicated this is the best range for VIB (Alemi et al., 2017; 2018). Each model was evaluated using test set accuracy and robustness to various adversarial attacks over 5 Tighter Bounds on the Information Bottleneck with Application to Deep Learning the test set. For image classification we employed the untar- geted Fast Gradient Sign (FGS) attack (Goodfellow et al., 2015) as well as the targeted CW L2 optimization attack (Carlini & Wagner, 2017), (Kaiwen, 2018). For text classifi- cation we used the untargeted Deep Word Bug attack (Gao et al., 2018) as well as the untargeted PWWS attack (Ren et al., 2019), (Morris et al., 2020). All models were trained using an Nvidia RTX3080 GPU. Code to reconstruct the experiments is available in the following github repository: https://github.com/hopl1t/vub. 4.1. Image classification A pre-trained inceptionV3 (Szegedy et al., 2016) base model was used and achieved a 77.21% accuracy on the ImageNet 2012 validation set (Test set for ImageNet is unavailable). Note that inceptionV3 yields a slightly worse single shot accuracy than inceptionV2 (80.4%) when run in a single model and single crop setting, however we’ve used Incep- tionV3 over V2 for simplicity. Each model was trained for 100 epochs. The entire validation set was used to measure accuracy and robustness to FGS attacks, while only 1% of it was used for CW attacks as they are computationally expensive. 4.1.1. EVALUATION AND ANALYSIS Image classification evaluation results are shown in Table 1, examples of successful attacks are shown in Figures 4, 5. The empirical results presented in Table 1 confirm that while VIB and VUB reduce performance on the validation set, they substantially improve robustness to adversarial attacks. Moreover, these results demonstrate that VUB significantly outperforms VIB in terms of validation set accuracy while providing competitive robustness to attacks. A comparison of the best VIB and VUB models further substantiates these findings, with statistical significance confirmed by a p-value of less than 0.05 in a Wilcoxon rank sum test. 4.2. Text classification A fine tuned BERT uncased (Devlin et al., 2019) base model was used and achieved a 93.0% accuracy on the IMDB sentiment analysis test set. Each model was trained for 150 epochs. The entire test set was used to measure accuracy, while only the first 200 entries in the test set were used for adversarial attacks as they are computationally expensive. 4.2.1. EVALUATION AND ANALYSIS Text classification evaluation results are shown in Table 2, examples of successful attacks are shown in Figure 3. In this modality VUB significantly outperforms VIB in both test set accuracy and robustness to both attacks. Moreover, VUB also outperomed the original model in terms of test β Val ↑ FGS ϵ=0.1 ↓ FGS ϵ=0.5 ↓ CW↑ Vanilla model - 77.2% 68.9% 67.7% 788 VIB models 10−3 73.7% ±.1% 59.5% ±.2% 63.9% ±.2% 3917 ±291 10−2 72.8% ±.1% 53.5% ±.2% 62.0% ±.1% 3318 ±293 10−1 72.1% ±.01% 58.4% ±.1% 62.0% ±.1% 3318 ±293 VUB models 10−3 75.5% ±.03% 62.8% ±.1% 66.4% ±.1% 2666 ±140 10−2 75.0% ±.05% 57.6% ±.2% 64.3% ±.1% 1564 ±218 10−1 74.8% ±0.09% 57.9% ±.5% 64.8% ±.5% 3575 ±456 Table 1. ImageNet evaluation scores for vanilla, VIB and VUB models, average over 5 runs with standard deviation. First column is performance on the ImageNet validation set (higher is better ↑), second and third columns are the % of successful FGS attacks at ϵ = 0.1, 0.5 (lower is better ↓) and the fourth column is the average L2 distance for a successful Carlini Wagner L2 targeted attack (higher is better ↑). Figure 3. Estimated information plane metrics per epoch for VUB trained on the IMDB dataset with β = 0.001. I(Z; X) is approxi- mated by H(R) − H(Z|X) and 1 CE(Y ; ˆY ) is used as an analog for I(Z; Y ). The epochs have been grouped and color-coded in inter- vals of 30 epochs in the order: Orange (0-30), gray (30-60), yellow (60-90), green (90-120) and red (120-150). We notice recurring patterns of distortion reduction followed by rate increase, resem- bling the ERM and representation compression stages described by Shwartz-Ziv & Tishby (2017). 6 Tighter Bounds on the Information Bottleneck with Application to Deep Learning Figure 4. Successful targeted CW attack examples. Images are perturbations of previously successfully classified instances from the ImageNet validation set. The target label is ’Soccer ball’. Average L2 distance required for a successful attack is shown on the left. The higher the required L2 distance the greater the visible change required to fool the model. Original and wrongly assigned labels are listed at the top of each image. Mind the difference in noticeable change as compared to the FGS perturbations presented in Figure 5, and between VIB and VUB perturbations. set accuracy. A comparison of the best VIB and VUB models further substantiates these findings, with statistical significance confirmed by a p-value of less than 0.05 in a Wilcoxon rank sum test. In addition to the above evaluation metrics, we also mea- sured approximated rate and distortion throughout training and plotted them on the information curve as shown in Fig- ure 3. We notice recurring patterns of distortion reduction followed by rate increase, resembling the ERM and repre- sentation compression stages described by Shwartz-Ziv & Tishby (2017). 5. Discussion While providing a complete framework for optimal data modeling, the IB, and it’s variational approximations, rely on three assumptions: (1) It suffices to optimize the mutual information metric to optimize a model’s performance; (2) Forgetting more information about the input while keep- ing the same information about the output induces better generalization; (3) Mutual information between the input, output and latent representation can be either computed or approximated to a desired level of accuracy. Our study strengthens the argument for using the Information Bottle- β Test↑ DWB↓ PWWS↓ Vanilla model - 93.0% 54.3% 100% VIB models 10−3 91.0% ±1.0% 35.1% ±4.4% 41.6% ±6.6% 10−2 90.8% ±0.5% 41.0% ±4.8% 62.9% ±14.3% 10−1 89.4% ±.9% 90.0% ±8.0% 99.1% ±0.9% VUB models 10−3 93.2% ±.5% 27.5% ±2.0% 28.4% ±1.3% 10−2 92.6% ±.8% 30.8% ±2.0% 50.0% ±4.8% 10−1 89.2% ±2.0% 99.2% ±0.5% 100% ±0% Table 2. Evaluation for vanilla, VIB and VUB models, average over 5 runs with standard deviation over the IMDB dataset. First column is performance on the test set (higher is better ↑), second is % of successful Deep Word Bug attacks (lower is better ↓) and the third column is % of successful PWWS attacks (lower is better ↓). Text perturbed with DWB gnreat historical movie, will not allow a viewer to leave once you begin to watch. View is presented differently than displayed by most school books on this sSubject [...] Text perturbed with PWWS the acting , costumes , music , cinematography and sound are all astoundingdumbfounding given the production ’s austere locales . Table 3. Examples of successful DWB and PWWS perturbations on a vanilla Bert model fine tuned over the IMDB dataset. The original input strings were perturbed such that inserted tokens are marked in underscored boldface and removed tokens in strikethrough. Both examples were classified correctly as ’Positive sentiment’ before the attack and ’Negative sentiment’ afterwards. 7 Tighter Bounds on the Information Bottleneck with Application to Deep Learning neck combined with variational approximations to obtain robust models that can withstand adversarial attacks. By de- riving a tighter bound on the IB functional, we demonstrate it’s utility as the Variational Upper Bound (VUB) objective for neural networks. We demonstrate that VUB outperforms the Variational Information Bottleneck (VIB) in terms of test accuracy while providing similar or superior robustness to adversarial attacks in challenging classification tasks of different modalities, suggesting an improvement in data modeling quality. Comparing VIB and VUB we observe that both methods promote a disentangled latent space by using a stochastic factorized prior, as suggested by Chen et al. (2018). In addition, both methods utilize KL regularization, enforcing clustering around a 0 mean which might increase latent smoothness. These traits can make it difficult for minor perturbations to significantly alter latent semantics, making the models more robust to attacks. In the case of VUB, the enhanced results induced by classifier regularization not only reinforce previous studies on the ELBO function, which suggest that overly powerful decoders diminish the quality of learned representations (Alemi et al., 2018), but also align with the confidence penalty proposed by Pereyra et al. (2017). In addition, we observed that in many cases VIB achieves lower validation set cross entropy while VUB achieves sig- nificantly higher test set accuracy. We attribute this gap to the VUB models becoming more calibrated, and we suggest that practitioners also monitor validation set accuracy and rate-distortion ratio during training. These metrics may be more informative indicators of model performance than val- idation set cross entropy alone, as validation cross entropy could increase as models become more calibrated. We made another interesting observation during our study regarding information plane behavior throughout the train- ing process. While previous research has documented the occurrence of error minimization and representation com- pression phases, our work revealed that these phases can occur in cycles throughout training. This finding is partic- ularly noteworthy because previous studies observed this phenomenon in simple toy problems, whereas our research demonstrated it in complex tasks of high dimensionality with unknown distributions. This suggests that this informa- tion plane behavior is not limited to simplified scenarios but is a characteristic of the learning process in more challeng- ing tasks as well. In conclusion, while the IB and its variational approxima- tions do not provide a complete theoretical framework for DNN data modeling and regularization, they offer a strong, measurable and theoretically grounded approach. VUB is presented as a tractable and tighter upper bound of the IB functional that can be easily adapted to any classifier DNN, including transformer based text classifiers, to significantly increase robustness to various adversarial attacks while in- flicting minimal decrease in test set performance, and in some cases even increasing it. This study opens many opportunities for further research. Besides further improvements to the upper bound, it is in- triguing to use VUB in self-supervised learning and in gener- ative tasks. Other possible directions, including measuring model calibration as proposed by Achille & Soatto (2018) are left for future work. Figure 5. Successful untargeted FGS attack examples. Images are perturbations of previously successfully classified instances from the ImageNet validation set. Perturbation magnitude is determined by the parameter ϵ shown on the left, the higher the more perturbed. Original and wrongly assigned labels are listed at the top of each image. Notice the deterioration of image quality as ϵ increases. 8 Tighter Bounds on the Information Bottleneck with Application to Deep Learning Impact This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. References Achille, A. and Soatto, S. Information dropout: Learning op- timal representations through noisy computation. IEEE Trans. Pattern Anal. Mach. Intell., 40(12):2897–2905, 2018. URL http://dblp.uni-trier.de/db/ journals/pami/pami40.html#AchilleS18. Alemi, A. A., Fischer, I., Dillon, J. V., and Murphy, K. Deep variational information bottleneck. In Proceedings of the International Conference on Learning Representations (ICLR), Google Research, 2017. Alemi, A. A., Poole, B., Fischer, I., Dillon, J. V., Saurous, R. A., and Murphy, K. Fixing a bro- ken elbo. In Proceedings of Machine Learning Research, volume 80, pp. 159–168, PMLR, 2018. URL http://dblp.uni-trier.de/db/conf/ icml/icml2018.html#AlemiPFDS018. Amjad, R. A. and Geiger, B. C. Learning rep- resentations for neural network-based classification using the information bottleneck principle. IEEE Trans. Pattern Anal. Mach. Intell., 42(9):2225–2239, 2020. URL http://dblp.uni-trier.de/db/ journals/pami/pami42.html#AmjadG20. Blahut, R. E. Computation of channel capacity and rate distortion function. IEEE Transactions on Information Theory, IT-18:460–473, 1972. doi: https://ieeexplore.ieee.org/document/1054855. URL https://ieeexplore.ieee.org/document/ 1054855. Carlini, N. and Wagner, D. A. Towards evaluating the ro- bustness of neural networks. In IEEE Symposium on Security and Privacy, pp. 39–57. IEEE Computer So- ciety, 2017. URL http://dblp.uni-trier.de/ db/conf/sp/sp2017.html#Carlini017. Chechik, G. et al. Gaussian information bottleneck. In Advances in Neural Information Processing Systems, 2003. URL https://proceedings. neurips.cc/paper/2003/hash/ 7e05d6f828574fbc975a896b25bb011e-Abstract. html. Chen, T. Q., Li, X., Grosse, R. B., and Duve- naud, D. Isolating sources of disentanglement in variational autoencoders. In Proceedings of the 32nd International Conference on Neural Infor- mation Processing Systems, pp. 2615–2625, 2018. URL http://dblp.uni-trier.de/db/conf/ nips/nips2018.html#ChenLGD18. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for lan- guage understanding. In Proceedings of the 2019 Confer- ence of the North American Chapter of the Association for Computational Linguistics: Human Language Tech- nologies, Volume 1 (Long and Short Papers), pp. 4171– 4186, Minneapolis, Minnesota, 2019. URL https: //www.aclweb.org/anthology/N19-1423. Fischer, I. The conditional entropy bottle- neck. Entropy, 22(9):999, 2020. URL http://dblp.uni-trier.de/db/journals/ entropy/entropy22.html#Fischer20. Gao, J., Lanchantin, J., Soffa, M. L., and Qi, Y. Black-box generation of adversarial text sequences to evade deep learning classifiers. In 2018 IEEE Security and Privacy Workshops (SPW), pp. 50–56. IEEE, 2018. Goodfellow, I. J., Shlens, J., and Szegedy, C. Explaining and harnessing adversarial examples. In ICLR (Poster), 2015. URL http://dblp.uni-trier.de/db/conf/ iclr/iclr2015.html#GoodfellowSS14. Guo, C., Pleiss, G., Sun, Y., and Weinberger, K. Q. On calibration of modern neural networks. In International conference on machine learning, pp. 1321–1330. PMLR, 2017. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta- vae: Learning basic visual concepts with a constrained variational framework. In ICLR (Poster), 2017. Kaiwen. pytorch-cw2, 2018. URL https://github. com/kkew3/pytorch-cw2. GitHub repository. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014. Kingma, D. P. and Welling, M. An introduc- tion to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307–392, 2019. URL http://dblp.uni-trier.de/db/ journals/ftml/ftml12.html#KingmaW19. 9 Tighter Bounds on the Information Bottleneck with Application to Deep Learning Maas, A., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., and Potts, C. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the asso- ciation for computational linguistics: Human language technologies, pp. 142–150, 2011. Morris, J., Lifland, E., Yoo, J. Y., Grigsby, J., Jin, D., and Qi, Y. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in nlp. In Proceed- ings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 119–126, 2020. Painsky, A. and Tishby, N. Gaussian lower bound for the information bottleneck limit. J. Mach. Learn. Res., 18:213:1–213:29, 2017. URL http://dblp.uni-trier.de/db/journals/ jmlr/jmlr18.html#PainskyT17. Pereyra, G., Tucker, G., Chorowski, J., Kaiser, L., and Hinton, G. E. Regularizing neural net- works by penalizing confident output distributions. In Proceedings of the International Conference on Learning Representations, OpenReview.net, 2017. URL http://dblp.uni-trier.de/db/conf/ iclr/iclr2017w.html#PereyraTCKH17. Ren, S., Deng, Y., He, K., and Che, W. Generating nat- ural language adversarial examples through probability weighted word saliency. In Proceedings of the 57th an- nual meeting of the association for computational linguis- tics, pp. 1085–1097, 2019. Shwartz-Ziv, R. and Tishby, N. Opening the black box of deep neural networks via information, 2017. URL http: //arxiv.org/abs/1703.00810. 19 pages, 8 fig- ures. Slonim, N. The information bottleneck: Theory and ap- plications. PhD thesis, Hebrew University of Jerusalem Jerusalem, Israel, 2002. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vi- sion. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2818–2826, 2016. Tishby, N. and Zaslavsky, N. Deep learning and the infor- mation bottleneck principle, 2015. Tishby, N., Pereira, F. C., and Bialek, W. The information bottleneck method. In The 37th annual Allerton Con- ference on Communication, Control, and Computing., Hebrew University, Jerusalem 91904, Israel, 1999. Wieczorek, A. and Roth, V. On the difference between the information bottleneck and the deep information bottleneck. CoRR, abs/1912.13480, 2019. URL http://dblp.uni-trier.de/db/journals/ corr/corr1912.html#abs-1912-13480. Ying, X. An overview of overfitting and its solu- tions. Journal of Physics: Conference Series, 1168 (2):022022, feb 2019. doi: 10.1088/1742-6596/1168/2/ 022022. URL https://dx.doi.org/10.1088/ 1742-6596/1168/2/022022. 10 "
}