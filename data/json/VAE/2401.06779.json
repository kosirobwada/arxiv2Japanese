{
    "optim": "VAE for Modified 1-Hot Generative Materials\nModeling\nA Step Towards Inverse Material Design\nKhalid El-Awady\nDepartment of Computer Science\nStanford University\nkae@stanford.edu\nAbstract\nWe investigate the construction of generative models capable of encoding physical\nconstraints that can be hard to express explicitly. For the problem of inverse\nmaterial design, where one seeks to design a material with a prescribed set of\nproperties, a significant challenge is ensuring synthetic viability of a proposed new\nmaterial. We encode an implicit dataset relationships, namely that certain materials\ncan be decomposed into other ones in the dataset, and present a VAE model capable\nof preserving this property in the latent space and generating new samples with the\nsame. This is particularly useful in sequential inverse material design, an emergent\nresearch area that seeks to design a material with specific properties by sequentially\nadding (or removing) elements using policies trained through deep reinforcement\nlearning.\n1\nIntroduction, Problem Statement, and Dataset\nInverse materials design, the problem of devising novel materials from first principles so they have\nan exact set of prescribed properties, has long been a holy grail of materials science [1]. Such a\ncapability would allow us to tune material parameters in a way that exhibits up-to-now unrealized\nmaterial behavior. The increasing performance of ML methods for computational quantum chemistry\n[2], along with recently available public datasets of materials and their properties [3], has attracted\nstrong interest in applying AI and machine learning techniques to their analysis, and generative ones\nfor tackling the inverse problem.\nThe most common way to represent materials in AI applications has been the so-called Simplified\nMolecular-Input Line-Entry System (SMILES) representation [4]. SMILES is a specification in the\nform of a line notation for describing the structure of chemical species using short ASCII strings [6].\nComputationally, it is a string obtained by printing the symbol nodes encountered in a depth-first\ntree traversal of a chemical graph. SMILES popularity stems from its ability to capture and encode\ninformation about a material including its atoms, bond structure, branching, and isotope information.\nIn deep learning architectures it has also been shown to work well with convolutional neural networks\nin capturing and encoding useful structural patterns such as ring structures. Figure (1) shows some\nexamples of the SMILES representation of materials.\nDespite its utility, SMILES has some limitations [5]:\n• Not all materials databases include a SMILES representation for arbitrary compounds and\nSMILES representations are not unique.\n• It is not clear how to ensure viability of generated SMILES materials for the inverse materials\ndesign problem.\nPreprint. Under review.\narXiv:2401.06779v1  [cond-mat.mtrl-sci]  25 Dec 2023\nFigure 1: Examples of the SMILES representation of a material.\n• SMILES encoding cannot easily capture relationships between compounds such as decom-\nposition For example, consider the following material decomposition (https://next-gen.\nmaterialsproject.org/materials/mp-559694?formula=Ga(MoS2)4):\nGa(MoS2)4\n=\n6\n91 · Ga31Mo6 + 49\n55 · MoS2 + 3\n70 · GaS\nThe decomposition property (and its inverse property - synthesis) is a key one of interest to us.\nRecently, a number of researchers (including the author) have demonstrated the use of reinforcement\nlearning to discover policies to synthesize viable materials using sequential additions or deletions of\nelements [7, 8]. A more natural material representation that readily captures decomposition is the\nmodified 1-hot material encoding. In our dataset there are 89 unique elements comprising all the\nmaterials. We sort the elements alphabetically by symbol and assign each a sequential index (Ac =\n1, Ag = 2, Al = 3, ...). We then represent each material as a 89-length vector where entry j is the\nnumber of atoms of that element in the molecule. For example the material, Ga(MoS2)4 would have\na 1 in position 26 (Gallium), a 4 in position 44 (Molybdenum), an 8 in position 66 (Sulfur), and zeros\nin all remaining entries in the vector. This is shown schematically in figure (2).\nFigure 2: Schematic representation of the modified 1-hot vector representation for Ga(MoS2)4.\nIn this work we explore a generative model for a modified 1-hot representation of materials. We show\nthat a variational autoencoder (VAE) architecture is capable of capturing the decomposition property\nand preserving it in the latent space. For our dataset we will use the Materials Project database [3],\nan open-access database of approximately 154,000 unique materials relevant to battery research.\nEach entry contains a variety of properties pertaining to thermodynamic stability, electronic structure,\nmagnetic properties, and others. The dataset also includes the constituent components in the dataset,\nif the material decomposes into others, and their relative weighting in the combination. An illustrative\nexample of one material, Ga(MoS2)4, is shown in figure (3).\n2\nRelated Work\nThe prior work of Gomez, et. al., [9] is usually cited as the first demonstration of the use of VAEs in\nnew molecule generation. They used a SMILES representation to produce a latent representation\nof a material and show that the latent variables correlate well with certain material properties and\ntherefore have efficacy in prediction. The work, though, suffers from the fact that from a generative\nperspective there is no enforcement that a sampled SMILES representation is either viable or valid.\nGAN-based models were introduced by Nouria, et. al. [10]. In their work the authors propose a GAN\narcihtecture they call, CrystalGAN, whose goal is to generate novel ternary metal hybrids from their\n2\nFigure 3: Example material entry in the Materials Project database: Ga(MoS2)4.\nobserved binary structures. The authors demonstrate an ability to generate these ternary crystals, and\nthough it has been scientifically known that novel ternary compounds can be predicted from known\nbinaries, whether this approach can be applied to predict complex crystal structures is uncertain.\nTherefore ensuring general viability is still an open question. A further review of GAN application in\ninverse material design can be found in [11].\nWhile both the VAE and GAN approaches have shown promise, challenges remain including [12, 13]:\n• generating materials with more than a small handful of target properties,\n• constraining sampling of the models to synthetically viable materials, and\n• dealing with what are still relatively small datasets (hundreds of thousands of data samples,\nnot billions).\nOur work deals primarily with addressing the viability question, specifically by exploring generative\nmodels that preserve material decomposition which can be exploited in the subsequent synthesis of\nviable new materials.\n3\nApproach\nIn the interest of starting simply, we choose a VAE-based architecture. To formulate a probabilistic\nmodel for the material generation process we examine the dataset and notice the following:\n• Materials’ modified 1-hot vectors are sparse. The most complex material in the dataset has\n10 unique elements, though 96% of the materials have 6 or fewer elements.\n• Chemical formulae are invariant under scaling and generally materials are represented such\nthat the atomic counts of different elements have a greatest common divisor of 1.\n• Examination of the element pair-wise correlation matrix across the dataset shows that here\nis almost no correlation between the presence of elements in a given material. Of the 3,916\nunique off-diagonal element pairs only 7 have correlations exceeding 0.2 in magnitude. The\nlargest is the pair (Nb, O) with a correlation coefficient of 0.54. But for practical purposes\nwe model the elements as uncorrelated.\nAccordingly we postulate a model of our material as a set of independent Bernoulli random variables\nindicating the presence of the atom in the material or not. To account for the relative weight of the\n3\nFigure 4: Model architecture.\natom we normalize the entries in the modified 1-hot vector so that it always sums to one in the dataset.\nThe full model architecture is shown in figure (4).\nA summary of the encoding / sampling procedure is as follows:\n• Material Representation: the chemical formula of the material is converted to a modified\n1-hot representation. For example the material, Ga(MoS2)4 would have a 1 in position 26\n(Gallium), a 4 in position 44 (Molybdenum), an 8 in position 66 (Sulfur), and zeros in all\nremaining entries in the vector.\n• Normalization: the modified 1-hot vector is normalized to sum to one. For example the\nmaterial, Ga(MoS2)4 which has a total of 13 atoms is normalized by dividing the elements\nof the modified 1-hot vector by 13. This normalized vector will be our data samples, x(i).\nWe also include a class variable, Y , representing the number of decomposition components\nof the given material data point. In our dataset materials decompose into between 0 and 9\ncomponents so that there are 10 total classes.\n• Encoder: we propose a deep network architecture similar to the one developed in class for\nMNIST digit generation. This was selected due to the similarity of our normalized 1-hot\nrepresentation to a gray-scale representation of images. The architecture comprises the\nfollowing layers:\nLinear → ELU → Linear → ELU → Linear\nThe output of the last layer is a 2 × n vector representing the mean and variance,\n(µθ(x), vθ(x)) of a latent variable, Z ∼ N(µθ, vθ) of dimension, n. The classifier has\na similar architecture to the encoder with a single output representing the material’s class\n(number of components). The main parameter we tune is the number of nodes per hidden\nlayer which we keep common between all layers and for both encoder, decoder, and classifier\nfor simplicity. Tuning of this parameter is discussed in the section on results.\nThe training of the VAE utilizes a negative ELBO loss function.\n• Decoder: The decoder is a mirror image of the encoder, comprising the following layers:\nLinear → ELU → Linear → ELU → Linear\n• Threshold: In the generation process, the decoder returns a continuous vector whose\nelements are interpreted as the contribution of that element to the overall atomic count. To\nconvert this to a vector with a discrete number of unique atoms we apply a threshold, T,\nbelow which elements are zeroed out. The value of the threshold is tuned to match the\ndistribution of the number of elements per material in the dataset. We find a value of 0.04\nworks well. The analysis for this parameter is discussed in more detail in the results section.\n• Scaling: after thresholding, the sample is converted to a material representation by scaling\nup all entries by 1/T, then computing the greatest common divisor, g, for the elements and\nreducing the formula to its simplest form.\nThe following section includes a description of the model tuning process and analysis of the results.\n4\n4\nTuning and Results\n4.1\nModel Tuning\nWe use a latent variable dimension of 10 and train for 2,000 epochs. Due to the sparsity of materials\nwith 8 or 9 elements we oversample them in the dataset. After training the model we collect 10,000\nsamples from the model and compare some statistical properties of the two datasets. To assess the\nquality of the model we consider three metrics:\n1. Negative ELBO: the loss function used in the model training assessed on the training set\nwhich comprises 20% of the samples in our dataset.\n2. KL divergence between the probability distribution of elementwise prevalence of elements\nin the dataset vs the samples. The distribution of elementwise prevalence is computed by\nsumming the modified 1-hot vectors across samples to obtain a total count of element atoms\nin the dataset then normalizing across all elements.\n3. Comparison of the distribution of number of elements in a sampled material vs the dataset.\nFigure (5) shows the impact of the size of the hidden layer on the quality of samples. We vary\nthe number of nodes in the hidden layer from 50 to 500 and compare the test set NELBO and KL\ndivergence of elementwise prevalence between the model and data. We see that a hidden layer\nof 100 nodes provides a good combination of the two metrics and that is the chosen value for\nsubsequent analysis. Further, this and subsequent plots were also used to tune the threshold parameter\nto T = 0.04.\nFigure 5: Impact of the size of the hidden layer in the VAE on the model performance. We choose\n100 hidden nodes as the ’optimal’ value.\nFigure (6) shows a comparison of the elementwise prevalence probability vectors between the data\nand samples. The KL divergence between the two is small: 0.08. But we do notice some over-\nrepresentation on certain elements such as O, F, C, N, Li, and P. And certain other elements are\nslightly under- represented including Mg, Co, Cu, S, and Si.\nFigure (7) shows the distribution of the number of elements in a material in the data and model\nsamples. We see a good match with the exception of some under-representation in the model of\nmaterials with a high number of elements (over 6). We are comfortable with this discrepancy since\nviability and synthesizability concerns generally grow quickly with complex many-atomed materials.\n5\nFigure 6: Comparison of the elementwise prevalence probability distribution between the dataset and\nmodel. Overall the KL divergence is low (0.08).\nFigure 7: Comparison of the distribution of the number of elements in a material in the dataset and\ngenerated samples.\n4.2\nAnalysis of Decomposition in the Latent Space\nWith a tuned model we proceed to analyze how well the model captures and preserves decomposition.\nOur dataset includes the constituent datapoints making up any given material along with their weights.\nNamely for any datapoint, x(i), we know\nx(i)\n=\nci\nX\nk=0\nakx(k)\nwhere ci is the number of components making up material, x(i), and that varies from 0 to 9 in our\ndataset, and ak are known constants.\nFor each of the dataset samples we utilize our encoder to compute the latent vector means: µθ(x(i)).\nNext we investigate how those vectors compare to the re-constituted latent vector composed of the\n6\nweighted sum of the latent mean components. For comparison we use the cosine similarity. Thus for\neach material that decomposes into 2 or more components we compute:\nlatent space decomposition similarity\n=\n< µθ(x(i)),\nci\nX\nk=0\nakµθ(x(k)) >\nWe plot the histogram of similarities by number of components for the dataset. The results are shown\nin figure (8).\nFigure 8: Latent space decomposition similarity of means and their weighted combined constituents.\nWe see remarkable similarity preservation of the decomposition property in the latent space. This\nindicates that our latent representation can learn and preserve the decomposition property and linear\nmanipulations on latent vectors are largely reflective of similar manipulations on the original material.\n5\nConclusions\nIn this study we present evidence in support of using modified 1-hot vector representations of\nmaterials versus the more commonly used SMILES representation. Such representations conveniently\npreserve decomposition, and by extension synthesis. This is particularly useful in sequential inverse\nmaterial design, an emergent research area that seeks to design a material with specific properties\nby sequentially adding (or removing) elements using policies trained through deep reinforcement\nlearning. We anticipate that RL policies designed to manipulate our latent space will provide\nadvantages in terms of flexibility of the action space on the continuous latent space over directly\ntrying to manipulate the modified 1-hot vectors while trying to simultaneously preserve constraints\non material viability.\nThe next steps for this work could include:\n7\n• Investigation of the predictive power of the latent space on other material properties of\ninterest, such as the phase diagram.\n• Augmention of the dataset with other material properties and analysis of the ability of the\nlatent space to capture those as well. Would the latent space be able to capture non-linear\neffects of material property superposition?\n• Exploration of training RL policies on the latent space and analyzing whether these policies\nreduce the complexity of viability constraints on inverse material design.\nReferences\n[1] A. Zunger, \"Inverse design in search of materials with target functionalities,\" Nat Rev Chem 2, 0121 (2018).\nhttps://doi.org/10.1038/s41570-018-0121.\n[2] S. Kurth, M.A.L. Marques, E.K.U. Gross, \"Density-Functional Theory,\" Encyclopedia of Condensed\nMatter Physics 2005, Pages 395-402. https://doi.org/10.1016/B0-12-369401-9/00445-9.\n[3] The Materials Project. https://materialsproject.org/\n[4] R. Apodaca, \"Chemical Line Notations for Deep Learning: DeepSMILES and Beyond,\" March 30,\n2019, https://depth-first.com/articles/2019/03/19/chemical-line-notations-for-deep-learning-deepsmiles-\nand-beyond.\n[5] S. Lim, S. Lee, Y. Piao, M. Choi, D. Bang, J. Gu, and S. Kim, \"On modeling and utilizing chemical\ncompound information with deep learning technologies: A task-oriented approach,\" Computational and\nStructural Biotechnology Journal 20 (2022) 4288–4304.\n[6] Simplified molecular-input line-entry system. Wikipedia,\nhttps://en.wikipedia.org/wiki/\nSimplified_molecular-input_line-entry_system.\n[7] E. Pan, C. Karpovich, and E. Olivetti, \"Deep Reinforcement Learning for Inverse Inorganic Materials\nDesign,\" arXiv:2210.11931v1 [cond-mat.mtrl-sci] 21 Oct 2022.\n[8] R. Lacombe, L. Hendren, and K. El-Awady, “AdsorbRL: Deep Multi-Objective Reinforcement Learning\nfor Inverse Catalysts Design,” AI for Accelerated Materials Design - NeurIPS 2023 Workshop.\n[9] R. Gómez-Bombarelli, J. Wei, D. Duvenaud, J. Hernández-Lobato, B. Sánchez-Lengeling, D. Sheberla, J.\nAguilera-Iparraguirre, T. Hirzel, R. Adams, and A. Aspuru-Guzik, \"Automatic Chemical Design Using a\nData-Driven Continuous Representation of Molecules,\" CS Cent. Sci. 2018, 4, 2, 268–276.\n[10] A. Nouira, N. Sokolovska, J.-C. Crivello, Crystalgan, \"Learning to discover crystallographic structures\nwith generative adversarial networks,\" 2018, arXiv:1810.11203.\n[11] R. Jabbar, R. Jabbar, S. Kamoun, \"Recent progress in generative adversarial networks applied to inversely\ndesigning inorganic materials: a brief review,\" Computational Materials Science 213 (2022) 111612.\n[12] S. Lu, Q. Zhou, X. Chen, Z. Song, and J. Wang, \"Inverse design with deep generative models: next step in\nmaterials discovery,\" National Science Review 9: nwac111, 2022.\n[13] D. Menon and R. Ranganathan, \"A Generative Approach to Materials Discovery, Design, and Optimiza-\ntion,\" ACS Omega 2022, 7, 25958-25973.\n8\n"
}