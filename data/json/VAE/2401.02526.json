{
    "optim": "1 \n \nBranched Variational Autoencoder Classifiers \nAhmed Salah*, David Yevick  \nDepartment of Physics and Astronomy, University of Waterloo, ON N2L 3G1, Canada \n*Corresponding author email address: asalah@uwaterloo.ca \nAbstract– This paper introduces a modified variational autoencoder (VAEs) that contains an \nadditional neural network branch. The resulting “branched VAE” (BVAE) contributes a \nclassification component based on the class labels to the total loss and therefore imparts categorical \ninformation to the latent representation.  As a result, the latent space distributions of the input \nclasses are separated and ordered, thereby enhancing the classification accuracy. The degree of \nimprovement is quantified by numerical calculations employing the benchmark MNIST dataset \nfor both unrotated and rotated digits. The proposed technique is then compared to and then \nincorporated into a VAE with fixed output distributions.  This procedure is found to yield improved \nperformance for a wide range of output distributions.   \n \nKeywords: variational autoencoder, neural networks, classification, clustering. \n \n \n1. Introduction \nClustering algorithms, which sort similar data samples into segregated groups, are employed \nin numerous practical applications involving complex and often noisy data, such as false news \ndetection [1] and document analysis [2].  In such cases, meaningful features for sample assignment \nare often identified by either of two clustering procedures: similarity-based clustering such as \nspectral clustering, which involves computing a distance matrix [3], and feature-based clustering \nrepresented by K-means and Gaussian mixture models, which instead minimizes the sum of the \nsquared errors between the feature values of the data points and the centroid of the cluster to which \nthey belong.  \nWhile at first sight unrelated to clustering algorithms, deep generative models such as \nvariational autoencoders (VAE) employ neural networks to organize input data in a manner that \nenables the subsequent creation of synthetic samples that exhibit or interpolate the features of the \n2 \n \ninput data [4].  In particular, the high-dimensional input data is encoded through a neural network \ninto a few dimensional latent variable space.  The latent variables are then passed through a \ndecoder neural network that generates an output distribution in the original high-dimensional \nspace. Standard VAEs minimize the difference between this output distribution and the input \ndistribution through variational inference. This procedure can compensate for insufficient data and \nunbalanced labels and has found applications in diverse fields [7] including intrusion detection [5] \nand target recognition [6].  \nThis paper, however, instead focuses on the ability of the autoencoder latent data representation \nto effect clustering.  Prior work involving clustering that have incorporated autoencoders have \nincluded computer vision applications, pattern recognition [8, 9] speech and audio recognition [10, \n11] wireless communication [12] and text classification [13]. These examples typically apply \nclustering algorithms, such as K-means, to the latent variables of the autoencoder [14]. However, \nthe distribution of classes in the latent space may not always be suitable for such procedures. \nNumerous generative models and clustering techniques for deep neural networks currently \nexist. The Variational Fair Autoencoder (VFAE) enhances the separation between latent variables \nand noise by introducing a regularization term based on the maximum mean discrepancy into the \nloss function [15].  The Deep Clustering Network (DCN) [14] employs a gradient descent-based \nformula to address numerical issues with clustering centers.  Deep Embedded Clustering (DEC), \n[16] jointly learns cluster assignments and feature representations through deep neural networks. \nHowever, DEC, like K-means, cannot model the data generation process and, therefore, cannot \ngenerate new, synthetic, samples.  \nIn contrast, neural network-based clustering models can learn high-quality representations that \ncapture data characteristics and can then generate new data samples, thus combining the strengths \nof deep clustering and generative models. For example, Tian et al. [17] introduced a \ncomprehensive clustering framework that uses the Alternating Direction of Multiplier Method \n(ADMM) to update clustering parameters.  The Information Maximizing Variational Autoencoder \n(IM-VAE) instead both increases the mutual information between the latent variables and the \nsamples and minimizes the divergence between the approximate posterior and the true posterior \ndistribution [18]. The Nouveau Variational Autoencoder (NVAE) employs deep separable \nconvolution and batch normalization to enhance the quality of the generated samples [19]. \n3 \n \nHowever, the features generated by traditional autoencoders rely on unsupervised techniques that \ndo not incorporate label information and are therefore not optimized for classification.  Therefore, \nwhen augmenting data for classification purposes, the class labels must be present during feature \nextraction to ensure that the latent space is discriminative. \nTo address this limitation, researchers have introduced supervised or semi-supervised \nautoencoders. For instance, Gao et al. [20] implemented a supervised autoencoder (AE) for face \nrecognition by incorporating a similarity preserving component into the AE's objective function, \nensuring that images of the same person are treated as similar. Another approach, which was shown \nto distinguish rotated digits accurately through their latent space representations, implemented an \nobjective function that evaluates the output linked to each rotated digit by comparing it to a fixed \nreference digit [21]. Similarly, the Conditional Variational Autoencoder (CVAE) [22] incorporates \none-hot encoded labels in order to utilize category information as a control mechanism. \nAbbasnejad [23] implemented semi-supervised classification by implementing a Dirichlet process \nthat dynamically adjusted the mixing coefficients of a combination of VAEs according to the \nproperties of the input data. The Orthogonal AutoEncoder (OAE) further ensures the orthogonality \nof the resulting embedding while the Clustering framework based on Orthogonal AutoEncoder \n(COAE) additionally enables both the extraction of latent embeddings and the generation of \nclusters [24]. A recent method introduced by Song et al. [25] combines both the reconstruction \nerror and the error derived from comparing K-means clustering with the encoded image into a \nsingle objective function.  In this manner, the dissimilarity between the original latent space \nlearned by AE and the feature space derived from it using traditional machine learning (ML) \ntechniques is minimized. Similarly, The Fisher Variational Autoencoder (FVAE) integrates the \nFisher criterion into the VAE by incorporating the Fisher regularization term into the loss function \nwith the aid of the class labels [26]. By maximizing the distance between different classes and \nminimizing the distance within the same class the latent variables can be more accurately \nclassified. A similar approach involving the addition of a supervised technique to the VAE to \nimprove classification accuracy will be introduced below. \nIn this work, we introduce a novel regularized Variational Autoencoder known as the Branched \nVariational Autoencoder (BVAE) in order to improve the identification of different classes from \nthe associated clusters in the VAE latent space.  This is achieved by processing the inputs within \neach cluster with traditional machine learning methods that are applied to a secondary classifier \n4 \n \nbranch.  The objective function then combines two elements, the reconstruction error of the \nvariational autoencoder and the classifier branch loss term. Both the data representation and the \nclassifier loss are iteratively updated. The resulting procedure is applicable to a broad range of \ndatasets as will be demonstrated in the context of the MNIST dataset by employing the procedure \nto enhance the performance of the K-Means clustering algorithm as well as to identify randomly \nrotated digits.   \n \n2. Variational Autoencoders \nVariational Autoencoders are generative models that represent the actual distribution of \ndata samples by a low-dimensional approximate “latent variable” distribution However, VAEs \nassume that the latent variable distribution is continuous and follows a normal distribution. This \nassumption does not necessarily reflect the true distribution of complex data, leading to a mismatch \nbetween the assumed and actual distributions. In addition, different classes may not be effectively \nseparated in the latent space under this assumption, especially when two distinct classes have very \nclose mean and variance values [27-29]. In the VAE, the latent space variables are mapped to an \nimage in the space of the input variables as indicated in Fig. 1 which depicts the VAE architecture. \nIn this model, 𝑧 represents the latent variable, while 𝜇 and 𝜎 denote the mean and standard \ndeviation of 𝑧, respectively. After the latent space is determined, mapping any point in the latent \nspace back to the image space generates a novel image. The conditional distributions, 𝑞𝜙(𝑧|𝑥) and \n𝑝𝜃(𝑥|𝑧) that are learned by the encoder and decoder, respectively are termed recognition and \ngeneration models, while 𝜑  and 𝜃 represent the corresponding model parameters. The VAE \ntypically employs Gaussian distributions with diagonal covariances for both the encoder 𝑞𝜑(𝑧|𝑥) \nand decoder 𝑝𝜃(𝑥|𝑧). The estimated posterior distribution 𝑞𝜑(𝑧|𝑥) is utilized to approximate the \nunknown prior distribution 𝑝(𝑧) that represents the distribution of latent variables 𝑧 in the absence \nof any specific input data 𝑥. \n \nFig. 1 Variational autoencoder structure \n5 \n \n \nOne main issue is that the marginal likelihood given by \n \n𝑝𝜃(𝑥) = ∫ 𝑝𝜃(𝑥, 𝑧) 𝑑𝑧 \n(1) \nis intractable as the integral does not have an analytic solution [30]. This intractability is related to \nthe intractability of 𝑝𝜃(𝑧|𝑥), where \n \n𝑝𝜃(𝑧|𝑥) = 𝑝𝜃(𝑥, 𝑧)\n𝑝𝜃(𝑥) = 𝑝𝜃(𝑥|𝑧) 𝑝𝜃(𝑧)\n𝑝𝜃(𝑥)\n \n(2) \nFurthermore 𝑝(𝑧) cannot be directly estimated [29]. To tackle the intractability problem , the \nposterior distribution 𝑞𝜑(𝑧|𝑥) is introduced where 𝜑 refers to the parameters of this inference \nmodel that is optimized so that 𝑞𝜑(𝑧|𝑥) ≈ 𝑝𝜃(𝑧|𝑥). \nThe optimization objective of the VAE is the evidence lower bound (ELBO) [30], and is \nrealized through maximum likelihood estimation, where, for any 𝑞𝜑(𝑧|𝑥), the log likelihood \nfunction of the VAE is \n \n \nlog 𝑝𝜃(𝑥) = 𝐾𝐿 (𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧|𝑥)) + 𝐿𝜃,𝜑(𝑥) \n(3) \n \nThe first term is the Kullback-Leibler (KL) divergence that quantifies the difference between the \ndistributions 𝑞𝜑(𝑧|𝑥) and 𝑝𝜃(𝑧|𝑥), \n \n𝐾𝐿 (𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧|𝑥)) = 𝐸𝑞𝜑(𝑧|𝑥) [𝑙𝑜𝑔 [𝑞𝜑(𝑧|𝑥)\n𝑝𝜃(𝑧|𝑥)]] \n(4) \n𝐿𝜃,𝜑(𝑥), which is the evidence lower bound (ELBO) of the likelihood function can be written as  \n \n𝐿𝜃,𝜑(𝑥) = 𝐸𝑞𝜑(𝑧|𝑥) [log (𝑝𝜃(𝑥, 𝑧)\n𝑞𝜑(𝑧|𝑥))] \n                = 𝐸𝑞𝜑(𝑧|𝑥)[log 𝑝𝜃(𝑥|𝑧) + log 𝑝𝜃(𝑧) −log 𝑞𝜑(𝑧|𝑥)] \n         = 𝐸𝑞𝜑(𝑧|𝑥)[log 𝑝𝜃(𝑥|𝑧)] − 𝐾𝐿(𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧)) \n(5) \n \nSince the KL divergence is non-negative, the ELBO is a lower bound on the log likelihood \nof the data as from Eq. (3), 𝐿𝜃,𝜑(𝑥) = log 𝑝𝜃(𝑥) − 𝐾𝐿 (𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧|𝑥)) ≤ log 𝑝𝜃(𝑥), hence \nmaximizing 𝐿𝜃,𝜑(𝑥) maximizes 𝑝𝜃(𝑥), and further minimizes the difference between the \n6 \n \napproximate 𝑞𝜑(𝑧|𝑥) and the true posterior 𝑝𝜃(𝑧|𝑥), therefore the VAE asymptotically minimizes \nthe loss function \n \n \nLossVAE = −𝐸𝑞𝜑(𝑧|𝑥)[log 𝑝𝜃(𝑥|𝑧)] +  𝐾𝐿(𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧)) \n(6) \n \ncomposed of the sum of the negative of the reconstruction error and the KL divergence. If 𝑝𝜃(𝑧) \nis assumed to be Gaussian with 𝑁(0, 𝐼), and setting  𝑞𝜑(𝑧|𝑥) = ∏ 𝑁(𝑧𝑖; 𝜇𝑖, 𝜎𝑖\n2)\n𝑖\n the KL divergence \ncan be evaluated as [21], \n \n𝐾𝐿 (𝑞𝜑(𝑧|𝑥)|𝑝𝜃(𝑧)) = − 1\n2 ∑(1 + 𝑙𝑜𝑔𝜎𝑖\n2(𝑥) − 𝜎𝑖\n2(𝑥) − 𝜇𝑖\n2(𝑥))\n𝑘\n𝑖=1\n \n(7) \n \nSince the sampling of the latent variable 𝑧 is non-differentiable, however, backpropagation \ncannot be employed in the gradient descent algorithm which would greatly increase the difficulty \nof optimizing the network parameters. This problem is circumvented with the reparameterization \ntrick which samples 𝜖 in  \n \n𝑧 =  𝜇 +  𝜎 ∗ 𝜖 \n(8) \n \nfrom a normal distribution with mean 0 and a diagonal covariance matrix with elements 𝜎. This \neffectively transforms the sampling of 𝑧 into a linear operation enabling backpropagation.  As 𝜖 is \na random variable, any point in proximity to a latent position at which inputs are encoded yields a \nreconstructed image that resembles the averaged input data mapped to or near the position.  \n \n3. Experimental results \n3.1 Structure and Objective Function of the BVAE \nThe BVAE architecture shown in Fig. 2 consists of two main components: the VAE of Fig. \n1 and a classifier branch such as a neural network, k-nearest neighbors, or random forest that \nsamples the latent space of the VAE. The VAE learns latent features, while the classifier branch \npromotes cluster formation. The latent variables 𝑧 together with the associated labels are input into \nclassifier in order to compensate for the absence of label information in the standard VAE. The \n7 \n \ntraining phase of the BVAE then incorporates the classifier branch loss, 𝐿𝐶, defined as the \ncategorical cross entropy between the labels predicted from the classifier branch and the true \nlabels, to promote the clustering of related features.  \n \n \n \nFig. 2 VAE with a classifier branch \n \nIncluding 𝐿𝐶, the objective function of the BVAE is  \n \n𝐿𝐵𝑉𝐴𝐸 = 𝛼𝐿𝑐𝑜𝑛 + 𝐿𝐾𝐿 +  𝜆𝐿𝐶 \n \n(9) \nwhere 𝜆 is a regularization parameter that facilitates balancing of the input dataset and 𝐿𝐾𝐿 and 𝐿𝐶 \ndenote the KL divergence and classifier branch loss respectively. The reconstruction loss, 𝐿𝑐𝑜𝑛, \nquantifies the error incurred when the input data is regenerated form the latent space distribution \nby the VAE, while the regularization term, 𝜆𝐿𝐶  incorporates the influence of the prior distribution, \nwhich is typically Gaussian on the latent space distribution. A further regularization parameter 𝛼 \nis introduced in Eq. (9) to control the relative amplitude of the reconstruction loss. Accordingly, \nminimizing the loss function in the BVAE, not only reduces the reconstruction error but also the \nKL divergence as well as the classification error among the latent variables. Note that Eq. (9) \nyields the standard VAE objective function when 𝜆 = 0. \n \n3.2 Training and Optimization \n \nThe BVAE implements encoding, sampling, and decoding in the same manner as the \nstandard VAE. The encoder employs the recognition model 𝑞𝜑(𝑧|𝑥) to compute the mean 𝜇 and \nstandard deviation 𝜎 of the latent variables from the input data 𝑥. In the sampling step, 𝜖 is \n8 \n \ngenerated from a standard normal distribution  𝑁(0, 𝐼) implemented through the variables 𝜇 and 𝜎, \nand the reparameterization trick employed. The decoder finally employs the generative model \n𝑝𝜃(𝑥|𝑧) to reconstruct the output pattern 𝑥̂. After training, the generative model 𝑝𝜃(𝑥|𝑧) can \nsynthesize new samples from appropriate choices of the latent variables. \n \nThe optimization process in VAEs maximizes the likelihood of the observed data, which is \ntermed the evidence lower bound (ELBO) by optimizing the model parameters 𝜑 and 𝜃 of the loss \nfunction through stochastic gradient descent (SGD) and back propagation.  The reconstruction \nloss, KL divergence, and, in the case of the BVAE, the classifier loss term introduces further \nparameters associated with the weighting of the individual loss terms, all of which must also be \nminimized.  Therefore, the range of acceptable metaparameters in e.g. the BVAE is more restricted \nthan in standard VAE calculations.  \n \n4. Results and Discussion \n \n4.1 Implementation  \n \nThe proposed approach is implemented by modifying and applying the readily manipulated \nTensorFlow based code of Chapter 12 of [31] to the standard benchmark MNIST dataset consisting \nof 70,000 images of handwritten digits discretized as 28×28 arrays of 8-bit pixels.  The encoder \nconsists of an input layer, two 2-D convolutional layers of sizes 32 and 64 respectively with 3×3 \nfilter functions, strides = 2, and padding = “same”. The resulting 7×7 filters are then flattened and \nfed to a 16-element dense layer followed by the dense two-dimensional latent space layer.  The \ndecoder network consists of a 3136 element dense layer that is equivalent to the product 7 ∙ 7 ∙ 64 \nto later create a feature map that is 7 units wide and 7 units high and depth of 64 (number of \nchannels), so it is then reshaped into a 7 × 7 × 64 tensor and fed into two 2-D transpose \nconvolutional layers  with 64 and 32 filters respectively each of size 3×3 and finally a 2-D 3×3 \nfilter layer that compresses the information present in the filter outputs from the second transpose \nlayer into the 28×28 matrix reconstructed image. All layers employ RELU activation functions \nexcept for the standard sigmoid final layer.  The VAE is trained with the Adam optimizer until \nconvergence is attained at 30 epochs for a batch size of 512.   \n9 \n \n \n4.2 Clustering Metrics \n \n  \nThe classification accuracy is determined by inserting the VAE latent variables of each test \ndata record and label into a NN, while the degree of clustering is quantified with either the \nNormalized Mutual Information (NMI), Accuracy (ACC) or the adjusted Rand Index (ARI) \nprocedures.  In particular, \n \n• \nThe NMI quantifies the similarity between pairs of clusters. In terms of information theory,  \n𝑁𝑀𝐼 =\n𝑀𝐼(𝑐, 𝑙)\nmax (𝐻(𝑐), 𝐻(𝑙)) \nwhere 𝑀𝐼(𝑐, 𝑙) denotes the mutual information between the predicted clusters (𝑐) and the \nground truth labels (𝑙), while 𝐻 denotes the entropy. \n• \nThe ACC determines the mean accuracy based on the alignment between the ground truth \nlabels and the predicted assignments according to \n𝐴𝐶𝐶 = 𝑚𝑎𝑥𝑚\n∑\n1{𝑙𝑖 = 𝑚(𝑐𝑖)}\n𝑁\n𝑖=1\n𝑁\n \nin which 𝑙𝑖 represents the true label, 𝑐𝑖 is the clustering assignment and the index 𝑚 ranges \nover all possible one-to-one mappings between 𝑐𝑖 and 𝑙𝑖. \n• \nThe ARI measures the correspondence between the true labels and the predicted clusters \nby counting pairs that are assigned to either the same or to different clusters as follows \n𝐴𝑅𝐼 =\n𝑅𝐼 − 𝐸(𝑅𝐼)\nmax(𝑅𝐼) − 𝐸(𝑅𝐼) \nHere 𝑅𝐼 = (𝑎 + 𝑏)/ 𝐶\n𝑛 2  is a random index which yields an estimate of the degree of \nresemblance of two data clusters, where a and b refer to the number of pairs assigned to \nidentical and different clusters respectively and 𝐶\n𝑛 2\n is a combinatorial coefficient.  \n \nThe clusters are determined by applying the k-means procedure to the latent variables of \nthe VAE. However, the k-means procedure assumes spherical clusters while the actual digit \nclusters can be elongated, reducing the 𝑘-means accuracy compared to metrics based on the NN \nperformance.  \n \n10 \n \n4.3 BVAE with a NN Classifier Branch \n \nAs detailed in the previous section, the BVAE integrates the VAE with a NN classifier \nbranch that employs the latent variables and labels as input data.  To examine the encoding and \nclustering of latent variables of different categories, a NN branch consisting of 3 dense layers with \n512, 256 and 128 neurons and RELU activation functions followed by a 10-element dense layer \nwith Softmax activation function, will be employed together with a categorical_crossentropy loss \nfunction. Surprisingly, although NN layers with fewer neurons give less classification accuracy, a \nsingle ‘linear’ layer of 2 neurons gives the same accuracy and performance as that obtained with \n3 huge nonlinear layers. Additionally, the weighting factors for the loss terms in the BVAE \nobjective function must be properly chosen.  For example, the classifier loss weight,  𝜆 , must be \nsufficiently large to ensure that the VAE behavior is affected by the classifier branch. The degree \nof improvement in the clustering is determined by comparing the BVAE with the standard VAE \n(i.e. the BAE result for 𝜆 = 0).  The BVAE will also be compared and subsequently combined \nwith the fixed output VAE method which modifies the VAE objective function such that the cross-\nentropy relies on predefined target output distributions (10 representative digits chosen from the \nMNIST dataset in the calculations below), while the full MNIST dataset is again employed as the \ninput data [21]. \nThe efficiency of the BVAE is evident from the clusters in Fig. 3 which displays the 2-D \nlatent variable spaces generated by the MNIST dataset.  The digits are distinguished by the \ngreyscale intensity shown on the color bar, such that for example occurrences of 0 and 9 are \ndisplayed as black and white dots, respectively. Fig. 3(a) refers to the results of the standard VAE \n(𝜆 = 0), while Fig.3(b) displays enhanced latent variable clusters obtained by the VAE with fixed \noutput distributions. However, improved clustering can also be realized by modifying the BAE \nweighting factors.  Employing the BVAE with 𝜆 = 100 yields the clusters of Fig.3(c) while the \nBVAE with  𝛼 = 0.01 generates distinct clusters with modified shapes as shown in Fig.3(d). \n \n11 \n \n \n(a) \n \n(b) \n \n(c) \n \n(d) \nFig. 3. The 2-D latent variable space distributions generated by the MNIST data set using (a) the \nstandard VAE, (b) the VAE with fixed output distributions (c)-(d) the BVAE with 𝜆 = 100 \nand 𝛼 = 0.01 respectively. \n \nThe reconstructed images from the BVAE are further analyzed in Fig. 4, which displays a \n30×30 grid of the patterns produced by the decoder when applied to equidistant latent points from \n-3 to +3 along both coordinate directions. Fig. 4(a) presents the output patterns for the standard \nVAE while Fig. 4(b) displays the corresponding result for the BVAE with 𝜆 = 100. The increased \nisolation of the digit regions in the latent space in the latter case is again evident from the figures. \n12 \n \n \n(a) \n \n(b) \nFig. 4. The output patterns corresponding to the realizations within a two-dimensional histogram \nin latent space for (a) the standard VAE, (b) the BVAE with 𝜆 = 100 . \n \nTo compare the BVAE performance to those of the VAE and the VAE with fixed output, \nthe metrics of the previous section are evaluated, and the results collected in Table 1, where the \nnumbers are averages over independent calculations. Evidently, for 𝜆 = 1, the BVAE does not \nyield any enhancement compared to the standard VAE.   When 𝜆 = 10 , however the NN branch \nslightly influences the VAE behavior while for 𝜆 = 100 the BVAE exhibits an increase in \nclassification accuracy from ≈67% to 98%. While the NMI generates nearly the same degree of \nenhancement, the enhancement of ACC and ARI is marginally lower. The classification accuracy \nis almost equivalent to that of the VAE with fixed output, although the other metric values are \nsuperior in the fixed output calculation. \nThe BVAE with 𝛼 = 1  and  𝜆 = 100, exhibits nearly identical classification accuracy but \nhigher values of the remaining metrics in Table 1 when compared to the parameter values 𝛼 =\n0.01 and 𝜆 = 1. The larger values of the NMI, ACC and ARI metrics for 𝛼 = 1  and  𝜆 =\n100 result largely from the circularity of the clusters of Fig. 3(d) relative to those of Fig. 3(c) since \nthese metrics are based on the k-means procedure. Hence, the optimal choice of regularization \nparameter is somewhat problem-dependent.   \n13 \n \nCombining both the BVAE with 𝜆 = 100 and the fixed output procedure yields both a \nsuperior classification accuracy of 99% and large values for the remaining performance metrics, \nattesting again to the superior performance that can be achieved with fixed output distributions. \nTable 1 Comparison of clustering performance and classification accuracy for different frameworks. 𝛼 is the standard VAE loss \nparameter, while 𝜆 parametrizes the influence of the NN classifier branch. \nParameter \nNMI \nACC \nARI \nClassification Accuracy \nVAE \n0.467 \n0.479 \n0.352 \n0.672 \nVAE + Fixed Output \n0.875 \n0.914 \n0.843 \n0.977 \nBVAE (𝜆 = 10) \n0.615 \n0.622 \n0.477 \n0.854 \nBVAE (𝜆 = 100) \n0.757 \n0.717 \n0.608 \n0.98 \nBVAE (𝛼 = 0.01) \n0.907 \n0.957 \n0.908 \n0.968 \nBVAE + Fixed Output \n0.854 \n0.867 \n0.802 \n0.99 \n \nThe behavior of the various methods can be quantified by confusion matrices such as those \nof Fig. 5 which pertain to the NN branch of the BAE.  The diagonal elements contain the number \nof correctly classified instances for each digit, while the off-diagonal elements indicate the number \nof misclassifications from the digit in the row number to that of the column number. Fig 5(a) and \n(b) were generated with the standard VAE and the BVAE with 𝜆 = 100, respectively.  Evidently, \nthe standard VAE confuses 4 and 9 while predicting the 5 as 3 and 9 as 7 about 500 times. The \nBVAE with 𝜆 = 100, however, tackles these issues, and the largest number of misidentifications \nfor any of the digits is ≈30 instances. \n(a) \n \n(b) \nFig. 5. The confusion matrix associated with the NN accuracy classifier applied to the latent \nvariables for the (a) Standard VAE, (b) BVAE with 𝜆 = 100 . \n14 \n \n \nAn advantage of the VAE with fixed output framework is that it can be employed with \neffectively arbitrary output distributions.  For example, one possible set of outputs is displayed in \nthe ten pictures of Fig. 6, each of which contains one of 10 Gaussian functions centered at a \ndifferent position compared to the other images. Each category of input digits (e.g. 0 through 9) is \nthen mapped to the corresponding output position.  An additional two sets of output distributions \nwere also generated, one which replaced the Gaussian functions of Fig. 6 with square shapes and \nthe second with a two-dimensional Haar wavelet defined on a square interval. To compare the \nVAE performance for the three sets to MNIST target outputs, the reconstruction loss is calculated \nwith the mean_squared_error rather than the binary_crossentropy routine and the sigmoid \nactivation function in the last decoder layer is replaced with a RELU activation.  As evident from \nFig. 7, which displays the latent space distributions for the (a) MNIST (b) Gaussian (c) square and \n(d) wavelet outputs, all four output distributions yield highly clustered latent space distributions \ncompared to the standard VAE procedure. Significantly, however, the latent space distributions \nfor cases (b)-(d) are more elongated and isolated than those for the MNIST digits in Fig. 7(a). \n \n \n \nFig. 6 Synthesized Gaussian fixed output distributions.  \n \n15 \n \n \n(a) \n \n(b) \n \n(c) \n \n(d) \nFig. 7. The 2-D latent variable space distributions for the fixed (a) MNIST (b) Gaussian (c) \nsquare and (d) wavelet outputs. \n \nTo further assess the VAE with different sets of target output, a sample point from the \nlatent space is  passed to the decoder to reconstruct  the image of Fig. 8. The result corresponds to \na mixture of two Gaussians that encode the digits 9 and 4. This implies that the latent space \ndistributions of these digits share a common boundary, in agreement with the result for the fixed \nMNIST digit outputs of Fig. 3(b). Further, from Table 2, while all sets of fixed outputs yield nearly \nthe same high degree of classification accuracy, the clustering metrics are largest for the MNIST \ndigit target outputs, presumably because they yield the most circular latent space digit distributions \nin Fig.7.  \n16 \n \n \nFig. 8. The reconstructed image generated by a sample point from the 2D latent space for a VAE \nwith Gaussians as fixed target distributions. \n \nTable 2 Comparison of clustering performance and classification accuracy for the VAE with different sets of fixed outputs. \nParameter \nNMI \nACC \nARI \nClassification Accuracy \nMNIST \n0.874 \n0.911 \n0.843 \n0.98 \nGaussians \n0.82 \n0.807 \n0.732 \n0.972 \nSquares \n0.793 \n0.754 \n0.674 \n0.977 \nWavelets \n0.817 \n0.807 \n0.732 \n0.974 \n \n \n4.4 BVAE with a NN Classifier for Rotated Digits \nTo demonstrate the applicability of the proposed architectures to a wide range of \nclassification problems, the randomly rotated MNIST character set will now be employed as input \ndata.  Accordingly, Fig. 9 compares the latent variable distributions for rotated digits with both (a) \nthe standard VAE and (b) the BVAE with 𝜆 = 100.  Consistent with the results reported in [21], \nthe digit distributions in latent space for randomly rotated digits with different values overlap \nconsiderably for the VAE while the digits form identifiable clusters when the BVAE is employed.  \nIndeed, as seen from Table 3, the BVAE with 𝜆 = 100 both significantly increases the clustering \nmetrics and improves the classification accuracy of the VAE from 30% to about 83%, which is \ncomparable with the enhancement obtained with the VAE with the MNIST fixed digit output \ndistributions. At the same time, an NN accuracy of about 87% is achieved when combining the \n17 \n \nfixed output with the BVAE.  While the standard VAE cannot distinguish between most rotated \ndigits, the major source of confusion in the BVAE is limited to 6 and 9 (since these digits yield a \nnearly identical signature when randomly rotated), and 3 and 8 as other digits are accurately \nclassified, as evident from the confusion matrices in Fig. 10. \nRotated digits are more compartmentalized in latent spaces of higher dimensions which \nintroduce additional degrees of freedom into the network.  However, the classification accuracy of \nthe BVAE with a 2-dimentional latent space is comparable to or even exceeds that of the VAE \nwith a 10-dimentional latent space as evident from table 4. Perhaps unexpectedly, while the \naccuracy of the VAE with fixed output distributions is comparable to that of BVAE for two-\ndimensional latent spaces, the fixed output VAE is more accurate for higher dimensional latent \nspaces. \n \n \n \n(a) \n \n(b) \nFig. 9. The two-dimensional latent variable space distribution for randomly rotated MNIST digits \nin case of the (a) standard VAE (b) BVAE with 𝜆 = 100. \n \n \n \n \n18 \n \nTable 3 Same as table 1 but for rotated MNIST digits. \nParameter \nNMI \nACC \nARI \nClassification Accuracy \nVAE \n0.097 \n0.234 \n0.06 \n0.307 \nVAE + Fixed Output \n0.646 \n0.718 \n0.551 \n0.837 \nBVAE (𝜆 = 10) \n0.223 \n0.304 \n0.14 \n0.435 \nBVAE (𝜆 = 100) \n0.571 \n0.589 \n0.434 \n0.833 \nBVAE (𝛼 = 0.01) \n0.544 \n0.554 \n0.419 \n0.701 \nBVAE + Fixed Output \n0.66 \n0.693 \n0.545 \n0.868 \n \n \n(a) \n \n(b) \nFig. 10. Same as Fig. 5 but for randomly rotated MNIST digits. \n \nTable 4 Classification accuracy for different frameworks with latent spaces of different dimensions and randomly rotated digits. \nDimension \nVAE \nVAE + Fixed Output \nBVAE \n2 \n0.31 \n0.837 \n0.833 \n3 \n0.423 \n0.914 \n0.904 \n5 \n0.624 \n0.932 \n0.91 \n10 \n0.791 \n0.956 \n0.938 \n \n \n4.5 BVAE with Classifiers \n \n \nThe NN branch in the BVAE framework can be replaced by any classifier, which \nintroduces additional hyperparmeters.  In the case of a k-nearest neighbor (knn) branch, the \n19 \n \nhyperparameter is the number, 𝑛, of nearest neighbors while for the Random Forest (RF) 𝑛 is \nassociated with the number of estimators.  These classifiers are here implemented with the \nKNeighborsClassifier and RandomForestClassifier routines in the scikit-learn python library. \nIn contrast to the NN branch, a classifier loss factor of 𝜆 = 10 rather than 𝜆 = 100 is found to \nyield improved performance.  Values of 𝑛 of from 5 to 50 increase the classification accuracy for \nboth the knn and RF methods from 66% to 70% − 73% with minimal further improvement for \n𝑛 > 50.  However, the enhancement can be increased by altering the weights of the input digits. \nFor example, for a BVAE with 𝜆 = 10 and a knn branch with 𝑛 = 40, multiplying the input digit \ndistributions for 0,1 and 2 by a factor of 10 while dividing the digits 3,6,7 and 9 by the same factor \nincreases the accuracy from 73% to 83%. As well, the latent space distribution is considerably \naffected by the selective nature of the weights as evident from Fig. 11, as the clusters for the digits \nwith smaller weight values are far more distinct. Other weightings can be identified that yield \nsimilar performance, for example multiplying the inputs for 0,1,2 and 6 by 2 and dividing those \nfor 3,5,7 and 9 by 2, or alternatively multiplying digits 0 and 6 by 2 and dividing 4,5 and 8 by 2.  \nAll such weight combinations, however were found to yield accuracies between 73% and 83%.  \nSimilar enhancements can, of course, be realized by weighing the inputs of the standard VAE. \n \n \nFig. 11. The 2D latent variable space distribution for a BVAE with 𝜆 = 10 and a knn branch for \nappropriately weighted inputs. \n \n \n20 \n \n5. Conclusions \nThis paper has advanced a novel regularized variational autoencoder termed the BVAE that \nsignificantly enhances the accuracy and the latent variable clustering of the VAE.  Further, by \nincorporating a classifier branch into the VAE, the BVAE transforms unsupervised into supervised \nlearning.  This requires redefining the VAE loss function to include the classifier branch loss and \ntherefore introduces an additional hyperparameter corresponding to the magnitude of the \nadditional loss, which when chosen judiciously maximizes the discrimination among classes.  \nThe BVAE with a NN classifier branch and a two-dimensional latent space applied to the \nstandard MNIST dataset yields a classification accuracy of 97% while the corresponding value for \nthe standard VAE is 67% at the same time that other clustering metrics are similarly increased.  \nWhile the VAE with fixed output distributions offers comparable improvements, combining the \nBVAE with fixed output distributions yields additional accuracy enhancements. These results are \neven more pronounced for randomly rotated MNIST input digits where the classification error of \nthe BVAE with a NN branch decreases by a factor of 2.5 compared to that of the of the VAE since \nthe number of occurrences of misclassification of similar digits is greatly reduced.  This \nperformance is in fact superior to that of the traditional VAE with a 10-dimentional latent space. \nSimilar results were obtained for the BVAE with a knn or RF classifier branch while weighting \nthe input data to increase the contribution of the most frequently misinterpreted digits to the loss \nfunction yields further accuracy improvements.  Consequently, these architectures appear to be \npromising candidates for numerous practical classification tasks. \n \n References: \n[1] S. Hosseinimotlagh and E.E. Papalexakis, “Unsupervised content-based identification of fake \nnews articles with tensor decomposition ensembles,” In Proceedings of the Workshop on \nMisinformation and Misbehavior Mining on the Web (MIS2), 2018. \n[2] Y. Zhao, and G. Karypis, “Evaluation of hierarchical clustering algorithms for document \ndatasets. In Proceedings of the eleventh international conference on Information and knowledge \nmanagement,” pp. 515-524, 2002. \n[3] S. Y. Cui, J. X. He, and G. X. Tian, “The generalized distance matrix. Linear algebra and its \napplications, vol. 563, pp. 1-23, 2019. \n21 \n \n[4] C. G. Turhan, and H. S. Bilge, “Recent trends in deep generative models: a review,” In 2018 \n3rd International Conference on Computer Science and Engineering (UBMK), pp. 574-579, 2018. \n[5] X. Xu, J. Li, Y. Yang, and F. Shen “Toward effective intrusion detection using log-cosh \nconditional variational autoencoder,” IEEE Internet of Things Journal, vol. 8, no. 8, pp. 6187-\n6196, 2020. \n[6] C. Satheesh, S. Kamal, A. Mujeeb, and M. H. Supriya, “Passive Sonar Target Classification \nUsing Deep Generative β-VAE,” IEEE Signal Processing Letters, vol. 28, pp. 808-812, 2021. \n[7] Y. Li, Y. Zhang, K. Yu, and X. Hu “Adversarial training with Wasserstein distance for learning \ncross-lingual word embeddings,” Applied Intelligence, vol. 51, no. 11, pp. 7666-7678, 2021. \n[8] S. Amini, and S. Ghaemmaghami, “A new framework to train autoencoders through non-\nsmooth regularization,” IEEE Transactions on Signal Processing, vol. 67, no. 7, pp. 1860-1874, \n2019. \n[9] M. Abavisani, and V. M. Patel, “Deep sparse representation-based classification,” IEEE Signal \nProcessing Letters, vol. 26, no. 6, pp. 948-952, 2019. \n[10] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, “ACVAE-VC: Non-parallel voice \nconversion with auxiliary classifier variational autoencoder,” IEEE/ACM Transactions on Audio, \nSpeech, and Language Processing, vol. 27, no. 9, pp. 1432-1443, 2019. \n[11] E. Karamatlı, A. T. Cemgil, and S. Kırbız, “Audio source separation using variational \nautoencoders and weak class supervision,” IEEE Signal Processing Letters, vol. 26, no. 9, pp. \n1349-1353, 2019. \n[12] C. Zou, F. Yang, J. Song, and Z. Han “Channel autoencoder for wireless communication: \nState of the art, challenges, and trends,” IEEE Communications Magazine, vol. 59, no. 5, pp. 136-\n142, 2021. \n[13] W. Xu, and Y. Tan “Semisupervised text classification by variational autoencoder,” IEEE \ntransactions on neural networks and learning systems, vol. 31, no. 1, pp. 295-308, 2019. \n22 \n \n[14] B. Yang, X. Fu, N. D. Sidiropoulos, and M. Hong “Towards k-means-friendly spaces: \nSimultaneous deep learning and clustering,” In international conference on machine learning, pp. \n3861-3870, 2017. \n[15] C. Louizos, K. Swersky, Y. Li, M. Welling, and R. Zemel, “The variational fair \nautoencoder,” arXiv preprint arXiv:1511.00830, 2015. \n[16] J. Xie, R. Girshick, and A. Farhadi, “Unsupervised deep embedding for clustering analysis,” \nIn International conference on machine learning, pp. 478-487, 2016. \n[17] K. Tian, S. Zhou, and J. Guan, “Deep cluster: A general clustering framework based on deep \nlearning. In Machine Learning and Knowledge Discovery in Databases: European Conference,” \nECML PKDD 2017, Skopje, Macedonia, September 18–22, 2017, Proceedings, Part II 17 (pp. \n809-825). Springer International Publishing. \n[18] S. Zhao, J. Song, and S. Ermon, “Infovae: Balancing learning and inference in variational \nautoencoders,” In Proceedings of the aaai conference on artificial intelligence, Vol. 33, No. 01, \npp. 5885-5892, 2019. \n[19] A. Vahdat, and J. Kautz, J. “NVAE: A deep hierarchical variational autoencoder. Advances \nin neural information processing systems,” vol. 33, pp. 19667-19679, 2020. \n[20] S. Gao, Y. Zhang, K. Jia, J. Lu, and Y. Zhang, “Single sample face recognition via learning \ndeep supervised autoencoders,” IEEE transactions on information forensics and security, vol. 10, \nno. 10, pp. 2108-2118, 2015. \n[21] D. Yevick, “Rotated Digit Recognition by Variational Autoencoders with Fixed Output \nDistributions, 2022. \n[22] K. Sohn, H.  Lee, and X. Yan, “Learning structured output representation using deep \nconditional generative models. Advances in neural information processing systems, vol. 28, 2015. \n[23] M. Ehsan Abbasnejad, A. Dick, and A. van den Hengel, “Infinite variational autoencoder for \nsemi-supervised learning,” In Proceedings of the IEEE Conference on Computer Vision and \nPattern Recognition, pp. 5888-5897, 2017. \n23 \n \n[24] W. Wang, D. Yang, F. Chen, Y. Pang, S. Huang, and Y. Ge, “Clustering with orthogonal \nautoencoder,” IEEE Access, vol. 7, pp. 62421-62432, 2019. \n[25] C. Song, F. Liu, Y. Huang, L. Wang, and T. Tan, “Auto-encoder based data clustering. \nIn Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications: 18th \nIberoamerican Congress, CIARP 2013, Havana, Cuba, November 20-23, 2013, Proceedings, Part \nI 18 (pp. 117-124). Springer Berlin Heidelberg, 2013. \n[26] J. Lai, X. Wang, Q. Xiang, R. Li, and Y. Song, “FVAE: a regularized variational autoencoder \nusing the Fisher criterion,” Applied Intelligence, vol. 52, no. 14, pp. 16869-16885, 2022. \n[27] D. Yevick, “Variational autoencoder analysis of Ising model statistical distributions and phase \ntransitions,” The European Physical Journal B, vol. 95, no. 3, pp. 56, 2022. \n[28] S. Xu, C. Guo, Y. Zhu, G. Liu, and N. Xiong, “CNN-VAE: An intelligent text representation \nalgorithm,” The Journal of Supercomputing, 1-26, 2023. \n[29] R. Wei, and A. Mahmood, “Recent advances in variational autoencoders with representation \nlearning for biomedical informatics: A survey,” IEEE Access, vol. 9, 4939-4956, 2020. \n[30] D. P. Kingma, and M. Welling, “Auto-encoding variational bayes,” arXiv preprint \narXiv:1312.6114, 2013. \n[31] F. Chollet, Deep Learning with Python, Second Edition, 2nd edition, Manning, Shelter Island, \n2021. \n \n"
}