{
    "optim": "Computer Methods and Programs in Biomedicine\nDiabetes detection using deep learning techniques with oversampling\nand feature augmentation\nMaría Teresa García-Ordás a , Carmen Benavides b , José Alberto Benítez-Andrades b , ∗,\nHéctor Alaiz-Moretón a , Isaías García-Rodríguez a \na SECOMUCI Research Groups, Escuela de Ingenierías Industrial e Informática, Universidad de León, Campus de Vegazana s/n, León C.P. 24071, Spain\nb SALBIS Research Group, Department of Electric, Systems and Automatics Engineering, Universidad de León, Campus of Vegazana s/n, León, León, 24071,\nSpain\na r t i c l e \ni n f o \nArticle history:\nReceived 30 July 2020\nAccepted 30 January 2021\nKeywords:\nDiabetes\nDetection\nDeep learning\nSparse autoencoder\nVariational autoencoder\nOversampling\na b s t r a c t \nBackground and objective : Diabetes is a chronic pathology which is affecting more and more people over\nthe years. It gives rise to a large number of deaths each year. Furthermore, many people living with the\ndisease do not realize the seriousness of their health status early enough. Late diagnosis brings about\nnumerous health problems and a large number of deaths each year so the development of methods for\nthe early diagnosis of this pathology is essential.\nMethods : In this paper, a pipeline based on deep learning techniques is proposed to predict diabetic peo- \nple. It includes data augmentation using a variational autoencoder (VAE), feature augmentation using an\nsparse autoencoder (SAE) and a convolutional neural network for classiﬁcation. Pima Indians Diabetes\nDatabase, which takes into account information on the patients such as the number of pregnancies, glu- \ncose or insulin level, blood pressure or age, has been evaluated.\nResults : A 92 . 31% of accuracy was obtained when CNN classiﬁer is trained jointly the SAE for featuring\naugmentation over a well balanced dataset. This means an increment of 3.17% of accuracy with respect\nthe state-of-the-art.\nConclusions : Using a full deep learning pipeline for data preprocessing and classiﬁcation has demonstrate\nto be very promising in the diabetes detection ﬁeld outperforming the state-of-the-art proposals.\n1. Introduction\nDiabetes is a chronic pathology that occurs when the amount of \nglucose in blood is too high. Glucose is the body’s main source of \nenergy and insulin is the hormone, secreted by the pancreas that \nregulates the amount of glucose in the cells to be used for energy. \nDiabetic people do not produce enough insulin so the glucose re- \nmains in the blood [15] . \nHaving too much glucose in blood, may cause a number of \nhealth problems [21] , such as heart and dental diseases, stroke, eye \nproblems, nerve damage, etc. \nIn 2020, Olawsky et al. [22] carried out an study to evaluate the \nrelationship of glycemic variability and 5 year hypoglycemia risk in \npatients with Type 2 Diabetes (T2DM). \n∗ Corresponding author.\nE-mail addresses: mgaro@unileon.es (M.T. García-Ordás), mcbenc@unileon.es (C.\nBenavides), jbena@unileon.es (J.A. Benítez-Andrades), hector.moreton@unileon.es (H.\nAlaiz-Moretón), isaias.garcia@unileon.es (I. García-Rodríguez).\nDiabetes gives rise to a large number of deaths each year. Fur- \nthermore, a lot of people that live with the disease do not realize \nthe seriousness of their health condition early enough. The number \nof diabetic people is predicted to increase year by year [17] . Many \ncomplications occur if diabetes remains untreated and unidenti- \nﬁed. In order to reduce the number of deaths brought about by di- \nabetes, the development of methods and techniques for the early \ndiagnosis of diabetes is essential, as a large number of deaths in \ndiabetic patients are due to a late diagnosis. \nA number of techniques over the years have been developed \nto deal with the detection problem. In the work developed by \nSisodia et al. [27] , three machine learning classiﬁcation techniques \nwere used: decision tree, support vector machine (SVM) and naive \nBayes. In these cases, the naive Bayes algorithm outperforms the \nother classiﬁers by obtaining an accuracy of 76 . 30% . \nIn [18] , diabetes is predicted using signiﬁcant attributes, with \nthe relationship between the differing attributes also being char- \nacterized. The selection of signiﬁcant attributes was made using \nthe principal component analysis method. The authors found a \nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nstrong relationship between diabetes and body mass index (BMI) \nand with the glucose level. After this process, artiﬁcial neural net- \nwork (ANN), random forest (RF) and k-means clustering techniques \nwere implemented for the classiﬁcation step, with the best accu- \nracy obtained using the ANN technique with a 75 . 7% of success \nrate. \nThe use of a rule extraction algorithm, Re-RX with J48graft, \ncombined with sampling selection techniques (sampling Re-RX \nwith J48graft) was proposed by Hayashi et al. [8] with the same \npurpose, obtaining results of up to 83 . 83% . \nFuzzy classiﬁcation rules are more interpretable with respect \nto other rules. For this reason, fuzzy classiﬁcation rules are used \nextensively in the classiﬁcation and decision support systems for \ndisease diagnosis: Polat et al. [23] , use principal component anal- \nysis (PCA) and an adaptive neuro-fuzzy inference system (ANFIS) \nwith the combination of both methods achieving a performance \nof 89 . 47% . Similar results are obtained in Lukmanto et al. [17] in \nwhich feature selection is used to identify the valuable features in \nthe dataset. SVM model is then train with the selected features to \ngenerate the fuzzy rules and fuzzy inference process is ﬁnally used \nto classify the output. \nSiva et al. [5] present a diabetes prediction model using the \nconcept of fuzzy rule and grey wolf optimization but the results \nare not very promising in comparison with other methods in the \nliterature. Fuzzy system is also employed in Mansourypoor and \nAsadi [20] . In this study, a reinforcement learning-based evolution- \nary fuzzy rule-based system (RLEFRBS) is developed for diabetes \ndiagnosis. The authors tested their method in two datasets obtain- \ning results of up to 84% in Pima-Indian diabetes dataset. In [3] , a \nhybrid decision support system based on rough set theory (RST) \nand bat optimization algorithm (BA) called RST-BatMiner is pre- \nsented. In the ﬁrst step, the data is preprocessed and redundant \nfeatures are removed. In the second stage, for each class BA is in- \nvoked to generate fuzzy rules by minimizing proposed ﬁtness func- \ntion. Finally, an ada-boosting technique is applied to the rules gen- \nerated by BA to increase the accuracy rate of generated fuzzy rules. \nThey achieved a performance of 85 . 33% in the Pima-Indian dataset. \nAlneamy et al. [19] , have developed a method based on the \nTeaching Learning-Based Optimization (TLBO) algorithm and Fuzzy \nWavelet Neural Network (FWNN) with Functional Link Neural Net- \nwork (FLNN). They tested the eﬃciency of their method in ﬁve dif- \nferent datasets for different purposes, obtaining a 88 . 67% of accu- \nracy for the Pima-Indian diabetes dataset, which is a very promis- \ning result. \nMore machine learning techniques other than fuzzy methods \nare also widely used to try to deal with this problem. In [10] the \nauthors decided to use machine learning techniques to solve the \nproblem, achieving a 86 . 26% success rate. They extracted the fea- \ntures from the dataset using stacked autoencoders and the dataset \nis classiﬁed using a softmax layer. Furthermore, the ﬁne tuning \nof the network is done using backpropagation with the training \ndataset. \nIn [12] ﬁve different predictive models were tested: Linear Ker- \nnel Support Vector Machine (SVM-linear), Radial Basis Function \n(RBF) Kernel Support Vector Machine, K-Nearest Neighbour (k-NN), \nArtiﬁcial Neural Network (ANN) and Multifactor Dimensionality \nReduction (MDR) obtaining the best results, with a success rate of \n89% using SVM. \nSingh et al. [26] , developed a stacking-based evolutionary en- \nsemble learning system called NSGA-II-Stacking. They carried out \na data pre-processing step, ﬁlling outliers and missing values with \nthe median values. For base learner selection, a multi-objective op- \ntimization algorithm is used. As for model combination, k-nearest \nneighbor is employed as a meta-classiﬁer that combines the pre- \ndictions of the base learners. The comparative results demonstrate \nthat their proposal achieves an accuracy of 83 . 8% . \nFig. 1. In (a), the vanilla autoencoder with a simple latent vector. In (b) a variational autoencoder scheme with the mean and standard deviation layers used to sample the\nlatent vector.\n2\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 2. Example of an SAE network architecture. In the latent space there are more\nneurons than in the input and output and L1 regularization term is applied.\nAll of these works, were developed on the Pima-Indian dataset, \nwhich has been a challenge for many years. \nDeep learning has been demonstrated to be able to solve many \ncomplex problems in recent years. Stacked autoencoders (SAE) \n[16,31,32] , Deep Belief Networks (DBN) [28] , Long Short Term \nMemory (LSTM) [30] or Convolutional Neural Networks (CNN) \n[25,29] are some examples of successful deep learning solution \nproposed in the last year. \nIn this paper, a fully deep learning pipeline for diabetes pre- \ndiction is proposed. Variational autoencoder (VAE), Sparse autoen- \ncoder (SAE) and Convolutional neural network (CNN) are existing \ntechnologies but rarely are used all together. In this work, we have \ncarried out data augmentation both in samples (VAE) and features \n(SAE). SAE has been trained jointly with a CNN classiﬁer which al- \nlow them to get feedback from each other in the backpropagation \nstep improving the quality of the features generated according to \nthe spatial representation forced by CNN. \nThe paper is organised as follows: In Section 2 , the data pre- \nprocessing steps and the different techniques used to carry out the \nfeature augmentation are detailed. Vanilla multilayer perception \nand convolutional neural networks are also introduced. The experi- \nments, results with all of the techniques are shown, discussed and \ncompared with other state-of-the-art works in Section 3 and ﬁ- \nnally, we conclude in Section 4 . \n2. Methodology\n2.1. Data preprocessing: normalization \nWhen a dataset is used to train a model, every of their features \nusually follows a different distribution. In these cases, it is very \ndiﬃcult for an artiﬁcial neural network to ﬁt the data. To solve \nthis problem, there are so many different techniques which try to \nadjust every feature to obtain a similar range in the real numbers \nset. Some of the most typical normalizers are: \n• MaxMin Normalization takes into account the maximum and\nthe minimum values to ﬁx the data to into the range [0,1] fol- \nlowing Eq. 1 .\nˆ \nx = x − x min\nx max − x min \n(1) \nwhere x is the sample, x min is the minimum value and x max \nwhich is the maximum value of each feature. \n• Standard Normalization uses the statistical information of the\ndistribution to adjust the data in order to have mean equal to\n0 and a standard deviation of 1. See Eq. (2) .\nˆ \nx = x − μ\nσ\n(2) \nFig. 3. A vanilla CNN representation over an example image.\nwith mean μ: \nμ = 1 \nN \nN \n\u0002 \ni =1\nx i \n(3) \nand standard deviation σ : \nσ = \n\u0003\n1 \nN \nN \n\u0002 \ni =1\n(x i − μ) 2 \n(4) \nwhere N is the number of samples and x i is the i th element of \nthe dataset. \n• Logarithmic Normalization applies the logarithmic scale to the\ndata following Eq. (5) .\nˆ \nx = log (x ) \n(5)\n2.2. Data augmentation: variational autoencoder (VAE) \nFrequently, when processing a labeled dataset it can be seen \nthat some of the classes predominated over all the others. This is \nvery common in medical datasets where the percentage of some \nrare diseases samples used to be lower than the healthy ones. This \ncan cause that the machine learning techniques does not focus on \nthese small classes and learns by only taking the most crowded \nclass into account. In order to solve this, there are two main re- \nsearch lines: undersampling and oversampling. In undersampling \ntechniques, the goal is to reduce the most representative class so \nthat all of them have a similar number of elements. On the other \nhand, oversampling methods aims to increase the number of el- \nements in the less representative classes. The Synthetic Minority \n3\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 4. Our VAE to generate more data of the less representative class.\nOver-Sampling Technique (SMOTE) [2] , Generative Adversarial Net- \nwork (GAN) [7] and Variational Autoencoders (VAEs) [14] are some \nexamples of oversampling methods, which are also known as gen- \nerative methods. In this paper, Variational Autoencoders (VAEs) are \nused to deal with unbalanced dataset because it has demonstrated \nthat it works better than the other known techniques [6] . \nVAE are part of a group of deep learning techniques known as \nautoencoders which tries to learn a deep representation of the data \nby compressing the features. To do so, a symmetric architecture is \nbuild in which the number of input neurons is the same as the \nnumber of output neurons, having a bottle neck in some hidden \nlayer. When it comes to getting the input itself out of the network, \nthe network is able to learn a deep and compressed representation \nof the data in the bottle neck layer. All of the layers before the \nbottle neck one is usually called an encoder, and all the layers after \nit, makes up the decoder. \nThe main feature of VAE is that the encoder learns a normal \ndistribution of the data instead of just representing each sample. \nWith this distribution, the latent layer (bottle neck layer) samples \na new element with the newly learnt distribution. Once the VAE \nis trained, each time a new element of the dataset is introduced \ninto the net, it will generate a new one which ﬁts into the same \nnormal distribution of the data. With this approximation, synthetic \ndata which is similar to the original one can be generated. \nAn example of an autoencoder and a variational autoencoder \nare shown in the Fig. 1 using an example image. \nThe vanilla autoencoder is trained using a mean squared error \nloss over the original element and the reconstructed one. However, \nin VAE we have to add a new part to the loss function which ﬁxes \nlatent space distribution close to a normal distribution using the \nKulback–Leibler divergence (see Eq. (6) ). \nVAE _ LOSS = || x − ¯x || 2 + KL [ N(μx , σx ) , N(0 , 1)]\n(6) \nwhere ¯x is the reconstruction of x, and N(μx , σx ) a normal dis- \ntribution with mean μx and standard deviation σx . KL [ p, q ] is the \nKulback–Leilber divergence deﬁned in Eq. (7) \nKL [ p, q ] = −\n\u0004 \np(x ) log q (x ) dx + \n\u0004 \np(x ) log p(x ) dx \n(7) \n2.3. Feature augmentation: sparse autoencoder (SAE) \nSparse autoencoders (SAEs) are a kind of autoencoder but with \nthe peculiarity that the latent space layer has more neurons than \nthe input and the output. However, an L1 regularization term was \nadded to this latent space layer to force the network to just use \nsome of its neurons each time. Eq. (8) shows the L1 norm. \n|| W || 1 = | w 1 | + | w 2 | + . . . + | w N |\n(8) \nwhere w x is the weight of the connection x and N is the number \nof connection in the layer. L1 regularization adds W to the loss \nfunction forcing the network to have small weights (see Eq. (9) ) \nLoss = Er ror (y, ˆ \ny ) + λ\nN \n\u0002 \ni =1\n| w i |\n(9) \nwhere y is the label of the sample, ˆ \ny is the predicted value and \nλ is the multiplication factor of the regularization term. As bigger \nlambda is, more inﬂuence of the regularization in the whole loss \ncalculation. With this, the network learns to represent our initial \ndata with more features, allowing us to analyze the data from an- \nother perspective. \nIn Fig. 2 the typical architecture of a SAE can be seen. \n2.4. Data classiﬁcation: multilayer perceptron (MLP) \nIn order to classify the data, one of the most typical neural net- \nwork architecture is the multilayer perceptron (MLP) [24] . This net- \nwork is made up of one input layer, one output layer and one or \nmore hidden layers. In deep learning, more than one layer is usu- \nally used in order to learn complex information on the input data. \n4\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 5. (a) Proposed SAE in this research to generate 400 feature data. (b) Our MLP scheme.\nIn an MLP, each neuron of the L layer is fully connected with all \nthe neurons of the L + 1 layer. When the classiﬁcation problem is \nbinary, the most common approach uses one output neuron with a \nsigmoid activation function which represents the probability of the \ninput belonging to the positive class(see Eq. (10) ). \nf(x ) = \n1 \n1 + e −x \n(10) \n2.5. Data classiﬁcation: convolutional neural networks (CNN) \nA convolutional neural network (CNN) [11] , is a deep learning \nneural network algorithm which can take in a bi-dimensional input \nand be able to extract complex features of the data. To do so, in \nthe training process of a CNN classiﬁcation, the network adjusts \nthe weights of ﬁlters in order to carrying out an accurate feature \nmap of each class. A basic modelling of a CNN is represented in \nFig. 3 \nAfter a convolutional layer, it is common to add a pooling layer. \nThese kinds of layers are used to decrease the number of parame- \nters in the network. This reduces the computational cost and con- \ntrols overﬁtting. The most frequent type of pooling is Max-pooling, \nwhich takes the maximum value in each window. In order to carry \nout a classiﬁcation or a regression problem with the features gen- \nerated by the convolutional layers, it is necessary to add dense lay- \ners at the end of the network. \n2.6. Classical machine learning techniques \nA wide number of well-known machine learning techniques \nhave also been used to compare the results with those obtained \nin our experiments: \nDecision Tree is a model in which each internal node (not leaf) \nis tagged with an input characteristic. Arcs that come from a node \ntagged with an input feature are tagged with each of the possible \nvalues of the target or output feature, or the arc leads to a sub- \nordinate decision node on a different input feature. Each leaf in \nthe tree is labeled with a class or probability distribution over the \nclasses, which means that the dataset has been classiﬁed by the \ntree into a speciﬁc class or into a particular probability distribu- \ntion. We have carried out a grid search to estimate the best value \nfor the max_leaf_nodes and the criterion. \nRandom Forest is a combination of decision trees such that \neach tree depends on the values of a random vector tested inde- \npendently and with the same distribution for each of them. We \nhave carried out a grid search to estimate the best value for the \nmax_leaf_nodes, the number of trees and the criterion. \nSVM converts the training samples to points in hyperspace to \nmaximize the width of the gap between the decision boundary of \nthe two categories. Also, using kernels, SVM can learn non-linear \nseparations of the data. We have carried out a grid search to es- \ntimate the best value for kernel type, the decision function shape \nand the nu parameter. \n5\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 6. Autoencoder with latent space classiﬁer.\n6\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 7. Proposed convolutional neural network architecture.\nk-NN is a nonparametric classiﬁcation method, which estimates\nthe value of the probability density function that an element be- \nlongs to a given class from the information provided by the set of \ntraining elements that are closest to it. We have carried out a grid \nsearch to estimate the best value for the number of neigbors and \nthe type of algorithm used (ball tree, kd tree or brute). \nXGBoost is a decision tree-based ensemble machine learning \nalgorithm that uses a gradient boost framework designed to min- \nimize execution speed and maximize performance. We have car- \nried out a grid search to estimate the best value for the minimum \nsum of instance weight (hessian) needed in a child, the gamma \nparameter, the subsample ratio of the training instances, the sub- \nsample ratio of columns when constructing each tree and the max- \nimum depth of a tree. \n3. Experiments and results\n3.1. Dataset \nThe Pima Indians Diabetes Database (PIDD) is sourced from the \nUCI machine learning repository [13] . It is made up of 768 samples, \neach one with eight features: Pregnancies, Glucose, BloodPres- \nsure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction and \nAge. Five hundred samples are labeled as non diabetic (class 0) and \nthe rest, two hundred and sixty eight samples, belong to the dia- \nbetic class (class 1). \n3.2. Experimental setup \nFirst of all, for training process in all the models proposed, we \nhave split our dataset for training (90%) and testing (10%) to avoid \nmisleading results, showing the results of the test subset. \nThe ﬁrst step of the present work was the normalisation of \nthe data. Min-Max feature normalization using training subset has \nbeen carried out to set the values of the numeric columns in the \ndataset to a common scale [0,1], without distorting differences in \nthe ranges of values. Test subset has been also normalized using \nthe training parameters. \nFurthermore, the pregnancies feature was transformed into 1 or \n0 (pregnancy or not respectively) instead of representing the num- \nber of pregnancies. In the original dataset, some features are 0, but \nthis value must be considered as a missing value because, accord- \ning to the experts, features such as glucose, blood pressure, insulin, \netc. cannot be 0. We solved this problem by ﬁlling in the missing \nvalues with the mean value of their column in the training subset. \nAfter that, the class distribution of the dataset was evaluated \nby taking two different values into account: Diabetic represented \nby 1 and non diabetic represented by 0. The training split contains \n449 class 0 elements and 242 belonging to class 1. Although the \ndata is not quite unbalanced, a Variational Autoencoder (VAE) was \ntrained over this subset in order to generate more data on the less \nrepresentative class. The scheme of our VAE is detailed in Fig. 4 . \n3.2.1. Sparse autoencoder and multi layer perceptron classiﬁer trained \nseparately (SAE + MLP) \nAfter this process, the training split is made up of 449 elements \nof class 0 and 484 of class 1. Deep learning techniques require large \namounts of data and features in order to improve their learning \nprocess. We proposed the used of a Sparse Autoencoder (SAE) to \ntransform our eigth-element data to 400-element data in order to \nimprove the performance of the classiﬁcation. The process was car- \nried out using the Sparse Autoencoder represented in Fig. 5 (a). \nAfter all these steps, our data is made up of 1036 elements de- \nscribed by 400 features each. This dataset was classiﬁed using a \nmultilayer perceptron. The Multi Layer Perceptron (MLP) includes \ntwo dropout layers to avoid overﬁtting and it was training for more \nthan 400 epochs. The architecture of our MLP is shown in Fig. 5 (b). \n3.2.2. Sparse autoencoder and multi layer perceptron classiﬁer \ntrained jointly (SAE with MLP) \nIt has been decided to train an autoencoder with classiﬁer ar- \nchitecture all together. With this, the features learnt in the la- \ntent space during the autoencoder training are also inﬂuenced by \nthe class of the samples thanks to the classiﬁer. This architecture \nTable 1\nComparison with state of the art methods.\nMethod\nAccuracy\nAuthors\nHierarchical Fuzzy Classiﬁcation\n79.71\nFeng et al [4]\nNSGA-II-Stacking\n83.80\nSingh and Singh [26]\nRe-RX with J48graft\n83.83\nHayashi et al [8]\nRLEFRBS\n84.00\nMansourypoor and Asadi [20]\nModiﬁed Artiﬁcial Bee Colony\n84.21\nBeloufa and Chikh [1]\nANN + FNN \n84.24\nKahramanli and Allahverdi [9]\nRST-BatMiner\n85.33\nCheruku et al [3]\nStacked autoencoders\n86.26\nKannadasan et al [10]\nTLBO-FWNN\n88.67\nMajeed-Alneamy et al [19]\nFuzzy SVM\n89.02\nLukmanto et al [17]\nPCA + ANFIS \n89.47\nPolat and Günes [23]\nSAE with CNN\n92.31\nOurs\n7\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 8. Proposed autoencoder with a classiﬁcation inline.\n8\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nFig. 9. Comparison of classical methods with both original and oversampled datasets.\nwas trained for more than 400 epochs using the Adam optimizer. \nFig. 6 shows the diagram of the layers of this architecture. \n3.2.3. Sparse autoencoder and convolutional neural network classiﬁer \ntrained separately (SAE + CNN) \nA convolutional neural network was trained for our third pro- \nposal. Eight features for each sample are not suﬃcient to train \na convolutional neural network (CNN), so in this experiment, we \nproposed the use of an Sparse Autoencoder (SAE) to transform our \neight-element data to 400 features in the same way as in previ- \nous sections and then, these 400 elements were transformed to a \n20 × 20 matrix data. As we can see in Fig. 7 , our CNN architecture \nincludes two dropout layers (with dropout rate = 0.2) to deal with \nthe overﬁtting problem and one maxpooling layer to reduce the \ndimensionality. The convolutional layer has 100 ﬁlters with a ker- \nnel size of (2,6) and stride of 1. Maxpooling layer has a pool size \nof (2,6) too. All of these hyperparameters were chosen after a grid \nsearch evaluation. CNN has been trained for more than 600 epochs \nand 50 elements as batch size. \n3.2.4. Sparse autoencoder and CNN classiﬁer trained jointly (SAE \nwith CNN) \nThe CNN classiﬁer has also been combined with the jointly pro- \nposed autoencoder. The CNN classiﬁer is exactly the same as that \nused in the previous section but the CNN classiﬁcation is carried \nout inside the autoencoder. This architecture was trained for more \nthan 600 epochs using Adam optimizer. Fig. 8 shows the architec- \nture diagram. \n3.3. Results \nFirst, a comparison was made between the original dataset \nand the oversampled VAE dataset using classical machine learning \ntechniques. Fig. 9 shows the results obtained after an exahustive \nhyperparameter grid search. As we can see, every model trained \nwith VAE dataset outperform the trained model with the original \ndataset. The best result has been achieved by using MLP with a \n79.22% of accuracy in the test subset. \nAs the PIMA Indians dataset only contains 8 features, a feature \naugmentation has been carried out using the Sparse autoencoder \nto extract 400 new features. Thanks to the high number of fea- \ntures we have extracted, a convolutional neural network can be \ntrained by reshaping the 400 features into a 2D array of 20 × 20 . \nAlso a MLP has been trained with the new SAE features as it was \nthe model with the best performance in the previous experiment \n(see Fig. 9 ). \nMoreover, a jointly net which combines SAE and the classiﬁer \n(MLP or CNN) has been implemented in order to increase the fea- \nture extraction ability by taking into account the classiﬁer infor- \nmation obtained as feedback in the backpropagation algorithm. \nFig. 10 shows the results obtained with the jointly nets (SAE \nwith CNN and SAE with MLP), with the classiﬁcation after the SAE \nFig. 10. Results achieved using SAE with a network, a network after SAE and a MLP without SAE.\n9\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\nfeature augmentation (SAE + CNN and SAE + MLP) and the previous \nresult achieved without SAE (MLP). \nThe best performance was achieved using SAE with CNN. As \nwe can see, training the sparse autoencoder with the classiﬁer all \nin the same architecture improves the results in comparison with \ntraining them sequentially. It is important to notice how MLP and \nCNN obtains the same results when SAE is trained before the clas- \nsiﬁcation training. However, when the SAE is trained jointly, CNN \noutperforms MLP in a 7.7% of accuracy. It indicates that CNN in- \nterfers in the SAE feature extraction by forcing it to extract more \nrelevant features with spatial location information. MLP also mod- \nify the feature extraction carried out by SAE but the improvement \nis clearly worse than in the convolutional net. \nA one-way between subjects analysis of variance (ANOVA) was \nperformed to compare the effect of applying different neural net- \nworks to the same data on precision. There was a signiﬁcant effect \nof neural network applied on accuracy at the p < 0 . 05 level for the \nﬁve conditions [ F (4 , 51) = 574 . 929 , p < 0 . 001 ]. Post hoc compar- \nisons using the Tukey HSD test indicated that the mean score for \nthe SAE with CNN experiment (M = 92.31, SD = 1.04) was sig- \nniﬁcantly different than SAE with MLP (M = 85.71, SD = 0.66), \nSAE + CNN (M = 80.52, SD = 0.65), SAE+MLP (M = 80.52, \nSD = 0.65), MLP (M = 79.22, SD = 0.77). Taken together, these \nresults suggest that SAE with CNN experiment really do have an \neffect on accuracy. Speciﬁcally, our results suggest that when SAE \nwith CNN is applied, accuracy is signiﬁcantly higher. \nFurthermore, we have evaluated our results with the most re- \ncent papers on the state of the art with this dataset. Since all other \npapers do not specify the concrete data split for training and test- \ning, the results can not be compared exactly. However, every pa- \nper show their best result demonstrating that with this dataset our \nproposal outperforms the state of the art. In Table 1 we can see the \ncomparison. \nThe SAE with CNN proposal obtains the greatest precision, sur- \npassing all methods seen in the state of the art. \n4. Conclusions\nThis paper proposes methods based on in-depth learning com- \nbined with augmentation techniques to address the prediction of \ndiabetes using a popular data set called Pima Indian Diabetes. \nThis dataset is made up of 768 examples with just 8 features per \nsample and a unbalanced number of classes. In the preprocessing \nstep, the data set has been augmented using a Variational Autoen- \ncoder (VAE) and the number of features has been expanded with \na Sparse Autoencoder. Thanks to this, it was possible to train a \nconvolutional neural network to carry out the classiﬁcation step. \nA new architecture approach which combines the Sparse Autoen- \ncoder and the Convolutional Classiﬁer was proposed obtaining a \n92.31% of accuracy, outperforming all the other techniques shown \nin the state of the art. \nUsing a multi task neural network with SAE and CNN jointly, \nthe contribution is not only the better classiﬁcation of the diabetes \nsamples, but also the way to generate new features for the dataset. \nThe proposed architecture can be used in a wide new research \nﬁelds. It helps CNN to deal with structured data by rearranging \ntheir data using SAE adding the optimum spatial representation by \nreordering the features and creating new ones as combination of \nthem \nUsing a full deep learning pipeline for data preprocessing and \nclassiﬁcation has demonstrate to be very promising in the diabetes \ndetection ﬁeld outperforming the state-of-the-art proposals. \nAlthough these results are very promising, this work is lim- \nited to the small number of samples in the dataset studied. It is \nvery possible that the results can be improved by creating, as fu- \nture work, a new dataset with more valuable characteristics and \nmore individuals that allow a better generalization of the learning \nmodel. \nDeclaration of Competing Interest \nAuthors declare that they have no conﬂict of interest. \nCRediT authorship contribution statement \nMaría Teresa García-Ordás: Conceptualization, Formal analysis, \nInvestigation, Methodology, Software, Validation, Writing - original \ndraft. Carmen Benavides: Investigation, Validation, Writing - re- \nview & editing. José Alberto Benítez-Andrades: Conceptualization, \nMethodology, Software, Writing - review & editing. Héctor Alaiz- \nMoretón: Methodology, Writing - review & editing. Isaías García- \nRodríguez: Formal analysis, Investigation, Validation, Writing - re- \nview & editing. \nAcknowledgements \nWe gratefully acknowledge the support provided by the \nConsejería de Educación, Junta de Castilla y León throught project \nLE078G18 . UXXI2018/0 0 0149 . U-220. \nReferences \n[1] F. Beloufa, M.A. Chikh, Design of fuzzy classiﬁer for diabetes disease using\nmodiﬁed artiﬁcial Bee Colony algorithm, Comput. Methods Prog. Biomed. 112\n(1) (2013) 92–103, doi: 10.1016/j.cmpb.2013.07.009 .\n[2] N.V. Chawla, K.W. Bowyer, L.O. Hall, W.P. Kegelmeyer, SMOTE: synthetic mi- \nnority over-sampling technique, J. Artif. Intell. Res. 16 (2002) 321–357, doi: 10.\n1613/jair.953 .\n[3] R. Cheruku, D.R. Edla, V. Kuppili, R. Dharavath, RST-BatMiner: a fuzzy rule\nminer integrating rough set feature selection and Bat optimization for detec- \ntion of diabetes disease, Appl. Soft Comput. 67 (2018) 764–780, doi: 10.1016/J.\nASOC.2017.06.032 .\n[4] T.C. Feng, T.H.S. Li, P.H. Kuo, Variable coded hierarchical fuzzy classiﬁcation\nmodel using DNA coding and evolutionary programming, Appl. Math. Model.\n39 (23-24) (2013) 7401–7419, doi: 10.1016/j.apm.2015.03.004 .\n[5] S.S. G., M. K., Diagnosis of diabetes diseases using optimized fuzzy rule set\nby grey wolf optimization, Pattern Recognit. Lett. 125 (2019) 432–438, doi: 10.\n1016/J.PATREC.2019.06.005 .\n[6] M.T. García-Ordás, J.A. Benítez-Andrades, I. García-Rodríguez, C. Benavides,\nH. Alaiz-Moretón, Detecting respiratory pathologies using convolutional neu- \nral networks and variational autoencoders for unbalancing data, Sensors 20 (4)\n(2020) 1214, doi: 10.3390/s20041214 .\n[7] I.J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, Y. Bengio, Generative adversarial nets, in: Advances in Neural In- \nformation Processing Systems, 3, Neural Information Processing Systems Foun- \ndation, 2014, pp. 2672–2680, doi: 10.3156/jsoft.29.5 _ 177 _ 2 .\n[8] Y. Hayashi, S. Yukita, Rule extraction using recursive-rule extraction algorithm\nwith J48graft combined with sampling selection techniques for the diagnosis\nof type 2 diabetes mellitus in the Pima Indian dataset, Inform. Med. Unlocked\n2 (2016) 92–104, doi: 10.1016/J.IMU.2016.02.001 .\n[9] H. Kahramanli, N. Allahverdi, Design of a hybrid system for the diabetes and\nheart diseases, Expert Syst. Appl. 35 (1-2) (2008) 82–89, doi: 10.1016/J.ESWA.\n20 07.06.0 04 .\n[10] K. Kannadasan, D.R. Edla, V. Kuppili, Type 2 diabetes data classiﬁcation using\nstacked autoencoders in deep neural networks, Clin. Epidemiol. Glob. Health 7\n(4) (2019) 530–535, doi: 10.1016/j.cegh.2018.12.004 .\n[11] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, L. Fei-Fei, Large- \nscale video classiﬁcation with convolutional neural networks, in: 2014 IEEE\nConference on Computer Vision and Pattern Recognition, 2014, pp. 1725–1732,\ndoi: 10.1109/CVPR.2014.223 .\n[12] H. Kaur, V. Kumari, Predictive modelling and analytics for diabetes using a ma- \nchine learning approach, Appl. Comput. Inform. (2018), doi: 10.1016/J.ACI.2018.\n12.004 .\n[13] K. Kayaer , T. Yildirim , Medical Diagnosis on Pima Indian Diabetes Using Gen- \neral Regression Neural Networks, Technical Report (2003) .\n[14] D.P. Kingma , M. Welling , Stochastic Gradient VB and the Variational Auto-En- \ncoder, Technical Report (2013) .\n[15] A. Lonappan, G. Bindu, V. Thomas, J. Jacob, C. Rajasekaran, K.T. Mathew, Diag- \nnosis of diabetes mellitus using microwaves, J. Electromagn. Waves Appl. 21\n(10) (2007) 1393–1401, doi: 10.1163/156939307783239429 .\n[16] I.B. Ltaifa, L. Hlaoua, L.B. Romdhane, Hybrid deep neural network-based text\nrepresentation model to improve microblog retrieval, Cybern. Syst. 51 (2)\n(2020) 115–139, doi: 10.1080/01969722.2019.1705548 .\n10\nM.T. García-Ordás, C. Benavides, J.A. Benítez-Andrades et al.\n[17] R.B. Lukmanto, Suharjito, A. Nugroho, H. Akbar, Early detection of diabetes\nmellitus using feature selection and fuzzy support vector machine, Procedia\nComput. Sci. 157 (2019) 46–54, doi: 10.1016/J.PROCS.2019.08.140 .\n[18] T. Mahboob Alam, M.A. Iqbal, Y. Ali, A. Wahab, S. Ijaz, T. Imtiaz Baig, A. Hus- \nsain, M.A. Malik, M.M. Raza, S. Ibrar, Z. Abbas, A model for early prediction\nof diabetes, Inform. Med. Unlocked 16 (2019) 100204, doi: 10.1016/J.IMU.2019.\n100204 .\n[19] J.S. Majeed Alneamy, Z. A. Hameed Alnaish, S. Mohd Hashim, R.A. Hamed Al- \nnaish, Utilizing hybrid functional fuzzy wavelet neural networks with a teach- \ning learning-based optimization algorithm for medical disease diagnosis, Com- \nput. Biol. Med. 112 (2019) 103348, doi: 10.1016/J.COMPBIOMED.2019.103348 .\n[20] F. Mansourypoor, S. Asadi, Development of a reinforcement learning-based\nevolutionary fuzzy rule-based system for diabetes diagnosis, Comput. Biol.\nMed. 91 (2017) 337–352, doi: 10.1016/J.COMPBIOMED.2017.10.024 .\n[21] D.M. Nathan, E. Barret-Connor, J.P. Crandall, S.L. Edelstein, R.B. Goldberg,\nE.S. Horton, W.C. Knowler, K.J. Mather, T.J. Orchard, X. Pi-Sunyer, D. Schade,\nM. Temprosa, Long-term effects of lifestyle intervention or metformin on\ndiabetes development and microvascular complications: the DPP outcomes\nstudy, Lancet Diabetes Endocrinol. 3 (11) (2016) 866–875, doi: 10.1016/\nS2213- 8587(15)00291- 0.Long- term .\n[22] E.A . Olawsky, Y. Zhang, A .C. Alvear, L.E. Eberly, L.S. Chow, 864-P: hyperglycemia\ndrives glycemic variability in patients with Type 2 diabetes (T2DM), Diabetes\n69 (Supplement 1) (2020) 864–P, doi: 10.2337/db20- 864- p .\n[23] K. Polat, S. Güne, An expert system approach based on principal component\nanalysis and adaptive neuro-fuzzy inference system to diagnosis of diabetes\ndisease, Digit. Signal Process. 17 (4) (2007) 702–710, doi: 10.1016/j.dsp.2006.09.\n005 .\n[24] D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning Internal Representations by\nError Propagation, MIT Press, Cambridge, MA , USA , p. 318–362.\n[25] K. Sim, J. Yang, W. Lu, X. Gao, MaD-DLS: mean and deviation of deep and local\nsimilarity for image quality assessment, IEEE Trans. Multimed. (2020) 1, doi: 10.\n1109/tmm.2020.3037482 .\n[26] N. Singh, P. Singh, Stacking-based multi-objective evolutionary ensemble\nframework for prediction of diabetes mellitus, Biocybern. Biomed. Eng. 40 (1)\n(2020) 1–22, doi: 10.1016/J.BBE.2019.10.001 .\n[27] D. Sisodia, D.S. Sisodia, Prediction of diabetes using classiﬁcation algorithms,\nProcedia Comput. Sci. 132 (2018) 1578–1585, doi: 10.1016/J.PROCS.2018.05.122 .\n[28] Y. Wang, Z. Pan, X. Yuan, C. Yang, W. Gui, A novel deep learning based fault\ndiagnosis approach for chemical process with extended deep belief network,\nISA Trans. 96 (2020) 457–467, doi: 10.1016/j.isatra.2019.07.001 .\n[29] J. Yang, C. Wang, B. Jiang, H. Song, Q. Meng, Visual perception enabled industry\nintelligence: state of the art, challenges and prospects, IEEE Trans. Ind. Inform.\n17 (3) (2021) 2204–2219, doi: 10.1109/TII.2020.2998818 .\n[30] X. Yuan, L. Li, Y. Shardt, Y. Wang, C. Yang, Deep learning with spatiotempo- \nral attention-based LSTM for industrial soft sensor model development, IEEE\nTrans. Ind. Electron. (2020) 1, doi: 10.1109/TIE.2020.2984 4 43 .\n[31] X. Yuan, C. Ou, Y. Wang, C. Yang, W. Gui, A layer-wise data augmentation strat- \negy for deep learning networks and its soft sensor application in an indus- \ntrial hydrocracking process, IEEE Trans. Neural Netw. Learn. Syst. (2019) 1–10,\ndoi: 10.1109/TNNLS.2019.2951708 .\n[32] X. Yuan, J. Zhou, B. Huang, Y. Wang, C. Yang, W. Gui, Hierarchical quality- \nrelevant feature representation for soft sensor modeling: a novel deep learning\nstrategy, IEEE Trans. Ind. Inform. 16 (6) (2020) 3721–3730, doi: 10.1109/TII.2019.\n2938890 .\nMaría Teresa García-Ordás, Ph.D. was born in León, Spain, in 1988. She received\nher degree in Computer Science from the University of León in 2010, and her Ph.D.\nin Intelligent Systems in 2017. She was a recipient of a special mention award for\nthe best doctoral thesis on digital transformation by Tecnalia. Since 2019, she works\nas teaching assistant at the University of León. Her research interests include com- \nputer vision and deep learning. She has published several articles in impact journals\nand patents. She has participated in many conferences all over the world.\nJosé Alberto Benítez-Andrades, Ph.D. was born in Granada, Spain, in 1988. He has\nreceived his degree in Computer Science from the University of León, and the Ph.D.\ndegree in Production and Computer Engineering in 2017 (University of Leon). He\nwas part time instructor who kept a parallel job from 2013 to 2018 and since 2018\nhe works as teaching assistant at the University of Leon. His research interests in- \nclude artiﬁcial intelligence, knowledge engineering, semantic technologies. He was\na recipient of award to the Best Doctoral Thesis 2018 by Colegio Profesional de In- \ngenieros en Informática en Castilla y León in 2018.\nIsaías García-Rodríguez, Ph.D. received his Bachelor degree in Industrial Technical\nEngineering from the University of León (Spain) in 1992 and her Master degree in\nIndustrial Engineering from the University of Oviedo (Spain) in 1996. Isaías obtained\nhis Ph.D. in Computer Science from the University of León in 2008, where he is\ncurrently a lecturer. His current research interests include practical applications of\nSoftware Deﬁned Networks, Network Security and applied Knowledge Engineering\ntechniques. He has published different scientiﬁc papers in journals, Conferences and\nSymposia around the world.\nCarmen Benavides, Ph.D. received her Bachelor degree in Industrial Technical En- \ngineer from the University of León (Spain) in 1996 and her Master degree in Elec- \ntronic Engineering from the University of Valladolid (Spain) in 1998. Carmen ob- \ntained her Ph.D. in Computer Science from the University of León in 2009 and she\nworks as an Assistant Professor at the same University since 2001. Her research\ninterests are focused on applied Knowledge Engineering techniques, practical ap- \nplications of Software Deﬁned Networks and Network Security. She has organized\nseveral congresses, and has presented and published different papers in Journals,\nConferences and Symposia.\nHéctor Alaiz-Moretón, Ph.D. received his degree in Computer Science, performing\nthe ﬁnal project at Dublin Institute of Technology, in 2003. He received his Ph.D.\nin Information Technologies in 2008 (University of Leon). He has worked like a lec- \nturer since 2005 at the School of Engineering at the University of Leon. His research\ninterests include knowledge engineering, machine & deep learning, networks com- \nmunication and security. He has several works published in international confer- \nences, as well as books and scientiﬁc papers in peer review journals. He has been\nmember of scientiﬁc committees in conferences. He has headed several Ph.D. Thesis\nand research projects.\n11\n"
}