{
    "optim": "Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation\nSobihan Surendran∗†, Adeline Fermanian†, Antoine Godichon-Baggioni∗, and Sylvain Le Corff∗\nAbstract\nStochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural\nnetworks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in\nseveral recent deep learning and reinforcement learning applications that use Monte Carlo methods. This\npaper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps\nfor convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes\nthe importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In\nparticular, we establish that Adagrad and RMSProp with biased gradients converge to critical points\nfor smooth non-convex functions at a rate similar to existing results in the literature for the unbiased\ncase. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our\nconvergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning.\nKeywords:\nStochastic Optimization, Biased Stochastic Approximation, Monte Carlo Methods, Variational\nAutoenconders\n1\nIntroduction\nStochastic Gradient Descent (SGD) algorithms are standard methods to train statistical models based on\ndeep architectures. Consider a general optimization problem:\nθ∗ ∈ argmin\nθ∈Rd V (θ) ,\n(1)\nwhere V is the objective function. Then, gradient descent methods produce a sequence of parameter estimates\nas follows: θ0 ∈ Rd and for all n ≥ 1,\nθn+1 = θn − γn+1∇V (θn) ,\nwhere ∇V denotes the gradient of V and for all n ≥ 1, γn > 0 is the learning rate. In many cases, it is not\npossible to compute the exact gradient of the objective function, hence the introduction of vanilla Stochastic\nGradient Descent, defined for all n ≥ 1 by:\nθn+1 = θn − γn+1d\n∇V (θn) ,\nwhere d\n∇V (θn) is an estimator of ∇V (θn). In deep learning, stochasticity emerges with the use of mini-batches,\nsince it is not feasible to compute gradients based on the entire dataset.\nWhile these algorithms have been extensively studied, both theoretically and practically (see, e.g., Bottou\net al., 2018), many questions remain open. In particular, most results are based on the case where the\nestimator d\n∇V is unbiased. Although this assumption is valid in the case of vanilla SGD, it breaks down\nin many common applications. For example, zeroth-order methods used to optimize black-box functions\n(Nesterov and Spokoiny, 2017) in generative adversarial networks (Moosavi-Dezfooli et al., 2017; Chen et al.,\n2017) have access only to noisy biased realizations of the objective functions.\nFurthermore, in reinforcement learning algorithms such as Q-learning (Jaakkola et al., 1993), policy\ngradient (Baxter and Bartlett, 2001), and temporal difference learning (Bhandari et al., 2018; Lakshmi-\nnarayanan and Szepesvari, 2018; Dalal et al., 2018), gradient estimators are often obtained using a Markov\n∗Laboratoire de Probabilit´es, Statistique et Mod´elisation (LPSM), Sorbonne Universit´e, 75005 Paris, France.\n†LOPF, Califrais’ Machine Learning Lab, Paris, France\n1\narXiv:2402.02857v1  [stat.ML]  5 Feb 2024\nchain with state-dependent transition probability. These estimators are then biased (Sun et al., 2018; Doan\net al., 2020). Other examples of biased gradients can be found in the field of generative modeling with\nMarkov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) (Gloaguen et al., 2022; Cardoso\net al., 2023). In particular, the Importance Weighted Autoencoder (IWAE) proposed by Burda et al. (2015),\nwhich is a variant of the standard Variational Autoencoder (VAE) (Kingma and Welling, 2013), yields biased\nestimators.\nIn practical applications, vanilla SGD exhibits difficulties to calibrate the step sequences. Therefore,\nmodern variants of SGD employ adaptive steps that use past stochastic gradients or Hessians to avoid saddle\npoints and deal with ill-conditioned problems. The idea of adaptive steps was first proposed in the online\nlearning literature by Auer et al. (2002) and later adopted in stochastic optimization, with the Adagrad\nalgorithm of Duchi et al. (2011). Adagrad aims to normalize the gradient by introducing information about\nthe square root of the inverse of the covariance of the gradient.\nWe give non-asymptotic convergence guarantees for modern variants of SGD where both the estimators\nare biased and the steps are adaptive. To our knowledge, existing results consider either adaptive steps but\nunbiased estimators or biased estimators and non-adaptive steps. Indeed, many standard analyses for SGD\n(Moulines and Bach, 2011) and SGD with adaptive steps (Duchi et al., 2011) require unbiased gradients\nto obtain convergence results. More recently, convergence results have been obtained for SGD with biased\ngradients (Tadi´c and Doucet, 2011; Karimi et al., 2019; Ajalloeian and Stich, 2020) but non-adaptive steps.\nMore precisely, we present convergence guarantees for SGD with biased gradients and adaptive steps,\nunder weak assumptions on the bias and Mean Squared Error (MSE) of the estimator. In many scenarios,\nit is indeed possible to control these quantities, as recently shown for example for Importance Sampling\nand Sequential Monte Carlo methods (Agapiou et al., 2017; Cardoso et al., 2022, 2023). In particular, we\nestablish that Adagrad and RMSProp with a biased gradient converge to a critical point for non-convex\nsmooth functions with a convergence rate of O(log n/√n + bn), where bn is related to the bias at iteration n.\nHowever, for convex functions, we achieve an improved convergence rate of O(1/√n + bn). Our theoretical\nresults provide us with hyperparameter tuning procedures to effectively eliminate the bias term, resulting in\nimproved convergence rates of O(log n/√n) and O(1/√n) respectively.\nOrganization of the paper.\nIn Section 2, we introduce the setting of the paper and relevant related\nworks. In Section 3, we present the Adaptive Stochastic Approximation framework and the main assumptions\nfor theoretical results. In Section 4, we propose convergence rates for the risk in the convex case and the\nsquared norm of gradients for non-convex smooth functions in the context of Biased Adaptive Stochastic\nApproximation. Finally, we extend the analysis to Adagrad and RMSProp with biased gradients. We\nillustrate our results using VAE in Section 5. All proofs are postponed to the appendix.\n2\nSetting and Related Works\nStochastic Approximation.\nStochastic Approximation (SA) methods go far beyond SGD. They consist\nof sequential algorithms designed to find the zeros of a function when only noisy observations are available.\nIndeed, Robbins and Monro (1951) introduced the Stochastic Approximation algorithm as an iterative\nrecursive algorithm to solve the following integration equation:\nh(θ) = Eπ [Hθ(X)] =\nZ\nX\nHθ(x)π(x)dx = 0 ,\n(2)\nwhere h is the mean field function, X is a random variable taking values in a measurable space (X, X), and\nEπ is the expectation under the distribution π. In this context, Hθ can be any arbitrary function. If Hθ(X)\nis an unbiased estimator of the gradient of the objective function, then h(θ) = ∇V (θ). As a result, the\nminimization problem (1) is then equivalent to solving problem (2), and we can note that SGD is a specific\ninstance of Stochastic Approximation. Stochastic Approximation methods are then defined as follows:\nθn+1 = θn − γn+1Hθn (Xn+1) ,\nwhere the term Hθn (Xn+1) is the n-th stochastic update, also known as the drift term, and is a potentially\nbiased estimator of ∇V (θn). It depends on a random variable Xn+1 which takes its values in (X, X). In\n2\nmachine learning, V typically represents theoretical risk, θ denotes model parameters, and Xn+1 stands for\nthe data. The gradient estimator generally corresponds to the gradient of the empirical risk when data is\nassumed to be independent and identically distributed. In a reinforcement learning setting, an agent interacts\nwith an environment to learn an optimal policy based on some parameters θ, taking actions, receiving\nrewards, and updating its policy according to observed outcomes. Here, V represents the reward function, θ\ndenotes policy parameters, and Xn+1 represents state-action sequences.\nAdaptive Stochastic Gradient Descent.\nSGD can be traced back to Robbins and Monro (1951), and its\naveraged counterpart was proposed by Polyak and Juditsky (1992). The convergence rates of algorithms for\nconvex optimization problems rely primarily on the strong convexity of the objective function, as established\nby Nemirovskij and Yudin (1983). The non-asymptotic results of SGD in the context of non-convex smooth\nfunctions can be found in Moulines and Bach (2011). Ghadimi and Lan (2013) prove the convergence of a\nrandom iterate of SGD for nonconvex smooth functions, which was already suggested by the results of Bottou\n(1991). They show that SGD with constant or decreasing stepsize γk = 1/\n√\nk converges to a stationary point\nof a non-convex smooth function V at a rate of O(1/√n) where n is the number of iterations.\nMost adaptive first-order methods, such as Adam (Kingma and Ba, 2014), Adadelta (Zeiler, 2012),\nRMSProp (Tieleman et al., 2012), and NADA (Dozat, 2016), are based on the blueprint provided by the\nAdagrad family of algorithms. The first known work on adaptive steps for non-convex stochastic optimization,\nin the asymptotic case, was presented by Kresoja et al. (2017). Ward et al. (2020) proved that Adagrad\nconverges to a critical point for non-convex objectives at a rate of O(log n/√n) when using a scalar adaptive\nstep. In addition, Zou et al. (2018) extended this proof to multidimensional settings. More recently, D´efossez\net al. (2020) focused on the convergence rates for Adagrad and Adam.\nBiased Stochastic Approximation.\nThe asymptotic results of Biased Stochastic Approximation have\nbeen studied by Tadi´c and Doucet (2011). The non-asymptotic analysis of Biased Stochastic Approximation\ncan be found in the reinforcement learning literature, especially in the context of temporal difference (TD)\nlearning, as explored by Bhandari et al. (2018); Lakshminarayanan and Szepesvari (2018); Dalal et al. (2018).\nThe case of non-convex smooth functions has been studied by Karimi et al. (2019). The authors establish\nconvergence results for the mean field function at a rate of O(log n/√n + b), where b corresponds to the\nbias and n to the number of iterations. For strongly convex functions, the convergence of SGD with biased\ngradients can be found in Ajalloeian and Stich (2020), who consider a constant step size. This analysis\napplies specifically to the case of Martingale noise. Biased Stochastic Approximation with Markov noise\nhas also been investigated by Li and Wai (2022). The authors focused on performative prediction problems,\nwhere the objective function is the expected value of a loss. They assumed the smoothness of V , along with\nspecific assumptions regarding the existence of a bounded solution of the Poisson equation and additional\nLipschitz properties. Our analysis provides non-asymptotic results in a more general setting, for a wide\nvariety of objective functions and treating both the Martingale and Markov chain cases.\n3\nAdaptive Stochastic Approximation\n3.1\nFramework\nConsider the optimization problem (1) where the objective function V is assumed to be differentiable. In\nthis paper, we focus on the following Stochastic Approximation (SA) algorithm with adaptive steps: θ0 ∈ Rd\nand for all n ≥ 0,\nθn+1 = θn − γn+1AnHθn (Xn+1) ,\n(3)\nwhere γn+1 > 0 and An is a sequence of symmetric and positive definite matrices. Let (Fn)n≥0 be the\nfiltration generated by the random variables (θ0, {Xk}k≤n) and assume that for all n ≥ 0, An is Fn-measurable.\nIn a context of biased gradient estimates, choosing\nAn =\n\u0014\nδId +\n\u0010 1\nn\nn−1\nX\nk=0\nHθk (Xk+1) Hθk (Xk+1)⊤ \u0011\u0015−1/2\ncan be assimilated to the full Adagrad algorithm (Duchi et al., 2011). However, computing the square root\nof the inverse becomes expensive in high dimensions, so in practice, Adagrad is often used with diagonal\n3\nmatrices. This approach has been shown to be particularly effective in sparse optimization settings. Denoting\nby Diag(A) the matrix formed with the diagonal terms of A and setting all other terms to 0, Adagrad with\ndiagonal matrices is defined in our context as:\nAn =\n\u0002\nδId + Diag( ¯Hn(X0:n, θ0:n−1))\n\u0003−1/2 ,\n(4)\nwhere\n¯Hn(X0:n, θ0:n−1) = 1\nn\nn−1\nX\nk=0\nHθk(Xk+1)Hθk(Xk+1)⊤ .\nIn RMSProp (Tieleman et al., 2012), ¯Hn(X0:n, θ0:n−1) in (4) is an exponential moving average of the past\nsquared gradients. It is defined by:\n(1 − ρ)\nn−1\nX\nk=0\nρn−kHθk(Xk+1)Hθk(Xk+1)⊤,\nwhere ρ is the moving average parameter. Furthermore, when An is a recursive estimate of the inverse\nHessian, it corresponds to the Stochastic Newton algorithm (Boyer and Godichon-Baggioni, 2023).\n3.2\nAssumptions\nWe state below the main assumptions necessary to our theoretical results.\nAssumption 1. The objective function V is convex and there exists a constant µ > 0 such that:\n2µ (V (θ) − V (θ∗)) ≤ ∥∇V (θ)∥2,\n∀θ ∈ Rd.\nThe second condition of Assumption 1 corresponds to the Polyak- Lojasiewicz condition. Furthermore,\nAssumption 1 is a weaker assumption compared to the function being strongly convex. In machine learning,\nthis assumption holds true for linear regression, logistic regression and Huber loss when the Hessian at the\nminimizer is positive. If this condition is not met, we can add a regularization term to the loss in the form of\nµ∥θ∥2/2, where µ represents the regularization parameter.\nAssumption 2. The objective function V is L-smooth. For all (θ, θ′) ∈\n\u0000Rd\u00012,\n\r\r∇V (θ) − ∇V\n\u0000θ′\u0001\r\r ≤ L\n\r\rθ − θ′\r\r .\nThis assumption is crucial to obtain our convergence rate and is very common (see, e.g., Moulines and\nBach, 2011; Bottou et al., 2018). Under this assumption, for all (θ, θ′) ∈\n\u0000Rd\u00012,\nV (θ) ≤ V\n\u0000θ′\u0001\n+\n\n∇V\n\u0000θ′\u0001\n, θ − θ′\u000b\n+ L\n2\n\r\rθ − θ′\r\r2 .\nAssumption 3. For all n ∈ N, there exists rn ≥ 0 and σ2\nn ≥ 0 such that:\n(i) Bias:\n\r\rE[Hθn(Xn+1) | Fn] − ∇V (θn)\n\r\r ≤ rn.\n(ii) Mean Squared Error (MSE):\nE\nh\n∥Hθn (Xn+1) − ∇V (θn)∥2 | Fn\ni\n≤ σ2\nn.\nIn this assumption, the sequences rn and σ2\nn control the bias and MSE without any specific assumption\nregarding their dependence on n, making our setting very general. This assumption can be verified, for\nexample, when the gradient estimator follows the form of Self-Normalized Importance Sampling (SNIS)\n(Agapiou et al., 2017) or in various Monte Carlo applications as discussed in Appendix C. Furthermore,\nthe first point of Assumption 3 is satisfied by a wide range of discrete-time stochastic processes, including\ni.i.d. random sequences and Markov chains, with some additional assumptions. In the case where {Xn, n ∈ N}\n4\nis an i.i.d. sequence, this can be easily verified by considering rn as the difference between the gradient and\nthe mean field function defined in (2). If h(θn) = ∇V (θn), i.e., for SGD with unbiased gradient estimators,\nthe second point is equivalent to ensuring that the variance of the noise term is bounded. In this context,\nthis assumption is standard, see for example Moulines and Bach (2011); Ghadimi and Lan (2013). In our\nparticular case, however, it accounts for the time-dependent variance of the noise term.\nWe consider also an additional assumption on An. Let ∥A∥ be the spectral norm of a matrix A.\nAssumption 4. For all n ∈ N, there exists βn, λn > 0 such that:\n∥An∥ = λmax(An) ≤ βn+1\nand\nλmin (An) ≥ λn+1.\nIn our setting, since An is assumed to be a symmetric matrix, the spectral norm is equal to the largest\neigenvalue. Assumption 4 plays a crucial role, as the estimates may diverge when this assumption is not\nsatisfied. Given a sequence {βn}n∈N, one way to ensure that Assumption 4 is satisfied is to replace the\nrandom matrices An with\n˜An = min{∥An∥, βn+1}\n∥An∥\nAn.\n(5)\nIt is then clear that ∥ ˜An∥ ≤ βn+1. Furthermore, in most cases, especially for Adagrad and Stochastic Newton\nalgorithm, control of λmin (An) in Assumption 4 is satisfied, as discussed by Godichon-Baggioni and Tarrago\n(2023, Section 3.2.1).\n4\nConvergence Results\n4.1\nConvex case\nIn this section, we study the convergence rate of SGD with biased gradients and adaptive steps in the convex\ncase. We give below a simplified version of the bound we obtained on the risk and refer to Theorem A.2 in\nthe appendix for a formal statement with explicit constants.\nTheorem 4.1. Let θn ∈ Rd be the n-th iterate of the recursion (3) and γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ\nwith Cγ > 0, Cβ > 0, and Cλ > 0. Assume that γ, β, λ ≥ 0 and γ + λ < 1. Then, under Assumptions 1 − 4,\nwe have:\nE [V (θn) − V (θ∗)]= O\n\u0010\nn−γ+2β+λ + bn\n\u0011\n,\n(6)\nwhere the bias term bn can be constant or decreasing. In the latter case, writing rn = n−α, we have:\nbn = O\n\u0010\nn−2α+2β+2λ\u0011\n.\nThe rate obtained is classical and shows the tradeoff between a term coming from the adaptive steps\n(with a dependence on γ, β, λ) and a term bn which depends on the control of the bias rn. To minimize\nthe right hand-side of (6), we would like to have β = λ = 0. However, this would require much stronger\nassumptions. For example, in the case of Adagrad and RMSProp, the gradients would need to be bounded,\nwhich will be discussed later.\nWe stress that Theorem 4.1 applies to any adaptive algorithm of the form (3), with the only assumption\nbeing Assumption 4 on the eigenvalues of An. Without any information on these eigenvalues, the choice that\nβn ∝ nβ and λn ∝ n−λ allows us to remain very general.\nAn interesting example where we can control the bias term rn is the case of Monte Carlo methods. In this\nscenario, the bias depends on a number N of samples used to estimate the gradient and is usually of order\n1/N, see for instance Agapiou et al. (2017); Del Moral et al. (2010); Cardoso et al. (2023). It can therefore\nbe controlled by choosing a different number of samples Nn at each iteration n ≥ 1. To obtain a bias of order\nO(n−α) at iteration n, we simply select Nn = nα. This idea will be further developed in Section 5.\nTo illustrate Theorem 4.1 and the impact of bias, we consider in Figure 1 a simple least squares objective\nfunction V (θ) = ∥Aθ∥2/2 in dimension d = 10. We artificially add to every gradient a zero-mean Gaussian\nnoise and a bias term rn = n−α at each iteration n. We use Adagrad with a learning rate γ = 1/2, β = 0\nand λ = 0. Then, the bound of Theorem 4.1 is of the form O(n−1/2 + n−2α). First, note that the impact of\n5\na constant bias term (rn = 1) never vanishes. From rn = 1 to rn = n−1/4, the effect of the bias decreases\nuntil a threshold is reached where there is no significant improvement. The convergence rate in the case\nrn = n−1/4 is then the same as in the case without bias, illustrating the fact that in this case the dominating\nterm comes from the learning rate.\n0\n2000\n4000\n6000\n8000\n10000\nEpochs\n10\n1\n100\nV(\nn)\nV(\n*)\nn\n1/2\nn\n1/4\nrn = 1\nrn = n\n1/8\nrn = n\n1/4\nrn = n\n1/2\nrn = n\n1\nrn = 0\nFigure 1: Value of V (θn)−V (θ∗) with Adagrad for different values of rn = n−α and a learning rate γn = n−1/2.\nThe dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8 and O(n−1/2) for\nα ≥ 1/4.\nFinally, note that non-adaptive SGD is a particular case of Theorem 4.1. Thus, our theorem gives new\nresults also in the non-adaptive case with generic step sizes and biased gradients, while the only existing\nresults in the literature assume constant step sizes (Ajalloeian and Stich, 2020).\n4.2\nNon-convex smooth case\nIn the non-convex smooth case, the theoretical results are based on a randomized version of Stochastic\nApproximation as described by Nemirovski et al. (2009); Ghadimi and Lan (2013); Karimi et al. (2019). In\nclassical SA, the update (3) is performed a fixed number of times n, and the quantity of interest is the last\nparameter θn. On the other hand, in Randomized Stochastic Approximation, we introduce a random variable\nR which takes its values in {1, . . . , n} and the quantity of interest is θR. We stress that this procedure is a\ntechnical tool for the proofs, in practical applications we will always use classical SA.\nThe following theorem provides a bound in expectation on the gradient of the objective function V , which\nis the best results we can have given that no assumption is made about existence of a global minimum of V .\nTheorem 4.2. Assume that for all k ≥ 0, we have γk+1 ≤ λk+1/(6Lβ2\nk+1). For any n ≥ 1, let R ∈ {0, . . . , n}\nbe a discrete random variable, independent of {Fn, n ∈ N}, such that:\nP(R = k) :=\nγk+1λk+1\nPn\nj=0 γj+1λj+1\n.\nThen, under Assumptions 2 − 4, we have:\nE\nh\n∥∇V (θR)∥2i\n≤ 3V ∗ + α1,n/2 + Lα2,n\nPn\nj=0 γj+1λj+1\n,\nwhere\nα1,n =\nn\nX\nk=0\nγk+1β2\nk+1r2\nk/λk+1,\nα2,n =\nn\nX\nk=0\nγ2\nk+1β2\nk+1σ2\nk,\nand\nV ∗ = E[V (θ0) − V (θ∗)].\n6\nTheorem 4.2 recovers the asymptotic rate of convergence obtained by Karimi et al. (2019) but with\nexplicit bounds with respect to the hyper-parameters γ, β, λ and the bias. We can observe that if γ ≤ λ + 2β,\nthe condition on (γk)k≥1 can be met simply by tuning Cγ. In particular, if An = Id, the requirement on the\nstep sizes can be expressed as γk+1 ≤ 1/(6L).\nWe give below the convergence rates obtained from Theorem 4.2 under the same assumptions on γn, βn\nand λn as in the convex case.\nCorollary 4.3. Let γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ with Cγ > 0, Cβ > 0, and Cλ > 0. Assume that\nγ, β, λ ≥ 0 and γ + λ < 1. Then, under Assumptions 2 − 4, we have:\nE\nh\n∥∇V (θR)∥2i\n=\n\n\n\n\n\nO\n\u0000n−γ+λ+2β + bn\n\u0001\nif ϑ < 1/2 ,\nO\n\u0000nγ+λ−1 + bn\n\u0001\nif ϑ > 1/2 ,\nO\n\u0000nγ+λ−1log n + bn\n\u0001\nif ϑ = 1/2 ,\nwith ϑ = γ −β and where the bias term bn can be constant or decreasing. In the latter case, writing rn = n−α,\nwe have:\nbn =\n\n\n\n\n\nO\n\u0000nβ−2α\u0001\nif 2α < 1 − γ + λ + 2β ,\nO\n\u0000nγ+λ−1\u0001\nif 2α > 1 − γ + λ + 2β ,\nO\n\u0000nγ+λ−1 log n\n\u0001\nif 2α = 1 − γ + λ + 2β .\nIn practice, the value of α is known in advance while the other parameters can be tuned to achieve the\noptimal rate of convergence. In any scenario, we can never achieve a bound of O(1/√n + bn), and the best\nrate we can achieve is O(log n/√n + bn) if and only if γ = 1/2, β = 0 and λ = 0. In this case, all eigenvalues\nof An must be bounded both from below and above. In the context of decreasing bias, if α ≥ 1/4, the bias\nterm contributes to the speed of the algorithm. Otherwise, the other term is the leading term of the upper\nbound. However, in both cases, the best achievable bound is O(log n/√n) if α ≥ 1/4.\nBounded Gradient Case.\nNow, we provide the convergence analysis of Randomized Adaptive\nStochastic Approximation with a bounded stochastic update. Consider the following additional assumption\nabout the stochastic update.\nAssumption 5. The stochastic update is bounded, i.e., there exists M ≥ 0 such that for all n ∈ N,\n∥Hθn (Xn+1)∥ ≤ M.\nIt is important to note that under Assumption 3, this is equivalent to bounding the stochastic gradient of\nthe objective function. Corollary 4.4 provides a bound on the gradient of the objective function V , which is\nsimilar to Theorem 4.2.\nCorollary 4.4. Let γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ with Cγ > 0, Cβ > 0, and Cλ > 0. Assume that\nγ, β, λ ≥ 0 and γ + λ < 1. For any n ≥ 1, let R ∈ {0, . . . , n} be a uniformly distributed random variable.\nThen, under Assumptions 2 − 4, 5, we have:\nE\nh\n∥∇V (θR)∥2i\n≤ 2V ∗ + α1,n + LM2α′\n2,n\n√n\n,\nwhere V ∗, α1,n are defined in Theorem 4.2 and α′\n2,n = Pn\nk=0 γ2\nk+1β2\nk+1.\nImportantly, in Corollary 4.4, there are no assumptions on the step sizes, and we obtain a better bound\nthan in Theorem 4.2.\n4.3\nApplication to Adagrad and RMSProp\nIn this section, we provide the convergence analysis of Adagrad and RMSProp with a biased gradient\nestimator.\nRemark 4.5. Under Assumption 5, for all eigenvalues λ of An, the adaptive matrix in Adagrad or RMSProp,\nit holds that (M2 + δ)−1/2 ≤ λ ≤ δ−1/2, i.e., Assumption 4 is satisfied with λ = 0 and β = 0.\n7\nCorollary 4.6. Let γn = cγn−1/2 and An denote the adaptive matrix in Adagrad or RMSProp. For any\nn ≥ 1, let R ∈ {0, . . . , n} be a uniformly distributed random variable. Then, under Assumptions 2, 3, 5, we\nhave:\nE\nh\n∥∇V (θR)∥2i\n= O\n\u0012log n\n√n + bn\n\u0013\n.\nThe bias bn is explicitly given in Appendix A.\nSince we do not have information about the global minimum of the objective function V , Corollary 4.6\nestablishes the rate of convergence of Adagrad and RMSProp with biased gradient to a critical point for\nnon-convex smooth functions. In the case of an unbiased gradient, we obtain the same bound as in Zou et al.\n(2018), under the same assumptions:\nE\nh\n∥∇V (θR)∥2i\n= O\n\u0012log n\n√n\n\u0013\n.\nIf the bias is of the order O(n−1/4), the algorithm achieves the same convergence rate as in the case of an\nunbiased gradient.\n5\nExperiments\nIn this section, we illustrate our theoretical results in the context of deep VAE. The experiments were\nconducted using PyTorch (Paszke et al., 2017), and the source code can be found here∗. In generative models,\nthe objective is to maximize the marginal likelihood defined as:\nlog pθ(x) = log Epθ(·|x)\n\u0014pθ(x, Z)\npθ(Z|x)\n\u0015\n,\nwhere (x, z) 7→ pθ(x, z) is the complete likelihood, x are the observations and Z is the latent variable. Under\nsome simple technical assumptions, by Fisher’s identity, we have:\n∇θ log pθ(x) =\nZ\n∇θ log pθ(x, z)pθ(z | x)dz.\n(7)\nHowever, in most cases, the conditional density z 7→ pθ(z | x) is intractable and can only be sampled.\nVariational Autoencoders introduce an additional parameter ϕ and a family of variational distributions\nz 7→ qϕ(z | x) to approximate the true posterior distribution. Parameters are estimated by maximizing the\nEvidence Lower Bound (ELBO):\nlog pθ(x) ≥ Eqϕ(·|x)\n\u0014\nlog pθ(x, Z)\nqϕ(Z|x)\n\u0015\n=: LELBO(θ, ϕ; x).\nThe Importance Weighted Autoencoder (IWAE) (Burda et al., 2015) is a variant of the VAE that incorporates\nimportance weighting to obtain a tighter ELBO. The IWAE objective can be written as follows:\nLIWAE\nk\n(θ, ϕ; x) = Eq⊗k\nϕ (·|x)\n\"\nlog 1\nk\nk\nX\nℓ=1\npθ(x, Z(ℓ))\nqϕ(Z(ℓ)|x)\n#\n,\nwhere k corresponds to the number of samples drawn from the variational posterior distribution. The\nestimator of the gradient of ELBO in IWAE corresponds to the biased Self-Normalized Importance Sampling\n(SNIS) estimator of the gradient of the marginal log likelihood log pθ(x). As a consequence of the results of\nAgapiou et al. (2017), both the bias and Mean Squared Error (MSE) are of order O(1/k). Since the bias\nhas an impact on the convergence rates, we propose to use the Biased Reduced Self-Normalized Importance\nSampling (BR-SNIS) algorithm, proposed by Cardoso et al. (2022), to the IWAE estimator, resulting in the\nBiased Reduced Importance Weighted Autoencoder (BR-IWAE). The BR-IWAE algorithm (Cardoso et al.,\n2022) proceeds in two steps, which are repeated during optimization:\n∗URL hidden during review process\n8\n• Update the parameter ϕ as in the IWAE algorithm, that is, for all n ≥ 1:\nϕn+1 = ϕn − γn+1An∇ϕLIWAE\nk\n(θn, ϕn; Xn+1).\n• Update the parameter θ by estimating (7) using BR-SNIS as detailed in Appendix B.\nDataset.\nWe conduct our experiments on the CIFAR-10 dataset (Krizhevsky et al., 2009), which is a\nwidely used dataset for image classification tasks. CIFAR-10 consists of 32x32 pixel images categorized into\n10 different classes. The dataset is divided into 60,000 images in the training set and 10,000 images in the\ntest set. Additional experiments are provided in Appendix B.\nModel.\nWe use a Convolutional Neural Network (CNN) architecture with the Rectified Linear Unit\n(ReLU) activation function for both the encoder and the decoder. The latent space dimension is set to 100.\nFurther details of the model are provided in Appendix B. We estimate the log-likelihood using the VAE,\nIWAE, and BR-IWAE models, all of which are trained for 100 epochs. Training is conducted using Adagrad\nand RMSProp with a decaying learning rate.\n0\n20\n40\n60\n80\n100\nEpochs\n1.825\n1.85\n1.875\n1.9\n1.925\n1.95\n1.975\n2\nTest loss (x10e3)\nVAE (Adagrad)\nIWAE (Adagrad)\nBR-IWAE (Adagrad)\nVAE (RMSProp)\nIWAE (RMSProp)\nBR-IWAE (RMSProp)\nFigure 2: Negative Log-Likelihood on the test set for Different Generative Models with Adagrad and\nRMSProp on the CIFAR-10 Dataset. Bold lines represent the mean over 5 independent runs.\nIn this section, we exclusively use Adagrad and RMSProp algorithms to illustrate the results of Section 4.3\nand we do not conduct a comparison with SGD. For the first experiment, we set a constant bias, i.e., we\nuse k = 5 samples in both IWAE and BR-IWAE, while restricting the maximum iteration of the MCMC\nalgorithm to 5 for BR-IWAE. The test losses are presented in Figure 2. We show the negative log-likelihood\non the test dataset for VAE, IWAE, and BR-IWAE with Adagrad and RMSProp. As expected, we observe\nthat IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias in both cases.\nTo illustrate our results, we choose to incorporate a time-dependent bias that decreases by choosing a bias\nof order O(n−α) at iteration n. The bias of the estimator of the gradient in IWAE is of the order O(1/k),\nwhere k is the number of importance weights. Therefore, choosing the bias of order O(n−α) is equivalent to\nusing nα samples at iteration n, to estimate the gradient. This procedure is detailed in Appendix B. We\nvary α only for IWAE for computational efficiency and plot the following quantities.\n• In Figures 3 and 4, the gradient squared norm ∥∇V (θn)∥2 to illustrate the convergence rate.\n• In Figure 5, the Negative Log-Likelihood along iterations.\nFigures 3 and 4 illustrate our results, while the other figures are meant to confirm the behavior of test\nloss with different values of α. All figures are plotted on a logarithmic scale for better visualization. It is\nimportant to note that all figures are in respect to epochs, whereas here, n represents the iteration (number\nof updates of the gradient).\nNote that the dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8 and\nO(log n/√n) for α = 1/4 and for α = 1/2. We can clearly observe that for each of the cases, fast convergence\n9\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n10\n2\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\nFigure 3: Value of ∥∇V (θn)∥2 in IWAE with Adagrad for different values of α. Bold lines represent the\nmean over 5 independent runs.\n0\n20\n40\n60\n80\n100\nEpochs\n100\n101\n102\nV(\nn) 2\n= 1/8 (RMSProp)\n= 1/4 (RMSProp)\n= 1/2 (RMSProp)\n= 1/8 (Adam)\n= 1/4 (Adam)\n= 1/2 (Adam)\nFigure 4: Value of ∥∇V (θn)∥2 in IWAE with RMSProp and Adam for different values of α. Bold lines\nrepresent the mean over 5 independent runs.\nis achieved when n is sufficiently large. There are several possible explanations for this rapid convergence\nwith a decreasing time-dependent bias. First, we may be able to improve the upper bound by obtaining for\ninstance a better bound for the bias.\nOur experiments show similar results for Adagrad and RMSProp in terms of convergence rates, although\nRMSProp performs slightly better. However, Adam’s behavior is somewhat more intricate due to the\nincorporation of momentum. Nevertheless, we consistently observe the impact of bias, although it tends to\nbe relatively small, as the bias correction terms may help mitigate bias in the moving averages.\nIt is clear that with a larger α, convergence in both squared gradient norm and negative log-likelihood\nis faster. However, beyond a certain threshold for α, we observe that the rate of convergence does not\nchange significantly. Since choosing a larger α induces an additional computational cost, it is crucial to select\nan appropriate value of α, that achieves fast convergence without being too computationally costly. One\napproach to choose such a value for α is to plot the test loss with respect to time, which is shown in Figure 6\nfor Adagrad. Although α = 1/2 seems to have faster convergence in terms of iterations in Figure 5, the best\nrate in terms of time in Figure 6 is α = 1/4. We obtain similar results for RMSProp, see Appendix B.\n10\n0\n20\n40\n60\n80\n100\nEpochs\n1.825\n1.85\n1.875\n1.9\n1.925\n1.95\n1.975\n2\nTest loss (x10e3)\n= 1/8 (Adagrad)\n= 1/4 (Adagrad)\n= 1/2 (Adagrad)\n= 1/8 (RMSProp)\n= 1/4 (RMSProp)\n= 1/2 (RMSProp)\nFigure 5: Negative Log-Likelihood on the test set on the CIFAR-10 Dataset for IWAE with Adagrad and\nRMSProp for Different Values of α. Bold lines represent the mean over 5 independent runs.\n0\n200\n400\n600\n800\n1000\nTime (s)\n1.84\n1.86\n1.88\n1.9\n1.92\n1.94\n1.96\n1.98\nTest loss (x10e3)\n= 1/8\n= 1/4\n= 1/2\nFigure 6: Negative Log-Likelihood on the test set of the CIFAR-10 Dataset for IWAE with Adagrad for\nDifferent Values of α over time (in seconds). Bold lines represent the mean over 5 independent runs.\n6\nDiscussion\nThis paper provides a comprehensive non-asymptotic analysis of Biased Adaptive Stochastic Approximation\nfor both convex and non-convex smooth functions. We derive a convergence rate of O(1/√n + bn) for convex\nfunctions and O(log n/√n+bn) for non-convex smooth functions, where bn corresponds to the time-dependent\ndecreasing bias. We also establish that Adagrad and RMSProp with biased gradients converges to critical\npoints for non-convex smooth functions. Our results provide insights on hyper-parameters tuning to achieve\nfast convergence and reduce unnecessary computational time.\nWe intend to investigate whether the eigenvalues control conditions can be satisfied in practical settings,\nsuch as in the case of Stochastic Newton or Gauss Newton methods. We conducted additional comparisons\nwith Adam to propose practical insights. However, a possible extension of our work is to extend our theoretical\nresults to Adam.\nReferences\nAgapiou, S., Papaspiliopoulos, O., Sanz-Alonso, D., and Stuart, A. M. (2017). Importance sampling: Intrinsic\ndimension and computational cost. Statistical Science, pages 405–431.\n11\nAjalloeian, A. and Stich, S. U. (2020). On the Convergence of SGD with Biased Gradients. arXiv preprint\narXiv:2008.00051.\nAuer, P., Cesa-Bianchi, N., and Gentile, C. (2002). Adaptive and self-confident on-line learning algorithms.\nJournal of Computer and System Sciences, 64(1):48–75.\nBaxter, J. and Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial\nIntelligence Research, 15:319–350.\nBhandari, J., Russo, D., and Singal, R. (2018). A finite time analysis of temporal difference learning with\nlinear function approximation. In Conference On Learning Theory, pages 1691–1692. PMLR.\nBottou, L. (1991). Une approche th´eorique de l’apprentissage connexioniste; applications `a la reconnaissance\nde la parole. PhD thesis, Paris 11.\nBottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for large-scale machine learning.\nSIAM Review, 60(2):223–311.\nBoyer, C. and Godichon-Baggioni, A. (2023). On the asymptotic rate of convergence of stochastic newton\nalgorithms and their weighted averaged versions. Computational Optimization and Applications, 84(3):921–\n972.\nBurda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint\narXiv:1509.00519.\nCardoso, G., Idrissi, Y. J. E., Le Corff, S., Moulines, ´E., and Olsson, J. (2023). State and parameter learning\nwith PaRIS particle Gibbs. arXiv preprint arXiv:2301.00900.\nCardoso, G., Samsonov, S., Thin, A., Moulines, E., and Olsson, J. (2022). BR-SNIS: bias reduced self-\nnormalized importance sampling. Advances in Neural Information Processing Systems, 35:716–729.\nChen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. (2017). Zoo: Zeroth order optimization based\nblack-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th\nACM Workshop on Artificial Intelligence and Security, pages 15–26.\nDalal, G., Sz¨or´enyi, B., Thoppe, G., and Mannor, S. (2018). Finite sample analyses for td (0) with function\napproximation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.\nD´efossez, A., Bottou, L., Bach, F., and Usunier, N. (2020). A simple convergence proof of Adam and Adagrad.\narXiv preprint arXiv:2003.02395.\nDel Moral, P., Doucet, A., and Singh, S. S. (2010). A backward particle interpretation of Feynman-Kac\nformulae. ESAIM: Mathematical Modelling and Numerical Analysis, 44(5):947–975.\nDoan, T. T., Nguyen, L. M., Pham, N. H., and Romberg, J. (2020). Finite-time analysis of stochastic\ngradient descent under markov randomness. arXiv preprint arXiv:2003.10973.\nDozat, T. (2016). Incorporating Nesterov momentum into Adam.\nDuchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic\noptimization. Journal of Machine Learning Research, 12(7).\nGhadimi, S. and Lan, G. (2013).\nStochastic first-and zeroth-order methods for nonconvex stochastic\nprogramming. SIAM Journal on Optimization, 23(4):2341–2368.\nGloaguen, P., Le Corff, S., and Olsson, J. (2022). A pseudo-marginal sequential Monte Carlo online smoothing\nalgorithm. Bernoulli, 28(4):2606–2633.\nGodichon-Baggioni, A. and Tarrago, P. (2023). Non asymptotic analysis of adaptive stochastic gradient\nalgorithms and applications. arXiv preprint arXiv:2303.01370.\n12\nJaakkola, T., Jordan, M., and Singh, S. (1993). Convergence of stochastic iterative dynamic programming\nalgorithms. Advances in Neural Information Processing Systems, 6.\nKarimi, B., Miasojedow, B., Moulines, E., and Wai, H.-T. (2019). Non-asymptotic analysis of biased\nstochastic approximation scheme. In Conference on Learning Theory, pages 1944–1974. PMLR.\nKingma, D. P. and Ba, J. (2014).\nAdam:\nA method for stochastic optimization.\narXiv preprint\narXiv:1412.6980.\nKingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114.\nKresoja, M., Luˇzanin, Z., and Stojkovska, I. (2017). Adaptive stochastic approximation algorithm. Numerical\nAlgorithms, 76(4):917–937.\nKrizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images.\nLakshminarayanan, C. and Szepesvari, C. (2018). Linear stochastic approximation: How far does constant\nstep-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics,\npages 1347–1355. PMLR.\nLi, Q. and Wai, H.-T. (2022). State dependent performative prediction with stochastic approximation. In\nInternational Conference on Artificial Intelligence and Statistics, pages 3164–3186. PMLR.\nMoosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P. (2017). Universal adversarial perturbations.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1765–1773.\nMoulines, E. and Bach, F. (2011). Non-asymptotic analysis of stochastic approximation algorithms for\nmachine learning. Advances in Neural Information Processing Systems, 24.\nNemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to\nstochastic programming. SIAM Journal on Optimization, 19(4):1574–1609.\nNemirovskij, A. S. and Yudin, D. B. (1983). Problem complexity and method efficiency in optimization.\nNesterov, Y. and Spokoiny, V. (2017). Random gradient-free minimization of convex functions. Foundations\nof Computational Mathematics, 17:527–566.\nOlsson, J. and Westerborn, J. (2017). Efficient particle-based online smoothing in general hidden Markov\nmodels: the paris algorithm.\nPaszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L.,\nand Lerer, A. (2017). Automatic differentiation in PyTorch.\nPolyak, B. T. and Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM\nJournal on Control and Optimization, 30(4):838–855.\nRobbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical\nStatistics, pages 400–407.\nSun, T., Sun, Y., and Yin, W. (2018). On Markov chain gradient descent. Advances in Neural Information\nProcessing Systems, 31.\nTadi´c, V. B. and Doucet, A. (2011). Asymptotic bias of stochastic gradient search. In 2011 50th IEEE\nConference on Decision and Control and European Control Conference, pages 722–727. IEEE.\nTieleman, T., Hinton, G., et al. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its\nrecent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2):26–31.\nTjelmeland, H. (2004). Using all metropolis–hastings proposals to estimate mean values. Technical report.\nWard, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex landscapes.\nThe Journal of Machine Learning Research, 21(1):9047–9076.\n13\nXiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine\nlearning algorithms. arXiv preprint arXiv:1708.07747.\nZeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701.\nZou, F., Shen, L., Jie, Z., Sun, J., and Liu, W. (2018). Weighted adagrad with unified momentum. arXiv\npreprint arXiv:1808.03408.\nA\nProofs\nA.1\nProof of Theorem 4.1\nWe first establish a technical lemma which is essential for the proof.\nLemma A.1. Let (δn)n≥0 , (γn)n≥1 , (ηn)n≥1, and (vn)n≥1 be some positive sequences satisfying the following\nassumptions.\n• The sequence δn follows the recursive relation:\nδn ≤ (1 − 2ωγn + ηnγn) δn−1 + vnγn,\nwith δ0 ≥ 0 and ω > 0.\n• Let n0 = inf {n ≥ 1 : ηn ≤ ω}, then for all n ≥ n0 + 1, we assume that ωγn ≤ 1.\nThen, for all n ∈ N,\nδn ≤ exp\n\n−ω\nn\nX\nk=n/2\nγk\n\n exp\n \n2\nn\nX\nk=1\nηkγk\n! \u0012\nδ0 + 2 max\n1≤k≤n\nvk\nηk\n\u0013\n+ 1\nω\nmax\nn/2≤k≤n vk.\nThe proof is given in Godichon-Baggioni and Tarrago (2023). We will present the detailed version of\nTheorem 4.1.\nTheorem A.2. Let θn ∈ Rd be the n-th iterate of the recursion (3). Under Assumptions 1 − 4, we have:\nE [V (θn) − V (θ∗)] ≤\n \nE [V (θ0) − V (θ∗)] + 1\nL2 max\n1≤k≤n\nλk+1vk\nβ2\nk+1γk+1\n!\nexp\n\n−µ\n2\nn\nX\nk=n/2\nλk+1γk+1\n\n\nexp\n \n4L2\nn\nX\nk=1\nCkβ2\nk+1γ2\nk+1\n!\n+ 2\nµ\nmax\nn/2≤k≤n vk,\nwhere\nCk = max\n(\n1, µ2λ2\nk+1\n8L2β2\nk+1\n)\nand\nvk = 1\n2\nLβ2\nk+1r2\nk\nµλ2\nk+1\n+ Lσ2\nk\nβ2\nk+1\nλk+1\nγk+1.\nProof. As V is L smooth (Assumption 2) and using the recursion (3) of Adaptive SA, we obtain:\nV (θn+1) ≤ V (θn) + ⟨∇V (θn) | θn+1 − θn⟩ + L\n2 ∥θn+1 − θn∥2\n≤ V (θn) − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2\nn+1\n2\n∥An∥2 ∥Hθn (Xn+1)∥2\n≤ V (θn) − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2\nn+1\n2\nβ2\nn+1 ∥Hθn (Xn+1) − ∇V (θn) + ∇V (θn)∥2 .\nUsing that for all x, y ≥ 0, ∥x + y∥2 ≤ 2\n\u0010\n∥x∥2 + ∥y∥2\u0011\n, and writing Vn = V (θn) − V (θ∗), we get\nVn+1 ≤ Vn − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2\nn+1β2\nn+1\n\u0010\n∥Hθn (Xn+1) − ∇V (θn)∥2 + ∥∇V (θn)∥2\u0011\n.\n14\nThen, using Assumption 3, and since ∥∇V (θn)∥2 ≤ 2LVn (Assumption 1 and 2),\nE [Vn+1 | Fn] ≤\n\u00001 + 2L2β2\nn+1γ2\nn+1\n\u0001\nVn − γn+1 ⟨∇V (θn) | An (E [Hθn (Xn+1) | Fn] − ∇V (θn))⟩\n− γn+1 ⟨∇V (θn) | An∇V (θn)⟩ + Lσ2\nnγ2\nn+1β2\nn+1\n≤\n\u00001 + 2L2β2\nn+1γ2\nn+1\n\u0001\nVn + γn+1βn+1rn ∥∇V (θn)∥ − γn+1 ⟨∇V (θn) | An∇V (θn)⟩\n+ Lσ2\nnγ2\nn+1β2\nn+1\n≤\n\u00001 + 2L2β2\nn+1γ2\nn+1\n\u0001\nVn +\n1\n2an\nγn+1βn+1r2\nn + anLγn+1βn+1Vn\n− γn+1 ⟨∇V (θn) | An∇V (θn)⟩ + Lσ2\nnγ2\nn+1β2\nn+1,\nwhere the second last inequality is due to the Cauchy-Schwarz inequality. In the last inequality, we used\n∥∇V (θn)∥2 ≤ 2LVn and the inequality xy ≤ x2/(2an) + any2/2 where an is defined below. Furthermore,\nsince V satisfies the Polyak- Lojasiewicz condition (Assumption 1),\n⟨∇V (θn) | An∇V (θn)⟩ ≥ λmin (An) ∥∇V (θn)∥2 ≥ 2λn+1µVn.\nFinally, choosing an = (µλn+1)/(Lβn+1) we obtain:\nE [Vn+1] ≤\n\u00001 − µλn+1γn+1 + 2L2β2\nn+1γ2\nn+1\n\u0001\nE [Vn] + 1\n2\nLβ2\nn+1r2\nn\nµλn+1\nγn+1 + Lσ2\nnγ2\nn+1β2\nn+1.\nBy choosing ¯γn+1 = λn+1γn+1, we get:\nE [Vn+1] ≤\n\u0012\n1 − µ¯γn+1 + 2L2 β2\nn+1\nλ2\nn+1\n¯γ2\nn+1\n\u0013\nE [Vn] + 1\n2\nLβ2\nn+1r2\nn\nµλ2\nn+1\n¯γn+1 + Lσ2\nn\nβ2\nn+1\nλ2\nn+1\n¯γ2\nn+1.\nIn order to satisfy the assumptions of Lemma A.1, consider Cn = max\n\b\n1, (µ2λ2\nn+1)/(8L2β2\nn+1)\n\t\n, and since\nCn ≥ 1, we have:\nE [Vn+1] ≤\n\u0012\n1 − µ¯γn+1 + 2L2 Cnβ2\nn+1\nλ2\nn+1\n¯γ2\nn+1\n\u0013\nE [Vn] + 1\n2\nLβ2\nn+1r2\nn\nµλ2\nn+1\n¯γn+1 + Lσ2\nn\nβ2\nn+1\nλ2\nn+1\n¯γ2\nn+1.\nNow, using lemma A.1 by choosing:\nδn = E [Vn] ,\nηn = 2L2 Cnβ2\nn+1\nλ2\nn+1\n¯γn+1,\nω = µ\n2 ,\nvn = 1\n2\nLβ2\nn+1r2\nn\nµλ2\nn+1\n+ Lσ2\nn\nβ2\nn+1\nλ2\nn+1\n¯γn+1,\nwe have:\nE [V (θn) − V (θ∗)] ≤\n \nE [V (θ0) − V (θ∗)] + 1\nL2 max\n1≤k≤n\nvkλ2\nk+1\nβ2\nk+1¯γk+1\n!\ne− µ\n2\nPn\nk=n/2 ¯γk+1e\n4L2 Pn\nk=1\nCkβ2\nk+1\nλ2\nk+1\n¯γ2\nk+1\n+ 2\nµ\nmax\nn/2≤k≤n {vk} ,\nwhich concludes the proof by replacing ¯γn+1 = λn+1γn+1.\nA.2\nProof of Theorem 4.2\nBy Assumption 2, V is L-smooth and using the recursion (3) of Adaptive SA together with a Taylor expansion,\nwe obtain:\nV (θk+1) ≤ V (θk) + ⟨∇V (θk) | θk+1 − θk⟩ + L\n2 ∥θk+1 − θk∥2 ,\nwhich yields\nV (θk+1) ≤ V (θk) − γk+1 ⟨∇V (θk) | Ak∇V (θk)⟩ − γk+1 ⟨∇V (θk) | Ak (Hθk (Xk+1) − ∇V (θk))⟩\n+ δk+1\n\u0000∥Hθk (Xk+1) − ∇V (θk)∥2 + ∥∇V (θk)∥2 \u0001\n,\n15\nwith δk+1 = Lγ2\nk+1β2\nk+1. By the Cauchy-Schwarz inequality, and using Assumptions (3) and (4),\nE[V (θk+1)|Fk] ≤ V (θk) + γk+1βk+1rk∥∇V (θk)∥ − γk+1λmin(Ak)∥∇V (θk)∥2 + δk+1(σ2\nk + ∥∇V (θk)∥2).\nTherefore,\nγk+1\n\u0012\nλk+1 − λk+1\n2\n− Lγk+1β2\nk+1\n\u0013\n∥∇V (θk)∥2 ≤ V (θk) − E [V (θk+1) |Fk] + γk+1β2\nk+1r2\nk\n2λk+1\n+ δk+1σ2\nk.\nand\nn\nX\nk=0\nγk+1λk+1\n \n1\n2 − Lγk+1β2\nk+1\nλk+1\n!\nE\nh\n∥∇V (θk)∥2i\n≤ E [V (θ0) − V (θn+1)] + 1\n2\nn\nX\nk=0\nγk+1β2\nk+1r2\nk\nλk+1\n+\nn\nX\nk=0\nδk+1σ2\nk.\nThen, given that γk+1 ≤ λk+1/(6Lβ2\nk+1), we have\n1\n3E\n\" n\nX\nk=0\nγk+1λk+1 ∥∇V (θk)∥2\n#\n≤ E [V (θ0) − V (θn+1)] + 1\n2\nn\nX\nk=0\nγk+1β2\nk+1r2\nk\nλk+1\n+\nn\nX\nk=0\nδk+1σ2\nk .\nConsequently, by definition of the discrete random variable R,\nE\nh\n∥∇V (θR)∥2i\n=\nn\nX\nk=0\nγk+1λk+1E\nh\n∥∇V (θk)∥2i\nPn\nj=0 γj+1λj+1\n≤ 3V0,n + 1\n2\nPn\nk=0 γk+1β2\nk+1r2\nk/λk+1 + Pn\nk=0 δk+1σ2\nk\nPn\nj=0 γj+1λj+1\n,\nwhere V0,n = E[V (θ0) − V (θn+1)], which concludes the proof by noting that V (θn+1) ≥ V (θ∗).\nA.3\nProof of Corollary 4.3\nThe proof is a direct consequence of the fact that for a sufficiently large n:\nn\nX\nk=1\n1\nks =\n\n\n\n\n\nO\n\u0000n−s+1\u0001\nif 0 ≤ s < 1 ,\nO (1)\nif s > 1 ,\nO (log n)\nif s = 1 .\nA.4\nProof of Corollary 4.4\nBy Assumption 2, V is L-smooth and using recursion (3) of Adaptive SA same as Theorem 4.2, we obtain:\nV (θk+1) ≤ V (θk) + ⟨∇V (θk) | θk+1 − θk⟩ + L\n2 ∥θk+1 − θk∥2\n≤ V (θk) − γk+1 ⟨∇V (θk) | AkHθk (Xk+1)⟩ + Lγ2\nk+1\n2\n∥Ak∥2 ∥Hθk (Xk+1)∥2 ,\nwhich, using Assumption (5) yields:\nV (θk+1) ≤ V (θk) − γk+1 ⟨∇V (θk) | Ak∇V (θk)⟩ − γk+1 ⟨∇V (θk) | Ak (Hθk (Xk+1) − ∇V (θk))⟩\n+ L\n2 γ2\nk+1β2\nk+1M2.\nBy the Cauchy-Schwarz inequality, and using Assumptions (3) and (4),\nE[V (θk+1)|Fk] ≤ V (θk) − γk+1λmin(Ak)∥∇V (θk)∥2 + γk+1βk+1rk∥∇V (θk)∥ + LM2\n2\nγ2\nk+1β2\nk+1\n≤ V (θk) − γk+1λk+1∥∇V (θk)∥2 + γk+1βk+1rk∥∇V (θk)∥ + LM2\n2\nγ2\nk+1β2\nk+1 .\n16\nTherefore,\nγk+1\n\u0012\nλk+1 − λk+1\n2\n\u0013\n∥∇V (θk)∥2 ≤ V (θk) − E [V (θk+1) |Fk] + γk+1β2\nk+1r2\nk\n2λk+1\n+ LM2\n2\nγ2\nk+1β2\nk+1,\nand\n1\n2\nn\nX\nk=0\nγk+1λk+1E\nh\n∥∇V (θk)∥2i\n≤ E [V (θ0) − V (θn+1)] + 1\n2\nn\nX\nk=0\nγk+1β2\nk+1r2\nk\nλk+1\n+ LM2\n2\nn\nX\nk=0\nγ2\nk+1β2\nk+1.\nConsequently, by definition of the discrete random variable R,\nE\nh\n∥∇V (θR)∥2i\n= 1\nn\nn\nX\nk=0\nE\nh\n∥∇V (θk)∥2i\n≤\nn\nX\nk=0\nγk+1λk+1\n√n\nE\nh\n∥∇V (θk)∥2i\n≤ 2V0,n + Pn\nk=0 γk+1β2\nk+1r2\nk/λk+1 + LM2 Pn\nk=0 γ2\nk+1β2\nk+1\n√n\n,\nwhere V0,n = E[V (θ0) − V (θn+1)], which conclude the proof by noting that V (θn+1) ≥ V (θ∗).\nA.5\nProof of Corollary 4.6\nAdagrad\n• Upper bound for the largest eigenvalue of An.\nBy assumption (5), we have:\n\r\r\r\r\r\n1\nn\nn−1\nX\nk=0\nHθk(Xk+1)Hθk(Xk+1)⊤\n\r\r\r\r\r ≤ M2.\nThis implies that:\nλmin(An) = λmax\n \nδId + Diag\n \n1\nn\nn−1\nX\nk=0\nHθk(Xk+1)Hθk(Xk+1)⊤\n!!−1/2\n≥ (δ + M2)−1/2.\n• Lower bound for the smallest eigenvalue of An.\nλmax(An) = λmin\n \nδId + Diag\n \n1\nn\nn−1\nX\nk=0\nHθk(Xk+1)Hθk(Xk+1)⊤\n!!−1/2\n≤ δ−1/2.\nTherefore, by setting λn+1 = 1/\n√\nδ + M2 and βn+1 = 1/\n√\nδ, we have λ = 0 and β = 0. The proof is\ncompleted by applying Corollary 4.4 and then Corollary 4.3.\nRMSProp\n• Upper bound for the largest eigenvalue of An.\nBy assumption (5), we have:\n∥Vn∥ ≤ (1 − ρ)\nn\nX\nk=1\nρn−k ∥Hθk(Xk+1)∥2 ≤ M2(1 − ρ)\nn\nX\nk=1\nρn−k ≤ M2,\nwhere we used the fact that Pn\nk=1 ρn−k ≤ (1 − ρ)−1. This implies that:\nλmin(An) = λmax (δId + Diag (Vn))−1/2 ≥ (δ + M2)−1/2.\n• Lower bound for the smallest eigenvalue of An.\nλmax(An) = λmin (δId + Diag (Vn))−1/2 ≤ δ−1/2.\n17\nThe bias term bn is given by:\nbn =\n\n\n\n\n\nO\n\u0000n−2α\u0001\nif α < 1/4 ,\nO\n\u0000n−1/2\u0001\nif α > 1/4 ,\nO\n\u0000n−1/2 log n\n\u0001\nif α = 1/4 .\nB\nAdditional Experimental Details\nB.1\nExperiment with a Synthetic Time-Dependent Bias\nIn this setup, we consider a simple least squares objective function V (θ) = ∥Aθ∥2/2 in dimension d = 10,\nwhere A is a positive matrix ensuring convexity. We introduce zero-mean Gaussian noise with variance\nσ2 = 0.01 to every gradient and artificially include the bias term rn at each iteration. We explore different\nvalues of rn ∈ {1, n−1/8, n−1/4, n−1/2, n−1, 0}, where rn = 1 corresponds to constant bias, rn = 0 for an\nunbiased gradient, and the others exhibit decreasing bias.\n0\n2000\n4000\n6000\n8000\n10000\nEpochs\n10\n1\n100\n101\nV(\nn) 2\nrn = 1\nrn = n\n1/8\nrn = n\n1/4\nrn = n\n1/2\nrn = n\n1\nrn = 0\nFigure 7: Value of ∥∇V (θn)∥2 with Adagrad for different values of rn.\nIn Figure 7, we observe the convergence rate of the squared norm of the gradient. Similar to Figure 1, we\nnotice the impact of bias on the squared norm of the gradient. When α ≥ 1/4, we observe nearly the same\nconvergence rate as in the case of an unbiased gradient.\nB.2\nImportance Weighted Autoencoder (IWAE)\nIn this section, we elaborate on the IWAE procedure within our framework to illustrate its convergence rate.\nFirst, let’s recall some basics of IWAE. The IWAE objective function is defined as:\nLIWAE\nk\n(θ, ϕ; x) = Eq⊗k\nϕ (·|x)\n\"\nlog 1\nk\nk\nX\nℓ=1\npθ(x, Z(ℓ))\nqϕ(Z(ℓ)|x)\n#\n=: Eq⊗k\nϕ (·|x)\nh\n˜LIWAE\nk\n(θ, ϕ; x)\ni\n,\nwhere k corresponds to the number of samples drawn from the encoder’s approximate posterior distribution.\nThe gradient of the empirical estimate of the IWAE objective is given by:\n∇ϕ,θ ˜LIWAE\nk\n(θ, ϕ; x) =\nk\nX\nl=1\nw(l)\nPk\nm=1 w(m) ∇ϕ,θ log w(l),\nwhere w(l) = pθ(x,z(l))\nqϕ(z(l)|x) the unnormalized importance weights. This expression corresponds exactly to the\nbiased SNIS estimator of the gradient of the marginal log likelihood log pθ(x).\n18\nAlgorithm 1 Adaptive Stochastic Approximation for IWAE\nInput: Initial point θ0, maximum number of iterations n, step sizes {γk}k≥1 and a hyperparameter α ≥ 0\nto control the bias and MSE.\nfor k = 0 to n − 1 do\nCompute the stochastic update ∇θ,ϕLIWAE\nkα\n(θk, ϕk; Xk+1) using kα samples from the variational posterior\ndistribution and adaptive steps Ak.\nSet θk+1 = θk − γk+1Ak∇θLIWAE\nkα\n(θk, ϕk; Xk+1) and ϕk+1 = ϕk − γk+1Ak∇ϕLIWAE\nkα\n(θk, ϕk; Xk+1).\nend for\nOutput: (θk)1≤k≤n\nB.3\nBR-IWAE\nIn this section, we provide additional details on the Biased Reduced Importance Weighted Autoencoder\n(BR-IWAE). Let π be a probability measure on a measurable space (X, X). The objective is to estimate π(f) =\nEπ[f(X)] for a measurable function f : X → R such that π(|f|) < ∞. Assume that π(dx) ∝ w(x)λ(dx), where\nw is a positive weight function and λ is a proposal probability distribution, and that λ(w) =\nR\nw(x)λ(dx) < ∞.\nWe first introduce the algorithm i-SIR (iterated Sampling Importance Resampling), described in Algorithm 2,\nwhich was proposed by Tjelmeland (2004).\nAlgorithm 2 i-SIR algorithm\nInput: Maximum number of iterations n, number of particles N, a proposal probability distribution λ\nand a positive weight function w.\nInitialization: Draw Y0 from the proposal distribution λ.\nfor k = 0 to n − 1 do\nDraw Ik+1 ∈ {1, . . . , N} uniformly at random and set XIk+1\nk+1 = Yk.\nDraw X1:N\\{Ik+1}\nk+1\nindependently from the proposal distribution λ.\nCompute the normalized importance weights:\nωi\nk+1 =\nw\n\u0000Xi\nk+1\n\u0001\nPN\nℓ=1 w\n\u0000Xℓ\nk+1\n\u0001\n∀i ∈ {1, . . . , N}.\nSelect Yk+1 from the set X1:N\nk+1 by choosing Xi\nk+1 with probability ωi\nk+1.\nend for\nOutput:\n\u0000X1:N\nk\n\u0001\n1≤k≤n and\n\u0000ω1:N\nk\n\u0001\n1≤k≤n.\nThe BR-SNIS estimator of (Cardoso et al., 2022) aims at reducing the bias of self-normalized importance\nsampling estimators without increasing the variance. The proposed estimator of π(f) is given by:\nΠ(n0,n),N(f) =\n1\nn − n0\nn\nX\nℓ=n0+1\nN\nX\ni=1\nωi\nℓf\n\u0000Xi\nℓ\n\u0001\n,\nwhere n0 corresponds to a burn-in period. All the importance weights in Π(n0,n),N(f) are obtained similarly\nas in the i-SIR algorithm so that its computational complexity is easily controlled. In addition, by Cardoso\net al. (2022, Theorem 4) the bias of the BR-SNIS estimator decreases exponentially with n0.\nTherefore, we propose to use this approach in the context of IWAE. To estimate (7) for the update\nin BR-IWAE algorithm, we use the BR-SNIS estimator with: π : z 7→ pθ(x, z), λ : z 7→ qϕ(z | x), f : z 7→\n∇θ log pθ(x, z) and w : z 7→ pθ(x, z)/qϕ(z|x).\nB.4\nAdditional Experiments in IWAE\nIn this section, we provide detailed information about the experiments on CIFAR-10. We also conduct\nadditional experiments on the FashionMNIST and CIFAR-100 datasets. For all experiments, we use Adagrad\n19\nand RMSProp with a learning rate decay given by γn = Cγ/√n, where Cγ = 0.01 for Adagrad and Cγ = 0.001\nfor RMSProp.\nDatasets.\nWe conduct our experiments on three datasets: FashionMNIST (Xiao et al., 2017), CIFAR-10\nand CIFAR-100. The FashionMNIST dataset is a variant of MNIST and consists of 28x28 pixel images of\nvarious fashion items, with 60,000 images in the training set and 10,000 images in the test set. CIFAR-100 is\na dataset similar to CIFAR-10 but with 100 different classes (instead of 10).\nModels.\nFor FashionMNIST, we use a fully connected neural network with a single hidden layer\nconsisting of 400 hidden units and ReLU activation functions for both the encoder and the decoder. The\nlatent space dimension is set to 20. We use 256 images per iteration (235 iterations per epoch). For CIFAR-10\nand CIFAR-100, we use a Convolutional Neural Network (CNN) architecture with 3 Convolutional layers and\n2 fully connected layers with ReLU activation functions. The latent space dimension is set to 100. For both\ndatasets, we use 256 images per iteration (196 iterations per epoch). We do not employ any acceleration\nmethods, such as adding momentum during training.\nWe estimate the log-likelihood using the VAE, IWAE, and BR-IWAE models, all of which are trained\nfor 100 epochs. Training is conducted using the SGD, Adagrad, and RMSProp algorithms with a decaying\nlearning rate, as mentioned before. For SGD, we employ the clipping method to clip the gradients at a\nspecified threshold, which is fixed at 10000, in order to prevent excessively large steps.\n0\n20\n40\n60\n80\n100\nEpochs\n2.4\n2.45\n2.5\n2.55\n2.6\n2.65\n2.7\n2.75\nTest loss (x10e2)\nVAE\nIWAE\nBR-IWAE\n0\n20\n40\n60\n80\n100\nEpochs\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3\n3.1\nTest loss (x10e2)\nVAE\nIWAE\nBR-IWAE\nFigure 8: Negative Log-Likelihood for Different Generative Models with Adagrad (on the left) and RMSProp\n(on the right) on the FashionMNIST Dataset. Bold lines represent the mean over 5 independent runs.\nFor this experiment, we set k = 5 samples in both IWAE and BR-IWAE, while restricting the maximum\niteration of the MCMC algorithm to 5 and the burn-in period to 2 for BR-IWAE. The test losses for Adagrad\nand RMSProp are illustrated in 8. More precisely, the figure shows the negative log-likelihood on the training\nand test datasets for VAE, IWAE, and BR-IWAE with Adagrad and RMSProp. Similar to the case of\nCIFAR-10, we observe that IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias\nin both cases. For comparison, we estimate the Negative Log-Likelihood using these three models with SGD,\nAdagrad and RMSProp, and the results are presented in Table 1.\nTable 1: Comparison of Negative Log-Likelihood on the FashionMNIST Test Set (Lower is Better).\nAlgorithm\nVAE\nIWAE\nBR-IWAE\nSGD\n247.2\n244.9\n244.0\nAdagrad\n245.8\n241.4\n240.5\nRMSProp\n242.6\n239.3\n237.8\nSimilarly, as we did in the case of CIFAR-10, we incorporate a time-dependent bias that decreases\nby choosing a bias of order O(n−α) at iteration n. We vary the value of α for both FashionMNIST and\nCIFAR-100.\n20\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n10\n0\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\n0\n20\n40\n60\n80\n100\nEpochs\n2.4\n2.45\n2.5\n2.55\n2.6\n2.65\n2.7\n2.75\n2.8\nTest loss (x10e2)\n= 1/8\n= 1/4\n= 1/2\nFigure 9: IWAE on the FashionMNIST Dataset with Adagrad for different values of α. Bold lines represent\nthe mean over 5 independent runs.\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n100\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\n0\n20\n40\n60\n80\n100\nEpochs\n2.4\n2.5\n2.6\n2.7\n2.8\n2.9\n3\nTest loss (x10e2)\n= 1/8\n= 1/4\n= 1/2\nFigure 10: IWAE on the FashionMNIST Dataset with RMSProp for different values of α. Bold lines represent\nthe mean over 5 independent runs.\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n10\n2\n10\n3\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\n0\n20\n40\n60\n80\n100\nEpochs\n1.775\n1.8\n1.825\n1.85\n1.875\n1.9\n1.925\n1.95\nTest loss (x10e3)\n= 1/8\n= 1/4\n= 1/2\nFigure 11: IWAE on the CIFAR-100 Dataset with Adagrad for different values of α. Bold lines represent the\nmean over 5 independent runs.\n21\n0\n20\n40\n60\n80\n100\nEpochs\n101\n102\n103\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\n0\n20\n40\n60\n80\n100\nEpochs\n1.75\n1.8\n1.85\n1.9\n1.95\n2\nTest loss (x10e3)\n= 1/8\n= 1/4\n= 1/2\nFigure 12: IWAE on the CIFAR-100 Dataset with RMSProp for different values of α. Bold lines represent\nthe mean over 5 independent runs.\nAll figures are plotted on a logarithmic scale for better visualization and with respect to the number\nof epochs. The dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8, and\nO(log n/√n) for α = 1/4, as well as for α = 1/2, just as in the case of CIFAR-10. We can clearly observe\nthat for all cases, convergence is achieved when n is sufficiently large. In the case of the FashionMNIST\ndataset, the bound seems tight, and the convergence rate of O(n−1/2) does not seem to be possible to reach,\nin contrast to the case of CIFAR-100 where the curves corresponding to α = 1/4 and α = 1/2 approach the\nO(n−1/2) convergence rate. For all figures, with a larger α, the convergence in both the squared gradient\nnorm and negative log-likelihood occurs more rapidly.\nThe effect of Cγ.\nFigure 13 illustrates the convergence in both the squared gradient norm and the negative log-likelihood\nfor Cγ = 0.001 and Cγ = 0.01 in Adagrad. In the case of the squared gradient norm, we have only plotted\nthe results for Cγ = 0.001 for better visualization, and the plot for Cγ = 0.01 was already presented in Figure\n3. It is clear that when Cγ is set to 0.001, the convergence of the negative log-likelihood is slower. Similarly,\nthe convergence in the squared gradient norm for Cγ = 0.001 achieves convergence, but it is slower compared\nto the case of Cγ = 0.01.\n0\n20\n40\n60\n80\n100\nEpochs\n10\n1\n10\n2\n10\n3\nV(\nn) 2\nn\n1/4\nlog(n)/ n\n1/ n\n= 1/8\n= 1/4\n= 1/2\n0\n20\n40\n60\n80\n100\nEpochs\n1.825\n1.85\n1.875\n1.9\n1.925\n1.95\n1.975\n2\nTest loss (x10e3)\n= 1/8, C = 0.001\n= 1/4, C = 0.001\n= 1/2, C = 0.001\n= 1/8, C = 0.01\n= 1/4, C = 0.01\n= 1/2, C = 0.01\nFigure 13: IWAE on the CIFAR-10 Dataset with Adagrad for different values of α and Cγ. Bold lines\nrepresent the mean over 5 independent runs.\nThe Impact of Bias over Time.\nOur experiments illustrate the negative log-likelihood with respect to epochs, and we observed that\na higher value of α leads to faster convergence. The key point to consider when tuning α is that while\nconvergence may be faster in terms of iterations, it may lead to higher computational costs. To illustrate\n22\nthis, we set a fixed time limit of 1000 seconds and tested different values of α, plotting the test loss as a\nfunction of time in Figure 14. It is clear that with α = 1/8, the convergence is always slower, whereas\nchoosing α = 1/4 achieves faster convergence than α = 1/2. While the difference may seem small here, with\nmore complex models, the disparity becomes significant. Therefore, it is essential to tune the value of α to\nattain fast convergence and reduce computational time.\n0\n200\n400\n600\n800\n1000\nTime (s)\n1.825\n1.85\n1.875\n1.9\n1.925\n1.95\n1.975\n2\nTest loss (x10e3)\n= 1/8\n= 1/4\n= 1/2\nFigure 14: Negative Log-Likelihood on the test set of the CIFAR-10 Dataset for IWAE with RMSProp for\nDifferent Values of α over time (in seconds). Bold lines represent the mean over 5 independent runs.\nIn this paper, all simulations were conducted using the Nvidia Tesla T4 GPU. The total computing hours\nrequired for the results presented in this paper are estimated to be around 50 to 100 hours of GPU usage.\nC\nSome Examples of Biased Gradients\nIn this section, we explore examples of applications using biased gradient estimators while having control\nover the bias.\nC.1\nSelf-Normalized Importance Sampling\nLet π be a probability measure on a measurable space (X, X). The objective is to estimate π(f) = Eπ[f(X)]\nfor a measurable function f : X → Rd such that π(|f|) < ∞. Assume that π(dx) ∝ w(x)λ(dx), where w is a\npositive weight function and λ is a proposal probability distribution, and that λ(w) =\nR\nw(x)λ(dx) < ∞.\nFor a function f : X → Rd such that π(|f|) < ∞, the identity\nπ(f) = λ(ωf)\nλ(ω) ,\n(8)\nleads to the Self-Normalized Importance Sampling (SNIS) estimator:\nΠNf\n\u0000X1:N\u0001\n=\nN\nX\ni=1\nωi\nNf\n\u0000Xi\u0001\n,\nωi\nN =\nw\n\u0000Xi\u0001\nPN\nℓ=1 w (Xℓ)\n,\nwhere X1:N =\n\u0000X1, . . . , XN\u0001\nare independent draws from λ and the ωi\nN are called the normalized weights.\nAgapiou et al. (2017) shows that the bias of SNIS estimator can be expressed as:\n\r\rE\n\u0002\nΠNf\n\u0000X1:N\u0001\n− π(f)\n\u0003\r\r ≤ 12\nN\nλ\n\u0000ω2\u0001\nλ(ω)2 .\nThis particular type of estimator can be found in the Importance Weighted Autoencoder (IWAE) framework\n(Burda et al., 2015), as illustrated in B. The estimator of the gradient of ELBO in IWAE corresponds to\nthe biased SNIS estimator of the gradient of the marginal log likelihood log pθ(x). Consequently, based on\nthese results, both the bias and Mean Squared Error (MSE) are of order O(1/k), where k corresponds to the\nnumber of samples drawn from the variational posterior distribution.\n23\nC.2\nSequential Monte Carlo Methods\nWe focus here in the task of estimating the parameters, denoted as θ, in Hidden Markov Models. In this\ncontext, the hidden Markov chain is denoted by (Xt)t≥0. The distribution of X0 has density χ with respect\nto the Lebesgue measure µ and for all t ≥ 0, the conditional distribution of Xt+1 given X0:t has density\nmθ(Xt, ·). It is assumed that this state is partially observed through an observation process (Yt)0≤t≤T . The\nobservations Y0:t are assumed to be independent conditionally on X0:t and, for all 0 ≤ t ≤ T, the distribution\nof Yt given X0:t depends on Xt only and has density gθ(Xt, ·) with respect to the Lebesgue measure. The\njoint distribution of hidden states and observations is given by\npθ(x0:T , y0:T ) = χ(x0)gθ(x0, y0)\nT−1\nY\nt=0\nmθ(xt, xt+1)gθ(xt+1, yt+1).\nOur objective is to maximize the likelihood of the model:\npθ(y0:T ) =\nZ\npθ(x0:T , y0:T ) dx0:T .\nTo use a gradient-based method for this maximization problem, we need to compute the gradient of the\nobjective function. Under simple technical assumptions, by Fisher’s identity,\n∇θ log pθ(y0:T ) =\nZ\n∇θ log pθ(x0:T , y0:T )pθ(x0:T |y0:T )dx0:T\n= Ex0:T ∼pθ(.|y0:T ) [∇θ log pθ(x0:T , y0:T )]\n= Ex0:T ∼pθ(.|y0:T )\n\"T−1\nX\nt=0\nst,θ (xt, xt+1)\n#\n,\nwhere st,θ (x, x′) = ∇θ log{mθ (x, x′) gθ (x, yt+1)} for t > 0 and by convention s0,θ (x, x′) = ∇θ log gθ (x, y0).\nGiven that the gradient of the log-likelihood represents the smoothed expectation of an additive functional,\none may opt for Online Smoothing algorithms to mitigate computational costs. The estimation of the\ngradient ∇θ log pθ(y0:T ) is given by:\nHθ (y0:T ) =\nN\nX\ni=1\nωi\nT\nΩT\nτ i\nT,θ,\nwhere {τ i\nT,θ}N\ni=1 are particle approximations obtained using particles {\n\u0000ξi\nT , ωi\nT\n\u0001\n}N\ni=1 targeting the filtering\ndistribution ϕT , i.e. the conditional distribution of xT given y0:T . In the Forward-only implementation of\nFFBSm (Del Moral et al., 2010), the particle approximations {τ i\nT,θ}N\ni=1 are computed using the following\nformula, with an initialization of τ i\n0 = 0 for all i ∈ J1, NK:\nτ i\nt+1,θ =\nN\nX\nj=1\nωj\nt mθ(ξj\nt , ξi\nt+1)\nP\nℓ=1 ωℓ\ntmθ(ξℓ\nt, ξi\nt+1)\nn\nτ j\nt,θ + st,θ(ξj\nt , ξi\nt+1)\no\n,\nt ∈ N.\nThe estimator of the gradient Hθ (y0:T ) computed by the Forward-only implementation of FFBSm is\nbiased. The bias and MSE of this estimator are of order O (1/N) (Del Moral et al., 2010), where N\ncorresponds to the number of particles used to estimate it. Using alternative recursion methods to compute\n{τ i\nT,θ}N\ni=1 results in different algorithms, such as the particle-based rapid incremental smoother (PARIS)\n(Olsson and Westerborn, 2017) and its pseudo-marginal extension Gloaguen et al. (2022) and Parisian particle\nGibbs (PPG) (Cardoso et al., 2023). In such cases, one can also control the bias and MSE of the estimator.\nC.3\nPolicy Gradient for Average Reward over Infinite Horizon\nConsider a finite Markov Decision Process (MDP) denoted as (S, A, R, P), where S represents the state\nspace, A denotes the action space, R : S × A → [0, Rmax] is a reward function, and P is the transition model.\n24\nThe agent’s decision-making process is characterized by a parametric family of policies {πθ}θ∈Rd, employing\nthe soft-max parameterization. The reward function is given by:\nV (θ) := E(S,A)∼vθ [R(S, A)] =\nX\n(s,a)∈S×A\nvθ(s, a)R(s, a),\nwhere vθ represents the unique stationary distribution of the state-action Markov Chain sequence {(St, At)}t≥1\ngenerated by the policy πθ. Let λ ∈ (0, 1) be a discount factor and T be sufficiently large, the estimator of\nthe gradient of the objective function V is given by:\nHθ (S1:T , A1:T ) = R (ST , AT )\nT−1\nX\ni=0\nλi∇ log πθ (AT−i; ST−i) ,\nwhere (S1:T , A1:T ) := (S1, A1, . . . , ST , AT ) is a realization of state-action sequence generated by the policy\nπθ. It’s important to note that this gradient estimator is biased, and the bias is of order O(1 − λ) (Karimi\net al., 2019).\nC.4\nZeroth-Order Gradient\nConsider the problem of minimizing the objective function V . The zeroth-order gradient method is partic-\nularly valuable in scenarios where direct access to the gradient of the objective function is challenging or\ncomputationally expensive. The zeroth-order gradient oracle obtained by Gaussian smoothing (Nesterov and\nSpokoiny, 2017) is given by:\nHθ (X) = V (θ + τX) − V (θ)\nτ\nX,\n(9)\nwhere τ > 0 is a smoothing parameter and X ∼ N(0, Id) a random Gaussian vector. Nesterov and\nSpokoiny (2017, Lemma 3) provide the bias of this estimator:\n∥E [Hθ (X)] − ∇V (θ) ∥ ≤ τ\n2L(d + 3)3/2.\n(10)\nThe application of these zeroth-order gradient methods can be found in generative adversarial networks\n(Moosavi-Dezfooli et al., 2017; Chen et al., 2017).\n25\n"
}