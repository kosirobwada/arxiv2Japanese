{
    "optim": "Non-asymptotic Analysis of Biased Adaptive Stochastic Approximation Sobihan Surendran∗†, Adeline Fermanian†, Antoine Godichon-Baggioni∗, and Sylvain Le Corff∗ Abstract Stochastic Gradient Descent (SGD) with adaptive steps is now widely used for training deep neural networks. Most theoretical results assume access to unbiased gradient estimators, which is not the case in several recent deep learning and reinforcement learning applications that use Monte Carlo methods. This paper provides a comprehensive non-asymptotic analysis of SGD with biased gradients and adaptive steps for convex and non-convex smooth functions. Our study incorporates time-dependent bias and emphasizes the importance of controlling the bias and Mean Squared Error (MSE) of the gradient estimator. In particular, we establish that Adagrad and RMSProp with biased gradients converge to critical points for smooth non-convex functions at a rate similar to existing results in the literature for the unbiased case. Finally, we provide experimental results using Variational Autoenconders (VAE) that illustrate our convergence results and show how the effect of bias can be reduced by appropriate hyperparameter tuning. Keywords: Stochastic Optimization, Biased Stochastic Approximation, Monte Carlo Methods, Variational Autoenconders 1 Introduction Stochastic Gradient Descent (SGD) algorithms are standard methods to train statistical models based on deep architectures. Consider a general optimization problem: θ∗ ∈ argmin θ∈Rd V (θ) , (1) where V is the objective function. Then, gradient descent methods produce a sequence of parameter estimates as follows: θ0 ∈ Rd and for all n ≥ 1, θn+1 = θn − γn+1∇V (θn) , where ∇V denotes the gradient of V and for all n ≥ 1, γn > 0 is the learning rate. In many cases, it is not possible to compute the exact gradient of the objective function, hence the introduction of vanilla Stochastic Gradient Descent, defined for all n ≥ 1 by: θn+1 = θn − γn+1d ∇V (θn) , where d ∇V (θn) is an estimator of ∇V (θn). In deep learning, stochasticity emerges with the use of mini-batches, since it is not feasible to compute gradients based on the entire dataset. While these algorithms have been extensively studied, both theoretically and practically (see, e.g., Bottou et al., 2018), many questions remain open. In particular, most results are based on the case where the estimator d ∇V is unbiased. Although this assumption is valid in the case of vanilla SGD, it breaks down in many common applications. For example, zeroth-order methods used to optimize black-box functions (Nesterov and Spokoiny, 2017) in generative adversarial networks (Moosavi-Dezfooli et al., 2017; Chen et al., 2017) have access only to noisy biased realizations of the objective functions. Furthermore, in reinforcement learning algorithms such as Q-learning (Jaakkola et al., 1993), policy gradient (Baxter and Bartlett, 2001), and temporal difference learning (Bhandari et al., 2018; Lakshmi- narayanan and Szepesvari, 2018; Dalal et al., 2018), gradient estimators are often obtained using a Markov ∗Laboratoire de Probabilit´es, Statistique et Mod´elisation (LPSM), Sorbonne Universit´e, 75005 Paris, France. †LOPF, Califrais’ Machine Learning Lab, Paris, France 1 arXiv:2402.02857v1  [stat.ML]  5 Feb 2024 chain with state-dependent transition probability. These estimators are then biased (Sun et al., 2018; Doan et al., 2020). Other examples of biased gradients can be found in the field of generative modeling with Markov Chain Monte Carlo (MCMC) and Sequential Monte Carlo (SMC) (Gloaguen et al., 2022; Cardoso et al., 2023). In particular, the Importance Weighted Autoencoder (IWAE) proposed by Burda et al. (2015), which is a variant of the standard Variational Autoencoder (VAE) (Kingma and Welling, 2013), yields biased estimators. In practical applications, vanilla SGD exhibits difficulties to calibrate the step sequences. Therefore, modern variants of SGD employ adaptive steps that use past stochastic gradients or Hessians to avoid saddle points and deal with ill-conditioned problems. The idea of adaptive steps was first proposed in the online learning literature by Auer et al. (2002) and later adopted in stochastic optimization, with the Adagrad algorithm of Duchi et al. (2011). Adagrad aims to normalize the gradient by introducing information about the square root of the inverse of the covariance of the gradient. We give non-asymptotic convergence guarantees for modern variants of SGD where both the estimators are biased and the steps are adaptive. To our knowledge, existing results consider either adaptive steps but unbiased estimators or biased estimators and non-adaptive steps. Indeed, many standard analyses for SGD (Moulines and Bach, 2011) and SGD with adaptive steps (Duchi et al., 2011) require unbiased gradients to obtain convergence results. More recently, convergence results have been obtained for SGD with biased gradients (Tadi´c and Doucet, 2011; Karimi et al., 2019; Ajalloeian and Stich, 2020) but non-adaptive steps. More precisely, we present convergence guarantees for SGD with biased gradients and adaptive steps, under weak assumptions on the bias and Mean Squared Error (MSE) of the estimator. In many scenarios, it is indeed possible to control these quantities, as recently shown for example for Importance Sampling and Sequential Monte Carlo methods (Agapiou et al., 2017; Cardoso et al., 2022, 2023). In particular, we establish that Adagrad and RMSProp with a biased gradient converge to a critical point for non-convex smooth functions with a convergence rate of O(log n/√n + bn), where bn is related to the bias at iteration n. However, for convex functions, we achieve an improved convergence rate of O(1/√n + bn). Our theoretical results provide us with hyperparameter tuning procedures to effectively eliminate the bias term, resulting in improved convergence rates of O(log n/√n) and O(1/√n) respectively. Organization of the paper. In Section 2, we introduce the setting of the paper and relevant related works. In Section 3, we present the Adaptive Stochastic Approximation framework and the main assumptions for theoretical results. In Section 4, we propose convergence rates for the risk in the convex case and the squared norm of gradients for non-convex smooth functions in the context of Biased Adaptive Stochastic Approximation. Finally, we extend the analysis to Adagrad and RMSProp with biased gradients. We illustrate our results using VAE in Section 5. All proofs are postponed to the appendix. 2 Setting and Related Works Stochastic Approximation. Stochastic Approximation (SA) methods go far beyond SGD. They consist of sequential algorithms designed to find the zeros of a function when only noisy observations are available. Indeed, Robbins and Monro (1951) introduced the Stochastic Approximation algorithm as an iterative recursive algorithm to solve the following integration equation: h(θ) = Eπ [Hθ(X)] = Z X Hθ(x)π(x)dx = 0 , (2) where h is the mean field function, X is a random variable taking values in a measurable space (X, X), and Eπ is the expectation under the distribution π. In this context, Hθ can be any arbitrary function. If Hθ(X) is an unbiased estimator of the gradient of the objective function, then h(θ) = ∇V (θ). As a result, the minimization problem (1) is then equivalent to solving problem (2), and we can note that SGD is a specific instance of Stochastic Approximation. Stochastic Approximation methods are then defined as follows: θn+1 = θn − γn+1Hθn (Xn+1) , where the term Hθn (Xn+1) is the n-th stochastic update, also known as the drift term, and is a potentially biased estimator of ∇V (θn). It depends on a random variable Xn+1 which takes its values in (X, X). In 2 machine learning, V typically represents theoretical risk, θ denotes model parameters, and Xn+1 stands for the data. The gradient estimator generally corresponds to the gradient of the empirical risk when data is assumed to be independent and identically distributed. In a reinforcement learning setting, an agent interacts with an environment to learn an optimal policy based on some parameters θ, taking actions, receiving rewards, and updating its policy according to observed outcomes. Here, V represents the reward function, θ denotes policy parameters, and Xn+1 represents state-action sequences. Adaptive Stochastic Gradient Descent. SGD can be traced back to Robbins and Monro (1951), and its averaged counterpart was proposed by Polyak and Juditsky (1992). The convergence rates of algorithms for convex optimization problems rely primarily on the strong convexity of the objective function, as established by Nemirovskij and Yudin (1983). The non-asymptotic results of SGD in the context of non-convex smooth functions can be found in Moulines and Bach (2011). Ghadimi and Lan (2013) prove the convergence of a random iterate of SGD for nonconvex smooth functions, which was already suggested by the results of Bottou (1991). They show that SGD with constant or decreasing stepsize γk = 1/ √ k converges to a stationary point of a non-convex smooth function V at a rate of O(1/√n) where n is the number of iterations. Most adaptive first-order methods, such as Adam (Kingma and Ba, 2014), Adadelta (Zeiler, 2012), RMSProp (Tieleman et al., 2012), and NADA (Dozat, 2016), are based on the blueprint provided by the Adagrad family of algorithms. The first known work on adaptive steps for non-convex stochastic optimization, in the asymptotic case, was presented by Kresoja et al. (2017). Ward et al. (2020) proved that Adagrad converges to a critical point for non-convex objectives at a rate of O(log n/√n) when using a scalar adaptive step. In addition, Zou et al. (2018) extended this proof to multidimensional settings. More recently, D´efossez et al. (2020) focused on the convergence rates for Adagrad and Adam. Biased Stochastic Approximation. The asymptotic results of Biased Stochastic Approximation have been studied by Tadi´c and Doucet (2011). The non-asymptotic analysis of Biased Stochastic Approximation can be found in the reinforcement learning literature, especially in the context of temporal difference (TD) learning, as explored by Bhandari et al. (2018); Lakshminarayanan and Szepesvari (2018); Dalal et al. (2018). The case of non-convex smooth functions has been studied by Karimi et al. (2019). The authors establish convergence results for the mean field function at a rate of O(log n/√n + b), where b corresponds to the bias and n to the number of iterations. For strongly convex functions, the convergence of SGD with biased gradients can be found in Ajalloeian and Stich (2020), who consider a constant step size. This analysis applies specifically to the case of Martingale noise. Biased Stochastic Approximation with Markov noise has also been investigated by Li and Wai (2022). The authors focused on performative prediction problems, where the objective function is the expected value of a loss. They assumed the smoothness of V , along with specific assumptions regarding the existence of a bounded solution of the Poisson equation and additional Lipschitz properties. Our analysis provides non-asymptotic results in a more general setting, for a wide variety of objective functions and treating both the Martingale and Markov chain cases. 3 Adaptive Stochastic Approximation 3.1 Framework Consider the optimization problem (1) where the objective function V is assumed to be differentiable. In this paper, we focus on the following Stochastic Approximation (SA) algorithm with adaptive steps: θ0 ∈ Rd and for all n ≥ 0, θn+1 = θn − γn+1AnHθn (Xn+1) , (3) where γn+1 > 0 and An is a sequence of symmetric and positive definite matrices. Let (Fn)n≥0 be the filtration generated by the random variables (θ0, {Xk}k≤n) and assume that for all n ≥ 0, An is Fn-measurable. In a context of biased gradient estimates, choosing An = \u0014 δId + \u0010 1 n n−1 X k=0 Hθk (Xk+1) Hθk (Xk+1)⊤ \u0011\u0015−1/2 can be assimilated to the full Adagrad algorithm (Duchi et al., 2011). However, computing the square root of the inverse becomes expensive in high dimensions, so in practice, Adagrad is often used with diagonal 3 matrices. This approach has been shown to be particularly effective in sparse optimization settings. Denoting by Diag(A) the matrix formed with the diagonal terms of A and setting all other terms to 0, Adagrad with diagonal matrices is defined in our context as: An = \u0002 δId + Diag( ¯Hn(X0:n, θ0:n−1)) \u0003−1/2 , (4) where ¯Hn(X0:n, θ0:n−1) = 1 n n−1 X k=0 Hθk(Xk+1)Hθk(Xk+1)⊤ . In RMSProp (Tieleman et al., 2012), ¯Hn(X0:n, θ0:n−1) in (4) is an exponential moving average of the past squared gradients. It is defined by: (1 − ρ) n−1 X k=0 ρn−kHθk(Xk+1)Hθk(Xk+1)⊤, where ρ is the moving average parameter. Furthermore, when An is a recursive estimate of the inverse Hessian, it corresponds to the Stochastic Newton algorithm (Boyer and Godichon-Baggioni, 2023). 3.2 Assumptions We state below the main assumptions necessary to our theoretical results. Assumption 1. The objective function V is convex and there exists a constant µ > 0 such that: 2µ (V (θ) − V (θ∗)) ≤ ∥∇V (θ)∥2, ∀θ ∈ Rd. The second condition of Assumption 1 corresponds to the Polyak- Lojasiewicz condition. Furthermore, Assumption 1 is a weaker assumption compared to the function being strongly convex. In machine learning, this assumption holds true for linear regression, logistic regression and Huber loss when the Hessian at the minimizer is positive. If this condition is not met, we can add a regularization term to the loss in the form of µ∥θ∥2/2, where µ represents the regularization parameter. Assumption 2. The objective function V is L-smooth. For all (θ, θ′) ∈ \u0000Rd\u00012, \r\r∇V (θ) − ∇V \u0000θ′\u0001\r\r ≤ L \r\rθ − θ′\r\r . This assumption is crucial to obtain our convergence rate and is very common (see, e.g., Moulines and Bach, 2011; Bottou et al., 2018). Under this assumption, for all (θ, θ′) ∈ \u0000Rd\u00012, V (θ) ≤ V \u0000θ′\u0001 +  ∇V \u0000θ′\u0001 , θ − θ′\u000b + L 2 \r\rθ − θ′\r\r2 . Assumption 3. For all n ∈ N, there exists rn ≥ 0 and σ2 n ≥ 0 such that: (i) Bias: \r\rE[Hθn(Xn+1) | Fn] − ∇V (θn) \r\r ≤ rn. (ii) Mean Squared Error (MSE): E h ∥Hθn (Xn+1) − ∇V (θn)∥2 | Fn i ≤ σ2 n. In this assumption, the sequences rn and σ2 n control the bias and MSE without any specific assumption regarding their dependence on n, making our setting very general. This assumption can be verified, for example, when the gradient estimator follows the form of Self-Normalized Importance Sampling (SNIS) (Agapiou et al., 2017) or in various Monte Carlo applications as discussed in Appendix C. Furthermore, the first point of Assumption 3 is satisfied by a wide range of discrete-time stochastic processes, including i.i.d. random sequences and Markov chains, with some additional assumptions. In the case where {Xn, n ∈ N} 4 is an i.i.d. sequence, this can be easily verified by considering rn as the difference between the gradient and the mean field function defined in (2). If h(θn) = ∇V (θn), i.e., for SGD with unbiased gradient estimators, the second point is equivalent to ensuring that the variance of the noise term is bounded. In this context, this assumption is standard, see for example Moulines and Bach (2011); Ghadimi and Lan (2013). In our particular case, however, it accounts for the time-dependent variance of the noise term. We consider also an additional assumption on An. Let ∥A∥ be the spectral norm of a matrix A. Assumption 4. For all n ∈ N, there exists βn, λn > 0 such that: ∥An∥ = λmax(An) ≤ βn+1 and λmin (An) ≥ λn+1. In our setting, since An is assumed to be a symmetric matrix, the spectral norm is equal to the largest eigenvalue. Assumption 4 plays a crucial role, as the estimates may diverge when this assumption is not satisfied. Given a sequence {βn}n∈N, one way to ensure that Assumption 4 is satisfied is to replace the random matrices An with ˜An = min{∥An∥, βn+1} ∥An∥ An. (5) It is then clear that ∥ ˜An∥ ≤ βn+1. Furthermore, in most cases, especially for Adagrad and Stochastic Newton algorithm, control of λmin (An) in Assumption 4 is satisfied, as discussed by Godichon-Baggioni and Tarrago (2023, Section 3.2.1). 4 Convergence Results 4.1 Convex case In this section, we study the convergence rate of SGD with biased gradients and adaptive steps in the convex case. We give below a simplified version of the bound we obtained on the risk and refer to Theorem A.2 in the appendix for a formal statement with explicit constants. Theorem 4.1. Let θn ∈ Rd be the n-th iterate of the recursion (3) and γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ with Cγ > 0, Cβ > 0, and Cλ > 0. Assume that γ, β, λ ≥ 0 and γ + λ < 1. Then, under Assumptions 1 − 4, we have: E [V (θn) − V (θ∗)]= O \u0010 n−γ+2β+λ + bn \u0011 , (6) where the bias term bn can be constant or decreasing. In the latter case, writing rn = n−α, we have: bn = O \u0010 n−2α+2β+2λ\u0011 . The rate obtained is classical and shows the tradeoff between a term coming from the adaptive steps (with a dependence on γ, β, λ) and a term bn which depends on the control of the bias rn. To minimize the right hand-side of (6), we would like to have β = λ = 0. However, this would require much stronger assumptions. For example, in the case of Adagrad and RMSProp, the gradients would need to be bounded, which will be discussed later. We stress that Theorem 4.1 applies to any adaptive algorithm of the form (3), with the only assumption being Assumption 4 on the eigenvalues of An. Without any information on these eigenvalues, the choice that βn ∝ nβ and λn ∝ n−λ allows us to remain very general. An interesting example where we can control the bias term rn is the case of Monte Carlo methods. In this scenario, the bias depends on a number N of samples used to estimate the gradient and is usually of order 1/N, see for instance Agapiou et al. (2017); Del Moral et al. (2010); Cardoso et al. (2023). It can therefore be controlled by choosing a different number of samples Nn at each iteration n ≥ 1. To obtain a bias of order O(n−α) at iteration n, we simply select Nn = nα. This idea will be further developed in Section 5. To illustrate Theorem 4.1 and the impact of bias, we consider in Figure 1 a simple least squares objective function V (θ) = ∥Aθ∥2/2 in dimension d = 10. We artificially add to every gradient a zero-mean Gaussian noise and a bias term rn = n−α at each iteration n. We use Adagrad with a learning rate γ = 1/2, β = 0 and λ = 0. Then, the bound of Theorem 4.1 is of the form O(n−1/2 + n−2α). First, note that the impact of 5 a constant bias term (rn = 1) never vanishes. From rn = 1 to rn = n−1/4, the effect of the bias decreases until a threshold is reached where there is no significant improvement. The convergence rate in the case rn = n−1/4 is then the same as in the case without bias, illustrating the fact that in this case the dominating term comes from the learning rate. 0 2000 4000 6000 8000 10000 Epochs 10 1 100 V( n) V( *) n 1/2 n 1/4 rn = 1 rn = n 1/8 rn = n 1/4 rn = n 1/2 rn = n 1 rn = 0 Figure 1: Value of V (θn)−V (θ∗) with Adagrad for different values of rn = n−α and a learning rate γn = n−1/2. The dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8 and O(n−1/2) for α ≥ 1/4. Finally, note that non-adaptive SGD is a particular case of Theorem 4.1. Thus, our theorem gives new results also in the non-adaptive case with generic step sizes and biased gradients, while the only existing results in the literature assume constant step sizes (Ajalloeian and Stich, 2020). 4.2 Non-convex smooth case In the non-convex smooth case, the theoretical results are based on a randomized version of Stochastic Approximation as described by Nemirovski et al. (2009); Ghadimi and Lan (2013); Karimi et al. (2019). In classical SA, the update (3) is performed a fixed number of times n, and the quantity of interest is the last parameter θn. On the other hand, in Randomized Stochastic Approximation, we introduce a random variable R which takes its values in {1, . . . , n} and the quantity of interest is θR. We stress that this procedure is a technical tool for the proofs, in practical applications we will always use classical SA. The following theorem provides a bound in expectation on the gradient of the objective function V , which is the best results we can have given that no assumption is made about existence of a global minimum of V . Theorem 4.2. Assume that for all k ≥ 0, we have γk+1 ≤ λk+1/(6Lβ2 k+1). For any n ≥ 1, let R ∈ {0, . . . , n} be a discrete random variable, independent of {Fn, n ∈ N}, such that: P(R = k) := γk+1λk+1 Pn j=0 γj+1λj+1 . Then, under Assumptions 2 − 4, we have: E h ∥∇V (θR)∥2i ≤ 3V ∗ + α1,n/2 + Lα2,n Pn j=0 γj+1λj+1 , where α1,n = n X k=0 γk+1β2 k+1r2 k/λk+1, α2,n = n X k=0 γ2 k+1β2 k+1σ2 k, and V ∗ = E[V (θ0) − V (θ∗)]. 6 Theorem 4.2 recovers the asymptotic rate of convergence obtained by Karimi et al. (2019) but with explicit bounds with respect to the hyper-parameters γ, β, λ and the bias. We can observe that if γ ≤ λ + 2β, the condition on (γk)k≥1 can be met simply by tuning Cγ. In particular, if An = Id, the requirement on the step sizes can be expressed as γk+1 ≤ 1/(6L). We give below the convergence rates obtained from Theorem 4.2 under the same assumptions on γn, βn and λn as in the convex case. Corollary 4.3. Let γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ with Cγ > 0, Cβ > 0, and Cλ > 0. Assume that γ, β, λ ≥ 0 and γ + λ < 1. Then, under Assumptions 2 − 4, we have: E h ∥∇V (θR)∥2i =      O \u0000n−γ+λ+2β + bn \u0001 if ϑ < 1/2 , O \u0000nγ+λ−1 + bn \u0001 if ϑ > 1/2 , O \u0000nγ+λ−1log n + bn \u0001 if ϑ = 1/2 , with ϑ = γ −β and where the bias term bn can be constant or decreasing. In the latter case, writing rn = n−α, we have: bn =      O \u0000nβ−2α\u0001 if 2α < 1 − γ + λ + 2β , O \u0000nγ+λ−1\u0001 if 2α > 1 − γ + λ + 2β , O \u0000nγ+λ−1 log n \u0001 if 2α = 1 − γ + λ + 2β . In practice, the value of α is known in advance while the other parameters can be tuned to achieve the optimal rate of convergence. In any scenario, we can never achieve a bound of O(1/√n + bn), and the best rate we can achieve is O(log n/√n + bn) if and only if γ = 1/2, β = 0 and λ = 0. In this case, all eigenvalues of An must be bounded both from below and above. In the context of decreasing bias, if α ≥ 1/4, the bias term contributes to the speed of the algorithm. Otherwise, the other term is the leading term of the upper bound. However, in both cases, the best achievable bound is O(log n/√n) if α ≥ 1/4. Bounded Gradient Case. Now, we provide the convergence analysis of Randomized Adaptive Stochastic Approximation with a bounded stochastic update. Consider the following additional assumption about the stochastic update. Assumption 5. The stochastic update is bounded, i.e., there exists M ≥ 0 such that for all n ∈ N, ∥Hθn (Xn+1)∥ ≤ M. It is important to note that under Assumption 3, this is equivalent to bounding the stochastic gradient of the objective function. Corollary 4.4 provides a bound on the gradient of the objective function V , which is similar to Theorem 4.2. Corollary 4.4. Let γn = Cγn−γ, βn = Cβnβ, λn = Cλn−λ with Cγ > 0, Cβ > 0, and Cλ > 0. Assume that γ, β, λ ≥ 0 and γ + λ < 1. For any n ≥ 1, let R ∈ {0, . . . , n} be a uniformly distributed random variable. Then, under Assumptions 2 − 4, 5, we have: E h ∥∇V (θR)∥2i ≤ 2V ∗ + α1,n + LM2α′ 2,n √n , where V ∗, α1,n are defined in Theorem 4.2 and α′ 2,n = Pn k=0 γ2 k+1β2 k+1. Importantly, in Corollary 4.4, there are no assumptions on the step sizes, and we obtain a better bound than in Theorem 4.2. 4.3 Application to Adagrad and RMSProp In this section, we provide the convergence analysis of Adagrad and RMSProp with a biased gradient estimator. Remark 4.5. Under Assumption 5, for all eigenvalues λ of An, the adaptive matrix in Adagrad or RMSProp, it holds that (M2 + δ)−1/2 ≤ λ ≤ δ−1/2, i.e., Assumption 4 is satisfied with λ = 0 and β = 0. 7 Corollary 4.6. Let γn = cγn−1/2 and An denote the adaptive matrix in Adagrad or RMSProp. For any n ≥ 1, let R ∈ {0, . . . , n} be a uniformly distributed random variable. Then, under Assumptions 2, 3, 5, we have: E h ∥∇V (θR)∥2i = O \u0012log n √n + bn \u0013 . The bias bn is explicitly given in Appendix A. Since we do not have information about the global minimum of the objective function V , Corollary 4.6 establishes the rate of convergence of Adagrad and RMSProp with biased gradient to a critical point for non-convex smooth functions. In the case of an unbiased gradient, we obtain the same bound as in Zou et al. (2018), under the same assumptions: E h ∥∇V (θR)∥2i = O \u0012log n √n \u0013 . If the bias is of the order O(n−1/4), the algorithm achieves the same convergence rate as in the case of an unbiased gradient. 5 Experiments In this section, we illustrate our theoretical results in the context of deep VAE. The experiments were conducted using PyTorch (Paszke et al., 2017), and the source code can be found here∗. In generative models, the objective is to maximize the marginal likelihood defined as: log pθ(x) = log Epθ(·|x) \u0014pθ(x, Z) pθ(Z|x) \u0015 , where (x, z) 7→ pθ(x, z) is the complete likelihood, x are the observations and Z is the latent variable. Under some simple technical assumptions, by Fisher’s identity, we have: ∇θ log pθ(x) = Z ∇θ log pθ(x, z)pθ(z | x)dz. (7) However, in most cases, the conditional density z 7→ pθ(z | x) is intractable and can only be sampled. Variational Autoencoders introduce an additional parameter ϕ and a family of variational distributions z 7→ qϕ(z | x) to approximate the true posterior distribution. Parameters are estimated by maximizing the Evidence Lower Bound (ELBO): log pθ(x) ≥ Eqϕ(·|x) \u0014 log pθ(x, Z) qϕ(Z|x) \u0015 =: LELBO(θ, ϕ; x). The Importance Weighted Autoencoder (IWAE) (Burda et al., 2015) is a variant of the VAE that incorporates importance weighting to obtain a tighter ELBO. The IWAE objective can be written as follows: LIWAE k (θ, ϕ; x) = Eq⊗k ϕ (·|x) \" log 1 k k X ℓ=1 pθ(x, Z(ℓ)) qϕ(Z(ℓ)|x) # , where k corresponds to the number of samples drawn from the variational posterior distribution. The estimator of the gradient of ELBO in IWAE corresponds to the biased Self-Normalized Importance Sampling (SNIS) estimator of the gradient of the marginal log likelihood log pθ(x). As a consequence of the results of Agapiou et al. (2017), both the bias and Mean Squared Error (MSE) are of order O(1/k). Since the bias has an impact on the convergence rates, we propose to use the Biased Reduced Self-Normalized Importance Sampling (BR-SNIS) algorithm, proposed by Cardoso et al. (2022), to the IWAE estimator, resulting in the Biased Reduced Importance Weighted Autoencoder (BR-IWAE). The BR-IWAE algorithm (Cardoso et al., 2022) proceeds in two steps, which are repeated during optimization: ∗URL hidden during review process 8 • Update the parameter ϕ as in the IWAE algorithm, that is, for all n ≥ 1: ϕn+1 = ϕn − γn+1An∇ϕLIWAE k (θn, ϕn; Xn+1). • Update the parameter θ by estimating (7) using BR-SNIS as detailed in Appendix B. Dataset. We conduct our experiments on the CIFAR-10 dataset (Krizhevsky et al., 2009), which is a widely used dataset for image classification tasks. CIFAR-10 consists of 32x32 pixel images categorized into 10 different classes. The dataset is divided into 60,000 images in the training set and 10,000 images in the test set. Additional experiments are provided in Appendix B. Model. We use a Convolutional Neural Network (CNN) architecture with the Rectified Linear Unit (ReLU) activation function for both the encoder and the decoder. The latent space dimension is set to 100. Further details of the model are provided in Appendix B. We estimate the log-likelihood using the VAE, IWAE, and BR-IWAE models, all of which are trained for 100 epochs. Training is conducted using Adagrad and RMSProp with a decaying learning rate. 0 20 40 60 80 100 Epochs 1.825 1.85 1.875 1.9 1.925 1.95 1.975 2 Test loss (x10e3) VAE (Adagrad) IWAE (Adagrad) BR-IWAE (Adagrad) VAE (RMSProp) IWAE (RMSProp) BR-IWAE (RMSProp) Figure 2: Negative Log-Likelihood on the test set for Different Generative Models with Adagrad and RMSProp on the CIFAR-10 Dataset. Bold lines represent the mean over 5 independent runs. In this section, we exclusively use Adagrad and RMSProp algorithms to illustrate the results of Section 4.3 and we do not conduct a comparison with SGD. For the first experiment, we set a constant bias, i.e., we use k = 5 samples in both IWAE and BR-IWAE, while restricting the maximum iteration of the MCMC algorithm to 5 for BR-IWAE. The test losses are presented in Figure 2. We show the negative log-likelihood on the test dataset for VAE, IWAE, and BR-IWAE with Adagrad and RMSProp. As expected, we observe that IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias in both cases. To illustrate our results, we choose to incorporate a time-dependent bias that decreases by choosing a bias of order O(n−α) at iteration n. The bias of the estimator of the gradient in IWAE is of the order O(1/k), where k is the number of importance weights. Therefore, choosing the bias of order O(n−α) is equivalent to using nα samples at iteration n, to estimate the gradient. This procedure is detailed in Appendix B. We vary α only for IWAE for computational efficiency and plot the following quantities. • In Figures 3 and 4, the gradient squared norm ∥∇V (θn)∥2 to illustrate the convergence rate. • In Figure 5, the Negative Log-Likelihood along iterations. Figures 3 and 4 illustrate our results, while the other figures are meant to confirm the behavior of test loss with different values of α. All figures are plotted on a logarithmic scale for better visualization. It is important to note that all figures are in respect to epochs, whereas here, n represents the iteration (number of updates of the gradient). Note that the dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8 and O(log n/√n) for α = 1/4 and for α = 1/2. We can clearly observe that for each of the cases, fast convergence 9 0 20 40 60 80 100 Epochs 10 1 10 2 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 Figure 3: Value of ∥∇V (θn)∥2 in IWAE with Adagrad for different values of α. Bold lines represent the mean over 5 independent runs. 0 20 40 60 80 100 Epochs 100 101 102 V( n) 2 = 1/8 (RMSProp) = 1/4 (RMSProp) = 1/2 (RMSProp) = 1/8 (Adam) = 1/4 (Adam) = 1/2 (Adam) Figure 4: Value of ∥∇V (θn)∥2 in IWAE with RMSProp and Adam for different values of α. Bold lines represent the mean over 5 independent runs. is achieved when n is sufficiently large. There are several possible explanations for this rapid convergence with a decreasing time-dependent bias. First, we may be able to improve the upper bound by obtaining for instance a better bound for the bias. Our experiments show similar results for Adagrad and RMSProp in terms of convergence rates, although RMSProp performs slightly better. However, Adam’s behavior is somewhat more intricate due to the incorporation of momentum. Nevertheless, we consistently observe the impact of bias, although it tends to be relatively small, as the bias correction terms may help mitigate bias in the moving averages. It is clear that with a larger α, convergence in both squared gradient norm and negative log-likelihood is faster. However, beyond a certain threshold for α, we observe that the rate of convergence does not change significantly. Since choosing a larger α induces an additional computational cost, it is crucial to select an appropriate value of α, that achieves fast convergence without being too computationally costly. One approach to choose such a value for α is to plot the test loss with respect to time, which is shown in Figure 6 for Adagrad. Although α = 1/2 seems to have faster convergence in terms of iterations in Figure 5, the best rate in terms of time in Figure 6 is α = 1/4. We obtain similar results for RMSProp, see Appendix B. 10 0 20 40 60 80 100 Epochs 1.825 1.85 1.875 1.9 1.925 1.95 1.975 2 Test loss (x10e3) = 1/8 (Adagrad) = 1/4 (Adagrad) = 1/2 (Adagrad) = 1/8 (RMSProp) = 1/4 (RMSProp) = 1/2 (RMSProp) Figure 5: Negative Log-Likelihood on the test set on the CIFAR-10 Dataset for IWAE with Adagrad and RMSProp for Different Values of α. Bold lines represent the mean over 5 independent runs. 0 200 400 600 800 1000 Time (s) 1.84 1.86 1.88 1.9 1.92 1.94 1.96 1.98 Test loss (x10e3) = 1/8 = 1/4 = 1/2 Figure 6: Negative Log-Likelihood on the test set of the CIFAR-10 Dataset for IWAE with Adagrad for Different Values of α over time (in seconds). Bold lines represent the mean over 5 independent runs. 6 Discussion This paper provides a comprehensive non-asymptotic analysis of Biased Adaptive Stochastic Approximation for both convex and non-convex smooth functions. We derive a convergence rate of O(1/√n + bn) for convex functions and O(log n/√n+bn) for non-convex smooth functions, where bn corresponds to the time-dependent decreasing bias. We also establish that Adagrad and RMSProp with biased gradients converges to critical points for non-convex smooth functions. Our results provide insights on hyper-parameters tuning to achieve fast convergence and reduce unnecessary computational time. We intend to investigate whether the eigenvalues control conditions can be satisfied in practical settings, such as in the case of Stochastic Newton or Gauss Newton methods. We conducted additional comparisons with Adam to propose practical insights. However, a possible extension of our work is to extend our theoretical results to Adam. References Agapiou, S., Papaspiliopoulos, O., Sanz-Alonso, D., and Stuart, A. M. (2017). Importance sampling: Intrinsic dimension and computational cost. Statistical Science, pages 405–431. 11 Ajalloeian, A. and Stich, S. U. (2020). On the Convergence of SGD with Biased Gradients. arXiv preprint arXiv:2008.00051. Auer, P., Cesa-Bianchi, N., and Gentile, C. (2002). Adaptive and self-confident on-line learning algorithms. Journal of Computer and System Sciences, 64(1):48–75. Baxter, J. and Bartlett, P. L. (2001). Infinite-horizon policy-gradient estimation. Journal of Artificial Intelligence Research, 15:319–350. Bhandari, J., Russo, D., and Singal, R. (2018). A finite time analysis of temporal difference learning with linear function approximation. In Conference On Learning Theory, pages 1691–1692. PMLR. Bottou, L. (1991). Une approche th´eorique de l’apprentissage connexioniste; applications `a la reconnaissance de la parole. PhD thesis, Paris 11. Bottou, L., Curtis, F. E., and Nocedal, J. (2018). Optimization methods for large-scale machine learning. SIAM Review, 60(2):223–311. Boyer, C. and Godichon-Baggioni, A. (2023). On the asymptotic rate of convergence of stochastic newton algorithms and their weighted averaged versions. Computational Optimization and Applications, 84(3):921– 972. Burda, Y., Grosse, R., and Salakhutdinov, R. (2015). Importance weighted autoencoders. arXiv preprint arXiv:1509.00519. Cardoso, G., Idrissi, Y. J. E., Le Corff, S., Moulines, ´E., and Olsson, J. (2023). State and parameter learning with PaRIS particle Gibbs. arXiv preprint arXiv:2301.00900. Cardoso, G., Samsonov, S., Thin, A., Moulines, E., and Olsson, J. (2022). BR-SNIS: bias reduced self- normalized importance sampling. Advances in Neural Information Processing Systems, 35:716–729. Chen, P.-Y., Zhang, H., Sharma, Y., Yi, J., and Hsieh, C.-J. (2017). Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models. In Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security, pages 15–26. Dalal, G., Sz¨or´enyi, B., Thoppe, G., and Mannor, S. (2018). Finite sample analyses for td (0) with function approximation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32. D´efossez, A., Bottou, L., Bach, F., and Usunier, N. (2020). A simple convergence proof of Adam and Adagrad. arXiv preprint arXiv:2003.02395. Del Moral, P., Doucet, A., and Singh, S. S. (2010). A backward particle interpretation of Feynman-Kac formulae. ESAIM: Mathematical Modelling and Numerical Analysis, 44(5):947–975. Doan, T. T., Nguyen, L. M., Pham, N. H., and Romberg, J. (2020). Finite-time analysis of stochastic gradient descent under markov randomness. arXiv preprint arXiv:2003.10973. Dozat, T. (2016). Incorporating Nesterov momentum into Adam. Duchi, J., Hazan, E., and Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. Journal of Machine Learning Research, 12(7). Ghadimi, S. and Lan, G. (2013). Stochastic first-and zeroth-order methods for nonconvex stochastic programming. SIAM Journal on Optimization, 23(4):2341–2368. Gloaguen, P., Le Corff, S., and Olsson, J. (2022). A pseudo-marginal sequential Monte Carlo online smoothing algorithm. Bernoulli, 28(4):2606–2633. Godichon-Baggioni, A. and Tarrago, P. (2023). Non asymptotic analysis of adaptive stochastic gradient algorithms and applications. arXiv preprint arXiv:2303.01370. 12 Jaakkola, T., Jordan, M., and Singh, S. (1993). Convergence of stochastic iterative dynamic programming algorithms. Advances in Neural Information Processing Systems, 6. Karimi, B., Miasojedow, B., Moulines, E., and Wai, H.-T. (2019). Non-asymptotic analysis of biased stochastic approximation scheme. In Conference on Learning Theory, pages 1944–1974. PMLR. Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980. Kingma, D. P. and Welling, M. (2013). Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114. Kresoja, M., Luˇzanin, Z., and Stojkovska, I. (2017). Adaptive stochastic approximation algorithm. Numerical Algorithms, 76(4):917–937. Krizhevsky, A., Hinton, G., et al. (2009). Learning multiple layers of features from tiny images. Lakshminarayanan, C. and Szepesvari, C. (2018). Linear stochastic approximation: How far does constant step-size and iterate averaging go? In International Conference on Artificial Intelligence and Statistics, pages 1347–1355. PMLR. Li, Q. and Wai, H.-T. (2022). State dependent performative prediction with stochastic approximation. In International Conference on Artificial Intelligence and Statistics, pages 3164–3186. PMLR. Moosavi-Dezfooli, S.-M., Fawzi, A., Fawzi, O., and Frossard, P. (2017). Universal adversarial perturbations. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1765–1773. Moulines, E. and Bach, F. (2011). Non-asymptotic analysis of stochastic approximation algorithms for machine learning. Advances in Neural Information Processing Systems, 24. Nemirovski, A., Juditsky, A., Lan, G., and Shapiro, A. (2009). Robust stochastic approximation approach to stochastic programming. SIAM Journal on Optimization, 19(4):1574–1609. Nemirovskij, A. S. and Yudin, D. B. (1983). Problem complexity and method efficiency in optimization. Nesterov, Y. and Spokoiny, V. (2017). Random gradient-free minimization of convex functions. Foundations of Computational Mathematics, 17:527–566. Olsson, J. and Westerborn, J. (2017). Efficient particle-based online smoothing in general hidden Markov models: the paris algorithm. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z., Desmaison, A., Antiga, L., and Lerer, A. (2017). Automatic differentiation in PyTorch. Polyak, B. T. and Juditsky, A. B. (1992). Acceleration of stochastic approximation by averaging. SIAM Journal on Control and Optimization, 30(4):838–855. Robbins, H. and Monro, S. (1951). A stochastic approximation method. The Annals of Mathematical Statistics, pages 400–407. Sun, T., Sun, Y., and Yin, W. (2018). On Markov chain gradient descent. Advances in Neural Information Processing Systems, 31. Tadi´c, V. B. and Doucet, A. (2011). Asymptotic bias of stochastic gradient search. In 2011 50th IEEE Conference on Decision and Control and European Control Conference, pages 722–727. IEEE. Tieleman, T., Hinton, G., et al. (2012). Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning, 4(2):26–31. Tjelmeland, H. (2004). Using all metropolis–hastings proposals to estimate mean values. Technical report. Ward, R., Wu, X., and Bottou, L. (2020). Adagrad stepsizes: Sharp convergence over nonconvex landscapes. The Journal of Machine Learning Research, 21(1):9047–9076. 13 Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747. Zeiler, M. D. (2012). Adadelta: an adaptive learning rate method. arXiv preprint arXiv:1212.5701. Zou, F., Shen, L., Jie, Z., Sun, J., and Liu, W. (2018). Weighted adagrad with unified momentum. arXiv preprint arXiv:1808.03408. A Proofs A.1 Proof of Theorem 4.1 We first establish a technical lemma which is essential for the proof. Lemma A.1. Let (δn)n≥0 , (γn)n≥1 , (ηn)n≥1, and (vn)n≥1 be some positive sequences satisfying the following assumptions. • The sequence δn follows the recursive relation: δn ≤ (1 − 2ωγn + ηnγn) δn−1 + vnγn, with δ0 ≥ 0 and ω > 0. • Let n0 = inf {n ≥ 1 : ηn ≤ ω}, then for all n ≥ n0 + 1, we assume that ωγn ≤ 1. Then, for all n ∈ N, δn ≤ exp  −ω n X k=n/2 γk   exp   2 n X k=1 ηkγk ! \u0012 δ0 + 2 max 1≤k≤n vk ηk \u0013 + 1 ω max n/2≤k≤n vk. The proof is given in Godichon-Baggioni and Tarrago (2023). We will present the detailed version of Theorem 4.1. Theorem A.2. Let θn ∈ Rd be the n-th iterate of the recursion (3). Under Assumptions 1 − 4, we have: E [V (θn) − V (θ∗)] ≤   E [V (θ0) − V (θ∗)] + 1 L2 max 1≤k≤n λk+1vk β2 k+1γk+1 ! exp  −µ 2 n X k=n/2 λk+1γk+1   exp   4L2 n X k=1 Ckβ2 k+1γ2 k+1 ! + 2 µ max n/2≤k≤n vk, where Ck = max ( 1, µ2λ2 k+1 8L2β2 k+1 ) and vk = 1 2 Lβ2 k+1r2 k µλ2 k+1 + Lσ2 k β2 k+1 λk+1 γk+1. Proof. As V is L smooth (Assumption 2) and using the recursion (3) of Adaptive SA, we obtain: V (θn+1) ≤ V (θn) + ⟨∇V (θn) | θn+1 − θn⟩ + L 2 ∥θn+1 − θn∥2 ≤ V (θn) − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2 n+1 2 ∥An∥2 ∥Hθn (Xn+1)∥2 ≤ V (θn) − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2 n+1 2 β2 n+1 ∥Hθn (Xn+1) − ∇V (θn) + ∇V (θn)∥2 . Using that for all x, y ≥ 0, ∥x + y∥2 ≤ 2 \u0010 ∥x∥2 + ∥y∥2\u0011 , and writing Vn = V (θn) − V (θ∗), we get Vn+1 ≤ Vn − γn+1 ⟨∇V (θn) | AnHθn (Xn+1)⟩ + Lγ2 n+1β2 n+1 \u0010 ∥Hθn (Xn+1) − ∇V (θn)∥2 + ∥∇V (θn)∥2\u0011 . 14 Then, using Assumption 3, and since ∥∇V (θn)∥2 ≤ 2LVn (Assumption 1 and 2), E [Vn+1 | Fn] ≤ \u00001 + 2L2β2 n+1γ2 n+1 \u0001 Vn − γn+1 ⟨∇V (θn) | An (E [Hθn (Xn+1) | Fn] − ∇V (θn))⟩ − γn+1 ⟨∇V (θn) | An∇V (θn)⟩ + Lσ2 nγ2 n+1β2 n+1 ≤ \u00001 + 2L2β2 n+1γ2 n+1 \u0001 Vn + γn+1βn+1rn ∥∇V (θn)∥ − γn+1 ⟨∇V (θn) | An∇V (θn)⟩ + Lσ2 nγ2 n+1β2 n+1 ≤ \u00001 + 2L2β2 n+1γ2 n+1 \u0001 Vn + 1 2an γn+1βn+1r2 n + anLγn+1βn+1Vn − γn+1 ⟨∇V (θn) | An∇V (θn)⟩ + Lσ2 nγ2 n+1β2 n+1, where the second last inequality is due to the Cauchy-Schwarz inequality. In the last inequality, we used ∥∇V (θn)∥2 ≤ 2LVn and the inequality xy ≤ x2/(2an) + any2/2 where an is defined below. Furthermore, since V satisfies the Polyak- Lojasiewicz condition (Assumption 1), ⟨∇V (θn) | An∇V (θn)⟩ ≥ λmin (An) ∥∇V (θn)∥2 ≥ 2λn+1µVn. Finally, choosing an = (µλn+1)/(Lβn+1) we obtain: E [Vn+1] ≤ \u00001 − µλn+1γn+1 + 2L2β2 n+1γ2 n+1 \u0001 E [Vn] + 1 2 Lβ2 n+1r2 n µλn+1 γn+1 + Lσ2 nγ2 n+1β2 n+1. By choosing ¯γn+1 = λn+1γn+1, we get: E [Vn+1] ≤ \u0012 1 − µ¯γn+1 + 2L2 β2 n+1 λ2 n+1 ¯γ2 n+1 \u0013 E [Vn] + 1 2 Lβ2 n+1r2 n µλ2 n+1 ¯γn+1 + Lσ2 n β2 n+1 λ2 n+1 ¯γ2 n+1. In order to satisfy the assumptions of Lemma A.1, consider Cn = max \b 1, (µ2λ2 n+1)/(8L2β2 n+1) \t , and since Cn ≥ 1, we have: E [Vn+1] ≤ \u0012 1 − µ¯γn+1 + 2L2 Cnβ2 n+1 λ2 n+1 ¯γ2 n+1 \u0013 E [Vn] + 1 2 Lβ2 n+1r2 n µλ2 n+1 ¯γn+1 + Lσ2 n β2 n+1 λ2 n+1 ¯γ2 n+1. Now, using lemma A.1 by choosing: δn = E [Vn] , ηn = 2L2 Cnβ2 n+1 λ2 n+1 ¯γn+1, ω = µ 2 , vn = 1 2 Lβ2 n+1r2 n µλ2 n+1 + Lσ2 n β2 n+1 λ2 n+1 ¯γn+1, we have: E [V (θn) − V (θ∗)] ≤   E [V (θ0) − V (θ∗)] + 1 L2 max 1≤k≤n vkλ2 k+1 β2 k+1¯γk+1 ! e− µ 2 Pn k=n/2 ¯γk+1e 4L2 Pn k=1 Ckβ2 k+1 λ2 k+1 ¯γ2 k+1 + 2 µ max n/2≤k≤n {vk} , which concludes the proof by replacing ¯γn+1 = λn+1γn+1. A.2 Proof of Theorem 4.2 By Assumption 2, V is L-smooth and using the recursion (3) of Adaptive SA together with a Taylor expansion, we obtain: V (θk+1) ≤ V (θk) + ⟨∇V (θk) | θk+1 − θk⟩ + L 2 ∥θk+1 − θk∥2 , which yields V (θk+1) ≤ V (θk) − γk+1 ⟨∇V (θk) | Ak∇V (θk)⟩ − γk+1 ⟨∇V (θk) | Ak (Hθk (Xk+1) − ∇V (θk))⟩ + δk+1 \u0000∥Hθk (Xk+1) − ∇V (θk)∥2 + ∥∇V (θk)∥2 \u0001 , 15 with δk+1 = Lγ2 k+1β2 k+1. By the Cauchy-Schwarz inequality, and using Assumptions (3) and (4), E[V (θk+1)|Fk] ≤ V (θk) + γk+1βk+1rk∥∇V (θk)∥ − γk+1λmin(Ak)∥∇V (θk)∥2 + δk+1(σ2 k + ∥∇V (θk)∥2). Therefore, γk+1 \u0012 λk+1 − λk+1 2 − Lγk+1β2 k+1 \u0013 ∥∇V (θk)∥2 ≤ V (θk) − E [V (θk+1) |Fk] + γk+1β2 k+1r2 k 2λk+1 + δk+1σ2 k. and n X k=0 γk+1λk+1   1 2 − Lγk+1β2 k+1 λk+1 ! E h ∥∇V (θk)∥2i ≤ E [V (θ0) − V (θn+1)] + 1 2 n X k=0 γk+1β2 k+1r2 k λk+1 + n X k=0 δk+1σ2 k. Then, given that γk+1 ≤ λk+1/(6Lβ2 k+1), we have 1 3E \" n X k=0 γk+1λk+1 ∥∇V (θk)∥2 # ≤ E [V (θ0) − V (θn+1)] + 1 2 n X k=0 γk+1β2 k+1r2 k λk+1 + n X k=0 δk+1σ2 k . Consequently, by definition of the discrete random variable R, E h ∥∇V (θR)∥2i = n X k=0 γk+1λk+1E h ∥∇V (θk)∥2i Pn j=0 γj+1λj+1 ≤ 3V0,n + 1 2 Pn k=0 γk+1β2 k+1r2 k/λk+1 + Pn k=0 δk+1σ2 k Pn j=0 γj+1λj+1 , where V0,n = E[V (θ0) − V (θn+1)], which concludes the proof by noting that V (θn+1) ≥ V (θ∗). A.3 Proof of Corollary 4.3 The proof is a direct consequence of the fact that for a sufficiently large n: n X k=1 1 ks =      O \u0000n−s+1\u0001 if 0 ≤ s < 1 , O (1) if s > 1 , O (log n) if s = 1 . A.4 Proof of Corollary 4.4 By Assumption 2, V is L-smooth and using recursion (3) of Adaptive SA same as Theorem 4.2, we obtain: V (θk+1) ≤ V (θk) + ⟨∇V (θk) | θk+1 − θk⟩ + L 2 ∥θk+1 − θk∥2 ≤ V (θk) − γk+1 ⟨∇V (θk) | AkHθk (Xk+1)⟩ + Lγ2 k+1 2 ∥Ak∥2 ∥Hθk (Xk+1)∥2 , which, using Assumption (5) yields: V (θk+1) ≤ V (θk) − γk+1 ⟨∇V (θk) | Ak∇V (θk)⟩ − γk+1 ⟨∇V (θk) | Ak (Hθk (Xk+1) − ∇V (θk))⟩ + L 2 γ2 k+1β2 k+1M2. By the Cauchy-Schwarz inequality, and using Assumptions (3) and (4), E[V (θk+1)|Fk] ≤ V (θk) − γk+1λmin(Ak)∥∇V (θk)∥2 + γk+1βk+1rk∥∇V (θk)∥ + LM2 2 γ2 k+1β2 k+1 ≤ V (θk) − γk+1λk+1∥∇V (θk)∥2 + γk+1βk+1rk∥∇V (θk)∥ + LM2 2 γ2 k+1β2 k+1 . 16 Therefore, γk+1 \u0012 λk+1 − λk+1 2 \u0013 ∥∇V (θk)∥2 ≤ V (θk) − E [V (θk+1) |Fk] + γk+1β2 k+1r2 k 2λk+1 + LM2 2 γ2 k+1β2 k+1, and 1 2 n X k=0 γk+1λk+1E h ∥∇V (θk)∥2i ≤ E [V (θ0) − V (θn+1)] + 1 2 n X k=0 γk+1β2 k+1r2 k λk+1 + LM2 2 n X k=0 γ2 k+1β2 k+1. Consequently, by definition of the discrete random variable R, E h ∥∇V (θR)∥2i = 1 n n X k=0 E h ∥∇V (θk)∥2i ≤ n X k=0 γk+1λk+1 √n E h ∥∇V (θk)∥2i ≤ 2V0,n + Pn k=0 γk+1β2 k+1r2 k/λk+1 + LM2 Pn k=0 γ2 k+1β2 k+1 √n , where V0,n = E[V (θ0) − V (θn+1)], which conclude the proof by noting that V (θn+1) ≥ V (θ∗). A.5 Proof of Corollary 4.6 Adagrad • Upper bound for the largest eigenvalue of An. By assumption (5), we have: \r\r\r\r\r 1 n n−1 X k=0 Hθk(Xk+1)Hθk(Xk+1)⊤ \r\r\r\r\r ≤ M2. This implies that: λmin(An) = λmax   δId + Diag   1 n n−1 X k=0 Hθk(Xk+1)Hθk(Xk+1)⊤ !!−1/2 ≥ (δ + M2)−1/2. • Lower bound for the smallest eigenvalue of An. λmax(An) = λmin   δId + Diag   1 n n−1 X k=0 Hθk(Xk+1)Hθk(Xk+1)⊤ !!−1/2 ≤ δ−1/2. Therefore, by setting λn+1 = 1/ √ δ + M2 and βn+1 = 1/ √ δ, we have λ = 0 and β = 0. The proof is completed by applying Corollary 4.4 and then Corollary 4.3. RMSProp • Upper bound for the largest eigenvalue of An. By assumption (5), we have: ∥Vn∥ ≤ (1 − ρ) n X k=1 ρn−k ∥Hθk(Xk+1)∥2 ≤ M2(1 − ρ) n X k=1 ρn−k ≤ M2, where we used the fact that Pn k=1 ρn−k ≤ (1 − ρ)−1. This implies that: λmin(An) = λmax (δId + Diag (Vn))−1/2 ≥ (δ + M2)−1/2. • Lower bound for the smallest eigenvalue of An. λmax(An) = λmin (δId + Diag (Vn))−1/2 ≤ δ−1/2. 17 The bias term bn is given by: bn =      O \u0000n−2α\u0001 if α < 1/4 , O \u0000n−1/2\u0001 if α > 1/4 , O \u0000n−1/2 log n \u0001 if α = 1/4 . B Additional Experimental Details B.1 Experiment with a Synthetic Time-Dependent Bias In this setup, we consider a simple least squares objective function V (θ) = ∥Aθ∥2/2 in dimension d = 10, where A is a positive matrix ensuring convexity. We introduce zero-mean Gaussian noise with variance σ2 = 0.01 to every gradient and artificially include the bias term rn at each iteration. We explore different values of rn ∈ {1, n−1/8, n−1/4, n−1/2, n−1, 0}, where rn = 1 corresponds to constant bias, rn = 0 for an unbiased gradient, and the others exhibit decreasing bias. 0 2000 4000 6000 8000 10000 Epochs 10 1 100 101 V( n) 2 rn = 1 rn = n 1/8 rn = n 1/4 rn = n 1/2 rn = n 1 rn = 0 Figure 7: Value of ∥∇V (θn)∥2 with Adagrad for different values of rn. In Figure 7, we observe the convergence rate of the squared norm of the gradient. Similar to Figure 1, we notice the impact of bias on the squared norm of the gradient. When α ≥ 1/4, we observe nearly the same convergence rate as in the case of an unbiased gradient. B.2 Importance Weighted Autoencoder (IWAE) In this section, we elaborate on the IWAE procedure within our framework to illustrate its convergence rate. First, let’s recall some basics of IWAE. The IWAE objective function is defined as: LIWAE k (θ, ϕ; x) = Eq⊗k ϕ (·|x) \" log 1 k k X ℓ=1 pθ(x, Z(ℓ)) qϕ(Z(ℓ)|x) # =: Eq⊗k ϕ (·|x) h ˜LIWAE k (θ, ϕ; x) i , where k corresponds to the number of samples drawn from the encoder’s approximate posterior distribution. The gradient of the empirical estimate of the IWAE objective is given by: ∇ϕ,θ ˜LIWAE k (θ, ϕ; x) = k X l=1 w(l) Pk m=1 w(m) ∇ϕ,θ log w(l), where w(l) = pθ(x,z(l)) qϕ(z(l)|x) the unnormalized importance weights. This expression corresponds exactly to the biased SNIS estimator of the gradient of the marginal log likelihood log pθ(x). 18 Algorithm 1 Adaptive Stochastic Approximation for IWAE Input: Initial point θ0, maximum number of iterations n, step sizes {γk}k≥1 and a hyperparameter α ≥ 0 to control the bias and MSE. for k = 0 to n − 1 do Compute the stochastic update ∇θ,ϕLIWAE kα (θk, ϕk; Xk+1) using kα samples from the variational posterior distribution and adaptive steps Ak. Set θk+1 = θk − γk+1Ak∇θLIWAE kα (θk, ϕk; Xk+1) and ϕk+1 = ϕk − γk+1Ak∇ϕLIWAE kα (θk, ϕk; Xk+1). end for Output: (θk)1≤k≤n B.3 BR-IWAE In this section, we provide additional details on the Biased Reduced Importance Weighted Autoencoder (BR-IWAE). Let π be a probability measure on a measurable space (X, X). The objective is to estimate π(f) = Eπ[f(X)] for a measurable function f : X → R such that π(|f|) < ∞. Assume that π(dx) ∝ w(x)λ(dx), where w is a positive weight function and λ is a proposal probability distribution, and that λ(w) = R w(x)λ(dx) < ∞. We first introduce the algorithm i-SIR (iterated Sampling Importance Resampling), described in Algorithm 2, which was proposed by Tjelmeland (2004). Algorithm 2 i-SIR algorithm Input: Maximum number of iterations n, number of particles N, a proposal probability distribution λ and a positive weight function w. Initialization: Draw Y0 from the proposal distribution λ. for k = 0 to n − 1 do Draw Ik+1 ∈ {1, . . . , N} uniformly at random and set XIk+1 k+1 = Yk. Draw X1:N\\{Ik+1} k+1 independently from the proposal distribution λ. Compute the normalized importance weights: ωi k+1 = w \u0000Xi k+1 \u0001 PN ℓ=1 w \u0000Xℓ k+1 \u0001 ∀i ∈ {1, . . . , N}. Select Yk+1 from the set X1:N k+1 by choosing Xi k+1 with probability ωi k+1. end for Output: \u0000X1:N k \u0001 1≤k≤n and \u0000ω1:N k \u0001 1≤k≤n. The BR-SNIS estimator of (Cardoso et al., 2022) aims at reducing the bias of self-normalized importance sampling estimators without increasing the variance. The proposed estimator of π(f) is given by: Π(n0,n),N(f) = 1 n − n0 n X ℓ=n0+1 N X i=1 ωi ℓf \u0000Xi ℓ \u0001 , where n0 corresponds to a burn-in period. All the importance weights in Π(n0,n),N(f) are obtained similarly as in the i-SIR algorithm so that its computational complexity is easily controlled. In addition, by Cardoso et al. (2022, Theorem 4) the bias of the BR-SNIS estimator decreases exponentially with n0. Therefore, we propose to use this approach in the context of IWAE. To estimate (7) for the update in BR-IWAE algorithm, we use the BR-SNIS estimator with: π : z 7→ pθ(x, z), λ : z 7→ qϕ(z | x), f : z 7→ ∇θ log pθ(x, z) and w : z 7→ pθ(x, z)/qϕ(z|x). B.4 Additional Experiments in IWAE In this section, we provide detailed information about the experiments on CIFAR-10. We also conduct additional experiments on the FashionMNIST and CIFAR-100 datasets. For all experiments, we use Adagrad 19 and RMSProp with a learning rate decay given by γn = Cγ/√n, where Cγ = 0.01 for Adagrad and Cγ = 0.001 for RMSProp. Datasets. We conduct our experiments on three datasets: FashionMNIST (Xiao et al., 2017), CIFAR-10 and CIFAR-100. The FashionMNIST dataset is a variant of MNIST and consists of 28x28 pixel images of various fashion items, with 60,000 images in the training set and 10,000 images in the test set. CIFAR-100 is a dataset similar to CIFAR-10 but with 100 different classes (instead of 10). Models. For FashionMNIST, we use a fully connected neural network with a single hidden layer consisting of 400 hidden units and ReLU activation functions for both the encoder and the decoder. The latent space dimension is set to 20. We use 256 images per iteration (235 iterations per epoch). For CIFAR-10 and CIFAR-100, we use a Convolutional Neural Network (CNN) architecture with 3 Convolutional layers and 2 fully connected layers with ReLU activation functions. The latent space dimension is set to 100. For both datasets, we use 256 images per iteration (196 iterations per epoch). We do not employ any acceleration methods, such as adding momentum during training. We estimate the log-likelihood using the VAE, IWAE, and BR-IWAE models, all of which are trained for 100 epochs. Training is conducted using the SGD, Adagrad, and RMSProp algorithms with a decaying learning rate, as mentioned before. For SGD, we employ the clipping method to clip the gradients at a specified threshold, which is fixed at 10000, in order to prevent excessively large steps. 0 20 40 60 80 100 Epochs 2.4 2.45 2.5 2.55 2.6 2.65 2.7 2.75 Test loss (x10e2) VAE IWAE BR-IWAE 0 20 40 60 80 100 Epochs 2.4 2.5 2.6 2.7 2.8 2.9 3 3.1 Test loss (x10e2) VAE IWAE BR-IWAE Figure 8: Negative Log-Likelihood for Different Generative Models with Adagrad (on the left) and RMSProp (on the right) on the FashionMNIST Dataset. Bold lines represent the mean over 5 independent runs. For this experiment, we set k = 5 samples in both IWAE and BR-IWAE, while restricting the maximum iteration of the MCMC algorithm to 5 and the burn-in period to 2 for BR-IWAE. The test losses for Adagrad and RMSProp are illustrated in 8. More precisely, the figure shows the negative log-likelihood on the training and test datasets for VAE, IWAE, and BR-IWAE with Adagrad and RMSProp. Similar to the case of CIFAR-10, we observe that IWAE outperforms VAE, while BR-IWAE outperforms IWAE by reducing bias in both cases. For comparison, we estimate the Negative Log-Likelihood using these three models with SGD, Adagrad and RMSProp, and the results are presented in Table 1. Table 1: Comparison of Negative Log-Likelihood on the FashionMNIST Test Set (Lower is Better). Algorithm VAE IWAE BR-IWAE SGD 247.2 244.9 244.0 Adagrad 245.8 241.4 240.5 RMSProp 242.6 239.3 237.8 Similarly, as we did in the case of CIFAR-10, we incorporate a time-dependent bias that decreases by choosing a bias of order O(n−α) at iteration n. We vary the value of α for both FashionMNIST and CIFAR-100. 20 0 20 40 60 80 100 Epochs 10 1 10 0 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 0 20 40 60 80 100 Epochs 2.4 2.45 2.5 2.55 2.6 2.65 2.7 2.75 2.8 Test loss (x10e2) = 1/8 = 1/4 = 1/2 Figure 9: IWAE on the FashionMNIST Dataset with Adagrad for different values of α. Bold lines represent the mean over 5 independent runs. 0 20 40 60 80 100 Epochs 10 1 100 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 0 20 40 60 80 100 Epochs 2.4 2.5 2.6 2.7 2.8 2.9 3 Test loss (x10e2) = 1/8 = 1/4 = 1/2 Figure 10: IWAE on the FashionMNIST Dataset with RMSProp for different values of α. Bold lines represent the mean over 5 independent runs. 0 20 40 60 80 100 Epochs 10 1 10 2 10 3 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 0 20 40 60 80 100 Epochs 1.775 1.8 1.825 1.85 1.875 1.9 1.925 1.95 Test loss (x10e3) = 1/8 = 1/4 = 1/2 Figure 11: IWAE on the CIFAR-100 Dataset with Adagrad for different values of α. Bold lines represent the mean over 5 independent runs. 21 0 20 40 60 80 100 Epochs 101 102 103 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 0 20 40 60 80 100 Epochs 1.75 1.8 1.85 1.9 1.95 2 Test loss (x10e3) = 1/8 = 1/4 = 1/2 Figure 12: IWAE on the CIFAR-100 Dataset with RMSProp for different values of α. Bold lines represent the mean over 5 independent runs. All figures are plotted on a logarithmic scale for better visualization and with respect to the number of epochs. The dashed curve corresponds to the expected convergence rate O(n−1/4) for α = 1/8, and O(log n/√n) for α = 1/4, as well as for α = 1/2, just as in the case of CIFAR-10. We can clearly observe that for all cases, convergence is achieved when n is sufficiently large. In the case of the FashionMNIST dataset, the bound seems tight, and the convergence rate of O(n−1/2) does not seem to be possible to reach, in contrast to the case of CIFAR-100 where the curves corresponding to α = 1/4 and α = 1/2 approach the O(n−1/2) convergence rate. For all figures, with a larger α, the convergence in both the squared gradient norm and negative log-likelihood occurs more rapidly. The effect of Cγ. Figure 13 illustrates the convergence in both the squared gradient norm and the negative log-likelihood for Cγ = 0.001 and Cγ = 0.01 in Adagrad. In the case of the squared gradient norm, we have only plotted the results for Cγ = 0.001 for better visualization, and the plot for Cγ = 0.01 was already presented in Figure 3. It is clear that when Cγ is set to 0.001, the convergence of the negative log-likelihood is slower. Similarly, the convergence in the squared gradient norm for Cγ = 0.001 achieves convergence, but it is slower compared to the case of Cγ = 0.01. 0 20 40 60 80 100 Epochs 10 1 10 2 10 3 V( n) 2 n 1/4 log(n)/ n 1/ n = 1/8 = 1/4 = 1/2 0 20 40 60 80 100 Epochs 1.825 1.85 1.875 1.9 1.925 1.95 1.975 2 Test loss (x10e3) = 1/8, C = 0.001 = 1/4, C = 0.001 = 1/2, C = 0.001 = 1/8, C = 0.01 = 1/4, C = 0.01 = 1/2, C = 0.01 Figure 13: IWAE on the CIFAR-10 Dataset with Adagrad for different values of α and Cγ. Bold lines represent the mean over 5 independent runs. The Impact of Bias over Time. Our experiments illustrate the negative log-likelihood with respect to epochs, and we observed that a higher value of α leads to faster convergence. The key point to consider when tuning α is that while convergence may be faster in terms of iterations, it may lead to higher computational costs. To illustrate 22 this, we set a fixed time limit of 1000 seconds and tested different values of α, plotting the test loss as a function of time in Figure 14. It is clear that with α = 1/8, the convergence is always slower, whereas choosing α = 1/4 achieves faster convergence than α = 1/2. While the difference may seem small here, with more complex models, the disparity becomes significant. Therefore, it is essential to tune the value of α to attain fast convergence and reduce computational time. 0 200 400 600 800 1000 Time (s) 1.825 1.85 1.875 1.9 1.925 1.95 1.975 2 Test loss (x10e3) = 1/8 = 1/4 = 1/2 Figure 14: Negative Log-Likelihood on the test set of the CIFAR-10 Dataset for IWAE with RMSProp for Different Values of α over time (in seconds). Bold lines represent the mean over 5 independent runs. In this paper, all simulations were conducted using the Nvidia Tesla T4 GPU. The total computing hours required for the results presented in this paper are estimated to be around 50 to 100 hours of GPU usage. C Some Examples of Biased Gradients In this section, we explore examples of applications using biased gradient estimators while having control over the bias. C.1 Self-Normalized Importance Sampling Let π be a probability measure on a measurable space (X, X). The objective is to estimate π(f) = Eπ[f(X)] for a measurable function f : X → Rd such that π(|f|) < ∞. Assume that π(dx) ∝ w(x)λ(dx), where w is a positive weight function and λ is a proposal probability distribution, and that λ(w) = R w(x)λ(dx) < ∞. For a function f : X → Rd such that π(|f|) < ∞, the identity π(f) = λ(ωf) λ(ω) , (8) leads to the Self-Normalized Importance Sampling (SNIS) estimator: ΠNf \u0000X1:N\u0001 = N X i=1 ωi Nf \u0000Xi\u0001 , ωi N = w \u0000Xi\u0001 PN ℓ=1 w (Xℓ) , where X1:N = \u0000X1, . . . , XN\u0001 are independent draws from λ and the ωi N are called the normalized weights. Agapiou et al. (2017) shows that the bias of SNIS estimator can be expressed as: \r\rE \u0002 ΠNf \u0000X1:N\u0001 − π(f) \u0003\r\r ≤ 12 N λ \u0000ω2\u0001 λ(ω)2 . This particular type of estimator can be found in the Importance Weighted Autoencoder (IWAE) framework (Burda et al., 2015), as illustrated in B. The estimator of the gradient of ELBO in IWAE corresponds to the biased SNIS estimator of the gradient of the marginal log likelihood log pθ(x). Consequently, based on these results, both the bias and Mean Squared Error (MSE) are of order O(1/k), where k corresponds to the number of samples drawn from the variational posterior distribution. 23 C.2 Sequential Monte Carlo Methods We focus here in the task of estimating the parameters, denoted as θ, in Hidden Markov Models. In this context, the hidden Markov chain is denoted by (Xt)t≥0. The distribution of X0 has density χ with respect to the Lebesgue measure µ and for all t ≥ 0, the conditional distribution of Xt+1 given X0:t has density mθ(Xt, ·). It is assumed that this state is partially observed through an observation process (Yt)0≤t≤T . The observations Y0:t are assumed to be independent conditionally on X0:t and, for all 0 ≤ t ≤ T, the distribution of Yt given X0:t depends on Xt only and has density gθ(Xt, ·) with respect to the Lebesgue measure. The joint distribution of hidden states and observations is given by pθ(x0:T , y0:T ) = χ(x0)gθ(x0, y0) T−1 Y t=0 mθ(xt, xt+1)gθ(xt+1, yt+1). Our objective is to maximize the likelihood of the model: pθ(y0:T ) = Z pθ(x0:T , y0:T ) dx0:T . To use a gradient-based method for this maximization problem, we need to compute the gradient of the objective function. Under simple technical assumptions, by Fisher’s identity, ∇θ log pθ(y0:T ) = Z ∇θ log pθ(x0:T , y0:T )pθ(x0:T |y0:T )dx0:T = Ex0:T ∼pθ(.|y0:T ) [∇θ log pθ(x0:T , y0:T )] = Ex0:T ∼pθ(.|y0:T ) \"T−1 X t=0 st,θ (xt, xt+1) # , where st,θ (x, x′) = ∇θ log{mθ (x, x′) gθ (x, yt+1)} for t > 0 and by convention s0,θ (x, x′) = ∇θ log gθ (x, y0). Given that the gradient of the log-likelihood represents the smoothed expectation of an additive functional, one may opt for Online Smoothing algorithms to mitigate computational costs. The estimation of the gradient ∇θ log pθ(y0:T ) is given by: Hθ (y0:T ) = N X i=1 ωi T ΩT τ i T,θ, where {τ i T,θ}N i=1 are particle approximations obtained using particles { \u0000ξi T , ωi T \u0001 }N i=1 targeting the filtering distribution ϕT , i.e. the conditional distribution of xT given y0:T . In the Forward-only implementation of FFBSm (Del Moral et al., 2010), the particle approximations {τ i T,θ}N i=1 are computed using the following formula, with an initialization of τ i 0 = 0 for all i ∈ J1, NK: τ i t+1,θ = N X j=1 ωj t mθ(ξj t , ξi t+1) P ℓ=1 ωℓ tmθ(ξℓ t, ξi t+1) n τ j t,θ + st,θ(ξj t , ξi t+1) o , t ∈ N. The estimator of the gradient Hθ (y0:T ) computed by the Forward-only implementation of FFBSm is biased. The bias and MSE of this estimator are of order O (1/N) (Del Moral et al., 2010), where N corresponds to the number of particles used to estimate it. Using alternative recursion methods to compute {τ i T,θ}N i=1 results in different algorithms, such as the particle-based rapid incremental smoother (PARIS) (Olsson and Westerborn, 2017) and its pseudo-marginal extension Gloaguen et al. (2022) and Parisian particle Gibbs (PPG) (Cardoso et al., 2023). In such cases, one can also control the bias and MSE of the estimator. C.3 Policy Gradient for Average Reward over Infinite Horizon Consider a finite Markov Decision Process (MDP) denoted as (S, A, R, P), where S represents the state space, A denotes the action space, R : S × A → [0, Rmax] is a reward function, and P is the transition model. 24 The agent’s decision-making process is characterized by a parametric family of policies {πθ}θ∈Rd, employing the soft-max parameterization. The reward function is given by: V (θ) := E(S,A)∼vθ [R(S, A)] = X (s,a)∈S×A vθ(s, a)R(s, a), where vθ represents the unique stationary distribution of the state-action Markov Chain sequence {(St, At)}t≥1 generated by the policy πθ. Let λ ∈ (0, 1) be a discount factor and T be sufficiently large, the estimator of the gradient of the objective function V is given by: Hθ (S1:T , A1:T ) = R (ST , AT ) T−1 X i=0 λi∇ log πθ (AT−i; ST−i) , where (S1:T , A1:T ) := (S1, A1, . . . , ST , AT ) is a realization of state-action sequence generated by the policy πθ. It’s important to note that this gradient estimator is biased, and the bias is of order O(1 − λ) (Karimi et al., 2019). C.4 Zeroth-Order Gradient Consider the problem of minimizing the objective function V . The zeroth-order gradient method is partic- ularly valuable in scenarios where direct access to the gradient of the objective function is challenging or computationally expensive. The zeroth-order gradient oracle obtained by Gaussian smoothing (Nesterov and Spokoiny, 2017) is given by: Hθ (X) = V (θ + τX) − V (θ) τ X, (9) where τ > 0 is a smoothing parameter and X ∼ N(0, Id) a random Gaussian vector. Nesterov and Spokoiny (2017, Lemma 3) provide the bias of this estimator: ∥E [Hθ (X)] − ∇V (θ) ∥ ≤ τ 2L(d + 3)3/2. (10) The application of these zeroth-order gradient methods can be found in generative adversarial networks (Moosavi-Dezfooli et al., 2017; Chen et al., 2017). 25 "
}