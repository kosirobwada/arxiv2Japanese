{
    "optim": "Preprint: accepted to Tiny Papers @ ICLR 2024\nTOWARD LEARNING LATENT-VARIABLE REPRESEN-\nTATIONS OF MICROSTRUCTURES BY OPTIMIZING IN\nSPATIAL STATISTICS SPACE\nSayed Sajad Hashemi\nDept. of Mechanical & Industrial Engineering\nUniversity of Toronto\nsajjads.hashemi@gmail.com\nMichael Guerzhoy\nDivision of Engineering Science and\nDept. of Mechanical & Industrial Engineering\nUniversity of Toronto\nguerzhoy@cs.toronto.edu\nNoah Paulson\nArgonne National Laboratory\nnpaulson@anl.gov\nABSTRACT\nIn Materials Science, material development involves evaluating and optimizing\nthe internal structures of the material, generically referred to as microstructures.\nMicrostructures structure is stochastic, analogously to image textures. A particular\nmicrostructure can be well characterized by its spatial statistics Paulson et al.\n(2017), analogously to image texture being characterized by the response to a\nFourier-like filter bank Varma & Zisserman (2002). Material design would benefit\nfrom low-dimensional representation of microstructures Paulson et al. (2017).\nIn this work, we train a Variational Autoencoders (VAE) to produce reconstruc-\ntions of textures that preserve the spatial statistics of the original texture, while not\nnecessarily reconstructing the same image in data space. We accomplish this by\nadding a differentiable term to the cost function in order to minimize the distance\nbetween the original and the reconstruction in spatial statistics space.\nOur experiments indicate that it is possible to train a VAE that minimizes the\ndistance in spatial statistics space between the original and the reconstruction of\nsynthetic images. In future work, we will apply the same techniques to microstruc-\ntures, with the goal of obtaining low-dimensional representations of material mi-\ncrostructures.\n1\nINTRODUCTION\nComputer-aided materials design holds the promise of designing materials with favorable properties\nby enabling researchers to computationally optimize potential materials.\nMaterial microstructure is often represented as a large 3D image. 2D images are often also used. In\nresearch, both real images (e.g., X-ray) and simulated microstructures are used.\nIn “data space,” the dimensionality of the microstructure representation is the number of pix-\nels/voxels, which can be quite large. This project is motivated by the need for working in low-\ndimensional latent space when optimizing material properties. Previous work includes representing\nthe microstructure using the first principal components of the spatial statistics representation Paulson\net al. (2017).\nIn this paper, we modify the Variational Autoencoder (VAE) Kingma & Welling (2014) to learn a\nlow-dimensional representation that preserves the statistical properties of the material by making the\noriginal and the reconstruction be similar in spatial statistics space. We work with 2D images.\n1\narXiv:2402.11103v1  [cs.LG]  16 Feb 2024\nPreprint: accepted to Tiny Papers @ ICLR 2024\n2\nBACKGROUND\nIn Materials Science, the microstructure m is often characterized using spatial statistics Cecen et al.\n(2016), f (h, h′ | r) =\n1\nVol(Ωr)\nR\nΩr m(h, x)m (h′, x + r) dx.\nHere, m(h, x) is the local state h (e.g., the crystal lattice orientation) at location x in the microstruc-\nture. The spatial statistics (or correlations) can be interpreted as the set of correlations between\nall locations at distance r, in states h and h′ respectively. In practice, the spatial statistics can be\ncomputed as the inverse Fourier transform of the magnitudes of the Fourier transform of the mi-\ncrostructure Paulson et al. (2017) Einstein (1914).\nGeneration of images with a specific texture that is specified by the response to a Fourier-like filter\nbank (or indeed the Fourier transform) goes back decades Matsuyama et al. (1983). Recently, the\nliterature on neural style transfer Gatys et al. (2016) constrained the image generator with two cost\nfunctions: one that controlled the content and one that controlled the style. The style cost function\nconstrained the response to a ConvNet in the lower layers, some of which are known to be Fourier-\nresponse-like Zeiler & Fergus (2014).\nPrior work includes incorporating the loss from the neural style transfer literature to train a VAE\nthat reconstructs microstructure Sardeshmukh et al. (2021) and using a Generative Adversarial Net-\nwork (GAN) when training a VAE to produce reconstructions of microstructure that preserve the\ntexture Zhang et al. (2024). Unlike previous work, we directly optimize in spatial statistics space.\nWe aim to use limited embedding space to only store texture information.\nEmbedding microstructure in a low-dimensional space while preserving microstructure information\nis useful since it can enable researchers to rapidly explore different microstructures in the process of\nmaterials design Kalidindi (2015) McDowell et al. (2009) Sundararaghavan & Zabaras (2009).\n3\nMETHODS\nWe train a VAE to reconstruct images of texture such that, in spatial statistics space, the original and\nthe reconstruction are close. That is, the VAE loss we use is\nL = α · ||f(x) − f(xrecon)||2 + β · LossKL.\nThat is, instead of minimizing the distance between the input x and the reconstruction xrecon, we\nminimize the distance between the spatial statistics f of x and xrecon. The spatial statistics function\ncan be efficiently computed using FFT and is differentiable.\n4\nRESULTS\nOn a dataset of images of 100,000 randomly-placed vertical and horizontal lines of various lengths\n(details in Appendix A), we show that our network successfully produces reconstructions that are\nclose to the original in spatial statistics space, but not necessarily in data space, and does so better\nthan the baseline VAE. Qualitative and quantitative comparisons are in Appendix B. More samples\nare in Appendix C.\nData\nSpatial statistics\nOriginal\nReconstructed\nOriginal\nReconstructed\nOriginal Image Sample 36\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 43\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n2\nPreprint: accepted to Tiny Papers @ ICLR 2024\nREFERENCES\nAhmet Cecen, Tony Fast, and Surya R Kalidindi. Versatile algorithms for the computation of 2-point\nspatial correlations in quantifying material structure. Integrating Materials and Manufacturing\nInnovation, 5:1–15, 2016.\nAlbert Einstein. M´ethode pour la d´etermination de valeurs statistiques d’observations concernant\ndes grandeurs soumises `a des fluctuations irr´eguli`eres. Archives des Sciences, 37:254–256, 1914.\nLeon A Gatys, Alexander S Ecker, and Matthias Bethge. Image style transfer using convolutional\nneural networks. In Proceedings of the IEEE conference on computer vision and pattern recog-\nnition, pp. 2414–2423, 2016.\nSurya R Kalidindi. Hierarchical materials informatics: novel analytics for materials data. Elsevier,\n2015.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proc. ICLR, 2014.\nTakashi Matsuyama, Shu-Ichi Miura, and Makoto Nagao. Structural analysis of natural textures by\nfourier transformation. Computer vision, graphics, and image processing, 24(3):347–362, 1983.\nDavid L McDowell, Jitesh Panchal, Hae-Jin Choi, Carolyn Seepersad, Janet Allen, and Farrokh\nMistree. Integrated design of multiscale, multifunctional materials and products. Butterworth-\nHeinemann, 2009.\nNoah H Paulson, Matthew W Priddy, David L McDowell, and Surya R Kalidindi. Reduced-order\nstructure-property linkages for polycrystalline microstructures based on 2-point statistics. Acta\nMaterialia, 129:428–438, 2017.\nAvadhut Sardeshmukh, Sreedhar Reddy, BP Gautham, and Pushpak Bhattacharyya. Texturevae:\nLearning interpretable representations of material microstructures using variational autoencoders.\nIn AAAI Spring Symposium: MLPS, 2021.\nVeera Sundararaghavan and Nicholas Zabaras. A statistical learning approach for the design of\npolycrystalline materials. Statistical Analysis and Data Mining: The ASA Data Science Journal,\n1(5):306–321, 2009.\nManik Varma and Andrew Zisserman. Classifying images of materials: Achieving viewpoint and il-\nlumination independence. In Computer Vision—ECCV 2002: 7th European Conference on Com-\nputer Vision Copenhagen, Denmark, May 28–31, 2002 Proceedings, Part III 7, pp. 255–271.\nSpringer, 2002.\nMatthew D Zeiler and Rob Fergus.\nVisualizing and understanding convolutional networks.\nIn\nComputer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12,\n2014, Proceedings, Part I 13, pp. 818–833. Springer, 2014.\nYichi Zhang, Paul Seibert, Alexandra Otto, Alexander Raßloff, Marreddy Ambati, and Markus\nK¨astner. Da-vegan: Differentiably augmenting vae-gan for microstructure reconstruction from\nextremely small data sets. Computational Materials Science, 232:112661, 2024.\n3\nPreprint: accepted to Tiny Papers @ ICLR 2024\nA\nEXPERIMENTAL SETUP\nIn our experiment, we trained a VAE on a dataset consisting of 100,000 images, each sized 224x224\npixels. The VAE utilizes a pretrained ResNet-152-based encoder with a bottleneck of size 9 and\nan untrained decoder with a spatial statistics loss. The training hyper-parameters were set to the\nfollowing: a total of 1500 epochs, a learning rate of 0.001, a batch size of 32, and a KLD beta\nvalue of 1. For detailed insights into the experimental configuration, including access to the code,\nmodels, and dataset, interested readers are directed to the README.md file of the GitHub reposi-\ntory: https://github.com/spatial-stats-vae/spatial-stats-vae.git. This\nrepository provides comprehensive resources for replicating this experiment.\nz\nℒ = 𝛼 ᐧ ||𝑓(𝒙) - 𝑓(𝒙recon)||2 + 𝛽 ᐧ LossKL\nResnet-152 Encoder\nq𝜙(z|𝒙)\nDecoder \np𝛳(𝒙|z)\nμ\n𝛔\n𝒙\n𝒙recon\nSpatial Statistics \n𝑓\nSpatial Statistics \n𝑓\nFigure 1: The VAE with spatial statistics loss.\nB\nEVALUATION\nWe assessed the performance of the VAE with 100 image pairs from the test set. From the 100\nreconstructions, 98 had the correct line orientation, and 63 had the correct number of lines as the\noriginal image\nThe reconstructions are characterized by randomly arranged lines, except one line remains constant\nacross all images, varying only in size and orientation in response to the source image. The black\npixels in the reconstructions, which represent lines, exhibited an average difference in volume frac-\ntion of 17%.\nOur model demonstrated a consistent ability to accurately reconstruct images that originally con-\ntained a single line. However, its performance varied with images having more lines. Notably, in\ninstances where the original images included two or three lines, our model correctly identified the\nnumber of lines in most cases. It is also worth mentioning that when comparing the original im-\nages to those with spatial statistics most similar to the reconstructed images, there was a noticeable\nincrease in accuracy regarding the correct identification of the number of lines (see A3).\nTo determine if our model was overfitting, we analyzed 1000 reconstructed samples from both the\ntraining and validation datasets, comparing them against their respective complete sets using Mean\nSquared Error (MSE) as the metric. Our focus was on the distribution of average MSE values for\neach reconstruction, where each average denotes the mean MSE of a single reconstruction compared\nagainst all samples in the respective dataset. Our findings indicate that the distribution of the average\nMSEs for training reconstructions relative to the training set closely mirrors the distribution of the\naverage MSEs for validation reconstructions relative to the validation set (refer to Figure 4). This\nsimilarity suggests that our model did not overfit to the training data. We also analyzed the MSE\nof spatial statistics, comparing the spatial statistics of 1000 training reconstruction samples with the\nspatial statistics of the entire training set, and similarly for the validation set. The findings revealed\nsimilar results for both training and validation sets (refer to Figure 5).\nLastly, we have tabulated reconstruction examples in Appendix B. In the post-processing stage of\nthe VAE reconstructions, a threshold value of 0.05 was utilized on the reconstructed tensors. This\n4\nPreprint: accepted to Tiny Papers @ ICLR 2024\nthresholding transforms grayscale images into binary ones, a step taken to improve the clarity of the\noutput images. Crucially, this thresholding process preserves the integrity of the reconstructions,\nensuring that the resulting binary images are appropriate for further visual analysis and comparison.\nIt should be noted that the reported MSE scores below do not depend on this thresholding function,\nand that the thresholding was solely done for visual purposes. For additional details, refer to the\nAppendix A2.\nB.1\nCALCULATION OF VOLUME FRACTION DIFFERENCE\nThe volume fraction difference percentage is a metric in evaluating the accuracy of the reconstructed\nimages in terms of the black pixel content. This metric quantifies the difference in the number\nof black pixels (representing lines in images) between the original and the thresholded (or recon-\nstructed) images. The calculation of this percentage is vital for understanding the degree to which\nthe reconstructed images deviate from the original in terms of pixel composition.\nThe formula for calculating the volume fraction difference percentage is as follows:\n\f\f\f\f\noriginal black pixel count − thresholded reconstruction black pixel count\noriginal black pixel count\n\f\f\f\f × 100\n(1)\nAfter calculating the percentage difference, the value is rounded to four decimal places for precision\nand ease of interpretation. This rounding process helps in standardizing the results for comparison\nand further analysis.\nNote: The calculated volume fraction percentage is rounded to four decimal places.\n0\n5\n10\n15\n20\n25\n30\n35\n40\nValue\n0\n2\n4\n6\n8\n10\nFrequency\nFigure 2: The difference between percentage of black pixels in the input image and the percentage\nof black pixels in the reconstruction.\nB.2\nIMAGE THRESHOLDING FUNCTION\nThe threshold function is implemented to modify the image based on a predefined threshold value,\nsetting pixel values below the threshold to zero and those equal to or above the threshold to one.\nThis process is crucial for enhancing image clarity in the VAE’s reconstructed outputs.\nThe mathematical representation of the threshold function is as follows:\nthreshold pixel(pixel, threshold) =\n\u001a0\nif pixel < threshold,\n1\notherwise.\n(2)\nB.3\nNUMBER OF LINES IN ORIGINAL IMAGES VS. RECONSTRUCTIONS\nWe randomly select 100 images in the test set, compute their reconstructions, and also find the\nclosest image in spatial statistics space to the original. We then compare the number of lines in the\n5\nPreprint: accepted to Tiny Papers @ ICLR 2024\ntest image to the number of lines in the reconstruction (left) and the number of lines in the closest\nimage in spatial statistics (right).\nWe show that images that are close together in spatial statistics space have similar numbers of lines,\nand that our reconstructions produce images with numbers of lines related to the number of lines in\nthe original.\n1\n2\n3\n4\nnumber of lines in the original\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of Lines\nNumber of Lines in Original vs. Reconstructed Image\n1\n2\n3\n4\nnumber of lines in the original\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of Lines\nNumber of Lines in Original vs. Most Similar Image\nFigure 3: Line count in the reconstructed image vs original image (left), line count in image closes\nin spatial statistics space vs line in count in original image (right)\nB.4\nQUANTITATIVE RESULTS FOR ORIGINAL VAE AND OUR VAE\nWe train a “vanilla” VAE that minimizes the distance in data space between the original and the\nreconstruction (“data space loss”), and our proposed model, which minimizes the distance in spatial\nstatistics space between the original and the reconstruction (“spatial statistics loss”). Our method\nproduces reconstructions that are closer together in spatial statistics space, but further apart in data\nspace.\nThis is to be expected: we directly try to make the reconstructions be as close as possible as the\noriginal in spatial statistics space, and do not try to match the original in data space at all.\nNote the exact reconstructions that are close to the original in data space would also be close to the\noriginal in spatial statistics space. For that reason, the spatial statistics error when training with data\nspace loss is larger, but not much larger, than when training with data space loss.\n6\nPreprint: accepted to Tiny Papers @ ICLR 2024\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nMean Squared Error (MSE) of the Dataspace\n0\n20\n40\n60\n80\n100\nCount\nMedian: 2.44e-02\nMean: 2.98e-02\nData Space Loss\nValidation MSE\n0.00000\n0.00005\n0.00010\n0.00015\n0.00020\n0.00025\nMean Squared Error (MSE) of the Spatial Statistics\n0\n25\n50\n75\n100\n125\n150\n175\n200\nCount\nMedian: 2.74e-05\nMean: 5.87e-05\nData Space Loss\nValidation MSE\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nMean Squared Error (MSE) of the Dataspace\n0\n20\n40\n60\n80\n100\nCount\nMedian: 1.08e-01\nMean: 1.08e-01\nSpatial Statistics Loss\nValidation MSE\n0.00000\n0.00005\n0.00010\n0.00015\n0.00020\n0.00025\nMean Squared Error (MSE) of the Spatial Statistics\n0\n25\n50\n75\n100\n125\n150\n175\n200\nCount\nMedian: 1.74e-05\nMean: 2.91e-05\nSpatial Statistics Loss\nValidation MSE\nC\nMORE ILLUSTRATIVE EXAMPLES\nC.1\nRECONSTRUCTIONS AND THEIR SPATIAL STATISTICS\nThe following table contains illustrative instances of reconstructions from the 100 samples. The\ntable shows the original image from the dataset, and the reconstructed image by our model in the\ntwo columns on the left. The two columns on the right show the auto-correlations (aka spatial\nstatistics) of the original and the reconstructed images, respectively. We have also included the 2\nreconstruction samples (out of 100) that did have the correct line orientation at the end of the table.\n7\nPreprint: accepted to Tiny Papers @ ICLR 2024\nTable 1: Original, reconstructed, and spatial statistics difference\nData\nAuto-correlations\nOriginal\nReconstructed\nOriginal\nReconstructed\nOriginal Image Sample 18\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nOriginal Image Sample 14\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 16\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nOriginal Image Sample 9\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 12\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nOriginal Image Sample 5\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n8\nPreprint: accepted to Tiny Papers @ ICLR 2024\nTable 2: Original, reconstructed, and spatial statistics difference\nData\nAuto-correlations\nOriginal\nReconstructed\nOriginal\nReconstructed\nOriginal Image Sample 43\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 36\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 39\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 31\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nOriginal Image Sample 32\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 30\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 25\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.005\n0.010\n0.015\n0.020\n9\nPreprint: accepted to Tiny Papers @ ICLR 2024\nTable 3: Original, reconstructed, and spatial statistics difference\nData\nAuto-correlations\nOriginal\nReconstructed\nOriginal\nReconstructed\nOriginal Image Sample 26\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 24\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 19\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 23\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 17\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nOriginal Image Sample 65\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nOriginal Image Sample 75\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n100\n50\n0\n50\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\n0.02\n0.04\n0.06\n0.08\n10\nPreprint: accepted to Tiny Papers @ ICLR 2024\nC.2\nRECONSTRUCTIONS ALONG WITH MOST SIMILAR IMAGES FROM THE TRAINING SET\nThe following table contains illustrative instances of reconstructions, and the most similar images\nto them. The table shows the original image from the dataset, and the reconstructed image by our\nmodel in the two columns on the left. The third columns shows the most similar image to the\nreconstructed image from the training set based on MSE of the reconstruction and the sample from\nthe training set. The fourth column shows the most similar image to the reconstructed image from\nthe training set based on MSE of the spatial statistics of the reconstruction and the sample from the\ntraining set.\nTable 4: Original, reconstructed, and spatial statistics difference\nData\nMost similar images\nOriginal\nReconstructed\nBased on image MSE\nBased on auto-corr. MSE\nOriginal Image Sample 18\nOriginal Image Sample 18\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nOriginal Image Sample 18\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\nMost Similar Image\nOriginal Image Sample 14\nOriginal Image Sample 14\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 14\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nMost Similar Image\nOriginal Image Sample 16\nOriginal Image Sample 16\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nOriginal Image Sample 16\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\nMost Similar Image\nOriginal Image Sample 9\nOriginal Image Sample 9\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 9\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nMost Similar Image\nOriginal Image Sample 12\nOriginal Image Sample 12\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nOriginal Image Sample 12\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08\nMost Similar Image\nOriginal Image Sample 5\nOriginal Image Sample 5\nReconstructed Image\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 5\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nMost Similar Image\n11\nPreprint: accepted to Tiny Papers @ ICLR 2024\nTable 5: Original, reconstructed, and spatial statistics difference\nData\nMost similar images\nOriginal\nReconstructed\nBased on image MSE\nBased on auto-corr. MSE\nOriginal Image Sample 43\nOriginal Image Sample 43\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 43\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nMost Similar Image\nOriginal Image Sample 36\nOriginal Image Sample 36\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nOriginal Image Sample 36\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n0.07\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\nMost Similar Image\nOriginal Image Sample 39\nOriginal Image Sample 39\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 39\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nMost Similar Image\nOriginal Image Sample 31\nOriginal Image Sample 31\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nOriginal Image Sample 31\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\n0.014\n0.016\nMost Similar Image\nOriginal Image Sample 32\nOriginal Image Sample 32\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 32\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nMost Similar Image\nOriginal Image Sample 30\nOriginal Image Sample 30\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 30\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nMost Similar Image\nOriginal Image Sample 25\nOriginal Image Sample 25\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\nOriginal Image Sample 25\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\nMost Similar Image\n12\nPreprint: accepted to Tiny Papers @ ICLR 2024\nTable 6: Original, reconstructed, and spatial statistics difference\nData\nMost similar images\nOriginal\nReconstructed\nBased on image MSE\nBased on auto-corr. MSE\nOriginal Image Sample 26\nOriginal Image Sample 26\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 26\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nMost Similar Image\nOriginal Image Sample 24\nOriginal Image Sample 24\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 24\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nMost Similar Image\nOriginal Image Sample 19\nOriginal Image Sample 19\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nOriginal Image Sample 19\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\nMost Similar Image\nOriginal Image Sample 23\nOriginal Image Sample 23\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nOriginal Image Sample 23\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\n0.06\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.01\n0.02\n0.03\n0.04\n0.05\nMost Similar Image\nOriginal Image Sample 17\nOriginal Image Sample 17\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nOriginal Image Sample 17\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nMost Similar Image\nOriginal Image Sample 21\nOriginal Image Sample 21\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nOriginal Image Sample 21\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nMost Similar Image\nOriginal Image Sample 15\nOriginal Image Sample 15\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\nOriginal Image Sample 15\nReconstructed Image\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nOriginal Spatial Stats\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n100\n75\n50\n25\n0\n25\n50\n75\n100\n100\n75\n50\n25\n0\n25\n50\n75\n100\nReconstructed Spatial Stats\n0.005\n0.010\n0.015\n0.020\n0.025\nMost Similar Image\n13\n"
}