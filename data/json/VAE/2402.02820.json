{
    "optim": "Revisiting VAE for Unsupervised Time Series Anomaly Detection:\nA Frequency Perspective\nZexin Wang∗\nComputer Network Information\nCenter, Chinese Academy of Sciences\nBeijing, China\nChanghua Pei†\nComputer Network Information\nCenter, Chinese Academy of Sciences\nBeijing, China\nMinghua Ma\nMicrosoft\nBeijing, China\nXin Wang\nStony Brook University\nNew York, USA\nZhihan Li\nKuaishou Technology\nBeijing, China\nDan Pei\nTsinghua University\nBeijing, China\nSaravan Rajmohan\nMicrosoft\nRedmond, USA\nDongmei Zhang\nQingwei Lin\nMicrosoft\nBeijing, China\nHaiming Zhang\nJianhui Li\nGaogang Xie\nComputer Network Information\nCenter, Chinese Academy of Sciences\nBeijing, China\nABSTRACT\nTime series Anomaly Detection (AD) plays a crucial role for web\nsystems. Various web systems rely on time series data to monitor\nand identify anomalies in real time, as well as to initiate diagno-\nsis and remediation procedures. Variational Autoencoders (VAEs)\nhave gained popularity in recent decades due to their superior de-\nnoising capabilities, which are useful for anomaly detection. How-\never, our study reveals that VAE-based methods face challenges\nin capturing long-periodic heterogeneous patterns and detailed\nshort-periodic trends simultaneously. To address these challenges,\nwe propose Frequency-enhanced Conditional Variational Autoen-\ncoder (FCVAE), a novel unsupervised AD method for univariate\ntime series. To ensure an accurate AD, FCVAE exploits an innova-\ntive approach to concurrently integrate both the global and local\nfrequency features into the condition of Conditional Variational\nAutoencoder (CVAE) to significantly increase the accuracy of re-\nconstructing the normal data. Together with a carefully designed\n“target attention” mechanism, our approach allows the model to pick\nthe most useful information from the frequency domain for better\nshort-periodic trend construction. Our FCVAE has been evaluated\non public datasets and a large-scale cloud system, and the results\ndemonstrate that it outperforms state-of-the-art methods. This con-\nfirms the practical applicability of our approach in addressing the\nlimitations of current VAE-based anomaly detection models.\n∗Also with University of Chinese Academy of Sciences.\n†Corresponding author. Email: chpei@cnic.cn\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nWWW ’24, May 13–17, 2024, Singapore\n© 2024 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nCCS CONCEPTS\n• Computing methodologies → Machine learning; • Security\nand privacy → Systems security.\nKEYWORDS\nUnivariate time series, Anomaly detection, Conditional variational\nautoencoder, Frequency information\nACM Reference Format:\nZexin Wang, Changhua Pei, Minghua Ma, Xin Wang, Zhihan Li, Dan Pei,\nSaravan Rajmohan, Dongmei Zhang, Qingwei Lin, Haiming Zhang, Jianhui\nLi, and Gaogang Xie. 2024. Revisiting VAE for Unsupervised Time Series\nAnomaly Detection: A Frequency Perspective. In Proceedings of ACM Web\nConference 2024 (WWW ’24). ACM, New York, NY, USA, 10 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nTime series anomaly detection (AD) is ubiquitous for web systems\n[9, 11, 13, 14, 17, 21, 33, 36, 49, 50]. Numerous web systems, such as\nonline advertising systems, are monitored using a vast array of time\nseries data (e.g., conversion rate) [52]. Deploying time series AD\nalgorithms is essential for timely detecting anomalies and initiating\nsubsequent diagnosis and remediation processes.\nAnomalies are rare in real-world time series data [41], making it\ndifficult to label them and train a supervised model for anomaly de-\ntection [50]. Instead, unsupervised machine learning techniques are\ncommonly used [5, 7, 16, 18, 27, 34, 50, 54, 57–59]. These techniques\ncan be divided into two categories: prediction-based [16, 18, 59] and\nconstruction-based [5, 27, 34, 50, 54]. Both types aim to identify nor-\nmal values and compare them to actual values to detect anomalies.\nPrediction-based methods were originally developed for forecast-\ning future data points, regardless of whether they were normal\nor anomalous. However, these methods may overfit to anomalous\npatterns and underperform. On the other hand, Variational Au-\ntoEncoders (VAEs) [23], the leading construction-based approach,\nencode raw time series into a lower-dimensional latent space and\narXiv:2402.02820v1  [cs.LG]  5 Feb 2024\nWWW ’24, May 13–17, 2024, Singapore\nWang et al.\nFigure 1: Comparison of four KPI reconstruction methods\npresented in our paper, highlighting anomalies in red ③.\nThe green shade ⑤ represents the difference between the\nreconstructed values and the original values, the red shade\n② represents a long period, and the blue ellipse ④ indicates\npeaks and valleys that are not properly reconstructed, the\nblue rectangle ① will be magnified in Figure 2 for detailed\ncomparison.\nthen reconstruct them back to their original dimensions. VAEs are\nwell-suited for detecting anomalies, but existing VAE-based anom-\naly detection models have not yet reached theoretically optimal\nperformance. In this paper, we aim to re-examine the VAE model\nand improve its effectiveness in anomaly detection.\nIn order to more effectively demonstrate the challenges associ-\nated with VAE-based techniques, we provide an example in Figure 1.\nThe original curve is displayed in the first sub-figure, with anom-\nalies highlighted in red ③. The subsequent four sub-figures repre-\nsent curves reconstructed by four distinct VAE-methods, including\nour proposed method (referred to as FCVAE). The reconstruction\nerror is indicated by the green shaded area ⑤. To achieve superior\nAD performance, the reconstructed result should closely resemble\nthe original curve for normal points, while deviating significantly\nfor anomalous points ③. As evident in the figure, all VAE-based\nmethods successfully disregard the anomalies during reconstruc-\ntion. However, the reconstruction results for some normal points,\nparticularly those marked by a blue rectangle ① and ellipse ④, are\nnot satisfactory. This substantially impacts the overall performance,\nleading us to identify three key challenges that we address in the\nsubsequent sections.\n(a) Original Values\n(b) Transformed Spectrogram\nFigure 2: A detailed view of the region enclosed by a blue\nrectangle ① in Figure 1, where the shaded area represents the\nvalue range before applying a sliding window average.\nChallenge 1: Capturing similar yet heterogeneous periodic\npatterns. From Figure 1, periodic patterns can be observed in the\ncurves, with one such period emphasized by the red shaded area\n②. However, the shapes across different periods vary. As demon-\nstrated by the blue ellipse, existing VAE-based methods (as shown\nin the second sub-figure) are unable to capture these heterogeneous\npatterns effectively. This observation naturally leads to the idea\nof utilizing conditional VAE to map data into distinct Gaussian\nspaces by considering the timestamp as a condition. Unfortunately,\nas illustrated in the fourth sub-figure (Time CVAE [27]), the results\nare unsatisfactory, which we will further discuss below.\nChallenge 2: Capturing detailed trends. Reconstructing monot-\nonous patterns (i.e., trends) might appear straightforward at first\nglance. However, upon a closer examination of the local window\n(highlighted in a blue rectangle ① in Figure 1 and magnified in\nFigure 2(a), it becomes evident that existing methods fail to restore\ndetailed patterns within this time frame. In Figure 2(a), the two\ngreen lines initially overestimate the ground truth (purple curve)\nbut subsequently underestimate it for the remainder of the window.\nThis is primarily because existing methods aim to minimize the\noverall reconstruction error without focusing on “point-to-point”\ndependencies, e.g., the precise upward and downward ranges fol-\nlowing a specific point. This omission results in fluctuating recon-\nstruction outcomes (as seen in the second sub-figure). Although\nCNN attempts to model point-to-point dependencies within the\nwindow, it still produces coarse-grained fluctuations (visible in the\nthird sub-figure in Figure 1). The reason of unsatisfactory result\nof CNN-CVAE lies in Figure 2(b). Upon converting the curves re-\nconstructed by various methods (Figure 2(b)), it becomes evident\nthat the primary cause of these phenomena is the absence of\nsome frequencies (smaller amplitude of certain frequencies) in\nexisting methods, hindering the reconstruction of detailed patterns.\nThis observation logically suggests the possibility of employing\nfrequency as the conditional factor in a Conditional Variational\nAutoencoder (CVAE). Nonetheless, employing frequency as the\ncondition in CVAE presented a new challenge.\nChallenge 3: A large number of sub-frequencies make the\nsignal in condition of CVAE noisy and difficult to use. Di-\nrectly converting the entire window into the frequency-domain\nresults in numerous sub-frequencies, adding noise and obstructing\nRevisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective\nWWW ’24, May 13–17, 2024, Singapore\neffective VAE-based reconstruction. To address these challenges,\nwe sub-divide the entire window into smaller ones and propose\na target attention method to select the most useful sub-window\nfrequencies.\nIn this paper, we introduce a novel unsupervised anomaly detec-\ntion algorithm, named FCVAE (Frequency-enhanced Conditional\nVariational AutoEncoder). Different from current VAE-based anom-\naly detection methods, FCVAE innovatively incorporates both global\nand local frequency information to guide the encoding-decoding\nprocedures, that both heterogeneous periodic and detailed trend\npatterns can be effectively captured. This in turn enables more\naccurate anomaly detection. Our paper’s contributions can be sum-\nmarized as follows:\n• Our analysis of the widely-used VAE model for anomaly\ndetection reveals that existing VAE-based models fail to cap-\nture both heterogeneous periodic patterns and detailed trend\npatterns. We attribute this failure to the missing of some\nfrequency-domain information, which current methods fail\nto reconstruct.\n• Our study systematically improves the long-standing VAE by\nfocusing on frequency. Our proposed FCVAE makes the VAE-\nbased approach the state-of-the-art in anomaly detection\nonce more. This is significant because VAE-based methods\ncan inherently handle mixed anomaly-normal training data,\nwhile prediction-based methods cannot.\n• Evaluations demonstrate that our FCVAE substantially sur-\npasses state-of-the-art methods (∼40% on public datasets\nand 10% in a real-world web system in terms of F1 score).\nComprehensive ablation studies provide an in-depth analy-\nsis of the model, revealing the reasons behind its superior\nperformance.\nThe replication package for this paper, including all our data,\nsource code, and documentation, is publicly available online at\nhttps://github.com/CSTCloudOps/FCVAE.\n2\nPRELIMINARIES\n2.1\nProblem Statement\nGiven a UTS x = [𝑥0,𝑥1,𝑥2, · · · ,𝑥𝑡] and label series L = [𝑙0,𝑙1,𝑙2, · · · ,𝑙𝑡],\nwhere 𝑥𝑖 ∈ R, 𝑙𝑖 ∈ {0, 1}, and 𝑡 ∈ N. x represents the entire time\nseries data array, while 𝑥𝑖 signifies the metric value at time 𝑖. L\ndenotes the label of time series x. We define the UTS anomaly\ndetection task as follows:\nGiven a UTS x = [𝑥0,𝑥1,𝑥2, · · · ,𝑥𝑡], the objective of UTS anomaly\ndetection is to utilize the data [𝑥0,𝑥1, · · · ,𝑥𝑖−1] preceding each point\n𝑥𝑖 to predict 𝑙𝑖.\n2.2\nVAEs and CVAEs\nVAE is composed of an encoder 𝑞𝜙 (z|x) and a decoder 𝑝𝜃 (z|x). VAE\ncan be trained by using the reparameterization trick. SGVB [39] is\na commonly used training method for VAE because of its simplicity\nand effectiveness. It maximizes the evidence lower bound (ELBO) to\nsimultaneously train the reconstruction and generation capabilities\nof VAE.\nDONUT [50] proposed the modified ELBO (M-ELBO) to weaken\nthe impact of abnormal and missing data in the window on the\nreconstruction. M-ELBO is defined in (1), 𝛼𝑤 is defined as an indi-\ncator, where 𝛼𝑤 = 1 indicates 𝑥𝑤 being not anomalous or missing,\nand 𝛼𝑤 = 0 otherwise. 𝛽 is defined as (Í𝑊\n𝑤=1 𝛼𝑤)/𝑊 .\nL = E𝑞𝜙 (z|x) [\n𝑊\n∑︁\n𝑤=1\n𝛼𝑤log𝑝𝜃 (𝑥𝑤 |z) + 𝛽log𝑝𝜃 (z|x) − log𝑞𝜙 (z|x)]\n(1)\nThe overall structure of CVAE [43] is similar to VAE, and it com-\nbines conditional generative models with VAE to achieve stronger\ncontrol over the generated data. The training objective of CVAE is\ndefined as (2), where c is the condition, similar to that of VAE. FC-\nVAE which will be elaborated on later extends the CVAE framework\nby incorporating frequency information.\nL = E𝑞𝜙 (z|x,c) [log𝑝𝜃 (x|z, c) + log𝑝𝜃 (z) − log𝑞𝜙 (z|x, c)]\n(2)\n3\nMETHODOLOGY\n3.1\nFramework Overview\nFigure 3: Overall Framework.\nThe proposed algorithm for anomaly detection is illustrated in\nFigure 3 and comprises three main components: data preprocessing,\ntraining, and testing.\n3.2\nData Preprocessing\nData preprocessing encompasses standardization, filling missing\nand anomaly points, and the newly introduced method of data\naugmentation. The efficacy of data standardization and filling\nmissing and anomaly points has been substantiated in prior studies\n[27, 30, 50]. Therefore, we directly incorporate these techniques\ninto our approach.\nPrevious data augmentation methods [26, 47, 53] often added\nnormal samples, such as variations of data from the time domain or\nfrequency domain. However, for our method, we train the model by\nincorporating all the time series from the dataset together, which\nprovides sufficient pattern diversity. Furthermore, FCVAE has the\nability to extract pattern information due to the addition of fre-\nquency information, so it can handle new patterns well. Nonethe-\nless, even with the introduction of frequency information, anom-\nalies are often challenging to be effectively addressed. For the model\nto learn how to handle anomalies, we primarily focus on abnormal\ndata augmentation. In time series data, anomalies are mostly mani-\nfested as pattern mutations or value mutations (shown in Figure 6),\nWWW ’24, May 13–17, 2024, Singapore\nWang et al.\nFigure 4: FCVAE Model Architecture.\nFigure 5: Architecure of LFM.\n(a) Pattern Anomaly\n(b) Value Anomaly\nFigure 6: Examples of the two most frequent anomalies,\nwhere the red shaded area denotes the abnormal segments.\nso our data augmentation mainly targets to these two aspects. The\naugmentation on the pattern mutation is generated by combining\ntwo windows from different curves, with the junction acting as the\nanomaly. Value mutation refers to changing some points in the win-\ndow to randomly assigned abnormal values. With the augmented\nanomaly data, M-ELBO in CVAE, which will be introduced in detail\nlater, can perform well even in an unsupervised setting without\ntrue labels.\n3.3\nNetwork Architecture\nThe proposed FCVAE model is illustrated in Figure 4. It comprises\nthree main components: encoder, decoder, and a condition extrac-\ntion block that includes a global frequency information extraction\nmodule (GFM) and a local frequency information extraction module\n(LFM). Equation (3) illustrates how our model works.\n𝜇, 𝜎 = Encoder(x, LFM(x), GFM(x))\nz = Sample(𝜇, 𝜎)\n𝜇x, 𝜎x = Decoder(z, LFM(x), GFM(x))\n(3)\n3.3.1\nGFM. The GFM module (Figure 7) extracts the global fre-\nquency information using the FFT transformation (F ). However,\nnot all frequency information is useful. The frequencies resulted\nfrom the noise and anomalies in the time series data appear as long\nFigure 7: Architecure of GFM.\ntails in the frequency domain. Therefore, we employ a linear layer\nafter the FFT to filter out the useful frequency information that can\nrepresent the current window pattern. Moreover, we incorporate\na dropout layer following Fedformer [60] to enhance the model’s\nability to learn the missing frequency information.\nThe 𝑓𝑔𝑙𝑜𝑏𝑎𝑙 ∈ R1×𝑑 is calculated as (4), where d is the embedding\ndimension of the global frequency information and F means FFT.\n𝑓𝑔𝑙𝑜𝑏𝑎𝑙 = Dropout(Dense(F(x)))\n(4)\n3.3.2\nLFM. The attention mechanism [46] has been widely adopted\nin time series data processing due to its ability to dynamically pro-\ncess dependencies between different time steps and focus on impor-\ntant ones. Target attention, which is developed based on attention,\nis widely used in the field of recommendation [4]. Specifically, tar-\nget attention can weigh the features of the target domain, leading\nto more accurate domain adaptation.\nThe GFM module extracts the frequency information from the\nentire window, proving to be effective in reconstructing the data\nwithin the whole window. However, we use a window to detect\nwhether the last point is abnormal, which poses a challenge because\nthe GFM module does not provide sufficient attention to the last\npoint. This can result in a situation where the reconstruction is\nsatisfactory for part of the window but not for another part, espe-\ncially when changes in system services lead to the concept drift\nin the time series data. Even in the absence of concept drift, GFM\ncannot capture local changes as it extracts the average frequency\ninformation from the entire window; hence, the reconstruction of\nthe last key point may be unsatisfactory. Nonetheless, as previously\nmentioned, target attention can effectively address this issue, as\nRevisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective\nWWW ’24, May 13–17, 2024, Singapore\nit captures the frequency information of the entire window while\npaying a greater attention to the latest time point. Therefore, we\npropose the LFM that incorporates the target attention.\nAs depicted in Figure 5, the LFM module operates by sliding\nthe entire window x to obtain several small windows x𝑠𝑤. Subse-\nquently, FFT and frequency information extraction are applied to\neach small window. The most recent small window is used as the\nquery 𝑄 because it contains the last point that we want to detect.\nThe remaining small windows are utilized as keys 𝐾 and values 𝑉\nfor target attention. Finally, a linear layer is employed to facilitate\nthe model in learning to extract the most important and useful\npart of the local frequency information, and dropout is also applied\nto enhance the model’s ability to reconstruct some of the local\nfrequency information like GFM.\nx𝑠𝑤 = SlidingWindow(x)\n𝑄 = Select(Dense(F(x𝑠𝑤 )))\n𝐾,𝑉 = Dense(F(x𝑠𝑤 ))\n𝑓𝑙𝑜𝑐𝑎𝑙 = Dropout(FeedFawrd((𝜎 (𝑄 · 𝐾⊤) · 𝑉 ))\n(5)\nThe calculation of 𝑓𝑙𝑜𝑐𝑎𝑙 ∈ R1×𝑑 in LFM is given by (5), where\n𝑑 is the embedding dimension of the local frequency information,\nwhich is the same as that of GFM. Here, x𝑠𝑤 ∈ R𝑛×𝑘 represents a\ngroup of small windows extracted from the original window, where\n𝑘 is the dimension of the small windows and 𝑛 is the number of\nsmall windows. The Select function is employed to select the latest\nwindow as the query 𝑄 and the Dense function means dense neural\nnetwork. The softmax function 𝜎 is used to calculate the attention\nweights for the small windows.\n3.4\nTraining and Testing\nThe training process of FCVAE incorporates three key technologies:\nCVAE-based modified evidence lower bound (CM-ELBO), missing\ndata injection, as well as the newly proposed masking the last\npoint. As shown in (6), CM-ELBO is obtained by applying M-\nELBO [50] to CVAE. Missing data injection [30, 50] is a commonly\nused technique in VAE that we directly apply. We observed that an\nanomalous point in time series data manifests as an outlier value\nin the time domain. However, when the data are transformed into\nthe frequency domain, all frequency information is shifted, leading\nto a challenge. The impact of this issue will be amplified when the\nlast point is abnormal, as we specifically aim to detect the last point\ngiven the whole window. While we use the frequency enhancement\nmethod and frequency selection to mitigate this problem to some\nextent, we mask the last point as zero during the extraction of the\nfrequency condition to address this issue further.\nL = E𝑞𝜙 (z|x,c) [\n𝑊\n∑︁\n𝑤=1\n𝛼𝑤log𝑝𝜃 (𝑥𝑤 |z, c) + 𝛽log𝑝𝜃 (z) − log𝑞𝜙 (z|x, c)]\n(6)\nWhile testing, FCVAE adopts the Markov Chain Monte Carlo\n(MCMC)-based missing imputation algorithm proposed in [39] and\napplied in [30] to mitigate the impact of missing data. Since our goal\nis to detect the last point of a window, the last point is set to missing\nfor MCMC to obtain a normal value. This also allows for a better\nadaptation to the last point mask mentioned earlier. FCVAE further\nutilizes reconstruction probabilities as anomaly scores, which are\ndefined in equation (7).\nAnomalyScore = −E𝑞𝜙 (z|x,c) [log𝑝𝜃 (x|z, c)]\n(7)\n4\nEXPERIMENTS\n4.1\nExperiment Settings\n4.1.1\nDatasets. To evaluate the effectiveness of our proposed al-\ngorithm, we conducted experiments on four datasets. Yahoo [2]\nis an open data set for anomaly detection released by Yahoo lab.\nKPI [28] KPI is collected from five large Internet companies (Sougo,\neBay, Baidu, Tencent, and Ali). WSD [1] Web service dataset (WSD)\ncontains real-world KPIs collected from three top-tier Internet com-\npanies, Baidu, Sogou, and eBay, providing large-scale Web services.\nNAB [25] The Numenta Anomaly Benchmark (NAB) is an open\ndataset created by Numenta company for evaluating the perfor-\nmance of time series anomaly detection algorithms.\n4.1.2\nBaseline Methods. To benchmark our model FCVAE against\nexisting methods, we chose the following approaches for evalua-\ntion: SPOT [42], SRCNN [38], TFAD [53], DONUT [50], Informer\n[59], Anomaly-Transformer [51], AnoTransfer [54], VQRAE [22].\nSPOT represents a traditional statistical method rooted in extreme\nvalue theory. SRCNN and TFAD are supervised methods relying\non high-quality labels. Donut, VQRAE, and AnoTransfer are unsu-\npervised reconstruction-based methods utilizing VAE for normal\nvalue reconstruction. Informer is an unsupervised prediction-based\nmethod that endeavors to predict normal values using an attention\nmechanism. Anomaly-Transformer is an unsupervised anomaly\ndetection method leveraging the transformer architecture.\n4.1.3\nEvaluation Metrics. In practical applications, operators tend\nto be less concerned with point-wise anomaly detection, i.e., whether\neach individual point is classified as anomalous or not, and focus\nmore on detecting continuous anomalous segments in time series\ndata. Moreover, due to the substantial impact of anomalous seg-\nments, operators aim to identify such segments as early as possible.\nTo address these requirements, we adopt two metrics, best F1 and\ndelay F1, which are based on the works of DONUT [50] and SRCNN\n[38], respectively.\nFigure 8: Illustration of the adjustment strategy.\nBest F1 is obtained by traversing all possible thresholds for anom-\naly scores, and subsequently applying a point adjustment strategy\nto the prediction in order to compute the F1 score. Delay F1 is\nsimilar to best F1 but employs a delay point adjustment strategy to\ntransform the prediction. The adjustment strategies are illustrated\nin Figure 8, with a delay set to 1 as an example. The detector misses\nthe second anomalous segment because it takes two-time intervals\nto detect this segment, exceeding the maximum delay threshold we\nestablished. We configure the delay for all datasets to be 7, except\nfor Yahoo, where it is set to 3, and NAB, where it is set to 150. This\nWWW ’24, May 13–17, 2024, Singapore\nWang et al.\nTable 1: Performance on test data. P means precison, R means recall, F1 means best F1 and F1* means delay F1.\nYahoo\nKPI\nWSD\nNAB\nMethod\nP\nR\nF1\nP\nR\nF1*\nP\nR\nF1\nP\nR\nF1*\nP\nR\nF1\nP\nR\nF1*\nP\nR\nF1\nP\nR\nF1*\nSPOT[42]\n0.572 0.328 0.417 0.572 0.328 0.417 0.966 0.221 0.360 0.911 0.077 0.143 0.947 0.315 0.472 0.887 0.137 0.237 0.992 0.713 0.829 0.992 0.713 0.829\nSRCNN[38]\n0.268 0.236 0.251 0.219 0.181 0.198 0.673 0.944 0.786 0.617 0.753 0.678 0.093 0.903 0.170 0.028 0.361 0.053 0.825 0.832 0.828 0.460 0.766 0.575\nDONUT[50]\n0.381 0.150 0.215 0.381 0.150 0.215 0.378 0.569 0.454 0.328 0.407 0.364 0.263 0.195 0.224 0.199 0.131 0.158 0.933 0.937 0.935 0.821 0.774 0.797\nVQRAE[22]\n0.706 0.399 0.510 0.691 0.381 0.492 0.202 0.418 0.272 0.167 0.117 0.137 0.518 0.233 0.312 0.241 0.066 0.103 0.990 0.882 0.933 0.806 1.000 0.893\nAnotransfer[54]\n0.902 0.413 0.567 0.575 0.437 0.496 0.815 0.591 0.685 0.557 0.394 0.461 0.695 0.654 0.674 0.331 0.444 0.379 0.962 0.968 0.965 0.837 0.908 0.871\nInformer[59]\n0.747 0.671 0.707 0.731 0.619 0.671 0.927 0.910 0.918 0.801 0.845 0.822 0.532 0.583 0.557 0.402 0.385 0.393 0.971 0.974 0.973 0.878 0.907 0.892\nTFAD[53]\n0.884 0.739 0.805 0.883 0.734 0.802 0.684 0.834 0.752 0.650 0.714 0.680 0.541 0.750 0.628 0.431 0.482 0.455 0.749 0.719 0.734 0.265 0.233 0.248\nAnomaly-Transformer[51] 0.588 0.179 0.274 0.054 0.020 0.029 0.930 0.814 0.868 0.622 0.240 0.346 0.861 0.630 0.728 0.144 0.129 0.137 0.944 1.000 0.971 0.891 0.932 0.911\nFCVAE\n0.897 0.821 0.857 0.897 0.792 0.842 0.930 0.924 0.927 0.906 0.772 0.835 0.786 0.881 0.831 0.705 0.571 0.631 0.953 1.000 0.976 0.925 0.909 0.917\nis because the anomaly segments in Yahoo are very short, while\nin NAB, they are typically much longer, often spanning several\nhundred data points.\n4.1.4\nImplementation Details. To guarantee the widespread appli-\ncability, all the experiments described below were conducted under\nentirely unsupervised conditions, without employing any actual\nlabels (all labels are set to zero). For consistency across all methods,\nwe trained a single model for all curves within a dataset. Regard-\ning hyperparameters, we conducted a grid search to identify the\nmost effective parameters for different datasets. Additionally, we\nlater evaluated the sensitivity of these parameters to ensure robust\nperformance.\n4.2\nOverall Performance\nThe performance of FCVAE and baseline methods across the four\ndatasets is depicted in Table 1. Our method surpasses all baselines\non four datasets regarding best F1 by 6.45%, 0.98%, 14.14% and 0.31%.\nIn terms of delay F1, our method outperforms all baselines on four\ndatasets by 4.98%, 1.58%, 38.68% and 0.65%.\nThe performance of various baseline methods on the datasets\nexhibits considerable variation. For instance, SPOT [42] does not\nexcel on most datasets, as it erroneously treats extreme values as\nanomalies, whereas anomalies are not always manifested as such.\nSRCNN [38] is a reasonably proficient classifier, yet its performance\nfalls short compared to most other models. This underscores the fact\nthat implicitly extracting abnormal features is challenging. Informer\n[59] outperforms most other baselines across different datasets,\nas many anomalies exhibit notable value jumps, and prediction-\nbased methods can effectively manage this situation. However, it\nstruggles with anomalies induced by frequency changes. Anomaly-\nTransformer [51] attains commendable results on most datasets\nin terms of best F1 but demonstrates a low delay F1. It detects\nanomalies based on the relationships with nearby points, and only\nwhen the anomalous point is relatively central within the window\ncan it easily capture the correlation. Conversely, TFAD [53] achieves\nfavorable results on various datasets but exhibits a certain delay in\ndetection.\nMoreover, our method surpasses DONUT [50] and VQRAE [22]\nin terms of reconstruction-based methods. Although VQRAE [22]\nintroduces numerous modifications to the VAE, employing RNN\nto capture temporal relationships, our method still outperforms it.\nThis finding implies that for UTS anomaly detection, it is imperative\nto incorporate only key information while avoiding overloading\nthe model with superfluous data.\n4.3\nDifferent Types of Conditions in CVAE\nWe conduct experiments under identical settings to evaluate differ-\nent types of conditions. The chosen conditions encompass infor-\nmation potentially useful for time series anomaly detection within\nthe scope of our understanding, including timestamps [54], time\ndomain information, and frequency domain information. To en-\nsure consistency, we apply the same operation on the time domain\ninformation as we do on the frequency domain information.\nAs illustrated in Figure 9(a), the performance of employing the\nfrequency information as a condition surpasses that using the times-\ntamp or time domain information. This can be readily compre-\nhended, as timestamps carry limited information and typically re-\nquire one-hot encoding, resulting in sparse data representation.\nTime domain information is already incorporated in VAE, and uti-\nlizing it as a condition may lead to redundant information without\nsignificantly benefiting the reconstruction. Conversely, frequency\ninformation, as a valuable and complementary prior, render-\ning it a more effective condition for anomaly detection.\n4.4\nFrequency VAE and FACVAE\nIs CVAE the optimal strategy for harnessing the frequency infor-\nmation in anomaly detection? In this study, we compare FCVAE\nwith an improved frequency-based VAE (FVAE) model, in which the\nfrequency information is integrated into VAE along with the input\nto reconstruct the original time series. As depicted in Figure 9(b),\nFCVAE surpasses FVAE. This outcome can be attributed to two pri-\nmary reasons. Firstly, CVAE, due to its unique architecture that in-\ncorporates conditional information, intrinsically outperforms VAE\nin numerous applications. Secondly, FVAE does not fully exploit\nfrequency information. Although it incorporates this additional in-\nformation, it still lacks efficient utilization in practice, particularly\nin the decoder. Consequently, the CVAE that incorporates the\nfrequency information as a condition represents the most\neffective structure known to date.\n4.5\nGFM and LFM\nWe propose GFM and LFM to extract global and local frequency\ninformation, respectively. However, do these two modules achieve\nour intended effects through their designs? Additionally, it is worth\nnoting that GFM and LFM may overlap to some degree. Thus, we\nwould like to determine if combining the two can further enhance\nthe performance.\nWe conduct experiments and the results are depicted in Fig-\nure 9(c). It can be observed that, across the four datasets, employing\nRevisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective\nWWW ’24, May 13–17, 2024, Singapore\n(a) Performance of CVAE using different\nconditions.\n(b) Performance of different ways using\nfrequency information.\n(c) Performance of different model struc-\nture.\n(d) Performance of whether using atten-\ntion mechanism.\nFigure 9: Delay F1 score of different settings.\neither LFM or GFM in FCVAE outperforms the VAE model under\nidentical conditions of other settings except for NAB, where the\nfrequent oscillation of data results in inconsistency between the\ninformation extracted from GFM and the data value of the cur-\nrent time. For all datasets, when both LFM and GFM modules are\nutilized concurrently, they synergistically enhance each other, re-\nsulting in superior performance. Consequently, both global and\nlocal frequency information play a crucial role in detecting\nanomalies.\n4.6\nAttention Mechanism\nIt is crucial to discern whether the enhancement in LFM stems from\nthe reduced window size or the attention mechanism. Thus, we per-\nform experiments by excluding the attention operation from LFM\nwhile keeping GFM unaltered. Specifically, we utilized frequency\ninformation either from the latest small window in LFM (Latest) or\nfrom the average pooling of frequency information across all small\nwindows in LFM (Average Pooling).\nThe findings in Figure 9(d) demonstrate that without attention,\nit is impossible to attain the original performance of FCVAE since\nit is not feasible to determine the specific weight of each small\nwindow in advance. However, the attention mechanism effec-\ntively addresses this issue by assigning higher weights to\nmore informative windows.\n(a) Spectrum of small windows for\ndata in the black dashed box on the\nright.\n(b) Heatmap of LFM attention in a\nbatch. The 8-th window is the latest\nwindow.\nFigure 10: An example of attention mechanism in LFM.\nWe present a comprehensive explanation of the attention mech-\nanism in LFM using a case. A specific data segment, denoted by the\nblack dashed box in Figure 10(b), is selected and all small windows\nproduced by LFM’s sliding window module are transformed into\nthe frequency domain to obtain their spectra. As illustrated in Fig-\nure 10(a), the 5-th (green) and the 8-th (red) windows exhibit the\nhighest similarity, where the 8-th window serves as the query 𝑄 for\nour attention. Upon examining Figure 10(b), it can be observed that\nthe heat value of the 5-th window is the highest, which corresponds\nwith the findings in Figure 10(a).\n4.7\nKey Techniques in Framework\nIn this section, we evaluate the effectiveness of our novel data aug-\nmentation technique, masking the last point, and the application\nof CM-ELBO on four distinct datasets. The results are presented\nin Table 2. Based on the results, it is clear that CM-ELBO plays\nthe most crucial role in most datasets, which aligns with our ex-\npectations. This is because it can tolerate abnormal or missing\ndata to a certain extent. Furthermore, masking the last point has\na substantial impact on the results, as when an anomaly occurs\nat the last point of the window, it affects the entire frequency in-\nformation. Effectively masking this point resolves the issue and\nimproves the detection accuracy. Data augmentation, on the other\nhand, introduces some artificial anomalies to boost the performance\nof CM-ELBO, particularly in unsupervised settings.\nTable 2: Delay F1 of different settings.\nVariants\nYahoo\nKPI\nWSD\nNAB\nw/o data augment\n0.841\n0.825\n0.626\n0.904\nw/o mask last point\n0.835\n0.830\n0.534\n0.877\nw/o CM-ELBO\n0.690\n0.757\n0.435\n0.897\nFCVAE\n0.842\n0.835\n0.631\n0.917\n4.8\nParameter Sensitivity\nThe stability of a model to different parameters is an important\naspect to consider, and therefore we test the sensitivity of our model\nparameters on two datasets, KPI and WSD. We examine four aspects:\nthe dimension of the condition, the window size, the proportion of\nmissing data injection, and the proportion of data augmentation.\nWWW ’24, May 13–17, 2024, Singapore\nWang et al.\n(a) Window Size\n(b) Embedding Dimension\n(c) Missing Data Injection Rate\n(d) Data Augment Rate\nFigure 11: Delay F1 score of different settings.\nThe results, shown in Figure 11, indicate that our model can achieve\nstable and excellent results under different parameter settings.\n5\nPRODUCTION IMPACT AND EFFICIENCY\nOur FCVAE approach has been incorporated as a crucial component\nin a large-scale cloud system that caters to millions of users globally\n[6, 19, 20]. The system generates billions of time series data points\non a daily basis. The FCVAE detects anomalies in the cloud system,\nwith the primary goal of identifying any potential regressions in\nthe system that may indicate the occurrence of an incident.\nTable 3: Online performance of FCVAE in production com-\npared to legacy detector. F1 and F1∗ are defined in Table 1.\nBaseline\nFCVAE\nImprovement\nInference efficiency\nF1\nF1∗\nF1\nF1∗\nF1\nF1∗\n[points/second]\n0.66\n0.63\n0.73\n0.69\n10.9%\n11.1%\n1195.7\nTable 3 presents the online performance improvement achieved\nby employing FCVAE over a period of one year. The experiments\nwere conducted on a 24GB memory 3090 GPU. The results demon-\nstrate substantial enhancements in both Best F1 and Delay F1 com-\npared to the legacy detector. This underscores the effectiveness\nand robustness of our proposed method. Furthermore, our model is\nlightweight and highly efficient, capable of processing over 1000\ndata points within 1 second. This far exceeds the speed at which\nthe system generates new temporal points.\n6\nRELATED WORK\nTraditional statistical methods [32, 35, 37, 40, 44, 45, 56] are\nwidely used in time series anomaly detection because of their great\nadvantages in time series data processing. For example, [37] find the\nhigh frequency abnormal part of data through FFT[45] and verify\nit twice. Twitter[44] uses STL[8] to detect anomaly points. SPOT\n[42] considers that some extreme values are abnormal, therefore,\ndetects them through Extreme Value Theory [10].\nSupervised methods [24, 31, 38, 57] mostly learn the features\nof anomalies and identify them through classifiers based on the\nfeatures learned. Opprentice [31] efficiently combines the results\nof many detectors through random forest. SRCNN [38] build a\nclassifier through spectral residual [15] and CNN. Some methods\n[3, 53] obtain pseudo-labels through data augmentation to enhance\nthe learning ability.\nUnsupervised methods are mainly divided into reconstruction-\nbased and prediction-based methods. Reconstruction-based meth-\nods [5, 22, 27, 29, 50] learn low-dimensional representations and\nreconstruct the “normal patterns” of data and detect anomalies\naccording to reconstruction error. DONUT[50] proposed the modi-\nfied ELBO to enhance the capability of VAE in reconstructing the\nnormal data. Buzz[5] is the first to propose a deep generative model.\nACVAE[29] adds active learning and contrastive learning on the\nbasis of VAE. Prediction-based methods [18, 59] try to predict the\nnormal values of metrics based on historical data and detect anom-\nalies according to the prediction error. Informer[59] changes the\nrelevant mechanism of self attention to achieve better prediction ef-\nfect and efficiency. In recent years, transformer-based methods have\nbeen widely proposed. Anomaly-Transformer [51] detect anom-\nalies by comparing Kullback-Leible (KL) divergence between two\ndistributions. Some methods [48, 60] have begun to solve some\npractical problems from the frequency domain. Moerover, many\ntransfer learning methods have been proposed[12, 27, 54, 55].\n7\nCONCLUSION\nOur paper presents a novel unsupervised method for detecting\nanomalies in UTS, termed FCVAE. At the model level, we introduce\nthe frequency domain information as a condition to work with\nCVAE. To capture the frequency information more accurately, we\npropose utilizing both GFM and LFM to concurrently capture the\nfeatures from global and local frequency domains, and employing\nthe target attention to more effectively extract local information. At\nthe architecture level, we propose several new technologies, includ-\ning CM-ELBO, data augmentation and masking the last point. We\ncarry out experiments on four dataset and an online cloud system\nto evaluate our approach’s accuracy, and comprehensive ablation\nexperiments to demonstrate the effectiveness of each module.\n8\nACKNOWLEDGMENTS\nThis work was supported in part by the National Key Research and\nDevelopment Program of China (No.2021YFE0111500), in part by\nthe National Natural Science Foundation of China (No.62202445),\nin part by the State Key Program of National Natural Science of\nChina under Grant 62072264.\nRevisiting VAE for Unsupervised Time Series Anomaly Detection: A Frequency Perspective\nWWW ’24, May 13–17, 2024, Singapore\nREFERENCES\n[1] [n. d.]. WSD dataset. Available: https://github.com/anotransfer/AnoTransfer-\ndata/.\n[2] [n. d.]. Yahoo dataset. Available: https://webscope.sandbox.yahoo.com/.\n[3] Chris U Carmona, François-Xavier Aubet, Valentin Flunkert, and Jan Gasthaus.\n2021.\nNeural contextual anomaly detection for time series.\narXiv preprint\narXiv:2107.07702 (2021).\n[4] Qiwei Chen, Changhua Pei, Shanshan Lv, Chao Li, Junfeng Ge, and Wenwu Ou.\n2021. End-to-end user behavior retrieval in click-through rateprediction model.\narXiv preprint arXiv:2108.04468 (2021).\n[5] Wenxiao Chen, Haowen Xu, Zeyan Li, Dan Pei, Jie Chen, Honglin Qiao, Yang\nFeng, and Zhaogang Wang. 2019. Unsupervised anomaly detection for intricate\nkpis via adversarial training of vae. In IEEE INFOCOM 2019-IEEE Conference on\nComputer Communications. IEEE, 1891–1899.\n[6] Yinfang Chen, Huaibing Xie, Minghua Ma, Yu Kang, Xin Gao, Liu Shi, Yunjie Cao,\nXuedong Gao, Hao Fan, Ming Wen, et al. 2024. Automatic Root Cause Analysis\nvia Large Language Models for Cloud Incidents. (2024).\n[7] Yuhang Chen, Chaoyun Zhang, Minghua Ma, Yudong Liu, Ruomeng Ding, Bowen\nLi, Shilin He, Saravan Rajmohan, Qingwei Lin, and Dongmei Zhang. 2023. Imdif-\nfusion: Imputed diffusion models for multivariate time series anomaly detection.\nVLDB (2023).\n[8] Robert B Cleveland, William S Cleveland, Jean E McRae, and Irma Terpenning.\n1990. STL: A seasonal-trend decomposition. J. Off. Stat 6, 1 (1990), 3–73.\n[9] Liang Dai, Tao Lin, Chang Liu, Bo Jiang, Yanwei Liu, Zhen Xu, and Zhi-Li Zhang.\n2021. SDFVAE: Static and Dynamic Factorized VAE for Anomaly Detection of\nMultivariate CDN KPIs. In Proceedings of the Web Conference 2021 (Ljubljana,\nSlovenia) (WWW ’21). Association for Computing Machinery, New York, NY,\nUSA, 3076–3086. https://doi.org/10.1145/3442381.3450013\n[10] L de Haan and A Ferreira. 2006. Extreme Value Theory: an Introduction Springer\nScience+ Business Media. LLC, New York (2006).\n[11] Shohreh Deldari, Daniel V. Smith, Hao Xue, and Flora D. Salim. 2021. Time Series\nChange Point Detection with Self-Supervised Contrastive Predictive Coding.\nIn Proceedings of the Web Conference 2021 (Ljubljana, Slovenia) (WWW ’21).\nAssociation for Computing Machinery, New York, NY, USA, 3124–3135. https:\n//doi.org/10.1145/3442381.3449903\n[12] XiaoYan Duan, NingJiang Chen, and YongSheng Xie. 2019. Intelligent detection\nof large-scale KPI streams anomaly based on transfer learning. In Big Data: 7th\nCCF Conference, BigData 2019, Wuhan, China, September 26–28, 2019, Proceedings\n7. Springer, 366–379.\n[13] Vaibhav Ganatra, Anjaly Parayil, Supriyo Ghosh, Yu Kang, Minghua Ma, Chetan\nBansal, Suman Nath, and Jonathan Mace. 2023. Detection Is Better Than Cure:\nA Cloud Incidents Perspective. In Proceedings of the 31st ACM Joint European\nSoftware Engineering Conference and Symposium on the Foundations of Software\nEngineering. 1891–1902.\n[14] Nikou Günnemann, Stephan Günnemann, and Christos Faloutsos. 2014. Robust\nMultivariate Autoregression for Anomaly Detection in Dynamic Product Ratings.\nIn Proceedings of the 23rd International Conference on World Wide Web (Seoul,\nKorea) (WWW ’14). Association for Computing Machinery, New York, NY, USA,\n361–372. https://doi.org/10.1145/2566486.2568008\n[15] Xiaodi Hou and Liqing Zhang. 2007. Saliency detection: A spectral residual\napproach. In 2007 IEEE Conference on computer vision and pattern recognition.\nIeee, 1–8.\n[16] Siteng Huang, Donglin Wang, Xuehan Wu, and Ao Tang. 2019. Dsanet: Dual\nself-attention network for multivariate time series forecasting. In Proceedings of\nthe 28th ACM international conference on information and knowledge management.\n2129–2132.\n[17] Tao Huang, Pengfei Chen, and Ruipeng Li. 2022. A Semi-Supervised VAE Based\nActive Anomaly Detection Framework in Multivariate Time Series for Online\nSystems. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon,\nFrance) (WWW ’22). Association for Computing Machinery, New York, NY, USA,\n1797–1806. https://doi.org/10.1145/3485447.3511984\n[18] Kyle Hundman, Valentino Constantinou, Christopher Laporte, Ian Colwell, and\nTom Soderstrom. 2018. Detecting spacecraft anomalies using lstms and nonpara-\nmetric dynamic thresholding. In Proceedings of the 24th ACM SIGKDD interna-\ntional conference on knowledge discovery & data mining. 387–395.\n[19] Yuxuan Jiang, Chaoyun Zhang, Shilin He, Zhihao Yang, Minghua Ma, Si Qin,\nYu Kang, Yingnong Dang, Saravan Rajmohan, Qingwei Lin, et al. 2024. Xpert:\nEmpowering Incident Management with Query Recommendations via Large\nLanguage Models. (2024).\n[20] Pengxiang Jin, Shenglin Zhang, Minghua Ma, Haozhe Li, Yu Kang, Liqun Li,\nYudong Liu, Bo Qiao, Chaoyun Zhang, Pu Zhao, et al. 2023. Assess and Summarize:\nImprove Outage Understanding with Large Language Models. (2023).\n[21] Harshavardhan Kamarthi, Lingkai Kong, Alexander Rodriguez, Chao Zhang, and\nB Aditya Prakash. 2022. CAMul: Calibrated and Accurate Multi-View Time-Series\nForecasting. In Proceedings of the ACM Web Conference 2022 (Virtual Event, Lyon,\nFrance) (WWW ’22). Association for Computing Machinery, New York, NY, USA,\n3174–3185. https://doi.org/10.1145/3485447.3512037\n[22] Tung Kieu, Bin Yang, Chenjuan Guo, Razvan-Gabriel Cirstea, Yan Zhao, Yale\nSong, and Christian S Jensen. 2022. Anomaly detection in time series with\nrobust variational quasi-recurrent autoencoders. In 2022 IEEE 38th International\nConference on Data Engineering (ICDE). IEEE, 1342–1354.\n[23] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[24] Nikolay Laptev, Saeed Amizadeh, and Ian Flint. 2015. Generic and scalable\nframework for automated time-series anomaly detection. In Proceedings of the\n21th ACM SIGKDD international conference on knowledge discovery and data\nmining. 1939–1947.\n[25] Alexander Lavin and Subutai Ahmad. 2015. Evaluating real-time anomaly detec-\ntion algorithms–the Numenta anomaly benchmark. In 2015 IEEE 14th international\nconference on machine learning and applications (ICMLA). IEEE, 38–44.\n[26] Arthur Le Guennec, Simon Malinowski, and Romain Tavenard. 2016. Data\naugmentation for time series classification using convolutional neural networks.\nIn ECML/PKDD workshop on advanced analytics and learning on temporal data.\n[27] Zeyan Li, Wenxiao Chen, and Dan Pei. 2018. Robust and unsupervised kpi\nanomaly detection based on conditional variational autoencoder. In 2018 IEEE 37th\nInternational Performance Computing and Communications Conference (IPCCC).\nIEEE, 1–9.\n[28] Zeyan Li, Nengwen Zhao, Shenglin Zhang, Yongqian Sun, Pengfei Chen, Xidao\nWen, Minghua Ma, and Dan Pei. 2022. Constructing Large-Scale Real-World\nBenchmark Datasets for AIOps. arXiv preprint arXiv:2208.03938 (2022).\n[29] Zhihan Li, Youjian Zhao, Yitong Geng, Zhanxiang Zhao, Hanzhang Wang, Wenx-\niao Chen, Huai Jiang, Amber Vaidya, Liangfei Su, and Dan Pei. 2022. Situation-\nAware Multivariate Time Series Anomaly Detection Through Active Learning\nand Contrast VAE-Based Models in Large Distributed Systems. IEEE Journal on\nSelected Areas in Communications 40, 9 (2022), 2746–2765.\n[30] Zhihan Li, Youjian Zhao, Jiaqi Han, Ya Su, Rui Jiao, Xidao Wen, and Dan Pei. 2021.\nMultivariate time series anomaly detection and interpretation using hierarchical\ninter-metric and temporal embedding. In Proceedings of the 27th ACM SIGKDD\nconference on knowledge discovery & data mining. 3220–3230.\n[31] Dapeng Liu, Youjian Zhao, Haowen Xu, Yongqian Sun, Dan Pei, Jiao Luo, Xi-\naowei Jing, and Mei Feng. 2015. Opprentice: Towards practical and automatic\nanomaly detection through machine learning. In Proceedings of the 2015 internet\nmeasurement conference. 211–224.\n[32] Wei Lu and Ali A Ghorbani. 2008. Network anomaly detection based on wavelet\nanalysis. EURASIP Journal on Advances in Signal processing 2009 (2008), 1–16.\n[33] Xiaofeng Lu, Xiaoyu Zhang, and Pietro Lio. 2023. GAT-DNS: DNS Multivariate\nTime Series Prediction Model Based on Graph Attention Network. In Companion\nProceedings of the ACM Web Conference 2023 (Austin, TX, USA) (WWW ’23\nCompanion). Association for Computing Machinery, New York, NY, USA, 127–131.\nhttps://doi.org/10.1145/3543873.3587329\n[34] Minghua Ma, Shenglin Zhang, Junjie Chen, Jim Xu, Haozhe Li, Yongliang Lin,\nXiaohui Nie, Bo Zhou, Yong Wang, and Dan Pei. 2021. {Jump-Starting} Mul-\ntivariate Time Series Anomaly Detection for Online Service Systems. In 2021\nUSENIX Annual Technical Conference (USENIX ATC 21). 413–426.\n[35] Ajay Mahimkar, Zihui Ge, Jia Wang, Jennifer Yates, Yin Zhang, Joanne Emmons,\nBrian Huntley, and Mark Stockert. 2011. Rapid detection of maintenance induced\nchanges in service performance. In Proceedings of the Seventh COnference on\nEmerging Networking EXperiments and Technologies. 1–12.\n[36] Oded Ovadia, Oren Elisha, and Elad Yom-Tov. 2022. Detection of Infectious\nDisease Outbreaks in Search Engine Time Series Using Non-Specific Syndromic\nSurveillance with Effect-Size Filtering. In Companion Proceedings of the Web\nConference 2022 (Virtual Event, Lyon, France) (WWW ’22). Association for Com-\nputing Machinery, New York, NY, USA, 924–929. https://doi.org/10.1145/3487553.\n3524672\n[37] Faraz Rasheed, Peter Peng, Reda Alhajj, and Jon Rokne. 2009. Fourier trans-\nform based spatial outlier mining. In Intelligent Data Engineering and Automated\nLearning-IDEAL 2009: 10th International Conference, Burgos, Spain, September\n23-26, 2009. Proceedings 10. Springer, 317–324.\n[38] Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang, Xiaoyu Kou,\nTony Xing, Mao Yang, Jie Tong, and Qi Zhang. 2019. Time-series anomaly detec-\ntion service at microsoft. In Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining. 3009–3017.\n[39] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. 2014. Stochas-\ntic backpropagation and approximate inference in deep generative models. In\nInternational conference on machine learning. PMLR, 1278–1286.\n[40] Bernard Rosner. 1983. Percentage points for a generalized ESD many-outlier\nprocedure. Technometrics 25, 2 (1983), 165–172.\n[41] Lifeng Shen, Zhuocong Li, and James Kwok. 2020. Timeseries anomaly detection\nusing temporal hierarchical one-class network. Advances in Neural Information\nProcessing Systems 33 (2020), 13016–13026.\n[42] Alban Siffer, Pierre-Alain Fouque, Alexandre Termier, and Christine Largouet.\n2017. Anomaly detection in streams with extreme value theory. In Proceedings of\nthe 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining. 1067–1075.\nWWW ’24, May 13–17, 2024, Singapore\nWang et al.\n[43] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. 2015. Learning structured output\nrepresentation using deep conditional generative models. Advances in neural\ninformation processing systems 28 (2015).\n[44] Owen Vallis, Jordan Hochenbaum, and Arun Kejariwal. 2014. A novel technique\nfor long-term anomaly detection in the cloud. In 6th {USENIX} workshop on hot\ntopics in cloud computing (HotCloud 14).\n[45] Charles Van Loan. 1992. Computational frameworks for the fast Fourier transform.\nSIAM.\n[46] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing systems 30 (2017).\n[47] Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun Gao, Xue Wang,\nand Huan Xu. 2020. Time series data augmentation for deep learning: A survey.\narXiv preprint arXiv:2002.12478 (2020).\n[48] Haixu Wu, Tengge Hu, Yong Liu, Hang Zhou, Jianmin Wang, and Mingsheng\nLong. 2023. TimesNet: Temporal 2D-Variation Modeling for General Time Series\nAnalysis. In International Conference on Learning Representations.\n[49] Sihong Xie, Guan Wang, Shuyang Lin, and Philip S. Yu. 2012. Review Spam\nDetection via Time Series Pattern Discovery. In Proceedings of the 21st Inter-\nnational Conference on World Wide Web (Lyon, France) (WWW ’12 Compan-\nion). Association for Computing Machinery, New York, NY, USA, 635–636.\nhttps://doi.org/10.1145/2187980.2188164\n[50] Haowen Xu, Wenxiao Chen, Nengwen Zhao, Zeyan Li, Jiahao Bu, Zhihan Li,\nYing Liu, Youjian Zhao, Dan Pei, Yang Feng, et al. 2018. Unsupervised anomaly\ndetection via variational auto-encoder for seasonal kpis in web applications. In\nProceedings of the 2018 world wide web conference. 187–196.\n[51] Jiehui Xu, Haixu Wu, Jianmin Wang, and Mingsheng Long. 2022. Anomaly\nTransformer: Time Series Anomaly Detection with Association Discrepancy. In\nInternational Conference on Learning Representations. https://openreview.net/\nforum?id=LzQQ89U1qm_\n[52] Zhiqiang Xu, Dong Li, Weijie Zhao, Xing Shen, Tianbo Huang, Xiaoyun Li,\nand Ping Li. 2021.\nAgile and Accurate CTR Prediction Model Training for\nMassive-Scale Online Advertising Systems. In Proceedings of the 2021 Inter-\nnational Conference on Management of Data (Virtual Event, China) (SIGMOD\n’21). Association for Computing Machinery, New York, NY, USA, 2404–2409.\nhttps://doi.org/10.1145/3448016.3457236\n[53] Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. 2022. TFAD: A De-\ncomposition Time Series Anomaly Detection Architecture with Time-Frequency\nAnalysis. In Proceedings of the 31st ACM International Conference on Information\n& Knowledge Management. 2497–2507.\n[54] Shenglin Zhang, Zhenyu Zhong, Dongwen Li, Qiliang Fan, Yongqian Sun, Man\nZhu, Yuzhi Zhang, Dan Pei, Jiyan Sun, Yinlong Liu, et al. 2022. Efficient kpi\nanomaly detection through transfer learning for large-scale web services. IEEE\nJournal on Selected Areas in Communications 40, 8 (2022), 2440–2455.\n[55] Xu Zhang, Qingwei Lin, Yong Xu, Si Qin, Hongyu Zhang, Bo Qiao, Yingnong\nDang, Xinsheng Yang, Qian Cheng, Murali Chintalapati, et al. 2019. Cross-dataset\nTime Series Anomaly Detection for Cloud Systems.. In USENIX Annual Technical\nConference. 1063–1076.\n[56] Yin Zhang, Zihui Ge, Albert Greenberg, and Matthew Roughan. 2005. Network\nanomography. In Proceedings of the 5th ACM SIGCOMM conference on Internet\nMeasurement. 30–30.\n[57] Chenyu Zhao, Minghua Ma, Zhenyu Zhong, Shenglin Zhang, Zhiyuan Tan, Xiao\nXiong, LuLu Yu, Jiayi Feng, Yongqian Sun, Yuzhi Zhang, et al. 2023. Robust\nMultimodal Failure Detection for Microservice Systems. 29th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD) (2023).\n[58] Nengwen Zhao, Jing Zhu, Yao Wang, Minghua Ma, Wenchi Zhang, Dapeng Liu,\nMing Zhang, and Dan Pei. 2019. Automatic and generic periodicity adaptation\nfor kpi anomaly detection. IEEE Transactions on Network and Service Management\n16, 3 (2019), 1170–1183.\n[59] Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong,\nand Wancai Zhang. 2021. Informer: Beyond efficient transformer for long se-\nquence time-series forecasting. In Proceedings of the AAAI conference on artificial\nintelligence. 11106–11115.\n[60] Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun, and Rong Jin.\n2022. Fedformer: Frequency enhanced decomposed transformer for long-term\nseries forecasting. In International Conference on Machine Learning. PMLR, 27268–\n27286.\n"
}