{
    "optim": "Double InfoGAN for Contrastive Analysis\nFlorence Carton1\nRobin Louiset1,2\nPietro Gori1\n1LTCI, T´el´ecom Paris, IPParis, France\n2NeuroSpin, CEA, Universite Paris-Saclay, France\nAbstract\nContrastive Analysis (CA) deals with the dis-\ncovery of what is common and what is dis-\ntinctive of a target domain compared to a\nbackground one. This is of great interest in\nmany applications, such as medical imaging.\nCurrent state-of-the-art (SOTA) methods are\nlatent variable models based on VAE (CA-\nVAEs). However, they all either ignore im-\nportant constraints or they don’t enforce fun-\ndamental assumptions. This may lead to sub-\noptimal solutions where distinctive factors are\nmistaken for common ones (or viceversa). Fur-\nthermore, the generated images have a rather\npoor quality, typical of VAEs, decreasing their\ninterpretability and usefulness. Here, we pro-\npose Double InfoGAN, the first GAN based\nmethod for CA that leverages the high-quality\nsynthesis of GAN and the separation power of\nInfoGAN. Experimental results on four visual\ndatasets, from simple synthetic examples to\ncomplex medical images, show that the pro-\nposed method outperforms SOTA CA-VAEs\nin terms of latent separation and image qual-\nity. Datasets and code are available online1.\n1\nIntroduction\nLearning disentangled generative factors in an unsuper-\nvised way has gathered much attention lately since it is\nof interest in many domains, such as medical imaging.\nMost approaches look for factors that capture distinct,\nnoticeable and semantically meaningful variations in\none dataset (e.g., presence of hat or glasses in CelebA).\nAuthors usually propose well adapted regularizations,\nwhich may promote, for instance, ”uncorrelatedness”\n(FactorVAE [Kim and Mnih, 2018]) or ”informative-\nness” (InfoGAN [Chen et al., 2016]).\nProceedings of the 27th International Conference on Artifi-\ncial Intelligence and Statistics (AISTATS) 2024, Valencia,\nSpain. PMLR: Volume TBD. Copyright 2024 by the au-\nthor(s).\nIn this paper, we focus on a related but differ-\nent\nproblem,\nthat has been named Contrastive\nAnalysis\n(CA)\n[Zou et al., 2013,\nAbid et al., 2018,\nWeinberger et al., 2022]. We wish to discover in an\nunsupervised way what is added or modified on a\ntarget dataset compared to a control (or background)\ndataset, as well as what is common between the two\ndomains. For example, in medical imaging, one would\nlike to discover the salient variations characterizing\na pathology that are only present in a population of\npatients and not in a population of healthy controls.\nBoth the target (patients) and the background\n(healthy) datasets are supposed to share uninteresting\n(healthy) variations.\nThe goal is thus to identify\nand separate the generative factors common to both\npopulations from the ones distinctive (i.e., specific)\nonly of the target dataset.\nThe\nmost\nrecent\nCA\nmethods\nare\nbased\non\nthe\nVariational\nAutoEncoders\n(VAE)\n[Kingma and Welling, 2014]\nmodel\nand\nthey\nare\ncalled Contrastive VAE (CA-VAE). These methods\nassume that samples from the target dataset are\ngenerated using two sets of latent factors, common\nz and salient s, whereas samples from the control\ndataset are generated using only the common z\nfactors.\nThe salient factors s should therefore\nmodel the specific patterns of variations of the\ntarget dataset.\nAll these methods share the same\ngeneral mathematical formulation,\nwhich derives\nfrom the standard VAE. However, they all either\nignore a term of the proposed loss (e.g., KL loss in\n[Abid and Zou, 2019, Ruiz et al., 2019]) or they don’t\nenforce important assumptions (e.g., independence\nbetween z and s in [Weinberger et al., 2022]), which\nmay lead to sub-optimal solutions where salient\nfactors are mistaken for common ones (or viceversa).\nFurthermore, they all share a typical downside of\nVAEs: a blurry and poor quality image generation.\nFor these reasons, we propose Double InfoGAN :\na novel Contrastive method which leverages the\nhigh-quality\nsynthesis\nof\nGenerative\nAdversarial\nNetworks (GANs) [Goodfellow et al., 2014] and the\nseparation power of InfoGAN [Chen et al., 2016]. To\nthe best of our knowledge, this is the first GAN\narXiv:2401.17776v1  [cs.CV]  31 Jan 2024\nDouble InfoGAN for Contrastive Analysis\nFigure 1:\nTwo examples of datasets for Con-\ntrastive Analysis.\nFirst figure:\nBrats dataset\n[Menze et al., 2014].\nTop:\nMRI images of healthy\nbrains (control dataset). Bottom: MRI images of brains\nwith tumor (target dataset). Second figure: CelebA\ndataset. Top: control dataset with regular faces (no\nsmile, no glasses). Bottom: target dataset that con-\ntains smiling faces with glasses.\nbased method proposed in the context of Contrastive\nAnalysis. The main contributions of this paper are:\n• The first GAN based method for Contrastive Analysis\n(CA) which allows high-quality synthesis.\n• A new regularization term for CA, inspired by Info-\nGAN.\n• Two new losses for an accurate separation and esti-\nmate of the common and salient generative factors.\n• Extensive experimental results on four visual datasets,\nfrom synthetic to complex ones, show that the proposed\nmethod outperforms SOTA CA-VAE methods in terms\nof latent separation and image quality. Datasets and\ncode are available online.1\n2\nRelated Work\nSeparating\ncommon\nfrom\ndistinctive\nlatent\nrep-\nresentations has become an active research area\nin\nseveral\nfields,\nsuch\nas\ndomain\nadaptation\n(DA)\n[Ganin et al., 2017,\nHoffman et al., 2018]\nand\nimage-to-image\ntranslation\n(IMI)\n[Zhu et al., 2017,\nIsola et al., 2017,\nLiu et al., 2017,\nLee et al., 2018, Huang et al., 2018].\nDA seeks to transfer a classifier from a source domain,\nwith many labelled samples, to a different target\ndomain, which has few or no labelled data. As shown\nin [Ganin et al., 2017], an effective classifier should\nuse shared features that cannot discriminate between\nthe two domains.\nThe goal of IMI is instead to\nestimate a transformation that maps images from\nthe source domain to the target one by disentangling\nand controlling high-level visual attributes (style,\ngender, objects) [Lee et al., 2018]. The main difference\nbetween these methods and the proposed one is the\n1https://github.com/Florence-C/Double_InfoGAN.\ngit\nobjective.\nOur goal is to statistically analyze two\ndomains (e.g., healthy and patients) looking for latent\nrepresentations that generate the background (e.g.,\nhealthy) and target (e.g., pathological) content. We\ndo not seek to transfer a classifier or to map an image\nto a different distribution. We wish, for instance, to\ngenerate new images and not only to translate them\nto another domain.\nAnother important difference\nis that we do not want to encode only a particular\ndistinctive\nattribute\n(e.g.,\nstyle\n[Ma et al., 2019],\ngender) but all distinctive variations of the target\ndomain with respect to the background one.\nFur-\nthermore, we do not plan to use a weight sharing\nconstraint [Lee et al., 2018, Liu et al., 2017], or other\narchitectural constraints, which assume that the main\ndifferences are, for instance, only in the low-level\nfeatures (color, texture, edges, etc.).\nOur work is also close to unsupervised anomaly\ndetection\n[Guillon et al., 2021, Baur et al., 2021,\nPANG et al., 2022, V´etil et al., 2022], which is usually\ncomposed of two steps.\nFirst, the distribution of\nthe background (control) domain is learned, using\ndeep generative models.\nThen, or at the same\ntime, a discriminator is optimized to detect the\ntarget (anomalous) samples.\nBy looking at the\nreconstruction errors [Guillon et al., 2021], attention\nscores [Venkataramanan et al., 2020], visual saliency\n[Kimura et al., 2020]\nor\nother\nfeatures,\none\ncan\nunderstand which are the salient patterns of the target\n(anomalous) domain.\nEven if this strategy can be\nhighly interpretable, the goal is to spot an anomalous\nsample and not to model the latent factors that\ngenerate the anomalous patterns.\nAnother class of methods,\nmainly used in the\nfields of data integration and data fusion, are the\nprojection based latent variables approaches,\nsuch\nas\n2B-PLS,\n02PLS,\nDISCO-SCA,\nGSVD,\nJIVE\n[Feng et al., 2018,\nRohlf and Corti, 2000,\nDeun et al., 2012,\nYu et al., 2017,\nTrygg, 2002,\nSmilde et al., 2017]. Contrary to these methods, we\ndo not use only linear transformations, but we leverage\nthe capacity of deep learning to estimate non-linear\nmappings.\nIn parallel, research on disentanglement has been\ndeveloped, making it possible to modify a single and\nsemantically meaningful pattern of the image (e.g.,\nperson’s smile, gender), by varying only one component\nof the latent representation [Kim and Mnih, 2018].\nAs shown in [Locatello et al., 2019], the unsupervised\nlearning of disentangled representations is theoretically\nimpossible from i.i.d. samples without inductive biases\n[Higgins et al., 2017, Chen et al., 2016], weak labels\n[Shu et al., 2020, Locatello et al., 2020], or supervision\n[Lample et al., 2017, Choi et al., 2018, He et al., 2019,\nShi et al., 2021, Joy et al., 2021]. These methods have\nF. Carton, R. Louiset, P. Gori\nall focused on the latent generative factors of a single\ndataset, and their goal is thus different from ours.\nWith\na\ndifferent\nperspective,\nmethods\nstem-\nming\nfrom\nthe\nrecent\nContrastive\nAnalysis\n(CA)\nsetting\n[Zou et al., 2013,\nAbid et al., 2018,\nTu et al., 2021,\nRuiz et al., 2019,\nZou et al., 2022,\nAbid and Zou, 2019,\nChoudhuri et al., 2019,\nSeverson et al., 2019, Weinberger et al., 2022] mainly\nuse variational autoencoders (VAE) to model la-\ntent variations only present among target samples\nand not in the background dataset.\nSimilarly, in\n[Benaim et al., 2019], authors used standard autoen-\ncoders to estimate common latent patterns between\ntwo domains as well as patterns unique to each\ndomain. Being based on auto-encoders, this method\ncannot sample in the latent space (i.e., no new image\ngeneration) and its goal is to map sample images from\none domain to the other, as in IMI. Another related\nmethod is NestedVAE [Vowels et al., 2020], whose\ngoal is bias reduction by estimating common factors\nbetween visual domains using paired data. Here, we\nwish to use unpaired datasets.\nLastly, CA is different from style vs. content sepa-\nration and style transfer. In particular, in recent\nworks [Kazemi et al., 2019, von K¨ugelgen et al., 2021],\ncontent usually refers to the invariant generative\nfactors across samples and views (i.e., transforma-\ntions/augmentations of a sample), while style refers to\nthe varying factors. Content and style thus depend\non the chosen semantic-invariant transformations, and\nthey are defined for a single dataset. In CA, we do\nnot necessarily need transformations or views, and we\njointly analyze two different datasets.\n3\nBackground\nInfoGAN In [Chen et al., 2016], differently from stan-\ndard GAN [Goodfellow et al., 2014], authors propose\na new method, called InfoGAN, where they decompose\nthe input noise vector of GANs into two parts: 1) z,\nwhich is considered as a nuisance and incompressible\nnoise and 2) c, which should model the salient semantic\nfeatures of the data distribution. The generator of this\nnew model, G(z, c), takes as input both z and c to\ngenerate samples x. As shown in [Chen et al., 2016],\nwithout regularisation, the generator G may ignore the\nadditional code c or find a trivial (and useless) solu-\ntion. To this end, authors propose to regularize the\nestimate of G by maximizing the mutual information\nI(c; x) between c and x ∼ G(z, c). Maximum I is\nobtained when c and x are completely dependent and\none becomes completely redundant with the knowledge\nof the other. This should increase the informativeness\nof c, namely all salient semantic information should\nbe in c and not in z, which should only account for\nadditional randomness (i.e., noise). Authors propose to\nmaximize a lower bound of I(c; x) by defining an aux-\niliary distribution Q(c|x), parameterized as a neural\nnetwork, to approximate P(c|x):\nI(c; x) ≥ Ez∼P (z),c∼P (c),x∼P (x|c,z) log(Q(c|x)) + H(c)\n(1)\nMore mathematical details in the Supplementary.\nContrastive\nVAE\n(CA-VAE)\nIn\nthis\nsection,\nwe\npresent\nthe\nCA-VAE\nmodels\n[Choudhuri et al., 2019,\nSeverson et al., 2019,\nAbid and Zou, 2019,\nRuiz et al., 2019,\nZou et al., 2022,\nWeinberger et al., 2022].\nLet\nX = {xi} and Y\n= {yj} be the background (or\ncontrol) and target data-sets of images respectively.\nBoth {xi} and {yj} are assumed to be i.i.d.\nfrom\ntwo different and unknown distributions (P(x) and\nP(y)) that depend on a pair of latent variables (z, s).\nHere, s is assumed to capture the salient generative\nfactors proper only to Y whereas z should describe the\ncommon generative factors between X and Y . The\ngenerative models (i.e. same decoder with parameters\nθ) are: xi ∼ Pθ(x|zi, si = s′) and yj ∼ Pθ(yj|zj, sj),\nwhere the salient factors si of X are fixed to a\nconstant value s′ (e.g., s′ = 0), thus enforcing z\nto fully encode alone X. The conditional posterior\ndistributions are approximated using another neural\nnetwork (i.e.\nencoder with parameters ϕ) shared\nbetween X and Y , Qϕ(zi, si|xi) and Qϕ(zj, sj|yj),\nwhich are usually assumed to be conditional inde-\npendent:\nQϕ(z, s|·) = Qϕ(z|·)Qϕ(s|·).\nThe latent\ngenerative factors (z, s) are also usually assumed to be\nindependent (i.e., P·(z, s) = P·(z)P·(s)). The common\nfactor z should follow the same prior distribution in X\nand Y (e.g., Px(z) = Py(z) = N(z; 0, I)). The salient\nfactor s follows instead a different prior distribution\nbetween X and Y , such as Py(s) = N(s; 0, I) and\nPx(s) = δ(s = s′), the Dirac distribution centered at s′.\nBased on this generative latent variable model, one can\nderive a lower bound of the marginal log likelihood:\nlog P(x) ≥EQϕ(z|x)Qϕ(s|x) log Pθ(x|z, s)−\nKL(Qϕ(z|x)||px(z)) − KL(Qϕ(s|x)||px(s))\n(2)\nand\nsimilarly\nfor\nlog P(y).\nAll\nexisting\nCA-\nVAE methods share this mathematical framework.\nThey mainly differ for optimization or architec-\ntural choices and new added losses.\nHowever,\nnone of these methods explicitly enforces the inde-\npendence between common and salient latent fac-\ntors2 and most of them ignore the KL divergence\n2[Abid and Zou, 2019]\nproposed\nto\nminimize\nthe\ntotal\ncorrelation\n(TC)\nbetween\nqϕz,ϕs(z, s|x)\nand\nqϕz(z|x)qϕs(s|x)\nvia\nthe\ndensity-ratio\ntrick\nDouble InfoGAN for Contrastive Analysis\nFigure 2: Double InfoGAN. Our model takes two\ninputs: z (common factors) and s (salient factors).\nThe generator G produces fake images that, together\nwith the real images, are passed to a discriminator and\nencoder. The discriminator has two modules: D for\ndetecting real from fake images, and C for classyfing\nimages in the correct domain (i.e., X or Y ).\nThe\nencoder Q has two modules, Qz and Qs, to reconstruct\nthe latent factors (z, s). D, C and Q share all layers\nbut the last one.\nterm about py(s) (except [Choudhuri et al., 2019] and\n[Weinberger et al., 2022]), thus allowing a possible in-\nformation leakage between salient and common factors,\nas discussed in [Weinberger et al., 2022]. Furthermore,\nthe quality of the generated images is rather poor.\n4\nProposed method - Double InfoGAN\nModel In Double InfoGAN, we use a generative model\nsimilar to the one proposed in CA-VAE but within\nthe framework of InfoGAN. We suppose that the back-\nground images {xi}\niid\n∼ P(x) and the target images\n{yj}\niid\n∼ P(y), where P(x) and P(y) are unknown and\ndepend on a pair of latent variables (z ∈ RL, s ∈ RM).\nDifferently from InfoGAN, and similarly to CA-VAE, z\nshould now capture the generative factors common to\nboth X and Y whereas s the salient factors proper only\nto Y . As in GAN [Goodfellow et al., 2014], we intro-\nduce a generator G and a discriminator. The generator\nG should generate samples that are indistinguishable\nfrom the true ones, whereas the discriminator is divided\ninto two modules. The first (and standard) one D is\ntrained to discriminate between fake and real samples.\nThe second module C is trained to correctly classify\nreal samples (i.e., X or Y ). As in InfoGAN, we also\nuse one encoder, divided into two modules, Qz and\nQs, to reconstruct the latent factors z and s. The dis-\ncriminator, D and C, and the encoder, Qz and Qs, are\nparametrized as neural networks, that share all layers\nbut the output one.\n[Kim and Mnih, 2018],\nbut\ntheir\nimplementation\nis\ninaccurate since they don’t use an independent optimizer.\nLet x = G(z, s = s′) and y = G(z, s) be the generated\nsamples. We suppose, and force it in practice, that the\nlatent variables z = {z1, ..., zL} and s = {s1, ..., sM}\nare independent and follow a factorized distribution:\nP(z) = QL\ni=1 P(cz) and P(s) = QM\nj=1 P(sj), for X and\nY . The total cost function is:\nmin\nG,Qz,QsCmax\nD\nwAdvLAdv(G, D) + wClassLcl(G, C)−\nwInfoLInfo(G, Qz, Qs) + wImLIm(G, Qz, Qs)\n(3)\nIn the following, we will describe each term.\nAdversarial\nGAN\nLoss\nAs\nin\n[Goodfellow et al., 2014],\nG\nand\nD\nare\ntrained\ntogether in a min-max\ngame using the original\nnonsaturating GAN (NSGAN) formulation:\nLAdv(D, G) = wbg\n\u0010\n−ExR∼P (xR)\n\u0002\nlog(D(xR)\n\u0003\n−\nEz∼Px(z)\n\u0002\nlog(1 − (D(G(z, 0))))\n\u0003\u0011\n+ wt\n\u0010\n−EyR∼P (yR)\n\u0002\nlog(D(yR)\n\u0003\n− Ez,s∼Py(z,s)\n\u0002\nlog(1 − (D(G(z, s))))\n\u0003\u0011\n(4)\nwhere D(I) indicates the probability that I is real\nor fake and xR ∼ P(xR) and yR ∼ P(yR) are real\nimages.\nFurthermore, we choose the same factor-\nized prior distribution P(z) for both X and Y (i.e.,\nPx(z) = Py(z) = P(z)), namely a Gaussian N(0, 1).\nWe also tested a uniform distribution U[−1,1] but the re-\nsults were slightly worse. Instead, about P(s), it should\nbe different between X and Y . We use a Dirac delta\ndistribution centered at 0 for X (i.e., Px(s) = δ(s = 0))\nand we have tested several distributions for Py(s). De-\npending on the data and related assumptions, one\ncould use, for instance, a factorized uniform distribu-\ntion, U(0,1], or a factorized Gaussian N(0, 1) (ignoring\nthe samples equal to 0). In our experiments, results\nwere slightly better when using N(0, 1).\nClass Loss\nTo make sure that generated images\nbelong to the correct class, we propose to add a second\ndiscriminator module C. It is trained on real images\nto predict the correct class: X or Y . At the same time,\nG is trained to produce images correctly classified by\nC. We (arbitrarily) assign 0 (resp. 1) for class X (resp.\nY ) and use the binary cross entropy (B). The loss is:\nLcl(C) =ExR∼P (xR)\n\u0002\nB(C(xR), 0)\n\u0003\n+ EyR∼P (yR)\n\u0002\nB(C(yR), 1)\n\u0003\nLcl(G) =Ez∼Px(z)[B(C(G(z, 0)), 0)]\n+ Ez,s∼Py(z,s)[B(C(G(z, s)), 1)]\n(5)\nInfo Loss\nSimilarly to InfoGAN, we propose two\nregularization terms based on mutual information,\nI((z, s); y) and I((z, s = s′); x), to encourage infor-\nmative latent codes. However, in our case, these two\nF. Carton, R. Louiset, P. Gori\nterms are not added to disentangle between informative\nand nuisance generative factors, but to enforce the sep-\naration between common and salient factors. Indeed,\nthe maximization of these two regularity terms should\nenforce z to fully encode X and at the same time to\nbe informative for the generation of Y . In parallel, s\nshould only encode distinctive semantic information of\nY . Please note that the inclusion of two other nuisance\nfactors, similarly to InfoGAN, describing the incom-\npressible noise of X and Y , would make the analysis\nmore complex (i.e., additional regularity terms) since\nthey should not model the common nor the salient\ngenerative factors.\nSince z and s are independent by construction, the\nmutual information I((z, s); ·) can be decomposed into\nthe sum of the two mutual information I(z; ·) + I(s; ·).\nThus, similarly to InfoGAN (see Eq. 1), we can re-\ntrieve four lower bounds.\nAs in [Chen et al., 2016,\nLin et al., 2020], to promote stability and efficiency,\nwe model the two auxiliary distributions, Qz and Qs,\nas factorized distributions. Beside a factorized Gaus-\nsian distribution with identity covariance, we have also\ntested a factorized Laplace distribution L(µ, b) with\nb = 1. This brings to a l1 reconstruction loss instead\nof a standard l2, and showed better performance in\npractice.\nFinally, to better train Qs, and since we know that s\nshould be equal to 0 for real images of domain X (i.e.,\nxR ∼ P(xR)), we also add as regularization the lower\nbound of the mutual information I(s; xR). As before,\nwe fix Px(s) = δ(s = 0). The sum of these five lower\nbounds defines the LInfo loss:\nLInfo(G, Qz, Qs) = wbgEz∼Py(z)\n\u0002\nwz\nInfo|(Qz(G(z, 0)) − z|\n+ ws\nInfo|Qs(G(z, 0)) − 0|\n\u0003\n+ wtEz,s∼Py(z,s)\n\u0002\nwz\nInfo|(Qz(G(z, s)) − z|\n+ ws\nInfo|Qs(G(z, s)) − s|\n\u0003\n+ wreal\nInfoExR∼P (xR)\n\u0002\n|(Qs(xR)) − 0|\n\u0003\n(6)\nImage reconstruction loss\nDifferently from usual\nGAN models, we also propose to maximize the log-\nlikelihood log(P(y)) (and log(P(x))) of the generated\nimages based on the proposed model. Indeed, no likeli-\nhood is generally available for optimizing the generator\nG in a GAN model [Goodfellow et al., 2014]. However,\nhere, given a real image yR (or xR), we can use the\nauxiliary encoder Q = (Qs, Qz) to estimate the latent\nfactors ˆz and ˆs that should generate yR (or xR) and\nthen maximize (an approximation) of the log-likelihood\nof the generated images y = G(ˆz, ˆs) (or x = G(ˆz, 0)):\nlog P(y) ≥ EyR∼P (yR),(z,s)∼Q(z,s|yR) log P(y|z, s, yR)\n− EyR∼P (yR)KL(Q(z, s|yR)||P(z, s|yR))\n(7)\nWe notice that the second term should tend towards 0\nduring training thanks to the previous Info Loss.3 We\ncan thus approximate log P(y) by computing only the\nleft term and modeling P(y|z, s, yR) as a Laplace dis-\ntribution L(µ, b) with b = 1. We use a Laplace distribu-\ntion, instead of a Gaussian one, since it has been shown,\nfor instance in [Isola et al., 2017], that a l1-loss encour-\nages sharper and better image reconstructions than a\nl2-loss. Similar computations can be done for log P(x).\nWe define LIm(G, Qz, Qs) = log P(x) + log P(y):\nLIm(G, Qz, Qs) = wbgExR∼P (xR),ˆz=Qz(xR)\n\u0002\n|G(ˆz, 0) − xR|\n\u0003\n+ wtEyR∼P (yR),ˆz,ˆs=Q(yR)\n\u0002\n|G(ˆz, ˆs) − yR|\n\u0003\n(8)\n5\nResults\nIn this section, we present the results of our model on\nfour different visual datasets. Three of them (CelebA\nwith accessories [Weinberger et al., 2022], Cifar-10-\nMNIST and dSprites-MNIST) have been conceived\nfor the CA setting, giving us the possibility to qualita-\ntively and quantitatively evaluate the performance of\nour model. We compare it with two SOTA Contrastive\nVAE algorithms (cVAE [Abid and Zou, 2019] and MM-\ncVAE [Weinberger et al., 2022]) that had the best re-\nsults in [Weinberger et al., 2022].4 The fourth dataset,\nBrats [Menze et al., 2014], comprises T1-w MR brain\nimages of healthy subject and patient with brain tu-\nmors, and is used for qualitative evaluation.\nFor quantitative evaluation, we use the fact that the in-\nformation about attributes (e.g. glasses/hats in CelebA,\nMNIST digits, Cifar objects) should be present either\nin the common or in the salient space. Given a test set\nof images, we first use Q to reconstruct ˆz and ˆs and\nthen train a classifier on them to predict the attribute\npresence. By evaluating the discriminative power of the\nclassifier, we can understand whether the information\nabout the attributes has been put in the common or\nsalient latent space by the method.\nQualitatively, the model can be evaluated by: 1) look-\ning at the image reconstruction, 2) generating new\nimages (sampling different salient features) and 3) swap-\nping salient features. Given two real images xR ∈ X\nand yR ∈ Y , we can first estimate the latent factors\n( ˆ\nzX, ˆ\nsX) and ( ˆzy, ˆ\nsY ), that should have generated xR\nand yR, using Q. Then, we can swap the estimated\nsalient features ˆ\nsX and ˆ\nsY , and re-generate the images\nG( ˆ\nzX, ˆ\nsY ) and G( ˆ\nzY , ˆ\nsX).\nImplementation details about the architectures and\nhyper-parameters used in the different experiments can\nbe found in the Supplementary.\n3Lower bounds become tight as Q resembles the true P.\n4We use the code provided by the authors of MM-cVAE.\nDouble InfoGAN for Contrastive Analysis\nˆsy ↑\nˆzy ↓\nBest\nAverage\nWorst\nBest\nAverage\nWorst\ncVAE*\n0.84\n0.82\n0.81\n0.78\n0.80\n0.81\nMM-cVAE*\n0.85\n0.82\nnan\n0.72\n0.76\nnan\ndouble InfoGAN\n0.95\n0.95\n0.94\n0.69\n0.71\n0.73\nTable 1: 5-fold average accuracy on Target CelebA\n(glasses vs hat). Std is always ≤ 0.01, so we don’t\nreport it for clarity. Best results in bold.\n*: Results are different from [Weinberger et al., 2022] where no ex-\nternal test set is used.\nCelebA with accessories\nWe use the dataset\nbased on CelebA [Liu et al., 2015b] presented in\n[Weinberger et al., 2022], where background images X\nare faces with neither hat of glasses, and target images\nY are faces with hat or glasses. We use 20,000 im-\nages for training, 10,000 background and 10,000 target,\nequally divided between glasses and hat. To evaluate\nthe target class separation, we create a test set with im-\nages (5,000 with glasses and 5,000 with hat) never seen\nduring training and compute the accuracy of a logistic\nregression (with 5-fold cross validation) on the recon-\nstructed latent factors ˆsy = Qs(s|y) and ˆzy = Qz(z|y).\nResults are available in Table 1. Please note that the\nevaluation protocol in [Weinberger et al., 2022] was dif-\nferent since authors did not use an external test set.\nFor a fair comparison, we run all methods 5 times\n(with different random seeds) for 500 epochs, and re-\nported the highest (best), average and lowest (worst)\nscores.\nExtensive results are presented in the Sup-\nplementary. It is interesting to underline that MM-\ncVAE [Weinberger et al., 2022] does not converge at\nevery run. We have observed a divergence of the KL\nloss in about 10% of the trainings, which led to a con-\nvergence failure. We have used the original architecture\nof the MM-cVAE paper [Weinberger et al., 2022] to re-\nproduce their results.\nWe provide qualitative results in Fig. 3 with image\nreconstruction and salient feature swap. Please note\nthat this would not be possible with SOTA IMI meth-\nods, such as CycleGAN [Zhu et al., 2017] and MU-\nNIT [Huang et al., 2018], not conceived for the CA set-\nting. First of all, we observe that our model produces\nimages of better quality than MM-cVAE, although\nthis could probably be improved using larger GAN\narchitectures, such as BigGAN [Brock et al., 2019] or\nStyleGAN [Karras et al., 2019]. From a quantitative\npoint of view, our model obtains an average Inception\nScore (IS) equal to 1.63 ± 0.03 for background images\nand 2.66 ± 0.02 for target images, whereas MM-cVAE\nobtains 1.43 ± 0.03 and 1.44 ± 0.01 for background\nand target images respectively. Similar results were\nobtained using the Fr´echet inception distance (FID).\nIt is interesting to notice that our model, contrarily to\nMM-cVAE, preserves the characteristics of the salient\nelements, such as the opacity of the glasses. Both mod-\nOriginal\nReconstruction\nSwap\nMM-\ndouble\nMM-\ndouble\ncVAE\nInfoGAN\ncVAE\nInfoGAN\nFigure 3: Image reconstruction and swap with the\nCelebA with accessories dataset.\nels struggle to preserve the shape of the original hat,\nalthough our method tends to generate a better hat\nbut based on the hairstyle of the person.\nIn Fig. 4, we present qualitative results where we gener-\nate images fixing a z in each row and using different s\n(0 for X, ̸= 0 for Y ). We can see that there is indeed a\nchange of domain, and that the model generates a wide\nvariety of images. When switching from background\nX to target Y , the characteristics of the person are\nwell preserved, and a salient feature is added, here\nglasses or hat. Furthermore, we can also notice that\nour model, being more accurate, is also more sensitive\nto dataset biases. For instance, we have noticed that\nin our dataset people with thin, transparent glasses\nare usually old men. This bias is clearly visible in the\nsecond row of Fig.3 and Fig.4. Removing such bias, as\nin [Barbano et al., 2023], is left as future work.\nCifar-10-MNIST dataset\nWe create a new dataset\nbased on Cifar-10 [Krizhevsky, 2009] and MNIST\n[LeCun, ]. Background images X are Cifar-10 images,\nand target images Y are also CIFAR-10 with a ran-\ndom MNIST digit overlaid on it. We use 50k training\nimages, equally divided between X and Y , and 10k\ntest images, equally divided among the MNIST digits.\nOur model should successfully capture the background\nvariability (i.e., CIFAR objects) only in the common\nF. Carton, R. Louiset, P. Gori\nX\nY\nG(z, 0)\nG(z, si ̸= 0)\nFigure 4: Fake images generated by our model. In each\nrow, we use the same common feature z for all images,\ns = 0 for X and different salient features s ̸= 0 for Y .\nlatent space zy, and the MNIST variability (i.e., digits)\nonly in the salient space sy. A perfect classifier would\nhave 100% accuracy on MNIST when using sy and\n10% (which corresponds to randomness) when using zy.\nConversely, it should have 100% accuracy on Cifar-10\nwhen trained on zy and 10% when trained on sy.\nWe compare our model with MM-cVAE. Since we used\nthe same image size as for CelebA (64 × 64 × 3), we\nkept the same network architecture. We tested several\nhyper-parameters for both methods and used the best\nconfiguration in our experiments. Results using two\ndifferent latent space size are shown in Table 2 (for\nMM-cVAE, we use: λ1 = 102, λ2 = 103). As before,\nwe run both methods 5 times (with different random\nseeds) for 500 epochs, and report the highest, average\nand lowest scores. More results in the Suppl.\nWe can notice that our method either outperforms\nMM-cVAE or obtains comparable results. Moreover,\nduring our numerous trainings, we noticed that the\nresults obtained with our method are very stable, while\nthose obtained with MM-cVAE, as before with CelebA,\nare more variable and may diverge (i.e., nan). Visual\nexamples are presented in Fig. 5, with image reconstruc-\ntion and salient feature swap (more in Supplementary).\nOur model offers sharper images than MM-cVAE and\nis able to better extract salient features.\nAblation study\nWe present in Table 3 a detailed\nablation study on the proposed losses using the Cifar-\nMNIST dataset and the architecture with a latent space\nof size 128 (since it obtained the best results in Table\n2). We can notice that the proposed combination of\nlosses obtains the best results.\nOriginal\nReconstruction\nSwap\nMM-\ndouble\nMM-\ndouble\ncVAE\nInfoGAN\ncVAE\nInfoGAN\nFigure 5: Image reconstruction and swap with Cifar-\n10-MNIST.\nBrats dataset\nIn this section, we present qualita-\ntive results on the Brats dataset [Menze et al., 2014].\nBackground data X contains T1-w MR brain images\nof healthy subject whereas the target dataset Y has\nimages of patients with brain tumors. Since images\nare bigger (128 × 128) than the other datatsets, we use\na different architecture. More details can be found in\nthe Supplementary. Please note that here there are\nno sub-categories (as in previous datasets) that can be\nexploited to compute quantitative metrics (subgroup\nclassification).\nFig. 6 shows fake images generated by our model\ntrained on Brats. On the left are healthy images (s = 0),\nand on the right images with tumor (s ̸= 0). Images\nin the same row are generated using the same z. We\ncan see that the general anatomy of the brain is pre-\nserved when changing domain, and that tumors with\ndifferent size and position are generated. By changing\nz (i.e. row), we can also notice that the model seems\nto have correctly encoded in z the general anatomical\nvariability of the brain.\nIn Fig.7, we generate healthy counterparts of tar-\nget images with tumor, setting s = 0. This is very\nvaluable in a clinical setting for multi-modal fusion\n[Fran¸cois et al., 2022, Maillard et al., 2022], where im-\nages from different modalities can exhibit a different\ntopology due to the tumor, and atlas construction\nDouble InfoGAN for Contrastive Analysis\nMnist (salient)\nCifar (background)\nsy ↑\nzy ↓\nsy ↓\nzy ↑\nBest Avg. Worst Best Avg. Worst Best Avg. Worst Best Avg. Worst\nMM-cVAE (size 128)\n0.81\n0.76\nnan\n0.43\n0.48\nnan\n0.14\n0.18\nnan\n0.36\n0.35\nnan\nMM-cVAE (size 200)\n0.82\n0.63\n0.13\n0.43\n0.58\n0.82\n0.12 0.17\n0.27\n0.37\n0.36\n0.34\ndouble InfoGAN (size 128) 0.87 0.87\n0.86\n0.25 0.26\n0.28\n0.17\n0.18\n0.19\n0.43\n0.42\n0.41\ndouble InfoGAN (size 200) 0.88 0.87\n0.86\n0.32\n0.32\n0.32\n0.20\n0.21\n0.23\n0.44 0.44\n0.43\nTable 2: MNIST-Cifar10 classification. Digits information should only be encoded in sy and not in zy, whereas\nthe contrary should be true for Objects information. Std ≤ 0.01. Best results in bold.\nMnist (salient) Cifar (bg)\nsy ↑\nzy ↓\nsy ↓\nzy ↑\n-LClass\n0.48\n0.83\n0.23\n0.37\n- LClass - LIm\n0.54\n0.72\n0.22\n0.38\n- LInfo - LClass - LIm 0.70\n0.70\n0.18 0.18\n- LInfo\n0.85\n0.60\n0.30\n0.36\n- LInfo - LIm\n0.59\n0.59\n0.20\n0.20\n- LClass - LInfo\n0.74\n0.67\n0.29\n0.35\n- LIm\n0.86\n0.25\n0.20 0.42\nFull\n0.87\n0.26\n0.18 0.42\nTable 3: Ablation study of the different losses on the\nCifar-MNIST dataset. For every configuration, 3 train-\nings were launched. We report average values.\nX - healthy\nY - tumor\nG(z, 0)\nG(z, si ̸= 0)\nFigure 6: Fake images generated by our model. In each\nrow, we use the same z for all images with s = 0 for X\nand different s ̸= 0 for each exemple of Y .\n[Liu et al., 2015a, Roux et al., 2019], where tumor im-\nages have to be registered to healthy templates. Please\nnote that here we use 2D slices with a small archi-\ntecture (DCGAN), and a small (and biased) dataset\n(Brats). Indeed, we have noticed that most of the slices\ncontaining a tumor are in the central part of the brain\n(greater size) whereas slices from the higher or lower\npart of the brain (smaller size) have less frequently a\ntumor. This might thus entail structural changes dur-\ning the generation of the healthy counterpart (swap),\nsuch as the one in size in the third row of Fig. 7. This\ncould be solved by directly working with 3D data, more\npowerful networks and debiasing strategies.\nTarget image Reconstruction Swap (s = 0)\nFigure 7: Reconstruction (middle) and generation of\nan healthy counterpart (swap, on the right) of a target\nimage with brain tumor (on the left) by setting s = 0\nand keeping the same z.\ndSprites-MNIST dataset\nA new toy dataset is\nproposed for evaluating CA methods. The background\ndataset X consists of 4 MNIST digits (1, 2, 3 and 4)\nregularly placed in a square. In the target dataset Y ,\ndSprites element [Matthey et al., 2017] are added on\ntop of the same 4 MNIST digits. Image reconstruction\nand salient feature swap are presented in Fig. 8. As\nbefore, we can see that, compared to MM-cVAE,\nimage reconstructions are more accurate and sharp\nand, when exchanging salient features, the dSprites\nelements are better preserved.\nDisentanglement\nAs\nin\n[Higgins et al., 2017,\nLin et al., 2020], we also use dSprites to evaluate the\ndisentanglement of our method in the salient space.\nIndeed, dSprites elements only exhibit 5 possible varia-\ntions, making it easy to evaluate the disentanglement.\nPossible variations are: 1) shape (heart, elipse and\nsquare), 2) size, 3) position in X, 4) position in Y and\n5) orientation (i.e. rotation). As metric, we use the\nFactorVAE (fvae) score [Kim and Mnih, 2018]. Initial\nresults using the proposed method showed a very poor\ndisentanglement. To further improve it, we adapted for\nour model the Contrastive Regularizer (CR) module\nF. Carton, R. Louiset, P. Gori\nof InfoGAN-CR [Lin et al., 2020] (more details in the\nSupplementary), obtaining a maximum fvae score of\n0.47. For comparison, InfoGAN-CR achieves a fvae\nscore of 0.88 on the dsprite dataset alone. This shows\nthat disentangling salient (or common) factors is much\nmore difficult in our case than when using a single\ndata-set. Exploring disentanglement regularizations\nmore suited for a CA setting is left as future work.\nIn Fig. 9, we show target images generated by our\nmodel when varying only one dimension (from -1.5 to\n1.5) of sy, while keeping zy fixed. We clearly see a high\nentanglement among the dSprites factors of variation.\nFor completeness, we also checked whether the CR\nmodule helped the separation between common and\nsalient information, and found similar quantitative\nresults (see Supplementary).\n6\nConclusions and Perspectives\nWe propose the first GAN-based model for Contrastive\nAnalysis (CA) that estimates and separates in an un-\nsupervised way all common and distinctive generative\nfactors of a target dataset with respect to a back-\nground dataset.\nCompared to current SOTA CA-\nVAE models, we demonstrate superior performance\non 4 visual datasets of increasing complexity and rang-\ning from simple toy examples to real medical data.\nOur method manages to better separate common from\nsalient factors, shows a better image generation qual-\nity and a greater stability during training. Further-\nmore, it allows the generation of multiple counterparts\nbetween domains by fixing the common factors and\nadding/removing the salient ones.\nWe believe that\nthe proposed method will benefit from more powerful\nGAN models and future progress in disentanglement,\nincreasing its accuracy and interpretability. This will\nwiden its fields of application to, for instance, clinically\nvaluable and challenging tasks, such as computer aided-\ndiagnosis.\nA last interesting research avenue could\nbe the extension to the recent diffusion based models,\nas [Song et al., 2021, Rombach et al., 2022].\nLimitations Recent works have shown that genera-\ntive models, such as VAE and GAN, are in general not\nidentifiable [Locatello et al., 2019]. To obtain identifi-\nability, two different solutions have been proposed: 1)\neither regularizing [Kivva et al., 2022] / constraining\n(e.g., making it linear) the encoder or 2) introducing\nan auxiliary variable so that the latent factors are\nconditionally independent given the auxiliary variable\n[Hyvarinen et al., 2019, Khemakhem et al., 2020]. Un-\nfortunately, in Contrastive Analysis, neither of these\nsolutions may be used5. While all losses proposed here,\nand in the related works, are needed to effectively sep-\narate common from salient factors, they do not assure\n5The dataset label could be considered as an auxiliary\nvariable but it does not make c and s independent\nOriginal\nReconstruction\nSwap\nMM-\ndouble\nMM-\ndouble\ncVAE\nInfoGAN\ncVAE\nInfoGAN\nFigure 8: Image reconstruction and swap of salient\nfeatures on the dSprites-MNIST dataset.\nFigure 9: Each row represents the variation of only\none element of the salient factor sy, while keeping\nzy fixed.\nWe can see a certain entanglement, with\nseveral parameters changing at the same time: shape\nand position (line 1), position and orientation (line 2).\nOnly the last line shows a disentanglement, with only\nthe orientation of the ellipse changing.\nthat all true generative factors have been identified.\nThis is the main limitation of this work, and actually\nof all concurrent CA-VAE models, and is left as future\nwork. Inspired by [Wyner, 1975], a possible research di-\nrection would be adding an information-theoretic loss\nthat quantifies the common and salient information\ncontent so that, under realistic assumptions, the model\ncould be identifiable.\nAcknowledgments\nThis work was supported by\nthe IDS department of T´el´ecom Paris and by the\nL’association T´el´ecom Paris Alumni.\nDouble InfoGAN for Contrastive Analysis\nReferences\n[Abid et al., 2018] Abid, A., Zhang, M. J., Bagaria,\nV. K., and Zou, J. (2018). Exploring patterns en-\nriched in a dataset with contrastive principal com-\nponent analysis. Nat Commun, 9(1):2134. Number:\n1 Publisher: Nature Publishing Group.\n[Abid and Zou, 2019] Abid, A. and Zou, J. (2019).\nContrastive\nVariational\nAutoencoder\nEnhances\nSalient Features. arXiv:1902.04601 [cs, stat].\n[Barbano et al., 2023] Barbano, C. A., Dufumier, B.,\nTartaglione, E., Grangetto, M., and Gori, P. (2023).\nUnbiased Supervised Contrastive Learning. In ICLR.\narXiv:2211.05568 [cs, stat].\n[Baur et al., 2021] Baur, C., Denner, S., Wiestler, B.,\nNavab, N., and Albarqouni, S. (2021). Autoencoders\nfor unsupervised anomaly segmentation in brain MR\nimages: A comparative study. Medical Image Anal-\nysis, 69(8):1–16.\n[Benaim et al., 2019] Benaim,\nS.,\nKhaitov,\nM.,\nGalanti, T., and Wolf, L. (2019).\nDomain inter-\nsection and domain difference. Proceedings of the\nIEEE International Conference on Computer Vision,\npages 3444–3452.\n[Brock et al., 2019] Brock, A., Donahue, J., and Si-\nmonyan, K. (2019). Large scale GaN training for\nhigh fidelity natural image synthesis. In International\nConference on Learning Representations (ICLR).\n[Chen et al., 2016] Chen, X., Duan, Y., Houthooft, R.,\nSchulman, J., Sutskever, I., and Abbeel, P. (2016). In-\nfoGAN: Interpretable Representation Learning by In-\nformation Maximizing Generative Adversarial Nets.\nIn Advances in Neural Information Processing Sys-\ntems, volume 29. Curran Associates, Inc.\n[Choi et al., 2018] Choi, Y., Choi, M., Kim, M., Ha,\nJ. W., Kim, S., and Choo, J. (2018).\nStarGAN:\nUnified Generative Adversarial Networks for Multi-\ndomain Image-to-Image Translation. Proceedings of\nthe IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, pages 8789–8797.\n[Choudhuri et al., 2019] Choudhuri,\nA.,\nMakkuva,\nA. V., Rana, R., Oh, S., Chowdhary, G., and\nSchwing, A. (2019). Towards Principled Objectives\nfor Contrastive Disentanglement.\n[Deun et al., 2012] Deun, K. V., Mechelen, I. V., Thor-\nrez, L., Schouteden, M., Moor, B. D., Werf, M. J.\nv. d., Lathauwer, L. D., Smilde, A. K., and Kiers,\nH. A. L. (2012).\nDISCO-SCA and Properly Ap-\nplied GSVD as Swinging Methods to Find Common\nand Distinctive Processes. PLOS ONE, 7(5):e37840.\nPublisher: Public Library of Science.\n[Feng et al., 2018] Feng, Q., Jiang, M., Hannig, J., and\nMarron, J. S. (2018). Angle-based joint and indi-\nvidual variation explained. Journal of Multivariate\nAnalysis, 166:241–265.\n[Fran¸cois et al., 2022] Fran¸cois, A., Maillard, M., Op-\npenheim, C., Pallud, J., Bloch, I., Gori, P., and\nGlaun`es, J. (2022). Weighted Metamorphosis for\nRegistration of Images with Different Topologies. In\nHering, A., Schnabel, J., Zhang, M., Ferrante, E.,\nHeinrich, M., and Rueckert, D., editors, Biomedi-\ncal Image Registration, Lecture Notes in Computer\nScience, pages 8–17, Cham. Springer International\nPublishing.\n[Ganin et al., 2017] Ganin, Y., Ustinova, E., Ajakan,\nH., Germain, P., Larochelle, H., Laviolette, F., Marc-\nhand, M., and Lempitsky, V. (2017).\nDomain-\nAdversarial Training of Neural Networks. In Csurka,\nG., editor, Domain Adaptation in Computer Vision\nApplications, Advances in Computer Vision and Pat-\ntern Recognition, pages 189–209. Springer Interna-\ntional Publishing, Cham.\n[Goodfellow et al., 2014] Goodfellow,\nI.,\nPouget-\nAbadie, J., Mirza, M., Xu, B., Warde-Farley, D.,\nOzair, S., Courville, A., and Bengio, Y. (2014).\nGenerative Adversarial Nets. In Advances in Neural\nInformation Processing Systems, volume 27.\n[Guillon et al., 2021] Guillon, L., Cagna, B., Dufumier,\nB., Chavas, J., Rivi`ere, D., and Mangin, J.-F. (2021).\nDetection of Abnormal Folding Patterns with Unsu-\npervised Deep Generative Models. In Abdulkadir,\nA., Kia, S. M., Habes, M., Kumar, V., Rondina,\nJ. M., Tax, C., and Wolfers, T., editors, Machine\nLearning in Clinical Neuroimaging, Lecture Notes\nin Computer Science, pages 63–72, Cham. Springer\nInternational Publishing.\n[He et al., 2019] He, Z., Zuo, W., Kan, M., Shan, S.,\nand Chen, X. (2019). AttGAN: Facial Attribute\nEditing by only Changing What You Want. IEEE\nTransactions on Image Processing, 28(11):5464–5478.\n[Higgins et al., 2017] Higgins, I., Matthey, L., Pal, A.,\nBurgess, C., Glorot, X., Botvinick, M., Mohamed,\nS., and Lerchner, A. (2017). β-VAE: learning ba-\nsic visual concepts with a constrained variational\nframework. In ICLR.\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park,\nT., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., and\nDarrell, T. (2018). CyCADA: Cycle-Consistent Ad-\nversarial Domain Adaptation. In Proceedings of the\n35th International Conference on Machine Learning,\npages 1989–1998. PMLR. ISSN: 2640-3498.\nF. Carton, R. Louiset, P. Gori\n[Huang et al., 2018] Huang, X., Liu, M.-Y., Belongie,\nS., and Kautz, J. (2018). Multimodal Unsupervised\nImage-to-Image Translation. In ECCV.\n[Hyvarinen et al., 2019] Hyvarinen, A., Sasaki, H., and\nTurner, R. E. (2019). Nonlinear ICA Using Auxiliary\nVariables and Generalized Contrastive Learning. In\nAISTATS.\n[Isola et al., 2017] Isola, P., Zhu, J.-Y., Zhou, T., and\nEfros, A. A. (2017). Image-to-Image Translation\nwith Conditional Adversarial Networks.\nIn 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 5967–5976, Honolulu, HI.\nIEEE.\n[Joy et al., 2021] Joy, T., Schmon, S. M., Torr, P. H. S.,\nSiddharth, N., and Rainforth, T. (2021).\nCap-\nturing Label Characteristics in VAEs.\nIn ICLR.\narXiv:2006.10102 [cs, stat].\n[Karras et al., 2019] Karras, T., Laine, S., and Aila,\nT. (2019). A style-based generator architecture for\ngenerative adversarial networks. Proceedings of the\nIEEE Computer Society Conference on Computer Vi-\nsion and Pattern Recognition, 2019-June:4396–4405.\n[Kazemi et al., 2019] Kazemi, H., Iranmanesh, S. M.,\nand Nasrabadi, N. (2019). Style and Content Dis-\nentanglement in Generative Adversarial Networks.\npages 848–856.\n[Khemakhem et al., 2020] Khemakhem, I., Kingma,\nD., Monti, R., and Hyvarinen, A. (2020). Varia-\ntional Autoencoders and Nonlinear ICA: A Unifying\nFramework.\nIn Proceedings of the Twenty Third\nInternational Conference on Artificial Intelligence\nand Statistics, pages 2207–2217. PMLR. ISSN: 2640-\n3498.\n[Kim and Mnih, 2018] Kim, H. and Mnih, A. (2018).\nDisentangling by Factorising. In Proceedings of the\n35th International Conference on Machine Learning,\npages 2649–2658. PMLR. ISSN: 2640-3498.\n[Kimura et al., 2020] Kimura,\nD.,\nChaudhury,\nS.,\nNarita, M., Munawar, A., and Tachibana, R. (2020).\nAdversarial Discriminative Attention for Robust\nAnomaly Detection. In 2020 IEEE Winter Confer-\nence on Applications of Computer Vision (WACV),\npages 2161–2170, Snowmass Village, CO, USA.\nIEEE.\n[Kingma and Welling, 2014] Kingma,\nD.\nP.\nand\nWelling, M. (2014).\nAuto-Encoding Variational\nBayes. In ICLR.\n[Kivva et al., 2022] Kivva, B., Rajendran, G., Raviku-\nmar, P., and Aragam, B. (2022). Identifiability of\ndeep generative models without auxiliary informa-\ntion. In NeurIPS.\n[Krizhevsky, 2009] Krizhevsky, A. (2009).\nLearning\nMultiple Layers of Features from Tiny Images. Tech-\nnical report.\n[Lample et al., 2017] Lample,\nG.,\nZeghidour,\nN.,\nUsunier, N., Bordes, A., Denoyer, L., and Ranzato,\nM. (2017). Fader networks: Manipulating images by\nsliding attributes. Advances in Neural Information\nProcessing Systems, 2017-Decem(Nips):5968–5977.\n[LeCun, ] LeCun, Y. The mnist database of handwrit-\nten digits. Technical report.\n[Lee et al., 2018] Lee, H.-Y., Tseng, H.-Y., Huang, J.-\nB., Singh, M., and Yang, M.-H. (2018).\nDiverse\nImage-to-Image Translation via Disentangled Repre-\nsentations. In Ferrari, V., Hebert, M., Sminchisescu,\nC., and Weiss, Y., editors, Computer Vision – ECCV\n2018, volume 11205, pages 36–52. Springer Interna-\ntional Publishing, Cham. Series Title: Lecture Notes\nin Computer Science.\n[Lin et al., 2020] Lin, Z., Thekumparampil, K., Fanti,\nG., and Oh, S. (2020). InfoGAN-CR and ModelCen-\ntrality: Self-supervised Model Training and Selection\nfor Disentangling GANs. In Proceedings of the 37th\nInternational Conference on Machine Learning.\n[Liu et al., 2017] Liu, M.-Y., Breuel, T., and Kautz,\nJ. (2017). Unsupervised Image-to-Image Transla-\ntion Networks. In Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates,\nInc.\n[Liu et al., 2015a] Liu, X., Niethammer, M., Kwitt, R.,\nSingh, N., McCormick, M., and Aylward, S. (2015a).\nLow-Rank Atlas Image Analyses in the Presence of\nPathologies. IEEE Transactions on Medical Imaging,\n34(12):2583–2591. Conference Name: IEEE Trans-\nactions on Medical Imaging.\n[Liu et al., 2015b] Liu, Z., Luo, P., Wang, X., and\nTang, X. (2015b). Deep learning face attributes in\nthe wild. In Proceedings of International Conference\non Computer Vision (ICCV).\n[Locatello et al., 2019] Locatello, F., Bauer, S., Lu-\ncic, M., Raetsch, G., Gelly, S., Sch¨olkopf, B., and\nBachem, O. (2019). Challenging Common Assump-\ntions in the Unsupervised Learning of Disentangled\nRepresentations. In Proceedings of the 36th Inter-\nnational Conference on Machine Learning, pages\n4114–4124. PMLR. ISSN: 2640-3498.\n[Locatello et al., 2020] Locatello,\nF.,\nPoole,\nB.,\nRaetsch, G., Sch¨olkopf, B., Bachem, O., and Tschan-\nnen, M. (2020). Weakly-Supervised Disentanglement\nDouble InfoGAN for Contrastive Analysis\nWithout Compromises. In Proceedings of the 37th\nInternational Conference on Machine Learning,\npages 6348–6359. PMLR. ISSN: 2640-3498.\n[Ma et al., 2019] Ma, L., Jia, X., Georgoulis, S.,\nTuytelaars, T., and Van Gool, L. (2019). Exem-\nplar Guided Unsupervised Image-to-Image Transla-\ntion with Semantic Consistency. In ICLR. arXiv.\narXiv:1805.11145 [cs].\n[Maillard et al., 2022] Maillard,\nM.,\nFran¸cois,\nA.,\nGlaun`es, J., Bloch, I., and Gori, P. (2022). A Deep\nResidual Learning Implementation of Metamorpho-\nsis. In 2022 IEEE 19th International Symposium\non Biomedical Imaging (ISBI), pages 1–4.\nISSN:\n1945-8452.\n[Matthey et al., 2017] Matthey,\nL.,\nHiggins,\nI.,\nHassabis,\nD.,\nand\nLerchner,\nA.\n(2017).\ndsprites: Disentanglement testing sprites dataset.\nhttps://github.com/deepmind/dsprites-dataset/.\n[Menze et al., 2014] Menze, B. H., Jakab, A., Bauer,\nS., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.\n(2014). The multimodal brain tumor image segmen-\ntation benchmark (brats). IEEE transactions on\nmedical imaging, 34(10):1993–2024.\n[PANG et al., 2022] PANG, G., SHEN, C., CAO, L.,\nand HENGEL, A. V. D. (2022). Deep learning for\nanomaly detection: A review.\nACM Computing\nSurveys, 54(2):1–38.\n[Rohlf and Corti, 2000] Rohlf, F. J. and Corti, M.\n(2000). Use of Two-Block Partial Least-Squares to\nStudy Covariation in Shape. Systematic Biology,\n49(4):740–753. Publisher: [Oxford University Press,\nSociety of Systematic Biologists].\n[Rombach et al., 2022] Rombach, R., Blattmann, A.,\nLorenz, D., Esser, P., and Ommer, B. (2022). High-\nResolution Image Synthesis with Latent Diffusion\nModels. In CVPR, pages 10674–10685.\n[Roux et al., 2019] Roux, A., Roca, P., Edjlali, M.,\nSato, K., Zanello, M., Dezamis, E., Gori, P., Lion,\nS., Fleury, A., Dhermain, F., Meder, J.-F., Chr´etien,\nF., Lechapt, E., Varlet, P., Oppenheim, C., and\nPallud, J. (2019). MRI Atlas of IDH Wild-Type\nSupratentorial Glioblastoma: Probabilistic Maps of\nPhenotype, Management, and Outcomes. Radiology,\n293(3):633–643. Publisher: Radiological Society of\nNorth America.\n[Ruiz et al., 2019] Ruiz, A., Martinez, O., Binefa, X.,\nand Verbeek, J. (2019). Learning Disentangled Repre-\nsentations with Reference-Based Variational Autoen-\ncoders. In ICLR workshop on Learning from Lim-\nited Labeled Data, pages 1–17, New Orleans, United\nStates.\n[Severson et al., 2019] Severson, K., Ghosh, S., and\nNg, K. (2019).\nUnsupervised learning with con-\ntrastive latent variable models.\nIn AAAI. arXiv.\narXiv:1811.06094 [cs, stat].\n[Shi et al., 2021] Shi, Y., Yang, X., Wan, Y., and Shen,\nX. (2021). SemanticStyleGAN: Learning Compo-\nsitional Generative Priors for Controllable Image\nSynthesis and Editing.\n[Shu et al., 2020] Shu, R., Chen, Y., Kumar, A., Er-\nmon, S., and Poole, B. (2020). Weakly Supervised\nDisentanglement with Guarantees. In ICLR. arXiv.\narXiv:1910.09772 [cs, stat].\n[Smilde et al., 2017] Smilde, A. K., M˚age, I., Næs,\nT.,\nHankemeier,\nT.,\nLips,\nM. A.,\nKiers,\nH.\nA. L., Acar, E., and Bro, R. (2017).\nCom-\nmon and distinct components in data fusion.\nJournal of Chemometrics, 31(7):e2900.\neprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2900.\n[Song et al., 2021] Song,\nY.,\nSohl-Dickstein,\nJ.,\nKingma, D. P., Kumar, A., Ermon, S., and Poole, B.\n(2021). Score-Based Generative Modeling through\nStochastic Differential Equations. In ICLR.\n[Trygg, 2002] Trygg, J. (2002). O2-PLS for qualitative\nand quantitative analysis in multivariate calibration.\nJournal of Chemometrics, 16(6):283–293.\neprint:\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.724.\n[Tu et al., 2021] Tu, R., Foss, A. H., and Zhao, S. D.\n(2021). Capturing patterns of variation unique to a\nspecific dataset. arXiv:2104.08157 [cs, stat].\n[Venkataramanan et al., 2020] Venkataramanan,\nS.,\nPeng, K.-C., Singh, R. V., and Mahalanobis, A.\n(2020). Attention Guided Anomaly Localization in\nImages. In Vedaldi, A., Bischof, H., Brox, T., and\nFrahm, J.-M., editors, Computer Vision – ECCV\n2020, volume 12362, pages 485–503. Springer Inter-\nnational Publishing, Cham. Series Title: Lecture\nNotes in Computer Science.\n[von K¨ugelgen et al., 2021] von K¨ugelgen, J., Sharma,\nY., Gresele, L., Brendel, W., Sch¨olkopf, B., Besserve,\nM., and Locatello, F. (2021). Self-Supervised Learn-\ning with Data Augmentations Provably Isolates Con-\ntent from Style. In Advances in Neural Information\nProcessing Systems, volume 34, pages 16451–16467.\n[Vowels et al., 2020] Vowels, M. J., Cihan Camgoz,\nN., and Bowden, R. (2020).\nNestedVAE: Isolat-\ning Common Factors via Weak Supervision. In 2020\nIEEE/CVF Conference on Computer Vision and\nF. Carton, R. Louiset, P. Gori\nPattern Recognition (CVPR), pages 9199–9209, Seat-\ntle, WA, USA. IEEE.\n[V´etil et al., 2022] V´etil, R., Abi-Nader, C., Bˆone, A.,\nVullierme, M.-P., Roh´e, M.-M., Gori, P., and Bloch,\nI. (2022). Learning Shape Distributions from Large\nDatabases of Healthy Organs: Applications to Zero-\nShot and Few-Shot Abnormal Pancreas Detection.\nIn Wang, L., Dou, Q., Fletcher, P. T., Speidel, S.,\nand Li, S., editors, Medical Image Computing and\nComputer Assisted Intervention – MICCAI 2022,\nLecture Notes in Computer Science, pages 464–473,\nCham. Springer Nature Switzerland.\n[Weinberger et al., 2022] Weinberger, E., Beebe-Wang,\nN., and Lee, S.-I. (2022). Moment Matching Deep\nContrastive Latent Variable Models. In AISTATS.\narXiv. arXiv:2202.10560 [cs].\n[Wyner, 1975] Wyner, A. (1975). The common infor-\nmation of two dependent random variables. IEEE\nTrans. Inform. Theory, 21(2):163–179.\n[Yu et al., 2017] Yu, Q., Risk, B. B., Zhang, K., and\nMarron, J. S. (2017). JIVE integration of imaging\nand behavioral data. NeuroImage, 152:38–49.\n[Zhu et al., 2017] Zhu, J.-Y., Park, T., Isola, P., and\nEfros, A. A. (2017). Unpaired Image-to-Image Trans-\nlation Using Cycle-Consistent Adversarial Networks.\nIn 2017 IEEE International Conference on Computer\nVision (ICCV), pages 2242–2251, Venice. IEEE.\n[Zou et al., 2013] Zou, J. Y., Hsu, D. J., Parkes, D. C.,\nand Adams, R. P. (2013).\nContrastive Learning\nUsing Spectral Methods.\nIn Advances in Neural\nInformation Processing Systems, volume 26. Curran\nAssociates, Inc.\n[Zou et al., 2022] Zou, K., Faisan, S., Heitz, F., and\nValette, S. (2022). Joint Disentanglement of Labels\nand Their Features with VAE. In 2022 IEEE In-\nternational Conference on Image Processing (ICIP),\npages 1341–1345, Bordeaux, France. IEEE.\n"
}