{
    "optim": "Double InfoGAN for Contrastive Analysis Florence Carton1 Robin Louiset1,2 Pietro Gori1 1LTCI, T´el´ecom Paris, IPParis, France 2NeuroSpin, CEA, Universite Paris-Saclay, France Abstract Contrastive Analysis (CA) deals with the dis- covery of what is common and what is dis- tinctive of a target domain compared to a background one. This is of great interest in many applications, such as medical imaging. Current state-of-the-art (SOTA) methods are latent variable models based on VAE (CA- VAEs). However, they all either ignore im- portant constraints or they don’t enforce fun- damental assumptions. This may lead to sub- optimal solutions where distinctive factors are mistaken for common ones (or viceversa). Fur- thermore, the generated images have a rather poor quality, typical of VAEs, decreasing their interpretability and usefulness. Here, we pro- pose Double InfoGAN, the first GAN based method for CA that leverages the high-quality synthesis of GAN and the separation power of InfoGAN. Experimental results on four visual datasets, from simple synthetic examples to complex medical images, show that the pro- posed method outperforms SOTA CA-VAEs in terms of latent separation and image qual- ity. Datasets and code are available online1. 1 Introduction Learning disentangled generative factors in an unsuper- vised way has gathered much attention lately since it is of interest in many domains, such as medical imaging. Most approaches look for factors that capture distinct, noticeable and semantically meaningful variations in one dataset (e.g., presence of hat or glasses in CelebA). Authors usually propose well adapted regularizations, which may promote, for instance, ”uncorrelatedness” (FactorVAE [Kim and Mnih, 2018]) or ”informative- ness” (InfoGAN [Chen et al., 2016]). Proceedings of the 27th International Conference on Artifi- cial Intelligence and Statistics (AISTATS) 2024, Valencia, Spain. PMLR: Volume TBD. Copyright 2024 by the au- thor(s). In this paper, we focus on a related but differ- ent problem, that has been named Contrastive Analysis (CA) [Zou et al., 2013, Abid et al., 2018, Weinberger et al., 2022]. We wish to discover in an unsupervised way what is added or modified on a target dataset compared to a control (or background) dataset, as well as what is common between the two domains. For example, in medical imaging, one would like to discover the salient variations characterizing a pathology that are only present in a population of patients and not in a population of healthy controls. Both the target (patients) and the background (healthy) datasets are supposed to share uninteresting (healthy) variations. The goal is thus to identify and separate the generative factors common to both populations from the ones distinctive (i.e., specific) only of the target dataset. The most recent CA methods are based on the Variational AutoEncoders (VAE) [Kingma and Welling, 2014] model and they are called Contrastive VAE (CA-VAE). These methods assume that samples from the target dataset are generated using two sets of latent factors, common z and salient s, whereas samples from the control dataset are generated using only the common z factors. The salient factors s should therefore model the specific patterns of variations of the target dataset. All these methods share the same general mathematical formulation, which derives from the standard VAE. However, they all either ignore a term of the proposed loss (e.g., KL loss in [Abid and Zou, 2019, Ruiz et al., 2019]) or they don’t enforce important assumptions (e.g., independence between z and s in [Weinberger et al., 2022]), which may lead to sub-optimal solutions where salient factors are mistaken for common ones (or viceversa). Furthermore, they all share a typical downside of VAEs: a blurry and poor quality image generation. For these reasons, we propose Double InfoGAN : a novel Contrastive method which leverages the high-quality synthesis of Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] and the separation power of InfoGAN [Chen et al., 2016]. To the best of our knowledge, this is the first GAN arXiv:2401.17776v1  [cs.CV]  31 Jan 2024 Double InfoGAN for Contrastive Analysis Figure 1: Two examples of datasets for Con- trastive Analysis. First figure: Brats dataset [Menze et al., 2014]. Top: MRI images of healthy brains (control dataset). Bottom: MRI images of brains with tumor (target dataset). Second figure: CelebA dataset. Top: control dataset with regular faces (no smile, no glasses). Bottom: target dataset that con- tains smiling faces with glasses. based method proposed in the context of Contrastive Analysis. The main contributions of this paper are: • The first GAN based method for Contrastive Analysis (CA) which allows high-quality synthesis. • A new regularization term for CA, inspired by Info- GAN. • Two new losses for an accurate separation and esti- mate of the common and salient generative factors. • Extensive experimental results on four visual datasets, from synthetic to complex ones, show that the proposed method outperforms SOTA CA-VAE methods in terms of latent separation and image quality. Datasets and code are available online.1 2 Related Work Separating common from distinctive latent rep- resentations has become an active research area in several fields, such as domain adaptation (DA) [Ganin et al., 2017, Hoffman et al., 2018] and image-to-image translation (IMI) [Zhu et al., 2017, Isola et al., 2017, Liu et al., 2017, Lee et al., 2018, Huang et al., 2018]. DA seeks to transfer a classifier from a source domain, with many labelled samples, to a different target domain, which has few or no labelled data. As shown in [Ganin et al., 2017], an effective classifier should use shared features that cannot discriminate between the two domains. The goal of IMI is instead to estimate a transformation that maps images from the source domain to the target one by disentangling and controlling high-level visual attributes (style, gender, objects) [Lee et al., 2018]. The main difference between these methods and the proposed one is the 1https://github.com/Florence-C/Double_InfoGAN. git objective. Our goal is to statistically analyze two domains (e.g., healthy and patients) looking for latent representations that generate the background (e.g., healthy) and target (e.g., pathological) content. We do not seek to transfer a classifier or to map an image to a different distribution. We wish, for instance, to generate new images and not only to translate them to another domain. Another important difference is that we do not want to encode only a particular distinctive attribute (e.g., style [Ma et al., 2019], gender) but all distinctive variations of the target domain with respect to the background one. Fur- thermore, we do not plan to use a weight sharing constraint [Lee et al., 2018, Liu et al., 2017], or other architectural constraints, which assume that the main differences are, for instance, only in the low-level features (color, texture, edges, etc.). Our work is also close to unsupervised anomaly detection [Guillon et al., 2021, Baur et al., 2021, PANG et al., 2022, V´etil et al., 2022], which is usually composed of two steps. First, the distribution of the background (control) domain is learned, using deep generative models. Then, or at the same time, a discriminator is optimized to detect the target (anomalous) samples. By looking at the reconstruction errors [Guillon et al., 2021], attention scores [Venkataramanan et al., 2020], visual saliency [Kimura et al., 2020] or other features, one can understand which are the salient patterns of the target (anomalous) domain. Even if this strategy can be highly interpretable, the goal is to spot an anomalous sample and not to model the latent factors that generate the anomalous patterns. Another class of methods, mainly used in the fields of data integration and data fusion, are the projection based latent variables approaches, such as 2B-PLS, 02PLS, DISCO-SCA, GSVD, JIVE [Feng et al., 2018, Rohlf and Corti, 2000, Deun et al., 2012, Yu et al., 2017, Trygg, 2002, Smilde et al., 2017]. Contrary to these methods, we do not use only linear transformations, but we leverage the capacity of deep learning to estimate non-linear mappings. In parallel, research on disentanglement has been developed, making it possible to modify a single and semantically meaningful pattern of the image (e.g., person’s smile, gender), by varying only one component of the latent representation [Kim and Mnih, 2018]. As shown in [Locatello et al., 2019], the unsupervised learning of disentangled representations is theoretically impossible from i.i.d. samples without inductive biases [Higgins et al., 2017, Chen et al., 2016], weak labels [Shu et al., 2020, Locatello et al., 2020], or supervision [Lample et al., 2017, Choi et al., 2018, He et al., 2019, Shi et al., 2021, Joy et al., 2021]. These methods have F. Carton, R. Louiset, P. Gori all focused on the latent generative factors of a single dataset, and their goal is thus different from ours. With a different perspective, methods stem- ming from the recent Contrastive Analysis (CA) setting [Zou et al., 2013, Abid et al., 2018, Tu et al., 2021, Ruiz et al., 2019, Zou et al., 2022, Abid and Zou, 2019, Choudhuri et al., 2019, Severson et al., 2019, Weinberger et al., 2022] mainly use variational autoencoders (VAE) to model la- tent variations only present among target samples and not in the background dataset. Similarly, in [Benaim et al., 2019], authors used standard autoen- coders to estimate common latent patterns between two domains as well as patterns unique to each domain. Being based on auto-encoders, this method cannot sample in the latent space (i.e., no new image generation) and its goal is to map sample images from one domain to the other, as in IMI. Another related method is NestedVAE [Vowels et al., 2020], whose goal is bias reduction by estimating common factors between visual domains using paired data. Here, we wish to use unpaired datasets. Lastly, CA is different from style vs. content sepa- ration and style transfer. In particular, in recent works [Kazemi et al., 2019, von K¨ugelgen et al., 2021], content usually refers to the invariant generative factors across samples and views (i.e., transforma- tions/augmentations of a sample), while style refers to the varying factors. Content and style thus depend on the chosen semantic-invariant transformations, and they are defined for a single dataset. In CA, we do not necessarily need transformations or views, and we jointly analyze two different datasets. 3 Background InfoGAN In [Chen et al., 2016], differently from stan- dard GAN [Goodfellow et al., 2014], authors propose a new method, called InfoGAN, where they decompose the input noise vector of GANs into two parts: 1) z, which is considered as a nuisance and incompressible noise and 2) c, which should model the salient semantic features of the data distribution. The generator of this new model, G(z, c), takes as input both z and c to generate samples x. As shown in [Chen et al., 2016], without regularisation, the generator G may ignore the additional code c or find a trivial (and useless) solu- tion. To this end, authors propose to regularize the estimate of G by maximizing the mutual information I(c; x) between c and x ∼ G(z, c). Maximum I is obtained when c and x are completely dependent and one becomes completely redundant with the knowledge of the other. This should increase the informativeness of c, namely all salient semantic information should be in c and not in z, which should only account for additional randomness (i.e., noise). Authors propose to maximize a lower bound of I(c; x) by defining an aux- iliary distribution Q(c|x), parameterized as a neural network, to approximate P(c|x): I(c; x) ≥ Ez∼P (z),c∼P (c),x∼P (x|c,z) log(Q(c|x)) + H(c) (1) More mathematical details in the Supplementary. Contrastive VAE (CA-VAE) In this section, we present the CA-VAE models [Choudhuri et al., 2019, Severson et al., 2019, Abid and Zou, 2019, Ruiz et al., 2019, Zou et al., 2022, Weinberger et al., 2022]. Let X = {xi} and Y = {yj} be the background (or control) and target data-sets of images respectively. Both {xi} and {yj} are assumed to be i.i.d. from two different and unknown distributions (P(x) and P(y)) that depend on a pair of latent variables (z, s). Here, s is assumed to capture the salient generative factors proper only to Y whereas z should describe the common generative factors between X and Y . The generative models (i.e. same decoder with parameters θ) are: xi ∼ Pθ(x|zi, si = s′) and yj ∼ Pθ(yj|zj, sj), where the salient factors si of X are fixed to a constant value s′ (e.g., s′ = 0), thus enforcing z to fully encode alone X. The conditional posterior distributions are approximated using another neural network (i.e. encoder with parameters ϕ) shared between X and Y , Qϕ(zi, si|xi) and Qϕ(zj, sj|yj), which are usually assumed to be conditional inde- pendent: Qϕ(z, s|·) = Qϕ(z|·)Qϕ(s|·). The latent generative factors (z, s) are also usually assumed to be independent (i.e., P·(z, s) = P·(z)P·(s)). The common factor z should follow the same prior distribution in X and Y (e.g., Px(z) = Py(z) = N(z; 0, I)). The salient factor s follows instead a different prior distribution between X and Y , such as Py(s) = N(s; 0, I) and Px(s) = δ(s = s′), the Dirac distribution centered at s′. Based on this generative latent variable model, one can derive a lower bound of the marginal log likelihood: log P(x) ≥EQϕ(z|x)Qϕ(s|x) log Pθ(x|z, s)− KL(Qϕ(z|x)||px(z)) − KL(Qϕ(s|x)||px(s)) (2) and similarly for log P(y). All existing CA- VAE methods share this mathematical framework. They mainly differ for optimization or architec- tural choices and new added losses. However, none of these methods explicitly enforces the inde- pendence between common and salient latent fac- tors2 and most of them ignore the KL divergence 2[Abid and Zou, 2019] proposed to minimize the total correlation (TC) between qϕz,ϕs(z, s|x) and qϕz(z|x)qϕs(s|x) via the density-ratio trick Double InfoGAN for Contrastive Analysis Figure 2: Double InfoGAN. Our model takes two inputs: z (common factors) and s (salient factors). The generator G produces fake images that, together with the real images, are passed to a discriminator and encoder. The discriminator has two modules: D for detecting real from fake images, and C for classyfing images in the correct domain (i.e., X or Y ). The encoder Q has two modules, Qz and Qs, to reconstruct the latent factors (z, s). D, C and Q share all layers but the last one. term about py(s) (except [Choudhuri et al., 2019] and [Weinberger et al., 2022]), thus allowing a possible in- formation leakage between salient and common factors, as discussed in [Weinberger et al., 2022]. Furthermore, the quality of the generated images is rather poor. 4 Proposed method - Double InfoGAN Model In Double InfoGAN, we use a generative model similar to the one proposed in CA-VAE but within the framework of InfoGAN. We suppose that the back- ground images {xi} iid ∼ P(x) and the target images {yj} iid ∼ P(y), where P(x) and P(y) are unknown and depend on a pair of latent variables (z ∈ RL, s ∈ RM). Differently from InfoGAN, and similarly to CA-VAE, z should now capture the generative factors common to both X and Y whereas s the salient factors proper only to Y . As in GAN [Goodfellow et al., 2014], we intro- duce a generator G and a discriminator. The generator G should generate samples that are indistinguishable from the true ones, whereas the discriminator is divided into two modules. The first (and standard) one D is trained to discriminate between fake and real samples. The second module C is trained to correctly classify real samples (i.e., X or Y ). As in InfoGAN, we also use one encoder, divided into two modules, Qz and Qs, to reconstruct the latent factors z and s. The dis- criminator, D and C, and the encoder, Qz and Qs, are parametrized as neural networks, that share all layers but the output one. [Kim and Mnih, 2018], but their implementation is inaccurate since they don’t use an independent optimizer. Let x = G(z, s = s′) and y = G(z, s) be the generated samples. We suppose, and force it in practice, that the latent variables z = {z1, ..., zL} and s = {s1, ..., sM} are independent and follow a factorized distribution: P(z) = QL i=1 P(cz) and P(s) = QM j=1 P(sj), for X and Y . The total cost function is: min G,Qz,QsCmax D wAdvLAdv(G, D) + wClassLcl(G, C)− wInfoLInfo(G, Qz, Qs) + wImLIm(G, Qz, Qs) (3) In the following, we will describe each term. Adversarial GAN Loss As in [Goodfellow et al., 2014], G and D are trained together in a min-max game using the original nonsaturating GAN (NSGAN) formulation: LAdv(D, G) = wbg \u0010 −ExR∼P (xR) \u0002 log(D(xR) \u0003 − Ez∼Px(z) \u0002 log(1 − (D(G(z, 0)))) \u0003\u0011 + wt \u0010 −EyR∼P (yR) \u0002 log(D(yR) \u0003 − Ez,s∼Py(z,s) \u0002 log(1 − (D(G(z, s)))) \u0003\u0011 (4) where D(I) indicates the probability that I is real or fake and xR ∼ P(xR) and yR ∼ P(yR) are real images. Furthermore, we choose the same factor- ized prior distribution P(z) for both X and Y (i.e., Px(z) = Py(z) = P(z)), namely a Gaussian N(0, 1). We also tested a uniform distribution U[−1,1] but the re- sults were slightly worse. Instead, about P(s), it should be different between X and Y . We use a Dirac delta distribution centered at 0 for X (i.e., Px(s) = δ(s = 0)) and we have tested several distributions for Py(s). De- pending on the data and related assumptions, one could use, for instance, a factorized uniform distribu- tion, U(0,1], or a factorized Gaussian N(0, 1) (ignoring the samples equal to 0). In our experiments, results were slightly better when using N(0, 1). Class Loss To make sure that generated images belong to the correct class, we propose to add a second discriminator module C. It is trained on real images to predict the correct class: X or Y . At the same time, G is trained to produce images correctly classified by C. We (arbitrarily) assign 0 (resp. 1) for class X (resp. Y ) and use the binary cross entropy (B). The loss is: Lcl(C) =ExR∼P (xR) \u0002 B(C(xR), 0) \u0003 + EyR∼P (yR) \u0002 B(C(yR), 1) \u0003 Lcl(G) =Ez∼Px(z)[B(C(G(z, 0)), 0)] + Ez,s∼Py(z,s)[B(C(G(z, s)), 1)] (5) Info Loss Similarly to InfoGAN, we propose two regularization terms based on mutual information, I((z, s); y) and I((z, s = s′); x), to encourage infor- mative latent codes. However, in our case, these two F. Carton, R. Louiset, P. Gori terms are not added to disentangle between informative and nuisance generative factors, but to enforce the sep- aration between common and salient factors. Indeed, the maximization of these two regularity terms should enforce z to fully encode X and at the same time to be informative for the generation of Y . In parallel, s should only encode distinctive semantic information of Y . Please note that the inclusion of two other nuisance factors, similarly to InfoGAN, describing the incom- pressible noise of X and Y , would make the analysis more complex (i.e., additional regularity terms) since they should not model the common nor the salient generative factors. Since z and s are independent by construction, the mutual information I((z, s); ·) can be decomposed into the sum of the two mutual information I(z; ·) + I(s; ·). Thus, similarly to InfoGAN (see Eq. 1), we can re- trieve four lower bounds. As in [Chen et al., 2016, Lin et al., 2020], to promote stability and efficiency, we model the two auxiliary distributions, Qz and Qs, as factorized distributions. Beside a factorized Gaus- sian distribution with identity covariance, we have also tested a factorized Laplace distribution L(µ, b) with b = 1. This brings to a l1 reconstruction loss instead of a standard l2, and showed better performance in practice. Finally, to better train Qs, and since we know that s should be equal to 0 for real images of domain X (i.e., xR ∼ P(xR)), we also add as regularization the lower bound of the mutual information I(s; xR). As before, we fix Px(s) = δ(s = 0). The sum of these five lower bounds defines the LInfo loss: LInfo(G, Qz, Qs) = wbgEz∼Py(z) \u0002 wz Info|(Qz(G(z, 0)) − z| + ws Info|Qs(G(z, 0)) − 0| \u0003 + wtEz,s∼Py(z,s) \u0002 wz Info|(Qz(G(z, s)) − z| + ws Info|Qs(G(z, s)) − s| \u0003 + wreal InfoExR∼P (xR) \u0002 |(Qs(xR)) − 0| \u0003 (6) Image reconstruction loss Differently from usual GAN models, we also propose to maximize the log- likelihood log(P(y)) (and log(P(x))) of the generated images based on the proposed model. Indeed, no likeli- hood is generally available for optimizing the generator G in a GAN model [Goodfellow et al., 2014]. However, here, given a real image yR (or xR), we can use the auxiliary encoder Q = (Qs, Qz) to estimate the latent factors ˆz and ˆs that should generate yR (or xR) and then maximize (an approximation) of the log-likelihood of the generated images y = G(ˆz, ˆs) (or x = G(ˆz, 0)): log P(y) ≥ EyR∼P (yR),(z,s)∼Q(z,s|yR) log P(y|z, s, yR) − EyR∼P (yR)KL(Q(z, s|yR)||P(z, s|yR)) (7) We notice that the second term should tend towards 0 during training thanks to the previous Info Loss.3 We can thus approximate log P(y) by computing only the left term and modeling P(y|z, s, yR) as a Laplace dis- tribution L(µ, b) with b = 1. We use a Laplace distribu- tion, instead of a Gaussian one, since it has been shown, for instance in [Isola et al., 2017], that a l1-loss encour- ages sharper and better image reconstructions than a l2-loss. Similar computations can be done for log P(x). We define LIm(G, Qz, Qs) = log P(x) + log P(y): LIm(G, Qz, Qs) = wbgExR∼P (xR),ˆz=Qz(xR) \u0002 |G(ˆz, 0) − xR| \u0003 + wtEyR∼P (yR),ˆz,ˆs=Q(yR) \u0002 |G(ˆz, ˆs) − yR| \u0003 (8) 5 Results In this section, we present the results of our model on four different visual datasets. Three of them (CelebA with accessories [Weinberger et al., 2022], Cifar-10- MNIST and dSprites-MNIST) have been conceived for the CA setting, giving us the possibility to qualita- tively and quantitatively evaluate the performance of our model. We compare it with two SOTA Contrastive VAE algorithms (cVAE [Abid and Zou, 2019] and MM- cVAE [Weinberger et al., 2022]) that had the best re- sults in [Weinberger et al., 2022].4 The fourth dataset, Brats [Menze et al., 2014], comprises T1-w MR brain images of healthy subject and patient with brain tu- mors, and is used for qualitative evaluation. For quantitative evaluation, we use the fact that the in- formation about attributes (e.g. glasses/hats in CelebA, MNIST digits, Cifar objects) should be present either in the common or in the salient space. Given a test set of images, we first use Q to reconstruct ˆz and ˆs and then train a classifier on them to predict the attribute presence. By evaluating the discriminative power of the classifier, we can understand whether the information about the attributes has been put in the common or salient latent space by the method. Qualitatively, the model can be evaluated by: 1) look- ing at the image reconstruction, 2) generating new images (sampling different salient features) and 3) swap- ping salient features. Given two real images xR ∈ X and yR ∈ Y , we can first estimate the latent factors ( ˆ zX, ˆ sX) and ( ˆzy, ˆ sY ), that should have generated xR and yR, using Q. Then, we can swap the estimated salient features ˆ sX and ˆ sY , and re-generate the images G( ˆ zX, ˆ sY ) and G( ˆ zY , ˆ sX). Implementation details about the architectures and hyper-parameters used in the different experiments can be found in the Supplementary. 3Lower bounds become tight as Q resembles the true P. 4We use the code provided by the authors of MM-cVAE. Double InfoGAN for Contrastive Analysis ˆsy ↑ ˆzy ↓ Best Average Worst Best Average Worst cVAE* 0.84 0.82 0.81 0.78 0.80 0.81 MM-cVAE* 0.85 0.82 nan 0.72 0.76 nan double InfoGAN 0.95 0.95 0.94 0.69 0.71 0.73 Table 1: 5-fold average accuracy on Target CelebA (glasses vs hat). Std is always ≤ 0.01, so we don’t report it for clarity. Best results in bold. *: Results are different from [Weinberger et al., 2022] where no ex- ternal test set is used. CelebA with accessories We use the dataset based on CelebA [Liu et al., 2015b] presented in [Weinberger et al., 2022], where background images X are faces with neither hat of glasses, and target images Y are faces with hat or glasses. We use 20,000 im- ages for training, 10,000 background and 10,000 target, equally divided between glasses and hat. To evaluate the target class separation, we create a test set with im- ages (5,000 with glasses and 5,000 with hat) never seen during training and compute the accuracy of a logistic regression (with 5-fold cross validation) on the recon- structed latent factors ˆsy = Qs(s|y) and ˆzy = Qz(z|y). Results are available in Table 1. Please note that the evaluation protocol in [Weinberger et al., 2022] was dif- ferent since authors did not use an external test set. For a fair comparison, we run all methods 5 times (with different random seeds) for 500 epochs, and re- ported the highest (best), average and lowest (worst) scores. Extensive results are presented in the Sup- plementary. It is interesting to underline that MM- cVAE [Weinberger et al., 2022] does not converge at every run. We have observed a divergence of the KL loss in about 10% of the trainings, which led to a con- vergence failure. We have used the original architecture of the MM-cVAE paper [Weinberger et al., 2022] to re- produce their results. We provide qualitative results in Fig. 3 with image reconstruction and salient feature swap. Please note that this would not be possible with SOTA IMI meth- ods, such as CycleGAN [Zhu et al., 2017] and MU- NIT [Huang et al., 2018], not conceived for the CA set- ting. First of all, we observe that our model produces images of better quality than MM-cVAE, although this could probably be improved using larger GAN architectures, such as BigGAN [Brock et al., 2019] or StyleGAN [Karras et al., 2019]. From a quantitative point of view, our model obtains an average Inception Score (IS) equal to 1.63 ± 0.03 for background images and 2.66 ± 0.02 for target images, whereas MM-cVAE obtains 1.43 ± 0.03 and 1.44 ± 0.01 for background and target images respectively. Similar results were obtained using the Fr´echet inception distance (FID). It is interesting to notice that our model, contrarily to MM-cVAE, preserves the characteristics of the salient elements, such as the opacity of the glasses. Both mod- Original Reconstruction Swap MM- double MM- double cVAE InfoGAN cVAE InfoGAN Figure 3: Image reconstruction and swap with the CelebA with accessories dataset. els struggle to preserve the shape of the original hat, although our method tends to generate a better hat but based on the hairstyle of the person. In Fig. 4, we present qualitative results where we gener- ate images fixing a z in each row and using different s (0 for X, ̸= 0 for Y ). We can see that there is indeed a change of domain, and that the model generates a wide variety of images. When switching from background X to target Y , the characteristics of the person are well preserved, and a salient feature is added, here glasses or hat. Furthermore, we can also notice that our model, being more accurate, is also more sensitive to dataset biases. For instance, we have noticed that in our dataset people with thin, transparent glasses are usually old men. This bias is clearly visible in the second row of Fig.3 and Fig.4. Removing such bias, as in [Barbano et al., 2023], is left as future work. Cifar-10-MNIST dataset We create a new dataset based on Cifar-10 [Krizhevsky, 2009] and MNIST [LeCun, ]. Background images X are Cifar-10 images, and target images Y are also CIFAR-10 with a ran- dom MNIST digit overlaid on it. We use 50k training images, equally divided between X and Y , and 10k test images, equally divided among the MNIST digits. Our model should successfully capture the background variability (i.e., CIFAR objects) only in the common F. Carton, R. Louiset, P. Gori X Y G(z, 0) G(z, si ̸= 0) Figure 4: Fake images generated by our model. In each row, we use the same common feature z for all images, s = 0 for X and different salient features s ̸= 0 for Y . latent space zy, and the MNIST variability (i.e., digits) only in the salient space sy. A perfect classifier would have 100% accuracy on MNIST when using sy and 10% (which corresponds to randomness) when using zy. Conversely, it should have 100% accuracy on Cifar-10 when trained on zy and 10% when trained on sy. We compare our model with MM-cVAE. Since we used the same image size as for CelebA (64 × 64 × 3), we kept the same network architecture. We tested several hyper-parameters for both methods and used the best configuration in our experiments. Results using two different latent space size are shown in Table 2 (for MM-cVAE, we use: λ1 = 102, λ2 = 103). As before, we run both methods 5 times (with different random seeds) for 500 epochs, and report the highest, average and lowest scores. More results in the Suppl. We can notice that our method either outperforms MM-cVAE or obtains comparable results. Moreover, during our numerous trainings, we noticed that the results obtained with our method are very stable, while those obtained with MM-cVAE, as before with CelebA, are more variable and may diverge (i.e., nan). Visual examples are presented in Fig. 5, with image reconstruc- tion and salient feature swap (more in Supplementary). Our model offers sharper images than MM-cVAE and is able to better extract salient features. Ablation study We present in Table 3 a detailed ablation study on the proposed losses using the Cifar- MNIST dataset and the architecture with a latent space of size 128 (since it obtained the best results in Table 2). We can notice that the proposed combination of losses obtains the best results. Original Reconstruction Swap MM- double MM- double cVAE InfoGAN cVAE InfoGAN Figure 5: Image reconstruction and swap with Cifar- 10-MNIST. Brats dataset In this section, we present qualita- tive results on the Brats dataset [Menze et al., 2014]. Background data X contains T1-w MR brain images of healthy subject whereas the target dataset Y has images of patients with brain tumors. Since images are bigger (128 × 128) than the other datatsets, we use a different architecture. More details can be found in the Supplementary. Please note that here there are no sub-categories (as in previous datasets) that can be exploited to compute quantitative metrics (subgroup classification). Fig. 6 shows fake images generated by our model trained on Brats. On the left are healthy images (s = 0), and on the right images with tumor (s ̸= 0). Images in the same row are generated using the same z. We can see that the general anatomy of the brain is pre- served when changing domain, and that tumors with different size and position are generated. By changing z (i.e. row), we can also notice that the model seems to have correctly encoded in z the general anatomical variability of the brain. In Fig.7, we generate healthy counterparts of tar- get images with tumor, setting s = 0. This is very valuable in a clinical setting for multi-modal fusion [Fran¸cois et al., 2022, Maillard et al., 2022], where im- ages from different modalities can exhibit a different topology due to the tumor, and atlas construction Double InfoGAN for Contrastive Analysis Mnist (salient) Cifar (background) sy ↑ zy ↓ sy ↓ zy ↑ Best Avg. Worst Best Avg. Worst Best Avg. Worst Best Avg. Worst MM-cVAE (size 128) 0.81 0.76 nan 0.43 0.48 nan 0.14 0.18 nan 0.36 0.35 nan MM-cVAE (size 200) 0.82 0.63 0.13 0.43 0.58 0.82 0.12 0.17 0.27 0.37 0.36 0.34 double InfoGAN (size 128) 0.87 0.87 0.86 0.25 0.26 0.28 0.17 0.18 0.19 0.43 0.42 0.41 double InfoGAN (size 200) 0.88 0.87 0.86 0.32 0.32 0.32 0.20 0.21 0.23 0.44 0.44 0.43 Table 2: MNIST-Cifar10 classification. Digits information should only be encoded in sy and not in zy, whereas the contrary should be true for Objects information. Std ≤ 0.01. Best results in bold. Mnist (salient) Cifar (bg) sy ↑ zy ↓ sy ↓ zy ↑ -LClass 0.48 0.83 0.23 0.37 - LClass - LIm 0.54 0.72 0.22 0.38 - LInfo - LClass - LIm 0.70 0.70 0.18 0.18 - LInfo 0.85 0.60 0.30 0.36 - LInfo - LIm 0.59 0.59 0.20 0.20 - LClass - LInfo 0.74 0.67 0.29 0.35 - LIm 0.86 0.25 0.20 0.42 Full 0.87 0.26 0.18 0.42 Table 3: Ablation study of the different losses on the Cifar-MNIST dataset. For every configuration, 3 train- ings were launched. We report average values. X - healthy Y - tumor G(z, 0) G(z, si ̸= 0) Figure 6: Fake images generated by our model. In each row, we use the same z for all images with s = 0 for X and different s ̸= 0 for each exemple of Y . [Liu et al., 2015a, Roux et al., 2019], where tumor im- ages have to be registered to healthy templates. Please note that here we use 2D slices with a small archi- tecture (DCGAN), and a small (and biased) dataset (Brats). Indeed, we have noticed that most of the slices containing a tumor are in the central part of the brain (greater size) whereas slices from the higher or lower part of the brain (smaller size) have less frequently a tumor. This might thus entail structural changes dur- ing the generation of the healthy counterpart (swap), such as the one in size in the third row of Fig. 7. This could be solved by directly working with 3D data, more powerful networks and debiasing strategies. Target image Reconstruction Swap (s = 0) Figure 7: Reconstruction (middle) and generation of an healthy counterpart (swap, on the right) of a target image with brain tumor (on the left) by setting s = 0 and keeping the same z. dSprites-MNIST dataset A new toy dataset is proposed for evaluating CA methods. The background dataset X consists of 4 MNIST digits (1, 2, 3 and 4) regularly placed in a square. In the target dataset Y , dSprites element [Matthey et al., 2017] are added on top of the same 4 MNIST digits. Image reconstruction and salient feature swap are presented in Fig. 8. As before, we can see that, compared to MM-cVAE, image reconstructions are more accurate and sharp and, when exchanging salient features, the dSprites elements are better preserved. Disentanglement As in [Higgins et al., 2017, Lin et al., 2020], we also use dSprites to evaluate the disentanglement of our method in the salient space. Indeed, dSprites elements only exhibit 5 possible varia- tions, making it easy to evaluate the disentanglement. Possible variations are: 1) shape (heart, elipse and square), 2) size, 3) position in X, 4) position in Y and 5) orientation (i.e. rotation). As metric, we use the FactorVAE (fvae) score [Kim and Mnih, 2018]. Initial results using the proposed method showed a very poor disentanglement. To further improve it, we adapted for our model the Contrastive Regularizer (CR) module F. Carton, R. Louiset, P. Gori of InfoGAN-CR [Lin et al., 2020] (more details in the Supplementary), obtaining a maximum fvae score of 0.47. For comparison, InfoGAN-CR achieves a fvae score of 0.88 on the dsprite dataset alone. This shows that disentangling salient (or common) factors is much more difficult in our case than when using a single data-set. Exploring disentanglement regularizations more suited for a CA setting is left as future work. In Fig. 9, we show target images generated by our model when varying only one dimension (from -1.5 to 1.5) of sy, while keeping zy fixed. We clearly see a high entanglement among the dSprites factors of variation. For completeness, we also checked whether the CR module helped the separation between common and salient information, and found similar quantitative results (see Supplementary). 6 Conclusions and Perspectives We propose the first GAN-based model for Contrastive Analysis (CA) that estimates and separates in an un- supervised way all common and distinctive generative factors of a target dataset with respect to a back- ground dataset. Compared to current SOTA CA- VAE models, we demonstrate superior performance on 4 visual datasets of increasing complexity and rang- ing from simple toy examples to real medical data. Our method manages to better separate common from salient factors, shows a better image generation qual- ity and a greater stability during training. Further- more, it allows the generation of multiple counterparts between domains by fixing the common factors and adding/removing the salient ones. We believe that the proposed method will benefit from more powerful GAN models and future progress in disentanglement, increasing its accuracy and interpretability. This will widen its fields of application to, for instance, clinically valuable and challenging tasks, such as computer aided- diagnosis. A last interesting research avenue could be the extension to the recent diffusion based models, as [Song et al., 2021, Rombach et al., 2022]. Limitations Recent works have shown that genera- tive models, such as VAE and GAN, are in general not identifiable [Locatello et al., 2019]. To obtain identifi- ability, two different solutions have been proposed: 1) either regularizing [Kivva et al., 2022] / constraining (e.g., making it linear) the encoder or 2) introducing an auxiliary variable so that the latent factors are conditionally independent given the auxiliary variable [Hyvarinen et al., 2019, Khemakhem et al., 2020]. Un- fortunately, in Contrastive Analysis, neither of these solutions may be used5. While all losses proposed here, and in the related works, are needed to effectively sep- arate common from salient factors, they do not assure 5The dataset label could be considered as an auxiliary variable but it does not make c and s independent Original Reconstruction Swap MM- double MM- double cVAE InfoGAN cVAE InfoGAN Figure 8: Image reconstruction and swap of salient features on the dSprites-MNIST dataset. Figure 9: Each row represents the variation of only one element of the salient factor sy, while keeping zy fixed. We can see a certain entanglement, with several parameters changing at the same time: shape and position (line 1), position and orientation (line 2). Only the last line shows a disentanglement, with only the orientation of the ellipse changing. that all true generative factors have been identified. This is the main limitation of this work, and actually of all concurrent CA-VAE models, and is left as future work. Inspired by [Wyner, 1975], a possible research di- rection would be adding an information-theoretic loss that quantifies the common and salient information content so that, under realistic assumptions, the model could be identifiable. Acknowledgments This work was supported by the IDS department of T´el´ecom Paris and by the L’association T´el´ecom Paris Alumni. Double InfoGAN for Contrastive Analysis References [Abid et al., 2018] Abid, A., Zhang, M. J., Bagaria, V. K., and Zou, J. (2018). Exploring patterns en- riched in a dataset with contrastive principal com- ponent analysis. Nat Commun, 9(1):2134. Number: 1 Publisher: Nature Publishing Group. [Abid and Zou, 2019] Abid, A. and Zou, J. (2019). Contrastive Variational Autoencoder Enhances Salient Features. arXiv:1902.04601 [cs, stat]. [Barbano et al., 2023] Barbano, C. A., Dufumier, B., Tartaglione, E., Grangetto, M., and Gori, P. (2023). Unbiased Supervised Contrastive Learning. In ICLR. arXiv:2211.05568 [cs, stat]. [Baur et al., 2021] Baur, C., Denner, S., Wiestler, B., Navab, N., and Albarqouni, S. (2021). Autoencoders for unsupervised anomaly segmentation in brain MR images: A comparative study. Medical Image Anal- ysis, 69(8):1–16. [Benaim et al., 2019] Benaim, S., Khaitov, M., Galanti, T., and Wolf, L. (2019). Domain inter- section and domain difference. Proceedings of the IEEE International Conference on Computer Vision, pages 3444–3452. [Brock et al., 2019] Brock, A., Donahue, J., and Si- monyan, K. (2019). Large scale GaN training for high fidelity natural image synthesis. In International Conference on Learning Representations (ICLR). [Chen et al., 2016] Chen, X., Duan, Y., Houthooft, R., Schulman, J., Sutskever, I., and Abbeel, P. (2016). In- foGAN: Interpretable Representation Learning by In- formation Maximizing Generative Adversarial Nets. In Advances in Neural Information Processing Sys- tems, volume 29. Curran Associates, Inc. [Choi et al., 2018] Choi, Y., Choi, M., Kim, M., Ha, J. W., Kim, S., and Choo, J. (2018). StarGAN: Unified Generative Adversarial Networks for Multi- domain Image-to-Image Translation. Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages 8789–8797. [Choudhuri et al., 2019] Choudhuri, A., Makkuva, A. V., Rana, R., Oh, S., Chowdhary, G., and Schwing, A. (2019). Towards Principled Objectives for Contrastive Disentanglement. [Deun et al., 2012] Deun, K. V., Mechelen, I. V., Thor- rez, L., Schouteden, M., Moor, B. D., Werf, M. J. v. d., Lathauwer, L. D., Smilde, A. K., and Kiers, H. A. L. (2012). DISCO-SCA and Properly Ap- plied GSVD as Swinging Methods to Find Common and Distinctive Processes. PLOS ONE, 7(5):e37840. Publisher: Public Library of Science. [Feng et al., 2018] Feng, Q., Jiang, M., Hannig, J., and Marron, J. S. (2018). Angle-based joint and indi- vidual variation explained. Journal of Multivariate Analysis, 166:241–265. [Fran¸cois et al., 2022] Fran¸cois, A., Maillard, M., Op- penheim, C., Pallud, J., Bloch, I., Gori, P., and Glaun`es, J. (2022). Weighted Metamorphosis for Registration of Images with Different Topologies. In Hering, A., Schnabel, J., Zhang, M., Ferrante, E., Heinrich, M., and Rueckert, D., editors, Biomedi- cal Image Registration, Lecture Notes in Computer Science, pages 8–17, Cham. Springer International Publishing. [Ganin et al., 2017] Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marc- hand, M., and Lempitsky, V. (2017). Domain- Adversarial Training of Neural Networks. In Csurka, G., editor, Domain Adaptation in Computer Vision Applications, Advances in Computer Vision and Pat- tern Recognition, pages 189–209. Springer Interna- tional Publishing, Cham. [Goodfellow et al., 2014] Goodfellow, I., Pouget- Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative Adversarial Nets. In Advances in Neural Information Processing Systems, volume 27. [Guillon et al., 2021] Guillon, L., Cagna, B., Dufumier, B., Chavas, J., Rivi`ere, D., and Mangin, J.-F. (2021). Detection of Abnormal Folding Patterns with Unsu- pervised Deep Generative Models. In Abdulkadir, A., Kia, S. M., Habes, M., Kumar, V., Rondina, J. M., Tax, C., and Wolfers, T., editors, Machine Learning in Clinical Neuroimaging, Lecture Notes in Computer Science, pages 63–72, Cham. Springer International Publishing. [He et al., 2019] He, Z., Zuo, W., Kan, M., Shan, S., and Chen, X. (2019). AttGAN: Facial Attribute Editing by only Changing What You Want. IEEE Transactions on Image Processing, 28(11):5464–5478. [Higgins et al., 2017] Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2017). β-VAE: learning ba- sic visual concepts with a constrained variational framework. In ICLR. [Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J.-Y., Isola, P., Saenko, K., Efros, A., and Darrell, T. (2018). CyCADA: Cycle-Consistent Ad- versarial Domain Adaptation. In Proceedings of the 35th International Conference on Machine Learning, pages 1989–1998. PMLR. ISSN: 2640-3498. F. Carton, R. Louiset, P. Gori [Huang et al., 2018] Huang, X., Liu, M.-Y., Belongie, S., and Kautz, J. (2018). Multimodal Unsupervised Image-to-Image Translation. In ECCV. [Hyvarinen et al., 2019] Hyvarinen, A., Sasaki, H., and Turner, R. E. (2019). Nonlinear ICA Using Auxiliary Variables and Generalized Contrastive Learning. In AISTATS. [Isola et al., 2017] Isola, P., Zhu, J.-Y., Zhou, T., and Efros, A. A. (2017). Image-to-Image Translation with Conditional Adversarial Networks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5967–5976, Honolulu, HI. IEEE. [Joy et al., 2021] Joy, T., Schmon, S. M., Torr, P. H. S., Siddharth, N., and Rainforth, T. (2021). Cap- turing Label Characteristics in VAEs. In ICLR. arXiv:2006.10102 [cs, stat]. [Karras et al., 2019] Karras, T., Laine, S., and Aila, T. (2019). A style-based generator architecture for generative adversarial networks. Proceedings of the IEEE Computer Society Conference on Computer Vi- sion and Pattern Recognition, 2019-June:4396–4405. [Kazemi et al., 2019] Kazemi, H., Iranmanesh, S. M., and Nasrabadi, N. (2019). Style and Content Dis- entanglement in Generative Adversarial Networks. pages 848–856. [Khemakhem et al., 2020] Khemakhem, I., Kingma, D., Monti, R., and Hyvarinen, A. (2020). Varia- tional Autoencoders and Nonlinear ICA: A Unifying Framework. In Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics, pages 2207–2217. PMLR. ISSN: 2640- 3498. [Kim and Mnih, 2018] Kim, H. and Mnih, A. (2018). Disentangling by Factorising. In Proceedings of the 35th International Conference on Machine Learning, pages 2649–2658. PMLR. ISSN: 2640-3498. [Kimura et al., 2020] Kimura, D., Chaudhury, S., Narita, M., Munawar, A., and Tachibana, R. (2020). Adversarial Discriminative Attention for Robust Anomaly Detection. In 2020 IEEE Winter Confer- ence on Applications of Computer Vision (WACV), pages 2161–2170, Snowmass Village, CO, USA. IEEE. [Kingma and Welling, 2014] Kingma, D. P. and Welling, M. (2014). Auto-Encoding Variational Bayes. In ICLR. [Kivva et al., 2022] Kivva, B., Rajendran, G., Raviku- mar, P., and Aragam, B. (2022). Identifiability of deep generative models without auxiliary informa- tion. In NeurIPS. [Krizhevsky, 2009] Krizhevsky, A. (2009). Learning Multiple Layers of Features from Tiny Images. Tech- nical report. [Lample et al., 2017] Lample, G., Zeghidour, N., Usunier, N., Bordes, A., Denoyer, L., and Ranzato, M. (2017). Fader networks: Manipulating images by sliding attributes. Advances in Neural Information Processing Systems, 2017-Decem(Nips):5968–5977. [LeCun, ] LeCun, Y. The mnist database of handwrit- ten digits. Technical report. [Lee et al., 2018] Lee, H.-Y., Tseng, H.-Y., Huang, J.- B., Singh, M., and Yang, M.-H. (2018). Diverse Image-to-Image Translation via Disentangled Repre- sentations. In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y., editors, Computer Vision – ECCV 2018, volume 11205, pages 36–52. Springer Interna- tional Publishing, Cham. Series Title: Lecture Notes in Computer Science. [Lin et al., 2020] Lin, Z., Thekumparampil, K., Fanti, G., and Oh, S. (2020). InfoGAN-CR and ModelCen- trality: Self-supervised Model Training and Selection for Disentangling GANs. In Proceedings of the 37th International Conference on Machine Learning. [Liu et al., 2017] Liu, M.-Y., Breuel, T., and Kautz, J. (2017). Unsupervised Image-to-Image Transla- tion Networks. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc. [Liu et al., 2015a] Liu, X., Niethammer, M., Kwitt, R., Singh, N., McCormick, M., and Aylward, S. (2015a). Low-Rank Atlas Image Analyses in the Presence of Pathologies. IEEE Transactions on Medical Imaging, 34(12):2583–2591. Conference Name: IEEE Trans- actions on Medical Imaging. [Liu et al., 2015b] Liu, Z., Luo, P., Wang, X., and Tang, X. (2015b). Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV). [Locatello et al., 2019] Locatello, F., Bauer, S., Lu- cic, M., Raetsch, G., Gelly, S., Sch¨olkopf, B., and Bachem, O. (2019). Challenging Common Assump- tions in the Unsupervised Learning of Disentangled Representations. In Proceedings of the 36th Inter- national Conference on Machine Learning, pages 4114–4124. PMLR. ISSN: 2640-3498. [Locatello et al., 2020] Locatello, F., Poole, B., Raetsch, G., Sch¨olkopf, B., Bachem, O., and Tschan- nen, M. (2020). Weakly-Supervised Disentanglement Double InfoGAN for Contrastive Analysis Without Compromises. In Proceedings of the 37th International Conference on Machine Learning, pages 6348–6359. PMLR. ISSN: 2640-3498. [Ma et al., 2019] Ma, L., Jia, X., Georgoulis, S., Tuytelaars, T., and Van Gool, L. (2019). Exem- plar Guided Unsupervised Image-to-Image Transla- tion with Semantic Consistency. In ICLR. arXiv. arXiv:1805.11145 [cs]. [Maillard et al., 2022] Maillard, M., Fran¸cois, A., Glaun`es, J., Bloch, I., and Gori, P. (2022). A Deep Residual Learning Implementation of Metamorpho- sis. In 2022 IEEE 19th International Symposium on Biomedical Imaging (ISBI), pages 1–4. ISSN: 1945-8452. [Matthey et al., 2017] Matthey, L., Higgins, I., Hassabis, D., and Lerchner, A. (2017). dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/. [Menze et al., 2014] Menze, B. H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., Burren, Y., Porz, N., Slotboom, J., Wiest, R., et al. (2014). The multimodal brain tumor image segmen- tation benchmark (brats). IEEE transactions on medical imaging, 34(10):1993–2024. [PANG et al., 2022] PANG, G., SHEN, C., CAO, L., and HENGEL, A. V. D. (2022). Deep learning for anomaly detection: A review. ACM Computing Surveys, 54(2):1–38. [Rohlf and Corti, 2000] Rohlf, F. J. and Corti, M. (2000). Use of Two-Block Partial Least-Squares to Study Covariation in Shape. Systematic Biology, 49(4):740–753. Publisher: [Oxford University Press, Society of Systematic Biologists]. [Rombach et al., 2022] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. (2022). High- Resolution Image Synthesis with Latent Diffusion Models. In CVPR, pages 10674–10685. [Roux et al., 2019] Roux, A., Roca, P., Edjlali, M., Sato, K., Zanello, M., Dezamis, E., Gori, P., Lion, S., Fleury, A., Dhermain, F., Meder, J.-F., Chr´etien, F., Lechapt, E., Varlet, P., Oppenheim, C., and Pallud, J. (2019). MRI Atlas of IDH Wild-Type Supratentorial Glioblastoma: Probabilistic Maps of Phenotype, Management, and Outcomes. Radiology, 293(3):633–643. Publisher: Radiological Society of North America. [Ruiz et al., 2019] Ruiz, A., Martinez, O., Binefa, X., and Verbeek, J. (2019). Learning Disentangled Repre- sentations with Reference-Based Variational Autoen- coders. In ICLR workshop on Learning from Lim- ited Labeled Data, pages 1–17, New Orleans, United States. [Severson et al., 2019] Severson, K., Ghosh, S., and Ng, K. (2019). Unsupervised learning with con- trastive latent variable models. In AAAI. arXiv. arXiv:1811.06094 [cs, stat]. [Shi et al., 2021] Shi, Y., Yang, X., Wan, Y., and Shen, X. (2021). SemanticStyleGAN: Learning Compo- sitional Generative Priors for Controllable Image Synthesis and Editing. [Shu et al., 2020] Shu, R., Chen, Y., Kumar, A., Er- mon, S., and Poole, B. (2020). Weakly Supervised Disentanglement with Guarantees. In ICLR. arXiv. arXiv:1910.09772 [cs, stat]. [Smilde et al., 2017] Smilde, A. K., M˚age, I., Næs, T., Hankemeier, T., Lips, M. A., Kiers, H. A. L., Acar, E., and Bro, R. (2017). Com- mon and distinct components in data fusion. Journal of Chemometrics, 31(7):e2900. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.2900. [Song et al., 2021] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. (2021). Score-Based Generative Modeling through Stochastic Differential Equations. In ICLR. [Trygg, 2002] Trygg, J. (2002). O2-PLS for qualitative and quantitative analysis in multivariate calibration. Journal of Chemometrics, 16(6):283–293. eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/cem.724. [Tu et al., 2021] Tu, R., Foss, A. H., and Zhao, S. D. (2021). Capturing patterns of variation unique to a specific dataset. arXiv:2104.08157 [cs, stat]. [Venkataramanan et al., 2020] Venkataramanan, S., Peng, K.-C., Singh, R. V., and Mahalanobis, A. (2020). Attention Guided Anomaly Localization in Images. In Vedaldi, A., Bischof, H., Brox, T., and Frahm, J.-M., editors, Computer Vision – ECCV 2020, volume 12362, pages 485–503. Springer Inter- national Publishing, Cham. Series Title: Lecture Notes in Computer Science. [von K¨ugelgen et al., 2021] von K¨ugelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch¨olkopf, B., Besserve, M., and Locatello, F. (2021). Self-Supervised Learn- ing with Data Augmentations Provably Isolates Con- tent from Style. In Advances in Neural Information Processing Systems, volume 34, pages 16451–16467. [Vowels et al., 2020] Vowels, M. J., Cihan Camgoz, N., and Bowden, R. (2020). NestedVAE: Isolat- ing Common Factors via Weak Supervision. In 2020 IEEE/CVF Conference on Computer Vision and F. Carton, R. Louiset, P. Gori Pattern Recognition (CVPR), pages 9199–9209, Seat- tle, WA, USA. IEEE. [V´etil et al., 2022] V´etil, R., Abi-Nader, C., Bˆone, A., Vullierme, M.-P., Roh´e, M.-M., Gori, P., and Bloch, I. (2022). Learning Shape Distributions from Large Databases of Healthy Organs: Applications to Zero- Shot and Few-Shot Abnormal Pancreas Detection. In Wang, L., Dou, Q., Fletcher, P. T., Speidel, S., and Li, S., editors, Medical Image Computing and Computer Assisted Intervention – MICCAI 2022, Lecture Notes in Computer Science, pages 464–473, Cham. Springer Nature Switzerland. [Weinberger et al., 2022] Weinberger, E., Beebe-Wang, N., and Lee, S.-I. (2022). Moment Matching Deep Contrastive Latent Variable Models. In AISTATS. arXiv. arXiv:2202.10560 [cs]. [Wyner, 1975] Wyner, A. (1975). The common infor- mation of two dependent random variables. IEEE Trans. Inform. Theory, 21(2):163–179. [Yu et al., 2017] Yu, Q., Risk, B. B., Zhang, K., and Marron, J. S. (2017). JIVE integration of imaging and behavioral data. NeuroImage, 152:38–49. [Zhu et al., 2017] Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. (2017). Unpaired Image-to-Image Trans- lation Using Cycle-Consistent Adversarial Networks. In 2017 IEEE International Conference on Computer Vision (ICCV), pages 2242–2251, Venice. IEEE. [Zou et al., 2013] Zou, J. Y., Hsu, D. J., Parkes, D. C., and Adams, R. P. (2013). Contrastive Learning Using Spectral Methods. In Advances in Neural Information Processing Systems, volume 26. Curran Associates, Inc. [Zou et al., 2022] Zou, K., Faisan, S., Heitz, F., and Valette, S. (2022). Joint Disentanglement of Labels and Their Features with VAE. In 2022 IEEE In- ternational Conference on Image Processing (ICIP), pages 1341–1345, Bordeaux, France. IEEE. "
}