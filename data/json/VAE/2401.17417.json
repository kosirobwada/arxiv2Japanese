{
    "optim": "THROUGH-WALL IMAGING BASED ON WIFI CHANNEL STATE INFORMATION\nJulian Strohmayer, Rafael Sterzinger, Christian Stippel, Martin Kampel\nComputer Vision Lab, TU Wien\nFavoritenstr. 9/193-1, 1040 Vienna, Austria\nABSTRACT\nThis work presents a seminal approach for synthesizing im-\nages from WiFi Channel State Information (CSI) in through-\nwall scenarios.\nLeveraging the strengths of WiFi, such\nas cost-effectiveness, illumination invariance, and wall-\npenetrating capabilities, our approach enables visual mon-\nitoring of indoor environments beyond room boundaries and\nwithout the need for cameras. More generally, it improves\nthe interpretability of WiFi CSI by unlocking the option to\nperform image-based downstream tasks, e.g., visual activity\nrecognition. In order to achieve this crossmodal translation\nfrom WiFi CSI to images, we rely on a multimodal Varia-\ntional Autoencoder (VAE) adapted to our problem specifics.\nWe extensively evaluate our proposed methodology through\nan ablation study on architecture configuration and a quan-\ntitative/qualitative assessment of reconstructed images. Our\nresults demonstrate the viability of our method and highlight\nits potential for practical applications.\nIndex Terms— WiFi, Channel State Information, Trough-\nWall Imaging, Image Synthesis, Multimodality\n1. INTRODUCTION\nWhile Human Behavior Modeling (HBM) has traditionally\nfocused on optical modalities such as RGB, IR, depth, or\nthermal imaging to meet the requirements of person-centric\nvision applications [1], there is a steady rise in the popular-\nity of WiFi as a sensing modality in this domain [2]. Its ap-\npeal lies in its capacity for discreet monitoring of vast indoor\nenvironments and its practical advantages, such as illumina-\ntion invariance [3], cost-effectiveness [4], and the protection\nof visual privacy – a crucial requirement in privacy-sensitive\napplications [5]. Despite these advantages, interpreting WiFi\nsignals poses a significant challenge: Deducting human ac-\ntivities or behavior solely based on WiFi Channel State In-\nformation (CSI) is inherently difficult, if not impossible (see\nFigure 1c, which showcases the CSI amplitude spectrogram\nof a person walking in a room).\nTo address this problem, our work aims to bridge the gap\nbetween WiFi CSI and images. We propose a new approach\nthat enables visual monitoring in through-wall scenarios\nand unlocks the potential for image-based downstream tasks\n(a) image ground truth\n(b) image reconstruction\n(c) CSI amplitude spectrogram\nFig. 1: Example of an image reconstructed from through-\nwall WiFi CSI. (a) Shows the ground truth image captured\nby a camera within the room and (b) the corresponding im-\nage reconstruction from (c) the CSI amplitude spectrogram\nrecorded by a WiFi antenna located outside of the room.\nbased on WiFi CSI, such as the visual annotation of activities.\nOn top of this, our method also preserves visual privacy as\nidentities are hallucinated based on training data. Inspired\nby brain decoding techniques that utilize brain waves for im-\nage synthesis [6], we apply a similar concept to the domain\nof WiFi CSI. Our approach represents, to our knowledge,\nthe first method for synthesizing images directly from WiFi\nCSI captured in a through-wall scenario.\nLeveraging the\nMoPoE-VAE [7], a Variational Autoencoder (VAE) designed\nfor multimodal data, our approach brings the theoretical ad-\nvantages of WiFi-based person-centric sensing to practical\napplications.\nThe capability of our method to facilitate visual monitor-\ning in through-wall scenarios without conventional cameras\nis illustrated in Figure 1. In this example, the image in Fig-\nure 1b is reconstructed solely from the WiFi CSI amplitude\nspectrogram shown in Figure 1c. Its close resemblance to the\nground truth (Figure 1a) demonstrates the effectiveness of our\narXiv:2401.17417v1  [cs.CV]  30 Jan 2024\napproach and its potential for enhancing the interpretability\nof WiFi CSI by unlocking image-based downstream tasks. To\nstimulate further research in this emerging field, data, models,\nand code are made publicly available1.\n2. RELATED WORK\nAdvancements in deep learning and sensing technologies\nhave enabled innovative approaches to extract person-centric\ninformation from modalities such as WiFi and radar in\nthrough-wall scenarios.\nAn initial successful attempt that\nuses WiFi signals in a through-wall setting to estimate human\nposes is presented by Zhao et al. [3], starting a whole line of\nresearch that focuses on RF-based HBM. Subsequently, Li\net al. [8] were able to generate binary segmentation masks\nfrom WiFi CSI, and Hernandez and Bulut [9] propose an\nadversarial approach for occupancy monitoring using WiFi\nCSI. In continuation of their work, Geng et al. [10] present an\nadvancement in WiFi-based pose estimation, focusing on a\nmulti-person setup, which is, however, limited to intra-room\nrecording. Another work, by Wu et al. [11], demonstrates the\ngeneration of binary segmentation masks directly from radar\nsignals, adding a new dimension to radar-based synthesis\nand enabling through-wall object segmentation using radar\nsignals.\nRecently, image synthesis from WiFi signals has experi-\nenced significant developments. Notably, the work by Yu et\nal. [12] demonstrates an innovative two-step process in this\ndomain: First, human poses are estimated from WiFi CSI and\nsubsequently used to synthesize images, employing a condi-\ntional generative adversarial network. Another approach, al-\nthough limited to intra-room scenarios, is introduced by Yu\net al. [13]. Here, they present a direct method for synthesiz-\ning images from radar signals, marking the first attempt in\nthis specific field. It is important to note that neither of these\napproaches was applied in through-wall scenarios, leaving a\ngap that is closed in this work: direct WiFi CSI-based image\nsynthesis in through-wall scenarios.\nBenchetrit et al. [6] present an approach for synthesizing\nimages from electroencephalography (EEG) measurements\nextracted from a human brain. In their approach, they map\nimages and EEG spectrograms into a shared latent space and\nhallucinate images using a latent diffusion model.\nVAEs\nhave emerged as a powerful tool in multimodal unsupervised\nlearning, which has experienced drastic improvements over\nthe years. For instance, Wu et al. [14] enhance the efficacy\nof VAEs by assuming the multimodal joint posterior is a\nproduct of unimodal posteriors. In this way, information can\nbe aggregated from any subset of unimodal posteriors and\ncan, therefore, be used efficiently when dealing with missing\nmodalities. However, their technique no longer guarantees a\nvalid lower bound on the joint log-likelihood. Expanding on\n1Supplementary Material, Zenodo (available from 27-10-2024)\nRX\n 5.3m \n 3.8m \ntable\ntable\nshelf        \ntable\nTX\n0.25m \nC\nrecording area\nFig. 2: Overview of the experimental setup, showing the ar-\nrangement of WiFi transmitter (TX), WiFi receiver (RX), and\nRGB camera (C) in the office recording environment.\nthis, Shi et al. [15] assume that the joint posterior is a mixture\nof unimodal posteriors, facilitating translation between pairs\nof modalities. However, it lacks the capability to leverage\nthe presence of multiple modalities, as it solely considers\nunimodal posteriors during training. Finally, Sutter et al. [7]\novercome the limitations of the previous two methodologies,\noffering efficient many-to-many translation while preserving\na valid lower bound on the joint log-likelihood. Our work\nis grounded in these foundational studies, leveraging the ad-\nvancements in VAEs for multimodal unsupervised learning to\nbridge WiFi CSI signals with images.\n3. DATA\nOur experimental setup, illustrated in Figure 2, is designed to\njointly capture through-wall WiFi CSI and images of activi-\nties within a target room (3.8m×5.3m office). In this setup,\na point-to-point WiFi transmitter-receiver arrangement is po-\nsitioned outside the room, accompanied by an intra-room\ncamera. The WiFi transmitter and receiver systems utilize\nthe Espressif ESP32-S3-DevKitC-1U2 development board,\nequipped with an ALFA Network APA-M253 directional panel\nantenna featuring a 66◦ horizontal beam width and a gain of\n8dBi @2.4GHz. The connection between transmitter and re-\nceiver is established using the Espressif ESP-NOW4 protocol,\nemploying a fixed packet-sending rate of 100Hz. Images are\ncaptured at a resolution of 640×480 pixels and a frame rate\nof 30Hz using an ESP32-S3-based camera board5. To ensure\ntemporal synchronization, both the WiFi receiver and cam-\nera are connected to a notebook that concurrently captures\nWiFi packets and images through serial over USB and WiFi\nconnections, respectively.\n2ESP32-S3-DevKitC-1U, https://docs.espressif.com/, (acc. 21-01-2024)\n3APA-M25, https://alfa-network.eu/apa-m25, (acc. 21-01-2024)\n4ESP-NOW, https://docs.espressif.com (acc. 21-01-2024)\n5ESP32-S3-CAM, https://github.com/ (acc. 21-01-2024)\nFor training and evaluation purposes, we create a dataset\nof temporally synchronized CSI and image pairs by recording\na ten-minute sequence of continuous walking activities in the\ntest environment, utilizing the described hardware setup. The\nraw time series resulting from this comprises 57,413 WiFi\npackets and 18,261 images, i.e., around three WiFi packets\nreceived per image. Using a visual cue, we eliminate exces-\nsive samples at the start and end of the sequence that do not\ndepict the target activity as part of cleaning the raw time se-\nries. Additionally, to account for the sampling rate difference,\neach WiFi packet is paired with the image having the closest\ntimestamp. Finally, the dataset is partitioned into training,\nvalidation, and test subsets using an 8:1:1 split ratio. Figure 3\nprovides an example of a CSI amplitude spectrogram show-\ning the amplitudes of 52 Legacy Long Training Field (L-LTF)\nsubcarriers over a ∼1.5-second time interval (150 WiFi pack-\nets) alongside the image corresponding to the central packet.\nOur data is made publicly available to ease further research1.\n4. ARCHITECTURE\nIn order to infer images from WiFi CSI, we employ MoPoE-\nVAE [7], a multimodal VAE to learn a posterior distribution\nover a joint latent variable z ∈ RD given our two modalities,\nas well as a corresponding decoder to reconstruct images from\na sampled latent vector.\nGiven a set of X :=\n\b\nXi\tN\ni=1 samples of images at hand,\neach paired with a fixed time interval of WiFi packets (the\ncentral packet determines the corresponding image), i.e.,\nXi =\n\b\nXi\nI, Xi\nW\n\t\n, we aim to maximize the log-likelihood of\nour data at hand:\nlog pθ(X) = log pθ\n\u0000\b\nXi\tN\ni=1\n\u0001\nWithin VAEs, this is achieved by maximizing a lower bound\nto this objective, the so-called evidence lower bound. For\na given sample Xi, this lower bound on the marginal log-\nlikelihood takes on the following form:\nL(θ, ϕ; Xi) := Eqϕ(z|Xi)\n\u0002\nlog\n\u0000pθ(Xi|z)\n\u0001\u0003\n−βDKL\n\u0000qϕ(z|Xi)||pθ(z)\n\u0001\n,\nwhere DKL denotes the Kullback-Leibler divergence [16]\nbetween the approximated posterior and the assumed, in our\ncase, Gaussian prior, and β being an additional weight pa-\nrameter (see Higgins et al. [17]), which promotes disentan-\nglement of the latent variable z, in the case that β > 1. During\ninference, when image data is missing, i.e. Xi\nW := Xi\\\n\b\nXi\nI\n\t\n,\nwe still aim to obtain a valid lower bound on the joint proba-\nbility log pθ(Xi). However, when using L(θ, ϕ; Xi\nW ), it only\nyields a lower bound on log pθ(Xi\nW ).\nHence, to obtain a\ncorrect lower bound on the joint probability, the following\n(a) CSI amplitude spectrogram\n(b) image\nFig. 3: Example of a CSI amplitude spectrogram showing the\namplitudes of 52 L-LTF subcarriers over a ∼1.5-second time\ninterval and the image corresponding to the central packet.\nadapted lower bound is used:\nLW (θ, ϕW ; Xi) := E˜qϕW (z|Xi\nW )\n\u0002\nlog\n\u0000pθ(Xi|z)\n\u0001\u0003\n−βDKL\n\u0000˜qϕW (z|Xi\nW )||pθ(z)\n\u0001\n.\nIn the general naive case with M different modalities, ap-\nproximating a lower bound of the joint probability in any case\nof missing modalities requires the optimization of 2M differ-\nent models, one for each subset contained within the power-\nset, posing a drastic scalability issue. Compared to prior lit-\nerature, Sutter et al. [7] circumvent this by modeling the joint\nposterior approximation as a so-called Mixture of Products of\nExperts (MoPoE), combining Product of Experts (PoE) [14]\nand Mixture of Experts (MoE) [15] through abstract mean\nfunctions [18], to enable efficient retrieval of the joint poste-\nrior for all subsets. For more information, we refer the reader\nto consult [7].\nWhile our task at hand considers only two modalities\n(WiFi CSI, images) and we focus solely on the reconstruction\nof images from WiFi CSI, we still opt for the same methodol-\nogy to allow for additional modalities in the future. However,\nwe employ custom VAEs designed for inference from WiFi\nCSI and adapt the loss to neglect the possibility of decoding\na sequence of images, as it is not required.\n4.1. Image Reconstruction\nGiven the stark difference of our modalities, we require two\narchitectural different models for learning the unimodal pos-\nterior distributions qϕI(z|Xi\nI) and qϕW (z|Xi\nW ):\nImage VAE. For the encoding and decoding of images,\nwe employ convolutional and transpose-convolutional layers,\nrespectively.\nAdditionally, we follow each of these layers\nwith batch normalization and a Leaky ReLU activation. Be-\nfore inferring the distribution parameters of the latent variable\nz given images using the encoder, we rescale the input from\na resolution of 640 × 480 down to 128 × 128 pixels in or-\nder to reduce computational complexity and apply normaliza-\ntion using per-channels means and standard deviations. Next,\nsix consecutive convolutional blocks increase the channel size\nfrom three (RGB) to 512, followed by a simple Multi-Layer\nPerceptron (MLP) to map from the flattened output of the con-\nvolutions to the unimodal distribution parameters. Decoding\nImage Decoder\nImage Encoder\n48\nConv. > BN > LReLU\nTConv. > BN > LReLU\nAggregation\nConv. > Tanh\nTemporal Encoder\nCSI Timestamps\nRGB Image\nCSI Amplitudes\nImage Timestamps\nCSI Encoder\n96\n128 192\n512\n256\n48\n96\n512 256 192 128\n3\nMLP\nMLP\nMLP\nPoE\nMoE\nZ\nReconstruction\n Inference \nFig. 4: Proposed MoPoE-VAE architecture for WiFi CSI-based image synthesis.\nis performed, taking a latent vector z and reversing the pro-\ncess using transpose-convolutions.\nCSI VAE. Concerning our CSI VAE, we first extract in-\nformation about the signal’s amplitude from the raw WiFi\nCSI and apply an MLP to embed the input per sample of a\ngiven sequence to obtain a richer feature representation. Next,\nwe explore the use of different/no aggregation options along\nthe time dimension before estimating the distribution param-\neters: uniform feature weighing, Gaussian feature weighing,\nand concatenation, i.e., flattening the output and leaving it\nunmodified. We describe these options in detail below; their\neffectiveness is evaluated within an ablation study (see Sec-\ntion 5). Lastly, we take the aggregated/concatenated output\nand employ another MLP to obtain the unimodal distribution\nparameters of WiFi CSI. Finally, note that we neglect the re-\nconstruction of WiFi CSI from a latent vector z as it is not\nrelevant to our task.\nDuring training, we combine the unimodal distribution\nparameters of our two VAEs, first, to subsets using PoE, and\nsecond, combine all subsets within the powerset using MoE.\nWith predicted parameters µ, σ ∈ RD, we then sample a la-\ntent vector z, using the reparametrization trick; z = µ+σ⊙ϵ,\nwith ϵ ∼ N(0, 1). At inference, we only consider the mode\nof the approximated posterior distribution, predicted by the\nCSI VAE, for reconstruction, i.e., the predicted mean.\nAn overview of our proposed methodology for synthesiz-\ning images from through-wall WiFi CSI is provided in Fig-\nure 4, illustrating both training and inference.\n4.2. Aggregation Options\nAs noted previously, regarding the CSI VAE, we consider dif-\nferent aggregations to find the most meaningful representa-\ntion for predicting an image (determined by the central WiFi\npacket) from a given sequence.\nLet ˜XW ∈ RL×H be the embedded WiFi CSI ampli-\ntudes after applying the first encoder MLP, where L is the\nsequence length, and H is the hidden dimension. First, as a\nbaseline, we consider a uniform weighing of our features, i.e.,\ngiving each packet equal weight and thus neglecting the order\nof arrival and the importance of the central packet. Next, to\nplace more importance on the central packet and reduce per-\nmutation invariance, we examine a Gaussian weighing with\nµ = σ = L/2. Both of these options are prone to flickering\nor, in other words, drastic changes between two consecutive\nframes. Finally, to overcome this issue, we consider concate-\nnation, leaving the features as is to be fully permutation sen-\nsitive. Hence, depending on the employed options, the input\nto the second encoder MLP lies either in RLH or RH.\n4.3. Temporal Encoding\nA final adjustment to our architecture is posed by temporal\nencoding. Although concatenation alleviates the issue of dras-\ntic changes between two consecutive frames, a slight jittering\nartifact remains. By adding time information using sinusoidal\nfunctions, we are able to improve upon this issue, following\na similar strategy as employed by NeRF [19]. The encoding\nfor a timestamp t and a set of frequencies F is defined as:\nT(t, F) =\nh\nsin\n\u001020πt\n3L\n\u0011\n, cos\n\u001020πt\n3L\n\u0011\n, . . . ,\nsin\n\u00102F −1πt\n3L\n\u0011\n, cos\n\u00102F −1πt\n3L\n\u0011i\n,\nwhere t is scaled by three times the window size L to have\ncontextual time information from before and after the current\nwindow. Specifically, when incorporating temporal encod-\ning, we concatenate the encoding with the image/WiFi fea-\ntures stemming from the CNN/MLP. This concatenated set of\nfeatures is then fed through the respective MLP to map to the\ncorresponding distribution parameters (see Figure 4).\n(a) image ground truth\n(b) UW\n(c) GW\n(d) C\n(e) C+T\nFig. 5: Comparison of reconstruction fidelity between MoPoE-VAE models employing the aggregation options (b) uniform\nweighing (UW), (c) Gaussian weighing (GW), (d) concatenation (C), and (e) concatenation with temporal encoding (C+T).\nThis visual comparison highlights the improvements in image clarity and reduction of artifacts.\nTable 1: Quantitative ablation study results for the aggre-\ngation options uniform weighing (UW), Gaussian weighing\n(GW), concatenation (C), and concatenation with temporal\nencoding (C+T). The metrics reported represent the mean and\nstandard deviation across ten independent training runs.\nModel\nPSNR↑\nSSIM↑\nRMSE↓\nFID↓\nUW\n20.03±.05\n0.734±.00\n8.02±.04\n142.95±2.4\nGW\n20.02±.07\n0.734±.00\n8.00±.05\n141.00±5.0\nC\n20.39±.05\n0.748±.00\n7.83±.05\n127.33±4.7\nC+T\n20.39±.04\n0.749±.00\n7.80±.02\n125.62±3.2\n5. EVALUATION\nTo assess the correlation between reconstruction fidelity and\nour architectural optimizations, an ablation study is con-\nducted.\nIn this study, we quantitatively and qualitatively\nreport the reconstruction performance of different model\nvariants, assessing perceived image quality and visual arti-\nfacts.\n5.1. Model Training\nUtilizing the captured dataset, we train models based on the\nproposed MoPoE-VAE architecture for WiFi-CSI-based im-\nage synthesis, considering the following variants: uniform\nweighing (UW), Gaussian weighing (GW), and concatena-\ntion (C), as well as the combination of concatenation and\ntemporal encoding (C+T). On top of this, a hyperparameter\nsearch is conducted, employing C+T on the validation subset,\nto determine suitable values for training hyperparameters. We\nevaluate over batch size b ∈ {16, 32, 64, 128, 256, 512}, win-\ndow size L ∈ {51, 101, 151, 201, 251, 301}, and the DKL\nweighing parameter β ∈ {1, 2, 4, 6}. This resulted in optimal\nvalues of b = 32, L = 151, and β = 1, which we subse-\nquently use for training all models. Concerning the recorded\nWiFi sequence, for each packet, we extract the 52 L-LTF sub-\ncarriers from the complex CSI matrix and precompute am-\nplitudes which are used to sample 52 × L spectrograms for\nthe CSI VAE to process. Furthermore, the raw images are\nrescaled to a resolution of 128×128 pixels and normalized\nusing the per-channel means and standard deviations of the\ndataset. For each configuration, utilizing the Adam optimizer\n(lr = 1e−3), we conduct ten independent training runs span-\nning 50 epochs and select the model with the lowest validation\nloss for the final evaluation on the test subset.\nMetrics. To assess the image reconstruction fidelity of\nour models, we employ metrics, well-established in compres-\nsion/generative model research, such as the Peak Signal-to-\nNoise Ratio (PSNR), the Structural Similarity Index Measure\n(SSIM) [20], the Root Mean Squared Error (RMSE), and the\nFr´echet Inception Distance (FID) [21].\n5.2. Quantitative Results\nTable 1 summarizes the quantitative results of our ablation\nstudy, providing a comparison between the MoPoE-VAE ag-\ngregation options UW, GW, C, and C+T based on the PSNR,\nSSIM, RMSE, and FID metric, averaged over ten indepen-\ndent training runs. Among these models, UW and GW are the\nleast effective, yielding comparatively low PSNR, SSIM, and\nhigh RMSE and FID scores. We improve upon this by intro-\nducing concatenation (C), which turns out to have a signifi-\ncant impact, evidenced by a higher PSNR and SSIM, along\nwith lower RMSE and FID scores.\nThis stark contrast in\nquantitative performance is also reflected visually, leading to\nnotable enhancements in sharpness and perceptual fidelity.\nHowever, C+T (Figure 5e) outperforms all previous aggrega-\ntion options, exhibiting slightly higher PSNR and SSIM, cou-\npled with lower RMSE and FID scores, compared to C (Fig-\nure 5d). Additionally, we point out the observed improvement\nin FID scores for C+T, suggesting reduced perceptual dis-\ntance and qualitative enhancements. Despite the only subtle\nimpact of temporal encoding when combined with concate-\nnation, reconstructed images are slightly sharper, and, more\nimportantly, frame transitions are more coherent. In sum-\nmary, while C+T demonstrates superior quantitative perfor-\nmance, its marginal differences highlight the importance of\nconsidering both quantitative metrics and visual assessments.\n5.3. Qualitative Results\nOur qualitative examination aligns with the trends observed in\nthe quantitative analysis; Figure 5 illustrates the impact of dif-\nferent aggregation options on image reconstruction fidelity. In\nFigure 5b, the uniform weighing (UW) model yields a blurry\nreconstruction, consistent with its lower PSNR and higher\nRMSE scores (see Table 1). We hypothesize that the lack\nof focus in the temporal domain results in diminished per-\nceptual quality. Moving to Figure 5c, the Gaussian weight-\ning (GW) model showcases a slight improvement in quality\ndespite the similarity in quantitative metrics. This improve-\nment is attributed to the prioritization of central WiFi packets\nduring weighing; that is, packets that are in closer temporal\nproximity to the ground truth image have a higher contribu-\ntion, resulting in a clearer and more accurate reconstruction.\nNext, Figure 5d illustrates the concatenation (C) model: By\nlearning the aggregation of WiFi features itself, not only fur-\nther improves the sharpness of images, but even enhances the\nquality on a frame-to-frame level, eliminating spatiotemporal\ndiscontinuities, as illustrated in Figure 6. Quantitatively, C\noutperforms UW and GW, yet subtle artifacts and jittering re-\nmain visible in videos compared to C+T, possibly contributed\nby marginal differences in FID scores. Finally, the peak of\nour architectural improvements is reached when employing\nthe C+T model, which is showcased in Figure 5e, incorporat-\ning both concatenation and temporal encoding. Employing\ncyclic temporal encoding effectively mitigates the previously\nmentioned jittering artifact, resulting in smoother frame tran-\nsitions and reduced FID scores. For a perspective on video\nquality, reconstructed videos complement these static images,\nshowcasing for each aggregation option its ability to recon-\nstruct images based on trough-wall WiFi CSI1.\nIn conclusion, these qualitative results demonstrate the ca-\npability of our method to facilitate through-wall visual mon-\nitoring without conventional cameras. The successful recon-\nstruction of images from WiFi CSI not only validates our ap-\nproach but also suggests its potential for image-based down-\nstream tasks. This represents a significant step toward en-\nhancing the interpretability of WiFi CSI, providing a founda-\ntion for further exploration and research in this evolving field.\n6. FUTURE WORK\nWhile the MoPoE-VAE+C+T model achieves promising re-\nsults on our dataset, effectively demonstrating the feasibility\nof image synthesis from through-wall WiFi CSI, we do not\nexpect it to generalize to new people or environments with-\nout further adaptations. Generalization is, in general, an open\nproblem in WiFi-based person-centric sensing, and achieving\nit across diverse scenarios in practice will require additional\nsteps, posing a promising opportunity for future research [22].\nIn addition to improving generalization, another promis-\ning direction for future work involves expanding upon the\nmodalities used in our approach. Employing the MoPoE-VAE\ni0\ni25\ni50\ni75\ni100\nimages\n1-SSIM\ndiscontinuity\nGaussian weighing\nconcatenation\nFig. 6: Example showing the elimination of spatiotemporal\ndiscontinuities in a sequence of 100 test images with high\n1-SSIM, highlighted in red, through the aggregation option\nconcatenation. From top to bottom, the rows show Gaussian\nweighing (GW), concatenation (C), and ground truth, respec-\ntively.\nframework, our model is inherently adaptable to multimodal\ndata, leaving the flexibility for incorporating additional sen-\nsory data. For instance, combining WiFi CSI with radar, au-\ndio, or structural vibrations could provide complementary in-\nformation and aid in scenarios where one modality alone may\nbe insufficient for accurate reconstructions, e.g., in settings\nwhere the WiFi signal is disturbed or ambiguous due to envi-\nronmental effects.\n7. CONCLUSION\nIn this work, we have demonstrated the direct synthesis of\nperson-centric images from WiFi CSI captured in a through-\nwall scenario. We collected a dataset of WiFi CSI and cor-\nresponding image time series, on the basis of which an ab-\nlation study was conducted to assess the fidelity of recon-\nstructed images stemming from different architectural varia-\ntions. From this evaluation, the model pairing the aggregation\noption concatenation with temporal encoding emerged as su-\nperior. Our results demonstrate the viability of the proposed\napproach, suggesting its potential for novel applications, such\nas through-wall visual monitoring without conventional cam-\neras, and improving the interpretability of WiFi CSI by un-\nlocking image-based downstream tasks, such as visual activ-\nity recognition.\n8. ACKNOWLEDGMENTS\nThis work is partially funded by the Vienna Business Agency\n(Blindsight-grant 4829418).\n9. REFERENCES\n[1] Julian Strohmayer and Martin Kampel, “A compact tri-\nmodal camera unit for rgbdt vision,” in 2022 the 5th\nInternational Conference on Machine Vision and Appli-\ncations (ICMVA), 2022, pp. 34–42.\n[2] Biying Fu, Naser Damer, Florian Kirchbuchner, and Ar-\njan Kuijper,\n“Sensing technology for human activity\nrecognition: A comprehensive survey,” IEEE Access,\nvol. PP, pp. 1–1, 01 2020.\n[3] Mingmin Zhao, Tianhong Li, Mohammad Abu Al-\nsheikh, Yonglong Tian, Hang Zhao, Antonio Torralba,\nand Dina Katabi, “Through-wall human pose estima-\ntion using radio signals,” in 2018 IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2018, pp.\n7356–7365.\n[4] Robert Schumann, Fr´ed´eric Li, and Marcin Grzegorzek,\n“Wifi sensing with single-antenna devices for ambient\nassisted living,” 10 2023, pp. 1–8.\n[5] Katrin Arning and Martina Ziefle,\n““get that camera\nout of my house!” conjoint measurement of preferences\nfor video-based healthcare monitoring systems in pri-\nvate and public places,” in Inclusive Smart Cities and\ne-Health, Antoine Geissb¨uhler, Jacques Demongeot,\nMounir Mokhtari, Bessam Abdulrazak, and Hamdi\nAloulou, Eds., Cham, 2015, pp. 152–164, Springer In-\nternational Publishing.\n[6] Yohann Benchetrit, Hubert Banville, and Jean-R´emi\nKing, “Brain decoding: toward real-time reconstruction\nof visual perception,” 2023.\n[7] Thomas M. Sutter, Imant Daunhawer, and Julia E. Vogt,\n“Generalized multimodal ELBO,” in International Con-\nference on Learning Representations, 2020.\n[8] Chenning Li, Zheng Liu, Yuguang Yao, Zhichao Cao,\nMi Zhang, and Yunhao Liu, “Wi-fi see it all: generative\nadversarial network-augmented versatile wi-fi imaging,”\nProceedings of the 18th Conference on Embedded Net-\nworked Sensor Systems, 2020.\n[9] Steven M. Hernandez and Eyuphan Bulut, “Adversar-\nial occupancy monitoring using one-sided through-wall\nwifi sensing,” in ICC 2021 - IEEE International Con-\nference on Communications, 2021, pp. 1–6.\n[10] Jiaqi Geng, Dong Huang, and Fernando De la Torre,\n“DensePose from WiFi,” .\n[11] Zhi Wu, Dongheng Zhang, Chunyang Xie, Cong Yu,\nJinbo Chen, Yang Hu, and Yan Chen, “Rfmask: A sim-\nple baseline for human silhouette segmentation with ra-\ndio signals,” IEEE Transactions on Multimedia, vol. 25,\npp. 4730–4741, 2022.\n[12] Cong Yu, Dongheng Zhang, Chunyang Xie, Zhi Lu,\nYang Hu, Houqiang Li, Qibin Sun, and Yan Chen,\n“Wifi-based human pose image generation,”\nin 2022\nIEEE 24th International Workshop on Multimedia Sig-\nnal Processing (MMSP), 2022, pp. 1–6.\n[13] Cong Yu, Zhi Wu, Dongheng Zhang, Zhi Lu, Yang Hu,\nand Yan Chen,\n“Rfgan: Rf-based human synthesis,”\nIEEE Transactions on Multimedia, 2022.\n[14] Mike Wu and Noah Goodman, “Multimodal generative\nmodels for scalable weakly-supervised learning,” in Ad-\nvances in Neural Information Processing Systems. 2018,\nvol. 31, Curran Associates, Inc.\n[15] Yuge Shi, Siddharth N, Brooks Paige, and Philip Torr,\n“Variational mixture-of-experts autoencoders for multi-\nmodal deep generative models,” in Advances in Neural\nInformation Processing Systems. 2019, vol. 32, Curran\nAssociates, Inc.\n[16] Solomon Kullback and Richard A Leibler, “On infor-\nmation and sufficiency,”\nThe annals of mathematical\nstatistics, vol. 22, no. 1, pp. 79–86, 1951.\n[17] Irina Higgins, Loic Matthey, Arka Pal, Christopher\nBurgess, Xavier Glorot, Matthew Botvinick, Shakir Mo-\nhamed, and Alexander Lerchner,\n“β-VAE: Learning\nBasic Visual Concepts with a Constrained Variational\nFramework,” 2017.\n[18] Frank Nielsen, “On the jensen–shannon symmetrization\nof distances relying on abstract means,” Entropy, vol.\n21, no. 5, 2019.\n[19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng,\n“Nerf: Representing scenes as neural radiance fields for\nview synthesis,” Communications of the ACM, vol. 65,\nno. 1, pp. 99–106, 2021.\n[20] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P\nSimoncelli, “Image quality assessment: from error vis-\nibility to structural similarity,”\nIEEE transactions on\nimage processing, vol. 13, no. 4, pp. 600–612, 2004.\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter, “Gans trained\nby a two time-scale update rule converge to a local nash\nequilibrium,” 2018.\n[22] Chen Chen, Gang Zhou, and Youfang Lin,\n“Cross-\ndomain wifi sensing with channel state information: A\nsurvey,” ACM Computing Surveys, vol. 55, no. 11, pp.\n1–37, 2023.\n"
}