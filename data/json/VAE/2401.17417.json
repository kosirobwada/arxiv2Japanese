{
    "optim": "THROUGH-WALL IMAGING BASED ON WIFI CHANNEL STATE INFORMATION Julian Strohmayer, Rafael Sterzinger, Christian Stippel, Martin Kampel Computer Vision Lab, TU Wien Favoritenstr. 9/193-1, 1040 Vienna, Austria ABSTRACT This work presents a seminal approach for synthesizing im- ages from WiFi Channel State Information (CSI) in through- wall scenarios. Leveraging the strengths of WiFi, such as cost-effectiveness, illumination invariance, and wall- penetrating capabilities, our approach enables visual mon- itoring of indoor environments beyond room boundaries and without the need for cameras. More generally, it improves the interpretability of WiFi CSI by unlocking the option to perform image-based downstream tasks, e.g., visual activity recognition. In order to achieve this crossmodal translation from WiFi CSI to images, we rely on a multimodal Varia- tional Autoencoder (VAE) adapted to our problem specifics. We extensively evaluate our proposed methodology through an ablation study on architecture configuration and a quan- titative/qualitative assessment of reconstructed images. Our results demonstrate the viability of our method and highlight its potential for practical applications. Index Terms— WiFi, Channel State Information, Trough- Wall Imaging, Image Synthesis, Multimodality 1. INTRODUCTION While Human Behavior Modeling (HBM) has traditionally focused on optical modalities such as RGB, IR, depth, or thermal imaging to meet the requirements of person-centric vision applications [1], there is a steady rise in the popular- ity of WiFi as a sensing modality in this domain [2]. Its ap- peal lies in its capacity for discreet monitoring of vast indoor environments and its practical advantages, such as illumina- tion invariance [3], cost-effectiveness [4], and the protection of visual privacy – a crucial requirement in privacy-sensitive applications [5]. Despite these advantages, interpreting WiFi signals poses a significant challenge: Deducting human ac- tivities or behavior solely based on WiFi Channel State In- formation (CSI) is inherently difficult, if not impossible (see Figure 1c, which showcases the CSI amplitude spectrogram of a person walking in a room). To address this problem, our work aims to bridge the gap between WiFi CSI and images. We propose a new approach that enables visual monitoring in through-wall scenarios and unlocks the potential for image-based downstream tasks (a) image ground truth (b) image reconstruction (c) CSI amplitude spectrogram Fig. 1: Example of an image reconstructed from through- wall WiFi CSI. (a) Shows the ground truth image captured by a camera within the room and (b) the corresponding im- age reconstruction from (c) the CSI amplitude spectrogram recorded by a WiFi antenna located outside of the room. based on WiFi CSI, such as the visual annotation of activities. On top of this, our method also preserves visual privacy as identities are hallucinated based on training data. Inspired by brain decoding techniques that utilize brain waves for im- age synthesis [6], we apply a similar concept to the domain of WiFi CSI. Our approach represents, to our knowledge, the first method for synthesizing images directly from WiFi CSI captured in a through-wall scenario. Leveraging the MoPoE-VAE [7], a Variational Autoencoder (VAE) designed for multimodal data, our approach brings the theoretical ad- vantages of WiFi-based person-centric sensing to practical applications. The capability of our method to facilitate visual monitor- ing in through-wall scenarios without conventional cameras is illustrated in Figure 1. In this example, the image in Fig- ure 1b is reconstructed solely from the WiFi CSI amplitude spectrogram shown in Figure 1c. Its close resemblance to the ground truth (Figure 1a) demonstrates the effectiveness of our arXiv:2401.17417v1  [cs.CV]  30 Jan 2024 approach and its potential for enhancing the interpretability of WiFi CSI by unlocking image-based downstream tasks. To stimulate further research in this emerging field, data, models, and code are made publicly available1. 2. RELATED WORK Advancements in deep learning and sensing technologies have enabled innovative approaches to extract person-centric information from modalities such as WiFi and radar in through-wall scenarios. An initial successful attempt that uses WiFi signals in a through-wall setting to estimate human poses is presented by Zhao et al. [3], starting a whole line of research that focuses on RF-based HBM. Subsequently, Li et al. [8] were able to generate binary segmentation masks from WiFi CSI, and Hernandez and Bulut [9] propose an adversarial approach for occupancy monitoring using WiFi CSI. In continuation of their work, Geng et al. [10] present an advancement in WiFi-based pose estimation, focusing on a multi-person setup, which is, however, limited to intra-room recording. Another work, by Wu et al. [11], demonstrates the generation of binary segmentation masks directly from radar signals, adding a new dimension to radar-based synthesis and enabling through-wall object segmentation using radar signals. Recently, image synthesis from WiFi signals has experi- enced significant developments. Notably, the work by Yu et al. [12] demonstrates an innovative two-step process in this domain: First, human poses are estimated from WiFi CSI and subsequently used to synthesize images, employing a condi- tional generative adversarial network. Another approach, al- though limited to intra-room scenarios, is introduced by Yu et al. [13]. Here, they present a direct method for synthesiz- ing images from radar signals, marking the first attempt in this specific field. It is important to note that neither of these approaches was applied in through-wall scenarios, leaving a gap that is closed in this work: direct WiFi CSI-based image synthesis in through-wall scenarios. Benchetrit et al. [6] present an approach for synthesizing images from electroencephalography (EEG) measurements extracted from a human brain. In their approach, they map images and EEG spectrograms into a shared latent space and hallucinate images using a latent diffusion model. VAEs have emerged as a powerful tool in multimodal unsupervised learning, which has experienced drastic improvements over the years. For instance, Wu et al. [14] enhance the efficacy of VAEs by assuming the multimodal joint posterior is a product of unimodal posteriors. In this way, information can be aggregated from any subset of unimodal posteriors and can, therefore, be used efficiently when dealing with missing modalities. However, their technique no longer guarantees a valid lower bound on the joint log-likelihood. Expanding on 1Supplementary Material, Zenodo (available from 27-10-2024) RX  5.3m   3.8m  table table shelf         table TX 0.25m  C recording area Fig. 2: Overview of the experimental setup, showing the ar- rangement of WiFi transmitter (TX), WiFi receiver (RX), and RGB camera (C) in the office recording environment. this, Shi et al. [15] assume that the joint posterior is a mixture of unimodal posteriors, facilitating translation between pairs of modalities. However, it lacks the capability to leverage the presence of multiple modalities, as it solely considers unimodal posteriors during training. Finally, Sutter et al. [7] overcome the limitations of the previous two methodologies, offering efficient many-to-many translation while preserving a valid lower bound on the joint log-likelihood. Our work is grounded in these foundational studies, leveraging the ad- vancements in VAEs for multimodal unsupervised learning to bridge WiFi CSI signals with images. 3. DATA Our experimental setup, illustrated in Figure 2, is designed to jointly capture through-wall WiFi CSI and images of activi- ties within a target room (3.8m×5.3m office). In this setup, a point-to-point WiFi transmitter-receiver arrangement is po- sitioned outside the room, accompanied by an intra-room camera. The WiFi transmitter and receiver systems utilize the Espressif ESP32-S3-DevKitC-1U2 development board, equipped with an ALFA Network APA-M253 directional panel antenna featuring a 66◦ horizontal beam width and a gain of 8dBi @2.4GHz. The connection between transmitter and re- ceiver is established using the Espressif ESP-NOW4 protocol, employing a fixed packet-sending rate of 100Hz. Images are captured at a resolution of 640×480 pixels and a frame rate of 30Hz using an ESP32-S3-based camera board5. To ensure temporal synchronization, both the WiFi receiver and cam- era are connected to a notebook that concurrently captures WiFi packets and images through serial over USB and WiFi connections, respectively. 2ESP32-S3-DevKitC-1U, https://docs.espressif.com/, (acc. 21-01-2024) 3APA-M25, https://alfa-network.eu/apa-m25, (acc. 21-01-2024) 4ESP-NOW, https://docs.espressif.com (acc. 21-01-2024) 5ESP32-S3-CAM, https://github.com/ (acc. 21-01-2024) For training and evaluation purposes, we create a dataset of temporally synchronized CSI and image pairs by recording a ten-minute sequence of continuous walking activities in the test environment, utilizing the described hardware setup. The raw time series resulting from this comprises 57,413 WiFi packets and 18,261 images, i.e., around three WiFi packets received per image. Using a visual cue, we eliminate exces- sive samples at the start and end of the sequence that do not depict the target activity as part of cleaning the raw time se- ries. Additionally, to account for the sampling rate difference, each WiFi packet is paired with the image having the closest timestamp. Finally, the dataset is partitioned into training, validation, and test subsets using an 8:1:1 split ratio. Figure 3 provides an example of a CSI amplitude spectrogram show- ing the amplitudes of 52 Legacy Long Training Field (L-LTF) subcarriers over a ∼1.5-second time interval (150 WiFi pack- ets) alongside the image corresponding to the central packet. Our data is made publicly available to ease further research1. 4. ARCHITECTURE In order to infer images from WiFi CSI, we employ MoPoE- VAE [7], a multimodal VAE to learn a posterior distribution over a joint latent variable z ∈ RD given our two modalities, as well as a corresponding decoder to reconstruct images from a sampled latent vector. Given a set of X := \b Xi\tN i=1 samples of images at hand, each paired with a fixed time interval of WiFi packets (the central packet determines the corresponding image), i.e., Xi = \b Xi I, Xi W \t , we aim to maximize the log-likelihood of our data at hand: log pθ(X) = log pθ \u0000\b Xi\tN i=1 \u0001 Within VAEs, this is achieved by maximizing a lower bound to this objective, the so-called evidence lower bound. For a given sample Xi, this lower bound on the marginal log- likelihood takes on the following form: L(θ, ϕ; Xi) := Eqϕ(z|Xi) \u0002 log \u0000pθ(Xi|z) \u0001\u0003 −βDKL \u0000qϕ(z|Xi)||pθ(z) \u0001 , where DKL denotes the Kullback-Leibler divergence [16] between the approximated posterior and the assumed, in our case, Gaussian prior, and β being an additional weight pa- rameter (see Higgins et al. [17]), which promotes disentan- glement of the latent variable z, in the case that β > 1. During inference, when image data is missing, i.e. Xi W := Xi\\ \b Xi I \t , we still aim to obtain a valid lower bound on the joint proba- bility log pθ(Xi). However, when using L(θ, ϕ; Xi W ), it only yields a lower bound on log pθ(Xi W ). Hence, to obtain a correct lower bound on the joint probability, the following (a) CSI amplitude spectrogram (b) image Fig. 3: Example of a CSI amplitude spectrogram showing the amplitudes of 52 L-LTF subcarriers over a ∼1.5-second time interval and the image corresponding to the central packet. adapted lower bound is used: LW (θ, ϕW ; Xi) := E˜qϕW (z|Xi W ) \u0002 log \u0000pθ(Xi|z) \u0001\u0003 −βDKL \u0000˜qϕW (z|Xi W )||pθ(z) \u0001 . In the general naive case with M different modalities, ap- proximating a lower bound of the joint probability in any case of missing modalities requires the optimization of 2M differ- ent models, one for each subset contained within the power- set, posing a drastic scalability issue. Compared to prior lit- erature, Sutter et al. [7] circumvent this by modeling the joint posterior approximation as a so-called Mixture of Products of Experts (MoPoE), combining Product of Experts (PoE) [14] and Mixture of Experts (MoE) [15] through abstract mean functions [18], to enable efficient retrieval of the joint poste- rior for all subsets. For more information, we refer the reader to consult [7]. While our task at hand considers only two modalities (WiFi CSI, images) and we focus solely on the reconstruction of images from WiFi CSI, we still opt for the same methodol- ogy to allow for additional modalities in the future. However, we employ custom VAEs designed for inference from WiFi CSI and adapt the loss to neglect the possibility of decoding a sequence of images, as it is not required. 4.1. Image Reconstruction Given the stark difference of our modalities, we require two architectural different models for learning the unimodal pos- terior distributions qϕI(z|Xi I) and qϕW (z|Xi W ): Image VAE. For the encoding and decoding of images, we employ convolutional and transpose-convolutional layers, respectively. Additionally, we follow each of these layers with batch normalization and a Leaky ReLU activation. Be- fore inferring the distribution parameters of the latent variable z given images using the encoder, we rescale the input from a resolution of 640 × 480 down to 128 × 128 pixels in or- der to reduce computational complexity and apply normaliza- tion using per-channels means and standard deviations. Next, six consecutive convolutional blocks increase the channel size from three (RGB) to 512, followed by a simple Multi-Layer Perceptron (MLP) to map from the flattened output of the con- volutions to the unimodal distribution parameters. Decoding Image Decoder Image Encoder 48 Conv. > BN > LReLU TConv. > BN > LReLU Aggregation Conv. > Tanh Temporal Encoder CSI Timestamps RGB Image CSI Amplitudes Image Timestamps CSI Encoder 96 128 192 512 256 48 96 512 256 192 128 3 MLP MLP MLP PoE MoE Z Reconstruction  Inference  Fig. 4: Proposed MoPoE-VAE architecture for WiFi CSI-based image synthesis. is performed, taking a latent vector z and reversing the pro- cess using transpose-convolutions. CSI VAE. Concerning our CSI VAE, we first extract in- formation about the signal’s amplitude from the raw WiFi CSI and apply an MLP to embed the input per sample of a given sequence to obtain a richer feature representation. Next, we explore the use of different/no aggregation options along the time dimension before estimating the distribution param- eters: uniform feature weighing, Gaussian feature weighing, and concatenation, i.e., flattening the output and leaving it unmodified. We describe these options in detail below; their effectiveness is evaluated within an ablation study (see Sec- tion 5). Lastly, we take the aggregated/concatenated output and employ another MLP to obtain the unimodal distribution parameters of WiFi CSI. Finally, note that we neglect the re- construction of WiFi CSI from a latent vector z as it is not relevant to our task. During training, we combine the unimodal distribution parameters of our two VAEs, first, to subsets using PoE, and second, combine all subsets within the powerset using MoE. With predicted parameters µ, σ ∈ RD, we then sample a la- tent vector z, using the reparametrization trick; z = µ+σ⊙ϵ, with ϵ ∼ N(0, 1). At inference, we only consider the mode of the approximated posterior distribution, predicted by the CSI VAE, for reconstruction, i.e., the predicted mean. An overview of our proposed methodology for synthesiz- ing images from through-wall WiFi CSI is provided in Fig- ure 4, illustrating both training and inference. 4.2. Aggregation Options As noted previously, regarding the CSI VAE, we consider dif- ferent aggregations to find the most meaningful representa- tion for predicting an image (determined by the central WiFi packet) from a given sequence. Let ˜XW ∈ RL×H be the embedded WiFi CSI ampli- tudes after applying the first encoder MLP, where L is the sequence length, and H is the hidden dimension. First, as a baseline, we consider a uniform weighing of our features, i.e., giving each packet equal weight and thus neglecting the order of arrival and the importance of the central packet. Next, to place more importance on the central packet and reduce per- mutation invariance, we examine a Gaussian weighing with µ = σ = L/2. Both of these options are prone to flickering or, in other words, drastic changes between two consecutive frames. Finally, to overcome this issue, we consider concate- nation, leaving the features as is to be fully permutation sen- sitive. Hence, depending on the employed options, the input to the second encoder MLP lies either in RLH or RH. 4.3. Temporal Encoding A final adjustment to our architecture is posed by temporal encoding. Although concatenation alleviates the issue of dras- tic changes between two consecutive frames, a slight jittering artifact remains. By adding time information using sinusoidal functions, we are able to improve upon this issue, following a similar strategy as employed by NeRF [19]. The encoding for a timestamp t and a set of frequencies F is defined as: T(t, F) = h sin \u001020πt 3L \u0011 , cos \u001020πt 3L \u0011 , . . . , sin \u00102F −1πt 3L \u0011 , cos \u00102F −1πt 3L \u0011i , where t is scaled by three times the window size L to have contextual time information from before and after the current window. Specifically, when incorporating temporal encod- ing, we concatenate the encoding with the image/WiFi fea- tures stemming from the CNN/MLP. This concatenated set of features is then fed through the respective MLP to map to the corresponding distribution parameters (see Figure 4). (a) image ground truth (b) UW (c) GW (d) C (e) C+T Fig. 5: Comparison of reconstruction fidelity between MoPoE-VAE models employing the aggregation options (b) uniform weighing (UW), (c) Gaussian weighing (GW), (d) concatenation (C), and (e) concatenation with temporal encoding (C+T). This visual comparison highlights the improvements in image clarity and reduction of artifacts. Table 1: Quantitative ablation study results for the aggre- gation options uniform weighing (UW), Gaussian weighing (GW), concatenation (C), and concatenation with temporal encoding (C+T). The metrics reported represent the mean and standard deviation across ten independent training runs. Model PSNR↑ SSIM↑ RMSE↓ FID↓ UW 20.03±.05 0.734±.00 8.02±.04 142.95±2.4 GW 20.02±.07 0.734±.00 8.00±.05 141.00±5.0 C 20.39±.05 0.748±.00 7.83±.05 127.33±4.7 C+T 20.39±.04 0.749±.00 7.80±.02 125.62±3.2 5. EVALUATION To assess the correlation between reconstruction fidelity and our architectural optimizations, an ablation study is con- ducted. In this study, we quantitatively and qualitatively report the reconstruction performance of different model variants, assessing perceived image quality and visual arti- facts. 5.1. Model Training Utilizing the captured dataset, we train models based on the proposed MoPoE-VAE architecture for WiFi-CSI-based im- age synthesis, considering the following variants: uniform weighing (UW), Gaussian weighing (GW), and concatena- tion (C), as well as the combination of concatenation and temporal encoding (C+T). On top of this, a hyperparameter search is conducted, employing C+T on the validation subset, to determine suitable values for training hyperparameters. We evaluate over batch size b ∈ {16, 32, 64, 128, 256, 512}, win- dow size L ∈ {51, 101, 151, 201, 251, 301}, and the DKL weighing parameter β ∈ {1, 2, 4, 6}. This resulted in optimal values of b = 32, L = 151, and β = 1, which we subse- quently use for training all models. Concerning the recorded WiFi sequence, for each packet, we extract the 52 L-LTF sub- carriers from the complex CSI matrix and precompute am- plitudes which are used to sample 52 × L spectrograms for the CSI VAE to process. Furthermore, the raw images are rescaled to a resolution of 128×128 pixels and normalized using the per-channel means and standard deviations of the dataset. For each configuration, utilizing the Adam optimizer (lr = 1e−3), we conduct ten independent training runs span- ning 50 epochs and select the model with the lowest validation loss for the final evaluation on the test subset. Metrics. To assess the image reconstruction fidelity of our models, we employ metrics, well-established in compres- sion/generative model research, such as the Peak Signal-to- Noise Ratio (PSNR), the Structural Similarity Index Measure (SSIM) [20], the Root Mean Squared Error (RMSE), and the Fr´echet Inception Distance (FID) [21]. 5.2. Quantitative Results Table 1 summarizes the quantitative results of our ablation study, providing a comparison between the MoPoE-VAE ag- gregation options UW, GW, C, and C+T based on the PSNR, SSIM, RMSE, and FID metric, averaged over ten indepen- dent training runs. Among these models, UW and GW are the least effective, yielding comparatively low PSNR, SSIM, and high RMSE and FID scores. We improve upon this by intro- ducing concatenation (C), which turns out to have a signifi- cant impact, evidenced by a higher PSNR and SSIM, along with lower RMSE and FID scores. This stark contrast in quantitative performance is also reflected visually, leading to notable enhancements in sharpness and perceptual fidelity. However, C+T (Figure 5e) outperforms all previous aggrega- tion options, exhibiting slightly higher PSNR and SSIM, cou- pled with lower RMSE and FID scores, compared to C (Fig- ure 5d). Additionally, we point out the observed improvement in FID scores for C+T, suggesting reduced perceptual dis- tance and qualitative enhancements. Despite the only subtle impact of temporal encoding when combined with concate- nation, reconstructed images are slightly sharper, and, more importantly, frame transitions are more coherent. In sum- mary, while C+T demonstrates superior quantitative perfor- mance, its marginal differences highlight the importance of considering both quantitative metrics and visual assessments. 5.3. Qualitative Results Our qualitative examination aligns with the trends observed in the quantitative analysis; Figure 5 illustrates the impact of dif- ferent aggregation options on image reconstruction fidelity. In Figure 5b, the uniform weighing (UW) model yields a blurry reconstruction, consistent with its lower PSNR and higher RMSE scores (see Table 1). We hypothesize that the lack of focus in the temporal domain results in diminished per- ceptual quality. Moving to Figure 5c, the Gaussian weight- ing (GW) model showcases a slight improvement in quality despite the similarity in quantitative metrics. This improve- ment is attributed to the prioritization of central WiFi packets during weighing; that is, packets that are in closer temporal proximity to the ground truth image have a higher contribu- tion, resulting in a clearer and more accurate reconstruction. Next, Figure 5d illustrates the concatenation (C) model: By learning the aggregation of WiFi features itself, not only fur- ther improves the sharpness of images, but even enhances the quality on a frame-to-frame level, eliminating spatiotemporal discontinuities, as illustrated in Figure 6. Quantitatively, C outperforms UW and GW, yet subtle artifacts and jittering re- main visible in videos compared to C+T, possibly contributed by marginal differences in FID scores. Finally, the peak of our architectural improvements is reached when employing the C+T model, which is showcased in Figure 5e, incorporat- ing both concatenation and temporal encoding. Employing cyclic temporal encoding effectively mitigates the previously mentioned jittering artifact, resulting in smoother frame tran- sitions and reduced FID scores. For a perspective on video quality, reconstructed videos complement these static images, showcasing for each aggregation option its ability to recon- struct images based on trough-wall WiFi CSI1. In conclusion, these qualitative results demonstrate the ca- pability of our method to facilitate through-wall visual mon- itoring without conventional cameras. The successful recon- struction of images from WiFi CSI not only validates our ap- proach but also suggests its potential for image-based down- stream tasks. This represents a significant step toward en- hancing the interpretability of WiFi CSI, providing a founda- tion for further exploration and research in this evolving field. 6. FUTURE WORK While the MoPoE-VAE+C+T model achieves promising re- sults on our dataset, effectively demonstrating the feasibility of image synthesis from through-wall WiFi CSI, we do not expect it to generalize to new people or environments with- out further adaptations. Generalization is, in general, an open problem in WiFi-based person-centric sensing, and achieving it across diverse scenarios in practice will require additional steps, posing a promising opportunity for future research [22]. In addition to improving generalization, another promis- ing direction for future work involves expanding upon the modalities used in our approach. Employing the MoPoE-VAE i0 i25 i50 i75 i100 images 1-SSIM discontinuity Gaussian weighing concatenation Fig. 6: Example showing the elimination of spatiotemporal discontinuities in a sequence of 100 test images with high 1-SSIM, highlighted in red, through the aggregation option concatenation. From top to bottom, the rows show Gaussian weighing (GW), concatenation (C), and ground truth, respec- tively. framework, our model is inherently adaptable to multimodal data, leaving the flexibility for incorporating additional sen- sory data. For instance, combining WiFi CSI with radar, au- dio, or structural vibrations could provide complementary in- formation and aid in scenarios where one modality alone may be insufficient for accurate reconstructions, e.g., in settings where the WiFi signal is disturbed or ambiguous due to envi- ronmental effects. 7. CONCLUSION In this work, we have demonstrated the direct synthesis of person-centric images from WiFi CSI captured in a through- wall scenario. We collected a dataset of WiFi CSI and cor- responding image time series, on the basis of which an ab- lation study was conducted to assess the fidelity of recon- structed images stemming from different architectural varia- tions. From this evaluation, the model pairing the aggregation option concatenation with temporal encoding emerged as su- perior. Our results demonstrate the viability of the proposed approach, suggesting its potential for novel applications, such as through-wall visual monitoring without conventional cam- eras, and improving the interpretability of WiFi CSI by un- locking image-based downstream tasks, such as visual activ- ity recognition. 8. ACKNOWLEDGMENTS This work is partially funded by the Vienna Business Agency (Blindsight-grant 4829418). 9. REFERENCES [1] Julian Strohmayer and Martin Kampel, “A compact tri- modal camera unit for rgbdt vision,” in 2022 the 5th International Conference on Machine Vision and Appli- cations (ICMVA), 2022, pp. 34–42. [2] Biying Fu, Naser Damer, Florian Kirchbuchner, and Ar- jan Kuijper, “Sensing technology for human activity recognition: A comprehensive survey,” IEEE Access, vol. PP, pp. 1–1, 01 2020. [3] Mingmin Zhao, Tianhong Li, Mohammad Abu Al- sheikh, Yonglong Tian, Hang Zhao, Antonio Torralba, and Dina Katabi, “Through-wall human pose estima- tion using radio signals,” in 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018, pp. 7356–7365. [4] Robert Schumann, Fr´ed´eric Li, and Marcin Grzegorzek, “Wifi sensing with single-antenna devices for ambient assisted living,” 10 2023, pp. 1–8. [5] Katrin Arning and Martina Ziefle, ““get that camera out of my house!” conjoint measurement of preferences for video-based healthcare monitoring systems in pri- vate and public places,” in Inclusive Smart Cities and e-Health, Antoine Geissb¨uhler, Jacques Demongeot, Mounir Mokhtari, Bessam Abdulrazak, and Hamdi Aloulou, Eds., Cham, 2015, pp. 152–164, Springer In- ternational Publishing. [6] Yohann Benchetrit, Hubert Banville, and Jean-R´emi King, “Brain decoding: toward real-time reconstruction of visual perception,” 2023. [7] Thomas M. Sutter, Imant Daunhawer, and Julia E. Vogt, “Generalized multimodal ELBO,” in International Con- ference on Learning Representations, 2020. [8] Chenning Li, Zheng Liu, Yuguang Yao, Zhichao Cao, Mi Zhang, and Yunhao Liu, “Wi-fi see it all: generative adversarial network-augmented versatile wi-fi imaging,” Proceedings of the 18th Conference on Embedded Net- worked Sensor Systems, 2020. [9] Steven M. Hernandez and Eyuphan Bulut, “Adversar- ial occupancy monitoring using one-sided through-wall wifi sensing,” in ICC 2021 - IEEE International Con- ference on Communications, 2021, pp. 1–6. [10] Jiaqi Geng, Dong Huang, and Fernando De la Torre, “DensePose from WiFi,” . [11] Zhi Wu, Dongheng Zhang, Chunyang Xie, Cong Yu, Jinbo Chen, Yang Hu, and Yan Chen, “Rfmask: A sim- ple baseline for human silhouette segmentation with ra- dio signals,” IEEE Transactions on Multimedia, vol. 25, pp. 4730–4741, 2022. [12] Cong Yu, Dongheng Zhang, Chunyang Xie, Zhi Lu, Yang Hu, Houqiang Li, Qibin Sun, and Yan Chen, “Wifi-based human pose image generation,” in 2022 IEEE 24th International Workshop on Multimedia Sig- nal Processing (MMSP), 2022, pp. 1–6. [13] Cong Yu, Zhi Wu, Dongheng Zhang, Zhi Lu, Yang Hu, and Yan Chen, “Rfgan: Rf-based human synthesis,” IEEE Transactions on Multimedia, 2022. [14] Mike Wu and Noah Goodman, “Multimodal generative models for scalable weakly-supervised learning,” in Ad- vances in Neural Information Processing Systems. 2018, vol. 31, Curran Associates, Inc. [15] Yuge Shi, Siddharth N, Brooks Paige, and Philip Torr, “Variational mixture-of-experts autoencoders for multi- modal deep generative models,” in Advances in Neural Information Processing Systems. 2019, vol. 32, Curran Associates, Inc. [16] Solomon Kullback and Richard A Leibler, “On infor- mation and sufficiency,” The annals of mathematical statistics, vol. 22, no. 1, pp. 79–86, 1951. [17] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mo- hamed, and Alexander Lerchner, “β-VAE: Learning Basic Visual Concepts with a Constrained Variational Framework,” 2017. [18] Frank Nielsen, “On the jensen–shannon symmetrization of distances relying on abstract means,” Entropy, vol. 21, no. 5, 2019. [19] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng, “Nerf: Representing scenes as neural radiance fields for view synthesis,” Communications of the ACM, vol. 65, no. 1, pp. 99–106, 2021. [20] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli, “Image quality assessment: from error vis- ibility to structural similarity,” IEEE transactions on image processing, vol. 13, no. 4, pp. 600–612, 2004. [21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter, “Gans trained by a two time-scale update rule converge to a local nash equilibrium,” 2018. [22] Chen Chen, Gang Zhou, and Youfang Lin, “Cross- domain wifi sensing with channel state information: A survey,” ACM Computing Surveys, vol. 55, no. 11, pp. 1–37, 2023. "
}