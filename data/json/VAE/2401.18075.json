{
    "optim": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting Jiezhi “Stephen” Yang1 Khushi Desai2 Charles Packer3 Harshil Bhatia4 Nicholas Rhinehart3 Rowan McAllister5 Joseph Gonzalez3 1Harvard University 2Columbia University 3UC Berkeley 4Avataar.ai 5Toyota Research Institute Abstract We propose CARFF: Conditional Auto-encoded Radi- ance Field for 3D Scene Forecasting, a method for pre- dicting future 3D scenes given past observations, such as 2D ego-centric images. Our method maps an image to a distribution over plausible 3D latent scene configura- tions using a probabilistic encoder, and predicts the evo- lution of the hypothesized scenes through time. Our la- tent scene representation conditions a global Neural Radi- ance Field (NeRF) to represent a 3D scene model, which enables explainable predictions and straightforward down- stream applications. This approach extends beyond pre- vious neural rendering work by considering complex sce- narios of uncertainty in environmental states and dynamics. We employ a two-stage training of Pose-Conditional-VAE and NeRF to learn 3D representations. Additionally, we auto-regressively predict latent scene representations as a partially observable Markov decision process, utilizing a mixture density network. We demonstrate the utility of our method in realistic scenarios using the CARLA driving sim- ulator, where CARFF can be used to enable efficient tra- jectory and contingency planning in complex multi-agent autonomous driving scenarios involving visual occlusions. Our website containing video demonstrations and code is available at: www.carff.website. 1. Introduction Humans often imagine what they cannot see given partial visual context. Consider a scenario where reasoning about the unobserved is critical to safe decision-making: for ex- ample, a driver navigating a blind intersection. An expert driver will plan according to what they believe may or may not exist in occluded regions of their vision. The driver’s belief – defined as the understanding of the world mod- eled with consideration for inherent uncertainties of real- world environments – is informed by their partial observa- tions (i.e., the presence of other vehicles on the road), as well as their prior knowledge (e.g., past experience nav- igating this intersection). When reasoning about the un- observed, humans are capable of holding complex beliefs CARFF  Encoder 3D  Planning 3D State Beliefs 3D Planning Results Input Image STOP CARFF 3D Driving Planning Application Figure 1. CARFF 3D planning application for driving. An in- put image containing a partially observable view of an intersec- tion is processed by CARFF’s encoder to establish 3D environ- ment state beliefs, i.e. the predicted possible state of the world: whether or not there could be another vehicle approaching the in- tersection. These beliefs are used to forecast the future in 3D for planning, generating one among two possible actions for the vehi- cle to merge into the other lane. over not just the existence and position of individual objects (e.g., whether or not there is an oncoming car), but also the shapes, colors, and textures composing the entire occluded portion of the scene. Traditionally, autonomous systems with high- dimensional sensor observations such as video or LiDAR process the data into low-dimensional state information, such as the position and velocity of tracked objects, which are then used for downstream prediction and planning. This object-centric framework can be extended to reason about partially observed settings by modeling the existence and state of potentially dangerous unobserved objects in addition to tracked fully observed objects. Such systems often plan with respect to worst-case hypotheses, e.g., by placing a “ghost car” traveling at speed on the edge of the visible field of view [41]. Recent advances in neural rendering have seen tremen- dous success in learning 3D scene representations directly from posed multi-view images using deep neural networks. Neural Radiance Fields (NeRF) allow for synthesizing novel images in a 3D scene from arbitrary viewing angles, making seeing behind an occlusion as simple as rendering from an unoccluded camera angle. Because NeRF can gen- erate RGB images from novel views, it decouples the de- pendency of the scene representation on the object detec- 1 arXiv:2401.18075v1  [cs.CV]  31 Jan 2024 tion and tracking pipeline. For example, images rendered from a NeRF may contain visual information that would be lost by an object detector, but may still be relevant for safe decision-making. Additionally, because NeRF repre- sents explicit geometry via an implicit density map, it can be used directly for motion planning without requiring any rendering [1]. NeRF’s ability to represent both visual and geometric information makes them a more general and in- tuitive 3D representation for autonomous systems. Despite NeRF’s advantages, achieving probabilistic pre- dictions in 3D based on reasoning from the occluded is challenging. For example, discriminative models that yield categorical predictions are unable to capture the underly- ing 3D structure, impeding their ability to model uncer- tainty. While prior work on 3D representation captures view-invariant structures, their application is primarily con- fined to simple scenarios [17]. We present CARFF, which to our knowledge, is the first forecasting approach in sce- narios with partial observations that uniquely facilitates stochastic predictions within a 3D representation, effec- tively integrating visual perception and explicit geometry. CARFF addresses the aforementioned difficulties by proposing PC-VAE: Pose-Conditioned Variational Autoen- coder, a framework that trains a convolution and Vision Transformer (ViT) [9] based image encoder. The encoder maps a potentially partially observable ego-centric image to latent scene representations, which hold state beliefs with implicit probabilities. The latents later condition a neural radiance field that functions as a 3D decoder to recover 3D scenes from arbitrary viewpoints. This is trained after PC- VAE in our two-stage training pipeline (see Sec. 3.1). Ad- ditionally, we design a mixture density model to predict the evolution of 3D scenes over time stochastically in the en- coder belief space (see Sec. 3.2). A potential application of CARFF is illustrated in Fig. 1. Using the CARLA driving simulator, we demonstrate how CARFF can be used to en- able contingency planning in real-world driving scenarios that require reasoning into visual occlusions. 2. Related work 2.1. Dynamic Neural Radiance Fields Neural radiance fields: Neural Radiance Fields (NeRF) [2, 23, 39] for 3D representations have gar- nered significant attention due to their ability to generate high-resolution, photorealistic scenes. Instant Neural Graphics Primitive (Instant-NGP) [24] speeds up training and rendering time by introducing a multi-resolution hash encoding. Other works like Plenoxels [11] and DirectVoxGo (DVGO) [38] also provide similar speedups. Given the wide adoption and speedups of Instant-NGP, we use it to model 3D radiance fields in our work. NeRFs have also been extended for several tasks such as modeling large-scale unbounded scenes [2, 40, 46], scene from sparse views [7, 35, 45] and multiple scenes [17, 48]. Tewari et al. [42] presents an in-depth survey on neural representation learning and its applications. Generalizable novel view synthesis models such as pix- elNeRF and pixelSplat [5, 51] learn a scene prior to ren- der novel views conditioned on sparse existing views. Dy- namic NeRF, on the other hand, models scenes with moving objects or objects undergoing deformation. A widely used approach is to construct a canonical space and predict a de- formation field [19, 29, 30, 32]. The canonical space is usu- ally a static scene, and the model learns an implicitly repre- sented flow field [29, 32]. A recent line of work also models dynamic scenes via different representations and decompo- sition [3, 37]. These approaches tend to perform better for spatially bounded and predictable scenes with relatively small variations [3, 20, 29, 51]. Moreover, these methods only solve for changes in the environment but are limited in incorporating stochasticity in the environment. Multi-scene NeRF: Our approach builds on multi-scene NeRF approaches [17, 44, 48, 49] that learn a global latent scene representation, which conditions the NeRF, allowing a single NeRF to effectively represent various scenes. A similar method, NeRF-VAE, was introduced by Kosiorek et al. [17] to create a geometrically consistent 3D genera- tive model with generalization to out-of-distribution cam- eras. However, NeRF-VAE [17] is prone to mode collapse when evaluated on complex, real-world datasets. 2.2. Scene Forecasting Planning in 2D space: In general, planning in large and continuous state-action spaces is difficult due to the result- ing exponentially large search space [28]. Consequently, several approximation methods have been proposed for tractability [22, 31]. Various model-free [12, 27, 43] and model-based [4] reinforcement learning frameworks emerge as viable approaches, along with other learning- based methods [6, 25]. Several other approaches forecast for downstream control [15], learn behavior models for con- tingency planning [34], or predict potential existence and intentions of possibly unobserved agents [26]. While these methods are in 2D, we similarly reason under partial obser- vations, and account for these factors in 3D. NeRF in robotics: Several recent works have explored the application of NeRFs in robotics, like localization [50], navigation [1, 21], dynamics-modeling [10, 19] and robotic grasping [14, 16]. Adamkiewicz et al. [1] proposes a method for quadcopter motion planning in NeRF models via sampling the learned density function, which is a de- sirable characteristic of NeRF that can be leveraged for forecasting and planning purposes. Additionally, Driess 2 Ego- centric Lane View Predicted 3D View Figure 2. Novel view planning application. CARFF allows rea- soning behind occluded views from the ego car as simple as mov- ing the camera to see the sampled belief predictions, allowing sim- ple downstream planning using, for example, density probing or 2D segmentation models from arbitrary angles. et al. [10] utilize a graph neural network to learn a dy- namics model in a multi-object scene represented through NeRF. Li et al. [18] primarily perform pushing tasks in a scene with basic shapes, and approach grasping and plan- ning with NeRF and a separately learned latent dynamics model. Prior work either only performs well on simple and static scenes [1] or has a deterministic dynamics model [18]. CARFF focuses on complicated realistic environments in- volving both state uncertainty and dynamics uncertainty, which account for the potential existence of an object and unknown object movements respectively. 3. Method 3D scene representation has witnessed significant advance- ments in recent years, allowing for modeling environments in a contextually rich and interactive 3D space. This offers many analytical benefits, such as producing soft occupancy grids for spatial analysis and novel view synthesis for ob- ject detection. Given the advantages, our primary objective is to develop a model for probabilistic 3D scene forecast- ing in dynamic environments. However, direct integration of 3D scene representation via NeRF and probabilistic mod- els like VAE often involves non-convex and inter-dependent optimization, which causes unstable training. For instance, NeRF’s optimization may rely on the VAE’s latent space being structured to provide informative gradients. To navigate these complexities, our method bifurcates the training process into two stages (see Fig. 3). First, we train the PC-VAE to learn view-invariant scene repre- sentations. Next, we replace the decoder with a NeRF to learn a 3D scene from the latent representations. The la- tent scene representations capture the environmental states and dynamics over possible underlying scenes, while NeRF synthesizes novel views within the belief space, giving us the ability to see the unobserved (see Fig. 2 and Sec. 3.1). During prediction, uncertainties can be modeled by sam- pling latents auto-regressively from a predicted Gaussian mixture, allowing for effective decision-making. To this extent, we approach scene forecasting as a partially observ- able Markov decision process (POMDP) over latent distri- butions, which enables us to capture multi-modal beliefs for planning amidst perceptual uncertainty (see Sec. 3.2). 3.1. NeRF Pose-Conditional VAE (PC-VAE) Architecture: Given a scene St at timestamp t, we render an ego-centric observation image It c captured from camera pose c. The objective is to formulate a 3D representation of the image where we can perform a forecasting step that evolves the scene forward. To achieve this, we utilize a radi- ance field conditioned on latent variable z sampled from the posterior distribution qϕ(z|It c). Now, to learn the posterior, we utilize PC-VAE. We construct an encoder using convolu- tional layers and a pre-trained ViT on ImageNet [9]. The en- coder learns a mapping from the image space to a Gaussian distributed latent space qϕ(z|It c) = N(µ, σ2) parametrized by mean µ and variance σ2. The decoder, p(I|z, c), con- ditioned on camera pose c, maps the latent z ∼ N(µ, σ2) into the image space I. This helps the encoder to generate latents that are invariant to the camera pose c. To enable 3D scene modeling, we employ Instant- NGP [24], which incorporates a hash grid and an occu- pancy grid to enhance computation efficiency. Addition- ally, a smaller multilayer perceptron (MLP), Fθ(z) can be utilized to model the density and appearance, given by: Fθ(z) : (x, d, z) → ((r, g, b), σ) (1) Here, x ∈ R3 and d ∈ (θ, ϕ) represent the location vector and the viewing direction respectively. The MLP is also conditioned on the sampled scene latents z ∼ qϕ(z|It c) (see Appendix C). Training methodology: The architecture alone does not enable us to model complex real-world scenarios, as seen through a similar example in NeRF-VAE [17]. A crucial contribution of our work is our two-stage training frame- work which stabilizes the training. First, we optimize the convolutional ViT based encoder and pose-conditional con- volutional decoder in the pixel space for reconstruction. This enables our method to deal with more complex and re- alistic scenes as the encoding is learned in a semantically rich 2D space. By conditioning the decoder on camera poses, we achieve disentanglement between camera view angles and scene context, making the representation view- invariant and the encoder 3D-aware. Next, once rich latent representations are learned, we replace the decoder with a latent-conditioned NeRF over the latent space of the frozen encoder. The NeRF reconstructs encoder beliefs in 3D for novel view synthesis. Loss: Our PC-VAE is trained using standard VAE loss, with mean square error (MSE) and a Kullback–Leibler (KL) divergence given by evidence lower bound (ELBO): LPC-VAE = LMSE, PC-VAE + LKLD, PC-VAE = ||p(I|z, c′′) − It c′′∥2 + Eq(z|It c)[log p(I|z)] − wKLDKL(qϕ(z|It c) || p(I|z)) (2) 3 Encoder Decoder Gaussian latent  distribution Conditioning on  camera pose Decoded images for  camera pose Ground truths for  camera pose NeRF Stage 1: VAE Encoder and Decoder training Stage 2: Frozen encoder, NeRF Decoder Posed Images * Images are shown as batched for training only Figure 3. Visualizing CARFF’s two stage training process. Left: The convolutional VIT based encoder encodes each of the images I at timestamps t, t′ and camera poses c, c′ into Gaussian latent distributions. Assuming only two timestamps and an overparameterized latent, one of the Gaussian distributions will have a smaller σ2, and different µ across timestamps. Upper Right: The pose-conditional decoder stochastically decodes the sampled latent z using the camera pose c′′ into images It c′′ and It′ c′′. The decoded reconstruction and ground truth images are used to take the loss LMSE, PC-VAE. Lower Right: A NeRF is trained by conditioning on the latent variables sampled from the optimized Gaussian parameters. These parameters characterize the distinct timestamp distributions derived from the PC-VAE. An MSE loss is calculated for NeRF as LMSE, NeRF. where wKL denotes the KL divergence loss weight and z ∼ qϕ(z|It c). To make our representation 3D-aware, our posterior is encoded using camera c while the decoder is conditioned on a randomly sampled pose c′′. KL divergence regularizes the latent space to balance conditioned reconstruction and stochasticity under occlu- sion. An elevated KL divergence loss weight wKL pushes the latents closer to a standard normal distribution, N(0, 1), thereby ensuring probabilistic sampling in scenarios un- der partial observation. However, excessive regularization causes the latents to be less separable, leading to mode col- lapse. To mitigate this, we adopt delayed linear KL diver- gence loss weight scheduling to strike a balanced wKL. Next, we learn a NeRF-based decoder on the posterior of the VAE to model scenes. At any timestamp t we use a standard pixel-wise MSE loss for training the NeRF, given by the following equation: LMSE, NeRF = ∥It c − render(Fθ(·|qϕ(z|It c)))∥2 (3) We use a standard rendering algorithm as proposed by M¨uller et al. [24]. Next, we build a forecasting module over the learned latent space of our pose-conditional encoder. 3.2. Scene Forecasting Formulation: The current formulation allows us to model scenes with different configurations across timestamps. In order to forecast future configurations of a scene given an ego-centric view, we need to predict future latent distribu- tions. We formulate the forecasting as a partially observable Markov decision process (POMDP) over the posterior dis- tribution qϕ(z|It c) in the PC-VAE’s latent space. During inference, we observe stochastic behaviors un- der occlusion, which motivates us to learn a mixture of sev- eral Gaussian distributions that potentially denote different scene possibilities. Therefore, we model the POMDP us- ing a Mixture Density Network (MDN), with multi-headed MLPs, that predicts a mixture of K Gaussians. At any timestamp t the distribution is given as: q′ ϕ(zt|It−1 c ) = MDN(qϕ(zt−1|It−1 c )) (4) The model is conditioned on the posterior distribu- tion qϕ(zt−1) to learn a predicted posterior distribution q′ ϕ(zt|It−1 c ) at each timestamp. The predicted posterior dis- tribution is given by the mixture of Gaussian: q′ ϕ(zt) = K X i=1 πi N(µi, σ2 i ) (5) here, πi, µi, and σ2 i denote the mixture weight, mean, and variance of the ith Gaussian distribution within the pos- terior distribution. Here, K is the total number of Gaus- sians. For brevity we remove their conditioning on the pos- terior qϕ(zt−1) and sampled latent zt−1. We sample zt from the mixture of Gaussians q′ ϕ(zt), where zt most likely falls within one of the Gaussian modes. The scene configura- tion corresponding to the mode is reflected in the 3D scene rendered by NeRF. Loss: To optimize the MDN, we minimize a negative log- likelihood function, given by: LMDN = − N X i=1 log   K X j=1 πjN(yi; µj, σ2 j )   (6) 4 Scene 1: Ego car with  actor ambulance Scene 2: Ego car only Scene 1: Ego car with  slow- moving ambulance Scene 2: Ego car with  fast- moving ambulance Multi- Scene Approaching Intersection Multi- Scene Two Lane Merge Figure 4. Multi-scene CARLA datasets. Images illustrating the varying car configurations and scenes for the Multi-Scene Two Lane Merge dataset (left) and the Multi-Scene Approaching In- tersection dataset (right). Figure 5. Blender dataset. Simple Blender dataset with a station- ary blue cube, accompanied by a potential red cylinder exhibit- ing probabilistic temporal movement. The different camera poses demonstrate how movement needs to be modeled probabilistically based on possible occlusions from different camera angles. where yi ∼ qϕ(zt) is sampled from the distribution of latent zt, learned by the encoder, and N denotes the total number of samples. Inference: We consider an unseen ego-centric image and retrieve its posterior qϕ(zt) through the encoder. Next, we predict the possible future posterior distribution q′ ϕ(zt+1). From the predicted posterior, we sample a scene latent and perform localization. We achieve this via (a) density prob- ing the NeRF or (b) segmenting the rendered novel views using off-the-shelf methods such as YOLO [33] (see Fig. 2). These allow us to retrieve a corresponding Gaussian dis- tribution qϕ(zt+1) in encoder latent space. This is auto- regressively fed back into the MDN to predict the next timestamp. See Fig. 6 for an overview of the pipeline. 4. Results Decision-making under perceptual uncertainty is a perva- sive challenge faced in robotics and autonomous driving, especially in partially observable environments encountered in driving tasks. In these scenarios, accurate inference re- garding the presence of potentially obscured agents is piv- otal. We evaluate the effectiveness of CARFF on similar real-world situations with partial observability. We imple- mented several scenarios in the CARLA driving simula- tor [8] (see Fig. 4). A single NVIDIA RTX 3090 GPU is used to train PC-VAE, NeRF, and the MDN. All models, trained sequentially, tend to converge within a combined time frame of 24 hours. A detailed experimental setup can be found in Appendix C. We show that, given partially ob- servable 2D inputs, CARFF performs well in predicting la- tent distributions that represent complete 3D scenes. Using these predictions we design a CARFF-based controller for performing downstream planning tasks. 4.1. Data Generation We generate datasets containing an ego object and vary- ing actor objects in different configurations to test the ro- bustness of our method. We conduct experiments on (a) synthetic blender dataset for simple, controllable simula- tion and (b) CARLA-based driving datasets for complicated real-world scenarios [8]. Blender synthetic dataset: This comprises of a station- ary blue cube (ego) accompanied by a red cylinder (actor) that may or may not be present (see Fig. 5). If the ac- tor is present, it exhibits lateral movement as depicted in Fig. 5. This simplistic setting provides us with an inter- pretable framework to evaluate our model. CARLA dataset: Each dataset is simulated for N times- tamps and uses C = 100 predefined camera poses to cap- ture images of the environment under full observation, par- tial observation, and no visibility. These datasets are mod- eled after common driving scenarios involving state uncer- tainty that have been proposed in related works such as Ac- tive Visual Planning [25]. a) Single-Scene Approaching Intersection: The ego ve- hicle is positioned at a T-intersection with an actor vehicle traversing the crossing along an evenly spaced, predefined trajectory. We simulate this for N = 10 timestamps. We mainly use this dataset to predict the evolution of times- tamps under full observation. b) Multi-Scene Approaching Intersection: We extend the aforementioned scenario to a more complicated setting with state uncertainty, by making the existence of the actor vehi- cle probabilistic. A similar intersection crossing is simu- lated for N = 3 timestamps for both possibilities. The ego vehicle’s view of the actor may be occluded as it approaches the T-intersection over the N timestamps. The ego vehicle can either move forward or halt at the junction (see Fig. 4). c) Multi-Scene Multi-actor Two Lane Merge: To add more environment dynamics uncertainty, we consider a multi-actor setting at an intersection of two merging lanes. We simulate the scenario at an intersection with partial oc- clusions, with the second approaching actor having variable speed. Here the ego vehicle can either merge into the left 5 NeRF Gaussian Mixture Repeated Sampling Mixture Density  Network Probing For Autoregressive Prediction Pretrained Encoder Sampled Beliefs Figure 6. Auto-regressive inference in scene prediction. The input image at timestamp t, It c, is encoded using the pre-trained encoder from PC-VAE. The corresponding latent distribution is fed into the Mixture Density Network, which predicts a mixture of Gaussians. Each of the K Gaussians is a latent distribution that may correspond to different beliefs at the next timestamp. The mixture of Gaussians is sampled repeatedly for the predicted latent beliefs, visualized as It+1 c′,scni, representing potentially the ith possible outcome. This is used to condition the NeRF to generate 3D views of the scene. To accomplish autoregressive predictions, we probe the NeRF for the location of the car and feed this information back to the pre-trained encoder to predict the scene at the next timestamp. lane before the second actor or after all the actors pass, (see Fig. 4). Each branch is simulated for N = 3 timestamps. 4.2. CARFF Evaluation A desirable behavior from our model is that it should predict a complete set of possible scenes consistent with the given ego-centric image, which could be partially observable. This is crucial for autonomous driving in unpredictable en- vironments as it ensures strategic decision-making based on potential hazards. To achieve this we require a rich PC- VAE latent space, high-quality novel view synthesis, and auto-regressive probabilistic predictions of latents at future timestamps. We evaluate CARFF on a simple synthetic blender-based dataset and each CARLA-based dataset. Evaluation on blender dataset: In Fig. 5, for both Scene 1a and 1b, our model correctly forecasts the lateral move- ment of the cylinder to be in either position approximately 50% of the time, considering a left viewing angle. In Scene 2, with the absence of the red cylinder in the input cam- era angle, the model predicts the potential existence of the red cylinder approximately 50% of the time, and predicts lateral movements with roughly equal probability. This val- idates PC-VAE’s ability to predict and infer from the oc- cluded in the latent space, consistent with human intuitions. Similar intuitions, demonstrated within the simple scenes of the Blender dataset, can be transferred to driving scenarios simulated in our CARLA datasets. PC-VAE performance and ablations: We evaluate the performance of PC-VAE on CARLA datasets with multiple encoder architectures. We show that PC-VAE effectively re- constructs complex environments involving variable scenes, actor configurations, and environmental noise given poten- tially partially observable inputs (see Fig. 9). We calcu- lated an average Peak Signal-to-Noise Ratio (PSNR) over the training data, as well as novel view encoder inputs. To evaluate the quality of the latent space generated by the encoder, we utilize t-SNE [47] plots to visualize the distribution of latent samples for each image in a given dataset (see Appendix E). We introduce a Support Vector Machine (SVM) [13] based metric to measure the visual- ized clustering quantitatively, where a higher value indi- cates better clustering based on timestamps. Most latent scene samples are separable by timestamps, which indicates that the latents are view-invariant. Samples that are mis- classified or lie on the boundary usually represent partially or fully occluded regions. This is desirable for forecast- ing, as it enables us to model probabilistic behavior over these samples. In this process, balancing KL divergence weight scheduling maintains the quality of the PC-VAE’s latent space and reconstructions (see Appendix C). The re- sults presented in Tab. 2 substantiate the benefits of our PC- VAE encoder architecture compared to other formulations. Specifically, a non-conditional VAE fails in SVM accuracy as it only reconstructs images and does not capture the un- derlying 3D structures. Vanilla PC-VAE and PC-VAE with- out freezing weights require careful fine-tuning of several hyper-parameters and don’t generalize well to drastic cam- era movements. Our experiments show that our proposed model is capable of sustaining stochastic characteristics via latent representations in the presence of occlusion, while si- multaneously ensuring precise reconstructions. 3D novel view synthesis: Given an unseen ego-centric view with potentially partial observations, our method maintains all possible current state beliefs in 3D, and faith- 6 Pose     Inputs PC- VAE Decoded Images From Set of New Pose Figure 7. PC-VAE reconstructions. The encoder input, It c, among the other ground truth images Ic viewed from camera pose c at different timestamps, is reconstructed across a new set of poses c′′ respecting timestamp t, generating It c′′. A complete grid is in Appendix E. Ground Truth Prediction Pair Avg. PSNR (Scene 1) Avg. PSNR (Scene 2) Single-Scene Approaching Intersection Matching Pairs 29.06 N.A Un-matching Pairs 24.01 N.A Multi-Scene Approaching Intersection Matching Pairs 28.00 28.26 Un-matching Pairs 23.27 24.56 Multi-Scene Two Lane Merge Matching Pairs 28.14 28.17 Un-matching Pairs 22.74 23.32 Table 1. Averaged PSNR for fully observable 3D predictions. CARFF correctly predicts scene evolution across all timestamps for each dataset. The average PSNR is significantly higher for each prediction ˆ Iti and corresponding ground truth, Iti. PSNR values for incorrect correspondences, ˆ Iti, Itj, is a result of matching sur- roundings. The complete table of predictions is in Appendix E. fully reconstructs novel views from arbitrary camera angles for each belief. Fig. 2 illustrates one of the possible 3D be- liefs that CARFF holds. This demonstrates our method’s ca- pacity for generating 3D beliefs that could be used for novel view synthesis in a view-consistent manner. Our model’s ability to achieve accurate and complete 3D environmental understanding is important for applications like prediction- based planning. Inference under full and partial observations: Under full observation, we use MDN to predict the subsequent car positions in all three datasets. PSNR values are calculated based on bird-eye view NeRF renderings and ground truth bird-eye view images of the scene across different times- tamps. In Tab. 1 we report the PSNR values for rendered images over the predicted posterior with the ground truth images at each timestamp. We also evaluate the efficacy of our prediction model using the accuracy curve given in Fig. 8. This represents CARFF’s ability to generate stable beliefs, without producing incorrect predictions, based on actor(s) localization results. For each number of samples between n = 0 to n = 50, we choose a random subset of 3 fully observable ego images and take an average of the Encoder Architectures Train PSNR SVM Accuracy NV PSNR PC-VAE 26.30 75.20 25.24 PC-VAE w/o CL 26.24 70.60 24.80 Vanilla PC-VAE 26.02 25.70 24.65 PC-VAE w/o Freezing 24.57 5.80 24.60 PC-VAE w/ MobileNet 17.14 19.70 17.16 Vanilla VAE 24.15 10.60 11.43 Table 2. PC-VAE metrics and ablations. CARFF’s PC-VAE en- coder outperforms other encoder architectures in both image re- construction and pose-conditioning. We evaluated the following ablations: PC-VAE without Conv Layer, PC-VAE with a vanilla encoder, PC-VAE without freezing weights in ViT, PC-VAE re- placing ViT with pre-trained MobileNet, and non pose-conditional Vanilla VAE. The table displays the average training PSNR, novel view (NV) input PSNR, and SVM accuracy for latent timestamp prediction. accuracies. In scenarios with partial observable ego-centric images where several plausible scenarios exist, we utilize recall instead of accuracy using a similar setup. This lets us evaluate the encoder’s ability to avoid false negative predic- tions of potential danger. Fig. 8 shows that our model achieves high accuracy and recall in both datasets, demonstrating the ability to model state uncertainty (Approaching Intersection) and dy- namic uncertainty (Two Lane Merge). The results indicate CARFF’s resilience against randomness in resampling, and completeness in probabilistic modeling of the belief space. Given these observations, we now build a reliable controller to plan and navigate through complex scenarios. 4.3. Planning In all our experiments, the ego vehicle must make decisions to advance under certain observability. The scenarios are designed such that the ego views contain partial occlusion and the state of the actor(s) is uncertain in some scenar- ios. In order to facilitate decision-making using CARFF, we design a controller that takes ego-centric input images and outputs an action. Decisions are made incorporating sample consistency from the mixture density network. For instance, the controller infers occlusion and promotes the ego car to 7 Multi-Scene Approaching Intersection Controller Type Actor Exists No Actor Underconfident 30/30 0/30 Overconfident 0/30 30/30 CARFF (n = 2) 17/30 30/30 CARFF (n = 10) 30/30 30/30 CARFF (n = 35) 30/30 19/30 Multi-Scene Two Lane Merge Controller Type Fast Actor Slow Actor Underconfident 30/30 0/30 Overconfident 0/30 30/30 CARFF (n = 2) 21/30 30/30 CARFF (n = 10) 30/30 30/30 CARFF (n = 35) 30/30 22/30 Table 3. Planning in 3D with controllers with varying sam- pling numbers n. CARFF-based controllers outperform base- lines in success rate over 30 trials. For n = 10, the CARFF-based controller consistently chooses the optimal action in potential col- lision scenarios. For actor exists and fast-actor scenes, we con- sider occluded ego-centric inputs to test CARFF’s ability to avoid collisions. For no-actor and slow-actor scenes, we consider state observability and test the controllers’ ability to recognize the opti- mal action to advance. To maintain consistency, we use one single image input across 30 trials. pause when scenes alternate between actor presence and ab- sence in the samples. We use the two multi-scene datasets to assess the performance of the CARFF-based controller as they contain actors with potentially unknown behaviors. To design an effective controller, we need to find a bal- ance between accuracy and recall (see Fig. 8). A lowered accuracy from excessive sampling means unwanted ran- domness in the predicted state. However, taking insufficient samples would generate low recall i.e., not recovering all plausible states. This would lead to incorrect predictions as we would be unable to account for the plausible uncertainty present in the environment. To find a balance, we design an open-loop planning controller opting for a sampling strat- egy that involves generating n = 2, 10, 35 samples, where n is a hyperparameter to be tuned for peak performance. For sampling values that lie on the borders of the accu- racy and recall margin, for example, n = 2 and 35, we see that the CARFF-based controller obtains lower success rates, whereas n = 10 produces the best result. Across the two datasets in Tab. 3, the overconfident controller will inevitably experience collisions in case of a truck approach- ing, since it does not cautiously account for occlusions. On the other hand, an overly cautious approach results in stasis, inhibiting the controller’s ability to advance in the scene. This nuanced decision-making using CARFF-based controller is especially crucial in driving scenarios, as it en- hances safety and efficiency by adapting to complex and 0 10 20 30 40 50 0.4 0.6 0.8 1 Num Samples % Accuracy Recall (a) Approaching Intersection 0 10 20 30 40 50 0.4 0.6 0.8 1 Num Samples % Accuracy Recall (b) Two Lane Merge Figure 8. Multi-Scene dataset accuracy and recall curves from predicted beliefs. We test our framework across n = 1 and n = 50 samples from MDN’s predicted latent distributions from ego-centric image input. Across the number of samples n, we achieve an ideal margin of belief state coverage generated under partial observation (recall), and the proportion of correct beliefs sampled under full observation (accuracy). As we significantly in- crease the number of samples, the accuracy starts to decrease due to randomness in latent distribution resampling. unpredictable road environments, thereby fostering a more reliable and human-like response in autonomous vehicles. 5. Discussion Limitations: Like other NeRF-based methods, CARFF currently relies on posed images of specific scenes such as road intersections, limiting its direct applicability to unseen environments. However, we anticipate enhanced generaliz- ability with the increasing deployment of cameras around populated areas, such as traffic cameras at intersections. Additionally, handling very complex dynamics with an ex- tremely large number of actors still poses a challenge for our method, requiring careful fine-tuning to balance compre- hensive dynamics modeling against accuracy. Potentially stronger models in the near future may offer a promising avenue for further enhancements in this regard. Conclusion: We presented CARFF, a novel method for probabilistic 3D scene forecasting from partial observa- tions. By employing a Pose-Conditional VAE, a NeRF conditioned on the learned posterior, and a mixture density network that forecasts future scenes, we effectively model complex real-world environments with state and dynam- ics uncertainty in occluded regions critical for planning. We demonstrated the capabilities of our method in realistic autonomous driving scenarios, where, under full observa- tions, we can forecast into the future providing high-fidelity 3D reconstructions of the environment, while we maintain complete recall of potential hazards given incomplete scene information. Overall, CARFF offers an intuitive and unified approach to perceiving, forecasting, and acting under un- certainty that could prove invaluable for vision algorithms in unstructured environments. 8 References [1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, Rachel Gardner, Preston Culbertson, Jeannette Bohg, and Mac Schwager. Vision-only robot navigation in a neural ra- diance world. IEEE Robotics and Automation Letters, 7(2): 4606–4613, 2022. 2, 3, 12 [2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neu- ral radiance fields. In Int. Conf. Comput. Vis., pages 5855– 5864, 2021. 2 [3] Ang Cao and Justin Johnson. Hexplane: A fast representa- tion for dynamic scenes. In IEEE Conf. Comput. Vis. Pattern Recog., pages 130–141, 2023. 2 [4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu. Instance-aware predictive navigation in multi-agent environ- ments. In IEEE Int. Conf. on Robotics and Automation, pages 5096–5102. IEEE, 2021. 2 [5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction, 2023. 2 [6] Felipe Codevilla, Eder Santana, Antonio M L´opez, and Adrien Gaidon. Exploring the limitations of behavior cloning for autonomous driving. In Int. Conf. Comput. Vis., pages 9329–9338, 2019. 2 [7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra- manan. Depth-supervised nerf: Fewer views and faster train- ing for free. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12882–12891, 2022. 2 [8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA: An open urban driving simulator. In Conf. on Robol Learning, pages 1–16, 2017. 5, 11 [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In Int. Conf. Learn. Represent., 2021. 2, 3, 11 [10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake, and Marc Toussaint. Learning multi-object dynamics with compositional neural radiance fields. arXiv preprint arXiv:2202.11855, 2022. 2, 3 [11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5501–5510, 2022. 2 [12] Matthew Hausknecht and Peter Stone. Deep recurrent q- learning for partially observable mdps. In AAAI, 2015. 2 [13] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt, and Bernhard Scholkopf. Support vector machines. IEEE In- telligent Systems and their applications, 13(4):18–28, 1998. 6 [14] Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, and Ken Gold- berg. Dex-nerf: Using a neural radiance field to grasp trans- parent objects. arXiv preprint arXiv:2110.14217, 2021. 2 [15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien Gaidon, and Marco Pavone. Mats: An interpretable trajec- tory forecasting representation for planning and control. In Conf. on Robol Learning, 2021. 2 [16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal, Matthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and Ken Goldberg. Evo-nerf: Evolving nerf for sequential robot grasping of transparent objects. In Conf. on Robol Learning, 2022. 2 [17] Adam R Kosiorek, Heiko Strathmann, Daniel Zo- ran, Pol Moreno, Rosalia Schneider, Sona Mokr´a, and Danilo Jimenez Rezende. NeRF-VAE: A geometry aware 3d scene generative model. pages 5742–5752, 2021. 2, 3, 12 [18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal, and Antonio Torralba. 3d neural scene representations for visuomotor control. In Conf. on Robol Learning, pages 112– 123, 2022. 3, 12 [19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang, David Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Devrf: Fast deformable voxel ra- diance fields for dynamic scenes. In Adv. Neural Inform. Process. Syst., pages 36762–36775, 2022. 2 [20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by per- sistent dynamic view synthesis, 2023. 2 [21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and Christian Wolf. Multi-object navigation with dynamically learned neural implicit representations. In Int. Conf. Com- put. Vis., pages 11004–11015, 2023. 2 [22] Rowan McAllister and Carl Edward Rasmussen. Data- efficient reinforcement learning in continuous state-action gaussian-pomdps. In Adv. Neural Inform. Process. Syst., 2017. 2 [23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view syn- thesis. In Eur. Conf. Comput. Vis., 2020. 2 [24] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan- der Keller. Instant neural graphics primitives with a mul- tiresolution hash encoding. ACM Trans. Graph., 41(4):1–15, 2022. 2, 3, 4, 11 [25] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl- lister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learn- ing a planner contingent on perceptual uncertainty. In Conf. on Robol Learning, 2022. 2, 5, 11, 12 [26] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl- lister, Matthew A. Wright, Xin Wang, Jeff He, Sergey Levine, and Joseph E. Gonzalez. Is anyone there? learn- ing a planner contingent on perceptual uncertainty. In Conf. on Robol Learning, pages 1607–1617, 2023. 2 [27] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual to real reinforcement learning for autonomous driving. arXiv preprint arXiv:1704.03952, 2017. 2 [28] Christos H Papadimitriou and John N Tsitsiklis. The com- plexity of markov decision processes. Mathematics of oper- ations research, 12(3):441–450, 1987. 2 9 [29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Int. Conf. Comput. Vis., pages 5865–5874, 2021. 2 [30] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin- Brualla, and Steven M Seitz. Hypernerf: A higher- dimensional representation for topologically varying neural radiance fields. ACM Trans. Graph., 2021. 2 [31] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point- based value iteration: An anytime algorithm for pomdps. In IJCAI, pages 1025–1032, 2003. 2 [32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and Francesc Moreno-Noguer. D-NeRF: Neural Radiance Fields for Dynamic Scenes. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 2 [33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object de- tection. In IEEE Conf. Comput. Vis. Pattern Recog., 2016. 5 [34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A Wright, Rowan McAllister, Joseph E Gonzalez, and Sergey Levine. Contingencies from observations: Tractable con- tingency planning with learned behavior models. In IEEE Int. Conf. on Robotics and Automation, pages 13663–13669, 2021. 2 [35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall, Pratul P Srinivasan, and Matthias Nießner. Dense depth priors for neural radiance fields from sparse input views. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12892– 12901, 2022. 2 [36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San- jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge, 2015. 11 [37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu, Hongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural 4d decomposition for high-fidelity dynamic reconstruction and rendering. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16632–16642, 2023. 2 [38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5459–5469, 2022. 2 [39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra- mamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn high frequency functions in low dimen- sional domains. In Adv. Neural Inform. Process. Syst., pages 7537–7547, 2020. 2 [40] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad- han, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In IEEE Conf. Comput. Vis. Pattern Recog., pages 8248–8258, 2022. 2 [41] Omer Sahin Tas and Christoph Stiller. Limited visibility and uncertainty aware motion planning for automated driving. In IEEE Intelligent Vehicles Symposium (IV), 2018. 1 [42] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk, W. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S. Lombardi, T. Simon, C. Theobalt, M. Nießner, J. T. Barron, G. Wetzstein, M. Zollh¨ofer, and V. Golyanik. Advances in Neural Rendering. Comput. Graph. Forum, 2022. 2 [43] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde. End-to-end model-free reinforcement learning for urban driving using implicit affordances. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7153–7162, 2020. 2 [44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer, Aljaz Bozic, Christoph Lassner, and Christian Theobalt. Scenerflow: Time-consistent reconstruction of general dy- namic scenes. In International Conference on 3D Vision (3DV), 2023. 2 [45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt, and Federico Tombari. Sparf: Neural radiance fields from sparse and noisy poses. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4190–4200, 2023. 2 [46] Haithem Turki, Deva Ramanan, and Mahadev Satya- narayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In IEEE Conf. Comput. Vis. Pattern Recog., pages 12922–12931, 2022. 2 [47] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579–2605, 2008. 6 [48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P Srinivasan, Howard Zhou, Jonathan T Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr- net: Learning multi-view image-based rendering. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4690–4699, 2021. 2 [49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point- based neural radiance fields. In IEEE Conf. Comput. Vis. Pattern Recog., pages 5438–5448, 2022. 2 [50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto Rodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Invert- ing neural radiance fields for pose estimation. In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1323–1330. IEEE, 2021. 2 [51] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images, 2021. 2 10 A. CARLA Datasets A complete figure of the actor and ego configurations across scenes and the progression of timestamps for the Single- Scene Approaching Intersection, Multi-Scene Approaching Intersection, and the Multi-Scene Two Lane Merge is visu- alized in Fig. 14. B. Related Work Comparison In this section, we draw a comparison between CARFF and other methods that perform tasks similar to our model. However, these methods do not precisely align with the ob- jectives we aim to achieve. For instance, while some meth- ods integrate 3D reasoning, others may omit this aspect. To establish a fair comparison between our model and current methods, we have conducted an in-depth analysis of their qualitative differences, as delineated in Tab. 4. We primar- ily compare to other NeRF works based on their ability to model uncertainty (state and dynamics) and perform fore- casting in the environment. Our work surpasses all the listed previous works and is on par with 2D-based forecasting ap- proaches in functionality [25]. This comparison highlights that our model comprehensively encompasses the key qual- itative factors that should be present to reason from the oc- cluded as humans do. C. Implementation Details C.1. Pose-Conditional VAE Architecture: We implement PC-VAE on top of a stan- dard PyTorch VAE framework. The encoder with convo- lutional layers is replaced with a single convolutional layer and a Vision Transformer (ViT) Large 16 [9] pre-trained on ImageNet [36]. We modify fully connected layers to project ViT output of size 1000 to mean and variances with size of the latent dimension, 8. During training, the data loader re- turns the pose of the camera angle represented by an integer value. This value is one-hot encoded and concatenated to the re-parameterized encoder outputs, before being passed to the decoder. The decoder input size is increased to add the number of poses to accommodate the additional pose information. Optimization: We utilize a single RTX 3090 graphics card for all our experiments. The PC-VAE model takes ap- proximately 22 hours to converge using this GPU. During this phase, we tune various hyperparameters including the latent size, learning rate and KL divergence loss weight to establish optimal training tailored to our model (see Tab. 5). In order to optimize for the varied actor configurations and scenarios generated within the CARLA [8] simulator, we slightly adjust hyperparameters differently for each dataset. The learning rate (LR) and KL divergence (KLD) weight are adjusted to find an appropriate balance between the effective reconstruction of pose conditioning in the latent space, and the regularization of latents. Regularization pushes the latents toward Gaussian distributions and keeps the non-expressive latents in an over-parameterized latent space to be standard normal. This stabilizes the sampling process and ensures stochastic behavior of latent samples in case of occlusion. To achieve this balance, we use a lin- ear KLD weight scheduler, where the weight is initialized at a low value for KLD increment start epoch (see Tab. 5). This allows the model to initially focus on achieving highly accurate conditioned reconstructions. The KLD weight is then steadily increased until KLD increment end epoch is reached, ensuring probabilistic behavior under partial ob- servability. C.2. Mixture Density Network The mixture density network (MDN) takes in the mean and variances of the latent distributions qϕ(zt−1|It−1 c ) and out- puts the estimated posterior distribution as a mixture of Gaussian q′ ϕ(zt|It−1 c ) through a multi-headed MLP. Architecture: The shared backbone simply contains 2 fully connected layers and rectified linear units (ReLU) ac- tivation with hidden layer size of 512. Additional heads with 2 fully connected layers are used to generate µi and σ2 i . The mixture weight, πi, is generated from a 3 layer MLP network. We limit the number of Gaussians, K = 2. Optimization: We train our network for 30, 000 epochs using the batch size of 128 and an initial LR of 0.005, and apply LR decay to optimize training. This takes approxi- mately 30 minutes to train utilizing the GPU. During train- ing, the dataloader outputs the means and variances at the current timestamp and indexed view, and the means and variances for the next timestamp, at a randomly sampled neighboring view. This allows the MDN to learn how oc- cluded views advance into all the possible configurations from potentially unoccluded neighboring views, as a mix- ture of Gaussian. At each iteration, the negative log-likelihood loss is com- puted for 1000 samples drawn from the predicted mixture of distributions q′ ϕ(zt|It−1 c ) with respect to the ground truth distribution qϕ(zt|It c). While the MDN is training, addi- tional Gaussian noise, given by ϵ ∼ N(0, σ2), is added to the means and variances of the current timestamp t − 1, where σ ∈ [0.001, 0.01]. The Gaussian noise and LR decay help prevent overfitting and reduce model sensitivity to en- vironmental artifacts like moving trees, moving water, etc. C.3. NeRF Architecture: We implement our NeRF decoder uti- lizing an existing PyTorch implementation of Instant- NGP [24]. We concatenate the latents to the inputs of two parts of the Instant-NGP architecture: the volume density 11 Method 3D Realistic Application State Uncertainty Dynamics Uncertainty Prediction Planning Code Released CARFF ✓ ✓ ✓ ✓ ✓ ✓ ✓ NeRF-VAE [17] ✓ ✓ NeRF for Visuomotor Con- trol [18] ✓ ✓ ✓ ✓ NeRF Navigation [1] ✓ ✓ ✓ ✓ AVP [25] ✓ ✓ ✓ ✓ ✓ Table 4. Qualitative comparison of CARFF to related works. CARFF accomplishes all the highlighted objectives as opposed to the similar works listed. A check mark indicates that the associated method incorporates the qualitative feature in each column, whereas empty spaces indicate that the method does not account for it. Here, 3D refers to methods that reason in a 3D environment and perform novel view synthesis. Realistic application refers to whether the method has been demonstrated in realistic and complex scenarios. State and dynamic uncertainty refer to whether the model predicts probabilistically under these conditions. Prediction refers to forecasting into the future, while planning refers to using model predictions for decision-making. Pose     Inputs PC- VAE Decoded Images From Set of New Pose Figure 9. PC-VAE encoder inputs, ground truth timestamps, and reconstructions. The encoder input, It c, among the other ground truth images Ic viewed from camera pose c at different timestamps, is reconstructed across a new set of poses c′′ respecting timestamp t, generating It c′′. This is a full grid of the reconstructions. 12 Figure 10. NeRF graphical user interface. The GUI allows us to toggle and predict with an input image path. The probe and predict function probes the current location of the car and predicts the next. The screenshot is sharpened for visual clarity in the paper. PC-VAE Hyperparameters Latent Size 8 LR 0.004 KLD Weight Start 0.000001 KLD Weight End 0.00001 − 0.00004* KLD Increment Start 50 epochs KLD Increment End 80 epochs Table 5. PC-VAE experimental setup and hyperparameters. The main hyperparameters in PC-VAE training on the three datasets are latent size, LR, and KLD weight. For KLD schedul- ing, the KLD increment start refers to the number of epochs at which the KLD weight begins to increase from the initial KLD weight. KLD increment end is the number of epochs at which the KLD weight stops increasing at the maximum KLD weight. The asterisk (*) marks the hyperparameter that is dataset-dependent. network, σ(x), for the density values, and the color net- work, C(r), for conditional RGB generation. While the overall architecture is kept constant, the input dimensions of each network are modified to allow additional latent con- catenation. Optimization: Empirically, we observe that it is essen- tial to train the NeRF such that it learns the distribution of scenes within the PC-VAE latent space. Using only pre- defined learned samples to train may run the risk of relying on non-representative samples. On the other hand, direct re- sampling during each training iteration in Instant-NGP may lead to delayed training progress, due to NeRF’s sensitive optimization. In our optimization procedure, we use an LR of 0.002 along with an LR decay and start with pre-defined latent samples. Then we slowly introduce the re-sampled latents. We believe that this strategy progressively dimin- ishes the influence of a single sample, while maintaining efficient training. Based on our observations, this strategy contributes towards Instant-NGP’s ability to rapidly assim- ilate fundamental conditioning and environmental recon- struction, while simultaneously pushing the learning pro- cess to be less skewed towards a single latent sample. D. GUI Interface For ease of interaction with our inference pipeline, our NeRF loads a pre-trained MDN checkpoint, and we build a graphical user interface (GUI) using DearPyGUi for vi- sualization purposes. We implement three features in the GUI: (a) predict, (b) probe and predict, and (c) toggle. Predict: We implement the function to perform predic- tion directly from a given image path in the GUI. We use the distribution qϕ(zt−1|It−1 c ) from PC-VAE encoder, cor- responding to the input image It−1 c , to predict the latent distribution for the next timestamp q′ ϕ(zt|It−1 c ). This pro- cess is done on the fly through the MDN. A sample from the predicted distribution is then generated and used to con- dition the NeRF. This advances the entire scene to the next timestamp. Probe and predict: The sampled latent from the pre- dicted distribution does not correspond to a singular distri- bution and hence we can not directly predict the next times- 13 0 20 40 60 80 100 120 140 0 5 10 15 20 25 Epochs Average PSNR (a) Single-Scene Approaching Intersection 0 20 40 60 80 100 120 140 0 5 10 15 20 25 Epochs Average PSNR (b) Multi-Scene Approaching Intersection 0 20 40 60 80 100 120 140 0 5 10 15 20 25 Epochs Average PSNR (c) Multi-Scene Two Lane Merge Figure 11. Average train PSNR plot for all CARLA datasets. The plot shows the increase in average training PSNR of all images for each dataset, over the period of the training process. tamp. To make our model auto-regressive in nature, we per- form density probing. We probe the density of the NeRF at the possible location coordinates of the car to obtain the current timestamp and scene. This is then used to match the latent to a corresponding distribution in the PC-VAE space. The new distribution enables auto-regressive predictions us- ing the predict function described above. Toggle: The NeRF generates a scene corresponding to the provided input image path using learned latents from PC- VAE. When the input image is a fully observable view, the NeRF renders clear actor and ego configurations respecting the input. This allows us to visualize the scene at different timestamps and in different configurations. Figure 12. Latent sample distribution clustering. The distri- butions of latent samples for the Multi-Scene Two Lane Merge dataset are separable through t-SNE clustering. In the figure, the clusters for Scene 0, Timestamp 0 and Scene 1, Timestamp 0 over- lap in distribution because they represent the same initial state of the environment under dynamics uncertainty. Encoder Architectures Train PSNR SVM Accuracy NV PSNR Multi-Scene Approaching Intersection PC-VAE 26.47 89.17 26.37 PC-VAE w/o CL 26.20 83.83 26.16 Vanilla PC-VAE 25.97 29.33 25.93 PC-VAE w/o Freezing 24.82 29.83 24.78 PC-VAE w/ MobileNet 19.37 29.50 19.43 Vanilla VAE 26.04 14.67 9.84 Multi-Scene Two Lane Merge PC-VAE 25.50 88.33 25.84 PC-VAE w/o CL 24.38 29.67 24.02 Vanilla PC-VAE 24.75 29.67 24.96 PC-VAE w/o Freezing 23.97 28.33 24.04 PC-VAE w/ MobileNet 17.70 75.00 17.65 Vanilla VAE 25.11 28.17 8.49 Table 6. PC-VAE metrics and ablations across Multi-Scene datasets. CARFF’s PC-VAE outperforms other encoder architec- tures across the Multi-Scene datasets in reconstruction and pose- conditioning. E. CARFF Evaluation E.1. Pose-Conditional VAE Reconstruction Quality: To analyze the reconstruction performance of the model during training, we periodically plot grids of reconstructed images. These grids consist of (a) randomly selected encoder inputs drawn from the dataset, (b) the corresponding ground truth images for those inputs at each timestamp at the same camera pose, and (c) reconstructed outputs at randomly sampled poses respecting 14 0 10 20 30 40 50 0.4 0.6 0.8 1 Num Samples % Accuracy Recall (a) Approaching Intersection 0 10 20 30 40 50 0.4 0.6 0.8 1 Num Samples % Accuracy Recall (b) Two Lane Merge Figure 13. Multi-Scene dataset accuracy and recall curves from learned latents. We test our framework across n = 1 and n = 50 samples from PC-VAE’s latent distributions from ego- centric image input. Across the number of samples n, we achieve an ideal margin of belief state coverage generated under partial observation (recall), and the proportion of correct beliefs sampled under full observation (accuracy) for the MDN to learn. As we significantly increase the number of samples, the accuracy starts to decrease due to randomness in latent distribution resampling. the input scene and timestamp. An example reconstruction grid is provided in Fig. 9. The grid enables visual assess- ment of whether the model is capable of accurately recon- structing reasonable images using the encoder inputs, con- ditioned on the poses. This evaluation provides us with vi- sual evidence of improvement in reconstruction quality. We also quantitatively analyze the progressive improvement of reconstruction through the average PSNR calculated over the training data (see Fig. 11). Latent Space Analysis To assess the quality of the la- tents generated by PC-VAE, we initially use t-SNE plots to visualize the latent distributions as clusters. Fig. 12 shows that the distributions of the latent samples for the Multi- Scene Two Lane Merge dataset are separable. While t-SNE is good at retaining nearest-neighbor information by pre- serving local structures, it performs weakly in preserving global structures. Therefore, t-SNE may be insufficient in capturing the differences in distributions for all our datasets. Instead, we pivot to Support Vector Machine to perform a quantitative evaluation of the separability of the latents. We utilize a Radial Basis Function (RBF) kernel with the standard regularization parameter (C = 1). We perform 10-fold validation on the latents to calculate the accuracy as a metric for clustering. See Tab. 6 for the results. Beyond separability, we analyze the recall and accuracy of the learned latents directly from PC-VAE under par- tial and full observations. This achieves very high accu- racy even under a large number of samples while retrain- ing decent recall, enabling downstream MDN training. (See Fig. 13) E.2. Fully Observable Predictions One of the tasks of the MDN is to forecast the future scene configurations under full observation. We quantitatively evaluate our model’s ability to forecast future scenes by comparing bird’s-eye views rendered from the NeRF with chosen ground truth images of the scene for the various timestamps (see Tab. 7). The values are calculated and dis- played for all three datasets. In Tab. 7, images are marked as either toggled (˜Iti) or predicted (ˆIti). Toggled images in the table cannot be predicted deterministically due to it being the first timestamp in the dataset, or the state of the previous timestamps across scenes being the same in case of dynamics uncertainty. Due to the same reason, in the Multi-Scene Two Lane Merge Dataset, there are additional bolded PSNR values for the pairs (It1, ˜It4) and (It4, ˜It1). 15 Single-Scene Approaching Intersection Result It1 It2 It3 It4 It5 It6 It7 It8 It9 It10 ˜It1 29.01 −5.97 −6.08 −6.52 −6.44 −6.03 −6.31 −6.36 −6.26 −6.28 ˆIt2 −5.42 27.51 −3.07 −4.67 −4.58 −4.17 −4.43 −4.51 −4.39 −4.39 ˆIt3 −6.06 −2.81 28.12 −4.47 −4.68 −4.19 −4.05 −4.61 −4.47 −4.52 ˆIt4 −7.01 −5.37 −5.03 29.40 −4.99 −5.08 −5.03 −5.41 −5.28 −5.32 ˆIt5 −6.87 −5.2 −4.93 −5.00 29.44 −4.53 −4.46 −5.19 −5.05 −5.09 ˆIt6 −6.29 −4.55 −4.27 −4.8 −4.24 29.02 −4.02 −4.53 −4.38 −4.44 ˆIt7 −6.76 −5.05 −4.76 −5.31 −5.14 −4.36 29.50 −4.50 −4.86 −4.93 ˆIt8 −6.73 −5.02 −4.74 −5.25 −5.10 −4.64 −4.76 29.46 −4.41 −4.86 ˆIt9 −6.75 −5.00 −4.70 −5.23 −5.07 −4.64 −4.85 −4.52 29.55 −4.42 ˆIt10 −6.79 −5.06 −4.75 −5.30 −5.15 −4.69 −4.93 −5.01 −4.34 29.55 Multi-Scene Approaching Intersection Result It1 It2 It3 It4 It5 It6 ˜It1 28.10 −5.24 −5.50 −1.67 −3.29 −3.92 ˆIt2 −5.23 28.02 −6.11 −4.70 −3.21 −4.84 ˆIt3 −5.43 −6.03 27.97 −4.85 −4.53 −2.93 ˜It4 −1.71 −4.73 −5.00 28.26 −2.25 −3.08 ˜It5 −3.68 −3.24 −4.91 −2.76 28.21 −2.99 ˆIt6 −4.02 −4.91 −3.27 −3.13 −2.61 28.26 Multi-Scene Two Lane Merge Result It1 It2 It3 It4 It5 It6 ˜It1 28.27 −5.31 −6.41 28.23 −4.77 −5.42 ˜It2 −5.22 28.23 −5.17 −5.27 −2.91 −4.01 ˆIt3 −6.32 −5.09 28.14 −6.33 −5.01 −4.28 ˜It4 28.27 −5.27 −6.37 28.23 −4.72 −5.37 ˜It5 −4.64 −2.73 −5.01 −4.71 28.08 −5.29 ˆIt6 −5.32 −4.02 −4.32 −5.33 −5.34 28.17 Table 7. Complete PSNR values for fully observable predictions for all CARLA datasets. The table contains PSNR values between the ground truth images and either a toggled image (marked as ˜Iti), or a predicted image (marked as ˆIti). Toggled or predicted images that correspond to the correct ground truth are bolded and have a significantly higher PSNR value. The PSNR values for incorrect correspon- dances are replaced with the difference between the incorrect PSNR and the bolded PSNR associated with a correct correspondance. 16 Scene 1: Ego car with  actor ambulance Scene 2: Ego car only Scene 2: Ego car with  fast- moving ambulance Scene 1: Ego car with  slow- moving ambulance Multi- Scene Approaching Intersection Multi- Scene Two Lane Merge Single- Scene Approaching Intersection Timestamp 0: Actor  ambulance is about to  cross intersection Timestamp 1 Timestamp 9: Actor has  crossed the intersection Timestamp 2 Timestamp 3 Timestamp 6 Timestamp 4 Timestamp 5 Timestamp 7 Timestamp 8 Timestamp 3 Timestamp 4 Timestamp 5 Timestamp 0 Timestamp 1 Timestamp 2 Timestamp 0 Timestamp 1 Timestamp 2 Timestamp 3 Timestamp 4 Timestamp 5 Figure 14. Single-Scene Approaching Intersection, Multi-Scene Approaching Intersection, and Multi-Scene Two Lane Merge Datasets. The actor and ego car configurations for the timestamps and scenes of the three CARLA datasets are visualized at a single camera pose. The colors of the cars for the Multi-Scene Approaching Intersection have been slightly modified for greater contrast and visual clarity in the paper. 17 "
}