{
    "optim": "CARFF: Conditional Auto-encoded Radiance Field for 3D Scene Forecasting\nJiezhi “Stephen” Yang1 Khushi Desai2 Charles Packer3\nHarshil Bhatia4 Nicholas Rhinehart3 Rowan McAllister5 Joseph Gonzalez3\n1Harvard University\n2Columbia University\n3UC Berkeley\n4Avataar.ai\n5Toyota Research Institute\nAbstract\nWe propose CARFF: Conditional Auto-encoded Radi-\nance Field for 3D Scene Forecasting, a method for pre-\ndicting future 3D scenes given past observations, such as\n2D ego-centric images.\nOur method maps an image to\na distribution over plausible 3D latent scene configura-\ntions using a probabilistic encoder, and predicts the evo-\nlution of the hypothesized scenes through time.\nOur la-\ntent scene representation conditions a global Neural Radi-\nance Field (NeRF) to represent a 3D scene model, which\nenables explainable predictions and straightforward down-\nstream applications.\nThis approach extends beyond pre-\nvious neural rendering work by considering complex sce-\nnarios of uncertainty in environmental states and dynamics.\nWe employ a two-stage training of Pose-Conditional-VAE\nand NeRF to learn 3D representations. Additionally, we\nauto-regressively predict latent scene representations as a\npartially observable Markov decision process, utilizing a\nmixture density network. We demonstrate the utility of our\nmethod in realistic scenarios using the CARLA driving sim-\nulator, where CARFF can be used to enable efficient tra-\njectory and contingency planning in complex multi-agent\nautonomous driving scenarios involving visual occlusions.\nOur website containing video demonstrations and code is\navailable at: www.carff.website.\n1. Introduction\nHumans often imagine what they cannot see given partial\nvisual context. Consider a scenario where reasoning about\nthe unobserved is critical to safe decision-making: for ex-\nample, a driver navigating a blind intersection. An expert\ndriver will plan according to what they believe may or may\nnot exist in occluded regions of their vision. The driver’s\nbelief – defined as the understanding of the world mod-\neled with consideration for inherent uncertainties of real-\nworld environments – is informed by their partial observa-\ntions (i.e., the presence of other vehicles on the road), as\nwell as their prior knowledge (e.g., past experience nav-\nigating this intersection). When reasoning about the un-\nobserved, humans are capable of holding complex beliefs\nCARFF \nEncoder\n3D \nPlanning\n3D State Beliefs\n3D Planning Results\nInput Image\nSTOP\nCARFF 3D Driving Planning Application\nFigure 1. CARFF 3D planning application for driving. An in-\nput image containing a partially observable view of an intersec-\ntion is processed by CARFF’s encoder to establish 3D environ-\nment state beliefs, i.e. the predicted possible state of the world:\nwhether or not there could be another vehicle approaching the in-\ntersection. These beliefs are used to forecast the future in 3D for\nplanning, generating one among two possible actions for the vehi-\ncle to merge into the other lane.\nover not just the existence and position of individual objects\n(e.g., whether or not there is an oncoming car), but also the\nshapes, colors, and textures composing the entire occluded\nportion of the scene.\nTraditionally,\nautonomous\nsystems\nwith\nhigh-\ndimensional sensor observations such as video or LiDAR\nprocess the data into low-dimensional state information,\nsuch as the position and velocity of tracked objects, which\nare then used for downstream prediction and planning.\nThis object-centric framework can be extended to reason\nabout partially observed settings by modeling the existence\nand state of potentially dangerous unobserved objects in\naddition to tracked fully observed objects. Such systems\noften plan with respect to worst-case hypotheses, e.g., by\nplacing a “ghost car” traveling at speed on the edge of the\nvisible field of view [41].\nRecent advances in neural rendering have seen tremen-\ndous success in learning 3D scene representations directly\nfrom posed multi-view images using deep neural networks.\nNeural Radiance Fields (NeRF) allow for synthesizing\nnovel images in a 3D scene from arbitrary viewing angles,\nmaking seeing behind an occlusion as simple as rendering\nfrom an unoccluded camera angle. Because NeRF can gen-\nerate RGB images from novel views, it decouples the de-\npendency of the scene representation on the object detec-\n1\narXiv:2401.18075v1  [cs.CV]  31 Jan 2024\ntion and tracking pipeline. For example, images rendered\nfrom a NeRF may contain visual information that would\nbe lost by an object detector, but may still be relevant for\nsafe decision-making. Additionally, because NeRF repre-\nsents explicit geometry via an implicit density map, it can\nbe used directly for motion planning without requiring any\nrendering [1]. NeRF’s ability to represent both visual and\ngeometric information makes them a more general and in-\ntuitive 3D representation for autonomous systems.\nDespite NeRF’s advantages, achieving probabilistic pre-\ndictions in 3D based on reasoning from the occluded is\nchallenging. For example, discriminative models that yield\ncategorical predictions are unable to capture the underly-\ning 3D structure, impeding their ability to model uncer-\ntainty.\nWhile prior work on 3D representation captures\nview-invariant structures, their application is primarily con-\nfined to simple scenarios [17]. We present CARFF, which\nto our knowledge, is the first forecasting approach in sce-\nnarios with partial observations that uniquely facilitates\nstochastic predictions within a 3D representation, effec-\ntively integrating visual perception and explicit geometry.\nCARFF addresses the aforementioned difficulties by\nproposing PC-VAE: Pose-Conditioned Variational Autoen-\ncoder, a framework that trains a convolution and Vision\nTransformer (ViT) [9] based image encoder. The encoder\nmaps a potentially partially observable ego-centric image to\nlatent scene representations, which hold state beliefs with\nimplicit probabilities. The latents later condition a neural\nradiance field that functions as a 3D decoder to recover 3D\nscenes from arbitrary viewpoints. This is trained after PC-\nVAE in our two-stage training pipeline (see Sec. 3.1). Ad-\nditionally, we design a mixture density model to predict the\nevolution of 3D scenes over time stochastically in the en-\ncoder belief space (see Sec. 3.2). A potential application of\nCARFF is illustrated in Fig. 1. Using the CARLA driving\nsimulator, we demonstrate how CARFF can be used to en-\nable contingency planning in real-world driving scenarios\nthat require reasoning into visual occlusions.\n2. Related work\n2.1. Dynamic Neural Radiance Fields\nNeural\nradiance\nfields:\nNeural\nRadiance\nFields\n(NeRF) [2, 23, 39] for 3D representations have gar-\nnered significant attention due to their ability to generate\nhigh-resolution, photorealistic scenes.\nInstant Neural\nGraphics Primitive (Instant-NGP) [24] speeds up training\nand rendering time by introducing a multi-resolution\nhash encoding.\nOther works like Plenoxels [11] and\nDirectVoxGo (DVGO) [38] also provide similar speedups.\nGiven the wide adoption and speedups of Instant-NGP, we\nuse it to model 3D radiance fields in our work.\nNeRFs\nhave also been extended for several tasks such as modeling\nlarge-scale unbounded scenes [2, 40, 46], scene from sparse\nviews [7, 35, 45] and multiple scenes [17, 48]. Tewari et al.\n[42] presents an in-depth survey on neural representation\nlearning and its applications.\nGeneralizable novel view synthesis models such as pix-\nelNeRF and pixelSplat [5, 51] learn a scene prior to ren-\nder novel views conditioned on sparse existing views. Dy-\nnamic NeRF, on the other hand, models scenes with moving\nobjects or objects undergoing deformation. A widely used\napproach is to construct a canonical space and predict a de-\nformation field [19, 29, 30, 32]. The canonical space is usu-\nally a static scene, and the model learns an implicitly repre-\nsented flow field [29, 32]. A recent line of work also models\ndynamic scenes via different representations and decompo-\nsition [3, 37]. These approaches tend to perform better\nfor spatially bounded and predictable scenes with relatively\nsmall variations [3, 20, 29, 51]. Moreover, these methods\nonly solve for changes in the environment but are limited in\nincorporating stochasticity in the environment.\nMulti-scene NeRF:\nOur approach builds on multi-scene\nNeRF approaches [17, 44, 48, 49] that learn a global latent\nscene representation, which conditions the NeRF, allowing\na single NeRF to effectively represent various scenes. A\nsimilar method, NeRF-VAE, was introduced by Kosiorek\net al. [17] to create a geometrically consistent 3D genera-\ntive model with generalization to out-of-distribution cam-\neras. However, NeRF-VAE [17] is prone to mode collapse\nwhen evaluated on complex, real-world datasets.\n2.2. Scene Forecasting\nPlanning in 2D space:\nIn general, planning in large and\ncontinuous state-action spaces is difficult due to the result-\ning exponentially large search space [28]. Consequently,\nseveral approximation methods have been proposed for\ntractability [22, 31].\nVarious model-free [12, 27, 43]\nand model-based [4] reinforcement learning frameworks\nemerge as viable approaches, along with other learning-\nbased methods [6, 25]. Several other approaches forecast\nfor downstream control [15], learn behavior models for con-\ntingency planning [34], or predict potential existence and\nintentions of possibly unobserved agents [26]. While these\nmethods are in 2D, we similarly reason under partial obser-\nvations, and account for these factors in 3D.\nNeRF in robotics:\nSeveral recent works have explored\nthe application of NeRFs in robotics, like localization [50],\nnavigation [1, 21], dynamics-modeling [10, 19] and robotic\ngrasping [14, 16].\nAdamkiewicz et al. [1] proposes a\nmethod for quadcopter motion planning in NeRF models\nvia sampling the learned density function, which is a de-\nsirable characteristic of NeRF that can be leveraged for\nforecasting and planning purposes.\nAdditionally, Driess\n2\nEgo- centric Lane View\nPredicted 3D View\nFigure 2. Novel view planning application. CARFF allows rea-\nsoning behind occluded views from the ego car as simple as mov-\ning the camera to see the sampled belief predictions, allowing sim-\nple downstream planning using, for example, density probing or\n2D segmentation models from arbitrary angles.\net al. [10] utilize a graph neural network to learn a dy-\nnamics model in a multi-object scene represented through\nNeRF. Li et al. [18] primarily perform pushing tasks in a\nscene with basic shapes, and approach grasping and plan-\nning with NeRF and a separately learned latent dynamics\nmodel. Prior work either only performs well on simple and\nstatic scenes [1] or has a deterministic dynamics model [18].\nCARFF focuses on complicated realistic environments in-\nvolving both state uncertainty and dynamics uncertainty,\nwhich account for the potential existence of an object and\nunknown object movements respectively.\n3. Method\n3D scene representation has witnessed significant advance-\nments in recent years, allowing for modeling environments\nin a contextually rich and interactive 3D space. This offers\nmany analytical benefits, such as producing soft occupancy\ngrids for spatial analysis and novel view synthesis for ob-\nject detection. Given the advantages, our primary objective\nis to develop a model for probabilistic 3D scene forecast-\ning in dynamic environments. However, direct integration\nof 3D scene representation via NeRF and probabilistic mod-\nels like VAE often involves non-convex and inter-dependent\noptimization, which causes unstable training. For instance,\nNeRF’s optimization may rely on the VAE’s latent space\nbeing structured to provide informative gradients.\nTo navigate these complexities, our method bifurcates\nthe training process into two stages (see Fig. 3).\nFirst,\nwe train the PC-VAE to learn view-invariant scene repre-\nsentations. Next, we replace the decoder with a NeRF to\nlearn a 3D scene from the latent representations. The la-\ntent scene representations capture the environmental states\nand dynamics over possible underlying scenes, while NeRF\nsynthesizes novel views within the belief space, giving us\nthe ability to see the unobserved (see Fig. 2 and Sec. 3.1).\nDuring prediction, uncertainties can be modeled by sam-\npling latents auto-regressively from a predicted Gaussian\nmixture, allowing for effective decision-making. To this\nextent, we approach scene forecasting as a partially observ-\nable Markov decision process (POMDP) over latent distri-\nbutions, which enables us to capture multi-modal beliefs for\nplanning amidst perceptual uncertainty (see Sec. 3.2).\n3.1. NeRF Pose-Conditional VAE (PC-VAE)\nArchitecture:\nGiven a scene St at timestamp t, we render\nan ego-centric observation image It\nc captured from camera\npose c. The objective is to formulate a 3D representation\nof the image where we can perform a forecasting step that\nevolves the scene forward. To achieve this, we utilize a radi-\nance field conditioned on latent variable z sampled from the\nposterior distribution qϕ(z|It\nc). Now, to learn the posterior,\nwe utilize PC-VAE. We construct an encoder using convolu-\ntional layers and a pre-trained ViT on ImageNet [9]. The en-\ncoder learns a mapping from the image space to a Gaussian\ndistributed latent space qϕ(z|It\nc) = N(µ, σ2) parametrized\nby mean µ and variance σ2. The decoder, p(I|z, c), con-\nditioned on camera pose c, maps the latent z ∼ N(µ, σ2)\ninto the image space I. This helps the encoder to generate\nlatents that are invariant to the camera pose c.\nTo enable 3D scene modeling, we employ Instant-\nNGP [24], which incorporates a hash grid and an occu-\npancy grid to enhance computation efficiency. Addition-\nally, a smaller multilayer perceptron (MLP), Fθ(z) can be\nutilized to model the density and appearance, given by:\nFθ(z) : (x, d, z) → ((r, g, b), σ)\n(1)\nHere, x ∈ R3 and d ∈ (θ, ϕ) represent the location vector\nand the viewing direction respectively. The MLP is also\nconditioned on the sampled scene latents z ∼ qϕ(z|It\nc) (see\nAppendix C).\nTraining methodology:\nThe architecture alone does not\nenable us to model complex real-world scenarios, as seen\nthrough a similar example in NeRF-VAE [17]. A crucial\ncontribution of our work is our two-stage training frame-\nwork which stabilizes the training. First, we optimize the\nconvolutional ViT based encoder and pose-conditional con-\nvolutional decoder in the pixel space for reconstruction.\nThis enables our method to deal with more complex and re-\nalistic scenes as the encoding is learned in a semantically\nrich 2D space.\nBy conditioning the decoder on camera\nposes, we achieve disentanglement between camera view\nangles and scene context, making the representation view-\ninvariant and the encoder 3D-aware. Next, once rich latent\nrepresentations are learned, we replace the decoder with a\nlatent-conditioned NeRF over the latent space of the frozen\nencoder. The NeRF reconstructs encoder beliefs in 3D for\nnovel view synthesis.\nLoss:\nOur PC-VAE is trained using standard VAE loss,\nwith mean square error (MSE) and a Kullback–Leibler (KL)\ndivergence given by evidence lower bound (ELBO):\nLPC-VAE = LMSE, PC-VAE + LKLD, PC-VAE =\n||p(I|z, c′′) − It\nc′′∥2 + Eq(z|It\nc)[log p(I|z)]\n− wKLDKL(qϕ(z|It\nc) || p(I|z))\n(2)\n3\nEncoder\nDecoder\nGaussian latent \ndistribution\nConditioning on \ncamera pose\nDecoded images for \ncamera pose\nGround truths for \ncamera pose\nNeRF\nStage 1: VAE Encoder and Decoder training\nStage 2: Frozen encoder, NeRF Decoder\nPosed Images\n* Images are shown as batched for training only\nFigure 3. Visualizing CARFF’s two stage training process. Left: The convolutional VIT based encoder encodes each of the images I at\ntimestamps t, t′ and camera poses c, c′ into Gaussian latent distributions. Assuming only two timestamps and an overparameterized latent,\none of the Gaussian distributions will have a smaller σ2, and different µ across timestamps. Upper Right: The pose-conditional decoder\nstochastically decodes the sampled latent z using the camera pose c′′ into images It\nc′′ and It′\nc′′. The decoded reconstruction and ground\ntruth images are used to take the loss LMSE, PC-VAE. Lower Right: A NeRF is trained by conditioning on the latent variables sampled from\nthe optimized Gaussian parameters. These parameters characterize the distinct timestamp distributions derived from the PC-VAE. An MSE\nloss is calculated for NeRF as LMSE, NeRF.\nwhere wKL denotes the KL divergence loss weight and\nz ∼ qϕ(z|It\nc). To make our representation 3D-aware, our\nposterior is encoded using camera c while the decoder is\nconditioned on a randomly sampled pose c′′.\nKL divergence regularizes the latent space to balance\nconditioned reconstruction and stochasticity under occlu-\nsion. An elevated KL divergence loss weight wKL pushes\nthe latents closer to a standard normal distribution, N(0, 1),\nthereby ensuring probabilistic sampling in scenarios un-\nder partial observation. However, excessive regularization\ncauses the latents to be less separable, leading to mode col-\nlapse. To mitigate this, we adopt delayed linear KL diver-\ngence loss weight scheduling to strike a balanced wKL.\nNext, we learn a NeRF-based decoder on the posterior\nof the VAE to model scenes. At any timestamp t we use a\nstandard pixel-wise MSE loss for training the NeRF, given\nby the following equation:\nLMSE, NeRF = ∥It\nc − render(Fθ(·|qϕ(z|It\nc)))∥2\n(3)\nWe use a standard rendering algorithm as proposed by\nM¨uller et al. [24]. Next, we build a forecasting module over\nthe learned latent space of our pose-conditional encoder.\n3.2. Scene Forecasting\nFormulation:\nThe current formulation allows us to model\nscenes with different configurations across timestamps. In\norder to forecast future configurations of a scene given an\nego-centric view, we need to predict future latent distribu-\ntions. We formulate the forecasting as a partially observable\nMarkov decision process (POMDP) over the posterior dis-\ntribution qϕ(z|It\nc) in the PC-VAE’s latent space.\nDuring inference, we observe stochastic behaviors un-\nder occlusion, which motivates us to learn a mixture of sev-\neral Gaussian distributions that potentially denote different\nscene possibilities. Therefore, we model the POMDP us-\ning a Mixture Density Network (MDN), with multi-headed\nMLPs, that predicts a mixture of K Gaussians.\nAt any\ntimestamp t the distribution is given as:\nq′\nϕ(zt|It−1\nc\n) = MDN(qϕ(zt−1|It−1\nc\n))\n(4)\nThe model is conditioned on the posterior distribu-\ntion qϕ(zt−1) to learn a predicted posterior distribution\nq′\nϕ(zt|It−1\nc\n) at each timestamp. The predicted posterior dis-\ntribution is given by the mixture of Gaussian:\nq′\nϕ(zt) =\nK\nX\ni=1\nπi N(µi, σ2\ni )\n(5)\nhere, πi, µi, and σ2\ni denote the mixture weight, mean, and\nvariance of the ith Gaussian distribution within the pos-\nterior distribution. Here, K is the total number of Gaus-\nsians. For brevity we remove their conditioning on the pos-\nterior qϕ(zt−1) and sampled latent zt−1. We sample zt from\nthe mixture of Gaussians q′\nϕ(zt), where zt most likely falls\nwithin one of the Gaussian modes. The scene configura-\ntion corresponding to the mode is reflected in the 3D scene\nrendered by NeRF.\nLoss:\nTo optimize the MDN, we minimize a negative log-\nlikelihood function, given by:\nLMDN = −\nN\nX\ni=1\nlog\n\n\nK\nX\nj=1\nπjN(yi; µj, σ2\nj )\n\n\n(6)\n4\nScene 1: Ego car with \nactor ambulance\nScene 2: Ego car only\nScene 1: Ego car with \nslow- moving ambulance\nScene 2: Ego car with \nfast- moving ambulance\nMulti- Scene Approaching Intersection\nMulti- Scene Two Lane Merge\nFigure 4. Multi-scene CARLA datasets. Images illustrating the\nvarying car configurations and scenes for the Multi-Scene Two\nLane Merge dataset (left) and the Multi-Scene Approaching In-\ntersection dataset (right).\nFigure 5. Blender dataset. Simple Blender dataset with a station-\nary blue cube, accompanied by a potential red cylinder exhibit-\ning probabilistic temporal movement. The different camera poses\ndemonstrate how movement needs to be modeled probabilistically\nbased on possible occlusions from different camera angles.\nwhere yi ∼ qϕ(zt) is sampled from the distribution of latent\nzt, learned by the encoder, and N denotes the total number\nof samples.\nInference:\nWe consider an unseen ego-centric image and\nretrieve its posterior qϕ(zt) through the encoder. Next, we\npredict the possible future posterior distribution q′\nϕ(zt+1).\nFrom the predicted posterior, we sample a scene latent and\nperform localization. We achieve this via (a) density prob-\ning the NeRF or (b) segmenting the rendered novel views\nusing off-the-shelf methods such as YOLO [33] (see Fig. 2).\nThese allow us to retrieve a corresponding Gaussian dis-\ntribution qϕ(zt+1) in encoder latent space. This is auto-\nregressively fed back into the MDN to predict the next\ntimestamp. See Fig. 6 for an overview of the pipeline.\n4. Results\nDecision-making under perceptual uncertainty is a perva-\nsive challenge faced in robotics and autonomous driving,\nespecially in partially observable environments encountered\nin driving tasks. In these scenarios, accurate inference re-\ngarding the presence of potentially obscured agents is piv-\notal. We evaluate the effectiveness of CARFF on similar\nreal-world situations with partial observability. We imple-\nmented several scenarios in the CARLA driving simula-\ntor [8] (see Fig. 4). A single NVIDIA RTX 3090 GPU is\nused to train PC-VAE, NeRF, and the MDN. All models,\ntrained sequentially, tend to converge within a combined\ntime frame of 24 hours. A detailed experimental setup can\nbe found in Appendix C. We show that, given partially ob-\nservable 2D inputs, CARFF performs well in predicting la-\ntent distributions that represent complete 3D scenes. Using\nthese predictions we design a CARFF-based controller for\nperforming downstream planning tasks.\n4.1. Data Generation\nWe generate datasets containing an ego object and vary-\ning actor objects in different configurations to test the ro-\nbustness of our method. We conduct experiments on (a)\nsynthetic blender dataset for simple, controllable simula-\ntion and (b) CARLA-based driving datasets for complicated\nreal-world scenarios [8].\nBlender synthetic dataset:\nThis comprises of a station-\nary blue cube (ego) accompanied by a red cylinder (actor)\nthat may or may not be present (see Fig. 5).\nIf the ac-\ntor is present, it exhibits lateral movement as depicted in\nFig. 5. This simplistic setting provides us with an inter-\npretable framework to evaluate our model.\nCARLA dataset:\nEach dataset is simulated for N times-\ntamps and uses C = 100 predefined camera poses to cap-\nture images of the environment under full observation, par-\ntial observation, and no visibility. These datasets are mod-\neled after common driving scenarios involving state uncer-\ntainty that have been proposed in related works such as Ac-\ntive Visual Planning [25].\na) Single-Scene Approaching Intersection: The ego ve-\nhicle is positioned at a T-intersection with an actor vehicle\ntraversing the crossing along an evenly spaced, predefined\ntrajectory. We simulate this for N = 10 timestamps. We\nmainly use this dataset to predict the evolution of times-\ntamps under full observation.\nb) Multi-Scene Approaching Intersection: We extend the\naforementioned scenario to a more complicated setting with\nstate uncertainty, by making the existence of the actor vehi-\ncle probabilistic. A similar intersection crossing is simu-\nlated for N = 3 timestamps for both possibilities. The ego\nvehicle’s view of the actor may be occluded as it approaches\nthe T-intersection over the N timestamps. The ego vehicle\ncan either move forward or halt at the junction (see Fig. 4).\nc) Multi-Scene Multi-actor Two Lane Merge: To add\nmore environment dynamics uncertainty, we consider a\nmulti-actor setting at an intersection of two merging lanes.\nWe simulate the scenario at an intersection with partial oc-\nclusions, with the second approaching actor having variable\nspeed. Here the ego vehicle can either merge into the left\n5\nNeRF\nGaussian Mixture\nRepeated Sampling\nMixture Density \nNetwork\nProbing For Autoregressive Prediction\nPretrained\nEncoder\nSampled Beliefs\nFigure 6. Auto-regressive inference in scene prediction. The input image at timestamp t, It\nc, is encoded using the pre-trained encoder\nfrom PC-VAE. The corresponding latent distribution is fed into the Mixture Density Network, which predicts a mixture of Gaussians. Each\nof the K Gaussians is a latent distribution that may correspond to different beliefs at the next timestamp. The mixture of Gaussians is\nsampled repeatedly for the predicted latent beliefs, visualized as It+1\nc′,scni, representing potentially the ith possible outcome. This is used to\ncondition the NeRF to generate 3D views of the scene. To accomplish autoregressive predictions, we probe the NeRF for the location of\nthe car and feed this information back to the pre-trained encoder to predict the scene at the next timestamp.\nlane before the second actor or after all the actors pass, (see\nFig. 4). Each branch is simulated for N = 3 timestamps.\n4.2. CARFF Evaluation\nA desirable behavior from our model is that it should predict\na complete set of possible scenes consistent with the given\nego-centric image, which could be partially observable.\nThis is crucial for autonomous driving in unpredictable en-\nvironments as it ensures strategic decision-making based on\npotential hazards. To achieve this we require a rich PC-\nVAE latent space, high-quality novel view synthesis, and\nauto-regressive probabilistic predictions of latents at future\ntimestamps.\nWe evaluate CARFF on a simple synthetic\nblender-based dataset and each CARLA-based dataset.\nEvaluation on blender dataset:\nIn Fig. 5, for both Scene\n1a and 1b, our model correctly forecasts the lateral move-\nment of the cylinder to be in either position approximately\n50% of the time, considering a left viewing angle. In Scene\n2, with the absence of the red cylinder in the input cam-\nera angle, the model predicts the potential existence of the\nred cylinder approximately 50% of the time, and predicts\nlateral movements with roughly equal probability. This val-\nidates PC-VAE’s ability to predict and infer from the oc-\ncluded in the latent space, consistent with human intuitions.\nSimilar intuitions, demonstrated within the simple scenes of\nthe Blender dataset, can be transferred to driving scenarios\nsimulated in our CARLA datasets.\nPC-VAE performance and ablations:\nWe evaluate the\nperformance of PC-VAE on CARLA datasets with multiple\nencoder architectures. We show that PC-VAE effectively re-\nconstructs complex environments involving variable scenes,\nactor configurations, and environmental noise given poten-\ntially partially observable inputs (see Fig. 9). We calcu-\nlated an average Peak Signal-to-Noise Ratio (PSNR) over\nthe training data, as well as novel view encoder inputs.\nTo evaluate the quality of the latent space generated by\nthe encoder, we utilize t-SNE [47] plots to visualize the\ndistribution of latent samples for each image in a given\ndataset (see Appendix E). We introduce a Support Vector\nMachine (SVM) [13] based metric to measure the visual-\nized clustering quantitatively, where a higher value indi-\ncates better clustering based on timestamps. Most latent\nscene samples are separable by timestamps, which indicates\nthat the latents are view-invariant. Samples that are mis-\nclassified or lie on the boundary usually represent partially\nor fully occluded regions. This is desirable for forecast-\ning, as it enables us to model probabilistic behavior over\nthese samples. In this process, balancing KL divergence\nweight scheduling maintains the quality of the PC-VAE’s\nlatent space and reconstructions (see Appendix C). The re-\nsults presented in Tab. 2 substantiate the benefits of our PC-\nVAE encoder architecture compared to other formulations.\nSpecifically, a non-conditional VAE fails in SVM accuracy\nas it only reconstructs images and does not capture the un-\nderlying 3D structures. Vanilla PC-VAE and PC-VAE with-\nout freezing weights require careful fine-tuning of several\nhyper-parameters and don’t generalize well to drastic cam-\nera movements. Our experiments show that our proposed\nmodel is capable of sustaining stochastic characteristics via\nlatent representations in the presence of occlusion, while si-\nmultaneously ensuring precise reconstructions.\n3D novel view synthesis:\nGiven an unseen ego-centric\nview with potentially partial observations, our method\nmaintains all possible current state beliefs in 3D, and faith-\n6\nPose     Inputs\nPC- VAE Decoded Images From Set of New Pose\nFigure 7. PC-VAE reconstructions. The encoder input, It\nc, among the other ground truth images Ic viewed from camera pose c at different\ntimestamps, is reconstructed across a new set of poses c′′ respecting timestamp t, generating It\nc′′. A complete grid is in Appendix E.\nGround Truth\nPrediction Pair\nAvg. PSNR\n(Scene 1)\nAvg. PSNR\n(Scene 2)\nSingle-Scene Approaching Intersection\nMatching Pairs\n29.06\nN.A\nUn-matching Pairs\n24.01\nN.A\nMulti-Scene Approaching Intersection\nMatching Pairs\n28.00\n28.26\nUn-matching Pairs\n23.27\n24.56\nMulti-Scene Two Lane Merge\nMatching Pairs\n28.14\n28.17\nUn-matching Pairs\n22.74\n23.32\nTable 1. Averaged PSNR for fully observable 3D predictions.\nCARFF correctly predicts scene evolution across all timestamps\nfor each dataset. The average PSNR is significantly higher for each\nprediction ˆ\nIti and corresponding ground truth, Iti. PSNR values\nfor incorrect correspondences, ˆ\nIti, Itj, is a result of matching sur-\nroundings. The complete table of predictions is in Appendix E.\nfully reconstructs novel views from arbitrary camera angles\nfor each belief. Fig. 2 illustrates one of the possible 3D be-\nliefs that CARFF holds. This demonstrates our method’s ca-\npacity for generating 3D beliefs that could be used for novel\nview synthesis in a view-consistent manner. Our model’s\nability to achieve accurate and complete 3D environmental\nunderstanding is important for applications like prediction-\nbased planning.\nInference under full and partial observations:\nUnder\nfull observation, we use MDN to predict the subsequent car\npositions in all three datasets. PSNR values are calculated\nbased on bird-eye view NeRF renderings and ground truth\nbird-eye view images of the scene across different times-\ntamps. In Tab. 1 we report the PSNR values for rendered\nimages over the predicted posterior with the ground truth\nimages at each timestamp. We also evaluate the efficacy\nof our prediction model using the accuracy curve given in\nFig. 8. This represents CARFF’s ability to generate stable\nbeliefs, without producing incorrect predictions, based on\nactor(s) localization results. For each number of samples\nbetween n = 0 to n = 50, we choose a random subset of\n3 fully observable ego images and take an average of the\nEncoder Architectures\nTrain\nPSNR\nSVM\nAccuracy\nNV\nPSNR\nPC-VAE\n26.30\n75.20\n25.24\nPC-VAE w/o CL\n26.24\n70.60\n24.80\nVanilla PC-VAE\n26.02\n25.70\n24.65\nPC-VAE w/o Freezing\n24.57\n5.80\n24.60\nPC-VAE w/ MobileNet\n17.14\n19.70\n17.16\nVanilla VAE\n24.15\n10.60\n11.43\nTable 2. PC-VAE metrics and ablations. CARFF’s PC-VAE en-\ncoder outperforms other encoder architectures in both image re-\nconstruction and pose-conditioning. We evaluated the following\nablations: PC-VAE without Conv Layer, PC-VAE with a vanilla\nencoder, PC-VAE without freezing weights in ViT, PC-VAE re-\nplacing ViT with pre-trained MobileNet, and non pose-conditional\nVanilla VAE. The table displays the average training PSNR, novel\nview (NV) input PSNR, and SVM accuracy for latent timestamp\nprediction.\naccuracies. In scenarios with partial observable ego-centric\nimages where several plausible scenarios exist, we utilize\nrecall instead of accuracy using a similar setup. This lets us\nevaluate the encoder’s ability to avoid false negative predic-\ntions of potential danger.\nFig. 8 shows that our model achieves high accuracy\nand recall in both datasets, demonstrating the ability to\nmodel state uncertainty (Approaching Intersection) and dy-\nnamic uncertainty (Two Lane Merge). The results indicate\nCARFF’s resilience against randomness in resampling, and\ncompleteness in probabilistic modeling of the belief space.\nGiven these observations, we now build a reliable controller\nto plan and navigate through complex scenarios.\n4.3. Planning\nIn all our experiments, the ego vehicle must make decisions\nto advance under certain observability. The scenarios are\ndesigned such that the ego views contain partial occlusion\nand the state of the actor(s) is uncertain in some scenar-\nios. In order to facilitate decision-making using CARFF, we\ndesign a controller that takes ego-centric input images and\noutputs an action. Decisions are made incorporating sample\nconsistency from the mixture density network. For instance,\nthe controller infers occlusion and promotes the ego car to\n7\nMulti-Scene Approaching Intersection\nController Type\nActor Exists\nNo Actor\nUnderconfident\n30/30\n0/30\nOverconfident\n0/30\n30/30\nCARFF (n = 2)\n17/30\n30/30\nCARFF (n = 10)\n30/30\n30/30\nCARFF (n = 35)\n30/30\n19/30\nMulti-Scene Two Lane Merge\nController Type\nFast Actor\nSlow Actor\nUnderconfident\n30/30\n0/30\nOverconfident\n0/30\n30/30\nCARFF (n = 2)\n21/30\n30/30\nCARFF (n = 10)\n30/30\n30/30\nCARFF (n = 35)\n30/30\n22/30\nTable 3. Planning in 3D with controllers with varying sam-\npling numbers n. CARFF-based controllers outperform base-\nlines in success rate over 30 trials. For n = 10, the CARFF-based\ncontroller consistently chooses the optimal action in potential col-\nlision scenarios. For actor exists and fast-actor scenes, we con-\nsider occluded ego-centric inputs to test CARFF’s ability to avoid\ncollisions. For no-actor and slow-actor scenes, we consider state\nobservability and test the controllers’ ability to recognize the opti-\nmal action to advance. To maintain consistency, we use one single\nimage input across 30 trials.\npause when scenes alternate between actor presence and ab-\nsence in the samples. We use the two multi-scene datasets\nto assess the performance of the CARFF-based controller as\nthey contain actors with potentially unknown behaviors.\nTo design an effective controller, we need to find a bal-\nance between accuracy and recall (see Fig. 8). A lowered\naccuracy from excessive sampling means unwanted ran-\ndomness in the predicted state. However, taking insufficient\nsamples would generate low recall i.e., not recovering all\nplausible states. This would lead to incorrect predictions as\nwe would be unable to account for the plausible uncertainty\npresent in the environment. To find a balance, we design an\nopen-loop planning controller opting for a sampling strat-\negy that involves generating n = 2, 10, 35 samples, where\nn is a hyperparameter to be tuned for peak performance.\nFor sampling values that lie on the borders of the accu-\nracy and recall margin, for example, n = 2 and 35, we\nsee that the CARFF-based controller obtains lower success\nrates, whereas n = 10 produces the best result. Across\nthe two datasets in Tab. 3, the overconfident controller will\ninevitably experience collisions in case of a truck approach-\ning, since it does not cautiously account for occlusions.\nOn the other hand, an overly cautious approach results in\nstasis, inhibiting the controller’s ability to advance in the\nscene. This nuanced decision-making using CARFF-based\ncontroller is especially crucial in driving scenarios, as it en-\nhances safety and efficiency by adapting to complex and\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(a) Approaching Intersection\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(b) Two Lane Merge\nFigure 8. Multi-Scene dataset accuracy and recall curves from\npredicted beliefs.\nWe test our framework across n = 1 and\nn = 50 samples from MDN’s predicted latent distributions from\nego-centric image input. Across the number of samples n, we\nachieve an ideal margin of belief state coverage generated under\npartial observation (recall), and the proportion of correct beliefs\nsampled under full observation (accuracy). As we significantly in-\ncrease the number of samples, the accuracy starts to decrease due\nto randomness in latent distribution resampling.\nunpredictable road environments, thereby fostering a more\nreliable and human-like response in autonomous vehicles.\n5. Discussion\nLimitations:\nLike other NeRF-based methods, CARFF\ncurrently relies on posed images of specific scenes such as\nroad intersections, limiting its direct applicability to unseen\nenvironments. However, we anticipate enhanced generaliz-\nability with the increasing deployment of cameras around\npopulated areas, such as traffic cameras at intersections.\nAdditionally, handling very complex dynamics with an ex-\ntremely large number of actors still poses a challenge for our\nmethod, requiring careful fine-tuning to balance compre-\nhensive dynamics modeling against accuracy. Potentially\nstronger models in the near future may offer a promising\navenue for further enhancements in this regard.\nConclusion:\nWe presented CARFF, a novel method for\nprobabilistic 3D scene forecasting from partial observa-\ntions.\nBy employing a Pose-Conditional VAE, a NeRF\nconditioned on the learned posterior, and a mixture density\nnetwork that forecasts future scenes, we effectively model\ncomplex real-world environments with state and dynam-\nics uncertainty in occluded regions critical for planning.\nWe demonstrated the capabilities of our method in realistic\nautonomous driving scenarios, where, under full observa-\ntions, we can forecast into the future providing high-fidelity\n3D reconstructions of the environment, while we maintain\ncomplete recall of potential hazards given incomplete scene\ninformation. Overall, CARFF offers an intuitive and unified\napproach to perceiving, forecasting, and acting under un-\ncertainty that could prove invaluable for vision algorithms\nin unstructured environments.\n8\nReferences\n[1] Michal Adamkiewicz, Timothy Chen, Adam Caccavale,\nRachel Gardner, Preston Culbertson, Jeannette Bohg, and\nMac Schwager. Vision-only robot navigation in a neural ra-\ndiance world. IEEE Robotics and Automation Letters, 7(2):\n4606–4613, 2022. 2, 3, 12\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter\nHedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.\nMip-nerf: A multiscale representation for anti-aliasing neu-\nral radiance fields. In Int. Conf. Comput. Vis., pages 5855–\n5864, 2021. 2\n[3] Ang Cao and Justin Johnson. Hexplane: A fast representa-\ntion for dynamic scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 130–141, 2023. 2\n[4] Jinkun Cao, Xin Wang, Trevor Darrell, and Fisher Yu.\nInstance-aware predictive navigation in multi-agent environ-\nments. In IEEE Int. Conf. on Robotics and Automation, pages\n5096–5102. IEEE, 2021. 2\n[5] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent\nSitzmann. pixelsplat: 3d gaussian splats from image pairs\nfor scalable generalizable 3d reconstruction, 2023. 2\n[6] Felipe Codevilla, Eder Santana, Antonio M L´opez, and\nAdrien Gaidon.\nExploring the limitations of behavior\ncloning for autonomous driving. In Int. Conf. Comput. Vis.,\npages 9329–9338, 2019. 2\n[7] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-\nmanan. Depth-supervised nerf: Fewer views and faster train-\ning for free.\nIn IEEE Conf. Comput. Vis. Pattern Recog.,\npages 12882–12891, 2022. 2\n[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. In Conf. on Robol Learning, pages 1–16, 2017. 5,\n11\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In Int. Conf. Learn. Represent., 2021. 2, 3, 11\n[10] Danny Driess, Zhiao Huang, Yunzhu Li, Russ Tedrake,\nand Marc Toussaint.\nLearning multi-object dynamics\nwith compositional neural radiance fields.\narXiv preprint\narXiv:2202.11855, 2022. 2, 3\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong\nChen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels:\nRadiance fields without neural networks.\nIn IEEE Conf.\nComput. Vis. Pattern Recog., pages 5501–5510, 2022. 2\n[12] Matthew Hausknecht and Peter Stone.\nDeep recurrent q-\nlearning for partially observable mdps. In AAAI, 2015. 2\n[13] Marti A. Hearst, Susan T Dumais, Edgar Osuna, John Platt,\nand Bernhard Scholkopf. Support vector machines. IEEE In-\ntelligent Systems and their applications, 13(4):18–28, 1998.\n6\n[14] Jeffrey Ichnowski, Yahav Avigal, Justin Kerr, and Ken Gold-\nberg. Dex-nerf: Using a neural radiance field to grasp trans-\nparent objects. arXiv preprint arXiv:2110.14217, 2021. 2\n[15] Boris Ivanovic, Amine Elhafsi, Guy Rosman, Adrien\nGaidon, and Marco Pavone. Mats: An interpretable trajec-\ntory forecasting representation for planning and control. In\nConf. on Robol Learning, 2021. 2\n[16] Justin Kerr, Letian Fu, Huang Huang, Yahav Avigal,\nMatthew Tancik, Jeffrey Ichnowski, Angjoo Kanazawa, and\nKen Goldberg. Evo-nerf: Evolving nerf for sequential robot\ngrasping of transparent objects. In Conf. on Robol Learning,\n2022. 2\n[17] Adam\nR\nKosiorek,\nHeiko\nStrathmann,\nDaniel\nZo-\nran, Pol Moreno, Rosalia Schneider, Sona Mokr´a, and\nDanilo Jimenez Rezende. NeRF-VAE: A geometry aware\n3d scene generative model. pages 5742–5752, 2021. 2, 3, 12\n[18] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit Agrawal,\nand Antonio Torralba. 3d neural scene representations for\nvisuomotor control. In Conf. on Robol Learning, pages 112–\n123, 2022. 3, 12\n[19] Jia-Wei Liu, Yan-Pei Cao, Weijia Mao, Wenqiao Zhang,\nDavid Junhao Zhang, Jussi Keppo, Ying Shan, Xiaohu Qie,\nand Mike Zheng Shou. Devrf: Fast deformable voxel ra-\ndiance fields for dynamic scenes.\nIn Adv. Neural Inform.\nProcess. Syst., pages 36762–36775, 2022. 2\n[20] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and\nDeva Ramanan. Dynamic 3d gaussians: Tracking by per-\nsistent dynamic view synthesis, 2023. 2\n[21] Pierre Marza, Laetitia Matignon, Olivier Simonin, and\nChristian Wolf. Multi-object navigation with dynamically\nlearned neural implicit representations. In Int. Conf. Com-\nput. Vis., pages 11004–11015, 2023. 2\n[22] Rowan McAllister and Carl Edward Rasmussen.\nData-\nefficient reinforcement learning in continuous state-action\ngaussian-pomdps.\nIn Adv. Neural Inform. Process. Syst.,\n2017. 2\n[23] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,\nJonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. In Eur. Conf. Comput. Vis., 2020. 2\n[24] Thomas M¨uller, Alex Evans, Christoph Schied, and Alexan-\nder Keller. Instant neural graphics primitives with a mul-\ntiresolution hash encoding. ACM Trans. Graph., 41(4):1–15,\n2022. 2, 3, 4, 11\n[25] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl-\nlister, Matthew A. Wright, Xin Wang, Jeff He, Sergey\nLevine, and Joseph E. Gonzalez. Is anyone there? learn-\ning a planner contingent on perceptual uncertainty. In Conf.\non Robol Learning, 2022. 2, 5, 11, 12\n[26] Charles Packer, Nicholas Rhinehart, Rowan Thomas McAl-\nlister, Matthew A. Wright, Xin Wang, Jeff He, Sergey\nLevine, and Joseph E. Gonzalez. Is anyone there? learn-\ning a planner contingent on perceptual uncertainty. In Conf.\non Robol Learning, pages 1607–1617, 2023. 2\n[27] Xinlei Pan, Yurong You, Ziyan Wang, and Cewu Lu. Virtual\nto real reinforcement learning for autonomous driving. arXiv\npreprint arXiv:1704.03952, 2017. 2\n[28] Christos H Papadimitriou and John N Tsitsiklis. The com-\nplexity of markov decision processes. Mathematics of oper-\nations research, 12(3):441–450, 1987. 2\n9\n[29] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien\nBouaziz, Dan B Goldman, Steven M Seitz, and Ricardo\nMartin-Brualla. Nerfies: Deformable neural radiance fields.\nIn Int. Conf. Comput. Vis., pages 5865–5874, 2021. 2\n[30] Keunhong Park, Utkarsh Sinha, Peter Hedman, Jonathan T\nBarron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin-\nBrualla, and Steven M Seitz.\nHypernerf:\nA higher-\ndimensional representation for topologically varying neural\nradiance fields. ACM Trans. Graph., 2021. 2\n[31] Joelle Pineau, Geoff Gordon, Sebastian Thrun, et al. Point-\nbased value iteration: An anytime algorithm for pomdps. In\nIJCAI, pages 1025–1032, 2003. 2\n[32] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and\nFrancesc Moreno-Noguer. D-NeRF: Neural Radiance Fields\nfor Dynamic Scenes. In IEEE Conf. Comput. Vis. Pattern\nRecog., 2020. 2\n[33] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali\nFarhadi. You only look once: Unified, real-time object de-\ntection. In IEEE Conf. Comput. Vis. Pattern Recog., 2016.\n5\n[34] Nicholas Rhinehart, Jeff He, Charles Packer, Matthew A\nWright, Rowan McAllister, Joseph E Gonzalez, and Sergey\nLevine.\nContingencies from observations: Tractable con-\ntingency planning with learned behavior models. In IEEE\nInt. Conf. on Robotics and Automation, pages 13663–13669,\n2021. 2\n[35] Barbara Roessle, Jonathan T Barron, Ben Mildenhall,\nPratul P Srinivasan, and Matthias Nießner.\nDense depth\npriors for neural radiance fields from sparse input views.\nIn IEEE Conf. Comput. Vis. Pattern Recog., pages 12892–\n12901, 2022. 2\n[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and\nLi Fei-Fei. Imagenet large scale visual recognition challenge,\n2015. 11\n[37] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,\nHongwen Zhang, and Yebin Liu. Tensor4d: Efficient neural\n4d decomposition for high-fidelity dynamic reconstruction\nand rendering. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 16632–16642, 2023. 2\n[38] Cheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel\ngrid optimization: Super-fast convergence for radiance fields\nreconstruction. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 5459–5469, 2022. 2\n[39] Matthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara\nFridovich-Keil, Nithin Raghavan, Utkarsh Singhal, Ravi Ra-\nmamoorthi, Jonathan Barron, and Ren Ng. Fourier features\nlet networks learn high frequency functions in low dimen-\nsional domains. In Adv. Neural Inform. Process. Syst., pages\n7537–7547, 2020. 2\n[40] Matthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Prad-\nhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron,\nand Henrik Kretzschmar. Block-nerf: Scalable large scene\nneural view synthesis. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 8248–8258, 2022. 2\n[41] Omer Sahin Tas and Christoph Stiller. Limited visibility and\nuncertainty aware motion planning for automated driving. In\nIEEE Intelligent Vehicles Symposium (IV), 2018. 1\n[42] A. Tewari, J. Thies, B. Mildenhall, P. Srinivasan, E. Tretschk,\nW. Yifan, C. Lassner, V. Sitzmann, R. Martin-Brualla, S.\nLombardi, T. Simon, C. Theobalt, M. Nießner, J. T. Barron,\nG. Wetzstein, M. Zollh¨ofer, and V. Golyanik. Advances in\nNeural Rendering. Comput. Graph. Forum, 2022. 2\n[43] Marin Toromanoff, Emilie Wirbel, and Fabien Moutarde.\nEnd-to-end model-free reinforcement learning for urban\ndriving using implicit affordances. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 7153–7162, 2020. 2\n[44] Edith Tretschk, Vladislav Golyanik, Michael Zollhoefer,\nAljaz Bozic, Christoph Lassner, and Christian Theobalt.\nScenerflow: Time-consistent reconstruction of general dy-\nnamic scenes.\nIn International Conference on 3D Vision\n(3DV), 2023. 2\n[45] Prune Truong, Marie-Julie Rakotosaona, Fabian Manhardt,\nand Federico Tombari. Sparf: Neural radiance fields from\nsparse and noisy poses. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 4190–4200, 2023. 2\n[46] Haithem Turki,\nDeva Ramanan,\nand Mahadev Satya-\nnarayanan. Mega-nerf: Scalable construction of large-scale\nnerfs for virtual fly-throughs. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 12922–12931, 2022. 2\n[47] Laurens van der Maaten and Geoffrey Hinton. Visualizing\ndata using t-SNE. Journal of Machine Learning Research,\n9:2579–2605, 2008. 6\n[48] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P\nSrinivasan, Howard Zhou, Jonathan T Barron, Ricardo\nMartin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibr-\nnet: Learning multi-view image-based rendering. In IEEE\nConf. Comput. Vis. Pattern Recog., pages 4690–4699, 2021.\n2\n[49] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu,\nKalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-\nbased neural radiance fields. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 5438–5448, 2022. 2\n[50] Lin Yen-Chen, Pete Florence, Jonathan T Barron, Alberto\nRodriguez, Phillip Isola, and Tsung-Yi Lin. inerf: Invert-\ning neural radiance fields for pose estimation. In IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1323–1330. IEEE, 2021. 2\n[51] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.\npixelnerf: Neural radiance fields from one or few images,\n2021. 2\n10\nA. CARLA Datasets\nA complete figure of the actor and ego configurations across\nscenes and the progression of timestamps for the Single-\nScene Approaching Intersection, Multi-Scene Approaching\nIntersection, and the Multi-Scene Two Lane Merge is visu-\nalized in Fig. 14.\nB. Related Work Comparison\nIn this section, we draw a comparison between CARFF\nand other methods that perform tasks similar to our model.\nHowever, these methods do not precisely align with the ob-\njectives we aim to achieve. For instance, while some meth-\nods integrate 3D reasoning, others may omit this aspect. To\nestablish a fair comparison between our model and current\nmethods, we have conducted an in-depth analysis of their\nqualitative differences, as delineated in Tab. 4. We primar-\nily compare to other NeRF works based on their ability to\nmodel uncertainty (state and dynamics) and perform fore-\ncasting in the environment. Our work surpasses all the listed\nprevious works and is on par with 2D-based forecasting ap-\nproaches in functionality [25]. This comparison highlights\nthat our model comprehensively encompasses the key qual-\nitative factors that should be present to reason from the oc-\ncluded as humans do.\nC. Implementation Details\nC.1. Pose-Conditional VAE\nArchitecture:\nWe implement PC-VAE on top of a stan-\ndard PyTorch VAE framework. The encoder with convo-\nlutional layers is replaced with a single convolutional layer\nand a Vision Transformer (ViT) Large 16 [9] pre-trained on\nImageNet [36]. We modify fully connected layers to project\nViT output of size 1000 to mean and variances with size of\nthe latent dimension, 8. During training, the data loader re-\nturns the pose of the camera angle represented by an integer\nvalue. This value is one-hot encoded and concatenated to\nthe re-parameterized encoder outputs, before being passed\nto the decoder. The decoder input size is increased to add\nthe number of poses to accommodate the additional pose\ninformation.\nOptimization:\nWe utilize a single RTX 3090 graphics\ncard for all our experiments. The PC-VAE model takes ap-\nproximately 22 hours to converge using this GPU. During\nthis phase, we tune various hyperparameters including the\nlatent size, learning rate and KL divergence loss weight to\nestablish optimal training tailored to our model (see Tab. 5).\nIn order to optimize for the varied actor configurations and\nscenarios generated within the CARLA [8] simulator, we\nslightly adjust hyperparameters differently for each dataset.\nThe learning rate (LR) and KL divergence (KLD) weight\nare adjusted to find an appropriate balance between the\neffective reconstruction of pose conditioning in the latent\nspace, and the regularization of latents.\nRegularization\npushes the latents toward Gaussian distributions and keeps\nthe non-expressive latents in an over-parameterized latent\nspace to be standard normal. This stabilizes the sampling\nprocess and ensures stochastic behavior of latent samples in\ncase of occlusion. To achieve this balance, we use a lin-\near KLD weight scheduler, where the weight is initialized\nat a low value for KLD increment start epoch (see Tab. 5).\nThis allows the model to initially focus on achieving highly\naccurate conditioned reconstructions. The KLD weight is\nthen steadily increased until KLD increment end epoch is\nreached, ensuring probabilistic behavior under partial ob-\nservability.\nC.2. Mixture Density Network\nThe mixture density network (MDN) takes in the mean and\nvariances of the latent distributions qϕ(zt−1|It−1\nc\n) and out-\nputs the estimated posterior distribution as a mixture of\nGaussian q′\nϕ(zt|It−1\nc\n) through a multi-headed MLP.\nArchitecture:\nThe shared backbone simply contains 2\nfully connected layers and rectified linear units (ReLU) ac-\ntivation with hidden layer size of 512. Additional heads\nwith 2 fully connected layers are used to generate µi and\nσ2\ni . The mixture weight, πi, is generated from a 3 layer\nMLP network. We limit the number of Gaussians, K = 2.\nOptimization:\nWe train our network for 30, 000 epochs\nusing the batch size of 128 and an initial LR of 0.005, and\napply LR decay to optimize training. This takes approxi-\nmately 30 minutes to train utilizing the GPU. During train-\ning, the dataloader outputs the means and variances at the\ncurrent timestamp and indexed view, and the means and\nvariances for the next timestamp, at a randomly sampled\nneighboring view. This allows the MDN to learn how oc-\ncluded views advance into all the possible configurations\nfrom potentially unoccluded neighboring views, as a mix-\nture of Gaussian.\nAt each iteration, the negative log-likelihood loss is com-\nputed for 1000 samples drawn from the predicted mixture\nof distributions q′\nϕ(zt|It−1\nc\n) with respect to the ground truth\ndistribution qϕ(zt|It\nc). While the MDN is training, addi-\ntional Gaussian noise, given by ϵ ∼ N(0, σ2), is added to\nthe means and variances of the current timestamp t − 1,\nwhere σ ∈ [0.001, 0.01]. The Gaussian noise and LR decay\nhelp prevent overfitting and reduce model sensitivity to en-\nvironmental artifacts like moving trees, moving water, etc.\nC.3. NeRF\nArchitecture:\nWe implement our NeRF decoder uti-\nlizing an existing PyTorch implementation of Instant-\nNGP [24]. We concatenate the latents to the inputs of two\nparts of the Instant-NGP architecture: the volume density\n11\nMethod\n3D\nRealistic\nApplication\nState\nUncertainty\nDynamics\nUncertainty Prediction\nPlanning\nCode\nReleased\nCARFF\n✓\n✓\n✓\n✓\n✓\n✓\n✓\nNeRF-VAE [17]\n✓\n✓\nNeRF\nfor\nVisuomotor\nCon-\ntrol [18]\n✓\n✓\n✓\n✓\nNeRF Navigation [1]\n✓\n✓\n✓\n✓\nAVP [25]\n✓\n✓\n✓\n✓\n✓\nTable 4. Qualitative comparison of CARFF to related works. CARFF accomplishes all the highlighted objectives as opposed to the\nsimilar works listed. A check mark indicates that the associated method incorporates the qualitative feature in each column, whereas empty\nspaces indicate that the method does not account for it. Here, 3D refers to methods that reason in a 3D environment and perform novel\nview synthesis. Realistic application refers to whether the method has been demonstrated in realistic and complex scenarios. State and\ndynamic uncertainty refer to whether the model predicts probabilistically under these conditions. Prediction refers to forecasting into the\nfuture, while planning refers to using model predictions for decision-making.\nPose     Inputs\nPC- VAE Decoded Images From Set of New Pose\nFigure 9. PC-VAE encoder inputs, ground truth timestamps, and reconstructions. The encoder input, It\nc, among the other ground\ntruth images Ic viewed from camera pose c at different timestamps, is reconstructed across a new set of poses c′′ respecting timestamp t,\ngenerating It\nc′′. This is a full grid of the reconstructions.\n12\nFigure 10. NeRF graphical user interface. The GUI allows us to toggle and predict with an input image path. The probe and predict\nfunction probes the current location of the car and predicts the next. The screenshot is sharpened for visual clarity in the paper.\nPC-VAE Hyperparameters\nLatent Size\n8\nLR\n0.004\nKLD Weight Start\n0.000001\nKLD Weight End\n0.00001 − 0.00004*\nKLD Increment Start\n50 epochs\nKLD Increment End\n80 epochs\nTable 5. PC-VAE experimental setup and hyperparameters.\nThe main hyperparameters in PC-VAE training on the three\ndatasets are latent size, LR, and KLD weight. For KLD schedul-\ning, the KLD increment start refers to the number of epochs at\nwhich the KLD weight begins to increase from the initial KLD\nweight. KLD increment end is the number of epochs at which the\nKLD weight stops increasing at the maximum KLD weight. The\nasterisk (*) marks the hyperparameter that is dataset-dependent.\nnetwork, σ(x), for the density values, and the color net-\nwork, C(r), for conditional RGB generation. While the\noverall architecture is kept constant, the input dimensions\nof each network are modified to allow additional latent con-\ncatenation.\nOptimization:\nEmpirically, we observe that it is essen-\ntial to train the NeRF such that it learns the distribution of\nscenes within the PC-VAE latent space. Using only pre-\ndefined learned samples to train may run the risk of relying\non non-representative samples. On the other hand, direct re-\nsampling during each training iteration in Instant-NGP may\nlead to delayed training progress, due to NeRF’s sensitive\noptimization. In our optimization procedure, we use an LR\nof 0.002 along with an LR decay and start with pre-defined\nlatent samples. Then we slowly introduce the re-sampled\nlatents. We believe that this strategy progressively dimin-\nishes the influence of a single sample, while maintaining\nefficient training. Based on our observations, this strategy\ncontributes towards Instant-NGP’s ability to rapidly assim-\nilate fundamental conditioning and environmental recon-\nstruction, while simultaneously pushing the learning pro-\ncess to be less skewed towards a single latent sample.\nD. GUI Interface\nFor ease of interaction with our inference pipeline, our\nNeRF loads a pre-trained MDN checkpoint, and we build\na graphical user interface (GUI) using DearPyGUi for vi-\nsualization purposes. We implement three features in the\nGUI: (a) predict, (b) probe and predict, and (c) toggle.\nPredict:\nWe implement the function to perform predic-\ntion directly from a given image path in the GUI. We use\nthe distribution qϕ(zt−1|It−1\nc\n) from PC-VAE encoder, cor-\nresponding to the input image It−1\nc\n, to predict the latent\ndistribution for the next timestamp q′\nϕ(zt|It−1\nc\n). This pro-\ncess is done on the fly through the MDN. A sample from\nthe predicted distribution is then generated and used to con-\ndition the NeRF. This advances the entire scene to the next\ntimestamp.\nProbe and predict:\nThe sampled latent from the pre-\ndicted distribution does not correspond to a singular distri-\nbution and hence we can not directly predict the next times-\n13\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(a) Single-Scene Approaching Intersection\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(b) Multi-Scene Approaching Intersection\n0\n20\n40\n60\n80 100 120 140\n0\n5\n10\n15\n20\n25\nEpochs\nAverage PSNR\n(c) Multi-Scene Two Lane Merge\nFigure 11. Average train PSNR plot for all CARLA datasets.\nThe plot shows the increase in average training PSNR of all images\nfor each dataset, over the period of the training process.\ntamp. To make our model auto-regressive in nature, we per-\nform density probing. We probe the density of the NeRF\nat the possible location coordinates of the car to obtain the\ncurrent timestamp and scene. This is then used to match the\nlatent to a corresponding distribution in the PC-VAE space.\nThe new distribution enables auto-regressive predictions us-\ning the predict function described above.\nToggle:\nThe NeRF generates a scene corresponding to the\nprovided input image path using learned latents from PC-\nVAE. When the input image is a fully observable view, the\nNeRF renders clear actor and ego configurations respecting\nthe input. This allows us to visualize the scene at different\ntimestamps and in different configurations.\nFigure 12. Latent sample distribution clustering. The distri-\nbutions of latent samples for the Multi-Scene Two Lane Merge\ndataset are separable through t-SNE clustering. In the figure, the\nclusters for Scene 0, Timestamp 0 and Scene 1, Timestamp 0 over-\nlap in distribution because they represent the same initial state of\nthe environment under dynamics uncertainty.\nEncoder Architectures\nTrain\nPSNR\nSVM\nAccuracy\nNV\nPSNR\nMulti-Scene Approaching Intersection\nPC-VAE\n26.47\n89.17\n26.37\nPC-VAE w/o CL\n26.20\n83.83\n26.16\nVanilla PC-VAE\n25.97\n29.33\n25.93\nPC-VAE w/o Freezing\n24.82\n29.83\n24.78\nPC-VAE w/ MobileNet\n19.37\n29.50\n19.43\nVanilla VAE\n26.04\n14.67\n9.84\nMulti-Scene Two Lane Merge\nPC-VAE\n25.50\n88.33\n25.84\nPC-VAE w/o CL\n24.38\n29.67\n24.02\nVanilla PC-VAE\n24.75\n29.67\n24.96\nPC-VAE w/o Freezing\n23.97\n28.33\n24.04\nPC-VAE w/ MobileNet\n17.70\n75.00\n17.65\nVanilla VAE\n25.11\n28.17\n8.49\nTable 6. PC-VAE metrics and ablations across Multi-Scene\ndatasets. CARFF’s PC-VAE outperforms other encoder architec-\ntures across the Multi-Scene datasets in reconstruction and pose-\nconditioning.\nE. CARFF Evaluation\nE.1. Pose-Conditional VAE\nReconstruction Quality:\nTo analyze the reconstruction\nperformance of the model during training, we periodically\nplot grids of reconstructed images.\nThese grids consist\nof (a) randomly selected encoder inputs drawn from the\ndataset, (b) the corresponding ground truth images for those\ninputs at each timestamp at the same camera pose, and (c)\nreconstructed outputs at randomly sampled poses respecting\n14\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(a) Approaching Intersection\n0\n10\n20\n30\n40\n50\n0.4\n0.6\n0.8\n1\nNum Samples\n%\nAccuracy\nRecall\n(b) Two Lane Merge\nFigure 13.\nMulti-Scene dataset accuracy and recall curves\nfrom learned latents. We test our framework across n = 1 and\nn = 50 samples from PC-VAE’s latent distributions from ego-\ncentric image input. Across the number of samples n, we achieve\nan ideal margin of belief state coverage generated under partial\nobservation (recall), and the proportion of correct beliefs sampled\nunder full observation (accuracy) for the MDN to learn. As we\nsignificantly increase the number of samples, the accuracy starts\nto decrease due to randomness in latent distribution resampling.\nthe input scene and timestamp. An example reconstruction\ngrid is provided in Fig. 9. The grid enables visual assess-\nment of whether the model is capable of accurately recon-\nstructing reasonable images using the encoder inputs, con-\nditioned on the poses. This evaluation provides us with vi-\nsual evidence of improvement in reconstruction quality. We\nalso quantitatively analyze the progressive improvement of\nreconstruction through the average PSNR calculated over\nthe training data (see Fig. 11).\nLatent Space Analysis\nTo assess the quality of the la-\ntents generated by PC-VAE, we initially use t-SNE plots to\nvisualize the latent distributions as clusters. Fig. 12 shows\nthat the distributions of the latent samples for the Multi-\nScene Two Lane Merge dataset are separable. While t-SNE\nis good at retaining nearest-neighbor information by pre-\nserving local structures, it performs weakly in preserving\nglobal structures. Therefore, t-SNE may be insufficient in\ncapturing the differences in distributions for all our datasets.\nInstead, we pivot to Support Vector Machine to perform\na quantitative evaluation of the separability of the latents.\nWe utilize a Radial Basis Function (RBF) kernel with the\nstandard regularization parameter (C = 1). We perform\n10-fold validation on the latents to calculate the accuracy as\na metric for clustering. See Tab. 6 for the results.\nBeyond separability, we analyze the recall and accuracy\nof the learned latents directly from PC-VAE under par-\ntial and full observations. This achieves very high accu-\nracy even under a large number of samples while retrain-\ning decent recall, enabling downstream MDN training. (See\nFig. 13)\nE.2. Fully Observable Predictions\nOne of the tasks of the MDN is to forecast the future scene\nconfigurations under full observation.\nWe quantitatively\nevaluate our model’s ability to forecast future scenes by\ncomparing bird’s-eye views rendered from the NeRF with\nchosen ground truth images of the scene for the various\ntimestamps (see Tab. 7). The values are calculated and dis-\nplayed for all three datasets. In Tab. 7, images are marked\nas either toggled (˜Iti) or predicted (ˆIti). Toggled images\nin the table cannot be predicted deterministically due to it\nbeing the first timestamp in the dataset, or the state of the\nprevious timestamps across scenes being the same in case\nof dynamics uncertainty. Due to the same reason, in the\nMulti-Scene Two Lane Merge Dataset, there are additional\nbolded PSNR values for the pairs (It1, ˜It4) and (It4, ˜It1).\n15\nSingle-Scene Approaching Intersection\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\nIt7\nIt8\nIt9\nIt10\n˜It1\n29.01\n−5.97\n−6.08\n−6.52\n−6.44\n−6.03\n−6.31\n−6.36\n−6.26\n−6.28\nˆIt2\n−5.42\n27.51\n−3.07\n−4.67\n−4.58\n−4.17\n−4.43\n−4.51\n−4.39\n−4.39\nˆIt3\n−6.06\n−2.81\n28.12\n−4.47\n−4.68\n−4.19\n−4.05\n−4.61\n−4.47\n−4.52\nˆIt4\n−7.01\n−5.37\n−5.03\n29.40\n−4.99\n−5.08\n−5.03\n−5.41\n−5.28\n−5.32\nˆIt5\n−6.87\n−5.2\n−4.93\n−5.00\n29.44\n−4.53\n−4.46\n−5.19\n−5.05\n−5.09\nˆIt6\n−6.29\n−4.55\n−4.27\n−4.8\n−4.24\n29.02\n−4.02\n−4.53\n−4.38\n−4.44\nˆIt7\n−6.76\n−5.05\n−4.76\n−5.31\n−5.14\n−4.36\n29.50\n−4.50\n−4.86\n−4.93\nˆIt8\n−6.73\n−5.02\n−4.74\n−5.25\n−5.10\n−4.64\n−4.76\n29.46\n−4.41\n−4.86\nˆIt9\n−6.75\n−5.00\n−4.70\n−5.23\n−5.07\n−4.64\n−4.85\n−4.52\n29.55\n−4.42\nˆIt10\n−6.79\n−5.06\n−4.75\n−5.30\n−5.15\n−4.69\n−4.93\n−5.01\n−4.34\n29.55\nMulti-Scene Approaching Intersection\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\n˜It1\n28.10\n−5.24\n−5.50\n−1.67\n−3.29\n−3.92\nˆIt2\n−5.23\n28.02\n−6.11\n−4.70\n−3.21\n−4.84\nˆIt3\n−5.43\n−6.03\n27.97\n−4.85\n−4.53\n−2.93\n˜It4\n−1.71\n−4.73\n−5.00\n28.26\n−2.25\n−3.08\n˜It5\n−3.68\n−3.24\n−4.91\n−2.76\n28.21\n−2.99\nˆIt6\n−4.02\n−4.91\n−3.27\n−3.13\n−2.61\n28.26\nMulti-Scene Two Lane Merge\nResult\nIt1\nIt2\nIt3\nIt4\nIt5\nIt6\n˜It1\n28.27\n−5.31\n−6.41\n28.23\n−4.77\n−5.42\n˜It2\n−5.22\n28.23\n−5.17\n−5.27\n−2.91\n−4.01\nˆIt3\n−6.32\n−5.09\n28.14\n−6.33\n−5.01\n−4.28\n˜It4\n28.27\n−5.27\n−6.37\n28.23\n−4.72\n−5.37\n˜It5\n−4.64\n−2.73\n−5.01\n−4.71\n28.08\n−5.29\nˆIt6\n−5.32\n−4.02\n−4.32\n−5.33\n−5.34\n28.17\nTable 7. Complete PSNR values for fully observable predictions for all CARLA datasets. The table contains PSNR values between\nthe ground truth images and either a toggled image (marked as ˜Iti), or a predicted image (marked as ˆIti). Toggled or predicted images that\ncorrespond to the correct ground truth are bolded and have a significantly higher PSNR value. The PSNR values for incorrect correspon-\ndances are replaced with the difference between the incorrect PSNR and the bolded PSNR associated with a correct correspondance.\n16\nScene 1: Ego car with \nactor ambulance\nScene 2: Ego car only\nScene 2: Ego car with \nfast- moving ambulance\nScene 1: Ego car with \nslow- moving ambulance\nMulti- Scene Approaching Intersection\nMulti- Scene Two Lane Merge\nSingle- Scene Approaching Intersection\nTimestamp 0: Actor \nambulance is about to \ncross intersection\nTimestamp 1\nTimestamp 9: Actor has \ncrossed the intersection\nTimestamp 2\nTimestamp 3\nTimestamp 6\nTimestamp 4\nTimestamp 5\nTimestamp 7\nTimestamp 8\nTimestamp 3\nTimestamp 4\nTimestamp 5\nTimestamp 0\nTimestamp 1\nTimestamp 2\nTimestamp 0\nTimestamp 1\nTimestamp 2\nTimestamp 3\nTimestamp 4\nTimestamp 5\nFigure 14. Single-Scene Approaching Intersection, Multi-Scene Approaching Intersection, and Multi-Scene Two Lane Merge Datasets. The actor and ego car configurations\nfor the timestamps and scenes of the three CARLA datasets are visualized at a single camera pose. The colors of the cars for the Multi-Scene Approaching Intersection have been\nslightly modified for greater contrast and visual clarity in the paper.\n17\n"
}