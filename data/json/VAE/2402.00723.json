{
    "optim": "Improving Semantic Control in Discrete Latent Spaces with\nTransformer Quantized Variational Autoencoders\nYingji Zhang1†, Danilo S. Carvalho1,3, Marco Valentino2,\nIan Pratt-Hartmann1, André Freitas1,2,3\n1 Department of Computer Science, University of Manchester, United Kingdom\n2 Idiap Research Institute, Switzerland\n3 National Biomarker Centre, CRUK-MI, Univ. of Manchester, United Kingdom\n1{firstname.lastname}@[postgrad.]†manchester.ac.uk\n2{firstname.lastname}@idiap.ch\nAbstract\nAchieving precise semantic control over the\nlatent spaces of Variational AutoEncoders\n(VAEs) holds significant value for downstream\ntasks in NLP as the underlying generative mech-\nanisms could be better localised, explained and\nimproved upon. Recent research, however, has\nstruggled to achieve consistent results, primar-\nily due to the inevitable loss of semantic infor-\nmation in the variational bottleneck and limited\ncontrol over the decoding mechanism. To over-\ncome these challenges, we investigate discrete\nlatent spaces in Vector Quantized Variational\nAutoEncoders (VQVAEs) to improve semantic\ncontrol and generation in Transformer-based\nVAEs. In particular, We propose T5VQVAE, a\nnovel model that leverages the controllability of\nVQVAEs to guide the self-attention mechanism\nin T5 at the token-level, exploiting its full gener-\nalization capabilities. Experimental results indi-\ncate that T5VQVAE outperforms existing state-\nof-the-art VAE models, including Optimus, in\nterms of controllability and preservation of se-\nmantic information across different tasks such\nas auto-encoding of sentences and mathemat-\nical expressions, text transfer, and inference.\nMoreover, T5VQVAE exhibits improved infer-\nence capabilities, suggesting potential appli-\ncations for downstream natural language and\nsymbolic reasoning tasks.\n1\nIntroduction\nThe emergence of deep generative neural networks\nsupported by Variational AutoEncoders (VAEs)\n(Kingma and Welling, 2013) enables the locali-\nsation of syntactic and semantic properties within\ncomplex sentence latent spaces. By localising and\nmanipulating these generative factors within the\nlatent spaces, one can better control the properties\nof the textual output, enhancing performance on\ndownstream tasks (Carvalho et al., 2023; John et al.,\n2019a), and providing mechanisms for representing\nand disentangling syntactic and semantic features\nAnimals require warmth in\ncold environment\nx\nL2\nindex\n...\n10\nx\n43\nargmin lookup\nVQ\nindex\nindex\n...\n...\n89\nlatent token space\nindex 123\ni\n...\nE(x)\nAnimals require warmth in\ncold environment\nDiscrete Space\nD(z)\nTask1: Language Modelling\nCan it understand data ?\nTask2: Text Transfer Task\nCan it deliver controlled generation?\nTask3: Inference Task\nCan it deliver inference control ?\nAnimal require warmth...\nAnimal need warmth...\nAnimal must find warmth...\nAnimal bring warmth ... \nLatent Traversal:\nLatent Arithmetic: xA + xB\nData: scientific explanations,\nMath expression\nBaselines: Optimus, Della,\nLSTM-base VAEs.\nMetrics: BLEU, BLEURT,\nCosine, Loss, PPL.\nLatent Interpolation: interpolation\nsmoothness (IS) metric\nData: explanatory inference, math\ninference\nBaselines: Optimus, Della, and\nTransformers (T5, Bart, etc.).\nMetrics: BLEU, BLEURT, etc.\nSymbolic Evaluation: operate\nvector to provide semantic control.\nTraining T5VQVAE:  \nT5VQVAE architecture:  \nTraining Latent Spaces:  \nSoft: k-mean, hard: Gumbel Softmax, Stable training: Exponential Moving Average (EMA) \nFigure 1: By controlling the token-level discrete latent\nspace in VAEs, we aim to explicitly guide the cross-\nattention mechanism in T5 to improve the generation\nprocess. We focus on three challenging tasks to assess\nprecise semantic control and inference.\nwithin natural language (Zhang et al., 2023a, 2022;\nMercatali and Freitas, 2021).\nRecent work (Carvalho et al., 2023; Zhang et al.,\n2022, 2023a) investigated controllable text gener-\nation via latent sentence geometry based on the\ncanonical Optimus architecture (the first large pre-\ntrained language VAE, Li et al. (2020)). However,\nthe Optimus architecture brings its associated chal-\nlenges since (i) the Optimus setup does not allow\nfor a fine-grained (i.e., token-level) semantic con-\ntrol as sentence-level representation features are\nignored by most attention heads especially in lower\nlayers, where lexical-level semantics is captured\n(Hu et al., 2022); (ii) the sentence bottleneck in the\nVAE architecture leads to inevitable information\nloss during inference (Zhang et al., 2023b,d).\nThis work concentrates on addressing these ar-\nchitectural limitations by aiming to minimise the\ninformation loss in the latent space and effectively\ncontrol the decoder and its attention mechanism.\narXiv:2402.00723v1  [cs.CL]  1 Feb 2024\nThe Vector Quantized Variational AutoEncoder\n(VQVAE) (Van Den Oord et al., 2017), as a discrete\nlatent variable model, can be considered an ideal\nmechanism to alleviate these issues since it pre-\nserves and closely integrates both a coarse-grained\ncontinuous latent sentence space and a fine-grained\nlatent token space that can preventinformation loss.\nMore importantly, its latent token space can directly\nwork on the cross-attention module (Vaswani et al.,\n2017) to guide the generation in seq2seq models,\nsuch as T5 (Raffel et al., 2020). Therefore, we hy-\npothesise that such a mechanism can enable better\ngeneralisation and semantic control in Transformer-\nbased VAEs.\nFollowing these insights, we propose a novel\napproach named T5VQVAE, a model that lever-\nages the controllability of VQVAE to guide the\ntoken-level self-attention mechanism during the\ngeneration process. We evaluate T5VQVAE on\nthree challenging and diverse downstream tasks in-\ncluding (1) language modelling, (2) text transfer\n(guided text generation via the movement of latent\nvectors), and (3) natural language and symbolic\ninference tasks. An illustration of the complete\nmodel architecture and experimental setup can be\nfound in Figure 1.\nThe overall contribution of the paper can be sum-\nmarised as follows:\n1. We propose T5VQVAE, the first pre-trained\nlanguage Vector-Quantised variational Au-\ntoencoder, bridging the gap between VAEs\nand token-level representations, improving\nsentence-level localisation, controllability,\nand generalisation under VAE architectures.\nThe experiments reveal that the proposed\nmodel outperforms previous state-of-the-art\nVAE models, including Optimus (Li et al.,\n2020), on three target tasks, as well as deliv-\nering improved semantic control when com-\npared to the previous state-of-the-art.\n2. We propose the Interpolation Smoothness\n(IS) metric for quantitatively evaluating sen-\ntence interpolation performance, a fundamen-\ntal proxy for measuring the localisation of syn-\ntactic and semantic properties within sentence\nlatent spaces. The experimental results indi-\ncate that T5VQVAE can lead to better interpo-\nlation paths (suggesting better interpretability\nand control).\n3. Experiments on syllogistic-deductive NLI and\nmathematical expression derivation reveal that\na quasi-symbolic behaviour may emerge in\nthe latent space of T5VQVAE, and that the\nmodel can be explicitly controlled to achieve\nsuperior reasoning capabilities.\nOur experimental code is available online1 to en-\ncourage future work in the field.\n2\nMethodology\nIn this section, we first present our model,\nT5VQVAE, whose primary goal is to learn a la-\ntent space by reconstructing input sentences. Next,\nwe illustrate its objective function, which consists\nof three parts designed to improve semantic control:\nreconstruction term, latent space optimization term,\nand encoder constraint term. Finally, we highlight\nthe architectural advantages of T5VQVAE com-\npared to Transformer-based VAEs.\nModel architecture.\nVan Den Oord et al. (2017)\nfirst proposed the VQVAE architecture for learn-\ning a discretised latent space of images, showing\nthat it can alleviate the issue of posterior collapse,\nin which the latent representations produced by\nthe Encoder are ignored by the Decoder (Kingma\nand Welling, 2013). In this work, we propose to\nintegrate T5 encoder/decoder into the VQVAE ar-\nchitecture for representation learning with natural\nlanguage. T5 was selected due to its consistent\nperformance across a large range of NLP tasks\nand its accessibility. To cast T5 into a VQVAE\nmodel, we first establish a latent token embedding\nspace, denoted as the codebook, represented by\nz ∈ RK×I. Here, K refers to the number of tokens\nin the codebook, and I represents the dimensional-\nity of each token embedding. When given a token\nx, the Encoder E maps it into a vector represen-\ntation, denoted as E(x). Then, the nearest latent\nrepresentation zk from the codebook z is selected\nbased on the L2 distance. The input of the cross-\nattention module can then be formalised as follows:\nˆx = MultiHead\n\u0010\nD(x)W q, zkW k, zkW v\u0011\nHere, zk is the key and value and D(x), which rep-\nresents the input token embedding of the decoder,\nis the query. ˆx represents the reconstructed token,\nwhile W q, W k, and W v are trainable weights of\nquery, key, and value.\n1https://github.com/SnowYJ/T5VQVAE\nTraining T5VQVAE\nThe training of T5VQVAE\ncan be then considered as the optimisation of three\nindependent parts, including D(zk), zk, and E(x).\nStarting from D, the model can be trained by max-\nimising the reconstruction probability P(x|D(zk))\nvia the teach-forcing scheme. Next, the zk is opti-\nmised by minimising the L2 distance between E(x)\nand zk, which can be described as (sg[E(x)]−zk)2\nwhere sg is the stop gradient operation. Finally,\nE(x) can be trained via the L2 distance. By ensur-\ning that E(x) can learn the latent embedding under\nthe constraint of RK×I rather than learning an em-\nbedding directly, we can guide the model to achieve\nbetter performance. A commitment weight β < 1\nis used to constraint the E close to zk, which can\nbe described as: β(E(x)−sg[zk])2. β is set to 0.25\nfollowing the same setup as (Van Den Oord et al.,\n2017) to preserve a behaviour consistent with their\nfindings. The final objective function of T5VQVAE\ncan be formalised as follows:\nLV QV AE =\nP(x|D(zk))\n|\n{z\n}\n(1)reconstruction\n+ (sg[E(x)] − zk)2\n|\n{z\n}\n(2)LatentSpace\n+\nβ (E(x) − sg[zk])2\n|\n{z\n}\n(3)LatentSpaceConstraint\nTraining the latent space.\nThere are two possi-\nble strategies to update the latent space: i. k-means\nand ii. Gumbel softmax. Regarding k-means, for\neach token embedding wi in a sentence, it selects\nthe nearest latent token embedding, zk, to its to-\nken embedding ewi. This process is equivalent to\nclassifying ewi using k-means and then choosing\nthe corresponding central point zk as the input for\nD(zk). This can be expressed as follows:\nzwi = zk, where k = argminj\n\r\rewi − zj\r\r\n2\nTo improve the stability of latent space training\n(term 2), we adapted the Exponential Moving Av-\nerage (EMA) training scheme to update z (Roy\net al., 2018). Figure 2 displays the training and\ntesting loss curves of T5VQVAE with EMA or not.\nMore details of EMA are provided in Appendix A.\nInstead of using k-means, which performs a soft\nselection of the index k, we can utilize the Gumbel\nsoftmax trick (Jang et al., 2016) for a hard sam-\npling of the index k. This trick involves sampling\na noise value gk from the Gumbel distribution and\nthen using the softmax function to normalize the\noutput, resulting in a probability distribution. By\nselecting the index with the highest probability, we\nFigure 2: Loss curves of T5VQVAEs (base) with and\nwithout EMA and Optimus on the WorldTree corpus.\nobtain a discrete choice. This entire process can be\ndescribed as follows:\nzwi = zk, where\nk = argmaxk\nexp(log(tk) + gk)/τ\nPK\nk=1 exp(log(tk) + gk)/τ\nIn this context, tk represents the probability of the\nk-th token, which can be obtained through a linear\ntransformation before being fed into the Gumbel\nsoftmax. The parameter τ serves as a temperature\nhyper-parameter that controls the closeness of the\nnew distribution to a discrete distribution. As τ\napproaches zero, the distribution becomes one-hot,\nwhile a non-zero value of τ leads to a more uniform\ndistribution. In our experiments, we experienced\nconvergence issues when using the Gumbel soft-\nmax scheme, and therefore decided to adopt the\nk-means mechanism which generally leads to bet-\nter results.\nAdvantages of T5VQVAE.\nCompared with\nstate-of-the-art Transformer VAEs such as Optimus\n(Li et al., 2020), our model has the following archi-\ntectural advantages: (i) efficient and stable latent\nspace compression. During the training of Opti-\nmus, in fact, the KL term in ELBO is regularized\ncyclically (Fu et al., 2019) to avoid KL vanishing\nand posterior collapse, which leads to an unstable\ntraining process (figure 2). In contrast, T5VQVAE\navoid the KL regularization term since it becomes\na constant value:\nKL (q(zk|x)||p(zk)) =\nX\nk\nq(zk|x) log q(zk|x)\np(z)\n= 1 × log\n1\n1/K = log K\nwhere the prior p(z) = 1/K is a uniform distri-\nbution. (ii) Better controllability. Hu et al. (2022)\nrevealed that in Optimus (Li et al., 2020), the la-\ntent representation is concatenated into key and\nvalue which is more likely to be ignored by most\nattention heads especially in lower layers where\nlexical-level semantics is captured. In contrast, the\nlatent representations of T5VQVAE are designed\nto act on the attention heads directly.\n3\nControllability Evaluation\nNext, we put forward two metrics for quantitatively\nevaluating the controllability of the proposed model\n(T5VQVAE), which we refer to as semantic disen-\ntanglement and interpolation smoothness. The for-\nmer evaluates the controllability from the perspec-\ntive of disentanglement of semantic factors (e.g.,\narguments and associated semantic roles). The lat-\nter evaluates the smoothness and coherence of the\nlatent space geometry during interpolation.\n3.1\nSemantic Disentanglement\nRecent studies have attempted to adapt metrics\nfrom the image domain to evaluate the semantic\ndisentanglement of sentences (Zhang et al., 2022;\nCarvalho et al., 2023). Semantic information in a\nsentence is more likely to be entangled, especially\nin the context of stacked multi-head self-attention\nmodels. As mentioned in (Zhang et al., 2022; Car-\nvalho et al., 2023), conceptually dense sentences\nare clustered according to role-content combina-\ntion over the VAE latent space. Each semantic role\nis jointly determined by multiple dimensions rather\nthan one single dimension. Therefore, calculating\nthe importance of one dimension to that semantic\nrole as a disentanglement metric is unreliable. In\nthis work, we quantitatively evaluate the disentan-\nglement of the semantic roles by: (1) calculating\nthe averaged Euclidean distance between different\ncontent under that role, such as the distance be-\ntween PRED-is and PRED-are, and (2) counting\nthe number of different indices of the same role-\ncontent after the vector quantisation. The smaller\nthe distance or the less the number of indices, the\nmore concentrated the distribution of this semantic\nrole in the latent space, indicating better disentan-\nglement.\n3.2\nInterpolation Smoothness\nInterpolation is a standard process for evaluating\nthe geometric properties of a latent space in both\nimage and language domains (Li et al., 2020; Liu\net al., 2021). It aims to generate a sequence of sen-\ntences following a spatial trajectory from source\nto target via latent arithmetics. For example, in\nthe VAE latent space, the interpolation path can\nbe described as zt = z1 · (1 − t) + z2 · t with t\nincreased from 0 to 1 by a step size of 0.1 where z1\nand z2 represent latent vectors of source and target\nsentences, respectively. In this case, each interme-\ndiate output D(zt) should change fewer semantic\nconcepts at each step if the latent space is smooth\nand regular. In this work, we employ a similar\nstrategy, however follow the more granular token\nlevel within the VQVAE. We directly manipulate\nthe interpolation within the latent token space. At\neach step t, we obtain the intermediate latent token\nembedding zwi\nt\nwithin a sentence by calculating\nthe weighted minimal distance between its preced-\ning token embedding zwi\nt−0.1 and the target token\nembeddings zwi\n2 . This process can be described as\nfollows:\nzwi\n1\n= ek1, zwi\n2\n= ek2, where i = [1, ..., L]\nzwi\nt\n= zk, where\nk = argminj (1 − t) ×\n\r\rzwi\nt−0.1 − zj\r\r\n2\n+ t ×\n\r\rzwi\n2 − zj\r\r\n2\nst = [zw1\nt ; . . . ; zwL\nt\n]\nwhere st represents the sentence embeddings at\nstep t. The final generated sentence can be de-\ncoded as st = D(st). Once we have obtained the\ninterpolation path, we introduce the interpolation\nsmoothness (IS) metric to quantitatively evaluate\nits smoothness. This metric involves calculating\nthe aligned semantic distance between the source\nand the target (referred to as the ideal semantic dis-\ntance). Subsequently, we calculate the sum of the\naligned semantic distances between each pair of ad-\njacent sentences in the path (referred to as the actual\nsemantic distance). Finally, by dividing the ideal\nsemantic distance by the actual semantic distance,\nwe obtain a measure of smoothness. If the result is\n1, it indicates that the actual path aligns perfectly\nwith the ideal path, suggesting better geometric\nproperties. Conversely, it suggests a less coherent\ntransformation path, indicating poorer geometric\nproperties. The metric is defined as follows:\nIS = E(s0,...,sT )∼P\nδ(align(s0, sT ))\nPT\nt=0 δ(align(st, st+0.1))\nwhere δ and align are sentence similarity and align-\nment functions, respectively. In this experiment,\nsentence similarity and alignment are performed\nvia Word Mover’s Distance (Zhao et al., 2019)\nsince it can softly perform the semantic alignment.\n4\nExperiments\n4.1\nAutoEncoding Task\nPre-training Data.\nIn this work, we focus on the\nuse of conceptually dense explanatory sentences\n(Dalvi et al., 2021) and mathematical latex expres-\nsions (Meadows et al., 2023b) to evaluate model\nperformance. The rationale behind this choice is\nthat (1) explanatory sentences provide a semanti-\ncally challenging yet sufficiently well-scoped sce-\nnario to evaluate the syntactic and semantic or-\nganisation of the space (Thayaparan et al., 2020;\nValentino et al., 2022a,b); (2) mathematical expres-\nsions follow a well-defined syntactic structure and\nset of symbolic rules that are notoriously difficult\nfor neural models (Meadows et al., 2023a). More-\nover, the set of rules applicable to a mathematical\nexpression fully determines its semantics, allow-\ning for an in-depth inspection and analysis of the\nprecision and level of generalisation achieved by\nthe models (Welleck et al., 2022; Valentino et al.,\n2023). Firstly, we conduct a pre-training phase,\nevaluating the performance of T5VQVAE in re-\nconstructing scientific explanatory sentences from\nWorldTree (Jansen et al., 2018) and mathemati-\ncal latex expressions from the dataset proposed by\nMeadows et al. (2023b).\nBaselines.\nWe consider both small and base ver-\nsions of pretrained T5 to initialise the T5VQVAE,\nwhere the codebook size is 10000. The effect of\ndifferent codebook sizes on its performance and the\noptimal point within the architecture (different hid-\nden layers of the encoder) to learn the codebook are\nreported in Table 11. As for the large VAE model,\nwe consider Optimus with random initial weights\nand pre-trained weights (Li et al., 2020) and Della\n(Hu et al., 2022). We chose two different latent di-\nmension sizes (32 and 768) for both of them. More-\nover, we also select several LSTM language autoen-\ncoders (AE), including denoising AE (Vincent et al.\n(2008), DAE), β-VAE (Higgins et al., 2016), ad-\nversarial AE (Makhzani et al. (2015), AAE), label\nadversarial AE (Rubenstein et al. (2018), LAAE),\nand denoising adversarial autoencoder (Shen et al.\n(2020), DAAE). Additional details on the training\nsetup are provided in Appendix A. The full source\ncode of the experimental pipeline is available at an\nanonymised link for reproducibility purposes.\nExplanatory sentences\nEvaluation Metrics\nBLEU BLEURT Cosine Loss ↓ PPL ↓\nDAE(768)\n0.74\n0.03\n0.91\n1.63\n5.10\nAAE(768)\n0.35\n-0.95\n0.80\n3.35\n28.50\nLAAE(768)\n0.26\n-1.07\n0.78\n3.71\n40.85\nDAAE(768)\n0.22\n-1.26\n0.76\n4.00\n54.59\nβ-VAE(768)\n0.06\n-1.14\n0.77\n3.69\n40.04\nOptimus(32, rand)\n0.54\n0.14\n0.92\n1.08\n2.94\nOptimus(32, pre)\n0.61\n0.29\n0.93\n0.86\n2.36\nOptimus(768, rand)\n0.49\n-0.04\n0.90\n1.32\n3.74\nOptimus(768, pre)\n0.68\n0.48\n0.95\n0.65\n1.91\nDELLA(32, rand)\n0.71\n0.06\n0.92\n0.50\n1.65\nDELLA(768, rand)\n0.72\n0.21\n0.95\n0.41\n1.51\nT5VQVAE(small, soft) 0.81\n0.62\n0.97\n0.46\n1.58\nT5VQVAE(base, soft)\n0.82\n0.62\n0.97\n0.75\n2.11\nMathematical expressions\nEvaluation Datasets\nEVAL VAR\nEASY EQ\nLEN\nDAE(768)\n0.94\n0.50\n0.80\n0.74\n0.58\nAAE(768)\n0.41\n0.41\n0.39\n0.41\n0.52\nLAAE(768)\n0.41\n0.45\n0.39\n0.39\n0.49\nDAAE(768)\n0.38\n0.48\n0.35\n0.38\n0.49\nβ-VAE(768)\n0.39\n0.48\n0.37\n0.39\n0.50\nOptimus(32, rand)\n0.95\n0.59\n0.75\n0.71\n0.50\nOptimus(768, rand)\n0.96\n0.61\n0.79\n0.75\n0.54\nDELLA(32, rand)\n1.00\n0.55\n0.89\n0.72\n0.63\nDELLA(768, rand)\n1.00\n0.55\n0.93\n0.79\n0.64\nT5VQVAE(small, soft) 0.97\n0.65\n0.95\n0.90\n0.69\nT5VQVAE(base, soft)\n0.98\n0.62\n0.95\n0.85\n0.68\nTable 1: AutoEncoding task evaluation on the test set\n(soft: k-means). The highest scores of large VAE mod-\nels and LSTM-based VAE models are highlighted in\nblue and in bold separately.\nQuantitative Evaluation.\nAs for modelling ex-\nplanatory sentences, we quantitatively evaluate the\nperformance of the models using five metrics, in-\ncluding BLEU (Papineni et al., 2002), BLEURT\n(Sellam et al., 2020), cosine similarity from pre-\ntrained sentence T5 (Ni et al., 2022), cross-entropy\n(Loss), and perplexity (PPL). As for modelling\nmathematical expressions, we use BLEU to eval-\nuate the robustness of models on the 5 test sets\nproposed by Meadows et al. (2023b), one designed\nto assess in-distribution performance, and four de-\nsigned to assess out-of-distribution generalisation.\nHere we provide a full characterisation of the test\nsets: (1) EVAL: contains mathematical statements\nfollowing the same distribution of the training set\n(like U + cos(n)), including expressions with simi-\nlar lengths and set of symbols (2) VAR: full mathe-\nmatical statements with variable perturbations (like\nU + cos(beta)), designed to test the robustness\nof the models when dealing with expressions con-\ntaining variables never seen during training; (3)\nEASY: simpler mathematical expressions with a\nlower number of variables, designed to test length\ngeneralisation (like cos(n)), (4) EQ: full mathe-\nmatical statements with equality insertions (like\nE = U +cos(n)), designed to test the behaviour of\nRole-content\nNUM centers AVG dis MAX dis MIN dis\nARG0-animal 3\n0.28\n0.52\n0.35\nARG1-animal 3\n0.28\n0.52\n0.35\nARG2-animal 4\n0.33\n0.55\n0.35\nPRED-is\n24\n0.60\n1.08\n0.22\nPRED-are\n6\n0.31\n0.64\n0.21\nMOD-can\n5\n0.40\n0.82\n0.28\nNEG-not\n2\n0.25\n0.51\n0.51\nTable 2: Semantic role disentanglement.\nthe model on equivalent mathematical expressions\nwith minimal perturbations (5) LEN: mathematical\nstatements with a higher number of variables (like\nU + cos(n)) + A + B), designed to test generali-\nsation on more complex expressions.\nAs shown in Table 1, the highest scores for large\nVAE models and LSTM-based VAE models are\nhighlighted in blue and bold, respectively. Among\nthem, T5VQVAEs with the k-means scheme out-\nperforms Optimus and LSTM-based VAEs in both\ncorpora and compared with Della, it can deliver\nbetter generation and generalization. We provide\nexamples with low BLEURT scores in Appendix C\nNext, we quantitatively evaluate the disentangle-\nment of T5VQVAE following the semantic disen-\ntanglement reference metric 3.1. As displayed in\nTable 2, the number of central points for PRED is\nhigher than the remaining role-content, being 24 in\nPRED-is and 6 in PRED-are. This indicates that\nthe semantic information of PRED is more widely\ndistributed in the latent space when compared to\nother roles. This behaviour might be attributed\nto the fact that the aforementioned predicates are\nwidely used across sentences in the corpus. The\nfull visualisation of the semantic disentanglement\nachieved by T5VQVAE is provided in Figure 3.\n4.2\nText Transfer Task\nNext,\nwe\ninvestigate\nthe\ncontrollability\nof\nT5VQVAE by manipulating the latent space via\ngeometric transformations. This is referred to as\nthe Text Transfer task. We compare the perfor-\nmance of T5VQVAE (base, soft) and Optimus (32,\npretrain) - both trained in the AutoEncoding task -\nas baselines. We evaluate the latent space using la-\ntent traversal, interpolation, and vector arithmetics.\nLatent Traversal.\nThe traversal is inspired by\nthe image domain, only changing the feature in-\nterpretation (Higgins et al., 2017; Kim and Mnih,\n2018). Specifically, if the vector projection within\nthe latent space can be modified when traversing\n(re-sampling) one dimension, the output should\nonly change well-defined semantic features corre-\nsponding to that dimension. In this experiment,\nthe traversal is set up from a starting sentence. As\nillustrated in Table 3, the T5VQVAE can provide\nlocalised semantic control by operating the discrete\nlatent space. Different dimensions in the discrete\nsentence space can control different parts of the\nsentence. The traversal for Optimus is provided in\nAppendix D.\nLatent Interpolation.\nAs described in section\n3.2, interpolation aims to generate a sequence of\nsentences from source to target via latent vector\narithmetic. An ideal interpolation should lead to\nreasonable semantic controls at each step. In Ta-\nble 4, we can observe that compared with Opti-\nmus’s interpolation (bottom) where the semantics\nare changed redundantly, e.g., from some birds\nto some species mammals to most birds and from\nhave to don’t have to have, T5VQVAE (top) leads\nto a more reasonable (coherent/smoother) pathway.\nE.g., from speckled brown color to speckled brown\nfeathers to speckled wings to wings. Additional\nexamples are provided in Appendix D.\nMore importantly, we quantitatively evaluate the\ninterpolation behaviour via the IS metric. We ran-\ndomly select 100 (source, target) pairs and interpo-\nlate the path between them. Then, we calculate the\naveraged, maximal, and minimal ISs. As shown in\nTable 5, T5VQVAE outperforms Optimus by over\n43% in average, which indicates that T5QVAE in-\nduces a latent space which can better separate the\nsyntactic and semantic factors when contrasted to\nOptimus.\nLatent Vector Arithmetics.\nInspired by word\nembedding arithmetics, e.g., king − man +\nwoman = queen, we explore the compositional\nsemantics via latent arithmetic with the target of\nsentence-level semantic control.\nAfter adding\ntwo latent vectors corresponding to two sentences\nsc = sA + sB, we expect the resulting sentence to\nexpress the semantic information of both sentences.\nFrom Table 6, we can observe that T5VQVAE can\ngenerate the outputs containing both inputs’ seman-\ntic information. E.g., the output contains are likely\nto and their environment from sA and to survive\nand / from sB. In contrast, Optimus is not able\nto preserve to support this behaviour. Additional\nexamples are provided in Appendix D (Table 16).\nan animal requires warmth in cold environments\ndim0: an animal requires warmth in cold environments\ndim0: a animal requires warmth in cold environments\ndim0: the animal requires warmth in cold environments\ndim1: an organism requires warmth in cold environments\ndim1: an animal requires warmth in cold environments\ndim1: an object requires warmth in cold environments\ndim2: an animal needs warmth in cold environments\ndim2: an animal must find warmth in cold environments\ndim2: an animal brings warmth in cold environments\ndim2: an animal wants warmth in cold environments\ndim4: an animal requires warmth during cold tempera-\ntures\ndim4: an animal requires warmth in cold environments\ndim4: an animal requires warmth to cold environments\ndim5: an animal requires warmth in temperatures\ndim5: an animal requires warmth in warm environments\ndim5: an animal requires warmth in a warm environment\ndim6: an animal requires warmth in cold temperatures\ndim6: an animal requires warmth in cold climates\ndim6: an animal requires warmth in cold systems\nTable 3: T5VQVAE(base): traversals showing controlled semantic concepts in explanations. We also provide the\ntraversal of Optimus latent space for comparison in Table 13.\nSource: some birds have a speckled brown color\n1. some birds have a speckled brown color\n2. some birds do not have speckled brown feathers\n3. some species mammals do not have speckled\nwings\n4. most species mammals do not have wings\n1. some birds have scales\n2. some birds have a speckled brown color\n3. some species mammals have wings\n4. most birds don’t have wings\n5. most insects have wings\n6. most species mammals don’t have wings\nTarget: most species mammals do not have wings\nTable 4: Interpolation for T5VQVAE (top) and Optimus\n(bottom) where blue, underline, and orange represent\nsubject, verb, and object, respectively. Only unique\nsentences are shown.\nEvaluation Metrics\navg IS max IS min IS\nOptimus(32, pretrain)\n0.22\n0.53\n0.13\nOptimus(768, pretrain) 0.21\n0.50\n0.10\nT5VQVAE(base, soft)\n0.65\n1.00\n0.18\nTable 5: Interpolation smoothness.\n4.3\nInference Task\nLastly, we move to downstream inference tasks,\nin which we aim to explore the controllability of\nT5VQVAE for reasoning with natural and sym-\nbolic languages. Specifically, we focus on two\ntasks including syllogistic-deductive natural lan-\nguage inference in EntailmentBank (Dalvi et al.,\n2021), where a natural language conclusion has\nto be inferred from two premises, and mathemati-\nsA: animals are likely to have the same color as\ntheir environment\nsB: animals require respiration to survive / use\nenergy\nT5VQVAE: animals are likely to survive / to survive\nin their environment\nOptimus: animals have evolved from animals with\ntraits that have an animal instinct\nTable 6: Latent arithmetic sA +sB for T5VQVAE(base)\nand Optimus(32). blue, orange, and shallow blue in-\ndicate the semantic information from both sA and sB,\nfrom sA only, from sB only, respectively.\ncal expression derivation (Meadows et al., 2023b),\nwhere the goal is to predict the result of applying a\nmathematical operation to a given premise expres-\nsion (written in latex).\nQuantitative\nEvaluation.\nWe\nquantitatively\nevaluate several baselines following the same pro-\ncedure as the AutoEncoding task. Table 7 shows\nthat T5VQVAE outperforms all VAE models on\nboth benchmarks.\nQualitative Evaluation.\nNext, we focus on\nthe NLI task to explore the controllability of\nT5VQVAE for sentence-level inference traversing\nthe latent space. As illustrated in Table 8, traversing\nthe dimension corresponding to an individual word\n(e.g., object from premise 1 (P1)) cannot preserve\nthe target word during the traversal along with the\nsemantic coherence of the transitions, indicating\nthat the inference is done entirely in the Encoder.\nTherefore, we next explore how to manipulate the\nlatent representation to deliver a more controllable\nNatural Language Inference (EntailmentBank)\nEvaluation Metrics\nBLEU Cosine BLEURT Loss ↓ PPL ↓\nT5(small)\n0.54\n0.96\n0.22\n0.69\n1.99\nT5(base)\n0.57\n0.96\n0.33\n0.61\n1.84\nBart(base)\n0.54\n0.96\n0.17\n0.63\n1.87\nFlanT5(small)\n0.22\n0.89\n-1.33\n0.99\n2.69\nFlanT5(base)\n0.32\n0.89\n-0.31\n0.95\n2.58\nT5bottleneck(base) 0.35\n0.91\n-0.20\n1.24\n3.45\nOptimus(32)\n0.07\n0.74\n-1.20\n1.13\n2.31\nOptimus(768)\n0.08\n0.74\n-1.21\n0.82\n2.27\nDELLA(32)\n0.08\n0.85\n-1.23\n1.69\n5.41\nDELLA(768)\n0.09\n0.87\n-1.09\n1.54\n4.66\nT5VQVAE(small)\n0.11\n0.73\n-1.23\n0.85\n2.33\nT5VQVAE(base)\n0.46\n0.94\n0.10\n0.84\n2.31\nMathematical Expression Derivation\nEvaluation Datasets EVAL SWAP EASY\nEQ\nLEN\nT5(small)\n0.69\n0.48\n0.57\n0.60\n0.63\nT5(base)\n0.97\n0.65\n0.90\n0.72\n0.81\nOptimus(32)\n0.72\n0.50\n0.59\n0.23\n0.40\nOptimus(768)\n0.79\n0.56\n0.63\n0.29\n0.44\nDELLA(32)\n0.12\n0.16\n0.13\n0.13\n0.13\nDELLA(768)\n0.13\n0.18\n0.12\n0.13\n0.14\nT5VQVAE(small)\n0.75\n0.57\n0.77\n0.48\n0.50\nT5VQVAE(base)\n0.76\n0.56\n0.78\n0.47\n0.50\nTable 7: Quantitative evaluation on inference tasks.\nP1: a human is a kind of object\nP2: a child is a kind of young human\nC: a child is a kind of object\ndim6: a young object is a kind of child\ndim6: a boy is a kind of young object\ndim6: a little boy is a kind of young human\nTable 8: T5VQVAE (base): traversed conclusions.\ninference behaviour.\nRecent work (Zhang et al., 2023c) has provided a\ngranular annotated dataset of step-wise explanatory\ninference types, which reflect symbolic (syllogistic-\nstyle) operations between premises and conclu-\nsions, including argument/verb substitution, fur-\nther specification, and conjunction. We leverage\nthis annotation to input two premises into the En-\ncoder to derive the latent token embeddings of in-\ndividual arguments and guide the generation of the\nconclusion via the Decoder. For example, for ar-\ngument substitution and verb substitution, which\nrefers to the process of obtaining a conclusion\nby substituting one argument/verb from the first\npremise to an argument/verb of the second premise,\nwe substitute the respective token embeddings in\nthe latent space and feed the resulting representa-\ntion to the decoder. Table 9 shows that by substi-\ntuting the embeddings of the arguments, we can\ncontrol the behaviour of the model and elicit a sys-\ntematic inference behaviour. We provide further\nP1: a shark is a kind of fish\nP2: a fish is a kind of aquatic animal\nPred: a shark is a kind of aquatic animal\nP1: to move something can mean to transfer some-\nthing\nP2: flowing is a kind of movement for energy\nPred: flowing is a kind of transfer of energy\nTable 9: T5VQVAE(base): quasi-symbolic inference ex-\namination in AutoEncoder (Top: argument substitution,\nBottom: Verb substitution).\nspecification and conjunction in Table 18. These\nresults show that the latent embeddings can be ma-\nnipulated to deliver a syllogistic-style inference\nbehaviour. In particular, we demonstrate that the\ndistributed semantic information in the latent space\ncontains information about co-occurring tokens\nwithin the sentence that can be systematically lo-\ncalised (within specific arguments, predicates or\nclauses) and manipulated to generate a sound con-\nclusion. This behaviour can be potentially lever-\naged as a foundation to build an interpretable and\nmulti-step natural language inference model. More\nexamples are reported in the Appendix E.\n5\nRelated work\nSemantic Control via Latent Spaces.\nZhang\net al. (2022, 2023a) investigated the semantic con-\ntrol of latent sentence spaces, demonstrating the\nbasic geometric-semantic properties of VAE-based\nmodels. Mercatali and Freitas (2021) defined dis-\nentangled latent spaces focusing on the separation\nbetween content and syntactic generative factors.\nMoreover, some works focused on defining two\nseparate latent spaces to control natural language\ngeneration on specific downstream tasks, such as\nstyle-transfer and paraphrasing (Bao et al., 2019a;\nJohn et al., 2019a). Comparatively, this work ex-\nplores more granular control and a broader spec-\ntrum of tasks: from syllogistic to symbolic infer-\nence.\nLanguage VAEs.\nInstead of Optimus (Li et al.,\n2020) and its variation (Fang et al., 2022; Hu et al.,\n2022) where the encoder and decoder are BERT\nand GPT2, respectively, most of the language VAE\nliterature are based on LSTM architectures instan-\ntiated on different text generation tasks, includ-\ning story generation (Fang et al., 2021), dialogue\ngeneration (Zhao et al., 2017), text style transfer\n(John et al., 2019a; Shen et al., 2020), text para-\nphrasing (Bao et al., 2019a), among others. Some\nworks also investigated different latent spaces or\npriors to improve representation capabilities (Dai\net al., 2021; Ding and Gimpel, 2021; Fang et al.,\n2022). Comparatively, this work contributes by\nfocusing on the close integration between language\nmodels and vector-quantized VAE-driven granular\ncontrol, instantiating it in the context of a state-\nof-the-art, accessible, and cross-task performing\nlanguage model (T5).\n6\nConclusion and Future Works\nIn this work, we build a model for improving the se-\nmantic and inference control for VAE-enabled lan-\nguage model (autoencoding) architectures. We pro-\npose a new model (i.e., T5VQVAE) which is based\non the close integration of a vector-quantized VAE\nand a consistently accessible and high-performing\nlanguage model (T5). The proposed model was\nextensively evaluated with regard to its syntactic,\nsemantic and inference controls using three down-\nstream tasks (autoencoding, text transfer, and infer-\nence task). Our experimental results indicate that\nthe T5VQVAE can outperform the canonical state-\nof-the-art models in those tasks and can deliver a\nquasi-symbolic behaviour in the inference task (via\nthe direct manipulation of the latent space).\nAs future work, we plan to further explore ap-\nplications on symbolic natural language inference\nvia the direct manipulation of the latent space, and\nto investigate the controllability of recent large lan-\nguage models through the VQVAE architecture.\nMoreover, additional research directions could be\ninformed by the current work:\nWord-level Disentanglement.\nOur architecture\nprovides a foundation to explore token/word-level\ndisentanglement for more general sentence and in-\nference representation tasks. While sentence-level\ndisentanglement is widely explored in the NLP\ndomain, such as sentiment-content (John et al.,\n2019b; Hu and Li, 2021), semantic-syntax (Bao\net al., 2019b; Zhang et al., 2023d), and negation-\nuncertainty (Vasilakes et al., 2022), or syntactic-\nlevel disentanglement (Felhi et al., 2022), this\nmechanism is still under-explored in other NLP\ntasks (Liao et al., 2020).\nInterpretability.\nDiscrete properties derived\nfrom vector quantization can enable the further\nprobing and interpretability of neural networks by\ndiscretizing continuous neural latent spaces, where\nsymbolic concepts are emerging in both images\n(Deng et al., 2021; Li and Zhang, 2023) and natural\nlanguage (Tamkin et al., 2023) domains.\nLimitations\nWhile T5VQVAE can improve inference perfor-\nmance and deliver inference control on syllogistic-\ndeductive style explanations, the application on\nmore complex reasoning tasks (e.g.\ninvolving\nquantifiers and multi-hop inference) is not fully\nexplored. Besides, we still observe limitations in\nout-of-distribution generalisation in the mathemat-\nical expressions corpus despite the improvement\nover existing VAE models in terms of robustness.\nThis, in particular, is highlighted by the decrease in\nperformance obtained on the length generalisation\nsplit (LEN) for both autoencoding and expression\nderivation tasks.\nAcknowledgements\nWe appreciate the reviewers for their insightful\ncomments and suggestions. This work was par-\ntially funded by the Swiss National Science Foun-\ndation (SNSF) project NeuMath (200021_204617),\nby the EPSRC grant EP/T026995/1 entitled “En-\nnCore: End-to-End Conceptual Guarding of Neural\nArchitectures” under Security for all in an AI en-\nabled society, by the CRUK National Biomarker\nCentre, and supported by the Manchester Exper-\nimental Cancer Medicine Centre and the NIHR\nManchester Biomedical Research Centre.\nReferences\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou,\nOlga Vechtomova, Xin-yu Dai, and Jiajun Chen.\n2019a. Generating sentences from disentangled syn-\ntactic and semantic spaces. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 6008–6019, Florence, Italy.\nAssociation for Computational Linguistics.\nYu Bao, Hao Zhou, Shujian Huang, Lei Li, Lili Mou,\nOlga Vechtomova, Xinyu Dai, and Jiajun Chen.\n2019b. Generating sentences from disentangled syn-\ntactic and semantic spaces. In Proceedings of the\n57th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6008–6019.\nDanilo S Carvalho, Giangiacomo Mercatali, Yingji\nZhang, and Andre Freitas. 2023. Learning disentan-\ngled representations for natural language definitions.\nIn Findings of the European chapter of Association\nfor Computational Linguistics (Findings of EACL).\nShuyang Dai, Zhe Gan, Yu Cheng, Chenyang Tao,\nLawrence Carin, and Jingjing Liu. 2021. APo-VAE:\nText generation in hyperbolic space. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 416–431, On-\nline. Association for Computational Linguistics.\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan\nXie, Hannah Smith, Leighanna Pipatanangkura, and\nPeter Clark. 2021. Explaining answers with entail-\nment trees. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 7358–7370, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nHuiqi Deng, Qihan Ren, Hao Zhang, and Quanshi\nZhang. 2021. Discovering and explaining the rep-\nresentation bottleneck of dnns.\narXiv preprint\narXiv:2111.06236.\nXiaoan Ding and Kevin Gimpel. 2021.\nFlowPrior:\nLearning expressive priors for latent variable sen-\ntence models. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 3242–3258, Online. Association for\nComputational Linguistics.\nLe Fang, Tao Zeng, Chaochun Liu, Liefeng Bo, Wen\nDong, and Changyou Chen. 2021.\nTransformer-\nbased\nconditional\nvariational\nautoencoder\nfor\ncontrollable story generation.\narXiv preprint\narXiv:2101.00828.\nXianghong Fang, Jian Li, Lifeng Shang, Xin Jiang, Qun\nLiu, and Dit-Yan Yeung. 2022. Controlled text gen-\neration using dictionary prior in variational autoen-\ncoders. In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, pages 97–111, Dublin,\nIreland. Association for Computational Linguistics.\nGhazi Felhi, Joseph Le Roux, and Djamé Seddah. 2022.\nTowards unsupervised content disentanglement in\nsentence representations via syntactic roles. arXiv\npreprint arXiv:2206.11184.\nHao Fu, Chunyuan Li, Xiaodong Liu, Jianfeng Gao,\nAsli Celikyilmaz, and Lawrence Carin. 2019. Cycli-\ncal annealing schedule: A simple approach to mit-\nigating KL vanishing. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 240–250, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson H S Liu, Matthew E.\nPeters, Michael Schmitz, and Luke Zettlemoyer.\n2017. A deep semantic natural language process-\ning platform.\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher P.\nBurgess, Xavier Glorot, Matthew M. Botvinick,\nShakir Mohamed, and Alexander Lerchner. 2016.\nbeta-vae: Learning basic visual concepts with a con-\nstrained variational framework. In International Con-\nference on Learning Representations.\nIrina Higgins, Loïc Matthey, Arka Pal, Christopher P.\nBurgess, Xavier Glorot, Matthew M. Botvinick,\nShakir Mohamed, and Alexander Lerchner. 2017.\nbeta-vae: Learning basic visual concepts with a con-\nstrained variational framework. In ICLR.\nJinyi Hu, Xiaoyuan Yi, Wenhao Li, Maosong Sun, and\nXing Xie. 2022. Fuse it more deeply! a variational\ntransformer with layer-wise latent variable inference\nfor text generation. In Proceedings of the 2022 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 697–716, Seattle, United\nStates. Association for Computational Linguistics.\nZhiting Hu and Li Erran Li. 2021. A causal lens for\ncontrollable text generation. Advances in Neural\nInformation Processing Systems, 34:24941–24955.\nEric Jang, Shixiang Gu, and Ben Poole. 2016. Categori-\ncal reparameterization with gumbel-softmax. arXiv\npreprint arXiv:1611.01144.\nPeter A Jansen, Elizabeth Wainwright, Steven Mar-\nmorstein, and Clayton T Morrison. 2018. Worldtree:\nA corpus of explanation graphs for elementary sci-\nence questions supporting multi-hop inference. arXiv\npreprint arXiv:1802.03052.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2019a.\nDisentangled representation\nlearning for non-parallel text style transfer. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 424–434,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nVineet John, Lili Mou, Hareesh Bahuleyan, and Olga\nVechtomova. 2019b. Disentangled representation\nlearning for non-parallel text style transfer. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 424–434.\nHyunjik Kim and Andriy Mnih. 2018. Disentangling\nby factorising. In Proceedings of the 35th Interna-\ntional Conference on Machine Learning, volume 80\nof Proceedings of Machine Learning Research, pages\n2649–2658. PMLR.\nDiederik P Kingma and Max Welling. 2013.\nAuto-\nencoding\nvariational\nbayes.\narXiv\npreprint\narXiv:1312.6114.\nBohan Li, Junxian He, Graham Neubig, Taylor Berg-\nKirkpatrick, and Yiming Yang. 2019. A surprisingly\neffective fix for deep latent variable modeling of text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3603–\n3614, Hong Kong, China. Association for Computa-\ntional Linguistics.\nChunyuan Li, Xiang Gao, Yuan Li, Baolin Peng, Xiujun\nLi, Yizhe Zhang, and Jianfeng Gao. 2020. Optimus:\nOrganizing sentences via pre-trained modeling of a\nlatent space. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 4678–4699.\nMingjie Li and Quanshi Zhang. 2023. Does a neural\nnetwork really encode symbolic concept?\narXiv\npreprint arXiv:2302.13080.\nKeng-Te Liao, Cheng-Syuan Lee, Zhong-Yu Huang, and\nShou-de Lin. 2020. Explaining word embeddings via\ndisentangled representation. In Proceedings of the\n1st Conference of the Asia-Pacific Chapter of the As-\nsociation for Computational Linguistics and the 10th\nInternational Joint Conference on Natural Language\nProcessing, pages 720–725, Suzhou, China. Associa-\ntion for Computational Linguistics.\nYahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao,\nHaoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang,\nand Marco De Nadai. 2021. Smoothing the disen-\ntangled latent style space for unsupervised image-to-\nimage translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 10785–10794.\nAlireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian\nGoodfellow, and Brendan Frey. 2015. Adversarial\nautoencoders. arXiv preprint arXiv:1511.05644.\nJordan Meadows, Marco Valentino, and Andre Fre-\nitas. 2023a.\nGenerating mathematical deriva-\ntions with large language models. arXiv preprint\narXiv:2307.09998.\nJordan Meadows, Marco Valentino, Damien Teney, and\nAndre Freitas. 2023b. A symbolic framework for\nsystematic evaluation of mathematical reasoning with\ntransformers. arXiv preprint arXiv:2305.12563.\nGiangiacomo Mercatali and André Freitas. 2021. Disen-\ntangling generative factors in natural language with\ndiscrete variational autoencoders.\narXiv preprint\narXiv:2109.07169.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-t5: Scalable sentence encoders from pre-\ntrained text-to-text models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1864–1874, Dublin, Ireland. Association for\nComputational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\njing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. pages 311–318.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nAurko Roy, Ashish Vaswani, Arvind Neelakantan,\nand Niki Parmar. 2018. Theory and experiments\non vector quantized autoencoders. arXiv preprint\narXiv:1805.11063.\nPaul K Rubenstein, Bernhard Schoelkopf, and Ilya Tol-\nstikhin. 2018. On the latent space of wasserstein\nauto-encoders. arXiv preprint arXiv:1802.03761.\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.\nBLEURT: Learning robust metrics for text genera-\ntion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7881–7892, Online. Association for Computational\nLinguistics.\nTianxiao Shen, Jonas Mueller, Regina Barzilay, and\nTommi Jaakkola. 2020. Educating text autoencoders:\nLatent representation guidance via denoising. In In-\nternational Conference on Machine Learning, pages\n8719–8729. PMLR.\nAlex Tamkin, Mohammad Taufeeque, and Noah D\nGoodman. 2023. Codebook features: Sparse and\ndiscrete interpretability for neural networks. arXiv\npreprint arXiv:2310.17230.\nMokanarangan Thayaparan, Marco Valentino, and An-\ndré Freitas. 2020.\nA survey on explainability in\nmachine reading comprehension.\narXiv preprint\narXiv:2010.00389.\nMarco Valentino, Jordan Meadows, Lan Zhang, and\nAndré Freitas. 2023. Multi-operational mathemat-\nical derivations in latent space.\narXiv preprint\narXiv:2311.01230.\nMarco Valentino, Mokanarangan Thayaparan, Deborah\nFerreira, and André Freitas. 2022a. Hybrid autore-\ngressive inference for scalable multi-hop explanation\nregeneration. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 11403–\n11411.\nMarco Valentino, Mokanarangan Thayaparan, and An-\ndré Freitas. 2022b. Case-based abductive natural\nlanguage inference. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 1556–1568, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural\ndiscrete representation learning. Advances in neural\ninformation processing systems, 30.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nJake Vasilakes, Chrysoula Zerva, Makoto Miwa, and\nSophia Ananiadou. 2022. Learning disentangled rep-\nresentations of negation and uncertainty. In Proceed-\nings of the 60th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 8380–8397, Dublin, Ireland. Association\nfor Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008. Extracting and com-\nposing robust features with denoising autoencoders.\nIn Proceedings of the 25th International Conference\non Machine Learning, ICML ’08, page 1096–1103,\nNew York, NY, USA. Association for Computing\nMachinery.\nSean Welleck, Peter West, Jize Cao, and Yejin Choi.\n2022. Symbolic brittleness in sequence models: on\nsystematic generalization in symbolic mathematics.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 36, pages 8629–8637.\nYingji Zhang, Danilo S Carvalho, Ian Pratt-Hartmann,\nand André Freitas. 2022. Quasi-symbolic explana-\ntory nli via disentanglement: A geometrical examina-\ntion. arXiv preprint arXiv:2210.06230.\nYingji Zhang, Danilo S Carvalho, Ian Pratt-Hartmann,\nand André Freitas. 2023a. Learning disentangled\nsemantic spaces of explanations via invertible neural\nnetworks. arXiv preprint arXiv:2305.01713.\nYingji Zhang, Danilo S Carvalho, Ian Pratt-Hartmann,\nand André Freitas. 2023b. Llamavae: Guiding large\nlanguage model generation via continuous latent sen-\ntence spaces. arXiv preprint arXiv:2312.13208.\nYingji Zhang, Danilo S Carvalho, Ian Pratt-Hartmann,\nand Andre Freitas. 2023c. Towards controllable natu-\nral language inference through lexical inference types.\narXiv preprint arXiv:2308.03581.\nYingji Zhang, Marco Valentino, Danilo S Carvalho, Ian\nPratt-Hartmann, and André Freitas. 2023d. Graph-\ninduced syntactic-semantic spaces in transformer-\nbased variational autoencoders.\narXiv preprint\narXiv:2311.08579.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi. 2017.\nLearning discourse-level diversity for neural dialog\nmodels using conditional variational autoencoders.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 654–664, Vancouver, Canada.\nAssociation for Computational Linguistics.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Chris-\ntian M. Meyer, and Steffen Eger. 2019. MoverScore:\nText generation evaluating with contextualized em-\nbeddings and earth mover distance. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 563–578, Hong\nKong, China. Association for Computational Lin-\nguistics.\nA\nTraining setup\nDatasets\nTable 10 displays the statistical informa-\ntion of the datasets used in the experiment. As for\nthe AutoEncoder setup, we use the non-repetitive\nexplanations selected from both datasets as the ex-\nperimental data. As for the Inference task, we use\nthe data from EntailmentBank and Math Symbol\nInference. The semantic roles of our data are an-\nnotated by automatic semantic role labelling tool\n(Gardner et al., 2017).\nCorpus\nNum data.\nAvg. length\nWorldTree\n11430\n8.65\nEntailmentBank\n5134\n10.35\nMath Symbol\n32000\n6.84\nMath Symbol Inference\n32000\n51.84\nTable 10: Statistics from datasets.\nT5VQVAE training\nWe use T5VQVAE(small)\nto choose the most appropriate codebook size be-\ntween 2000 and 22000. In the experiment, the\nmaximal epoch is 100. The learning rate is 5e-5.\nWe use exponential moving averages (EMA) to up-\ndate the codebook. Besides, we also investigated\nthe optimal point within the architecture to learn\nthe codebook. As shown in Table 11, T5VQVAE\nperforms better when the codebook is learned at\nthe end of the Encoder. This observation suggests\nthat cross-attention is crucial in vector quantisation\n(VQ) learning.\nMetrics\nBLEU BLEURT cosine Loss ↓ PPL ↓\n02000\n0.73\n0.21\n0.93\n0.79\n2.20\n06000\n0.79\n0.45\n0.95\n0.61\n1.84\n10000\n0.81\n0.62\n0.97\n0.46\n1.58\n14000\n0.82\n0.62\n0.96\n0.42\n1.52\n18000\n0.83\n0.64\n0.96\n0.38\n1.46\n22000\n0.83\n0.67\n0.96\n0.34\n1.40\nT5VQVAE(small) with different depth L in Encoder\nT5VQVAE(L=05) 0.47\n-0.80\n0.80\n0.91\n2.48\nT5VQVAE(L=04) 0.59\n-0.56\n0.84\n0.76\n2.13\nT5VQVAE(L=03) 0.65\n-0.42\n0.85\n0.68\n1.97\nT5VQVAE(L=02) 0.70\n-0.21\n0.88\n0.65\n1.91\nTable 11: T5VQVAE(small): Different sizes of code-\nbook and optimal point.\nExpotential\nMoving\nAverage\n(EMA)\nLet\n{E(xk,1), ..., E(xk,nk)} be the set of word embed-\nding xk,i belonging to the zk. The optimal value\nfor zk is the average of elements in this set, which\ncan be described as:\nzk = 1\nnk\nnk\nX\ni\nE(xi)\nHowever, we cannot use this to update zk since we\nusually work on mini-batches. Instead, we can use\nEMA to update zk.\nN(t)\nk\n:= N(t−1)\nk\n× λ + n(t)\nk (1 − λ)\nm(t)\nk\n:= m(t−1)\nk\n× λ +\nX\ni\nE(xk,i)\nzk := m(t)\nk\nN(t)\nk\nWhere λ is 0.99 following the setup of (Van\nDen Oord et al., 2017).\nOptimus and DELLA training setup\nBoth\nof them can be trained via the evidence lower\nbound (ELBO) on the log-likelihood of the data\nx (Kingma and Welling, 2013). To avoid KL van-\nishing issue, which refers to the Kullback-Leibler\n(KL) divergence term in the ELBO becomes very\nsmall or approaches zero, we select the cyclical\nschedule to increase weights of KL β from 0 to 1\n(Fu et al., 2019) and KL thresholding scheme (Li\net al., 2019) that chooses the maximal between KL\nand threshold λ. The final objective function can\nbe described as follows:\nLVAE =Eqϕ(z|x)\nh\nlog pθ(x|z)\ni\n− β max [λ, KLqϕ(z|x)||p(z)]\nB\nVisualization\nIn Figure 3, we visualise the latent space of\nT5VQVAE via t-distributed Stochastic Neighbor\nEmbedding (T-SNE) (Van der Maaten and Hinton,\n2008) to analyse the organization of key semantic\nclusters. Specifically, we visualize the clusters of\ntoken embeddings with the same role-content, dif-\nferent roles, and the same content with different\nroles, respectively. We can observe that under the\nsame role-content (left), the latent token embed-\ndings are widely distributed in the latent space as\nthe representation of the role-content is affected by\nthe context, which indicates poor disentanglement.\nFor different roles (middle), there are big overlaps\nbetween different semantic roles, which indicates\npoor disentanglement of semantic role structure.\nFor the same content with different roles (right), it\ncan be observed that different semantic role clusters\nare fully overlapped. Those visualizations indicate\nthat the semantic information is naturally entangled\nafter an attention-based Encoder.\nFigure 3: t-SNE plot of the T5VQVAE latent space.\nLeft: same role-content(PRED-is, ARG2-animal). Mid-\ndle: different role-content(ARG0-PRED-ARG1, ARG1-\nPRED-ARG2). Right: different roles with same content\n(ARG0, 1, 2 - animal, ARG0, 1, 2 - water).\nC\nAutoEncoding Task\nWe provide more reconstructed explanations with\nlow BLEURT scores in Table 12. we manually\nevaluate its performance and show the common\nissues in the AutoEncoding setup. (1) repetition:\nsome explanations that describe the synonym are\nsuffered from information loss. E.g., the prediction\nis the grand canyon is a kind of canyon where the\ngolden is the grand canyon is a kind of place. (2)\nwrong numerical token: the model cannot precisely\nreconstruct the numerical token. E.g., the speed of\nthe boat can be calculated by dividing the length\nof a boat compared with the golden: the speed of\nthe sailboat can be calculated by dividing 35 by 5.\nD\nText Transfer Task\nWe provide more traversal, interpolation, and arith-\nmetic examples in Tables 13,14, 15, and 16.\nE\nInference Task\nWe provide more examples in Tables 17 and 18.\nGolden Explanations\nPredicted Explanations\nBLEURT\nBLEU\nthe grand canyon is a kind of place\nthe grand canyon is a kind of canyon\n0.26\n0.87\na blood thinner can be used to treat people with\nheart attacks and strokes\na heart thinner can be used to treat people with\nblood and heart\n-0.05\n0.44\nthe plant offspring has yellow flowers\noffspring means offspring\n-1.30\n0.12\nlack is similar to ( low ; little )\nlittle means ( little ; little ) in quality\n-1.18\n0.44\npreserved means ( from the past ; from long ago\n)\npreserved means used to be ( preserved ; pre-\nserved ) from a long time\n-0.01\n0.50\nthe plant offspring has yellow flowers\noffspring means offspring\n-1.30\n0.12\nelectricity causes less pollution than gasoline\ngasoline causes less gasoline than gasoline\n-0.22\n0.66\ninsulin is a kind of hormone\ninsulin is made of insulin\n-0.31\n0.49\nliving things all require a producers for survival living things all require a living thing for survival 0.03\n0.77\ngravity causes nebulas to collapse\ngravity causes a sleef of an artery to collapse\n-1.30\n0.44\nout is synonymous with outside\noutward is synonymous with out\n-0.36\n0.80\nto prevent means to make it not happen\nto make means to not happen\n-0.74\n0.71\na branch is a kind of object\na branch is a kind of branch\n-0.03\n0.85\nforce requires energy\nforce means amount\n-0.40\n0.33\nspot means location\nplace means kind of place\n-0.14\n0.20\ngritty is similar to rough\ngrease is similar to grease\n-0.80\n0.60\nsidewalk means pavement\nbike means bike\n-0.62\n0.33\na gravel pit is a kind of environment\na gravel pit is a kind of gravel\n0.03\n0.87\na electron has a negative ( -1 ) electric charge\na electron has a negative ( electric charge ; nega-\ntive charge )\n0.23\n0.75\nfish is a kind of meat\nfish are a kind of fish\n-0.29\n0.66\njogging is similar to running\nrunning is a kind of running\n-0.23\n0.33\nthe speed of the sailboat can be calculated by\ndividing 35 by 5\nthe speed of the boat can be calculated by divid-\ning the length of a boat\n0.20\n0.60\nif an object has 0 mechanical energy then the\nobject will stop moving\nif an object has a mechanical energy then the\nobject has to move to 0\n0.09\n0.66\nTable 12: T5VQVAE(base): more examples with low BLEURT score.\nTraversal\nan animal requires warmth in cold environments\ndim0: animals usually maintain a safe distance from\npredators during the hibernation process\ndim0: animals usually require warmth in cold tempera-\ntures for survival\ndim0: animals must sense prey to survive / find food\ndim0: animals must sense food to survive in the cold\nenvironment\ndim1: animals must protect themselves ( against predators\n; from predators )\ndim1: animals with pacemakers must sense danger in\norder to eat prey\ndim1: animals with sensory organs provided shelter in\ncold environments\ndim1: animals with diabetes should be protected from\npredators in the water\ndim2: animals must sense ( predators ; food ) to survive\ndim2: animals must sense other animals for food / shelter\ndim2: animals must sense other animals for survival in\ncold environments\ndim2: animals with circulatory system have a positive\nimpact on themselves by breathing air\ndim4: animals with cold cardiovascular systems can\nsurvive in cold environments by breathing\ndim4: animals must sense prey to survive in cold environ-\nments\ndim4: animals must sense other animals for survival\nwhile they are at sea; in an environment\ndim4: animals usually nurse their offspring through the\nwinter\ndim5: animals must sense prey to survive and reproduce\ndim5: animals must sense food to find food\ndim5: animals must sense prey in order to survive survival\nin the cold environment\ndim5: animals require warmth in cold environments to (\nsurvive ; find food )\ndim6: animals must sense food in order to survive in cold\nenvironments\ndim6: animals must sense prey in order to survive / find\nfood\ndim6: animals with heat - circulatory system must cool\nthemselves in cold environments\ndim6: animals must sense prey to survive in cold environ-\nments\nTable 13: Traversal for Optimus latent space.\nTraversal\nan astronaut requires the oxygen in a spacesuit backpack to breathe\ndim1: an astronaut requires the oxygen in a spacesuit\nbackpack to breathe\ndim1: an organism requires the oxygen in a spacesuit\nbackpack to breathe\ndim1: an animal requires the oxygen in a spacesuit back-\npack to breathe\ndim1: an student requires the oxygen in a spacesuit back-\npack to breathe\ndim2: an astronaut requires the oxygen in a spacesuit\nbackpack to breathe\ndim2: an astronaut can wear the oxygen in a spacesuit\nbackpack to breathe\ndim2: an astronaut requires the oxygen in a spacesuit\nbackpack to breathe\ndim2: an astronaut requires the oxygen in a spacesuit\nbackpack to breathe\ndim1: astronauts wear spacesuits in the space station to\navoid the issue of heat loss after a space probe\ndim1: astronauts wear spacesuits in the space environ-\nment to protect the astronaut from harmful chemical\nreactions\ndim1: astronauts wear spacesuits in the space station to\nkeep the body warm\ndim1: astronauts wear spacesuits in the spacesuit worn\nby the astronauts to take in oxygen\ndim2: astronauts wear spacesuits in the space station in\nspace\ndim2: astronauts conducting the orbit of the moon in\nspace during the last stage of a lunar cell might cause\ndirect sunlight to lands on the moon\ndim2: astronauts wear on the body the oxygen in a space-\nsuit backpack after the spacecraft escapes the atmosphere\ndim2: astronauts wear spacesuits in the space station to\nprotect the body of an astronaut\nTable 14: Traversal comparison (left: T5VQVAE(base), right: Optimus).\nTraversal\npedals are a kind of object\ndim0: pedals are a kind of pedal\ndim0: pedaling is a kind of object\ndim0: a pedal is a kind of object\ndim0: leather is a kind of object\ndim1: a pedal is a kind of object\ndim1: pedals are a kind of object\ndim1: pedals are a kind of object\ndim1: a pedal is a kind of object\ndim0: objects are a kind of kind of nonliving thing\ndim0: rust is a kind of object\ndim0: objects are a kind of kind of heavy object\ndim0: rust is a kind of object\ndim1: objects are a kind of kind of nonliving thing\ndim1: rust is a kind of object\ndim1: bones are a kind of object\ndim1: objects are a kind of kind of small particle\ntravel means to move\ndim2: travel means move\ndim2: travel is similar to move\ndim2: travel is used to move\ndim2: travel is a kind of movement\ndim3: travel means to move\ndim3: travel means stay\ndim3: travel means to withstand travel\ndim3: travel means to be transported\ndim2: to move means to move\ndim2: to pedal means to move something faster\ndim2: to move means to move\ndim2: to move means to move\ndim3: to raise means to move something\ndim3: to pedal means to move faster\ndim3: to move means to move\ndim3: to pedal means to move quickly\nTable 15: Traversal comparison (top: T5VQVAE(base),\nbottom: Optimus). We can observe that T5VQVAE can\nprovide better semantic control than Optimus.\nArithmetic\nxA: a forest is a kind of land\nxB: a tornado is narrow in width\nT5VQVAE: a tornado is small in land\nOptimus: plants are a kind of resource\nxA: a rabbit is a kind of animal that may live in a\nmeadow xB: december is during the winter in the\nnorthern hemisphere\nT5VQVAE: december is a kind of animal that may\nbe in a winter\nOptimus: a animal can usually find something to eat\nxA: fossil fuels are formed from dead prehistoric\norganisms xB: orange is a kind of color\nT5VQVAE: orange fossil fuels are formed from dead\nprey\nOptimus: prehistoric organisms developed defenses\nagainst disease by compacting and burying large\namounts of remains\nxA: waves travel outward from the source xB:\nwater is made of matter\nT5VQVAE: water points away from the source\nOptimus: transverse waves cause the person to move\nperpendicular to the direction of the wave\nxA: rotation is a kind of motion xB: Leo is a kind\nof constellation\nT5VQVAE: Leo is a kind of motion\nOptimus: friction occurs when two object colliding\ncauses the speed of their movement to increase\nxA: the milky way is a kind of galaxy xB: a rock\nis usually a solid\nT5VQVAE: the milky way is usually a solid\nOptimus: x -sex cells are inherited characteristics\nTable 16: Addition Arithmetic comparison. We can\nobserve that the T5VQVAE can hold the semantic infor-\nmation of both sentences after addition arithmetic.\nArgument and Verb substitution\nP1: heat is a kind of energy\nP2: flowing can be a kind of transfer of energy\nPred: flowing can be a kind of transfer of heat\nP1: aluminum is always nonmagnetic\nP2: aluminum cans are made of aluminums\nPred: aluminum cans are nonmagnetic\nP1: ground water is a kind of water\nP2: a desert environment is low in availability of\nwater / availability of food\nPred: a desert environment is low in availability of\nground water / availability of food\nP1: marine fossils are fossils of water animals\nP2: marine fossils are found in mountains\nPred: marine fossils are fossils of marine animals\nGold: fossils of water animals are found in mountains\nP1: in order to breath air you must take in air\nP2: a lung is a kind of organ for breath air\nPred: a lung is a kind of organ for taking in air\nP1: running is a kind of movement\nP2: an animal requires energy to move\nPred: an animal requires energy to run\nP1: eating is a kind of method for consuming\nP2: decomposers obtain nutrients by eating waste\nPred: decomposers obtain nutrients by consuming\nwaste\nP1: rolling on a surface is a kind of moving on a\nsurface\nP2: the rubber ball is rolling across a surface\nPred: the rubber ball is moving across a surface\nTable 17: T5VQVAE(base): quasi-symbolic inference\nexamination in AutoEncoder (Top: argument substitu-\ntion, Bottom: Verb substitution).\nFurther specification and Conjunction\nP1: a plant requires energy to make food\nP2: plants get energy from sunlight\nPred: plants get energy from sunlight to make food\nP1: a mountain is made of rocks\nP2: a mountain range contains mountains\nPred: a mountain range contains mountains made of\nrocks\nP1: a dinosaur is a kind of animal\nP2: dinosaurs are cold-blooded\nPred: dinosaurs are cold - blooded animals\nP1: gravity causes orbits\nP2: planets in the solar system orbit the sun\nPred: gravity causes planets in the solar system to\norbit the sun\nP1: scattering light decreases visibility\nP2: water droplets scatter light\nPred: water droplets scattering light decreases air\ndroplets\nGold: water droplets scattering light decreases the\nvisibility\nP1: sound travels slowest through gas\nP2: sound travels fastest through solid\nPred: sound waves travel fastest through gas and gas\nGold: sound travels fastest through solid and slowest\nthrough gas\nP1: matter in the solid phase has definite volume\nP2: matter in the solid phase has definite shape\nPred: matter in the solid phase has definite shape and\nvolume\nP1: a plant requires nutrients from soil to grow\nP2: a plant requires water from soil to grow\nPred: a plant requires water and nutrients from soil\nto grow\nP1: a hurricane has high wind speed\nP2: a hurricane has large amount of rain\nPred: a hurricane has high wind speed and large\namount of rain\nP1: fungi can be multicellular\nP2: fungi have no chlorophyll\nPred:\nfungi have no chlorophyll and can be\nmulticellular\nTable 18: T5VQVAE(base): quasi-symbolic inference\nexamination in AutoEncoder (Top: further specification,\nBottom: conjunction).\n"
}