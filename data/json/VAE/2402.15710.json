{
    "optim": "A Statistical Analysis of Wasserstein Autoencoders for Intrinsically Low-dimensional Data Saptarshi Chakraborty∗1 and Peter L. Bartlett†1,2,3 1Department of Statistics, UC Berkeley 2Department of Electrical Engineering and Computer Sciences, UC Berkeley 3Google DeepMind Abstract Variational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful tool for understanding unknown distributions based on limited samples. This popularity stems partly from their impressive performance and partly from their ability to provide meaningful feature representa- tions in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve model efficiency but also interpretability. However, there has been limited focus on analyzing their sta- tistical guarantees. The matter is further complicated by the fact that the data distributions to which WAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional structure within a high-dimensional feature space, which current theory does not adequately account for, rendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this paper, we show that WAEs can learn the data distributions when the network architectures are properly chosen. We show that the convergence rates of the expected excess risk in the number of samples for WAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of the data distribution. 1 Introduction The problem of understanding and possibly simulating samples from an unknown distribution only through some independent realization of the same is a key question for the machine learning community. Parallelly with the appearance of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational Autoencoders (Kingma and Welling, 2014) have also gained much attention not only due to their useful feature representation properties in the latent space but also for data generation capabilities. It is important to note that in GANs, the generator network learns to create new samples that are similar to the training ∗email: saptarshic@berkeley.edu †email: peter@berkeley.edu 1 arXiv:2402.15710v1  [cs.LG]  24 Feb 2024 Chakraborty and Bartlett data by fooling the discriminator network. However, GANs and their popular variants do not directly provide a way to manipulate the generated data or explore the latent space of the generator. On the other hand, a VAE learns a latent space representation of the input data and allows for interpolation between the representations of different samples. Several variants of VAEs have been proposed to improve their generative performance. One popular variant is the conditional VAE (CVAE) (Sohn et al., 2015), which adds a conditioning variable to the generative model and has shown remarkable empirical success. Other variants include InfoVAE (Zhao et al., 2017), β-VAE (Higgins et al., 2017), and VQ-VAE (Van Den Oord et al., 2017), etc., which address issues such as disentanglement, interpretability, scalability, etc. Recent works have shown the effectiveness of VAEs and their variants in a variety of applications, including image (Gregor et al., 2015) and text generation (Yang et al., 2017), speech synthesis (Tachibana et al., 2018), and drug discovery (G´omez-Bombarelli et al., 2018). A notable example is the DALL-E model (Ramesh et al., 2021), which uses a VAE to generate images from textual descriptions. However, despite their effectiveness in unsupervised representation learning, VAEs have been heavily criticized for their poor performance in approximating multi-modal distributions. Influenced by the superior performance of GANs, researchers have attempted to leverage this advantage of adversarial losses by incor- porating them into VAE objective (Makhzani et al., 2016; Mescheder et al., 2017). Wasserstein Autoencoder (WAEs) (Tolstikhin et al., 2018) tackles the problem from an optimal transport viewpoint. Incorporating such a GAN-like architecture, not only preserves the latent space representation that is unavailable in GANs but also enhances data generation capabilities. Both VAEs and WAEs attempt to minimize the sum of a reconstruction cost and a regularizer that penalizes the difference between the distribution induced by the encoder and the prior distribution on the latent space. While VAEs force the encoder to match the prior distribution for each input example, which can lead to overlapping latent codes and reconstruction issues, WAEs force the continuous mixture of the encoder distribution over all input examples to match the prior distribution, allowing different examples to have more distant latent codes and better reconstruction results. Furthermore, the use of the Wasserstein distance allows WAEs to incorporate domain-specific constraints into the learning process. For example, if the data is known to have a certain structure or topology, this information can be used to guide the learning process and improve the quality of generated samples. This results in a more robust model that can handle a wider range of distributions, including multimodal and heavy-tailed distributions. While VAE and its variants have demonstrated empirical success, little attention has been given to analyzing their statistical properties. Recent developments from an optimization viewpoint include Rolinek et al. (2019), who showed VAEs pursue Principal Component Analysis (PCA) embedding under certain situations, and Koehler et al. (2022), who analyzed the implicit bias of VAEs under linear activation with two layers. For explaining generalization, Tang and Yang (2021) proposed a framework for analyzing excess risk 2 Chakraborty and Bartlett for vanilla VAEs through M-estimation. When having access to n i.i.d. samples from the target distribution, Chakrabarty and Das (2021) derived a bound based on the Vapnik-Chervonenkis (VC) dimension, providing a guarantee of O(n−1/2)-convergence with a non-zero margin of error, even under model specification. However, their analysis is limited to a parametric regime under restricted assumptions and only considers a theoretical variant of WAEs, known as f-WAEs (Husain et al., 2019), which is typically not implemented in practice. Despite recent advancements in the understanding of VAEs and their variants, existing analyses fail to account for the fundamental goal of these models, i.e. to understand the data generation mechanism where one can expect the data to have an intrinsically low-dimensional structure. For instance, a key application of WAEs is to understand natural image generation mechanisms and it is believed that natural images have a low-dimensional structure, despite their high-dimensional pixel-wise representation (Pope et al., 2020). Furthermore, the current state-of-the-art views the problem only through a classical learning theory approach to derive O(n−1/2) or faster rates (under additional assumptions) ignoring the model misspecification error. Thus, such rates do not align with the well-known rates for classical non-parametric density estimation approaches (Kim et al., 2019). Additionally, these approaches only consider the scenario where the network architecture is fixed, but in practice, larger models are often employed for big datasets. In this paper, we aim to address the aforementioned shortcomings in the current literature and bridge the gap between the theory and practice of WAEs. Our contributions include: • We propose a framework to provide an error analysis of Wasserstein Autoencoders (WAEs) when the data lies in a low-dimensional structure in the high-dimensional representative feature space. • Informally, our results indicate that if one has n independent and identically distributed (i.i.d.) samples from the target distribution, then under the assumption of Lipschitz-smoothness of the true model, if the corresponding networks are properly chosen, the error rate for the problem scales as ˜O \u0010 n− 1 2+dµ \u0011 , where, dµ is the upper Minkowski dimension of the support of the target distribution. • The networks can be chosen as having O(nγe) many weights for the encoder and O(nγg) for the generator, where, γe, γg ≤ 1 and only depend on dµ and ℓ (dimension of the latent space), respectively. Furthermore, the values of γe and γg decrease as the true model becomes smoother. • We show that one can ensure encoding and decoding guarantees, i.e. the encoded distribution is close enough to the target latent distribution, and the generator maps back the encoded points close to the original points. Under additional regularity assumptions, we show that the approximating push- forward measure, induced by the generator, is close to the target distribution, in the Wasserstein sense, almost surely. 3 Chakraborty and Bartlett 2000 4000 6000 8000 10000 # Datapoints 150 200 250 300 350 FID Score d_int = 2 d_int = 16 (a) FID scores for GAN- WAE 2000 4000 6000 8000 10000 # Datapoints 0.02 0.04 0.06 Reconstruction Error d_int = 2 d_int = 16 (b) Reconstruction error for GAN-WAE 2000 4000 6000 8000 10000 # Datapoints 150 200 250 300 FID Score d_int = 2 d_int = 16 (c) FID scores for MMD- WAE 2000 4000 6000 8000 10000 # Datapoints 0.005 0.010 0.015 0.020 0.025 Reconstruction Error d_int = 2 d_int = 16 (d) Reconstruction error for MMD-WAE Figure 1: Average generalization error (in terms of FID scores) and reconstruction test errors for different values of n for GAN and MMD variants of WAE. The error bars denote the standard deviation out of 10 replications. 2 A Proof of Concept Before we theoretically explore the problem, we discuss an experiment to demonstrate that the error rates for WAEs depend primarily only on the intrinsic dimension of the data. Since it is difficult to assess the intrinsic dimensionality of natural images, we follow the prescription of Pope et al. (2020) to generate low- dimensional synthetic images. We use a pre-trained Bi-directional GAN (Donahue et al., 2017) with 128 latent entries and outputs of size 128 × 128 × 3, trained on the ImageNet dataset (Deng et al., 2009). Using the decoder of this pre-trained BiGAN, we generate 11, 000 images, from the class, soap-bubble where we fix most entries of the latent vectors to zero leaving only d int free entries. We take d int to be 2 and 16, respectively. We reduce the image sizes to 28 × 28 for computational ease. We train a WAE model with the standard architecture as proposed by Tolstikhin et al. (2018) with the number of training samples varying in {2000, 4000, . . . , 10000} and keep the last 1000 images for testing. For the latent distribution, we use the standard Gaussian distribution on the latent space R8 and use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.0001. We also take λ = 10 for the penalty on the dissimilarity in objective (4). After training for 10 epochs, we generate 1000 sample images from the distribution ˆG♯ν (see Section 3 for notations) and compute the Frechet Inception Distance (FID) (Heusel et al., 2017) to assess the quality of the generated samples with respect to the target distribution. We also compute the reconstruction error for these test images. We repeat the experiment 10 times and report the average. The experimental results for both variants of WAE, i.e. the GAN and MMD are shown in Fig. 1. It is clear from Fig. 1 that the error rates for d int = 2 is lower than for the case d int = 16. The codes for this experimental study can be found at https://github.com/SaptarshiC98/WAE. 4 Chakraborty and Bartlett 3 Background 3.1 Notations and some Preliminary Concepts This section introduces preliminary notation and concepts for theoretical analyses. Notation We use notations x ∨ y := max{x, y} and x ∧ y := min{x, y}. T♯µ denotes the push-forward of measure µ by the map T. For function f : S → R, and probability measure γ on S, let ∥f∥Lp(γ) := \u0000R S |f(x)|pdγ(x) \u00011/p. Similarly, ∥f∥L∞(A) := supx∈A |f(x)|. For any function class F, and distributions P and Q, ∥P − Q∥F = supf∈F | R fdP − R fdQ| denotes the Integral Probability Metric (IPM) w.r.t. F. We say An ≲ Bn (also written as An = O(Bn)) if there exists C > 0, independent of n, such that An ≤ CBn. Similarly, we use the notation, An ≾ Bn (also written as An = ˜O(Bn)) if An ≤ CBn logC(en), for some C > 0. We say An ≍ Bn, if An ≲ Bn and Bn ≲ An. For a function f : Rd1 → Rd2, we write, ∥f∥Lip = supx̸=y ∥f(x)−f(y)∥2 ∥x−y∥2 . Definition 1 (Neural networks). Let L ∈ N and {Ni}i∈[L] ⊂ N. Then a L-layer neural network f : Rd → RNL is defined as, f(x) = AL ◦ σL−1 ◦ AL−1 ◦ · · · ◦ σ1 ◦ A1(x) (1) Here, Ai(y) = Wiy + bi, with Wi ∈ RNi×Ni−1 and bi ∈ RNi−1, with N0 = d. σj is applied component-wise. Here, {Wi}1≤i≤L are known as weights, and {bi}1≤i≤L are known as biases. {σi}1≤i≤L−1 are known as the activation functions. Without loss of generality, one can take σℓ(0) = 0, ∀ ℓ ∈ [L−1]. We define the following quantities: (Depth) L(f) := L is known as the depth of the network; (Number of weights) The number of weights of the network f is denoted as W(f). NN{σi}i∈[L−1](L, W, R) = {f of the form (1) : L(f) ≤ L, W(f) ≤ W, sup x∈Rd ∥f(x)∥∞ ≤ R}. If σj(x) = x ∨ 0, for all j = 1, . . . , L − 1, we denote NN{σi}1≤i≤L−1(L, W, R) as RN(L, W, R). We often omit R in cases where it is clear that R is bounded by a constant. Definition 2 (H¨older functions). Let f : S → R be a function, where S ⊆ Rd. For a multi-index s = (s1, . . . , sd), let, ∂sf = ∂|s|f ∂xs1 1 ...∂x sd d , where, |s| = Pd ℓ=1 sℓ. We say that a function f : S → R is β-H¨older (for β > 0) if ∥f∥Hβ := X s:0≤|s|<⌊β⌋ ∥∂sf∥∞ + X s:|s|=⌊β⌋ sup x̸=y ∥∂sf(x) − ∂sf(y)∥ ∥x − y∥β−⌊β⌋ < ∞. If f : Rd1 → Rd2, then we define ∥f∥Hβ = Pd2 j=1 ∥fj∥Hβ. For notational simplicity, let, Hβ(S1, S2, C) = {f : S1 → S2 : ∥f∥Hβ ≤ C}. Here, both S1 and S2 are both subsets of a real vector spaces. 5 Chakraborty and Bartlett Definition 3 (Maximum Mean Discrepancy (MMD)). Let HK be the Reproducible Kernel Hilbert Space (RKHS) corresponding to the reproducing kernel K(·, ·), defined on Rd. Let the corresponding norm in this RKHS be ∥ · ∥HK. The Maximum Mean Discrepancy between two distributions P and Q is defined as: MMDK(P, Q) = supf:∥f∥HK≤1 \u0000R fdP − R fdQ \u0001 . 3.2 Wasserstein Autoencoders Let µ be a distribution in the data-space X = [0, 1]d and Z = [0, 1]ℓ be the latent space. In Wasserstein Autoencoders (Tolstikhin et al., 2018), one tries to learn a generator map, G : Z → X and an encoder map E : X → Z by minimizing the following objective, V (µ, ν, G, E) = Z c(x, G ◦ E(x))dµ(x) + λdiss(E♯µ, ν). (2) Here, λ > 0 is a hyper-parameter, often tuned based on the data. The first term in (2) aims to minimize a reconstruction error, i.e. the decoded value of the encoding should approximately result in the same value. The second term ensures that the encoded distribution is close to a known distribution ν that is easy to sample from. The function c(·, ·)-is a loss function on the data space. For example, Tolstikhin et al. (2018) took c(x, y) = ∥x − y∥2 2. diss(·, ·) is a dissimilarity measure between probability distributions defined on the latent space. Tolstikhin et al. (2018) recommended either a GAN-based dissimilarity measure or a Maximum Mean Discrepancy (MMD)-based measure (Gretton et al., 2012). In this paper, we will consider the special cases, where this dissimilarity measure is taken to be the Wasserstein-1 metric, which is the dissimilarity measure for WGANs (Arjovsky et al., 2017; Gulrajani et al., 2017) or the squared MMD-metric. In practice, however, one does not have access to µ but only a sample {Xi}i∈[n], assumed to be inde- pendently generated from µ. Let ˆµn be the empirical measure based on the data. One then minimizes the following empirical objective to estimate E and G. V (ˆµn, ν, G, E) = Z c(x, G ◦ E(x))dˆµn(x) + λd diss(E♯ˆµn, ν). (3) Here, d diss(·, ·) is an estimate of diss(·, ·), based only on the data, {Xi}i∈[n]. For example, if diss(·, ·) is taken to be the Wasserstein-1 metric, then, d diss(E♯ˆµn, ν) = W1(E♯ˆµn, ν). On the other hand, if diss(·, ·) is taken to be the MMD2 K-measure, one can take, d diss(E♯ˆµn, ν) = 1 n(n − 1) X i̸=j K(E(Xi), E(Xj)) + EZ,Z′∼νK(Z, Z′) − 2 n n X i=1 Z K(E(Xi), z)dν(z). Of course, in practice, one does a further estimation of the involved dissimilarity measure through taking an estimate ˆνm, based on m i.i.d samples {Zj}j∈[m] from ν, i.e. ˆνm = 1 m Pm j=1 δZj. In this case the estimate of V in (2) is given by, V (ˆµn, ˆνm, G, E) = Z c(x, G ◦ E(x))dˆµn(x) + λd diss(E♯ˆµn, νm). (4) 6 Chakraborty and Bartlett If diss(·, ·) is taken to be the Wasserstein-1 metric, then, d diss(E♯ˆµn, ˆνm) = W1(E♯ˆµn, ˆνm). On the other hand, if diss(·, ·) is taken to be the MMD2 K-measure, one can take, d diss(E♯ˆµn, ˆνm) = 1 n(n − 1) X i̸=j K(E(Xi), E(Xj)) + 1 m(m − 1) X i̸=j K(Zi, Zj) − 2 nm n X i=1 m X j=1 K(E(Xj), Zj). Suppose that ∆opt > 0 is the optimization error. The empirical WAE estimates satisfy the following properties: ( ˆGn, ˆEn) ∈ \u001a G ∈ G, E ∈ E : V (ˆµn, ν, G, E) ≤ inf G∈G,E∈E V (ˆµn, ν, G, E) + ∆opt \u001b (5) ( ˆGn,m, ˆEn,m) ∈ \u001a G ∈ G, E ∈ E : V (ˆµn, ˆνn, G, E) ≤ inf G∈G,E∈E V (ˆµn, ˆνm, G, E) + ∆opt \u001b . (6) The functions in G and E are implemented through neural networks with ReLU activation RN(Lg, Wg) and RN(Le, We), respectively. 4 Intrinsic Dimension of Data Distribution Real data is often assumed to have a lower-dimensional structure within the high-dimensional feature space. Various approaches have been proposed to characterize this low dimensionality, with many using some form of covering number to measure the effective dimension of the underlying measure. Recall that the ϵ-covering number of S w.r.t. the metic ϱ is defined as N(ϵ; S, ϱ) = inf{n ∈ N : ∃ x1, . . . xn such that ∪n i=1 Bϱ(xi, ϵ) ⊇ S}, with Bϱ(x, ϵ) = {y : ϱ(x, y) < ϵ}. We characterize this low-dimensional nature of the data, through the (upper) Minkowski dimension of the support of µ. We recall the definition of Minkowski dimensions, Definition 4 (Upper Minkowski dimension). For a bounded metric space (S, ϱ), the upper Minkwoski di- mension of S is defined as dimM(S, ϱ) = lim supϵ↓0 log N (ϵ; S, ϱ) log(1/ϵ) . Throughout this analysis, we will assume that ϱ is the ℓ∞-norm and simplify the notation to dimM(S). dimM(S, ϱ) essentially measures how the covering number of S is affected by the radius of balls covering that set. As the concept of dimensionality relies solely on covering numbers and doesn’t require a smooth mapping to a lower-dimensional Euclidean space, it encompasses both smooth manifolds and even highly irregular sets like fractals. In the literature, Kolmogorov and Tikhomirov (1961) provided a comprehensive study on the dependence of the covering number of different function classes on the underlying Minkowski dimension of the support. Nakada and Imaizumi (2020) showed how deep regression learners can incorporate this low-dimensionality of the data that is also reflected in their convergence rates. Recently, Huang et al. (2022) showed that WGANs can also adapt to this low-dimensionality of the data. For any measure µ on [0, 1]d, we use the notation dµ := dimM(supp(µ)). When the data distribution is supported on a low-dimensional structure in the nominal high-dimensional feature space, one can expect dµ ≪ d. 7 Chakraborty and Bartlett It can be observed that the image of a unit hypercube under a H¨older map has a Minkowski dimension that is no more than the dimension of the pre-image divided by the exponent of the H¨older map. Lemma 5. Let, f ∈ Hγ \u0000A, [0, 1]d2, C \u0001 , with A ⊆ [0, 1]d1. Then, dimM (f (A)) ≤ dimM(A)/(γ ∧ 1). 5 Theoretical Analyses 5.1 Assumptions and Error Decomposition To facilitate theoretical analysis, we assume that the data distributions are realizable, meaning that a “true” generator and a “true” encoder exist. Specifically, we make the assumption that there is a true smooth encoder that maps the µ to ν, and the left inverse of this true encoder exists and is also smooth. Formally, A1. There exists ˜G ∈ Hαg([0, 1]d, [0, 1]ℓ, C) and ˜E ∈ Hαe([0, 1]ℓ, [0, 1]d, C), such that, ˜E♯µ = ν and ( ˜G ◦ ˜E)(·) = id(·), a.e. [µ]. It is also important to note that A1 entails that the manifold has a single chart, in a probabilistic sense, which is a strong assumption. Naturally, when it comes to GANs, one can work with a weaker assumption as the learning task becomes notably much simpler as one does not have to learn an inverse map to the latent space. A similar problem, while analyzing autoencoders, was faced by Liu et al. (2023) where they tackled the problem by considering chart-autoencoders, which have additional components in the network architecture, compared to regular autoencoders. A similar approach of employing chart-based WAEs could be proposed and subjected to rigorous analysis. This potential avenue could be an intriguing direction for future research. One immediate consequence of assumption A1 ensures that the generator maps ν to the target µ. We can also ensure that the latent distribution remains unchanged if one passes it through the generator and maps it back through the encoder. Furthermore, the objective function (2) at this true encoder-generator pair takes the value, zero, as expected. Proposition 6. Under assumption A1, the following holds: (a) ˜G♯ν = µ, (b) ( ˜E◦ ˜G)♯ν = ν, (c) V (µ, ν, ˜G, ˜E) = 0. From Lemma 5, It is clear that dµ = dimM (supp(µ)) ≤ dimM \u0010 ˜G \u0000[0, 1]ℓ\u0001\u0011 ≤ max {ℓ/(αg ∧ 1), d} . If ℓ ≪ d and αg is not very small, then, dµ = (αg ∧ 1)−1ℓ ≪ d. Thus, the usual conjecture of dµ ≪ d can be modeled through assumption A1 when the latent space has a much smaller dimension and the true generator is well-behaved, i.e. αg is not too small. A key step in the theoretical analysis is the following oracle inequality that bounds the excess risk in terms of the optimization error, misspecification error, and generalization error. 8 Chakraborty and Bartlett Lemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Then the following hold: V (µ, ν, ˆGn, ˆEn) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup E∈E |d diss(E♯ˆµn, ν) − diss(E♯µ, ν)|. (7) V (µ, ν, ˆGn,m, ˆEn,m) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup E∈E |d diss(E♯ˆµn, ˆνm) − diss(E♯µ, ν)|. (8) Here, ∆miss = infG∈G,E∈E V (µ, ν, G, E) denotes the misspecification error for the problem. For our theoretical analysis, we need to ensure that the used kernel in the MMD and the loss function c(·, ·) are regular enough. To impose such regularity, we assume the following: A2. We assume that, (a) for some B > 0, K(x, y) ≤ B2, for all x, y ∈ [0, 1]ℓ; (b) For some τk, |K(x, y) − K(x′, y′)| ≤ τk(∥x − x′∥2 + ∥y − y′∥2). A3. The loss function c(·, ·) is Lipschitz on [0, 1]d ×[0, 1]d, i.e. |c(x, y)−c(x′, y′)| ≤ τc(∥x−x′∥2 +∥y −y′∥2) and c(x, y) ≤ Bc, for all, x, y ∈ [0, 1]d. 5.2 Main Result Under assumptions A1–3, one can control the expected excess risk of the WAE problem for both the W1 and MMD dissimilarities. The main idea is to select appropriate sizes for the encoder and generator networks, that minimize both the misspecification errors and generalization errors to bound the expected excess risk using Lemma 7. Theorem 8 shows that one can appropriately select the network size in terms of the number of samples available, i.e n, to achieve a trade-off between the generalization and misspecification errors as selecting a larger network facilitates better approximation but makes the generalization gap wider and vice-versa. The main result of this paper is stated as follows. Theorem 8. Suppose that assumptions A1–3 hold and ∆opt ≤ ∆ for some fixed non-negative threshold ∆. Furthermore, suppose that s > dµ. Then we can find n0 ∈ N and β > 0, that might depend on d, ℓ, αg, αe, ˜G and ˜E, such that if n ≥ n0, we can choose G = RN(Lg, Wg) and E = RN(Le, We), with, Le ≤ β log n, We ≤ βn s 2αe+s log n, Lg ≤ β log n and Wg ≤ βn ℓ αe(αg∧1)+ℓ log n, then, for the estimation problem (5), (a) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) ,ℓ \u001b log2 n, for diss(·, ·) = W1(·, ·), (b) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 2+max \u001a ℓ αg , s αe(αg∧1) \u001b log2 n, for diss(·, ·) = MMD2 K(·, ·). Furthermore, for the estimation problem (6), if m ≥ n ∨ n \u0010 max n 2+ ℓ αg ,2+ dµ αe(αg∧1) ,ℓ o\u0011−1(ℓ∨2) (c) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) ,ℓ \u001b log2 n, for diss(·, ·) = W1(·, ·), (d) EV (µ, ν, ˆGn,m, ˆEn,m) ≲ ∆ + n − 1 2+max \u001a ℓ αg , s αe(αg∧1) \u001b log2 n, for diss(·, ·) = MMD2 K(·, ·). Before we proceed, we observe some key consequences of Theorem 8. 9 Chakraborty and Bartlett Remark 9 (Number of Weights). We note that Theorem 8 suggests that one can choose the networks to have number of weights to be an exponent of n, which is smaller than 1. Moreover, this exponent only depends on the dimensions of the latent space and the intrinsic dimension of the data. Furthermore, for smooth models i.e. αe and αg are large, one can choose smaller networks that require less many parameters as opposed to non-smooth models as also observed in practice since easier problems require less complicated networks. Remark 10 (Rates for Lipschitz models). For all practical purposes, one can assume that the dimension of the latent space is at least 2. If the true models are Lipschitz, i.e. if αe = αg = 1, then we can conclude that ℓ = dµ. Hence, for both models, we observe that the excess risk scales as ˜O(n− 1 2+dµ ), barring the poly-log factors. This closely matches rates for the excess risks for GANs (Huang et al., 2022). Remark 11 (Inference for Data on Manifolds). We recall that we call a set M is ˜d-regular w.r.t. the ˜d-dimensional Hausdorff measure H ˜d if H(Bϱ(x, r)) ≍ r ˜d, for all x ∈ M (see Definition 6 of Weed and Bach (2019)). It is known (Mattila, 1999) that if M is ˜d-regular, then the Minkowski dimension of M is ˜d. Thus, when supp(µ) is ˜d-regular, dµ = ˜d. Since compact ˜d-dimensional differentiable manifolds are ˜d-regular (Proposition 9 of Weed and Bach (2019)), this implies that for when supp(µ) is a compact differentiable ˜d-dimensional manifold, the error rates for the sample estimates scale as in Theorem 8, with dµ replaced with ˜d. A similar result holds when supp(µ) is a nonempty, compact convex set spanned by an affine space of dimension ˜d; the relative boundary of a nonempty, compact convex set of dimension ˜d + 1; or self-similar set with similarity dimension ˜d. 5.3 Related work on GANs To contextualize our contributions, we conduct a qualitative comparison with existing GAN literature. Notably, Chen et al. (2020) expressed the generalization rates for GAN when the data is restricted to an affine subspace or has a mixture representation with smooth push-forward measures; while Dahal et al. (2022) derived the convergence rates under the Wasserstein-1 distance in terms of the manifold dimension. Both Liang (2021) and Schreuder et al. (2021) study the expected excess risk of GANs for smooth generator and discriminator function classes. Liu et al. (2021) studied the properties of Bidirectional GANs, expressing the rates in terms of the number of data points, where the exponents depend on the full data and latent space dimensions. It is important to note that both Dahal et al. (2022) and Liang (2021) assume that the densities of the target distribution (either w.r.t Hausdorff or the Lebesgue measure) are bounded and smooth. In comparison, we do not make any assumption of the existence of density (or its smoothness) for the target distribution and consider the practical case where the generator is realized through neural networks as opposed to smooth functions as done by Liang (2021) and Schreuder et al. (2021). Diverging 10 Chakraborty and Bartlett from the hypotheses of Chen et al. (2020), we do not presuppose that the support of the target measure forms an affine subspace. Furthermore, the analysis by Liu et al. (2021) derives rates that depend on the dimension of the entire space and not the manifold dimension of the support of the data as done in this analysis. It is important to emphasize that Huang et al. (2022) arrived at a rate comparable to ours concerning WGANs (Arjovsky et al., 2017). While both studies share a common overarching approach in addressing the problem by bounding the error using an oracle inequality and managing individual terms, our method necessitates extra assumptions to guarantee the generative capability of WAEs, which does not apply to WGANs due to their simpler structure. Interestingly, our derived rates closely resemble those found in GAN literature. This suggests limited room for substantial improvement. However, demonstrating minimaxity remains a significant challenge and a promising avenue for future research. 5.4 Proof Overview From Lemma 7, it is clear that the expected excess risk can be bounded by the misspecification error ∆miss and the generalization gap, ∥ˆµn − µ∥F + λ supE∈E |d diss(E♯ˆµn, ν) − diss(E♯µ, ν)|. To control ∆miss, we first show that if the generator and encoders are chosen as G = RN(Wg, Lg) and E = RN(We, Le), with Le ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe e log(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg g log(1/ϵg) then, ∆miss ≲ ϵg + ϵαg∧1 e . On the other hand, we show that the generalization error is roughly p n−1WeLe log We log n + p n−1(We + Wg)(Le + Lg) log(We + Wg) log n, with additional terms depending on the estimator. Thus, the bounds in Lemma 7, leads to a bound roughly, ∆ + ϵg + ϵ αg∧1 e + p n−1WeLe log We log n + p n−1(We + Wg)(Le + Lg) log(We + Wg) log n. (9) By the choice of the networks, we can upper bound the above as a function of ϵg and ϵe and then minimize the expression w.r.t. these two variables to arrive at the bounds of Theorem 8. Of course, the bound in (9) changes slightly based on the estimates and the dissimilarity measure. We refer the reader to the appendix, which contains the details of the proof. 5.5 Implications of the Theoretical Results Apart from finding the error rates for the excess risk for the WAE problem, in what follows, we also ensure a few desirable properties of the obtained estimates. For simplicity, we ignore the optimization error and set ∆opt = 0. Encoding Guarantee Suppose we fix λ > 0, then, it is clear from Theorem 8 that EW1( ˆE♯µ, ν) ≲ n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) ,ℓ \u001b log2 n and EMMD2 K( ˆE♯µ, ν) ≲ n − 1 2+max \u001a ℓ αg , s αe(αg∧1) \u001b log2 n. We can not only char- acterize the expected rate of convergence of ˆE♯µ to ν but also can say that ˆE♯µ converges in distribution to ν, almost surely. This is formally stated in the following proposition. 11 Chakraborty and Bartlett Proposition 12. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·) and MMD2 K(·, ·) and the estimates (5) and (6), ˆE♯µ d−→ ν, almost surely. Therefore, if the number of data points is large, i.e., n is large, then the estimated encoded distribution ˆE♯µ will converge to the true target latent distribution ν almost surely, indicating that the latent distribution can be approximated through encoding with a high degree of accuracy. Decoding Guarantee One can also show that E R c(x, ˆG ◦ ˆE(x))dµ(x) ≤ EV (µ, ν, ˆG, ˆE), for both the estimates in (5) and (6). For simplicity, if we let c(x, y) = ∥x − y∥2 2, then, it can easily seen that, E∥id(·) − ˆG ◦ ˆE(·)∥2 L2(µ) → 0 as n → ∞, where id(x) = x is the identity map from Rd → Rd. Furthermore, it can be shown that, ∥id(·) − ˆG ◦ ˆE(·)∥2 L2(µ) a.s. −−→ 0 as stated in Corollary 13 Proposition 13. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·) and MMD2 K(·, ·) and the estimates (5) and (6), ∥id(·) − ˆG ◦ ˆE(·)∥2 L2(µ) a.s. −−→ 0. Proposition 13 guarantees that the generator is able to map back the encoded points to the original data if a sufficiently large amount of data is available. In other words, if one has access to a large number of samples from the data distribution, then the generator is able to learn a mapping, from the encoded points to the original data, that is accurate enough to be useful. Data Generation Guarantees A key interest in this theoretical exploration is whether one can guarantee that one can generate samples from the unknown target distribution µ, through the generator, i.e. whether ˆG♯ν is close enough to µ in some sense. However, one requires some additional assumptions (Chakrabarty and Das, 2021; Tang and Yang, 2021) on ˆG or the nature of convergence of ˆE♯µ to ν to ensure this. We present the corresponding results subsequently as follows. Before proceeding, we recall the definition of Total Variation (TV) distance between two measures γ1 and γ2, defined on Ω, as, TV (γ1, γ2) = supB∈B(Ω) |γ1(B) − γ2(B)|, where, B(Ω) denotes the Borel σ-algebra on Ω. Theorem 14. Suppose that assumptions A1–3 hold and TV ( ˆE♯µ, ν) → 0, almost surely. Then, ˆG♯ν d−→ µ, almost surely. We note that convergence in TV is a much stronger assumption than convergence in W1 or MMD in the sense that TV convergence implies weak convergence but not the other way around. Another way to ensure that ˆG♯ν converges to µ is to put some sort of regularity on the generator estimates. Tang and Yang (2021) imposed a Lipschitz assumption to ensure this, but one can also work with something weaker, such as uniform equicontinuity of the generators. Recall that we say a family of functions, F is uniformly equicontinuous if, for anyf ∈ F and for all ϵ > 0, there exists a δ > 0 such that, |f(x) − f(y)| ≤ ϵ, whenever, ∥x − y∥ ≤ δ. 12 Chakraborty and Bartlett Theorem 15. Suppose that assumptions A1–3 hold and let the family of estimated generators { ˆGn}n∈N be uniformly equicontinuous, almost surely. Then, ˆGn ♯ ν d−→ µ, almost surely. Uniformly Lipschitz Generators Suppose that diss(·, ·) = W1(·, ·). If one assumes that the estimated generators are uniformly Lipschitz, then, one can say that W1( ˆG♯ν, µ) is upper bounded by V (µ, ν, ˆG, ˆE), disregarding some constants.Thus, the same rate of convergence as in Theorem 8 holds for uniformly Lipschitz generator. We state this result formally as a corollary as follows. Corollary 16. Let diss(·, ·) = W1(·, ·) and suppose that the assumptions of Theorem 8 are satisfied and s > dµ. Also let supn∈N ∥ ˆGn∥Lip, supm,n∈N ∥ ˆGn,m∥Lip ≤ L, almost surely, for some L > 0. W1( ˆG♯ν, µ) ≲ V (µ, ν, ˆG, ˆE) for both estimators (5) and (6). It is important to note that although assumptions A1–3 do not directly guarantee either of these two conditions, it is reasonable to expect the assumptions made in Theorems 14 and 15 to hold in practice. This is because regularization techniques are commonly used to ensure the learned networks ˆE and ˆG are sufficiently well-behaved. These techniques can impose various constraints, such as weight decay or dropout, that encourage the networks to have desirable properties, such as smoothness or sparsity. Therefore, while the assumptions made in the theorems cannot be directly ensured by A1–3, they are likely to hold in practice with appropriate regularization techniques applied to the network training. It would be a key step in furthering our understanding to develop a similar error analysis for such regularized networks and we leave this as a promising direction for future research. 6 Discussions and Conclusion In this paper, we developed a framework to analyze error rates for learning unknown distributions using Wasserstein Autoencoders, especially when data points exhibit an intrinsically low-dimensional structure in the representative high-dimensional feature space. We characterized this low dimensionality with the so- called Minkowski dimension of the support of the target distribution. We developed an oracle inequality to characterize excess risk in terms of misspecification, generalization, and optimization errors for the problem. The excess risk bounds are obtained by balancing model-misspecification and stochastic errors to find proper network architectures in terms of the number of samples that achieve this tradeoff. Our framework allows us to analyze the accuracy of encoding and decoding guarantees, i.e., how well the encoded distribution approximates the target latent distribution, and how well the generator maps back the latent codes close to the original data points. Furthermore, with additional regularity assumptions, we establish that the approximating push-forward measure can effectively approximate the target distribution. 13 Chakraborty and Bartlett While our findings provide valuable insights into the theoretical characteristics of Wasserstein Autoen- coders (WAEs), it’s crucial to acknowledge that achieving accurate estimates of the overall error in practical applications necessitates the consideration of an optimization error term. However, the precise estimation of this term poses a significant challenge due to the non-convex and intricate nature of the optimization process. Importantly, our error analysis remains independent of this optimization error and can seamlessly integrate with analyses involving such optimization complexities. Future work in this direction might in- volve attempting to improve the derived bounds by replacing the Minkowski dimension with the Wasserstein (Weed and Bach, 2019) or entropic dimension (Chakraborty and Bartlett, 2024). Furthermore, the minimax optimality of the upper bounds derived remains an open question, offering opportunities for fruitful research in understanding the model’s theoretical properties from a statistical viewpoint. Exploring future directions in deep federated classification models can yield fruitful research avenues. Acknowledgment We gratefully acknowledge the support of the NSF and the Simons Foundation for the Collaboration on the Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639 and the NSF’s support of FODSI through grant DMS-2023505. References Anthony, M. and Bartlett, P. (2009). Neural network learning: Theoretical foundations. Cambridge Univer- sity Press. Arjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein generative adversarial networks. In Interna- tional Conference on Machine Learning, pages 214–223. PMLR. Bartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight VC-dimension and pseu- dodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research, 20(1):2285–2301. Bartlett, P. L. and Mendelson, S. (2002). Rademacher and Gaussian Complexities: Risk Bounds and Struc- tural Results. Journal of Machine Learning Research, 3(Nov):463–482. Boucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic theory of independence. Oxford University Press. Chakrabarty, A. and Das, S. (2021). Statistical regeneration guarantees of the wasserstein autoencoder with latent space consistency. Advances in Neural Information Processing Systems, 34:17098–17110. 14 Chakraborty and Bartlett Chakraborty, S. and Bartlett, P. L. (2024). On the statistical properties of generative adversarial models for low intrinsic data dimension. arXiv preprint arXiv:2401.15801. Chen, M., Liao, W., Zha, H., and Zhao, T. (2020). Distribution approximation and statistical estimation guarantees of generative adversarial networks. arXiv preprint arXiv:2002.03938. Dahal, B., Havrilla, A., Chen, M., Zhao, T., and Liao, W. (2022). On deep generative models for approxi- mation and estimation of distributions on manifolds. Advances in Neural Information Processing Systems, 35:10615–10628. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255. Donahue, J., Kr¨ahenb¨uhl, P., and Darrell, T. (2017). Adversarial feature learning. In International Confer- ence on Learning Representations. G´omez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern´andez-Lobato, J. M., S´anchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. (2018). Automatic chemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268– 276. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. (2014). Generative Adversarial Nets. In Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc. Gregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015). Draw: A recurrent neural network for image generation. In International Conference on Machine Learning, pages 1462–1471. PMLR. Gretton, A., Borgwardt, K. M., Rasch, M. J., Sch¨olkopf, B., and Smola, A. (2012). A kernel two-sample test. Journal of Machine Learning Research, 13(25):723–773. Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved Training of Wasserstein GANs. Advances in Neural Information Processing Systems, 30. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems, 30. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. (2017). beta-vae: Learning basic visual concepts with a constrained variational framework. In International Conference on Learning Representations. 15 Chakraborty and Bartlett Huang, J., Jiao, Y., Li, Z., Liu, S., Wang, Y., and Yang, Y. (2022). An error analysis of generative adversarial networks for learning distributions. Journal of Machine Learning Research, 23(116):1–43. Husain, H., Nock, R., and Williamson, R. C. (2019). A primal-dual link between gans and autoencoders. In Wallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. Karr, A. F. (1993). Probability. Springer Texts in Statistics. Springer New York, NY, 1 edition. Kim, J., Shin, J., Rinaldo, A., and Wasserman, L. (2019). Uniform convergence rate of the kernel density estimator adaptive to intrinsic volume dimension. In International Conference on Machine Learning, pages 3398–3407. PMLR. Kingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. International Conference on Learning Representations. Kingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In International Conference on Learning Representations. Koehler, F., Mehta, V., Zhou, C., and Risteski, A. (2022). Variational autoencoders in the presence of low- dimensional data: landscape and implicit bias. In International Conference on Learning Representations. Kolmogorov, A. N. and Tikhomirov, V. M. (1961). ϵ-entropy and ϵ-capacity of sets in function spaces. Translations of the American Mathematical Society, 17:277–364. Liang, T. (2021). How well generative adversarial networks learn distributions. The Journal of Machine Learning Research, 22(1):10366–10406. Liu, H., Havrilla, A., Lai, R., and Liao, W. (2023). Deep nonparametric estimation of intrinsic data structures by chart autoencoders: Generalization error and robustness. arXiv preprint arXiv:2303.09863. Liu, S., Yang, Y., Huang, J., Jiao, Y., and Wang, Y. (2021). Non-asymptotic error bounds for bidirectional gans. Advances in Neural Information Processing Systems, 34:12328–12339. Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B. (2016). Adversarial autoencoders. In International Conference on Learning Representations. Mescheder, L., Nowozin, S., and Geiger, A. (2017). Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 2391–2400. JMLR. org. 16 Chakraborty and Bartlett Nakada, R. and Imaizumi, M. (2020). Adaptive approximation and generalization of deep neural network with intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1–38. Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2020). The intrinsic dimension of images and its impact on learning. In International Conference on Learning Representations. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021). Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831. PMLR. Rolinek, M., Zietlow, D., and Martius, G. (2019). Variational autoencoders pursue pca directions (by accident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12406–12415. Rudelson, M. and Vershynin, R. (2013). Hanson-Wright inequality and sub-gaussian concentration. Electronic Communications in Probability, 18(none):1 – 9. Schreuder, N., Brunel, V.-E., and Dalalyan, A. (2021). Statistical guarantees for generative models without domination. In Algorithmic Learning Theory, pages 1051–1071. PMLR. Sohn, K., Lee, H., and Yan, X. (2015). Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28. Tachibana, H., Uenoyama, K., and Aihara, S. (2018). Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 4784–4788. IEEE. Tang, R. and Yang, Y. (2021). On empirical bayes variational autoencoder: An excess risk bound. In Conference on Learning Theory, pages 4068–4125. PMLR. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. (2018). Wasserstein auto-encoders. International Conference on Learning Representations. Van Den Oord, A., Vinyals, O., et al. (2017). Neural discrete representation learning. Advances in Neural Information Processing Systems, 30. Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, vol- ume 47. Cambridge university press. Wainwright, M. J. (2019). High-dimensional statistics: A Non-asymptotic Viewpoint, volume 48. Cambridge University Press. 17 Chakraborty and Bartlett Weed, J. and Bach, F. (2019). Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance. Bernoulli, 25(4A):2620 – 2648. Yang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T. (2017). Improved variational autoencoders for text modeling using dilated convolutions. In International Conference on Machine Learning, pages 3881–3890. PMLR. Yarotsky, D. (2017). Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114. Zhao, S., Song, J., and Ermon, S. (2017). Infovae: Information maximizing variational autoencoders. arXiv preprint arXiv:1706.02262. Appendix Contents 1 Introduction 1 2 A Proof of Concept 4 3 Background 5 3.1 Notations and some Preliminary Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 3.2 Wasserstein Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 4 Intrinsic Dimension of Data Distribution 7 5 Theoretical Analyses 8 5.1 Assumptions and Error Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 5.2 Main Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 5.3 Related work on GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.4 Proof Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.5 Implications of the Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 6 Discussions and Conclusion 13 A Additional Notations 19 18 Chakraborty and Bartlett B Proof of the Main Result (Theorem 8) 20 B.1 Misspecification Error . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.2 Bounding the Generalization Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 B.3 Proof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 C Detailed Proofs 24 C.1 Proofs from Section 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2 Proofs from Section 5.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2.1 Proof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 C.2.2 Proof of Lemma 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3 Proofs from Section B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3.1 Proof of Theorem 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 C.3.2 Proof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 C.4 Proofs from Section B.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.4.1 Proof of Lemma 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 C.4.2 Proof of Corollary 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C.4.3 Proof of Lemma 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C.4.4 Proof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 C.4.5 Proof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 C.4.6 Proof of Lemma 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 C.5 Proofs from Section 5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 C.6 Proofs from Section 5.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 C.6.1 Proof of Proposition 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.6.2 Proof of Proposition 13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.6.3 Proof of Theorem 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 C.6.4 Proof of Corollary 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 D Supporting Results for Approximation Guarantees 40 E Supporting Results from the Literature 41 A Additional Notations For function classes F1 and F2, F1 ◦ F2 = {f1 ◦ f2 : f1 ∈ F1, f2 ∈ F2}. 19 Chakraborty and Bartlett Definition 17 (Covering and Packing Numbers). For a metric space (S, ϱ), the ϵ-covering number w.r.t. ϱ is defined as: N(ϵ; S, ϱ) = inf{n ∈ N : ∃ x1, . . . xn such that ∪n i=1 Bϱ(xi, ϵ) ⊇ S}. A minimal ϵ cover of S is denoted as C(ϵ; S, ϱ). Similarly, the ϵ-packing number is defined as: M(ϵ; S, ϱ) = sup{m ∈ N : ∃ x1, . . . xm ∈ S such that ϱ(xi, xj) ≥ ϵ, for all i ̸= j}. B Proof of the Main Result (Theorem 8) B.1 Misspecification Error We begin with a theoretical result to approximate any function on a low-dimensional structure using a ReLU network with sufficiently large depth and width. Let f belong to the space Hβ(Rd, R, C), with C > 0, and let γ be a measure on Rd. For notational simplicity, let M = supp(γ). Then, for any ϵ > 0 and s > dγ, we prove that there exists a ReLU network, denoted by ˆf, with a depth of at most O(log(1/ϵ)), and number of weights not exceeding O(ϵ−s/β log(1/ϵ)), and bounded weights. This network satisfies the condition ∥f − ˆf∥L∞(M) ≤ ϵ. A similar result with bounded depth but unbounded weights was derived by Nakada and Imaizumi (2020). Theorem 18. Let f be an element of Hβ(Rd, R, C), where C > 0. Then, for any s > dγ, there exists constants ϵ0 (which may depend on γ) and α, (which may depend on β, d, and C), such that if ϵ ∈ (0, ϵ0], a ReLU network ˆf can be constructed with L( ˆf) ≤ α log(1/ϵ) and W( ˆf) ≤ α log(1/ϵ)ϵ−s/β, satisfying the condition, ∥f − ˆf∥L∞(M) ≤ ϵ. Applying the above theorem, one can control ∆miss, when the network size is large enough. Under assumptions A1–3, we derive the following bound on the misspecification error. It is important to note that none of the network dimensions depend on the dimensionality of the entire data space, i.e. d. Lemma 19. Suppose assumptions A1–3 hold and let, diss(·, ·) ≡ W1(·, ·) or MMD2 K(·, ·). Also, let s > dµ. Then, we can find positive constants ϵ0, α0 and R, that might depend on d, ℓ, ˜G and ˜E, such that if 0 < ϵg, ϵe ≤ ϵ0 and G = RN(Wg, Lg, R) and E = RN(We, Le, R), with Le ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe e log(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg g log(1/ϵg) then, ∆miss ≲ ϵg + ϵαg∧1 e . B.2 Bounding the Generalization Gap Let f : Rd → Rd′ and {Xi}i∈[n] ⊂ Rd. We define f|X1:n as [f(X1) : · · · : f(Xn)] ∈ Rd′×n. For a function class F, we define F|X1:n = {f|X1:n : f ∈ F} ⊆ Rd′×n. 20 Chakraborty and Bartlett The covering number of F|X1:n with respect to the ℓ∞-norm is denoted by N(ϵ; F|X1:n , ℓ∞). The result extends the seminal works of Bartlett et al. (2019) to determine the metric entropy of deep learners with multivariate outputs. Lemma 20. Suppose that n ≥ 6 and F are a class neural network with depth at most L and number of weights at most W. Furthermore, the activation functions are piece-wise polynomial activation with the number of pieces and degree at most k ∈ N. Then, there is a constant θ (that might depend on d and d′), such that, if n ≥ θ(W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3), log N(ϵ; F|X1:n , ℓ∞) ≲ (W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3) log \u0012nd′ ϵ \u0013 , where d′ is the output dimension of the networks in F. We can use the result above to provide bounds on the metric entropies of the function classes described in Lemma 7. This bound is a function of the number of samples and the size of the network classes G and E. Corollary 21. Suppose that W(E) ≤ We, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ1, such that if n ≥ ξ1(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg), log N \u0010 ϵ; E|X1:n , ℓ∞ \u0011 ≲WeLe(log We + Le) log \u0012nℓ ϵ \u0013 , log N \u0010 ϵ; (G ◦ E)|X1:n , ℓ∞ \u0011 ≲(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log \u0012nd ϵ \u0013 . Using Corollary 21, the following lemma provides a bound on the distance between the empirical and target distributions w.r.t. the IPM based on F. Lemma 22. Suppose R(G) ≲ 1 and F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Furthermore, let, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ2, such that if n ≥ ξ2(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) E∥ˆµn − µ∥F ≲ n−1/2\u0000(We + Wg)(Le + Lg) \u0000log(We + Wg) + Le + Lg \u0001 log(nd) \u00011/2 . To control the fourth terms in (7) and (8), we first consider the case when diss(·, ·) is the W1-distance. Lemma 23 controls this uniform concentration via the size of the networks in E and the sample size n. Lemma 23. Let ˆµn = 1 n Pn i=1 δXi and E = RN(Le, We). Then, sup E∈E |W1(E♯µn, ν) − W1(E♯µ, ν)| ≲ \u0010 n−1/ℓ ∨ n−1/2 log n \u0011 + r WeLe(log We + Le) log(nℓ) n . 21 Chakraborty and Bartlett Furthermore, sup E∈E |W1(E♯ˆµn, ˆνm) − W1(E♯µ, ν)| ≲ \u0010 n−1/ℓ ∨ n−1/2 log n \u0011 + \u0010 m−1/ℓ ∨ m−1/2 log m \u0011 + r WeLe(log We + Le) log(nℓ) n . Before deriving the corresponding uniform concentration bounds for | \\ MMD2 K(E♯ˆµn, ν)−MMD2 K(E♯ˆµn, ν)| or | \\ MMD2 K(E♯ˆµn, ˆνm) − MMD2 K(E♯ˆµn, ν)|, we recall the definition of Rademacher complexity (Bartlett and Mendelson, 2002). For any real-valued function class F and data points X1:n = {X1, . . . , Xn}, the empirical Rademacher complexity is defined as: R(F, X1:n) = 1 nEσ sup f∈F n X i=1 σif(Xi), where σi’s are i.i.d. Rademacher random variables taking values in {−1, +1}, with equal probability. In the following lemma, we derive a bound on the Rademacher complexity of the class of functions in the unit ball w.r.t. the HK-norm composed with E. This lemma plays a key role in the proof of Lemma 24. The proof crucially uses the results by Rudelson and Vershynin (2013). Lemma 24. Suppose assumption A2 holds and let, L(E) ≤ Le and L(G) ≤ Lg, with Le ≥ 3, We ≥ 2ℓ(3+Le). Also suppose that, Φ = {ϕ ∈ HK : ∥ϕ∥HK ≤ 1}, then, R((Φ ◦ E), X1:n) ≲ r WeLe(log We + Le) log(nℓ) n . Using Lemma 24, we bound the fourth term in (7) and (8) for diss(·, ·) = MMD2 K(·, ·), in Lemma 25. Lemma 25. Under assumption A2, the following holds: (a) E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ν) − MMD2 K(E♯µ, ν) \f\f\f ≲ q WeLe log We log(nℓ) n , (b) E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ˆνm) − MMD2 K(E♯µ, ν) \f\f\f ≲ q WeLe(log We+Le) log(nℓ) n + 1 √m. B.3 Proof of Theorem 8 Theorem 8. Suppose that assumptions A1–3 hold and ∆opt ≤ ∆ for some fixed non-negative threshold ∆. Furthermore, suppose that s > dµ. Then we can find n0 ∈ N and β > 0, that might depend on d, ℓ, αg, αe, ˜G and ˜E, such that if n ≥ n0, we can choose G = RN(Lg, Wg) and E = RN(Le, We), with, Le ≤ β log n, We ≤ βn s 2αe+s log n, Lg ≤ β log n and Wg ≤ βn ℓ αe(αg∧1)+ℓ log n, then, for the estimation problem (5), (a) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) ,ℓ \u001b log2 n, for diss(·, ·) = W1(·, ·), (b) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 2+max \u001a ℓ αg , s αe(αg∧1) \u001b log2 n, for diss(·, ·) = MMD2 K(·, ·). 22 Chakraborty and Bartlett Furthermore, for the estimation problem (6), if m ≥ n ∨ n \u0010 max n 2+ ℓ αg ,2+ dµ αe(αg∧1) ,ℓ o\u0011−1(ℓ∨2) (c) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) ,ℓ \u001b log2 n, for diss(·, ·) = W1(·, ·), (d) EV (µ, ν, ˆGn,m, ˆEn,m) ≲ ∆ + n − 1 2+max \u001a ℓ αg , s αe(αg∧1) \u001b log2 n, for diss(·, ·) = MMD2 K(·, ·). Proof. Proof of part (a) From Lemmas 7, 22 and 23, we get, EV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1 e + r WeLe log We log n n + r (We + Wg)(Le + Lg) log(We + Wg) log n n + \u0010 n−1/ℓ ∨ n−1/2\u0011 ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s (ϵ−s/αe e + ϵ−ℓ/αg g ) log n n   + \u0010 n−1/ℓ ∨ n−1/2\u0011 ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s ϵ−ℓ/αg g log n n   + \u0010 n−1/ℓ ∨ n−1/2\u0011 We choose, ϵg ≍ n − 1 2+ ℓ αg and ϵe ≍ n − 1 2(αg∧1)+ s αe . This makes, EV (µ, ν, ˆGn, ˆEn) ≲∆ + log2 n × n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) \u001b + n−1/ℓ. Proof of part (b) From Lemmas 7, 22 and 25, we get, EV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1 e + r WeLe log We log n n + r (We + Wg)(Le + Lg) log(We + Wg) log n n ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s (ϵ−s/αe e + ϵ−ℓ/αg g ) log n n   ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s ϵ−ℓ/αg g log n n   We choose, ϵg ≍ n − 1 2+ ℓ αg and ϵe ≍ n − 1 2(αg∧1)+ s αe . This makes, EV (µ, ν, ˆGn, ˆEn) ≲∆ + log2 n × n − 1 max \u001a 2+ ℓ αg ,2+ s αe(αg∧1) \u001b . Proof of part (c) Again from Lemmas 7, 22 and 23, we get, EV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s ϵ−ℓ/αg g log n n   + \u0010 n−1/ℓ ∨ n−1/2\u0011 + \u0010 m−1/ℓ ∨ m−1/2\u0011 23 Chakraborty and Bartlett Choosing ϵg ≍ n − 1 2+ ℓ αg , ϵe ≍ n − 1 2(αg∧1)+ s αe and m as in the theorem statement gives us the desired result. Proof of part (d) Similarly, from Lemmas 7, 22 and 25, we get, EV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1 e + \u0012 log \u0012 1 ϵe ∧ ϵg \u0013\u00133/2   s ϵ−s/αe e log n n + s ϵ−ℓ/αg g log n n   + 1 √m Choosing ϵg ≍ n − 1 2+ ℓ αg , ϵe ≍ n − 1 2(αg∧1)+ s αe and m as in the theorem statement gives us the desired result. C Detailed Proofs C.1 Proofs from Section 4 Lemma 5. Let, f ∈ Hγ \u0000A, [0, 1]d2, C \u0001 , with A ⊆ [0, 1]d1. Then, dimM (f (A)) ≤ dimM(A)/(γ ∧ 1). Proof. Then, for any x, y ∈ A, ∥f(x) − f(y)∥∞ ≤ ∥f(x) − f(y)∥2 ≤ L∥x − y∥γ∧1 2 ≤ Ld(γ∧1)/2 1 ∥x − y∥γ∧1 ∞ . Thus, N (ϵ; f (A) , ∥ · ∥∞) ≤ N \u0010 1 √d1 (ϵ/L)(γ∧1)−1; A, ∥ · ∥∞ \u0011 . dimM (f (A)) = lim ϵ→0 log N (ϵ; f (A) , ∥ · ∥∞) log(1/ϵ) ≤ lim ϵ→0 log N \u0010 1 √d1 (ϵ/L)(γ∧1)−1; A, ∥ · ∥∞ \u0011 log(1/ϵ) ≤ dimM(A) γ ∧ 1 . C.2 Proofs from Section 5.1 C.2.1 Proof of Proposition 6 Proposition 6. Under assumption A1, the following holds: (a) ˜G♯ν = µ, (b) ( ˜E◦ ˜G)♯ν = ν, (c) V (µ, ν, ˜G, ˜E) = 0. Proof. (a) Let f : Z → R be any bounded continuous function. Then, Z f(x)d( ˜G♯ν)(x) = Z f( ˜G(z))dν(z) = Z f( ˜G( ˜E(x)))dµ(x) (10) = Z f(x)dµ(x) (11) Hence, ˜G♯ν = µ. Here both (10) and (11) follows from A1. (b) Let f : X → R be any bounded continuous function. Then, Z f(x)d \u0010 ( ˜E ◦ ˜G)♯ν \u0011 = Z f(E(G(z)))dν(z) = Z f(E(x))dµ(x) (12) = Z f(z)dν(z). (13) 24 Chakraborty and Bartlett Here, (12) follows from part (a) and (13) follows from A1. (c) To prove part (c), We note that, W1(E♯µ, ν), MMD2 K(E♯µ, ν) = 0 and ( ˜G ◦ ˜E)(·) = id(·), a.e. [µ]. C.2.2 Proof of Lemma 7 Lemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Then the following hold: V (µ, ν, ˆGn, ˆEn) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup E∈E |d diss(E♯ˆµn, ν) − diss(E♯µ, ν)|. (7) V (µ, ν, ˆGn,m, ˆEn,m) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup E∈E |d diss(E♯ˆµn, ˆνm) − diss(E♯µ, ν)|. (8) Here, ∆miss = infG∈G,E∈E V (µ, ν, G, E) denotes the misspecification error for the problem. Proof. To prove the first inequality, we observe that, V (ˆµn, ν, ˆGn, ˆEn) ≤ V (ˆµn, ν, G, E)+∆opt, for any G ∈ G and E ∈ E. Thus, V (µ, ν, ˆGn, ˆEn) =V (µ, ν, G, E) + \u0010 V (µ, ν, ˆGn, ˆEn) − V (ˆµn, ν, ˆGn, ˆEn) \u0011 + \u0010 V (ˆµn, ν, ˆGn, ˆEn) − V (µ, νG, E) \u0011 ≤V (µ, ν, G, E) + \u0010 V (µ, ν, ˆGn, ˆEn) − V (ˆµn, ν, ˆGn, ˆEn) \u0011 + (V (ˆµn, ν, G, E) − V (µ, ν, G, E)) + ∆opt ≤∆opt + V (µ, ν, G, E) + 2 sup G∈G, E∈E |V (ˆµn, ν, G, E) − V (µ, ν, G, E)| =∆opt + V (µ, ν, G, E) + 2 sup G∈G, E∈E \f\f\f\f Z c(x, G ◦ E(x))dˆµn(x) + λd diss(E♯ˆµn, ν) − Z c(x, G ◦ E(x))dµ(x) − λdiss(E♯µ, ν) \f\f\f\f ≤∆opt + V (µ, ν, G, E) + 2∥ˆµn − µ∥F + 2λ sup E∈E |d diss(E♯ˆµn, ν) − diss(E♯µ, ν)|. Taking infimum on G and E, we get inequality (7). Inequality (8) follows from a similar derivation. C.3 Proofs from Section B.1 C.3.1 Proof of Theorem 18 Fix ϵ > 0. For s > dµ, we can say that N(ϵ; M, ℓ∞) ≤ Cϵ−s, for all ϵ > 0. Let K = ⌈ 1 2ϵ⌉. For any i ∈ [K]d, let θi = (ϵ + 2(i1 − 1)ϵ, . . . , ϵ + 2(id − 1)ϵ). We also let, Pϵ = {Bℓ∞(θi, ϵ) : i ∈ [K]d}. By construction, the sets in Pϵ are disjoint. We first claim the following: Lemma 26. |{A ∈ Pϵ : A ∩ M ̸= ∅}| ≤ C2dϵ−s. 25 Chakraborty and Bartlett a − b −a + b a −b 0 1 Figure 2: Plot of ξa,b(·) Proof. Let, r = N(ϵ; M, ℓ∞) and suppose that {a1, . . . , ar} be an ϵ-net of M and P∗ ϵ = {Bℓ∞(ai, ϵ) : i ∈ [r]} be an optimal ϵ-cover of M. Note that each box in P∗ ϵ can intersect at most 2d boxes in Pϵ. This implies that, |Pϵ ∩ M| ≤ |Pϵ ∩ (∪r i=1Bℓ∞(ai, ϵ))| = |∪r i=1 (Pϵ ∩ Bℓ∞(ai, ϵ))| ≤ 2dr, which concludes the proof. We are now ready to prove Theorem 18. For the ease of readability, we restate the theorem as follows: Theorem 18. Let f be an element of Hβ(Rd, R, C), where C > 0. Then, for any s > dγ, there exists constants ϵ0 (which may depend on γ) and α, (which may depend on β, d, and C), such that if ϵ ∈ (0, ϵ0], a ReLU network ˆf can be constructed with L( ˆf) ≤ α log(1/ϵ) and W( ˆf) ≤ α log(1/ϵ)ϵ−s/β, satisfying the condition, ∥f − ˆf∥L∞(M) ≤ ϵ. Proof. We also let I = n i ∈ [K]d : Bℓ∞(θi, ϵ) ∩ M ̸= ∅ o . We also let I† = {j ∈ [K]d : mini∈I ∥i − j∥1 ≤ 1}. We know that |I†| ≤ 3d|I| ≤ 6dN(ϵ; M, ℓ∞). For 0 < b ≤ a, let, ξa,b(x) = ReLU \u0012x + a a − b \u0013 − ReLU \u0012x + b a − b \u0013 − ReLU \u0012x − b a − b \u0013 + ReLU \u0012x − a a − b \u0013 . A pictorial view of this function is given in Fig. 2 and can be implemented by a ReLU network of depth two and width four. Thus, L(ξa,b) = 2 and W(ξa,b) = 12. Suppose that 0 < δ < ϵ/3 and let, ζ(x) = Qd ℓ=1 ξϵ+δ,δ(xℓ). It is easy to observe that {ζ(· − θi) : i ∈ I†} forms a partition of unity on M, i.e. P i∈I† ζ(x − θi) = 1, ∀x ∈ M. We consider the Taylor approximation of f around θ as, Pθ(x) = X |s|<⌊β⌋ ∂sf(θ) s! (x − θ)s . Note that for any x ∈ [0, 1]d, f(x) − Pθ(x) = P s:|s|=⌊β⌋ (x−θ)s s! (∂sf(y) − ∂sf(θ)), for some y, which is a 26 Chakraborty and Bartlett convex combination of x and θ. Thus, f(x) − Pθ(x) = X s:|s|=⌊β⌋ (x − θ)s s! (∂sf(y) − ∂sf(θ)) ≤∥x − θ∥⌊β⌋ ∞ X s:|s|=⌊β⌋ 1 s!|∂sf(y) − ∂sf(θ)| ≤∥x − θ∥⌊β⌋ ∞ ∥y − θ∥β−⌊β⌋ ∞ ≤∥x − θ∥β ∞. (14) Next we define ˜f(x) = P i∈I† ζ(x − θi)Pθi(x). Thus, if x ∈ M, |f(x) − ˜f(x)| = \f\f\f\f\f\f X i∈I† ζ(x − θi)(f(x) − Pθi(x)) \f\f\f\f\f\f ≤ X i∈I†:∥x−θi∥∞≤2ϵ |f(x) − Pθi(x)| ≤2d(2ϵ)β =2d+βϵβ. (15) We note that, ˜f(x) = P i∈I† ζ(x − θi)Pθi(x) = P i∈I† P |s|<⌊β⌋ ∂sf(θi) s! ζ(x − θi) \u0010 x − θi\u0011s . Let ai,s = ∂sf(θi) s! and ˆfi,s(x) = prod(d+|s|) m (ξϵ1,δ1(x1 − θi 1), . . . , ξϵd,δd(xd − θi d), (x1 − θi 1), . . . , (x1 − θi 1) | {z } s1 times , . . . , (x1 − θi d), . . . , (xd − θi d) | {z } sd times ), where, prod(·) is defined in Lemma 34. Here, prod(d+|s|) m has at most d + |s| ≤ d + ⌊β⌋ many inputs. By Lemma 34, prod(d+|s|) m can be implemented by a ReLU network with L(prod(d+|s|) m ), W(prod(d+|s|) m ) ≤ c3m. Thus, L( ˆfi,s) ≤ c3m + 2 and W( ˆfi,s) ≤ c3m + 8d + 4|s| ≤ c3m + 8d + 4k. With this ˆfi,s, we observe that, \f\f\f ˆfi,s(x) − ζ(x − θi) \u0010 x − θi\u0011s\f\f\f ≤ (d + ⌊β⌋)3 22m+2 , ∀x ∈ M. (16) Finally, let, ˆf(x) = P i∈I† P |s|≤⌊β⌋ ai,s ˆfi,s(x). Clearly, L( ˆfi,s) ≤ c3m+3 and W( ˆfi,s) ≤ kd (c3m + 8d + 4k). This implies that, | ˆf(x) − ˜f(x)| ≤ X i∈I†:∥x−θi∥∞≤2ϵ X |s|<⌊β⌋ |ai,s|ζ(x − θi)| ˆfis(x) − \u0010 x − θi\u0011s | ≤2d X |s|<⌊β⌋ |aθ,s| \f\f\f ˆfθi(x),s(x) − ζϵ,δ(x − θ(i(x)) \u0010 x − θi(x)\u0011s\f\f\f ≤(d + ⌊β⌋)3C 22m+2−d . We thus get that if x ∈ M, |f(x) − ˆf(x)| ≤|f(x) − ˜f(x)| + | ˆf(x) − ˜f(x)| ≤ 2d+βϵβ + (d + ⌊β⌋)3C 22m+2−d . (17) 27 Chakraborty and Bartlett We choose ϵ = \u0000η 2d+k+2 \u00011/β and m = l log2 \u0010 (d+k)3C η \u0011m + d − 1. Then, ∥f − ˆf∥L∞(M) ≤η. We note that ˆf has |I†| ≤ 6dNϵ(M) ≲ 6dϵ−s many networks with depth c3m + 3 and number of weights ⌊β⌋d (c3m + 8d + 4⌊β⌋). Thus, L( ˆf) ≤ c3m + 4 and W( ˆf) ≤ ϵ−s(6⌊β⌋)d (c3m + 8d + 4⌊β⌋). we thus get, L( ˆf) ≤ c3m + 4 ≤ c3 \u0012\u0018 log2 \u0012(d + ⌊β⌋)3Cδ η \u0013\u0019 + d − 1 \u0013 + 4 ≤ c4 log \u00121 η \u0013 , where c4 is a function of δ, ⌊β⌋ and d. Similarly, W( ˆf) ≤ϵ−s(6⌊β⌋)d (c3m + 8d + 4⌊β⌋) ≤ \u0010 η 2d+k+2 \u0011−s/β (6⌊β⌋)d \u0012 c3 \u0012 log2 \u0012(d + ⌊β⌋)3Cδ η \u0013 + d − 1 \u0013 + 8d + 4⌊β⌋ \u0013 ≤c6 log(1/η)η−s/β. Taking α = c4 ∨ c6 gives the result. C.3.2 Proof of Lemma 19 Lemma 19. Suppose assumptions A1–3 hold and let, diss(·, ·) ≡ W1(·, ·) or MMD2 K(·, ·). Also, let s > dµ. Then, we can find positive constants ϵ0, α0 and R, that might depend on d, ℓ, ˜G and ˜E, such that if 0 < ϵg, ϵe ≤ ϵ0 and G = RN(Wg, Lg, R) and E = RN(We, Le, R), with Le ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe e log(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg g log(1/ϵg) then, ∆miss ≲ ϵg + ϵαg∧1 e . Proof. We first prove the result for the Wasserstein-1 distance and then for the MMDK-metric. Case 1: diss(·, ·) ≡ W1(·, ·) For any G ∈ G and E ∈ E, we observe that, V (µ, ν, G, E) ≤V (µ, ν, ˜G, ˜E) + |V (µ, ν, G, E) − V (µ, ν, ˜G, ˜E)| ≤∥c(·, ˜G ◦ ˜E(·)) − c(·, G ◦ E(·))∥L∞(M) + |W1(E♯µ, ν) − W1( ˜E♯µ, ν)| ≲∥G ◦ E − ˜G ◦ ˜E∥L∞(M) + W1( ˜E♯µ, E♯µ) ≲∥G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)) ≤∥G ◦ E − ˜G ◦ E∥L∞(supp(µ)) + ∥ ˜G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)) ≲∥G − ˜G∥L∞([0,1]ℓ) + ∥E − ˜E∥αg∧1 L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)) We can take G = RN(log(1/ϵg), ϵ−ℓ/αg g log(1/ϵg)) and E = RN(log(1/ϵe), ϵ−s/αe e log(1/ϵe)) by approximating in each of the individual coordinate-wise output of the vector-valued functions ˜G and ˜E and stacking them 28 Chakraborty and Bartlett parallelly. This makes, V (µ, ν, G, E) ≲ ϵg + ϵαg∧1 e + ϵe ≲ ϵg + ϵαg∧1 e . Case 2: diss(·, ·) ≡ MMD2 k(·, ·) Before we begin, we note that, |MMD2 K(E♯µ, ν) − MMD2 K( ˜E♯µ, ν)| ≤|EX∼µ,X′∼µK(E(X), E(X′)) − EX∼µ,X′∼µK( ˜E(X), ˜E(X′))| + 2|EX∼µ,Z∼νK(E(X), Z) − EX∼µ,Z∼νK( ˜E(X), Z)| ≤2τk∥E − ˜E∥L∞(supp(µ)) + 2τk∥E − ˜E∥L∞(supp(µ)) =4τk∥E − ˜E∥L∞(supp(µ)). (18) For any G ∈ G and E ∈ E, we observe that, V (µ, ν, G, E) =V (µ, ν, ˜G, ˜E) + |V (µ, ν, G, E) − V (µ, ν, ˜G, ˜E)| =∥c(·, ˜G ◦ ˜E(·)) − c(·, G ◦ E(·))∥L∞(supp(µ)) + |MMD2 K(E♯µ, ν) − MMD2 K( ˜E♯µ, ν)| ≲∥G ◦ E − ˜G ◦ ˜E∥L∞(µ) + 4τk∥E − ˜E∥L∞(supp(µ)) (19) ≲∥G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)) ≤∥G ◦ E − ˜G ◦ E∥L∞(supp(µ)) + ∥ ˜G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)) ≲∥G − ˜G∥L∞([0,1]ℓ) + ∥E − ˜E∥αg∧1 L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)). In the above calculations, we have used (18) to arrive at (19). As before, we take G = RN(log(1/ϵg), ϵ−ℓ/αg g log(1/ϵg)) and E = RN(log(1/ϵe), ϵ−s/αe e log(1/ϵe)) by approximating in each of the individual coordinate-wise output of the vector-valued functions ˜G and ˜E and stacking them parallelly. This makes, V (µ, ν, G, E) ≲ ϵg + ϵαg∧1 e + ϵe ≲ ϵg + ϵαg∧1 e . C.4 Proofs from Section B.2 C.4.1 Proof of Lemma 20 Lemma 20. Suppose that n ≥ 6 and F are a class neural network with depth at most L and number of weights at most W. Furthermore, the activation functions are piece-wise polynomial activation with the number of pieces and degree at most k ∈ N. Then, there is a constant θ (that might depend on d and d′), such that, if n ≥ θ(W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3), log N(ϵ; F|X1:n , ℓ∞) ≲ (W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3) log \u0012nd′ ϵ \u0013 , 29 Chakraborty and Bartlett x f y id f(x) y f(x) + y f(x) − y h(x, y) 1 −1 0.25 −0.25 1 1 Figure 3: A representation of the network h(·, ·). The magenta lines represent d′ weights of value 1. Similarly, cyan lines represent d′ weights of value −1. Finally, the orange and teal lines represent d′ weights (each) with values +0.25 and −0.25, respectively. The identity map takes 2d′L(f) many weights (see remark 15 (iv) of Nakada and Imaizumi (2020)). The magenta, cyan, orange and teal connections take 6d′ many weights. All activations are taken to be ReLU, except the output of the yellow nodes, whose activation is σ(x) = x2. where d′ is the output dimension of the networks in F. Proof. We let, h(x, y) = y⊤f(x) and let H = {h(x, y) = y⊤f(x) : f ∈ F}. Also, let, T = {(h(Xi, eℓ)|i∈[n],ℓ∈[d′]) ∈ Rnd′ : h ∈ H}. Here eℓ denotes the ℓ-th unit vector. By construction of T , it is clear that, N(ϵ; F|X1:n , ℓ∞) = N(ϵ; T , ℓ∞). We observe that, h(x, y) = 1 4(∥y + f(x)∥2 2 − ∥y − f(x)∥2 2) Clearly, h can be implemented by a network with L(h) = L(f) + 3 and W(h) = W(f) + 6d′ + 2d′L(f) (see Fig. 3 for such a construction). Thus, from Theorem 12.9 of Anthony and Bartlett (2009) (see Lemma 37), we note that, if n ≥ Pdim(H), N(ϵ; T , ℓ∞) ≤ \u0012 2end′ ϵPdim(H) \u0013Pdim(H) , with, Pdim(H) ≲ W(h)L(h) log W(h) + W(h)L2(h), from applying Theorem 6 of Bartlett et al. (2019) (see Lemma 38). This implies that, log N(ϵ; H, ℓ∞) ≤Pdim(H) log \u0012 2end′ ϵPdim(H) \u0013 ≤Pdim(H) log \u0012nd′ ϵ \u0013 ≲ \u0000W(h)L(h) log W(h) + W(h)L2(h) \u0001 log \u0012nd′ ϵ \u0013 . Plugging in the values of W(h) and L(h) yields the result. 30 Chakraborty and Bartlett C.4.2 Proof of Corollary 21 Corollary 21. Suppose that W(E) ≤ We, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ1, such that if n ≥ ξ1(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg), log N \u0010 ϵ; E|X1:n , ℓ∞ \u0011 ≲WeLe(log We + Le) log \u0012nℓ ϵ \u0013 , log N \u0010 ϵ; (G ◦ E)|X1:n , ℓ∞ \u0011 ≲(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log \u0012nd ϵ \u0013 . Proof. The proof easily follows from applying Lemma 20 and noting the sizes of the networks in E and G ◦ E. C.4.3 Proof of Lemma 22 Lemma 22. Suppose R(G) ≲ 1 and F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Furthermore, let, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ2, such that if n ≥ ξ2(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) E∥ˆµn − µ∥F ≲ n−1/2\u0000(We + Wg)(Le + Lg) \u0000log(We + Wg) + Le + Lg \u0001 log(nd) \u00011/2 . Proof. Let R(G) ≤ B, for some B > 0 and let Bc = sup0≤x≤B |c(x)| From Dudley’s chaining (Wainwright, 2019, Theorem 5.22), E∥ˆµn − µ∥F ≲EX1:n inf 0≤δ≤Bc/2   δ + 1 √n Z Bc/2 δ q log N(ϵ, F|X1:n , ℓ∞)dϵ ! . (20) Let For any G ∈ G and E ∈ E, we can find v ∈ C(ϵ; (G◦E)|X1:n , ℓ∞), such that, ∥(G◦E)|X1:n −v∥∞ ≤ ϵ. This implies that ∥(G ◦ E)(Xi) − vi∥∞ ≤ ϵ, for all i ∈ [n]. Let, A = {(c(X1, v1), . . . , c(Xn, vn)) : v ∈ (G ◦ E)|X1:n }. Thus, For any G ∈ G, E ∈ E, max 1≤i≤n |c(Xi, G ◦ E(Xi)) − c(Xi, vi)| ≤τc max 1≤i≤n ∥(G ◦ E)(Xi) − vi∥∞ ≤ τcϵ. Thus, A constitutes a τcϵ-cover of F|X1:n . Hence, N(ϵ, F|X1:n , ℓ∞) ≤ N(ϵ/τc, (G ◦ E)|X1:n , ℓ∞) ≤ (We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log \u0012τcnd ϵ \u0013 . Here, the last inequality follows from Lemma 20. Plugging in the above bound in equation (20), we get, E∥ˆµn − µ∥F ≲EX1:n inf 0≤δ≤Bc/2   δ + 1 √n Z Bc/2 δ q log N(ϵ, F|X1:n , ℓ∞)dϵ ! ≤ r (We + Wg)(Le + Lg) log(We + Wg) n Z Bc 0 s log \u0012τcnd ϵ \u0013 dϵ ≲ r (We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log(nd) n . 31 Chakraborty and Bartlett C.4.4 Proof of Lemma 23 Lemma 23. Let ˆµn = 1 n Pn i=1 δXi and E = RN(Le, We). Then, sup E∈E |W1(E♯µn, ν) − W1(E♯µ, ν)| ≲ \u0010 n−1/ℓ ∨ n−1/2 log n \u0011 + r WeLe(log We + Le) log(nℓ) n . Furthermore, sup E∈E |W1(E♯ˆµn, ˆνm) − W1(E♯µ, ν)| ≲ \u0010 n−1/ℓ ∨ n−1/2 log n \u0011 + \u0010 m−1/ℓ ∨ m−1/2 log m \u0011 + r WeLe(log We + Le) log(nℓ) n . Proof. Note that if diss(·, ·) = W1(·, ·), then sup E∈E |d diss(E♯µ, ν) − diss(E♯µ, ν)| = sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)| ≤ sup E∈E W1(E♯ˆµn, E♯µ) We note that, sup E∈E W(E♯ˆµ, E♯µ) = sup E∈E sup f:∥f∥Lip≤1 EX∼µ, ˆ X∼ˆµf(E(X)) − f(E( ˆX)) We take F1 = {f : [0, 1]ℓ → R : ∥f∥Lip ≤ 1} = H1( √ ℓ). By the result of Kolmogorov and Tikhomirov (1961) (Lemma 36), we note that log N(ϵ; F1, ℓ∞) ≲ ϵ−ℓ. Furthermore, if we take F2 = E, we observe that, log N(ϵ; E|X1:n , ℓ∞) ≲ WeLe(log We + Le) log \u0000 nℓ ϵ \u0001 from Lemma 21. From Dudley’s chaining, we observe the following: E sup E∈E W(E♯ˆµ, E♯µ) =E sup E∈E sup f:∥f∥Lip≤1 EX∼µ, ˆ X∼ˆµf(E(X)) − f(E( ˆX)) =E∥ˆµ − µ∥F1◦E ≲E inf 0≤δ≤Re   δ + 1 √n Z Re δ r log N \u0010 ϵ; (F1 ◦ E)|X1:n , ℓ∞ \u0011 dϵ ! ≤E inf 0≤δ≤Re   δ + 1 √n Z Re δ r log N (ϵ/2; F1, ℓ∞) + log N \u0010 ϵ/2; E|X1:n , ℓ∞ \u0011 dϵ ! ≲E inf 0≤δ≤Re   δ + 1 √n Z Re δ   p log N (ϵ/2; F1, ℓ∞) + r log N \u0010 ϵ/2; E|X1:n , ℓ∞ \u0011! dϵ ! ≲E inf 0≤δ≤Re   δ + 1 √n Z Re δ p log N (ϵ/2; F1, ℓ∞)dϵ + 1 √n Z Re δ r log N \u0010 ϵ/2; E|X1:n , ℓ∞ \u0011 dϵ ! ≲ inf 0≤δ≤Re   δ + 1 √n Z Re δ ϵ−ℓ/2dϵ + 1 √n Z 1 0 s WeLe(log We + Le) log \u00122enℓ ϵ \u0013 dϵ ! ≲ inf 0≤δ≤Re   δ + 1 √n Z Re δ ϵ−ℓ/2dϵ ! + r WeLe(log We + Le) log(nℓ) n 32 Chakraborty and Bartlett ≲ inf 0≤δ≤Re   δ + 1 √n Z Re δ ϵ−ℓ/2dϵ ! + r ℓWeLe log We log n n ≲ \u0010 n−1/ℓ ∨ n−1/2 log n \u0011 + r WeLe(log We + Le) log(nℓ) n . C.4.5 Proof of Lemma 24 Lemma 24. Suppose assumption A2 holds and let, L(E) ≤ Le and L(G) ≤ Lg, with Le ≥ 3, We ≥ 2ℓ(3+Le). Also suppose that, Φ = {ϕ ∈ HK : ∥ϕ∥HK ≤ 1}, then, R((Φ ◦ E), X1:n) ≲ r WeLe(log We + Le) log(nℓ) n . Proof. R((Φ ◦ E), X1:n) = 1 nE sup ϕ∈Φ, f∈E \f\f\f\f\f n X i=1 σiϕ(f(Xi)) \f\f\f\f\f = 1 nE sup ϕ∈Φ, f∈E \f\f\f\f\f n X i=1 σi⟨K(f(Xi), ·), ϕ⟩ \f\f\f\f\f = 1 nE sup ϕ∈Φ, f∈E \f\f\f\f\f * n X i=1 σiK(f(Xi), ·), ϕ +\f\f\f\f\f ≤ 1 nE sup f∈E \r\r\r\r\r n X i=1 σiK(f(Xi), ·) \r\r\r\r\r HK = 1 nE sup v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011 sup f∈E \r\r\r\r\r n X i=1 σi (K(vi, ·) + K(f(Xi), ·) − K(vi, ·)) \r\r\r\r\r HK ≤ 1 nE sup v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011 sup f∈E   \r\r\r\r\r n X i=1 σiK(vi, ·) \r\r\r\r\r HK + 1 n n X i=1 ∥K(f(Xi), ·) − K(vi, ·)∥HK   ≤ 1 nE max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011 \r\r\r\r\r n X i=1 σiK(vi, ·) \r\r\r\r\r HK + √ 2τkϵ (21) For any v ∈ C \u0010 ϵ, E|X1:n , ℓ∞ \u0011 , let, Yv = ∥Pn i=1 σiK(vi, ·)∥HK and Kv = ((K(vi, vj)) ∈ Rn×n. It is easy to observe that, Y 2 v = σ⊤Kvσ and Yv = ∥K1/2 v σ∥. By Theorem 2.1 of Rudelson and Vershynin (2013), we note that, P \u0010\f\f\f∥K1/2 v σ∥ − ∥K1/2 v ∥HS \f\f\f > t \u0011 ≤ 2 exp ( − ct2 ∥K1/2 v ∥2 ) = 2 exp \u001a − ct2 ∥Kv∥ \u001b , for some universal constant c > 0. From Perron–Frobenius theorem, we note that, ∥Kv∥ ≤ max 1≤i≤n n X j=1 K(vi, vj) ≤ B2n. 33 Chakraborty and Bartlett Hence, P \u0010\f\f\f∥K1/2 v σ∥ − ∥K1/2 v ∥HS \f\f\f > t \u0011 ≤ 2 exp \u001a − ct2 nB2 \u001b . This implies that, exp(λ(∥K1/2 v σ∥ − ∥K1/2 v ∥HS)) ≤ exp \u001a −c′λ2 n \u001b , for some absolute constant c′, by applying Proposition 2.5.2 of Vershynin (2018). From Theorem 2.5 of Boucheron et al. (2013), we observe that, E max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011(∥K1/2 v σ∥ − ∥K1/2 v ∥HS) ≲ r n log N \u0010 ϵ, E|X1:n , ℓ∞ \u0011 . From equation (21), we observe that, R(Φ ◦ E, X1:n) ≤ 1 nE max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011 \r\r\r\r\r n X i=1 σiK(vi, ·) \r\r\r\r\r HK + √ 2τkϵ = 1 nE max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011(∥K1/2 v σ∥ − ∥K1/2 v ∥HS + ∥K1/2 v ∥HS) + √ 2τkϵ ≤ 1 nE max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011(∥K1/2 v σ∥ − ∥K1/2 v ∥HS) + 1 n max v∈C \u0010 ϵ,E|X1:n ,ℓ∞ \u0011 ∥K1/2 v ∥HS + √ 2τkϵ ≲ v u u tlog N \u0010 ϵ, E|X1:n , ℓ∞ \u0011 n + B √n + √ϵ ≲ s WeLe log We log \u0000 nℓ ϵ \u0001 n + √ϵ (22) We take ϵ = q WeLe(log We+Le) log(nℓ) n makes R((Φ ◦ E), X1:n) ≲ q WeLe(log We+Le) log(nℓ) n . C.4.6 Proof of Lemma 25 To prove Lemma 25, we need some supporting results, which we sequentially state and prove as follows. The first such result, i.e. Lemma 27 ensures that the kernel function is Lipschitz when it is considered as a map from a real vector space to the corresponding Hilbert space. Lemma 27. Suppose assumption A2 holds. Then, ∥K(x, ·) − K(y, ·)∥2 HK ≤ 2τk∥x − y∥2. Proof. We observe the following: ∥K(x, ·) − K(y, ·)∥2 HK =K(x, x) + K(y, y) − 2K(x, y) =(K(x, x) − K(x, y)) + (K(y, y) − K(x, y)) ≤2τk∥x − y∥2. 34 Chakraborty and Bartlett Lemma 28 states that the difference between the estimated and actual squared MMD-dissimilarity scales as O(1/n) for estimates (5) and O(1/n + 1/m) for estimates (6). Lemma 28. Suppose assumption A2 holds. Then, for any E ∈ E, (a) \f\f\f\\ MMD 2 K(E♯ˆµn, ν) − MMD2 K(E♯ˆµn, ν) \f\f\f ≤ 2B2 n . (b) \f\f\f\\ MMD 2 K(E♯ˆµn, ˆνm) − MMD2 K(E♯ˆµn, ν) \f\f\f ≤ 2B2 \u0000 1 n + 1 m \u0001 . Proof. We note that, \\ MMD 2 K(E♯ˆµn, ν) − MMD2(E♯ˆµn, ν) = 1 n(n − 1) X i̸=j K(E(Xi), E(Xj)) − 1 n2 n X i,j=1 K(E(Xi), E(Xj)) = 1 n2(n − 1) X i̸=j K(E(Xi), E(Xj)) − 1 n2 n X i=1 K(E(Xi), E(Xi)) Thus, \f\f\f\\ MMD 2 K(E♯ˆµn, ν) − MMD2(E♯ˆµn, ν) \f\f\f ≤ 1 n2(n − 1) × n(n − 1)B2 + 1 n2 × nB2 = 2B2 n . Part (b) follows similarly. We also note that the MMDK-metric is bounded under A2 as seen in Lemma 29. Lemma 29. Under assumption A2, MMDK(P, Q) ≤ 2B, for any two distributions P and Q. Proof. |f(x)| = ⟨K(x, ·), f⟩ ≤ ∥K(x, ·)∥HK = B. This implies that MMDK(P, Q) = supϕ∈Φ( R ϕdP − R ϕdQ) ≤ 2B Lemma 30. Suppose assumption A2 holds. Then, (a) E supE∈E \f\f\f\\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \f\f\f ≲ q WeLe(log We+Le) log(nℓ) n . (b) E supE∈E \f\f\f\\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \f\f\f ≲ q WeLe(log We+Le) log(nℓ) n + 1 √m. Proof. Proof of Part (a) We begin by noting that, E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \f\f\f ≤E sup E∈E |MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)| + E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ν) − MMDK(E♯ˆµn, ν) \f\f\f 35 Chakraborty and Bartlett ≤E sup E∈E MMDK(E♯ˆµn, E♯µ) + 2B r 1 n (23) =E sup E∈E sup ϕ∈Φ \u0012Z ϕ(E(x))dˆµn(x) − Z ϕ(E(x))dµ(x) \u0013 + 2B r 1 n ≤2R(Φ ◦ E, µ) + 2B r 1 n (24) ≲ r WeLe(log We + Le) log(nℓ) n . (25) In the above calculations, (23) follows from Lemma 28. Inequality (24) follows from symmetrization, whereas, (25) follows from Lemma 24. Proof of Part (b) Similar to the calculations in part (a), we note the following: E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \f\f\f ≤E sup E∈E |MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)| + E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯ˆµn, ν) \f\f\f ≤E sup E∈E MMDK(E♯ˆµn, E♯µ) + EMMDK(ˆνm, ν) + 2B r 1 n + 1 m ≤2R(Φ ◦ E, µ) + R(Φ, ν) + 2B r 1 n + 1 m ≲ r WeLe(log We + Le) log(nℓ) n + 1 √m. We are now ready to prove Lemma 25. For ease of readability, we restate the Lemma as follows. Lemma 25. Under assumption A2, the following holds: (a) E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ν) − MMD2 K(E♯µ, ν) \f\f\f ≲ q WeLe log We log(nℓ) n , (b) E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ˆνm) − MMD2 K(E♯µ, ν) \f\f\f ≲ q WeLe(log We+Le) log(nℓ) n + 1 √m. Proof. Proof of part (a) We begin by noting the following: E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ν) − MMD2 K(E♯µ, ν) \f\f\f =E sup E∈E \f\f\f\f2MMDK(E♯µ, ν) \u0010 \\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \u0011 + \u0010 \\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \u00112\f\f\f\f ≤2BE sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \f\f\f + E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν) \f\f\f 2 (26) ≲ r ℓWeLe(log We + Le) log n n . (27) Inequality (26) follows from applying Lemma 29, whereas, (27) is a consequence of Lemma (30). 36 Chakraborty and Bartlett Proof of part (b) Similarly, E sup E∈E \f\f\f\\ MMD 2 K(E♯ˆµn, ˆνm) − MMD2 K(E♯µ, ν) \f\f\f =E sup E∈E \f\f\f\f2MMDK(E♯µ, ν) \u0010 \\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \u0011 + \u0010 \\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \u00112\f\f\f\f ≤2BE sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \f\f\f + E sup E∈E \f\f\f\\ MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν) \f\f\f 2 ≲ r ℓWeLe(log We + Le) log n n + 1 √m. C.5 Proofs from Section 5.2 In this section, we prove the main result of this paper, i.e. Theorem 8. C.6 Proofs from Section 5.5 To begin our analysis, we first show the following: Theorem 31. Under assumptions, A1–3, V (µ, ν, ˆGn, ˆE) → 0, almost surely. Proof. For simplicity, we consider the estimator (5). A similar proof holds for estimator (6). Consider the oracle inequality (7). We only consider the case, when, diss = W1, the case when, diss = MMD2 K can be proved similarly. We note that F is a bounded function class, with bound Bc. Thus, a simple application of the bounded difference inequality yields that with probability at least 1 − δ/2, ∥ˆµn − µ∥F ≤ E∥ˆµn − µ∥F + θ1 r log(1/δ) n , for some positive constant θ1. The fourth term in (5) can be written as: sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)|. Suppose that ˆµ′ n denotes the empirical distribution on (X1, . . . , Xi−1, X′ i, . . . , Xn). Then replacing ˆµn with ˆµ′ n, yields an error at most, sup E∈E |W1(E♯ˆµn, ν) − W1(E♯ˆµ′ n, ν)| ≤ sup E∈E W1(E♯ˆµn, E♯ˆµ′ n) ≤ sup E∈E sup f:∥f∥Lip≤1 1 n|f(E(Xi)) − f(E(X′ i)| ≲ 1 n, since by construction, E’s are chosen from bounded ReLU functions. Again by a simple application of bounded difference inequality, we get, 2λ sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)| ≤ 2λE sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)| + θ2 r log(1/δ) n , 37 Chakraborty and Bartlett with probability at least 1 − δ/2. Hence, by union bound, with probability at least 1 − δ 2∥ˆµn − µ∥F + 2λ sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)| ≤2E∥ˆµn − µ∥F + 2λE sup E∈E |W1(E♯ˆµn, ν) − W1(E♯µ, ν)| + θ3 r log(1/δ) n , for some absolute constant θ3. Since, all other terms in (7) are bounded independent of the random sample, with probability at least 1 − δ, V (µ, ν, ˆG, ˆE) ≤ EV (µ, ν, ˆG, ˆE) + θ3 r log(1/δ) n . From the above, P(|V (µ, ν, ˆG, ˆE) − EV (µ, ν, ˆG, ˆE)| > ϵ) ≤ e− nϵ2 θ3 . this implies that P n≥1 P(|V (µ, ν, ˆG, ˆE) − EV (µ, ν, ˆG, ˆE)| > ϵ) < ∞. A simple application of the first Borel-Cantelli Lemma yields (see Proposi- tion 5.7 of Karr (1993)) that this implies that |V (µ, ν, ˆG, ˆE) − EV (µ, ν, ˆG, ˆE)| → 0, almost surely. Since, limn→∞ EV (µ, ν, ˆG, ˆE) = 0, the result follows. C.6.1 Proof of Proposition 12 Proposition 12. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·) and MMD2 K(·, ·) and the estimates (5) and (6), ˆE♯µ d−→ ν, almost surely. Proof. Let diss = W1. From Theorem 31, it is clear that, W1( ˆE♯µ, ν) → 0, almost surely. Since convergence in Wasserstein distance characterizes convergence in distribution, ˆE♯µ d−→ ν, almost surely. When, diss = MMD2 K, we can similarly say that MMD2 K( ˆE♯µ, ν) → 0, almost surely. From Theorem 3.2 (b) of Schreuder et al. (2021), we conclude that ˆE♯µ d−→ ν, almost surely. C.6.2 Proof of Proposition 13 Proposition 13. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·) and MMD2 K(·, ·) and the estimates (5) and (6), ∥id(·) − ˆG ◦ ˆE(·)∥2 L2(µ) a.s. −−→ 0. Proof. The proof follows from observing that 0 ≤ ∥id(·) − ˆG ◦ ˆE(·)∥2 L2(µ) ≤ V (µ, ν, ˆG, ˆE) and applying Theorem 31. C.6.3 Proof of Theorem 14 Theorem 14. Suppose that assumptions A1–3 hold and TV ( ˆE♯µ, ν) → 0, almost surely. Then, ˆG♯ν d−→ µ, almost surely. Proof. We begin by observing that, W1( ˆG♯ν, µ) ≤W1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) + W1( ˆG ◦ ˆE)♯µ, µ) (28) 38 Chakraborty and Bartlett We first note that TV ( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) = sup B∈B(Rd) |( ˆG♯ν)(B) − (( ˆG ◦ ˆE)♯µ)(B)| = sup B∈B(Rd) |ν( ˆG−1(B)) − ( ˆE♯µ)( ˆG−1(B))| ≤ sup B∈B(Rℓ) |ν(B) − ( ˆE♯µ)(B)| (29) =TV (ν, ˆE♯µ) → 0, almost surely. Here (29) follows from the fact that n ˆG−1(B) : B ∈ Rdo ⊆ Rℓ, since ˆG’s are measurable. Thus, TV ( ˆG♯ν, ( ˆG◦ ˆE)♯µ) → 0, almost surely. Since convergence in TV implies convergence in distribution, this implies that W1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) → 0, almost surely. We also note that, from Proposition 13, EX∼µ∥X − ˆG ◦ ˆE(X)∥2 → 0, almost surely. This implies that ∥X − ˆG ◦ ˆE(X)∥ P−→ 0, almost surely, which further implies that ˆG ◦ ˆE(X) d−→ X, almost surely. Hence, W1( ˆG ◦ ˆE)♯µ, µ) → 0, almost surely. Plugging these in (28) gives us the desired result. Theorem 15. Suppose that assumptions A1–3 hold and let the family of estimated generators { ˆGn}n∈N be uniformly equicontinuous, almost surely. Then, ˆGn ♯ ν d−→ µ, almost surely. Proof. We note that from the proof of Theorem 14, equation (28) holds and W1( ˆGn ◦ ˆEn)♯µ, µ) → 0, almost surely. We fix an ω in the sample space, for which, W1( ˆGn ω ◦ ˆEn ω)♯µ, µ) → 0 and ( ˆEn ω)♯µ d−→ ν. Here we use the subscript ω to show that ˆGn and ˆEn might depend on ω. Clearly, the set of all ω’s, for which this convergence holds, has probability 1. By Skorohod’s theorem, we note that we can find a sequence of random variables {Yn}n∈N and Z, such that Yn follows the distribution ˆEn ♯ µ and Z ∼ ν, such that Yn a.s. −−→ Z. Since { ˆGn ω}n∈N are uniformly equicontinuous, for any ϵ > 0, we can find δ > 0, such that if |yn − z| < δ, | ˆGn ω(yn) − ˆGn ω(z)| < ϵ. Thus, ˆGn ω(Yn) − ˆGn ω(Z) a.s. −−→ 0. Since this implies that ˆGn ω(Yn) − ˆGn ω(Z) d−→ 0, it is easy to see that, W1( ˆGn ω(Yn), ˆGn ω(Z)) → 0. Now, since, W1( ˆGn ω(Yn), ˆGn ω(Z)) = W1(( ˆGn ω)♯ν, ( ˆGn ω ◦ ˆEn ω)♯µ), we conclude that W1(( ˆGn ω)♯ν, ( ˆGn ω ◦ ˆEn ω)♯µ) → 0, as n → ∞. Thus, with probability one, the RHS of (28) goes to 0 as n → ∞. Hence, W1( ˆGn ♯ ν, µ) → 0, almost surely. C.6.4 Proof of Corollary 16 Corollary 16. Let diss(·, ·) = W1(·, ·) and suppose that the assumptions of Theorem 8 are satisfied and s > dµ. Also let supn∈N ∥ ˆGn∥Lip, supm,n∈N ∥ ˆGn,m∥Lip ≤ L, almost surely, for some L > 0. W1( ˆG♯ν, µ) ≲ V (µ, ν, ˆG, ˆE) for both estimators (5) and (6). Proof. Denoting ˆG as either of the estimators (5) and (6), it is easy to see that, W1( ˆG♯ν, µ) ≤W1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) + W1( ˆG ◦ ˆE)♯µ, µ) 39 Chakraborty and Bartlett ≤LW1(ν, ˆE♯µ) + W1( ˆG ◦ ˆE)♯µ, µ) ≲W1(ν, ˆE♯µ) + Z ∥ ˆG ◦ ˆE(x) − x∥2 2dµ(x) D Supporting Results for Approximation Guarantees Lemma 32. (Proposition 2 of Yarotsky (2017)) The function f(x) = x2 on the segment [0, 1] can be approximated with any error by a ReLU network, sqm(·), such that, 1. L(sqm), W(sqm) ≤ c1m. 2. sqm \u0000 k 2m \u0001 = \u0000 k 2m \u00012, for all k = 0, 1, . . . , 2m. 3. ∥sqm − x2∥L∞([0,1]) ≤ 1 22m+2 . Lemma 33. Let sqm(·) be taken as in Lemma 32, then, ∥sqm − x2∥Hβ ≤ 1 2m−1 . Proof. We begin by noting that, sqm(x) = \u0010 (k+1)2 2m − k2 2m \u0011 \u0000x − k 2m \u0001 + \u0000 k 2m \u00012, whenever, x ∈ \u0002 k 2m , k+1 2m \u0001 . Thus, on ( k 2m , k+1 2m )), ∥sqm − x2∥Hβ =∥sqm − x2∥L∞(( k 2m , k+1 2m )) + \r\r\r\r (k + 1)2 2m − k2 2m − 2x \r\r\r\r L∞(( k 2m , k+1 2m )) = 1 22m+2 + 1 2m ≤ 1 2m−1 . This implies that, ∥sqm − x2∥Hβ ≤ 1 2m−1 . Lemma 34. Let M > 0, then we can find a ReLU network prod(2) m , such that, 1. L(prod(2) m ), W(prod(2) m ) ≤ c2m, for some absolute constant c2. 2. ∥prod(2) m − xy∥L∞([−M,M]×[−M,M]) ≤ M 2 22m+1 . Proof. Let prod(2) m (x, y) = M 2 \u0010 sqm \u0010 |x+y| 2M \u0011 − sqm \u0010 |x−y| 2M \u0011\u0011 . Clearly, prod(2) m (x, y) = 0, if xy = 0. We note that, L(prod(2) m ) ≤ c1m+1 ≤ c2m and W(prod(2) m ) ≤ 2c1m+2 ≤ c2m, for some absolute constant c2. Clearly, ∥prod(2) m − xy∥L∞([−M,M]×[−M,M]) ≤ 2M 2∥sq − x2∥L([0,1]) ≤ M 2 22m+1 . Lemma 35. For any m ≥ 3, we can construct a ReLU network prod(d) m : Rd → R, such that for any x1, . . . , xd ∈ [−1, 1], ∥prod(d) m (x1, . . . , xd) − x1 . . . xd∥L∞([−1,1]d) ≤ d3 22m+2 . 40 Chakraborty and Bartlett Proof. Let M = 1 and d ≥ 2. We define prod(k) m (x1, . . . , xk) = prod(2) m (prod(k−1) m (x1, . . . , xk−1), xd), k ≥ 3. Clearly W(prod(d) m ), L(prod(d) m ) ≤ c3dm, for some absolute constant c3. We also note that, |prod(d) m (x1, . . . , xd)| ≤ M 2 22m+1 +xd|prod(d−1) m (x1, . . . , xd−1)| ≤ M 2 22m+1 +M|prod(d−1) m (x1, . . . , xd−1)| ≤ M 2 22m+1 + M 3 22m+1 + · · · + M d−1 22m+1 + M d ≤ M 2 22m+1 + (d − 2)M d = d − 2 + 1 22m+1 ≤ d − 1. From induction, it is easy to see that, prod(k) m ≤ d − 1. Taking M = d − 1, we get that, ∥prod(d) m (x1, . . . , xd) − x1 . . . xd∥L∞([−1,1]d) =∥prod(2) m (prod(d−1) m (x1, . . . , xd−1), xd) − x1 . . . xd∥L∞([−1,1]d) ≤∥prod(d−1) m (x1, . . . , xd−1) − x1 . . . xd−1∥L∞([−1,1]d) + M 2 22m+2 ≤ dM 2 22m+2 = d3 22m+2 . E Supporting Results from the Literature This section lists some of the supporting results from the literature, used in the paper. Lemma 36. (Kolmogorov and Tikhomirov, 1961) The ϵ-covering number of Hβ([0, 1]d, R, 1) can be bounded as, log N \u0000(ϵ; Hβ([0, 1]d), ∥ · ∥∞ \u0001 ≲ ϵ−d/β. Lemma 37. (Theorem 12.2 of Anthony and Bartlett (2009)) Assume for all f ∈ F, ∥f∥∞ ≤ M. Denote the pseudo-dimension of F as Pdim(F), then for n ≥ Pdim(F), we have for any ϵ and any X1, . . . , Xn, Nϵ, F|X1:n , ℓ∞) ≤ \u0012 2eMn ϵPdim(F) \u0013Pdim(F) . Lemma 38. (Theorem 6 of Bartlett et al. (2019)) Consider the function class computed by a feed-forward neural network architecture with W many weight parameters and U many computation units arranged in L layers. Suppose that all non-output units have piecewise-polynomial activation functions with p + 1 pieces and degrees no more than d, and the output unit has the identity function as its activation function. Then the VC-dimension and pseudo-dimension are upper-bounded as VCdim(F), Pdim(F) ≤ C · LW log(pU) + L2W log d. 41 "
}