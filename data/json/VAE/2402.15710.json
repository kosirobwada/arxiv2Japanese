{
    "optim": "A Statistical Analysis of Wasserstein Autoencoders for Intrinsically\nLow-dimensional Data\nSaptarshi Chakraborty∗1 and Peter L. Bartlett†1,2,3\n1Department of Statistics, UC Berkeley\n2Department of Electrical Engineering and Computer Sciences, UC Berkeley\n3Google DeepMind\nAbstract\nVariational Autoencoders (VAEs) have gained significant popularity among researchers as a powerful\ntool for understanding unknown distributions based on limited samples. This popularity stems partly\nfrom their impressive performance and partly from their ability to provide meaningful feature representa-\ntions in the latent space. Wasserstein Autoencoders (WAEs), a variant of VAEs, aim to not only improve\nmodel efficiency but also interpretability. However, there has been limited focus on analyzing their sta-\ntistical guarantees. The matter is further complicated by the fact that the data distributions to which\nWAEs are applied - such as natural images - are often presumed to possess an underlying low-dimensional\nstructure within a high-dimensional feature space, which current theory does not adequately account for,\nrendering known bounds inefficient. To bridge the gap between the theory and practice of WAEs, in this\npaper, we show that WAEs can learn the data distributions when the network architectures are properly\nchosen. We show that the convergence rates of the expected excess risk in the number of samples for\nWAEs are independent of the high feature dimension, instead relying only on the intrinsic dimension of\nthe data distribution.\n1\nIntroduction\nThe problem of understanding and possibly simulating samples from an unknown distribution only through\nsome independent realization of the same is a key question for the machine learning community. Parallelly\nwith the appearance of Generative Adversarial Networks (GANs) (Goodfellow et al., 2014), Variational\nAutoencoders (Kingma and Welling, 2014) have also gained much attention not only due to their useful\nfeature representation properties in the latent space but also for data generation capabilities. It is important\nto note that in GANs, the generator network learns to create new samples that are similar to the training\n∗email: saptarshic@berkeley.edu\n†email: peter@berkeley.edu\n1\narXiv:2402.15710v1  [cs.LG]  24 Feb 2024\nChakraborty and Bartlett\ndata by fooling the discriminator network.\nHowever, GANs and their popular variants do not directly\nprovide a way to manipulate the generated data or explore the latent space of the generator. On the other\nhand, a VAE learns a latent space representation of the input data and allows for interpolation between\nthe representations of different samples.\nSeveral variants of VAEs have been proposed to improve their\ngenerative performance. One popular variant is the conditional VAE (CVAE) (Sohn et al., 2015), which\nadds a conditioning variable to the generative model and has shown remarkable empirical success. Other\nvariants include InfoVAE (Zhao et al., 2017), β-VAE (Higgins et al., 2017), and VQ-VAE (Van Den Oord\net al., 2017), etc., which address issues such as disentanglement, interpretability, scalability, etc. Recent\nworks have shown the effectiveness of VAEs and their variants in a variety of applications, including image\n(Gregor et al., 2015) and text generation (Yang et al., 2017), speech synthesis (Tachibana et al., 2018), and\ndrug discovery (G´omez-Bombarelli et al., 2018). A notable example is the DALL-E model (Ramesh et al.,\n2021), which uses a VAE to generate images from textual descriptions.\nHowever, despite their effectiveness in unsupervised representation learning, VAEs have been heavily\ncriticized for their poor performance in approximating multi-modal distributions. Influenced by the superior\nperformance of GANs, researchers have attempted to leverage this advantage of adversarial losses by incor-\nporating them into VAE objective (Makhzani et al., 2016; Mescheder et al., 2017). Wasserstein Autoencoder\n(WAEs) (Tolstikhin et al., 2018) tackles the problem from an optimal transport viewpoint. Incorporating\nsuch a GAN-like architecture, not only preserves the latent space representation that is unavailable in GANs\nbut also enhances data generation capabilities. Both VAEs and WAEs attempt to minimize the sum of a\nreconstruction cost and a regularizer that penalizes the difference between the distribution induced by the\nencoder and the prior distribution on the latent space. While VAEs force the encoder to match the prior\ndistribution for each input example, which can lead to overlapping latent codes and reconstruction issues,\nWAEs force the continuous mixture of the encoder distribution over all input examples to match the prior\ndistribution, allowing different examples to have more distant latent codes and better reconstruction results.\nFurthermore, the use of the Wasserstein distance allows WAEs to incorporate domain-specific constraints\ninto the learning process. For example, if the data is known to have a certain structure or topology, this\ninformation can be used to guide the learning process and improve the quality of generated samples. This\nresults in a more robust model that can handle a wider range of distributions, including multimodal and\nheavy-tailed distributions.\nWhile VAE and its variants have demonstrated empirical success, little attention has been given to\nanalyzing their statistical properties. Recent developments from an optimization viewpoint include Rolinek\net al. (2019), who showed VAEs pursue Principal Component Analysis (PCA) embedding under certain\nsituations, and Koehler et al. (2022), who analyzed the implicit bias of VAEs under linear activation with two\nlayers. For explaining generalization, Tang and Yang (2021) proposed a framework for analyzing excess risk\n2\nChakraborty and Bartlett\nfor vanilla VAEs through M-estimation. When having access to n i.i.d. samples from the target distribution,\nChakrabarty and Das (2021) derived a bound based on the Vapnik-Chervonenkis (VC) dimension, providing a\nguarantee of O(n−1/2)-convergence with a non-zero margin of error, even under model specification. However,\ntheir analysis is limited to a parametric regime under restricted assumptions and only considers a theoretical\nvariant of WAEs, known as f-WAEs (Husain et al., 2019), which is typically not implemented in practice.\nDespite recent advancements in the understanding of VAEs and their variants, existing analyses fail to\naccount for the fundamental goal of these models, i.e. to understand the data generation mechanism where\none can expect the data to have an intrinsically low-dimensional structure. For instance, a key application\nof WAEs is to understand natural image generation mechanisms and it is believed that natural images have\na low-dimensional structure, despite their high-dimensional pixel-wise representation (Pope et al., 2020).\nFurthermore, the current state-of-the-art views the problem only through a classical learning theory approach\nto derive O(n−1/2) or faster rates (under additional assumptions) ignoring the model misspecification error.\nThus, such rates do not align with the well-known rates for classical non-parametric density estimation\napproaches (Kim et al., 2019). Additionally, these approaches only consider the scenario where the network\narchitecture is fixed, but in practice, larger models are often employed for big datasets.\nIn this paper, we aim to address the aforementioned shortcomings in the current literature and bridge\nthe gap between the theory and practice of WAEs. Our contributions include:\n• We propose a framework to provide an error analysis of Wasserstein Autoencoders (WAEs) when the\ndata lies in a low-dimensional structure in the high-dimensional representative feature space.\n• Informally, our results indicate that if one has n independent and identically distributed (i.i.d.) samples\nfrom the target distribution, then under the assumption of Lipschitz-smoothness of the true model, if\nthe corresponding networks are properly chosen, the error rate for the problem scales as ˜O\n\u0010\nn−\n1\n2+dµ\n\u0011\n,\nwhere, dµ is the upper Minkowski dimension of the support of the target distribution.\n• The networks can be chosen as having O(nγe) many weights for the encoder and O(nγg) for the\ngenerator, where, γe, γg ≤ 1 and only depend on dµ and ℓ (dimension of the latent space), respectively.\nFurthermore, the values of γe and γg decrease as the true model becomes smoother.\n• We show that one can ensure encoding and decoding guarantees, i.e. the encoded distribution is close\nenough to the target latent distribution, and the generator maps back the encoded points close to\nthe original points. Under additional regularity assumptions, we show that the approximating push-\nforward measure, induced by the generator, is close to the target distribution, in the Wasserstein sense,\nalmost surely.\n3\nChakraborty and Bartlett\n2000\n4000\n6000\n8000\n10000\n# Datapoints\n150\n200\n250\n300\n350\nFID Score\nd_int = 2\nd_int = 16\n(a) FID scores for GAN-\nWAE\n2000\n4000\n6000\n8000\n10000\n# Datapoints\n0.02\n0.04\n0.06\nReconstruction Error\nd_int = 2\nd_int = 16\n(b)\nReconstruction\nerror\nfor GAN-WAE\n2000\n4000\n6000\n8000\n10000\n# Datapoints\n150\n200\n250\n300\nFID Score\nd_int = 2\nd_int = 16\n(c) FID scores for MMD-\nWAE\n2000\n4000\n6000\n8000\n10000\n# Datapoints\n0.005\n0.010\n0.015\n0.020\n0.025\nReconstruction Error\nd_int = 2\nd_int = 16\n(d) Reconstruction error for\nMMD-WAE\nFigure 1: Average generalization error (in terms of FID scores) and reconstruction test errors for different\nvalues of n for GAN and MMD variants of WAE. The error bars denote the standard deviation out of 10\nreplications.\n2\nA Proof of Concept\nBefore we theoretically explore the problem, we discuss an experiment to demonstrate that the error rates\nfor WAEs depend primarily only on the intrinsic dimension of the data. Since it is difficult to assess the\nintrinsic dimensionality of natural images, we follow the prescription of Pope et al. (2020) to generate low-\ndimensional synthetic images. We use a pre-trained Bi-directional GAN (Donahue et al., 2017) with 128\nlatent entries and outputs of size 128 × 128 × 3, trained on the ImageNet dataset (Deng et al., 2009). Using\nthe decoder of this pre-trained BiGAN, we generate 11, 000 images, from the class, soap-bubble where we\nfix most entries of the latent vectors to zero leaving only d int free entries. We take d int to be 2 and 16,\nrespectively. We reduce the image sizes to 28 × 28 for computational ease. We train a WAE model with the\nstandard architecture as proposed by Tolstikhin et al. (2018) with the number of training samples varying\nin {2000, 4000, . . . , 10000} and keep the last 1000 images for testing. For the latent distribution, we use the\nstandard Gaussian distribution on the latent space R8 and use the Adam optimizer (Kingma and Ba, 2015)\nwith a learning rate of 0.0001. We also take λ = 10 for the penalty on the dissimilarity in objective (4).\nAfter training for 10 epochs, we generate 1000 sample images from the distribution ˆG♯ν (see Section 3 for\nnotations) and compute the Frechet Inception Distance (FID) (Heusel et al., 2017) to assess the quality of\nthe generated samples with respect to the target distribution. We also compute the reconstruction error for\nthese test images. We repeat the experiment 10 times and report the average. The experimental results for\nboth variants of WAE, i.e. the GAN and MMD are shown in Fig. 1. It is clear from Fig. 1 that the error\nrates for d int = 2 is lower than for the case d int = 16. The codes for this experimental study can be\nfound at https://github.com/SaptarshiC98/WAE.\n4\nChakraborty and Bartlett\n3\nBackground\n3.1\nNotations and some Preliminary Concepts\nThis section introduces preliminary notation and concepts for theoretical analyses.\nNotation\nWe use notations x ∨ y := max{x, y} and x ∧ y := min{x, y}. T♯µ denotes the push-forward\nof measure µ by the map T. For function f : S → R, and probability measure γ on S, let ∥f∥Lp(γ) :=\n\u0000R\nS |f(x)|pdγ(x)\n\u00011/p. Similarly, ∥f∥L∞(A) := supx∈A |f(x)|. For any function class F, and distributions\nP and Q, ∥P − Q∥F = supf∈F |\nR\nfdP −\nR\nfdQ| denotes the Integral Probability Metric (IPM) w.r.t.\nF. We say An ≲ Bn (also written as An = O(Bn)) if there exists C > 0, independent of n, such that\nAn ≤ CBn. Similarly, we use the notation, An ≾ Bn (also written as An = ˜O(Bn)) if An ≤ CBn logC(en),\nfor some C > 0. We say An ≍ Bn, if An ≲ Bn and Bn ≲ An. For a function f : Rd1 → Rd2, we write,\n∥f∥Lip = supx̸=y\n∥f(x)−f(y)∥2\n∥x−y∥2\n.\nDefinition 1 (Neural networks). Let L ∈ N and {Ni}i∈[L] ⊂ N. Then a L-layer neural network f : Rd →\nRNL is defined as,\nf(x) = AL ◦ σL−1 ◦ AL−1 ◦ · · · ◦ σ1 ◦ A1(x)\n(1)\nHere, Ai(y) = Wiy + bi, with Wi ∈ RNi×Ni−1 and bi ∈ RNi−1, with N0 = d. σj is applied component-wise.\nHere, {Wi}1≤i≤L are known as weights, and {bi}1≤i≤L are known as biases. {σi}1≤i≤L−1 are known as the\nactivation functions. Without loss of generality, one can take σℓ(0) = 0, ∀ ℓ ∈ [L−1]. We define the following\nquantities: (Depth) L(f) := L is known as the depth of the network; (Number of weights) The number of\nweights of the network f is denoted as W(f).\nNN{σi}i∈[L−1](L, W, R) = {f of the form (1) : L(f) ≤ L, W(f) ≤ W, sup\nx∈Rd ∥f(x)∥∞ ≤ R}.\nIf σj(x) = x ∨ 0, for all j = 1, . . . , L − 1, we denote NN{σi}1≤i≤L−1(L, W, R) as RN(L, W, R). We often omit\nR in cases where it is clear that R is bounded by a constant.\nDefinition 2 (H¨older functions). Let f : S → R be a function, where S ⊆ Rd. For a multi-index s =\n(s1, . . . , sd), let, ∂sf =\n∂|s|f\n∂xs1\n1 ...∂x\nsd\nd , where, |s| = Pd\nℓ=1 sℓ. We say that a function f : S → R is β-H¨older (for\nβ > 0) if\n∥f∥Hβ :=\nX\ns:0≤|s|<⌊β⌋\n∥∂sf∥∞ +\nX\ns:|s|=⌊β⌋\nsup\nx̸=y\n∥∂sf(x) − ∂sf(y)∥\n∥x − y∥β−⌊β⌋\n< ∞.\nIf f : Rd1 → Rd2, then we define ∥f∥Hβ = Pd2\nj=1 ∥fj∥Hβ.\nFor notational simplicity, let, Hβ(S1, S2, C) = {f : S1 → S2 : ∥f∥Hβ ≤ C}. Here, both S1 and S2 are\nboth subsets of a real vector spaces.\n5\nChakraborty and Bartlett\nDefinition 3 (Maximum Mean Discrepancy (MMD)). Let HK be the Reproducible Kernel Hilbert Space\n(RKHS) corresponding to the reproducing kernel K(·, ·), defined on Rd. Let the corresponding norm in\nthis RKHS be ∥ · ∥HK. The Maximum Mean Discrepancy between two distributions P and Q is defined as:\nMMDK(P, Q) = supf:∥f∥HK≤1\n\u0000R\nfdP −\nR\nfdQ\n\u0001\n.\n3.2\nWasserstein Autoencoders\nLet µ be a distribution in the data-space X = [0, 1]d and Z = [0, 1]ℓ be the latent space. In Wasserstein\nAutoencoders (Tolstikhin et al., 2018), one tries to learn a generator map, G : Z → X and an encoder map\nE : X → Z by minimizing the following objective,\nV (µ, ν, G, E) =\nZ\nc(x, G ◦ E(x))dµ(x) + λdiss(E♯µ, ν).\n(2)\nHere, λ > 0 is a hyper-parameter, often tuned based on the data. The first term in (2) aims to minimize a\nreconstruction error, i.e. the decoded value of the encoding should approximately result in the same value.\nThe second term ensures that the encoded distribution is close to a known distribution ν that is easy to\nsample from. The function c(·, ·)-is a loss function on the data space. For example, Tolstikhin et al. (2018)\ntook c(x, y) = ∥x − y∥2\n2. diss(·, ·) is a dissimilarity measure between probability distributions defined on the\nlatent space. Tolstikhin et al. (2018) recommended either a GAN-based dissimilarity measure or a Maximum\nMean Discrepancy (MMD)-based measure (Gretton et al., 2012). In this paper, we will consider the special\ncases, where this dissimilarity measure is taken to be the Wasserstein-1 metric, which is the dissimilarity\nmeasure for WGANs (Arjovsky et al., 2017; Gulrajani et al., 2017) or the squared MMD-metric.\nIn practice, however, one does not have access to µ but only a sample {Xi}i∈[n], assumed to be inde-\npendently generated from µ. Let ˆµn be the empirical measure based on the data. One then minimizes the\nfollowing empirical objective to estimate E and G.\nV (ˆµn, ν, G, E) =\nZ\nc(x, G ◦ E(x))dˆµn(x) + λd\ndiss(E♯ˆµn, ν).\n(3)\nHere, d\ndiss(·, ·) is an estimate of diss(·, ·), based only on the data, {Xi}i∈[n]. For example, if diss(·, ·) is taken\nto be the Wasserstein-1 metric, then, d\ndiss(E♯ˆµn, ν) = W1(E♯ˆµn, ν). On the other hand, if diss(·, ·) is taken\nto be the MMD2\nK-measure, one can take,\nd\ndiss(E♯ˆµn, ν) =\n1\nn(n − 1)\nX\ni̸=j\nK(E(Xi), E(Xj)) + EZ,Z′∼νK(Z, Z′) − 2\nn\nn\nX\ni=1\nZ\nK(E(Xi), z)dν(z).\nOf course, in practice, one does a further estimation of the involved dissimilarity measure through taking an\nestimate ˆνm, based on m i.i.d samples {Zj}j∈[m] from ν, i.e. ˆνm = 1\nm\nPm\nj=1 δZj. In this case the estimate of\nV in (2) is given by,\nV (ˆµn, ˆνm, G, E) =\nZ\nc(x, G ◦ E(x))dˆµn(x) + λd\ndiss(E♯ˆµn, νm).\n(4)\n6\nChakraborty and Bartlett\nIf diss(·, ·) is taken to be the Wasserstein-1 metric, then, d\ndiss(E♯ˆµn, ˆνm) = W1(E♯ˆµn, ˆνm). On the other\nhand, if diss(·, ·) is taken to be the MMD2\nK-measure, one can take,\nd\ndiss(E♯ˆµn, ˆνm) =\n1\nn(n − 1)\nX\ni̸=j\nK(E(Xi), E(Xj)) +\n1\nm(m − 1)\nX\ni̸=j\nK(Zi, Zj) −\n2\nnm\nn\nX\ni=1\nm\nX\nj=1\nK(E(Xj), Zj).\nSuppose that ∆opt > 0 is the optimization error. The empirical WAE estimates satisfy the following\nproperties:\n( ˆGn, ˆEn) ∈\n\u001a\nG ∈ G, E ∈ E : V (ˆµn, ν, G, E) ≤\ninf\nG∈G,E∈E V (ˆµn, ν, G, E) + ∆opt\n\u001b\n(5)\n( ˆGn,m, ˆEn,m) ∈\n\u001a\nG ∈ G, E ∈ E : V (ˆµn, ˆνn, G, E) ≤\ninf\nG∈G,E∈E V (ˆµn, ˆνm, G, E) + ∆opt\n\u001b\n.\n(6)\nThe functions in G and E are implemented through neural networks with ReLU activation RN(Lg, Wg) and\nRN(Le, We), respectively.\n4\nIntrinsic Dimension of Data Distribution\nReal data is often assumed to have a lower-dimensional structure within the high-dimensional feature space.\nVarious approaches have been proposed to characterize this low dimensionality, with many using some form\nof covering number to measure the effective dimension of the underlying measure. Recall that the ϵ-covering\nnumber of S w.r.t. the metic ϱ is defined as N(ϵ; S, ϱ) = inf{n ∈ N : ∃ x1, . . . xn such that ∪n\ni=1 Bϱ(xi, ϵ) ⊇\nS}, with Bϱ(x, ϵ) = {y : ϱ(x, y) < ϵ}. We characterize this low-dimensional nature of the data, through the\n(upper) Minkowski dimension of the support of µ. We recall the definition of Minkowski dimensions,\nDefinition 4 (Upper Minkowski dimension). For a bounded metric space (S, ϱ), the upper Minkwoski di-\nmension of S is defined as dimM(S, ϱ) = lim supϵ↓0\nlog N (ϵ; S, ϱ)\nlog(1/ϵ)\n.\nThroughout this analysis, we will assume that ϱ is the ℓ∞-norm and simplify the notation to dimM(S).\ndimM(S, ϱ) essentially measures how the covering number of S is affected by the radius of balls covering that\nset. As the concept of dimensionality relies solely on covering numbers and doesn’t require a smooth mapping\nto a lower-dimensional Euclidean space, it encompasses both smooth manifolds and even highly irregular\nsets like fractals. In the literature, Kolmogorov and Tikhomirov (1961) provided a comprehensive study on\nthe dependence of the covering number of different function classes on the underlying Minkowski dimension\nof the support.\nNakada and Imaizumi (2020) showed how deep regression learners can incorporate this\nlow-dimensionality of the data that is also reflected in their convergence rates. Recently, Huang et al. (2022)\nshowed that WGANs can also adapt to this low-dimensionality of the data. For any measure µ on [0, 1]d,\nwe use the notation dµ := dimM(supp(µ)). When the data distribution is supported on a low-dimensional\nstructure in the nominal high-dimensional feature space, one can expect dµ ≪ d.\n7\nChakraborty and Bartlett\nIt can be observed that the image of a unit hypercube under a H¨older map has a Minkowski dimension\nthat is no more than the dimension of the pre-image divided by the exponent of the H¨older map.\nLemma 5. Let, f ∈ Hγ \u0000A, [0, 1]d2, C\n\u0001\n, with A ⊆ [0, 1]d1. Then, dimM (f (A)) ≤ dimM(A)/(γ ∧ 1).\n5\nTheoretical Analyses\n5.1\nAssumptions and Error Decomposition\nTo facilitate theoretical analysis, we assume that the data distributions are realizable, meaning that a “true”\ngenerator and a “true” encoder exist. Specifically, we make the assumption that there is a true smooth\nencoder that maps the µ to ν, and the left inverse of this true encoder exists and is also smooth. Formally,\nA1. There exists ˜G ∈ Hαg([0, 1]d, [0, 1]ℓ, C) and ˜E ∈ Hαe([0, 1]ℓ, [0, 1]d, C), such that, ˜E♯µ = ν and ( ˜G ◦\n˜E)(·) = id(·), a.e. [µ].\nIt is also important to note that A1 entails that the manifold has a single chart, in a probabilistic sense,\nwhich is a strong assumption. Naturally, when it comes to GANs, one can work with a weaker assumption\nas the learning task becomes notably much simpler as one does not have to learn an inverse map to the\nlatent space. A similar problem, while analyzing autoencoders, was faced by Liu et al. (2023) where they\ntackled the problem by considering chart-autoencoders, which have additional components in the network\narchitecture, compared to regular autoencoders. A similar approach of employing chart-based WAEs could\nbe proposed and subjected to rigorous analysis. This potential avenue could be an intriguing direction for\nfuture research.\nOne immediate consequence of assumption A1 ensures that the generator maps ν to the target µ. We\ncan also ensure that the latent distribution remains unchanged if one passes it through the generator and\nmaps it back through the encoder. Furthermore, the objective function (2) at this true encoder-generator\npair takes the value, zero, as expected.\nProposition 6. Under assumption A1, the following holds: (a) ˜G♯ν = µ, (b) ( ˜E◦ ˜G)♯ν = ν, (c) V (µ, ν, ˜G, ˜E) =\n0.\nFrom Lemma 5, It is clear that dµ = dimM (supp(µ)) ≤ dimM\n\u0010\n˜G\n\u0000[0, 1]ℓ\u0001\u0011\n≤ max {ℓ/(αg ∧ 1), d} . If\nℓ ≪ d and αg is not very small, then, dµ = (αg ∧ 1)−1ℓ ≪ d. Thus, the usual conjecture of dµ ≪ d can be\nmodeled through assumption A1 when the latent space has a much smaller dimension and the true generator\nis well-behaved, i.e. αg is not too small.\nA key step in the theoretical analysis is the following oracle inequality that bounds the excess risk in\nterms of the optimization error, misspecification error, and generalization error.\n8\nChakraborty and Bartlett\nLemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Then the\nfollowing hold:\nV (µ, ν, ˆGn, ˆEn) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|d\ndiss(E♯ˆµn, ν) − diss(E♯µ, ν)|.\n(7)\nV (µ, ν, ˆGn,m, ˆEn,m) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|d\ndiss(E♯ˆµn, ˆνm) − diss(E♯µ, ν)|.\n(8)\nHere, ∆miss = infG∈G,E∈E V (µ, ν, G, E) denotes the misspecification error for the problem.\nFor our theoretical analysis, we need to ensure that the used kernel in the MMD and the loss function\nc(·, ·) are regular enough. To impose such regularity, we assume the following:\nA2. We assume that, (a) for some B > 0, K(x, y) ≤ B2, for all x, y ∈ [0, 1]ℓ; (b) For some τk, |K(x, y) −\nK(x′, y′)| ≤ τk(∥x − x′∥2 + ∥y − y′∥2).\nA3. The loss function c(·, ·) is Lipschitz on [0, 1]d ×[0, 1]d, i.e. |c(x, y)−c(x′, y′)| ≤ τc(∥x−x′∥2 +∥y −y′∥2)\nand c(x, y) ≤ Bc, for all, x, y ∈ [0, 1]d.\n5.2\nMain Result\nUnder assumptions A1–3, one can control the expected excess risk of the WAE problem for both the W1 and\nMMD dissimilarities. The main idea is to select appropriate sizes for the encoder and generator networks,\nthat minimize both the misspecification errors and generalization errors to bound the expected excess risk\nusing Lemma 7. Theorem 8 shows that one can appropriately select the network size in terms of the number\nof samples available, i.e n, to achieve a trade-off between the generalization and misspecification errors\nas selecting a larger network facilitates better approximation but makes the generalization gap wider and\nvice-versa. The main result of this paper is stated as follows.\nTheorem 8. Suppose that assumptions A1–3 hold and ∆opt ≤ ∆ for some fixed non-negative threshold ∆.\nFurthermore, suppose that s > dµ. Then we can find n0 ∈ N and β > 0, that might depend on d, ℓ, αg, αe, ˜G\nand ˜E, such that if n ≥ n0, we can choose G = RN(Lg, Wg) and E = RN(Le, We), with, Le ≤ β log n, We ≤\nβn\ns\n2αe+s log n, Lg ≤ β log n and Wg ≤ βn\nℓ\nαe(αg∧1)+ℓ log n, then, for the estimation problem (5),\n(a) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1) ,ℓ\n\u001b\nlog2 n, for diss(·, ·) = W1(·, ·),\n(b) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\n2+max\n\u001a\nℓ\nαg ,\ns\nαe(αg∧1)\n\u001b\nlog2 n, for diss(·, ·) = MMD2\nK(·, ·).\nFurthermore, for the estimation problem (6), if m ≥ n ∨ n\n\u0010\nmax\nn\n2+\nℓ\nαg ,2+\ndµ\nαe(αg∧1) ,ℓ\no\u0011−1(ℓ∨2)\n(c) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1) ,ℓ\n\u001b\nlog2 n, for diss(·, ·) = W1(·, ·),\n(d) EV (µ, ν, ˆGn,m, ˆEn,m) ≲ ∆ + n\n−\n1\n2+max\n\u001a\nℓ\nαg ,\ns\nαe(αg∧1)\n\u001b\nlog2 n, for diss(·, ·) = MMD2\nK(·, ·).\nBefore we proceed, we observe some key consequences of Theorem 8.\n9\nChakraborty and Bartlett\nRemark 9 (Number of Weights). We note that Theorem 8 suggests that one can choose the networks to\nhave number of weights to be an exponent of n, which is smaller than 1. Moreover, this exponent only\ndepends on the dimensions of the latent space and the intrinsic dimension of the data. Furthermore, for\nsmooth models i.e. αe and αg are large, one can choose smaller networks that require less many parameters\nas opposed to non-smooth models as also observed in practice since easier problems require less complicated\nnetworks.\nRemark 10 (Rates for Lipschitz models). For all practical purposes, one can assume that the dimension of\nthe latent space is at least 2. If the true models are Lipschitz, i.e. if αe = αg = 1, then we can conclude that\nℓ = dµ. Hence, for both models, we observe that the excess risk scales as ˜O(n−\n1\n2+dµ ), barring the poly-log\nfactors. This closely matches rates for the excess risks for GANs (Huang et al., 2022).\nRemark 11 (Inference for Data on Manifolds). We recall that we call a set M is ˜d-regular w.r.t. the\n˜d-dimensional Hausdorff measure H ˜d if H(Bϱ(x, r)) ≍ r ˜d, for all x ∈ M (see Definition 6 of Weed and\nBach (2019)). It is known (Mattila, 1999) that if M is ˜d-regular, then the Minkowski dimension of M is ˜d.\nThus, when supp(µ) is ˜d-regular, dµ = ˜d. Since compact ˜d-dimensional differentiable manifolds are ˜d-regular\n(Proposition 9 of Weed and Bach (2019)), this implies that for when supp(µ) is a compact differentiable\n˜d-dimensional manifold, the error rates for the sample estimates scale as in Theorem 8, with dµ replaced\nwith ˜d. A similar result holds when supp(µ) is a nonempty, compact convex set spanned by an affine space\nof dimension ˜d; the relative boundary of a nonempty, compact convex set of dimension ˜d + 1; or self-similar\nset with similarity dimension ˜d.\n5.3\nRelated work on GANs\nTo contextualize our contributions, we conduct a qualitative comparison with existing GAN literature.\nNotably, Chen et al. (2020) expressed the generalization rates for GAN when the data is restricted to an\naffine subspace or has a mixture representation with smooth push-forward measures; while Dahal et al.\n(2022) derived the convergence rates under the Wasserstein-1 distance in terms of the manifold dimension.\nBoth Liang (2021) and Schreuder et al. (2021) study the expected excess risk of GANs for smooth generator\nand discriminator function classes. Liu et al. (2021) studied the properties of Bidirectional GANs, expressing\nthe rates in terms of the number of data points, where the exponents depend on the full data and latent\nspace dimensions.\nIt is important to note that both Dahal et al. (2022) and Liang (2021) assume that\nthe densities of the target distribution (either w.r.t Hausdorff or the Lebesgue measure) are bounded and\nsmooth. In comparison, we do not make any assumption of the existence of density (or its smoothness)\nfor the target distribution and consider the practical case where the generator is realized through neural\nnetworks as opposed to smooth functions as done by Liang (2021) and Schreuder et al. (2021). Diverging\n10\nChakraborty and Bartlett\nfrom the hypotheses of Chen et al. (2020), we do not presuppose that the support of the target measure forms\nan affine subspace. Furthermore, the analysis by Liu et al. (2021) derives rates that depend on the dimension\nof the entire space and not the manifold dimension of the support of the data as done in this analysis. It is\nimportant to emphasize that Huang et al. (2022) arrived at a rate comparable to ours concerning WGANs\n(Arjovsky et al., 2017). While both studies share a common overarching approach in addressing the problem\nby bounding the error using an oracle inequality and managing individual terms, our method necessitates\nextra assumptions to guarantee the generative capability of WAEs, which does not apply to WGANs due\nto their simpler structure. Interestingly, our derived rates closely resemble those found in GAN literature.\nThis suggests limited room for substantial improvement. However, demonstrating minimaxity remains a\nsignificant challenge and a promising avenue for future research.\n5.4\nProof Overview\nFrom Lemma 7, it is clear that the expected excess risk can be bounded by the misspecification error\n∆miss and the generalization gap, ∥ˆµn − µ∥F + λ supE∈E |d\ndiss(E♯ˆµn, ν) − diss(E♯µ, ν)|. To control ∆miss, we\nfirst show that if the generator and encoders are chosen as G = RN(Wg, Lg) and E = RN(We, Le), with\nLe ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe\ne\nlog(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg\ng\nlog(1/ϵg) then, ∆miss ≲\nϵg + ϵαg∧1\ne\n. On the other hand, we show that the generalization error is roughly\np\nn−1WeLe log We log n +\np\nn−1(We + Wg)(Le + Lg) log(We + Wg) log n, with additional terms depending on the estimator. Thus, the\nbounds in Lemma 7, leads to a bound roughly,\n∆ + ϵg + ϵ\nαg∧1\ne\n+\np\nn−1WeLe log We log n +\np\nn−1(We + Wg)(Le + Lg) log(We + Wg) log n.\n(9)\nBy the choice of the networks, we can upper bound the above as a function of ϵg and ϵe and then minimize\nthe expression w.r.t. these two variables to arrive at the bounds of Theorem 8. Of course, the bound in (9)\nchanges slightly based on the estimates and the dissimilarity measure. We refer the reader to the appendix,\nwhich contains the details of the proof.\n5.5\nImplications of the Theoretical Results\nApart from finding the error rates for the excess risk for the WAE problem, in what follows, we also ensure\na few desirable properties of the obtained estimates. For simplicity, we ignore the optimization error and set\n∆opt = 0.\nEncoding Guarantee\nSuppose we fix λ > 0, then, it is clear from Theorem 8 that EW1( ˆE♯µ, ν) ≲\nn\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1) ,ℓ\n\u001b\nlog2 n and EMMD2\nK( ˆE♯µ, ν) ≲ n\n−\n1\n2+max\n\u001a\nℓ\nαg ,\ns\nαe(αg∧1)\n\u001b\nlog2 n. We can not only char-\nacterize the expected rate of convergence of ˆE♯µ to ν but also can say that ˆE♯µ converges in distribution to\nν, almost surely. This is formally stated in the following proposition.\n11\nChakraborty and Bartlett\nProposition 12. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·)\nand MMD2\nK(·, ·) and the estimates (5) and (6), ˆE♯µ\nd−→ ν, almost surely.\nTherefore, if the number of data points is large, i.e., n is large, then the estimated encoded distribution\nˆE♯µ will converge to the true target latent distribution ν almost surely, indicating that the latent distribution\ncan be approximated through encoding with a high degree of accuracy.\nDecoding Guarantee\nOne can also show that E\nR\nc(x, ˆG ◦ ˆE(x))dµ(x) ≤ EV (µ, ν, ˆG, ˆE), for both the\nestimates in (5) and (6). For simplicity, if we let c(x, y) = ∥x − y∥2\n2, then, it can easily seen that, E∥id(·) −\nˆG ◦ ˆE(·)∥2\nL2(µ) → 0 as n → ∞, where id(x) = x is the identity map from Rd → Rd. Furthermore, it can be\nshown that, ∥id(·) − ˆG ◦ ˆE(·)∥2\nL2(µ)\na.s.\n−−→ 0 as stated in Corollary 13\nProposition 13. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·)\nand MMD2\nK(·, ·) and the estimates (5) and (6), ∥id(·) − ˆG ◦ ˆE(·)∥2\nL2(µ)\na.s.\n−−→ 0.\nProposition 13 guarantees that the generator is able to map back the encoded points to the original data\nif a sufficiently large amount of data is available. In other words, if one has access to a large number of\nsamples from the data distribution, then the generator is able to learn a mapping, from the encoded points\nto the original data, that is accurate enough to be useful.\nData Generation Guarantees\nA key interest in this theoretical exploration is whether one can guarantee\nthat one can generate samples from the unknown target distribution µ, through the generator, i.e. whether\nˆG♯ν is close enough to µ in some sense. However, one requires some additional assumptions (Chakrabarty and\nDas, 2021; Tang and Yang, 2021) on ˆG or the nature of convergence of ˆE♯µ to ν to ensure this. We present the\ncorresponding results subsequently as follows. Before proceeding, we recall the definition of Total Variation\n(TV) distance between two measures γ1 and γ2, defined on Ω, as, TV (γ1, γ2) = supB∈B(Ω) |γ1(B) − γ2(B)|,\nwhere, B(Ω) denotes the Borel σ-algebra on Ω.\nTheorem 14. Suppose that assumptions A1–3 hold and TV ( ˆE♯µ, ν) → 0, almost surely. Then, ˆG♯ν\nd−→ µ,\nalmost surely.\nWe note that convergence in TV is a much stronger assumption than convergence in W1 or MMD in the\nsense that TV convergence implies weak convergence but not the other way around.\nAnother way to ensure that ˆG♯ν converges to µ is to put some sort of regularity on the generator estimates.\nTang and Yang (2021) imposed a Lipschitz assumption to ensure this, but one can also work with something\nweaker, such as uniform equicontinuity of the generators. Recall that we say a family of functions, F is\nuniformly equicontinuous if, for anyf ∈ F and for all ϵ > 0, there exists a δ > 0 such that, |f(x) − f(y)| ≤ ϵ,\nwhenever, ∥x − y∥ ≤ δ.\n12\nChakraborty and Bartlett\nTheorem 15. Suppose that assumptions A1–3 hold and let the family of estimated generators { ˆGn}n∈N be\nuniformly equicontinuous, almost surely. Then, ˆGn\n♯ ν\nd−→ µ, almost surely.\nUniformly Lipschitz Generators\nSuppose that diss(·, ·) = W1(·, ·). If one assumes that the estimated\ngenerators are uniformly Lipschitz, then, one can say that W1( ˆG♯ν, µ) is upper bounded by V (µ, ν, ˆG, ˆE),\ndisregarding some constants.Thus, the same rate of convergence as in Theorem 8 holds for uniformly Lipschitz\ngenerator. We state this result formally as a corollary as follows.\nCorollary 16. Let diss(·, ·) = W1(·, ·) and suppose that the assumptions of Theorem 8 are satisfied and\ns > dµ. Also let supn∈N ∥ ˆGn∥Lip, supm,n∈N ∥ ˆGn,m∥Lip ≤ L, almost surely, for some L > 0. W1( ˆG♯ν, µ) ≲\nV (µ, ν, ˆG, ˆE) for both estimators (5) and (6).\nIt is important to note that although assumptions A1–3 do not directly guarantee either of these two\nconditions, it is reasonable to expect the assumptions made in Theorems 14 and 15 to hold in practice.\nThis is because regularization techniques are commonly used to ensure the learned networks ˆE and ˆG are\nsufficiently well-behaved. These techniques can impose various constraints, such as weight decay or dropout,\nthat encourage the networks to have desirable properties, such as smoothness or sparsity. Therefore, while\nthe assumptions made in the theorems cannot be directly ensured by A1–3, they are likely to hold in\npractice with appropriate regularization techniques applied to the network training. It would be a key step\nin furthering our understanding to develop a similar error analysis for such regularized networks and we\nleave this as a promising direction for future research.\n6\nDiscussions and Conclusion\nIn this paper, we developed a framework to analyze error rates for learning unknown distributions using\nWasserstein Autoencoders, especially when data points exhibit an intrinsically low-dimensional structure in\nthe representative high-dimensional feature space. We characterized this low dimensionality with the so-\ncalled Minkowski dimension of the support of the target distribution. We developed an oracle inequality to\ncharacterize excess risk in terms of misspecification, generalization, and optimization errors for the problem.\nThe excess risk bounds are obtained by balancing model-misspecification and stochastic errors to find proper\nnetwork architectures in terms of the number of samples that achieve this tradeoff. Our framework allows\nus to analyze the accuracy of encoding and decoding guarantees, i.e., how well the encoded distribution\napproximates the target latent distribution, and how well the generator maps back the latent codes close\nto the original data points.\nFurthermore, with additional regularity assumptions, we establish that the\napproximating push-forward measure can effectively approximate the target distribution.\n13\nChakraborty and Bartlett\nWhile our findings provide valuable insights into the theoretical characteristics of Wasserstein Autoen-\ncoders (WAEs), it’s crucial to acknowledge that achieving accurate estimates of the overall error in practical\napplications necessitates the consideration of an optimization error term. However, the precise estimation\nof this term poses a significant challenge due to the non-convex and intricate nature of the optimization\nprocess. Importantly, our error analysis remains independent of this optimization error and can seamlessly\nintegrate with analyses involving such optimization complexities. Future work in this direction might in-\nvolve attempting to improve the derived bounds by replacing the Minkowski dimension with the Wasserstein\n(Weed and Bach, 2019) or entropic dimension (Chakraborty and Bartlett, 2024). Furthermore, the minimax\noptimality of the upper bounds derived remains an open question, offering opportunities for fruitful research\nin understanding the model’s theoretical properties from a statistical viewpoint. Exploring future directions\nin deep federated classification models can yield fruitful research avenues.\nAcknowledgment\nWe gratefully acknowledge the support of the NSF and the Simons Foundation for the Collaboration on\nthe Theoretical Foundations of Deep Learning through awards DMS-2031883 and #814639 and the NSF’s\nsupport of FODSI through grant DMS-2023505.\nReferences\nAnthony, M. and Bartlett, P. (2009). Neural network learning: Theoretical foundations. Cambridge Univer-\nsity Press.\nArjovsky, M., Chintala, S., and Bottou, L. (2017). Wasserstein generative adversarial networks. In Interna-\ntional Conference on Machine Learning, pages 214–223. PMLR.\nBartlett, P. L., Harvey, N., Liaw, C., and Mehrabian, A. (2019). Nearly-tight VC-dimension and pseu-\ndodimension bounds for piecewise linear neural networks. The Journal of Machine Learning Research,\n20(1):2285–2301.\nBartlett, P. L. and Mendelson, S. (2002). Rademacher and Gaussian Complexities: Risk Bounds and Struc-\ntural Results. Journal of Machine Learning Research, 3(Nov):463–482.\nBoucheron, S., Lugosi, G., and Massart, P. (2013). Concentration inequalities: A nonasymptotic theory of\nindependence. Oxford University Press.\nChakrabarty, A. and Das, S. (2021). Statistical regeneration guarantees of the wasserstein autoencoder with\nlatent space consistency. Advances in Neural Information Processing Systems, 34:17098–17110.\n14\nChakraborty and Bartlett\nChakraborty, S. and Bartlett, P. L. (2024). On the statistical properties of generative adversarial models for\nlow intrinsic data dimension. arXiv preprint arXiv:2401.15801.\nChen, M., Liao, W., Zha, H., and Zhao, T. (2020). Distribution approximation and statistical estimation\nguarantees of generative adversarial networks. arXiv preprint arXiv:2002.03938.\nDahal, B., Havrilla, A., Chen, M., Zhao, T., and Liao, W. (2022). On deep generative models for approxi-\nmation and estimation of distributions on manifolds. Advances in Neural Information Processing Systems,\n35:10615–10628.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. (2009). Imagenet: A large-scale hierarchical\nimage database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255.\nDonahue, J., Kr¨ahenb¨uhl, P., and Darrell, T. (2017). Adversarial feature learning. In International Confer-\nence on Learning Representations.\nG´omez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern´andez-Lobato, J. M., S´anchez-Lengeling, B., Sheberla,\nD., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and Aspuru-Guzik, A. (2018).\nAutomatic\nchemical design using a data-driven continuous representation of molecules. ACS central science, 4(2):268–\n276.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. (2014). Generative Adversarial Nets. In Advances in Neural Information Processing Systems, volume 27.\nCurran Associates, Inc.\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. (2015). Draw: A recurrent neural\nnetwork for image generation. In International Conference on Machine Learning, pages 1462–1471. PMLR.\nGretton, A., Borgwardt, K. M., Rasch, M. J., Sch¨olkopf, B., and Smola, A. (2012). A kernel two-sample\ntest. Journal of Machine Learning Research, 13(25):723–773.\nGulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A. C. (2017). Improved Training of\nWasserstein GANs. Advances in Neural Information Processing Systems, 30.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. (2017). Gans trained by a two\ntime-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing\nSystems, 30.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A.\n(2017). beta-vae: Learning basic visual concepts with a constrained variational framework. In International\nConference on Learning Representations.\n15\nChakraborty and Bartlett\nHuang, J., Jiao, Y., Li, Z., Liu, S., Wang, Y., and Yang, Y. (2022). An error analysis of generative adversarial\nnetworks for learning distributions. Journal of Machine Learning Research, 23(116):1–43.\nHusain, H., Nock, R., and Williamson, R. C. (2019). A primal-dual link between gans and autoencoders. In\nWallach, H., Larochelle, H., Beygelzimer, A., d'Alch´e-Buc, F., Fox, E., and Garnett, R., editors, Advances\nin Neural Information Processing Systems, volume 32. Curran Associates, Inc.\nKarr, A. F. (1993). Probability. Springer Texts in Statistics. Springer New York, NY, 1 edition.\nKim, J., Shin, J., Rinaldo, A., and Wasserman, L. (2019). Uniform convergence rate of the kernel density\nestimator adaptive to intrinsic volume dimension. In International Conference on Machine Learning, pages\n3398–3407. PMLR.\nKingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. International Conference\non Learning Representations.\nKingma, D. P. and Welling, M. (2014). Auto-encoding variational bayes. In International Conference on\nLearning Representations.\nKoehler, F., Mehta, V., Zhou, C., and Risteski, A. (2022). Variational autoencoders in the presence of low-\ndimensional data: landscape and implicit bias. In International Conference on Learning Representations.\nKolmogorov, A. N. and Tikhomirov, V. M. (1961).\nϵ-entropy and ϵ-capacity of sets in function spaces.\nTranslations of the American Mathematical Society, 17:277–364.\nLiang, T. (2021). How well generative adversarial networks learn distributions. The Journal of Machine\nLearning Research, 22(1):10366–10406.\nLiu, H., Havrilla, A., Lai, R., and Liao, W. (2023). Deep nonparametric estimation of intrinsic data structures\nby chart autoencoders: Generalization error and robustness. arXiv preprint arXiv:2303.09863.\nLiu, S., Yang, Y., Huang, J., Jiao, Y., and Wang, Y. (2021). Non-asymptotic error bounds for bidirectional\ngans. Advances in Neural Information Processing Systems, 34:12328–12339.\nMakhzani, A., Shlens, J., Jaitly, N., Goodfellow, I., and Frey, B. (2016). Adversarial autoencoders. In\nInternational Conference on Learning Representations.\nMescheder, L., Nowozin, S., and Geiger, A. (2017).\nAdversarial variational bayes: Unifying variational\nautoencoders and generative adversarial networks. In Proceedings of the 34th International Conference on\nMachine Learning-Volume 70, pages 2391–2400. JMLR. org.\n16\nChakraborty and Bartlett\nNakada, R. and Imaizumi, M. (2020). Adaptive approximation and generalization of deep neural network\nwith intrinsic dimensionality. Journal of Machine Learning Research, 21(174):1–38.\nPope, P., Zhu, C., Abdelkader, A., Goldblum, M., and Goldstein, T. (2020). The intrinsic dimension of\nimages and its impact on learning. In International Conference on Learning Representations.\nRamesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. (2021).\nZero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821–8831.\nPMLR.\nRolinek, M., Zietlow, D., and Martius, G. (2019).\nVariational autoencoders pursue pca directions (by\naccident). In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 12406–12415.\nRudelson, M. and Vershynin, R. (2013). Hanson-Wright inequality and sub-gaussian concentration. Electronic\nCommunications in Probability, 18(none):1 – 9.\nSchreuder, N., Brunel, V.-E., and Dalalyan, A. (2021). Statistical guarantees for generative models without\ndomination. In Algorithmic Learning Theory, pages 1051–1071. PMLR.\nSohn, K., Lee, H., and Yan, X. (2015). Learning structured output representation using deep conditional\ngenerative models. Advances in neural information processing systems, 28.\nTachibana, H., Uenoyama, K., and Aihara, S. (2018). Efficiently trainable text-to-speech system based on\ndeep convolutional networks with guided attention. In 2018 IEEE international conference on acoustics,\nspeech and signal processing (ICASSP), pages 4784–4788. IEEE.\nTang, R. and Yang, Y. (2021).\nOn empirical bayes variational autoencoder: An excess risk bound.\nIn\nConference on Learning Theory, pages 4068–4125. PMLR.\nTolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. (2018). Wasserstein auto-encoders. International\nConference on Learning Representations.\nVan Den Oord, A., Vinyals, O., et al. (2017). Neural discrete representation learning. Advances in Neural\nInformation Processing Systems, 30.\nVershynin, R. (2018). High-dimensional probability: An introduction with applications in data science, vol-\nume 47. Cambridge university press.\nWainwright, M. J. (2019). High-dimensional statistics: A Non-asymptotic Viewpoint, volume 48. Cambridge\nUniversity Press.\n17\nChakraborty and Bartlett\nWeed, J. and Bach, F. (2019). Sharp asymptotic and finite-sample rates of convergence of empirical measures\nin Wasserstein distance. Bernoulli, 25(4A):2620 – 2648.\nYang, Z., Hu, Z., Salakhutdinov, R., and Berg-Kirkpatrick, T. (2017). Improved variational autoencoders\nfor text modeling using dilated convolutions. In International Conference on Machine Learning, pages\n3881–3890. PMLR.\nYarotsky, D. (2017). Error bounds for approximations with deep relu networks. Neural Networks, 94:103–114.\nZhao, S., Song, J., and Ermon, S. (2017). Infovae: Information maximizing variational autoencoders. arXiv\npreprint arXiv:1706.02262.\nAppendix\nContents\n1\nIntroduction\n1\n2\nA Proof of Concept\n4\n3\nBackground\n5\n3.1\nNotations and some Preliminary Concepts . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n5\n3.2\nWasserstein Autoencoders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n4\nIntrinsic Dimension of Data Distribution\n7\n5\nTheoretical Analyses\n8\n5.1\nAssumptions and Error Decomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n8\n5.2\nMain Result . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n5.3\nRelated work on GANs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n5.4\nProof Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n5.5\nImplications of the Theoretical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n11\n6\nDiscussions and Conclusion\n13\nA Additional Notations\n19\n18\nChakraborty and Bartlett\nB Proof of the Main Result (Theorem 8)\n20\nB.1\nMisspecification Error\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.2\nBounding the Generalization Gap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n20\nB.3\nProof of Theorem 8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nC Detailed Proofs\n24\nC.1 Proofs from Section 4\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2 Proofs from Section 5.1\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2.1\nProof of Proposition 6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nC.2.2\nProof of Lemma 7\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.3 Proofs from Section B.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.3.1\nProof of Theorem 18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nC.3.2\nProof of Lemma 19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nC.4 Proofs from Section B.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.4.1\nProof of Lemma 20 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n29\nC.4.2\nProof of Corollary 21 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nC.4.3\nProof of Lemma 22 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n31\nC.4.4\nProof of Lemma 23 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n32\nC.4.5\nProof of Lemma 24 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n33\nC.4.6\nProof of Lemma 25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n34\nC.5 Proofs from Section 5.2\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nC.6 Proofs from Section 5.5\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n37\nC.6.1\nProof of Proposition 12\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nC.6.2\nProof of Proposition 13\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nC.6.3\nProof of Theorem 14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n38\nC.6.4\nProof of Corollary 16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n39\nD Supporting Results for Approximation Guarantees\n40\nE Supporting Results from the Literature\n41\nA\nAdditional Notations\nFor function classes F1 and F2, F1 ◦ F2 = {f1 ◦ f2 : f1 ∈ F1, f2 ∈ F2}.\n19\nChakraborty and Bartlett\nDefinition 17 (Covering and Packing Numbers). For a metric space (S, ϱ), the ϵ-covering number w.r.t. ϱ\nis defined as: N(ϵ; S, ϱ) = inf{n ∈ N : ∃ x1, . . . xn such that ∪n\ni=1 Bϱ(xi, ϵ) ⊇ S}. A minimal ϵ cover of S is\ndenoted as C(ϵ; S, ϱ). Similarly, the ϵ-packing number is defined as: M(ϵ; S, ϱ) = sup{m ∈ N : ∃ x1, . . . xm ∈\nS such that ϱ(xi, xj) ≥ ϵ, for all i ̸= j}.\nB\nProof of the Main Result (Theorem 8)\nB.1\nMisspecification Error\nWe begin with a theoretical result to approximate any function on a low-dimensional structure using a\nReLU network with sufficiently large depth and width. Let f belong to the space Hβ(Rd, R, C), with C > 0,\nand let γ be a measure on Rd.\nFor notational simplicity, let M = supp(γ).\nThen, for any ϵ > 0 and\ns > dγ, we prove that there exists a ReLU network, denoted by ˆf, with a depth of at most O(log(1/ϵ)),\nand number of weights not exceeding O(ϵ−s/β log(1/ϵ)), and bounded weights. This network satisfies the\ncondition ∥f − ˆf∥L∞(M) ≤ ϵ. A similar result with bounded depth but unbounded weights was derived by\nNakada and Imaizumi (2020).\nTheorem 18. Let f be an element of Hβ(Rd, R, C), where C > 0. Then, for any s > dγ, there exists\nconstants ϵ0 (which may depend on γ) and α, (which may depend on β, d, and C), such that if ϵ ∈ (0, ϵ0],\na ReLU network ˆf can be constructed with L( ˆf) ≤ α log(1/ϵ) and W( ˆf) ≤ α log(1/ϵ)ϵ−s/β, satisfying the\ncondition, ∥f − ˆf∥L∞(M) ≤ ϵ.\nApplying the above theorem, one can control ∆miss, when the network size is large enough.\nUnder\nassumptions A1–3, we derive the following bound on the misspecification error. It is important to note that\nnone of the network dimensions depend on the dimensionality of the entire data space, i.e. d.\nLemma 19. Suppose assumptions A1–3 hold and let, diss(·, ·) ≡ W1(·, ·) or MMD2\nK(·, ·). Also, let s > dµ.\nThen, we can find positive constants ϵ0, α0 and R, that might depend on d, ℓ, ˜G and ˜E, such that if 0 <\nϵg, ϵe ≤ ϵ0 and G = RN(Wg, Lg, R) and E = RN(We, Le, R), with\nLe ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe\ne\nlog(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg\ng\nlog(1/ϵg)\nthen, ∆miss ≲ ϵg + ϵαg∧1\ne\n.\nB.2\nBounding the Generalization Gap\nLet f : Rd → Rd′ and {Xi}i∈[n] ⊂ Rd. We define f|X1:n as [f(X1) : · · · : f(Xn)] ∈ Rd′×n. For a function class\nF, we define\nF|X1:n = {f|X1:n : f ∈ F} ⊆ Rd′×n.\n20\nChakraborty and Bartlett\nThe covering number of F|X1:n with respect to the ℓ∞-norm is denoted by N(ϵ; F|X1:n , ℓ∞).\nThe result\nextends the seminal works of Bartlett et al. (2019) to determine the metric entropy of deep learners with\nmultivariate outputs.\nLemma 20. Suppose that n ≥ 6 and F are a class neural network with depth at most L and number of\nweights at most W.\nFurthermore, the activation functions are piece-wise polynomial activation with the\nnumber of pieces and degree at most k ∈ N. Then, there is a constant θ (that might depend on d and d′),\nsuch that, if n ≥ θ(W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3),\nlog N(ϵ; F|X1:n , ℓ∞) ≲ (W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3) log\n\u0012nd′\nϵ\n\u0013\n,\nwhere d′ is the output dimension of the networks in F.\nWe can use the result above to provide bounds on the metric entropies of the function classes described\nin Lemma 7. This bound is a function of the number of samples and the size of the network classes G and\nE.\nCorollary 21. Suppose that W(E) ≤ We, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3,\nWe ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ1, such that if n ≥ ξ1(We + Wg)(Le +\nLg) (log(We + Wg) + Le + Lg),\nlog N\n\u0010\nϵ; E|X1:n , ℓ∞\n\u0011\n≲WeLe(log We + Le) log\n\u0012nℓ\nϵ\n\u0013\n,\nlog N\n\u0010\nϵ; (G ◦ E)|X1:n , ℓ∞\n\u0011\n≲(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log\n\u0012nd\nϵ\n\u0013\n.\nUsing Corollary 21, the following lemma provides a bound on the distance between the empirical and\ntarget distributions w.r.t. the IPM based on F.\nLemma 22. Suppose R(G) ≲ 1 and F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Furthermore, let,\nL(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there\nis a constant ξ2, such that if n ≥ ξ2(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg)\nE∥ˆµn − µ∥F ≲ n−1/2\u0000(We + Wg)(Le + Lg)\n\u0000log(We + Wg) + Le + Lg\n\u0001\nlog(nd)\n\u00011/2 .\nTo control the fourth terms in (7) and (8), we first consider the case when diss(·, ·) is the W1-distance.\nLemma 23 controls this uniform concentration via the size of the networks in E and the sample size n.\nLemma 23. Let ˆµn = 1\nn\nPn\ni=1 δXi and E = RN(Le, We). Then,\nsup\nE∈E\n|W1(E♯µn, ν) − W1(E♯µ, ν)| ≲\n\u0010\nn−1/ℓ ∨ n−1/2 log n\n\u0011\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\n21\nChakraborty and Bartlett\nFurthermore,\nsup\nE∈E\n|W1(E♯ˆµn, ˆνm) − W1(E♯µ, ν)| ≲\n\u0010\nn−1/ℓ ∨ n−1/2 log n\n\u0011\n+\n\u0010\nm−1/ℓ ∨ m−1/2 log m\n\u0011\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nBefore deriving the corresponding uniform concentration bounds for | \\\nMMD2\nK(E♯ˆµn, ν)−MMD2\nK(E♯ˆµn, ν)|\nor | \\\nMMD2\nK(E♯ˆµn, ˆνm) − MMD2\nK(E♯ˆµn, ν)|, we recall the definition of Rademacher complexity (Bartlett and\nMendelson, 2002). For any real-valued function class F and data points X1:n = {X1, . . . , Xn}, the empirical\nRademacher complexity is defined as:\nR(F, X1:n) = 1\nnEσ sup\nf∈F\nn\nX\ni=1\nσif(Xi),\nwhere σi’s are i.i.d. Rademacher random variables taking values in {−1, +1}, with equal probability. In the\nfollowing lemma, we derive a bound on the Rademacher complexity of the class of functions in the unit ball\nw.r.t. the HK-norm composed with E. This lemma plays a key role in the proof of Lemma 24. The proof\ncrucially uses the results by Rudelson and Vershynin (2013).\nLemma 24. Suppose assumption A2 holds and let, L(E) ≤ Le and L(G) ≤ Lg, with Le ≥ 3, We ≥ 2ℓ(3+Le).\nAlso suppose that, Φ = {ϕ ∈ HK : ∥ϕ∥HK ≤ 1}, then,\nR((Φ ◦ E), X1:n) ≲\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nUsing Lemma 24, we bound the fourth term in (7) and (8) for diss(·, ·) = MMD2\nK(·, ·), in Lemma 25.\nLemma 25. Under assumption A2, the following holds:\n(a) E sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2\nK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe log We log(nℓ)\nn\n,\n(b) E sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ˆνm) − MMD2\nK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe(log We+Le) log(nℓ)\nn\n+\n1\n√m.\nB.3\nProof of Theorem 8\nTheorem 8. Suppose that assumptions A1–3 hold and ∆opt ≤ ∆ for some fixed non-negative threshold ∆.\nFurthermore, suppose that s > dµ. Then we can find n0 ∈ N and β > 0, that might depend on d, ℓ, αg, αe, ˜G\nand ˜E, such that if n ≥ n0, we can choose G = RN(Lg, Wg) and E = RN(Le, We), with, Le ≤ β log n, We ≤\nβn\ns\n2αe+s log n, Lg ≤ β log n and Wg ≤ βn\nℓ\nαe(αg∧1)+ℓ log n, then, for the estimation problem (5),\n(a) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1) ,ℓ\n\u001b\nlog2 n, for diss(·, ·) = W1(·, ·),\n(b) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\n2+max\n\u001a\nℓ\nαg ,\ns\nαe(αg∧1)\n\u001b\nlog2 n, for diss(·, ·) = MMD2\nK(·, ·).\n22\nChakraborty and Bartlett\nFurthermore, for the estimation problem (6), if m ≥ n ∨ n\n\u0010\nmax\nn\n2+\nℓ\nαg ,2+\ndµ\nαe(αg∧1) ,ℓ\no\u0011−1(ℓ∨2)\n(c) EV (µ, ν, ˆGn, ˆEn) ≲ ∆ + n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1) ,ℓ\n\u001b\nlog2 n, for diss(·, ·) = W1(·, ·),\n(d) EV (µ, ν, ˆGn,m, ˆEn,m) ≲ ∆ + n\n−\n1\n2+max\n\u001a\nℓ\nαg ,\ns\nαe(αg∧1)\n\u001b\nlog2 n, for diss(·, ·) = MMD2\nK(·, ·).\nProof. Proof of part (a) From Lemmas 7, 22 and 23, we get,\nEV (µ, ν, ˆGn, ˆEn)\n≲∆ + ϵg + ϵαg∧1\ne\n+\nr\nWeLe log We log n\nn\n+\nr\n(We + Wg)(Le + Lg) log(We + Wg) log n\nn\n+\n\u0010\nn−1/ℓ ∨ n−1/2\u0011\n≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\n(ϵ−s/αe\ne\n+ ϵ−ℓ/αg\ng\n) log n\nn\n\n +\n\u0010\nn−1/ℓ ∨ n−1/2\u0011\n≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\nϵ−ℓ/αg\ng\nlog n\nn\n\n +\n\u0010\nn−1/ℓ ∨ n−1/2\u0011\nWe choose, ϵg ≍ n\n−\n1\n2+\nℓ\nαg and ϵe ≍ n\n−\n1\n2(αg∧1)+ s\nαe . This makes,\nEV (µ, ν, ˆGn, ˆEn) ≲∆ + log2 n × n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1)\n\u001b\n+ n−1/ℓ.\nProof of part (b) From Lemmas 7, 22 and 25, we get,\nEV (µ, ν, ˆGn, ˆEn)\n≲∆ + ϵg + ϵαg∧1\ne\n+\nr\nWeLe log We log n\nn\n+\nr\n(We + Wg)(Le + Lg) log(We + Wg) log n\nn\n≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\n(ϵ−s/αe\ne\n+ ϵ−ℓ/αg\ng\n) log n\nn\n\n\n≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\nϵ−ℓ/αg\ng\nlog n\nn\n\n\nWe choose, ϵg ≍ n\n−\n1\n2+\nℓ\nαg and ϵe ≍ n\n−\n1\n2(αg∧1)+ s\nαe . This makes,\nEV (µ, ν, ˆGn, ˆEn) ≲∆ + log2 n × n\n−\n1\nmax\n\u001a\n2+\nℓ\nαg ,2+\ns\nαe(αg∧1)\n\u001b\n.\nProof of part (c)\nAgain from Lemmas 7, 22 and 23, we get,\nEV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\nϵ−ℓ/αg\ng\nlog n\nn\n\n\n+\n\u0010\nn−1/ℓ ∨ n−1/2\u0011\n+\n\u0010\nm−1/ℓ ∨ m−1/2\u0011\n23\nChakraborty and Bartlett\nChoosing ϵg ≍ n\n−\n1\n2+\nℓ\nαg , ϵe ≍ n\n−\n1\n2(αg∧1)+ s\nαe and m as in the theorem statement gives us the desired result.\nProof of part (d) Similarly, from Lemmas 7, 22 and 25, we get,\nEV (µ, ν, ˆGn, ˆEn) ≲∆ + ϵg + ϵαg∧1\ne\n+\n\u0012\nlog\n\u0012\n1\nϵe ∧ ϵg\n\u0013\u00133/2\n\n\ns\nϵ−s/αe\ne\nlog n\nn\n+\ns\nϵ−ℓ/αg\ng\nlog n\nn\n\n +\n1\n√m\nChoosing ϵg ≍ n\n−\n1\n2+\nℓ\nαg , ϵe ≍ n\n−\n1\n2(αg∧1)+ s\nαe and m as in the theorem statement gives us the desired result.\nC\nDetailed Proofs\nC.1\nProofs from Section 4\nLemma 5. Let, f ∈ Hγ \u0000A, [0, 1]d2, C\n\u0001\n, with A ⊆ [0, 1]d1. Then, dimM (f (A)) ≤ dimM(A)/(γ ∧ 1).\nProof. Then, for any x, y ∈ A, ∥f(x) − f(y)∥∞ ≤ ∥f(x) − f(y)∥2 ≤ L∥x − y∥γ∧1\n2\n≤ Ld(γ∧1)/2\n1\n∥x − y∥γ∧1\n∞ .\nThus, N (ϵ; f (A) , ∥ · ∥∞) ≤ N\n\u0010\n1\n√d1 (ϵ/L)(γ∧1)−1; A, ∥ · ∥∞\n\u0011\n.\ndimM (f (A)) = lim\nϵ→0\nlog N (ϵ; f (A) , ∥ · ∥∞)\nlog(1/ϵ)\n≤ lim\nϵ→0\nlog N\n\u0010\n1\n√d1 (ϵ/L)(γ∧1)−1; A, ∥ · ∥∞\n\u0011\nlog(1/ϵ)\n≤ dimM(A)\nγ ∧ 1\n.\nC.2\nProofs from Section 5.1\nC.2.1\nProof of Proposition 6\nProposition 6. Under assumption A1, the following holds: (a) ˜G♯ν = µ, (b) ( ˜E◦ ˜G)♯ν = ν, (c) V (µ, ν, ˜G, ˜E) =\n0.\nProof.\n(a) Let f : Z → R be any bounded continuous function. Then,\nZ\nf(x)d( ˜G♯ν)(x) =\nZ\nf( ˜G(z))dν(z)\n=\nZ\nf( ˜G( ˜E(x)))dµ(x)\n(10)\n=\nZ\nf(x)dµ(x)\n(11)\nHence, ˜G♯ν = µ. Here both (10) and (11) follows from A1.\n(b) Let f : X → R be any bounded continuous function. Then,\nZ\nf(x)d\n\u0010\n( ˜E ◦ ˜G)♯ν\n\u0011\n=\nZ\nf(E(G(z)))dν(z)\n=\nZ\nf(E(x))dµ(x)\n(12)\n=\nZ\nf(z)dν(z).\n(13)\n24\nChakraborty and Bartlett\nHere, (12) follows from part (a) and (13) follows from A1.\n(c) To prove part (c), We note that, W1(E♯µ, ν), MMD2\nK(E♯µ, ν) = 0 and ( ˜G ◦ ˜E)(·) = id(·), a.e. [µ].\nC.2.2\nProof of Lemma 7\nLemma 7 (Oracle Inequality). Suppose that, F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Then the\nfollowing hold:\nV (µ, ν, ˆGn, ˆEn) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|d\ndiss(E♯ˆµn, ν) − diss(E♯µ, ν)|.\n(7)\nV (µ, ν, ˆGn,m, ˆEn,m) ≤ ∆miss + ∆opt + 2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|d\ndiss(E♯ˆµn, ˆνm) − diss(E♯µ, ν)|.\n(8)\nHere, ∆miss = infG∈G,E∈E V (µ, ν, G, E) denotes the misspecification error for the problem.\nProof. To prove the first inequality, we observe that, V (ˆµn, ν, ˆGn, ˆEn) ≤ V (ˆµn, ν, G, E)+∆opt, for any G ∈ G\nand E ∈ E. Thus,\nV (µ, ν, ˆGn, ˆEn)\n=V (µ, ν, G, E) +\n\u0010\nV (µ, ν, ˆGn, ˆEn) − V (ˆµn, ν, ˆGn, ˆEn)\n\u0011\n+\n\u0010\nV (ˆµn, ν, ˆGn, ˆEn) − V (µ, νG, E)\n\u0011\n≤V (µ, ν, G, E) +\n\u0010\nV (µ, ν, ˆGn, ˆEn) − V (ˆµn, ν, ˆGn, ˆEn)\n\u0011\n+ (V (ˆµn, ν, G, E) − V (µ, ν, G, E)) + ∆opt\n≤∆opt + V (µ, ν, G, E) + 2\nsup\nG∈G, E∈E\n|V (ˆµn, ν, G, E) − V (µ, ν, G, E)|\n=∆opt + V (µ, ν, G, E)\n+ 2\nsup\nG∈G, E∈E\n\f\f\f\f\nZ\nc(x, G ◦ E(x))dˆµn(x) + λd\ndiss(E♯ˆµn, ν) −\nZ\nc(x, G ◦ E(x))dµ(x) − λdiss(E♯µ, ν)\n\f\f\f\f\n≤∆opt + V (µ, ν, G, E) + 2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|d\ndiss(E♯ˆµn, ν) − diss(E♯µ, ν)|.\nTaking infimum on G and E, we get inequality (7). Inequality (8) follows from a similar derivation.\nC.3\nProofs from Section B.1\nC.3.1\nProof of Theorem 18\nFix ϵ > 0. For s > dµ, we can say that N(ϵ; M, ℓ∞) ≤ Cϵ−s, for all ϵ > 0. Let K = ⌈ 1\n2ϵ⌉. For any i ∈ [K]d,\nlet θi = (ϵ + 2(i1 − 1)ϵ, . . . , ϵ + 2(id − 1)ϵ). We also let, Pϵ = {Bℓ∞(θi, ϵ) : i ∈ [K]d}. By construction, the\nsets in Pϵ are disjoint. We first claim the following:\nLemma 26. |{A ∈ Pϵ : A ∩ M ̸= ∅}| ≤ C2dϵ−s.\n25\nChakraborty and Bartlett\na − b\n−a + b\na\n−b\n0\n1\nFigure 2: Plot of ξa,b(·)\nProof. Let, r = N(ϵ; M, ℓ∞) and suppose that {a1, . . . , ar} be an ϵ-net of M and P∗\nϵ = {Bℓ∞(ai, ϵ) : i ∈ [r]}\nbe an optimal ϵ-cover of M. Note that each box in P∗\nϵ can intersect at most 2d boxes in Pϵ. This implies\nthat,\n|Pϵ ∩ M| ≤ |Pϵ ∩ (∪r\ni=1Bℓ∞(ai, ϵ))| = |∪r\ni=1 (Pϵ ∩ Bℓ∞(ai, ϵ))| ≤ 2dr,\nwhich concludes the proof.\nWe are now ready to prove Theorem 18. For the ease of readability, we restate the theorem as follows:\nTheorem 18. Let f be an element of Hβ(Rd, R, C), where C > 0. Then, for any s > dγ, there exists\nconstants ϵ0 (which may depend on γ) and α, (which may depend on β, d, and C), such that if ϵ ∈ (0, ϵ0],\na ReLU network ˆf can be constructed with L( ˆf) ≤ α log(1/ϵ) and W( ˆf) ≤ α log(1/ϵ)ϵ−s/β, satisfying the\ncondition, ∥f − ˆf∥L∞(M) ≤ ϵ.\nProof. We also let I =\nn\ni ∈ [K]d : Bℓ∞(θi, ϵ) ∩ M ̸= ∅\no\n. We also let I† = {j ∈ [K]d : mini∈I ∥i − j∥1 ≤ 1}.\nWe know that |I†| ≤ 3d|I| ≤ 6dN(ϵ; M, ℓ∞). For 0 < b ≤ a, let,\nξa,b(x) = ReLU\n\u0012x + a\na − b\n\u0013\n− ReLU\n\u0012x + b\na − b\n\u0013\n− ReLU\n\u0012x − b\na − b\n\u0013\n+ ReLU\n\u0012x − a\na − b\n\u0013\n.\nA pictorial view of this function is given in Fig. 2 and can be implemented by a ReLU network of depth\ntwo and width four. Thus, L(ξa,b) = 2 and W(ξa,b) = 12. Suppose that 0 < δ < ϵ/3 and let, ζ(x) =\nQd\nℓ=1 ξϵ+δ,δ(xℓ).\nIt is easy to observe that {ζ(· − θi) : i ∈ I†} forms a partition of unity on M, i.e.\nP\ni∈I† ζ(x − θi) = 1, ∀x ∈ M.\nWe consider the Taylor approximation of f around θ as,\nPθ(x) =\nX\n|s|<⌊β⌋\n∂sf(θ)\ns!\n(x − θ)s .\nNote that for any x ∈ [0, 1]d, f(x) − Pθ(x) = P\ns:|s|=⌊β⌋\n(x−θ)s\ns!\n(∂sf(y) − ∂sf(θ)), for some y, which is a\n26\nChakraborty and Bartlett\nconvex combination of x and θ. Thus,\nf(x) − Pθ(x) =\nX\ns:|s|=⌊β⌋\n(x − θ)s\ns!\n(∂sf(y) − ∂sf(θ)) ≤∥x − θ∥⌊β⌋\n∞\nX\ns:|s|=⌊β⌋\n1\ns!|∂sf(y) − ∂sf(θ)|\n≤∥x − θ∥⌊β⌋\n∞ ∥y − θ∥β−⌊β⌋\n∞\n≤∥x − θ∥β\n∞.\n(14)\nNext we define ˜f(x) = P\ni∈I† ζ(x − θi)Pθi(x). Thus, if x ∈ M,\n|f(x) − ˜f(x)| =\n\f\f\f\f\f\f\nX\ni∈I†\nζ(x − θi)(f(x) − Pθi(x))\n\f\f\f\f\f\f\n≤\nX\ni∈I†:∥x−θi∥∞≤2ϵ\n|f(x) − Pθi(x)|\n≤2d(2ϵ)β\n=2d+βϵβ.\n(15)\nWe note that, ˜f(x) = P\ni∈I† ζ(x − θi)Pθi(x) = P\ni∈I†\nP\n|s|<⌊β⌋\n∂sf(θi)\ns!\nζ(x − θi)\n\u0010\nx − θi\u0011s\n. Let ai,s =\n∂sf(θi)\ns!\nand\nˆfi,s(x) = prod(d+|s|)\nm\n(ξϵ1,δ1(x1 − θi\n1), . . . , ξϵd,δd(xd − θi\nd), (x1 − θi\n1), . . . , (x1 − θi\n1)\n|\n{z\n}\ns1 times\n,\n. . . , (x1 − θi\nd), . . . , (xd − θi\nd)\n|\n{z\n}\nsd times\n),\nwhere, prod(·) is defined in Lemma 34. Here, prod(d+|s|)\nm\nhas at most d + |s| ≤ d + ⌊β⌋ many inputs. By\nLemma 34, prod(d+|s|)\nm\ncan be implemented by a ReLU network with L(prod(d+|s|)\nm\n), W(prod(d+|s|)\nm\n) ≤ c3m.\nThus, L( ˆfi,s) ≤ c3m + 2 and W( ˆfi,s) ≤ c3m + 8d + 4|s| ≤ c3m + 8d + 4k. With this ˆfi,s, we observe that,\n\f\f\f ˆfi,s(x) − ζ(x − θi)\n\u0010\nx − θi\u0011s\f\f\f ≤ (d + ⌊β⌋)3\n22m+2\n, ∀x ∈ M.\n(16)\nFinally, let, ˆf(x) = P\ni∈I†\nP\n|s|≤⌊β⌋ ai,s ˆfi,s(x). Clearly, L( ˆfi,s) ≤ c3m+3 and W( ˆfi,s) ≤ kd (c3m + 8d + 4k).\nThis implies that,\n| ˆf(x) − ˜f(x)| ≤\nX\ni∈I†:∥x−θi∥∞≤2ϵ\nX\n|s|<⌊β⌋\n|ai,s|ζ(x − θi)| ˆfis(x) −\n\u0010\nx − θi\u0011s\n|\n≤2d\nX\n|s|<⌊β⌋\n|aθ,s|\n\f\f\f ˆfθi(x),s(x) − ζϵ,δ(x − θ(i(x))\n\u0010\nx − θi(x)\u0011s\f\f\f\n≤(d + ⌊β⌋)3C\n22m+2−d\n.\nWe thus get that if x ∈ M,\n|f(x) − ˆf(x)| ≤|f(x) − ˜f(x)| + | ˆf(x) − ˜f(x)| ≤ 2d+βϵβ + (d + ⌊β⌋)3C\n22m+2−d\n.\n(17)\n27\nChakraborty and Bartlett\nWe choose ϵ =\n\u0000η\n2d+k+2\n\u00011/β and m =\nl\nlog2\n\u0010\n(d+k)3C\nη\n\u0011m\n+ d − 1. Then,\n∥f − ˆf∥L∞(M) ≤η.\nWe note that ˆf has |I†| ≤ 6dNϵ(M) ≲ 6dϵ−s many networks with depth c3m + 3 and number of weights\n⌊β⌋d (c3m + 8d + 4⌊β⌋). Thus, L( ˆf) ≤ c3m + 4 and W( ˆf) ≤ ϵ−s(6⌊β⌋)d (c3m + 8d + 4⌊β⌋). we thus get,\nL( ˆf) ≤ c3m + 4 ≤ c3\n\u0012\u0018\nlog2\n\u0012(d + ⌊β⌋)3Cδ\nη\n\u0013\u0019\n+ d − 1\n\u0013\n+ 4 ≤ c4 log\n\u00121\nη\n\u0013\n,\nwhere c4 is a function of δ, ⌊β⌋ and d. Similarly,\nW( ˆf) ≤ϵ−s(6⌊β⌋)d (c3m + 8d + 4⌊β⌋)\n≤\n\u0010\nη\n2d+k+2\n\u0011−s/β\n(6⌊β⌋)d\n\u0012\nc3\n\u0012\nlog2\n\u0012(d + ⌊β⌋)3Cδ\nη\n\u0013\n+ d − 1\n\u0013\n+ 8d + 4⌊β⌋\n\u0013\n≤c6 log(1/η)η−s/β.\nTaking α = c4 ∨ c6 gives the result.\nC.3.2\nProof of Lemma 19\nLemma 19. Suppose assumptions A1–3 hold and let, diss(·, ·) ≡ W1(·, ·) or MMD2\nK(·, ·). Also, let s > dµ.\nThen, we can find positive constants ϵ0, α0 and R, that might depend on d, ℓ, ˜G and ˜E, such that if 0 <\nϵg, ϵe ≤ ϵ0 and G = RN(Wg, Lg, R) and E = RN(We, Le, R), with\nLe ≤ α0 log(1/ϵg), Lg ≤ α0 log(1/αg), We ≤ α0ϵ−s/αe\ne\nlog(1/ϵe) and Wg ≤ α0ϵ−ℓ/αg\ng\nlog(1/ϵg)\nthen, ∆miss ≲ ϵg + ϵαg∧1\ne\n.\nProof. We first prove the result for the Wasserstein-1 distance and then for the MMDK-metric.\nCase 1: diss(·, ·) ≡ W1(·, ·)\nFor any G ∈ G and E ∈ E, we observe that,\nV (µ, ν, G, E) ≤V (µ, ν, ˜G, ˜E) + |V (µ, ν, G, E) − V (µ, ν, ˜G, ˜E)|\n≤∥c(·, ˜G ◦ ˜E(·)) − c(·, G ◦ E(·))∥L∞(M) + |W1(E♯µ, ν) − W1( ˜E♯µ, ν)|\n≲∥G ◦ E − ˜G ◦ ˜E∥L∞(M) + W1( ˜E♯µ, E♯µ)\n≲∥G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ))\n≤∥G ◦ E − ˜G ◦ E∥L∞(supp(µ)) + ∥ ˜G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ))\n≲∥G − ˜G∥L∞([0,1]ℓ) + ∥E − ˜E∥αg∧1\nL∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ))\nWe can take G = RN(log(1/ϵg), ϵ−ℓ/αg\ng\nlog(1/ϵg)) and E = RN(log(1/ϵe), ϵ−s/αe\ne\nlog(1/ϵe)) by approximating\nin each of the individual coordinate-wise output of the vector-valued functions ˜G and ˜E and stacking them\n28\nChakraborty and Bartlett\nparallelly. This makes,\nV (µ, ν, G, E) ≲ ϵg + ϵαg∧1\ne\n+ ϵe ≲ ϵg + ϵαg∧1\ne\n.\nCase 2: diss(·, ·) ≡ MMD2\nk(·, ·)\nBefore we begin, we note that,\n|MMD2\nK(E♯µ, ν) − MMD2\nK( ˜E♯µ, ν)|\n≤|EX∼µ,X′∼µK(E(X), E(X′)) − EX∼µ,X′∼µK( ˜E(X), ˜E(X′))|\n+ 2|EX∼µ,Z∼νK(E(X), Z) − EX∼µ,Z∼νK( ˜E(X), Z)|\n≤2τk∥E − ˜E∥L∞(supp(µ)) + 2τk∥E − ˜E∥L∞(supp(µ))\n=4τk∥E − ˜E∥L∞(supp(µ)).\n(18)\nFor any G ∈ G and E ∈ E, we observe that,\nV (µ, ν, G, E) =V (µ, ν, ˜G, ˜E) + |V (µ, ν, G, E) − V (µ, ν, ˜G, ˜E)|\n=∥c(·, ˜G ◦ ˜E(·)) − c(·, G ◦ E(·))∥L∞(supp(µ)) + |MMD2\nK(E♯µ, ν) − MMD2\nK( ˜E♯µ, ν)|\n≲∥G ◦ E − ˜G ◦ ˜E∥L∞(µ) + 4τk∥E − ˜E∥L∞(supp(µ))\n(19)\n≲∥G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ))\n≤∥G ◦ E − ˜G ◦ E∥L∞(supp(µ)) + ∥ ˜G ◦ E − ˜G ◦ ˜E∥L∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ))\n≲∥G − ˜G∥L∞([0,1]ℓ) + ∥E − ˜E∥αg∧1\nL∞(supp(µ)) + ∥ ˜E − E∥L∞(supp(µ)).\nIn the above calculations, we have used (18) to arrive at (19). As before, we take G = RN(log(1/ϵg), ϵ−ℓ/αg\ng\nlog(1/ϵg))\nand E = RN(log(1/ϵe), ϵ−s/αe\ne\nlog(1/ϵe)) by approximating in each of the individual coordinate-wise output\nof the vector-valued functions ˜G and ˜E and stacking them parallelly. This makes,\nV (µ, ν, G, E) ≲ ϵg + ϵαg∧1\ne\n+ ϵe ≲ ϵg + ϵαg∧1\ne\n.\nC.4\nProofs from Section B.2\nC.4.1\nProof of Lemma 20\nLemma 20. Suppose that n ≥ 6 and F are a class neural network with depth at most L and number of\nweights at most W.\nFurthermore, the activation functions are piece-wise polynomial activation with the\nnumber of pieces and degree at most k ∈ N. Then, there is a constant θ (that might depend on d and d′),\nsuch that, if n ≥ θ(W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3),\nlog N(ϵ; F|X1:n , ℓ∞) ≲ (W + 6d′ + 2d′L)(L + 3) (log(W + 6d′ + 2d′L) + L + 3) log\n\u0012nd′\nϵ\n\u0013\n,\n29\nChakraborty and Bartlett\nx\nf\ny\nid\nf(x)\ny\nf(x) + y\nf(x) − y\nh(x, y)\n1\n−1\n0.25\n−0.25\n1\n1\nFigure 3: A representation of the network h(·, ·). The magenta lines represent d′ weights of value 1. Similarly,\ncyan lines represent d′ weights of value −1. Finally, the orange and teal lines represent d′ weights (each)\nwith values +0.25 and −0.25, respectively. The identity map takes 2d′L(f) many weights (see remark 15 (iv)\nof Nakada and Imaizumi (2020)). The magenta, cyan, orange and teal connections take 6d′ many weights.\nAll activations are taken to be ReLU, except the output of the yellow nodes, whose activation is σ(x) = x2.\nwhere d′ is the output dimension of the networks in F.\nProof. We let, h(x, y) = y⊤f(x) and let H = {h(x, y) = y⊤f(x) : f ∈ F}. Also, let, T = {(h(Xi, eℓ)|i∈[n],ℓ∈[d′]) ∈\nRnd′ : h ∈ H}. Here eℓ denotes the ℓ-th unit vector. By construction of T , it is clear that, N(ϵ; F|X1:n , ℓ∞) =\nN(ϵ; T , ℓ∞). We observe that,\nh(x, y) = 1\n4(∥y + f(x)∥2\n2 − ∥y − f(x)∥2\n2)\nClearly, h can be implemented by a network with L(h) = L(f) + 3 and W(h) = W(f) + 6d′ + 2d′L(f) (see\nFig. 3 for such a construction). Thus, from Theorem 12.9 of Anthony and Bartlett (2009) (see Lemma 37),\nwe note that, if n ≥ Pdim(H),\nN(ϵ; T , ℓ∞) ≤\n\u0012\n2end′\nϵPdim(H)\n\u0013Pdim(H)\n,\nwith,\nPdim(H) ≲ W(h)L(h) log W(h) + W(h)L2(h),\nfrom applying Theorem 6 of Bartlett et al. (2019) (see Lemma 38). This implies that,\nlog N(ϵ; H, ℓ∞) ≤Pdim(H) log\n\u0012\n2end′\nϵPdim(H)\n\u0013\n≤Pdim(H) log\n\u0012nd′\nϵ\n\u0013\n≲\n\u0000W(h)L(h) log W(h) + W(h)L2(h)\n\u0001\nlog\n\u0012nd′\nϵ\n\u0013\n.\nPlugging in the values of W(h) and L(h) yields the result.\n30\nChakraborty and Bartlett\nC.4.2\nProof of Corollary 21\nCorollary 21. Suppose that W(E) ≤ We, L(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3,\nWe ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there is a constant ξ1, such that if n ≥ ξ1(We + Wg)(Le +\nLg) (log(We + Wg) + Le + Lg),\nlog N\n\u0010\nϵ; E|X1:n , ℓ∞\n\u0011\n≲WeLe(log We + Le) log\n\u0012nℓ\nϵ\n\u0013\n,\nlog N\n\u0010\nϵ; (G ◦ E)|X1:n , ℓ∞\n\u0011\n≲(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log\n\u0012nd\nϵ\n\u0013\n.\nProof. The proof easily follows from applying Lemma 20 and noting the sizes of the networks in E and\nG ◦ E.\nC.4.3\nProof of Lemma 22\nLemma 22. Suppose R(G) ≲ 1 and F = {f(x) = c(x, G ◦ E(x)) : G ∈ G, E ∈ E}. Furthermore, let,\nL(E) ≤ Le, W(G) ≤ Wg and L(G) ≤ Lg, with Le, Lg ≥ 3, We ≥ 6ℓ + 2ℓLe and Wg ≥ 6d + 2dLg. Then, there\nis a constant ξ2, such that if n ≥ ξ2(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg)\nE∥ˆµn − µ∥F ≲ n−1/2\u0000(We + Wg)(Le + Lg)\n\u0000log(We + Wg) + Le + Lg\n\u0001\nlog(nd)\n\u00011/2 .\nProof. Let R(G) ≤ B, for some B > 0 and let Bc = sup0≤x≤B |c(x)| From Dudley’s chaining (Wainwright,\n2019, Theorem 5.22),\nE∥ˆµn − µ∥F ≲EX1:n\ninf\n0≤δ≤Bc/2\n \nδ +\n1\n√n\nZ Bc/2\nδ\nq\nlog N(ϵ, F|X1:n , ℓ∞)dϵ\n!\n.\n(20)\nLet For any G ∈ G and E ∈ E, we can find v ∈ C(ϵ; (G◦E)|X1:n , ℓ∞), such that, ∥(G◦E)|X1:n −v∥∞ ≤ ϵ. This\nimplies that ∥(G ◦ E)(Xi) − vi∥∞ ≤ ϵ, for all i ∈ [n]. Let, A = {(c(X1, v1), . . . , c(Xn, vn)) : v ∈ (G ◦ E)|X1:n }.\nThus, For any G ∈ G, E ∈ E,\nmax\n1≤i≤n |c(Xi, G ◦ E(Xi)) − c(Xi, vi)| ≤τc max\n1≤i≤n ∥(G ◦ E)(Xi) − vi∥∞ ≤ τcϵ.\nThus, A constitutes a τcϵ-cover of F|X1:n . Hence,\nN(ϵ, F|X1:n , ℓ∞) ≤ N(ϵ/τc, (G ◦ E)|X1:n , ℓ∞) ≤ (We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log\n\u0012τcnd\nϵ\n\u0013\n.\nHere, the last inequality follows from Lemma 20. Plugging in the above bound in equation (20), we get,\nE∥ˆµn − µ∥F ≲EX1:n\ninf\n0≤δ≤Bc/2\n \nδ +\n1\n√n\nZ Bc/2\nδ\nq\nlog N(ϵ, F|X1:n , ℓ∞)dϵ\n!\n≤\nr\n(We + Wg)(Le + Lg) log(We + Wg)\nn\nZ Bc\n0\ns\nlog\n\u0012τcnd\nϵ\n\u0013\ndϵ\n≲\nr\n(We + Wg)(Le + Lg) (log(We + Wg) + Le + Lg) log(nd)\nn\n.\n31\nChakraborty and Bartlett\nC.4.4\nProof of Lemma 23\nLemma 23. Let ˆµn = 1\nn\nPn\ni=1 δXi and E = RN(Le, We). Then,\nsup\nE∈E\n|W1(E♯µn, ν) − W1(E♯µ, ν)| ≲\n\u0010\nn−1/ℓ ∨ n−1/2 log n\n\u0011\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nFurthermore,\nsup\nE∈E\n|W1(E♯ˆµn, ˆνm) − W1(E♯µ, ν)| ≲\n\u0010\nn−1/ℓ ∨ n−1/2 log n\n\u0011\n+\n\u0010\nm−1/ℓ ∨ m−1/2 log m\n\u0011\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nProof. Note that if diss(·, ·) = W1(·, ·), then\nsup\nE∈E\n|d\ndiss(E♯µ, ν) − diss(E♯µ, ν)| = sup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)| ≤ sup\nE∈E\nW1(E♯ˆµn, E♯µ)\nWe note that,\nsup\nE∈E\nW(E♯ˆµ, E♯µ) = sup\nE∈E\nsup\nf:∥f∥Lip≤1\nEX∼µ, ˆ\nX∼ˆµf(E(X)) − f(E( ˆX))\nWe take F1 = {f : [0, 1]ℓ → R : ∥f∥Lip ≤ 1} = H1(\n√\nℓ). By the result of Kolmogorov and Tikhomirov\n(1961) (Lemma 36), we note that log N(ϵ; F1, ℓ∞) ≲ ϵ−ℓ. Furthermore, if we take F2 = E, we observe that,\nlog N(ϵ; E|X1:n , ℓ∞) ≲ WeLe(log We + Le) log\n\u0000 nℓ\nϵ\n\u0001\nfrom Lemma 21. From Dudley’s chaining, we observe the\nfollowing:\nE sup\nE∈E\nW(E♯ˆµ, E♯µ)\n=E sup\nE∈E\nsup\nf:∥f∥Lip≤1\nEX∼µ, ˆ\nX∼ˆµf(E(X)) − f(E( ˆX))\n=E∥ˆµ − µ∥F1◦E\n≲E\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\nr\nlog N\n\u0010\nϵ; (F1 ◦ E)|X1:n , ℓ∞\n\u0011\ndϵ\n!\n≤E\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\nr\nlog N (ϵ/2; F1, ℓ∞) + log N\n\u0010\nϵ/2; E|X1:n , ℓ∞\n\u0011\ndϵ\n!\n≲E\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\n \np\nlog N (ϵ/2; F1, ℓ∞) +\nr\nlog N\n\u0010\nϵ/2; E|X1:n , ℓ∞\n\u0011!\ndϵ\n!\n≲E\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\np\nlog N (ϵ/2; F1, ℓ∞)dϵ +\n1\n√n\nZ Re\nδ\nr\nlog N\n\u0010\nϵ/2; E|X1:n , ℓ∞\n\u0011\ndϵ\n!\n≲\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\nϵ−ℓ/2dϵ +\n1\n√n\nZ 1\n0\ns\nWeLe(log We + Le) log\n\u00122enℓ\nϵ\n\u0013\ndϵ\n!\n≲\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\nϵ−ℓ/2dϵ\n!\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n32\nChakraborty and Bartlett\n≲\ninf\n0≤δ≤Re\n \nδ +\n1\n√n\nZ Re\nδ\nϵ−ℓ/2dϵ\n!\n+\nr\nℓWeLe log We log n\nn\n≲\n\u0010\nn−1/ℓ ∨ n−1/2 log n\n\u0011\n+\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nC.4.5\nProof of Lemma 24\nLemma 24. Suppose assumption A2 holds and let, L(E) ≤ Le and L(G) ≤ Lg, with Le ≥ 3, We ≥ 2ℓ(3+Le).\nAlso suppose that, Φ = {ϕ ∈ HK : ∥ϕ∥HK ≤ 1}, then,\nR((Φ ◦ E), X1:n) ≲\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\nProof.\nR((Φ ◦ E), X1:n)\n= 1\nnE\nsup\nϕ∈Φ, f∈E\n\f\f\f\f\f\nn\nX\ni=1\nσiϕ(f(Xi))\n\f\f\f\f\f\n= 1\nnE\nsup\nϕ∈Φ, f∈E\n\f\f\f\f\f\nn\nX\ni=1\nσi⟨K(f(Xi), ·), ϕ⟩\n\f\f\f\f\f\n= 1\nnE\nsup\nϕ∈Φ, f∈E\n\f\f\f\f\f\n* n\nX\ni=1\nσiK(f(Xi), ·), ϕ\n+\f\f\f\f\f\n≤ 1\nnE sup\nf∈E\n\r\r\r\r\r\nn\nX\ni=1\nσiK(f(Xi), ·)\n\r\r\r\r\r\nHK\n= 1\nnE\nsup\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011 sup\nf∈E\n\r\r\r\r\r\nn\nX\ni=1\nσi (K(vi, ·) + K(f(Xi), ·) − K(vi, ·))\n\r\r\r\r\r\nHK\n≤ 1\nnE\nsup\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011 sup\nf∈E\n\n\n\r\r\r\r\r\nn\nX\ni=1\nσiK(vi, ·)\n\r\r\r\r\r\nHK\n+ 1\nn\nn\nX\ni=1\n∥K(f(Xi), ·) − K(vi, ·)∥HK\n\n\n≤ 1\nnE\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011\n\r\r\r\r\r\nn\nX\ni=1\nσiK(vi, ·)\n\r\r\r\r\r\nHK\n+\n√\n2τkϵ\n(21)\nFor any v ∈ C\n\u0010\nϵ, E|X1:n , ℓ∞\n\u0011\n, let, Yv = ∥Pn\ni=1 σiK(vi, ·)∥HK and Kv = ((K(vi, vj)) ∈ Rn×n. It is easy\nto observe that, Y 2\nv = σ⊤Kvσ and Yv = ∥K1/2\nv\nσ∥. By Theorem 2.1 of Rudelson and Vershynin (2013), we\nnote that,\nP\n\u0010\f\f\f∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS\n\f\f\f > t\n\u0011\n≤ 2 exp\n(\n−\nct2\n∥K1/2\nv\n∥2\n)\n= 2 exp\n\u001a\n− ct2\n∥Kv∥\n\u001b\n,\nfor some universal constant c > 0. From Perron–Frobenius theorem, we note that,\n∥Kv∥ ≤ max\n1≤i≤n\nn\nX\nj=1\nK(vi, vj) ≤ B2n.\n33\nChakraborty and Bartlett\nHence,\nP\n\u0010\f\f\f∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS\n\f\f\f > t\n\u0011\n≤ 2 exp\n\u001a\n− ct2\nnB2\n\u001b\n.\nThis implies that,\nexp(λ(∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS)) ≤ exp\n\u001a\n−c′λ2\nn\n\u001b\n,\nfor some absolute constant c′, by applying Proposition 2.5.2 of Vershynin (2018). From Theorem 2.5 of\nBoucheron et al. (2013), we observe that,\nE\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011(∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS) ≲\nr\nn log N\n\u0010\nϵ, E|X1:n , ℓ∞\n\u0011\n.\nFrom equation (21), we observe that,\nR(Φ ◦ E, X1:n) ≤ 1\nnE\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011\n\r\r\r\r\r\nn\nX\ni=1\nσiK(vi, ·)\n\r\r\r\r\r\nHK\n+\n√\n2τkϵ\n= 1\nnE\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011(∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS + ∥K1/2\nv\n∥HS) +\n√\n2τkϵ\n≤ 1\nnE\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011(∥K1/2\nv\nσ∥ − ∥K1/2\nv\n∥HS)\n+ 1\nn\nmax\nv∈C\n\u0010\nϵ,E|X1:n ,ℓ∞\n\u0011 ∥K1/2\nv\n∥HS +\n√\n2τkϵ\n≲\nv\nu\nu\ntlog N\n\u0010\nϵ, E|X1:n , ℓ∞\n\u0011\nn\n+ B\n√n + √ϵ\n≲\ns\nWeLe log We log\n\u0000 nℓ\nϵ\n\u0001\nn\n+ √ϵ\n(22)\nWe take ϵ =\nq\nWeLe(log We+Le) log(nℓ)\nn\nmakes R((Φ ◦ E), X1:n) ≲\nq\nWeLe(log We+Le) log(nℓ)\nn\n.\nC.4.6\nProof of Lemma 25\nTo prove Lemma 25, we need some supporting results, which we sequentially state and prove as follows. The\nfirst such result, i.e. Lemma 27 ensures that the kernel function is Lipschitz when it is considered as a map\nfrom a real vector space to the corresponding Hilbert space.\nLemma 27. Suppose assumption A2 holds. Then, ∥K(x, ·) − K(y, ·)∥2\nHK ≤ 2τk∥x − y∥2.\nProof. We observe the following:\n∥K(x, ·) − K(y, ·)∥2\nHK =K(x, x) + K(y, y) − 2K(x, y)\n=(K(x, x) − K(x, y)) + (K(y, y) − K(x, y))\n≤2τk∥x − y∥2.\n34\nChakraborty and Bartlett\nLemma 28 states that the difference between the estimated and actual squared MMD-dissimilarity scales\nas O(1/n) for estimates (5) and O(1/n + 1/m) for estimates (6).\nLemma 28. Suppose assumption A2 holds. Then, for any E ∈ E,\n(a)\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2\nK(E♯ˆµn, ν)\n\f\f\f ≤ 2B2\nn .\n(b)\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ˆνm) − MMD2\nK(E♯ˆµn, ν)\n\f\f\f ≤ 2B2 \u0000 1\nn + 1\nm\n\u0001\n.\nProof. We note that,\n\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2(E♯ˆµn, ν)\n=\n1\nn(n − 1)\nX\ni̸=j\nK(E(Xi), E(Xj)) − 1\nn2\nn\nX\ni,j=1\nK(E(Xi), E(Xj))\n=\n1\nn2(n − 1)\nX\ni̸=j\nK(E(Xi), E(Xj)) − 1\nn2\nn\nX\ni=1\nK(E(Xi), E(Xi))\nThus,\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2(E♯ˆµn, ν)\n\f\f\f ≤\n1\nn2(n − 1) × n(n − 1)B2 + 1\nn2 × nB2 = 2B2\nn .\nPart (b) follows similarly.\nWe also note that the MMDK-metric is bounded under A2 as seen in Lemma 29.\nLemma 29. Under assumption A2, MMDK(P, Q) ≤ 2B, for any two distributions P and Q.\nProof. |f(x)| = ⟨K(x, ·), f⟩ ≤ ∥K(x, ·)∥HK = B. This implies that MMDK(P, Q) = supϕ∈Φ(\nR\nϕdP −\nR\nϕdQ) ≤\n2B\nLemma 30. Suppose assumption A2 holds. Then,\n(a) E supE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe(log We+Le) log(nℓ)\nn\n.\n(b) E supE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe(log We+Le) log(nℓ)\nn\n+\n1\n√m.\nProof. Proof of Part (a)\nWe begin by noting that,\nE sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\f\f\f\n≤E sup\nE∈E\n|MMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)| + E sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯ˆµn, ν)\n\f\f\f\n35\nChakraborty and Bartlett\n≤E sup\nE∈E\nMMDK(E♯ˆµn, E♯µ) + 2B\nr\n1\nn\n(23)\n=E sup\nE∈E\nsup\nϕ∈Φ\n\u0012Z\nϕ(E(x))dˆµn(x) −\nZ\nϕ(E(x))dµ(x)\n\u0013\n+ 2B\nr\n1\nn\n≤2R(Φ ◦ E, µ) + 2B\nr\n1\nn\n(24)\n≲\nr\nWeLe(log We + Le) log(nℓ)\nn\n.\n(25)\nIn the above calculations, (23) follows from Lemma 28. Inequality (24) follows from symmetrization, whereas,\n(25) follows from Lemma 24.\nProof of Part (b) Similar to the calculations in part (a), we note the following:\nE sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\f\f\f\n≤E sup\nE∈E\n|MMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)| + E sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯ˆµn, ν)\n\f\f\f\n≤E sup\nE∈E\nMMDK(E♯ˆµn, E♯µ) + EMMDK(ˆνm, ν) + 2B\nr\n1\nn + 1\nm\n≤2R(Φ ◦ E, µ) + R(Φ, ν) + 2B\nr\n1\nn + 1\nm\n≲\nr\nWeLe(log We + Le) log(nℓ)\nn\n+\n1\n√m.\nWe are now ready to prove Lemma 25. For ease of readability, we restate the Lemma as follows.\nLemma 25. Under assumption A2, the following holds:\n(a) E sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2\nK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe log We log(nℓ)\nn\n,\n(b) E sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ˆνm) − MMD2\nK(E♯µ, ν)\n\f\f\f ≲\nq\nWeLe(log We+Le) log(nℓ)\nn\n+\n1\n√m.\nProof. Proof of part (a) We begin by noting the following:\nE sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ν) − MMD2\nK(E♯µ, ν)\n\f\f\f\n=E sup\nE∈E\n\f\f\f\f2MMDK(E♯µ, ν)\n\u0010\n\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\u0011\n+\n\u0010\n\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\u00112\f\f\f\f\n≤2BE sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\f\f\f + E sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ν) − MMDK(E♯µ, ν)\n\f\f\f\n2\n(26)\n≲\nr\nℓWeLe(log We + Le) log n\nn\n.\n(27)\nInequality (26) follows from applying Lemma 29, whereas, (27) is a consequence of Lemma (30).\n36\nChakraborty and Bartlett\nProof of part (b) Similarly,\nE sup\nE∈E\n\f\f\f\\\nMMD\n2\nK(E♯ˆµn, ˆνm) − MMD2\nK(E♯µ, ν)\n\f\f\f\n=E sup\nE∈E\n\f\f\f\f2MMDK(E♯µ, ν)\n\u0010\n\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\u0011\n+\n\u0010\n\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\u00112\f\f\f\f\n≤2BE sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\f\f\f + E sup\nE∈E\n\f\f\f\\\nMMDK(E♯ˆµn, ˆνm) − MMDK(E♯µ, ν)\n\f\f\f\n2\n≲\nr\nℓWeLe(log We + Le) log n\nn\n+\n1\n√m.\nC.5\nProofs from Section 5.2\nIn this section, we prove the main result of this paper, i.e. Theorem 8.\nC.6\nProofs from Section 5.5\nTo begin our analysis, we first show the following:\nTheorem 31. Under assumptions, A1–3, V (µ, ν, ˆGn, ˆE) → 0, almost surely.\nProof. For simplicity, we consider the estimator (5). A similar proof holds for estimator (6). Consider the\noracle inequality (7). We only consider the case, when, diss = W1, the case when, diss = MMD2\nK can be\nproved similarly.\nWe note that F is a bounded function class, with bound Bc. Thus, a simple application of the bounded\ndifference inequality yields that with probability at least 1 − δ/2,\n∥ˆµn − µ∥F ≤ E∥ˆµn − µ∥F + θ1\nr\nlog(1/δ)\nn\n,\nfor some positive constant θ1. The fourth term in (5) can be written as:\nsup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)|.\nSuppose that ˆµ′\nn denotes the empirical distribution on (X1, . . . , Xi−1, X′\ni, . . . , Xn). Then replacing ˆµn with\nˆµ′\nn, yields an error at most,\nsup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯ˆµ′\nn, ν)| ≤ sup\nE∈E\nW1(E♯ˆµn, E♯ˆµ′\nn) ≤ sup\nE∈E\nsup\nf:∥f∥Lip≤1\n1\nn|f(E(Xi)) − f(E(X′\ni)| ≲ 1\nn,\nsince by construction, E’s are chosen from bounded ReLU functions.\nAgain by a simple application of\nbounded difference inequality, we get,\n2λ sup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)| ≤ 2λE sup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)| + θ2\nr\nlog(1/δ)\nn\n,\n37\nChakraborty and Bartlett\nwith probability at least 1 − δ/2. Hence, by union bound, with probability at least 1 − δ\n2∥ˆµn − µ∥F + 2λ sup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)|\n≤2E∥ˆµn − µ∥F + 2λE sup\nE∈E\n|W1(E♯ˆµn, ν) − W1(E♯µ, ν)| + θ3\nr\nlog(1/δ)\nn\n,\nfor some absolute constant θ3. Since, all other terms in (7) are bounded independent of the random sample,\nwith probability at least 1 − δ,\nV (µ, ν, ˆG, ˆE) ≤ EV (µ, ν, ˆG, ˆE) + θ3\nr\nlog(1/δ)\nn\n.\nFrom the above, P(|V (µ, ν, ˆG, ˆE) − EV (µ, ν, ˆG, ˆE)| > ϵ) ≤ e− nϵ2\nθ3 . this implies that P\nn≥1 P(|V (µ, ν, ˆG, ˆE) −\nEV (µ, ν, ˆG, ˆE)| > ϵ) < ∞.\nA simple application of the first Borel-Cantelli Lemma yields (see Proposi-\ntion 5.7 of Karr (1993)) that this implies that |V (µ, ν, ˆG, ˆE) − EV (µ, ν, ˆG, ˆE)| → 0, almost surely. Since,\nlimn→∞ EV (µ, ν, ˆG, ˆE) = 0, the result follows.\nC.6.1\nProof of Proposition 12\nProposition 12. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·)\nand MMD2\nK(·, ·) and the estimates (5) and (6), ˆE♯µ\nd−→ ν, almost surely.\nProof. Let diss = W1. From Theorem 31, it is clear that, W1( ˆE♯µ, ν) → 0, almost surely. Since convergence\nin Wasserstein distance characterizes convergence in distribution, ˆE♯µ\nd−→ ν, almost surely.\nWhen, diss = MMD2\nK, we can similarly say that MMD2\nK( ˆE♯µ, ν) → 0, almost surely. From Theorem 3.2\n(b) of Schreuder et al. (2021), we conclude that ˆE♯µ\nd−→ ν, almost surely.\nC.6.2\nProof of Proposition 13\nProposition 13. Suppose that assumptions A1–3 hold. Then, for both the dissimilarity measures W1(·, ·)\nand MMD2\nK(·, ·) and the estimates (5) and (6), ∥id(·) − ˆG ◦ ˆE(·)∥2\nL2(µ)\na.s.\n−−→ 0.\nProof. The proof follows from observing that 0 ≤ ∥id(·) − ˆG ◦ ˆE(·)∥2\nL2(µ) ≤ V (µ, ν, ˆG, ˆE) and applying\nTheorem 31.\nC.6.3\nProof of Theorem 14\nTheorem 14. Suppose that assumptions A1–3 hold and TV ( ˆE♯µ, ν) → 0, almost surely. Then, ˆG♯ν\nd−→ µ,\nalmost surely.\nProof. We begin by observing that,\nW1( ˆG♯ν, µ) ≤W1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) + W1( ˆG ◦ ˆE)♯µ, µ)\n(28)\n38\nChakraborty and Bartlett\nWe first note that\nTV ( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) =\nsup\nB∈B(Rd)\n|( ˆG♯ν)(B) − (( ˆG ◦ ˆE)♯µ)(B)|\n=\nsup\nB∈B(Rd)\n|ν( ˆG−1(B)) − ( ˆE♯µ)( ˆG−1(B))|\n≤\nsup\nB∈B(Rℓ)\n|ν(B) − ( ˆE♯µ)(B)|\n(29)\n=TV (ν, ˆE♯µ) → 0, almost surely.\nHere (29) follows from the fact that\nn\nˆG−1(B) : B ∈ Rdo\n⊆ Rℓ, since ˆG’s are measurable. Thus, TV ( ˆG♯ν, ( ˆG◦\nˆE)♯µ) → 0, almost surely. Since convergence in TV implies convergence in distribution, this implies that\nW1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) → 0, almost surely.\nWe also note that, from Proposition 13, EX∼µ∥X − ˆG ◦ ˆE(X)∥2 → 0, almost surely. This implies that\n∥X − ˆG ◦ ˆE(X)∥\nP−→ 0, almost surely, which further implies that ˆG ◦ ˆE(X)\nd−→ X, almost surely. Hence,\nW1( ˆG ◦ ˆE)♯µ, µ) → 0, almost surely. Plugging these in (28) gives us the desired result.\nTheorem 15. Suppose that assumptions A1–3 hold and let the family of estimated generators { ˆGn}n∈N be\nuniformly equicontinuous, almost surely. Then, ˆGn\n♯ ν\nd−→ µ, almost surely.\nProof. We note that from the proof of Theorem 14, equation (28) holds and W1( ˆGn ◦ ˆEn)♯µ, µ) → 0, almost\nsurely. We fix an ω in the sample space, for which, W1( ˆGn\nω ◦ ˆEn\nω)♯µ, µ) → 0 and ( ˆEn\nω)♯µ\nd−→ ν. Here we\nuse the subscript ω to show that ˆGn and ˆEn might depend on ω. Clearly, the set of all ω’s, for which this\nconvergence holds, has probability 1.\nBy Skorohod’s theorem, we note that we can find a sequence of random variables {Yn}n∈N and Z, such\nthat Yn follows the distribution ˆEn\n♯ µ and Z ∼ ν, such that Yn\na.s.\n−−→ Z.\nSince { ˆGn\nω}n∈N are uniformly\nequicontinuous, for any ϵ > 0, we can find δ > 0, such that if |yn − z| < δ, | ˆGn\nω(yn) − ˆGn\nω(z)| < ϵ.\nThus, ˆGn\nω(Yn) − ˆGn\nω(Z)\na.s.\n−−→ 0.\nSince this implies that ˆGn\nω(Yn) − ˆGn\nω(Z)\nd−→ 0, it is easy to see that,\nW1( ˆGn\nω(Yn), ˆGn\nω(Z)) → 0. Now, since, W1( ˆGn\nω(Yn), ˆGn\nω(Z)) = W1(( ˆGn\nω)♯ν, ( ˆGn\nω ◦ ˆEn\nω)♯µ), we conclude that\nW1(( ˆGn\nω)♯ν, ( ˆGn\nω ◦ ˆEn\nω)♯µ) → 0, as n → ∞. Thus, with probability one, the RHS of (28) goes to 0 as n → ∞.\nHence, W1( ˆGn\n♯ ν, µ) → 0, almost surely.\nC.6.4\nProof of Corollary 16\nCorollary 16. Let diss(·, ·) = W1(·, ·) and suppose that the assumptions of Theorem 8 are satisfied and\ns > dµ. Also let supn∈N ∥ ˆGn∥Lip, supm,n∈N ∥ ˆGn,m∥Lip ≤ L, almost surely, for some L > 0. W1( ˆG♯ν, µ) ≲\nV (µ, ν, ˆG, ˆE) for both estimators (5) and (6).\nProof. Denoting ˆG as either of the estimators (5) and (6), it is easy to see that,\nW1( ˆG♯ν, µ) ≤W1( ˆG♯ν, ( ˆG ◦ ˆE)♯µ) + W1( ˆG ◦ ˆE)♯µ, µ)\n39\nChakraborty and Bartlett\n≤LW1(ν, ˆE♯µ) + W1( ˆG ◦ ˆE)♯µ, µ)\n≲W1(ν, ˆE♯µ) +\nZ\n∥ ˆG ◦ ˆE(x) − x∥2\n2dµ(x)\nD\nSupporting Results for Approximation Guarantees\nLemma 32. (Proposition 2 of Yarotsky (2017)) The function f(x) = x2 on the segment [0, 1] can be\napproximated with any error by a ReLU network, sqm(·), such that,\n1. L(sqm), W(sqm) ≤ c1m.\n2. sqm\n\u0000 k\n2m\n\u0001\n=\n\u0000 k\n2m\n\u00012, for all k = 0, 1, . . . , 2m.\n3. ∥sqm − x2∥L∞([0,1]) ≤\n1\n22m+2 .\nLemma 33. Let sqm(·) be taken as in Lemma 32, then, ∥sqm − x2∥Hβ ≤\n1\n2m−1 .\nProof. We begin by noting that, sqm(x) =\n\u0010\n(k+1)2\n2m\n− k2\n2m\n\u0011 \u0000x −\nk\n2m\n\u0001\n+\n\u0000 k\n2m\n\u00012, whenever, x ∈\n\u0002 k\n2m , k+1\n2m\n\u0001\n.\nThus, on ( k\n2m , k+1\n2m )),\n∥sqm − x2∥Hβ =∥sqm − x2∥L∞((\nk\n2m , k+1\n2m )) +\n\r\r\r\r\n(k + 1)2\n2m\n− k2\n2m − 2x\n\r\r\r\r\nL∞((\nk\n2m , k+1\n2m ))\n=\n1\n22m+2 + 1\n2m ≤\n1\n2m−1 .\nThis implies that, ∥sqm − x2∥Hβ ≤\n1\n2m−1 .\nLemma 34. Let M > 0, then we can find a ReLU network prod(2)\nm , such that,\n1. L(prod(2)\nm ), W(prod(2)\nm ) ≤ c2m, for some absolute constant c2.\n2. ∥prod(2)\nm − xy∥L∞([−M,M]×[−M,M]) ≤\nM 2\n22m+1 .\nProof. Let prod(2)\nm (x, y) = M 2 \u0010\nsqm\n\u0010\n|x+y|\n2M\n\u0011\n− sqm\n\u0010\n|x−y|\n2M\n\u0011\u0011\n. Clearly, prod(2)\nm (x, y) = 0, if xy = 0. We note\nthat, L(prod(2)\nm ) ≤ c1m+1 ≤ c2m and W(prod(2)\nm ) ≤ 2c1m+2 ≤ c2m, for some absolute constant c2. Clearly,\n∥prod(2)\nm − xy∥L∞([−M,M]×[−M,M]) ≤ 2M 2∥sq − x2∥L([0,1]) ≤\nM 2\n22m+1 .\nLemma 35. For any m ≥ 3, we can construct a ReLU network prod(d)\nm\n: Rd → R, such that for any\nx1, . . . , xd ∈ [−1, 1], ∥prod(d)\nm (x1, . . . , xd) − x1 . . . xd∥L∞([−1,1]d) ≤\nd3\n22m+2 .\n40\nChakraborty and Bartlett\nProof. Let M = 1 and d ≥ 2.\nWe define prod(k)\nm (x1, . . . , xk) = prod(2)\nm (prod(k−1)\nm\n(x1, . . . , xk−1), xd),\nk ≥ 3.\nClearly W(prod(d)\nm ), L(prod(d)\nm ) ≤ c3dm, for some absolute constant c3.\nWe also note that,\n|prod(d)\nm (x1, . . . , xd)| ≤\nM 2\n22m+1 +xd|prod(d−1)\nm\n(x1, . . . , xd−1)| ≤\nM 2\n22m+1 +M|prod(d−1)\nm\n(x1, . . . , xd−1)| ≤\nM 2\n22m+1 +\nM 3\n22m+1 + · · · + M d−1\n22m+1 + M d ≤\nM 2\n22m+1 + (d − 2)M d = d − 2 +\n1\n22m+1 ≤ d − 1. From induction, it is easy to see\nthat, prod(k)\nm ≤ d − 1. Taking M = d − 1, we get that,\n∥prod(d)\nm (x1, . . . , xd) − x1 . . . xd∥L∞([−1,1]d)\n=∥prod(2)\nm (prod(d−1)\nm\n(x1, . . . , xd−1), xd) − x1 . . . xd∥L∞([−1,1]d)\n≤∥prod(d−1)\nm\n(x1, . . . , xd−1) − x1 . . . xd−1∥L∞([−1,1]d) +\nM 2\n22m+2\n≤ dM 2\n22m+2\n=\nd3\n22m+2 .\nE\nSupporting Results from the Literature\nThis section lists some of the supporting results from the literature, used in the paper.\nLemma 36. (Kolmogorov and Tikhomirov, 1961) The ϵ-covering number of Hβ([0, 1]d, R, 1) can be bounded\nas,\nlog N\n\u0000(ϵ; Hβ([0, 1]d), ∥ · ∥∞\n\u0001\n≲ ϵ−d/β.\nLemma 37. (Theorem 12.2 of Anthony and Bartlett (2009)) Assume for all f ∈ F, ∥f∥∞ ≤ M. Denote\nthe pseudo-dimension of F as Pdim(F), then for n ≥ Pdim(F), we have for any ϵ and any X1, . . . , Xn,\nNϵ, F|X1:n , ℓ∞) ≤\n\u0012\n2eMn\nϵPdim(F)\n\u0013Pdim(F)\n.\nLemma 38. (Theorem 6 of Bartlett et al. (2019)) Consider the function class computed by a feed-forward\nneural network architecture with W many weight parameters and U many computation units arranged in L\nlayers. Suppose that all non-output units have piecewise-polynomial activation functions with p + 1 pieces\nand degrees no more than d, and the output unit has the identity function as its activation function. Then\nthe VC-dimension and pseudo-dimension are upper-bounded as\nVCdim(F), Pdim(F) ≤ C · LW log(pU) + L2W log d.\n41\n"
}