{
    "optim": "Published as a conference paper at ICLR 2024\nVONET: UNSUPERVISED VIDEO OBJECT LEARNING\nWITH PARALLEL U-NET ATTENTION AND OBJECT-\nWISE SEQUENTIAL VAE\nHaonan Yu\nHorizon Robotics\nCupertino, CA 95014, USA\nhaonan.yu@horizon.cc\nWei Xu\nHorizon Robotics\nCupertino, CA 95014, USA\nwei.xu@horizon.cc\nABSTRACT\nUnsupervised video object learning seeks to decompose video scenes into struc-\ntural object representations without any supervision from depth, optical flow,\nor segmentation.\nWe present VONet, an innovative approach that is inspired\nby MONet. While utilizing a U-Net architecture, VONet employs an efficient\nand effective parallel attention inference process, generating attention masks for\nall slots simultaneously. Additionally, to enhance the temporal consistency of\neach mask across consecutive video frames, VONet develops an object-wise se-\nquential VAE framework. The integration of these innovative encoder-side tech-\nniques, in conjunction with an expressive transformer-based decoder, establishes\nVONet as the leading unsupervised method for object learning across five MOVI\ndatasets, encompassing videos of diverse complexities.\nCode is available at\nhttps://github.com/hnyu/vonet.\n1\nINTRODUCTION\nUnsupervised video object learning has garnered increasing attention in recent years. It focuses on\nthe extraction of structural object representations from video sequences, without the aid of any super-\nvision, such as depth information, optical flow, or labeled segmentation masks. The goal is to enable\nmachines to automatically learn to discern and understand objects within video streams, an essential\ncapability with wide-ranging applications in fields such as autonomous robotics (Veerapaneni et al.,\n2020; Creswell et al., 2021), surveillance (Jiang et al., 2019), and video content analysis (Zhou\net al., 2022). The utilization of such object-centric representations could lead to improved sample\nefficiency, robustness, and generalization to novel tasks (Greff et al., 2020).\nSlot attention methods (Locatello et al., 2020; Kipf et al., 2021; Elsayed et al., 2022; Singh et al.,\n2022b) have recently demonstrated significant successes in video object learning. They typically\nutilize a CNN to extract a feature map from an input image. This feature map is spatially flattened\ninto a sequence of features, which are then queried by each slot latent to generate an attention mask.\nOur observation is that slot attention often encounters a dilemma, referred to as “granularity versus\ncontinuity”. To generate fine-grained attention masks, it is necessary to select a large spatial shape\nfor the feature map. However, doing so makes it challenging to ensure the smoothness of the mask\ndue to the nature of the Key-Query-Value attention mechanism (Locatello et al., 2020). Sometimes\nthe absence of smoothness may result in significant mask quality degradation.\nThis paper introduces VONet for unsupervised video object learning. Inspired by MONet (Burgess\net al., 2019) for image object learning, we posit that the inductive bias for spatial locality, as seen\nin the U-Net (Ronneberger et al., 2015) of MONet, offers a solution to the dilemma. However,\nMONet’s recurrent attention generation, forwarding the same U-Net multiple times sequentially,\nis very inefficient when handling a large number of slots, and consequently impedes its further\napplication to video. Our first key innovation is an efficient and effective parallel attention inference\nprocess (Figure 1, b) that generates attention masks for all slots simultaneously from a U-Net. It can\nsustain a nearly constant inference time as the number of slots increases within a reasonable range.\nFurthermore, to achieve temporal consistency of objects between adjacent video frames, VONet\nincorporates an object-wise sequential VAE framework. This framework adapts the conventional\n1\narXiv:2401.11110v1  [cs.CV]  20 Jan 2024\nPublished as a conference paper at ICLR 2024\nsequential VAE (Kingma & Welling, 2013; Hafner et al., 2019) to the context of multi-object inter-\naction and dynamics. The minimization of the KLD between the posterior and a forecasted prior\nmodels the dynamic interaction and coevolvement of multiple objects in the scene. This encourages\nthe emergence of slot content that is temporally predictable and thus consistent in a holistic manner.\nBy adjusting the weight of the KLD, we are able to control the importance of temporal consistency\nrelative to video reconstruction quality.\nTo further bolster its capabilities, VONet employs an expressive transformer-based decoder (Singh\net al., 2022b) that empowers itself to successfully derive object representations from complex video\nscenes. To showcase the effectiveness of VONet, we conduct extensive evaluations across five MOVI\ndatasets (Greff et al., 2022) encompassing video scenes of varying complexities. The evaluation\nresults position VONet as the new state-of-the-art unsupervised method for video object learning.\n2\nRELATED WORK\nNumerous prior studies, such as Burgess et al. (2019); Greff et al. (2019); Locatello et al. (2020);\nEngelcke et al. (2021); Singh et al. (2022a); Zoran et al. (2021); Emami et al. (2021); H´enaff et al.\n(2022); Seitzer et al. (2022), explored unsupervised object learning in single images. For unsuper-\nvised video object learning, applying these image-based methods to video frames independently\nis not a viable approach, as it would likely result in slot masks lacking temporal consistency. A\nconventional strategy for transitioning from image object learning to video object learning entails\ninheriting and modifying the slot content from the preceding time step. For instance, AIR (Eslami\net al., 2016), SQAIR (Kosiorek et al., 2018), STOVE (Kossen et al., 2019), and SCALOR (Jiang\net al., 2019) employed a propagation process in which a subset of currently existing objects is prop-\nagated to the next time step; TBA (He et al., 2019), ViMON (Weis et al., 2021), SAVI (Kipf et al.,\n2021), SAVI++ (Elsayed et al., 2022), STEVE (Singh et al., 2022b), and VideoSAUR (Zadaianchuk\net al., 2023) initialized slots for the current step using the output of a forward predictor/tracker\napplied to the preceding slots. Another technique for ensuring temporal consistency is to model\nconstant object latents across time, as demonstrated in Kabra et al. (2021). These object latents re-\nmain invariant across all frames by design, enabling stable object tracking. Alternatively, an explicit\ntemporal consistency loss could be incorporated. Creswell et al. (2021) proposed an alignment loss\nwhich ensures that each object is represented in the same slot across time; Bao et al. (2022) encour-\naged similarity between the feature representations of slots in consecutive frames. Our approach\ninherits preceding slot content while also introducing a KLD loss of a sequential VAE to further\nenhance temporal consistency.\nDue to the absence of supervision signals, most existing methods could only handle uncomplicated\nvideo scenes with uniformly-colored objects or objects with simple sizes and shapes. For instance,\nin an unsupervised setting, Creswell et al. (2021); Kipf et al. (2021); Elsayed et al. (2022) demon-\nstrated effectiveness primarily on uniformly-colored objects with clean backgrounds. Similarly,\nwhile SCALOR (Jiang et al., 2019) showcases an impressive capability in discovering and tracking\ndozens of simple objects of similar sizes in certain videos, it exhibits sensitivity to hyperparameters\nrelated to object scale and size ratio. As a result, it performs inadequately when dealing with textured\nobjects of diverse shapes and sizes. STEVE (Singh et al., 2022b), on the other hand, successfully im-\nproved performance in complex videos through the introduction of an expressive transformer-based\ndecoder. Nevertheless, it faces significant issues of over-segmentation and object identity swapping\nin less complex videos. Our method adopts the same transformer-based decoder to handle complex\nvideo scenes, while still being able to maintain superior performance in simple scenes.\nA substantial body of related work in the field of video segmentation (Zhou et al., 2022) relies on\nsupervision signals such as segmentation masks, depth information, optical flow, and more. Our\nproblem also bears relevance to the domain of video object tracking (Ciaparrone et al., 2020), where\nobject locations or bounding boxes are specified in the initial video frames and tracked across sub-\nsequent frames. Since we do not assume these additional learning signals, due to space constraints,\nwe will not delve into detailed discussions of these two topics here. Notably, certain previous works,\nsuch as Zadaianchuk et al. (2023), employ a vision backbone pretrained on large-scale datasets for\nfeature extraction from video frames, aiding in the process of object learning. While their objective\nremains unsupervised, the use of pretrained features integrates valuable prior knowledge of every-\nday object appearances. Consequently, the results they reported, though surpassing what typical\nunsupervised methods including ours could achieve, hold limited reference value in this context.\n2\nPublished as a conference paper at ICLR 2024\n\u0003\u0003$WW\u00031HW\n«\n\u0003\u0003$WW\u00031HW\n\u0003\u0003$WW\u00031HW\nVFS\nVFS\nVFS\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u00033DUDOOHO\u0003DWWHQWLRQ\u0003QHWZRUN\n«\n\u0003\u0003\u0003\u0003\u0003\u0003&RQWH[W\u0003JHQHUDWRU\n(a)\n(b)\nFigure 1: Attention processes of MONet (a) and VONet (b) on a single image. Red arrows represent\nsequential operations (“scp” stands for the MONet scope), while blue arrows at the same horizontal\nlevel represent parallel operations. The dependency on the input image has been omitted for clarity.\nVONet shares a similarity with ViMON (Weis et al., 2021) in that both extend MONet (Burgess et al.,\n2019) from images to video. However, several major differences exist between the two: i) VONet\nparallelizes the original MONet architecture for efficient inference while ViMON still generates\nslots recurrently. ii) VONet seeds the attention network with context vectors while ViMON does this\nusing previous attention masks. iii) VONet formulates an object-wise sequential VAE to promote\ntemporal consistency, whereas ViMON ignores this by using a typical intra-step VAE.\n3\nPRELIMINARIES\nMONet for unsupervised image object learning. The Multi-Object network (MONet) (Burgess\net al., 2019) is a scene decomposition model designed for single images. A forward pass of MONet\ngenerally consists of two stages: mask generation and image reconstruction with a component VAE.\nGiven an input RGB image x ∈ RH×W ×C, MONet utilizes a recurrent procedure to sequentially\ngenerate K attention masks mk ∈ [0, 1]H×W , with K representing the predefined number of slots.\nA slot represents either an object or a background region. The procedure comprises K −1 steps, and\nat each step, the mask is determined as mk = sk−1α(x, sk−1), where the scope sk−1 ∈ [0, 1]H×W\nrepresents the remaining attention for each pixel, with the initial scope being s0 = 1. The atten-\ntion network, denoted as α, is implemented as a U-Net (Ronneberger et al., 2015), to predict the\namount of attention the k-th step will consume from the current scope sk−1. The scope is then\nupdated as sk = sk−1(1 − α(x, sk−1)). At the last step, MONet directly sets mK = sK−1,\nwhich ensures that the entire image is explained by all slots (PK\nk=1 mk = 1). Conditioned on\nthe predicted attention mask, each slot undergoes independent processing through a VAE to (par-\ntially) reconstruct the input image. The VAE first encodes each slot into a compact embedding by\nzk ∼ q(zk|x, mk) and then derives a decoded image distribution o(x|zk) from this embedding. To\ndo so, pixels {xn} are decoded independently from each other conditioned on the slot embedding,\nnamely o(x|zk) = Q\nn o(xn|zk). Finally, to consolidate the reconstruction results from different\nslots, MONet formulates a mixture of components decoder distribution in its training loss 1:\nLMONet = −\nH×W\nX\nn=1\nlog\nK\nX\nk=1\no(xn|zk)mk,n + β\nK\nX\nk=1\nDKL\n\u0010\nq(zk|x, mk)\n\f\f\f\n\f\f\fp(zk)\n\u0011\n,\n(1)\nwhere the prior p(zk) is a unit Gaussian. Intuitively, each slot only needs to encode/decode image\nregions that has been selected by its attention mask. Figure 1 (a) depicts the simplified attention\nprocess of MONet, emphasizing the data flow during a forward pass. MONet is deterministic and\nits masks have only unidirectional dependencies. Due to the recurrent nature of mask generation, as\nK increases, the inference time for a forward pass will become prohibitive.\nUnsupervised video object learning. In the context of unsupervised video object learning, each\ninput sample is a sequence of RGB frames {x1, . . . , xT }. The goal is to determine a set of attention\nmasks, {mt,k}K\nk=1, for each frame xt as in the single-image case. Additionally, the mask sequence,\n{mt,k}T\nt=1, associated with a specific slot k, should focus on a consistent set of objects. Because\nMONet was originally proposed for single images, there is no guarantee that directly applying it\nto two adjacent video frames will result in temporally coherent object masks, even though the two\nvideo frames might look similar. It is yet to be determined how MONet can be extended to facilitate\nvideo object learning with temporal consistency.\n1MONet included a third loss term to encourage the geometrical simplicity of the attention masks mk by\nhaving the decoder also reconstruct these masks. This loss is not discussed here.\n3\nPublished as a conference paper at ICLR 2024\n\u0003\u0003\u0003\u0003,PDJH\n\u0003\u0003\u0003\u0003,PDJH\u0003\u0003\u0003\u0003\n\u0003\u0003\u0003,PDJH\u0003\u0003\u0003\u0003\u0003\u0003\n«\n\u000b\u0003\u0003\u0003\u0003\u0003\u0003\f\n8\u00101HW\u0003GRZQ\n\u000b\u0003\u0003\u0003\u0003\u0003\u0003\f \u000b\u0003\u0003\u0003\u0003\u0003\u0003\f\n8\u00101HW\u0003GRZQ\n8\u00101HW\u0003GRZQ\n\u0003\u0003\u0003\u0003\u0003\u0003\u00037UDQVIRUPHU\n\u0003\u0003\u0003\u0003\u0003\u00038\u00101HW\u0003XS\n\u0003\u0003\u0003\u0003\u0003\u00038\u00101HW\u0003XS\n\u0003\u0003\u0003\u0003\u0003\u00038\u00101HW\u0003XS\n\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u0003\u00036RIWPD[\nFigure 2: Diagram of the parallel attention network. Except for the transformer and softmax opera-\ntor, it is possible to parallelize the executions related to the U-Net components. The skip connections\nbetween the U-Net downsampling and upsampling layers have been omitted for clarity.\n4\nVONET FOR UNSUPERVISED VIDEO OBJECT LEARNING\nParallel U-Net attention. VONet begins by eliminating the concept of “scope” in MONet and\nintroduces context, a compact embedding vector expected to encompass prior object information\nfor each slot to be generated. Let the context vector of slot k at time step t be ct−1,k. Instead of\nrecurrent mask generation, VONet generates all masks at step t at once (Figure 1, b):\nmt,1, . . . , mt,K = ParallelAttn(xt, ct−1,1, . . . , ct−1,K).\n(2)\nThe parallel attention network operates by simultaneously applying the same U-Net architecture\nto the K context inputs in parallel, while establishing communication among the slots at the U-\nNet bottleneck layer. To achieve this, the output of the U-Net downsampling path is first flattened,\nand the outputs from different slots are pooled to create a sequence. This sequence is then input\ninto a decoder-only transformer that produces a sequence of latent mask embeddings. Each mask\nembedding is further projected and reshaped to a 2D feature map, which then serves as the input for\nthe U-Net upsampling path. In the final step, the K output logits maps undergo pixel-wise softmax\nto derive the ultimate attention masks (Figure 2). Formally,\nht,k = Udown(xt, ct−1,k),\nqt,1, . . . , qt,K = Transformermask(ht,1, . . . , ht,K).\nlt,k = Uup(qt,k),\nmt,1, . . . , mt,K = Softmax(lt,1, . . . , lt,K).\n(3)\nDue to the single shared downsampling/upsampling path among the slots, in practice, Udown and Uup\ncan be executed efficiently by rearranging the slot dimension into the batch dimension.\nCalculating the contexts. One may pose the question: how can the context vectors be acquired?\nAs previously mentioned, a context vector for a specific slot should encapsulate prior information\nregarding the aspects of the scene that this particular slot is intended to encode. Indeed, this prior\ninformation can naturally be derived from the content of that slot at the preceding time steps. For\neach slot k, we derive its context ct,k (used by t + 1) based on the history of the slot up to step t:\nyt,k = SlotEnc(xt, mt,k),\nrt,k = RNN(yt,k, rt−1,k),\nct,k = MLPcxt(rt,k),\n(4)\nwhere the per-frame slot latent yt,k is extracted through a slot encoder that takes both the image\nxt and the generated attention mask mt,k as the inputs. Meanwhile, rt,k can be viewed as the per-\ntrajectory slot latent, as it accumulates previous per-frame slot latents via a recurrent network. We\ninitialize the per-trajectory slot latent by transforming a collection of noise vectors independently\ndrawn from a unit Gaussian:\nr0,1, . . . , r0,K = Transformerslot(ϵ1, . . . , ϵK), ϵk ∼ N(0, 1),\n(5)\nThis initialization signifies a stochastic process where the slots are collaboratively seeded prior to\nany exposure to the video content. Figure 9 (Appendix A.2) illustrates the temporal unrolling of\nVONet during a forward pass on a video.\nObject-wise sequential VAE. Finally, we compute the slot posterior distribution directly based on\nits most recent per-trajectory slot latent at t as\nzt,k ∼ q(zt,k|rt,k).\n(6)\n4\nPublished as a conference paper at ICLR 2024\n\u0003\u00030/3\n\u00033DUDOOHO\u0003DWWQ\u0003QHW\n\u00036ORW(QF\n\u00037UDQV'HF\n\u0003511\n'HF\u0003ORVV\n\u00033ULRU\n\u0003WUDQVIRUPHU\n./'\n3HU\u0010WUDMHFWRU\\\u0003\nVORW\u0003ODWHQW\n&RQWH[W\u0003YHFWRU\n$WWHQWLRQ\u0003PDVN\n3HU\u0010IUDPH\u0003VORW\u0003ODWHQW\n3UHGLFWHG\u0003SHU\u0010WUDMHFWRU\\\u0003\nVORW\u0003ODWHQW\n6ORW\u0003SRVWHULRU\n6ORW\u0003SULRU\n3RVWHULRU\u0003HQFRGLQJ\nFigure 3: VONet’s architecture. The dependency on the input image xt has been omitted for clarity.\n* in the subscripts represents the collection of K (K = 2 here) slots in parallel.\nMeanwhile, we derive the slot prior distribution using all the K per-trajectory slot latents up to t−1,\np(¯zt,k|rt−1,1, . . . , rt−1,K),\nwhich in fact results in an object-wise sequential VAE. In essence, for proper learning of the prior,\nVONet must anticipate how each slot will evolve in interaction with the other K − 1 slots. A\nstraightforward approach to instantiating the prior would involve utilizing a transformer to predict a\nfuture slot latent for each slot. This prediction can then serve as the basis for computing the prior:\nr′\nt,1, . . . , r′\nt,K = Transformerprior(rt−1,1, . . . , rt−1,K),\n¯zt,k ∼ p(¯zt,k|r′\nt,k).\n(7)\nFor reconstruction, we opt for the transformer-based decoder (Singh et al., 2022a;b), owing to its\nremarkable performance observed in handling complex images. Thus our overall training loss is\nLVONet =\nT\nX\nt=1\nh\n− log PTransDec(xt|zt,1, . . . , zt,K)\n+β\nK\nX\nk=1\nDKL\n\u0010\nq(zt,k|rt,k)\n\f\f\f\n\f\f\fp(zt,k|rt−1,1, . . . , rt−1,K)\n\u0011i\n.\n(8)\nNote that our KLD term, which integrates a learned prior, plays a pivotal role in strengthening the\ntemporal consistency of individual slots across consecutive video frames. The rationale behind this\nenhancement lies in the fact that only slot representations that exhibit temporal consistency can\nexhibit predictability, consequently resulting in a reduced KLD loss.\nAn overview of the architecture of VONet is illustrated in Figure 3.\n5\nIMPLEMENTATION\nBackbone. In the very initial stage of the encoding step, we use a CNN backbone (Appendix A.2)\nto extract a feature map from each input image xt . The output from the backbone is shared between\nthe parallel attention network (Eq. 3) and the slot encoder (Eq. 4), serving as the actual image input.\nThrough parameter sharing, this backbone effectively reduces the total number of model parameters.\nIt is trained from scratch using VONet’s training loss.\nParallel attention network.\nDirectly training the attention network formulated in Eq. 3 could be\nchallenging due to the very depth of the U-Net. In each individual frame, this challenge stems from\nthe intricate path that the gradient signal must traverse, starting from the VAE decoder, progressing\nthrough the slot encoder, then navigating the U-Net, and ultimately reaching the context vectors.\nConsequently, the generated masks mt,k may be trapped in a trivial solution, where each pixel\nreceives equal attention values from the K slots. To address this issue, we first convolve each context\nvector across the backbone feature locations, yielding an estimated attention mask for each slot. This\noperation is analogous to slot attention (Locatello et al., 2020; Kipf et al., 2021). Subsequently, the\nU-Net is tasked with generating only delta changes (in log space) atop the estimated mask. This\neffectively establishes a special skip connection between the U-Net’s input and output layers. We\nfound that this skip connection plays a crucial role in learning meaningful attention masks.\nSlot encoder. We adopt a simple architecture for the slot encoder. The attention mask is first\nelement-wise multiplied with the backbone feature map, with broadcasting in the channel dimension.\n5\nPublished as a conference paper at ICLR 2024\nMOVI-A\nMOVI-B\nMOVI-C\nMOVI-D\nMOVI-E\nFigure 4: Example video frames of the MOVI datasets. A,B,C contain up to 10 objects while D,E\ncontain up to 23 objects in each video.\nThen, the per-frame slot latent yt,k is obtained by performing average pooling on the masked feature\nmap across the spatial domain. While there may be other possible implementations of the slot\nencoder, we have found this straightforward approach to be highly effective and easy to train.\nComputing the per-trajectory slot latent. We instantiate the RNN for calculating rt,k (Eq. 4) as\na GRU (Cho et al., 2014) followed by an MLP. Specifically, it is defined as\nr′\nt,k = GRU(yt,k, rt−1,k),\nrt,k = LayerNorm(r′\nt,k + MLPslot(r′\nt,k)).\n(9)\nLayerNorm (Ba et al., 2016) ensures that the latent values will not explode even for a long video.\nComputing the KLD. We model both the posterior and prior distributions as diagonal Gaussians.\nWhen minimizing the KLD loss in Eq. 8, two primary influences come into play: the posterior is\nregularized by the prior, and conversely, the prior is shaped towards the posterior. These dynamics\njointly contribute to the learning of slot representations. Following Hafner et al. (2020), we use a\nKL balancing coefficient κ > 0.5 to accelerate the learning of the prior relative to the posterior. This\nis done to prevent the regularization of the posterior by a poorly trained prior. Mathematically,\nKLD = κ · DKL(StopGrad(q)∥p) + (1 − κ) · DKL(q∥StopGrad(p)).\n(10)\n6\nEXPERIMENTS\nBenchmarks. We assess VONet on five well-established public datasets MOVI-{A,B,C,D,E} (Greff\net al., 2022), which include both synthetic and naturalistic videos. MOVI-A and MOVI-B consist of\nsimple scenarios featuring nearly uniform backgrounds and objects with varying colors but minimal\ntexture. MOVI-B exhibits greater complexity compared to MOVI-A, owing to the inclusion of 8\nadditional object shapes. MOVI-{C,D,E} stand out as the most challenging, featuring realistic,\nintricately textured everyday objects and backgrounds. While MOVI-{A,B,C} include up to 10\nobjects in a given scene, MOVI-D and MOVI-E include up to 23 objects. Each video within the five\ndatasets comprises 24 frames, lasting 2 seconds. We adhere to the official dataset splits, except that\nthe validation split is employed as the test split, following Kipf et al. (2021). Some example frames\nof the five datasets are shown in Figure 4.\nMetrics. We assess the quality of 3D slot segmentation masks generated from slot attention masks\nacross the full length of 24 video frames (details in Appendix A.2). Two common metrics for video\nobject learning are used: FG-ARI (Greff et al., 2019) and mIoU. FG-ARI serves as a clustering\nsimilarity metric, measuring the degree to which predicted segmentation masks align with ground-\ntruth masks in a permutation-invariant manner. It only considers foreground pixels, where each\ncluster corresponds to a foreground segmentation mask. To also evaluate background pixels, mIoU\ncalculates the mean Intersection-over-Union by first employing the Hungarian algorithm to find the\noptimal bipartite matching (in terms of IoU) between predicted and groundtruth masks, and then\ndividing sum of the optimal IoUs by the number of groundtruth masks. Both FG-ARI and mIoU\ndemand temporal consistency and penalize object identity switches at any video frame. Nonetheless,\nneither of them is perfect. For a comprehensive evaluation, a combined interpretation is necessary.\nGeneral training setup. VONet learns 11 slots on MOVI-{A,B,C} and 16 slots on MOVI-{D,E}.\nWe use a mini-batch size of 32 for MOVI-{A,B,C} and 24 for MOVI-{D,E}. Each sample in a batch\nhas a sequence length of 3. A replay buffer (detailed in Appendix A.2) is employed to enable training\nfrom past slot states while preserving the i.i.d. assumption of training data. All video frames are\nresized to 128 × 128. In all subsequent experiments, the learning is entirely unsupervised, meaning\nthat no additional supervision signals, such as depth, optical flow, or segmentation, are provided.\nFor each experiment, we run three random seeds to report the mean and standard deviation of metric\n6\nPublished as a conference paper at ICLR 2024\nMethod\nFG-ARI\nmIoU\nMOVI-A\nMOVI-B\nMOVI-C\nMOVI-D\nMOVI-E\nMOVI-A\nMOVI-B\nMOVI-C\nMOVI-D\nMOVI-E\nSAVI\n45.1±1.3\n31.8±1.2\n22.9±1.3\n29.4±0.5\n36.0±3.3\n32.2±1.7\n31.2±4.3\n15.9±0.7\n15.6±0.3\n15.8±2.2\nSIMONe\n50.2±6.2\n36.5±0.5\n19.5±0.1\n34.8±0.2\n41.3±0.3\n37.6±1.0\n35.5±0.8\n20.2±0.1\n22.7±0.1\n20.8±0.2\nSCALOR\n68.1±1.4\n45.3±0.6\n21.3±2.3\n33.5±2.8\n39.6±0.5\n59.8±0.9\n46.6±0.1\n14.3±0.8\n14.2±1.1\n12.1±0.2\nViMON\n62.8±2.3\n26.2±4.4\n18.0±2.1\n22.5±5.7\n17.7±1.5\n50.0±1.5\n34.4±5.0\n27.1±0.9\n19.6±1.5\n17.8±1.2\nSTEVE\n47.8±8.2\n29.6±0.3\n38.1±0.3\n42.9±2.8\n49.7±1.0\n53.3±3.1\n42.7±0.1\n30.5±0.2\n23.8±3.7\n26.2±1.3\nVONet\n91.0±1.5\n60.6±0.8\n45.3±0.4\n50.7±1.1\n56.3±0.5\n60.5±2.0\n59.7±2.9\n42.8±0.5\n37.7±0.3\n36.4±0.7\nTable 1: Results of VONet and the five baselines, in an unsupervised learning setting.\nvalues. Additional architectural specifics and hyperparameter details are provided in Appendix A.3\nfor reference. It takes about 36 hours to train VONet on 4x 3090 GPUs on each of the five datasets,\nfor a total number of 150k gradient updates.\n6.1\nEFFICIENCY OF THE PARALLEL ATTENTION\n5\n11\n16\n24\nNumber of Slots\n0.00\n0.01\n0.04\n0.07\nAttn inference time (sec)\nMONet\nVONet\nFigure 5:\nComparison\nof the attention inference\nefficiencies.\nWe start with measuring how the efficient our parallel attention is com-\npared to the recurrent attention of MONe. With the same U-Net architec-\nture (Appendix A.2), we report the average time of generating attention\nmasks for a varying number of slots on an image of size 128 × 128. The\nresults are plotted in Figure 5. VONet demonstrates a strong advantage\nof being able to maintain a nearly constant inference time regardless of\nthe increasing number of slots. Conversely, MONet’s time grows linearly\nwith respect to the slot number, which is expected as MONet generates\none attention mask after another. The superior efficiency of VONet on\nimages forms the basis for extending it to video frame sequences.\n6.2\nCOMPARISON WITH BASELINES\nBaselines. We conduct a comparative analysis of VONet against five representative unsupervised\nvideo object learning baselines: SCALOR (Jiang et al., 2019), ViMON (Weis et al., 2021), SI-\nMONe (Kabra et al., 2021), SAVI (Kipf et al., 2021; Elsayed et al., 2022), and STEVE (Singh\net al., 2022b). A crucial criterion for baseline selection is the accessibility of an official or public\nimplementation. We employ official implementations of SCALOR, ViMON, SAVI, and STEVE,\nand a re-implementation of SIMONe (Appendix A.4). SCALOR, ViMON, SIMONe, and STEVE\nwere originally designed for unsupervised learning, with their reconstruction targets being RGB\nvideo frames. SAVI, on the other hand, relies on object bounding boxes in initial video frames and\nreconstructs supervision signals like depth or optical flow. For a fair comparison, we eliminated\nthe bounding box conditioning and only allowed SAVI to reconstruct RGB frames. We adhered to\nbest practices from the literature for configuring the hyperparameters of the five baselines, selecting\nvalues recommended either by official codebases or by hyperparameter search results.\nResults.\nIn Table 1, it is evident that VONet surpasses all the baselines in terms of both FG-ARI\nand mIoU metrics across all five MOVI datasets. Furthermore, the low std values imply its stability,\nan important characteristic often lacking in unsupervised video object learning methods. It can\nalso be seen that MOVI-{C,D,E} are indeed much more challenging than MOVI-{A,B}, due to the\nrich textures and complex backgrounds contained in these datasets. SAVI, SCALOR, and ViMON\nface challenges when dealing with complex video scenes. Surprisingly, while STEVE performed\nadmirably on MOVI-{C,D,E}, its FG-ARI performance on MOVI-{A,B} was unexpectedly low.\nUpon closer examination of its visualized results, STEVE tends to over-segment foreground objects\nand swap object identities, when provided with redundant slots in simpler scenarios (see examples\nin Figure 7). This behavior significantly diminishes its FG-ARI performance.\n6.3\nHOW CRITICAL ARE DIFFERENT COMPONENTS TO VONET?\nWe conduct an ablation study to quantify the contribution of various components to the effectiveness\nof VONet. We select several key ingredients for ablation, resulting in four variants of VONet.\na) wo-UNet ablates the U-Net for attention mask generation by removing it, along with the mask\ntransformer at its bottleneck layer, from the parallel attention pipeline. Instead, it directly outputs\nthe estimated attention masks (Section 5). This variant aims to investigate whether the mask\nrefinement provided by the U-Net is indispensable.\n7\nPublished as a conference paper at ICLR 2024\nFigure 6: Ablation study results. Each error bar represents the std of three random seeds.\nMOVI-A\nMOVI-B\nMOVI-C\nMOVI-D\nMOVI-E\nVideo\nGT\nV (FG)\nV\nS (FG)\nS\nFigure 7: Example segmentation masks of STEVE (S) and VONet (V). Each column showcases\nan example video from one of the five datasets. Three key frames are presented in a row for each\nvideo. Each unique color represents the mask for a specific slot. S/V (FG) shows foreground-only\nsegmentation after background pixels being masked out.\nb) KL- β\nW reduces the importance of the KLD term in the training loss, where β is the KLD weight\nused by VONet in the experiments. We set W to four values: 20, 200, 20k, and ∞, where ∞\ncorresponds to completely eliminating the KLD.\nc) wo-Replay removes the replay buffer technique and trains from fresh states for each sampled\nshort video segment, as commonly seen in prior works (Kipf et al., 2021; Singh et al., 2022b).\nd) wo-KLBal excludes the KL balancing technique and calculates a standard KLD loss directly.\nAll four variants were trained using three random seeds on the five MOVI datasets. For each variant,\nthe remaining training configurations are exactly the same as those used for VONet.\nResults.\nFigure 6 shows the impact on the performance if a specific component is removed or\nweakened in VONet. Notably, without KL balancing, the outcome is very poor on MOVi-A. In this\ncase, VONet struggles to acquire any meaningful mask, as each pixel receives uniform attention\nfrom all slots. It is obvious that the posterior has been heavily forced into resembling the prior\nbefore the latter is adequately learned. Interestingly, the replay buffer is far more important on\nMOVI-{A,B} than on the other three. One plausible explanation is that these two datasets can have\nobjects with similar appearances in a video, which makes object tracking more challenging. Training\nwith replayed slot states enhances the long-term object tracking ability of VONet. As for the U-Net\narchitecture, the mask refinement it offers proves to be particularly crucial for MOVI-{C,D,E}. This\naligns with our expectation that its inherent spatial locality bias is beneficial for handling complex\nvideos. Lastly, we have observed that the complete removal of the KLD term (W = ∞) consistently\nresults in unstable training and eventual crashes, and thus its results were not plotted. Apart from\nthis, we do observe performance declines as the weight decreases, especially on MOVI-{B,C,D,E}.\nIn summary, the four components all prove to be essential. The removal of any of them results in\ncatastrophic outcomes on at least one dataset.\n6.4\nVISUALIZATION AND ANALYSIS\nObject masks. Figure 7 shows example object masks for VONet and STEVE. While VONet occa-\nsionally splits the background, it excels at preserving the structural integrity of moving foreground\nobjects. On the contrary, STEVE tends to over-segment foreground objects, and its object masks\nexhibit temporal shifts over time. VONet generates smoother and more compact foreground object\nmasks, whereas STEVE produces jagged ones with irregular shapes. We believe that this difference\n8\nPublished as a conference paper at ICLR 2024\nInput\nPosterior Rec\nPrior Rec\nInput\nPosterior Rec\nPrior Rec\nFigure 8: Left: Reconstruction results from the posterior and prior. Right: KLD curves of slot 0 on\ntwo example videos. The KLD value has been divided by the slot embedding dimension (128).\narise partially from the strong inductive bias of spatial locality of VONet’s U-Net architecture for\nattention masks. This bias is absent in slot-attention methods like STEVE.\nVAE prior. One can evaluate the quality of the VAE prior modeling in VONet by reconstructing\ninput frames using forward-predicted slots. We randomly sample multiple video frames and utilize\nthe predicted priors for their reconstruction. The results are shown in Figure 8 (left). We can see\nthat the prior is effectively modeled, since the decoded images generated from the prior closely\nresembles reconstructed image derived from the posterior slots.\nKLD visualization. One can also inspect how the per-slot KLD loss in Eq. 8 varies from frame\nto frame. We anticipate that when a slot exhibits greater temporal variation at specific frames, the\ncorresponding KLD losses will also rise. Figure 8 (right) illustrates two example videos in which\nthe KLD curves are plotted for slot 0. It is evident that the KLD effectively measures the degree\nof variation in slot content. When the content undergoes significant changes, the KLD loss rises,\nwhereas when the object represented by the slot remains stable, the KLD loss stays at a low level.\nFailure modes. The first failure mode is over-segmentation (Zimmermann et al., 2023), happening\nwhen the video scene’s object count is much lower than the pre-allocated slot number. Without extra\nconstraints or priors, VONet aims to use all available slots to encode the video scene for a reduced\ndecoder loss. This causes some objects attended to by multiple slots or the background fragmented\ninto parts (Figure 7, MOVI-A). To address over-segmentation, the model needs to deactivate “re-\ndundant” slots. Extra losses may be needed to penalize the use of too many slots. The second\nfailure mode is incomplete object understanding due to the absence of objectness priors. Although\nwe use temporal information for object discovery in videos, the overall learning system remains\nunder-constrained. When an object exhibits multiple texture regions, the model faces challenges in\ndiscerning whether it represents a single entity with visually distinct components in motion or multi-\nple distinct objects moving closely together (Figure 7, MOVI-D). Integrating pretrained knowledge\nabout the appearances of everyday objects could help (Zadaianchuk et al., 2023). Lastly, the en-\nforcement of slot temporal consistency may occasionally prove unsuccessful. In certain instances,\neven when a slot appears to lose temporal consistency, upon closer examination, its KLD loss re-\nmains low (Figure 7, the dark-green background slot in MOVI-B), simply because the VAE prior\nis accurately predicted. This suggests an opportunity for improving our KLD loss. The temporal\nconsistency might also benefit from using a long-term memory model (Cheng & Schwing, 2022) as\nopposed to the current short-term GRU memory which might get expired under long-time occlusion.\n7\nCONCLUSION\nWe have presented VONet, a state-of-the-art approach for unsupervised video object learning.\nVONet incorporates a parallel attention process that simultaneously generates attention masks for\nall slots from a U-Net. The strong inductive bias of spatial locality in the U-Net leads to smoother\nand more compact object segmentation masks, compared to those derived from slot attention. Fur-\nthermore, VONet effectively tackles the challenge of temporal consistency in video object learning\nby propagating context vectors across time and adopting an object-wise sequential VAE framework.\nAcross five datasets comprising videos of diverse complexities, VONet consistently demonstrates\nsuperior effectiveness compared to several strong baselines in generating high-quality object repre-\nsentations. We hope that our findings will offer valuable insights for future research in this field.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nZhipeng Bao, Pavel Tokmakov, Allan Jabri, Yu-Xiong Wang, Adrien Gaidon, and Martial Hebert.\nDiscovering objects that can move. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11789–11798, 2022.\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt\nBotvinick, and Alexander Lerchner. Monet: Unsupervised scene decomposition and represen-\ntation. arXiv preprint arXiv:1901.11390, 2019.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In International conference on machine learning, pp. 1691–\n1703. PMLR, 2020.\nHo Kei Cheng and Alexander G. Schwing. Xmem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In ECCV, 2022.\nKyunghyun Cho, Bart Van Merri¨enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nGioele Ciaparrone, Francisco Luque S´anchez, Siham Tabik, Luigi Troiano, Roberto Tagliaferri, and\nFrancisco Herrera. Deep learning in video multi-object tracking: A survey. Neurocomputing,\n381:61–88, 2020.\nAntonia Creswell, Rishabh Kabra, Chris Burgess, and Murray Shanahan. Unsupervised object-\nbased transition models for 3d partially observable environments. Advances in Neural Information\nProcessing Systems, 34:27344–27355, 2021.\nGamaleldin Elsayed, Aravindh Mahendran, Sjoerd van Steenkiste, Klaus Greff, Michael C Mozer,\nand Thomas Kipf. Savi++: Towards end-to-end object-centric learning from real-world videos.\nAdvances in Neural Information Processing Systems, 35:28940–28954, 2022.\nPatrick Emami, Pan He, Sanjay Ranka, and Anand Rangarajan. Efficient iterative amortized in-\nference for learning symmetric and disentangled multi-object representations. In International\nConference on Machine Learning, pp. 2970–2981. PMLR, 2021.\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object\nrepresentations without iterative refinement. Advances in Neural Information Processing Systems,\n34:8085–8094, 2021.\nSM Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, Geoffrey E Hinton,\net al. Attend, infer, repeat: Fast scene understanding with generative models. Advances in neural\ninformation processing systems, 29, 2016.\nRohit Girdhar and Deva Ramanan. Cater: A diagnostic dataset for compositional actions and tem-\nporal reasoning. arXiv preprint arXiv:1910.04744, 2019.\nKlaus Greff, Rapha¨el Lopez Kaufman, Rishabh Kabra, Nick Watters, Christopher Burgess, Daniel\nZoran, Loic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation\nlearning with iterative variational inference. In International Conference on Machine Learning,\npp. 2424–2433. PMLR, 2019.\nKlaus Greff, Sjoerd Van Steenkiste, and J¨urgen Schmidhuber. On the binding problem in artificial\nneural networks. arXiv preprint arXiv:2012.05208, 2020.\nKlaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David J\nFleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, Thomas Kipf, Abhijit Kundu,\nDmitry Lagun, Issam Laradji, Hsueh-Ti (Derek) Liu, Henning Meyer, Yishu Miao, Derek\nNowrouzezahrai, Cengiz Oztireli, Etienne Pot, Noha Radwan, Daniel Rebain, Sara Sabour, Mehdi\nS. M. Sajjadi, Matan Sela, Vincent Sitzmann, Austin Stone, Deqing Sun, Suhani Vora, Ziyu Wang,\n10\nPublished as a conference paper at ICLR 2024\nTianhao Wu, Kwang Moo Yi, Fangcheng Zhong, and Andrea Tagliasacchi. Kubric: a scalable\ndataset generator. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2022.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with dis-\ncrete world models. arXiv preprint arXiv:2010.02193, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770–778, 2016.\nZhen He, Jian Li, Daxue Liu, Hangen He, and David Barber. Tracking by animation: Unsuper-\nvised learning of multi-object attentive trackers. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 1318–1327, 2019.\nOlivier J H´enaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisser-\nman, Jo˜ao Carreira, and Relja Arandjelovi´c. Object discovery and representation networks. In\nEuropean Conference on Computer Vision, pp. 123–143. Springer, 2022.\nJindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world\nmodels with scalable object representations. arXiv preprint arXiv:1910.02384, 2019.\nRishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matt Botvinick,\nAlexander Lerchner, and Chris Burgess. Simone: View-invariant, temporally-abstracted object\nrepresentations via unsupervised video decomposition. Advances in Neural Information Process-\ning Systems, 34:20146–20159, 2021.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nThomas Kipf, Gamaleldin F Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg\nHeigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional object-centric\nlearning from video. arXiv preprint arXiv:2111.12594, 2021.\nAdam Kosiorek, Hyunjik Kim, Yee Whye Teh, and Ingmar Posner. Sequential attend, infer, repeat:\nGenerative modelling of moving objects. Advances in Neural Information Processing Systems,\n31, 2018.\nJannik Kossen, Karl Stelzner, Marcel Hussing, Claas Voelcker, and Kristian Kersting.\nStruc-\ntured object-aware physics prediction for video modeling and planning.\narXiv preprint\narXiv:1910.02425, 2019.\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot atten-\ntion. Advances in Neural Information Processing Systems, 33:11525–11538, 2020.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention–\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234–241. Springer, 2015.\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann\nSimon-Gabriel, Tong He, Zheng Zhang, Bernhard Sch¨olkopf, Thomas Brox, et al. Bridging the\ngap to real-world object-centric learning. arXiv preprint arXiv:2209.14860, 2022.\nGautam Singh, Fei Deng, and Sungjin Ahn. Illiterate dall-e learns to compose. In ICLR, 2022a.\n11\nPublished as a conference paper at ICLR 2024\nGautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for com-\nplex and naturalistic videos. Advances in Neural Information Processing Systems, 35:18181–\n18196, 2022b.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing in-\ngredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\nRishi Veerapaneni, John D Co-Reyes, Michael Chang, Michael Janner, Chelsea Finn, Jiajun Wu,\nJoshua Tenenbaum, and Sergey Levine. Entity abstraction in visual model-based reinforcement\nlearning. In Conference on Robot Learning, pp. 1439–1456. PMLR, 2020.\nMarissa A Weis, Kashyap Chitta, Yash Sharma, Wieland Brendel, Matthias Bethge, Andreas Geiger,\nand Alexander S Ecker. Benchmarking unsupervised object representations for video sequences.\nThe Journal of Machine Learning Research, 22(1):8253–8313, 2021.\nAndrii Zadaianchuk, Maximilian Seitzer, and Georg Martius. Object-centric learning for real-world\nvideos by predicting temporal feature similarities. arXiv preprint arXiv:2306.04829, 2023.\nTianfei Zhou, Fatih Porikli, David J Crandall, Luc Van Gool, and Wenguan Wang. A survey on deep\nlearning technique for video segmentation. IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 45(6):7099–7122, 2022.\nRoland S Zimmermann, Sjoerd van Steenkiste, Mehdi SM Sajjadi, Thomas Kipf, and Klaus Gr-\neff.\nSensitivity of slot-based object-centric models to their number of slots.\narXiv preprint\narXiv:2305.18890, 2023.\nDaniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised seg-\nmentation with slots, attention and independence maximization. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10439–10447, 2021.\n12\nPublished as a conference paper at ICLR 2024\nMethod\nImage FG-ARI\nImage mIoU\nMOVI-C\nMOVI-D\nMOVI-E\nMOVI-C\nMOVI-D\nMOVI-E\nMONet\n24.8±0.1\n22.0±1.7\n18.8±0.9\n34.8±0.9\n17.4±2.0\n13.6±1.6\nVONet\n47.0±2.8\n51.0±1.2\n50.3±3.6\n36.2±3.0\n28.5±0.6\n18.1±1.3\nTable 2: Results of VONet and MONet in the single-frame scenario.\n\u0003\u0003\u00037UDQVIRUPHU\n«\n\u0003\u00030/3\n«\n\u00033DUDOOHO\u0003DWWQ\u0003QHW\n\u00036ORW(QF\n\u0003511\n\u0003\u00030/3\n\u00036ORW(QF\n\u00033DUDOOHO\u0003DWWQ\u0003QHW\n«\n7LPH\u0003VWHSV\nFigure 9: Illustration of the unrolling process for VONet on a video sequence. Only slot k is depicted\nin the diagram, with the other slots following similar flows. Slot computations can be parallelized\nwithin each step, except when inter-slot interaction is required (e.g. initial slot transformer and\nparallel attention). For simplicity, we omit depicting the dependencies on the input frames xt.\nA\nAPPENDIX\nA.1\nSINGLE-FRAME SCENARIO\nWe compared VONet with MONet in the single-frame scenario, as MONet was designed for object\ndiscovery in individual images only. We configured the U-Net architecture of MONet to match\nthat of VONet, and set MONet’s β = 1 and γ = 2 after a brief exploration within a reasonable\nparameter range. VONet was modified to train with a sequence length of 1 and excluded training\nfrom past slot states. For the training and evaluation data, we utilized video frames from MOVI-\n{C,D,E}, treating them as independent images with no temporal relationship. FG-ARI and mIoU\nare calculated on individual frames, referred to as Image FG-ARI and Image mIoU. The results are\nshown in Table 2. They suggest that VONet is not only more efficient than MONet, but also more\ncompetent to discover objects from complex images.\nA.2\nMORE DETAILS OF VONET\nBackbone for image features. We train a backbone based on a small ResNet (He et al., 2016) to\nextract a feature map from each input frame xt. The backbone architecture includes five residual\nblocks, each employing a 3×3 kernel and a stride of 1, except for the first block which has a stride of\n2. All the blocks have an output channel number of 64. Following the ResNet, there is a concluding\nconvolution layer with a 1 × 1 kernel and a linear activation, with an output channel number of\n128. Finally, a position embedding network (Kipf et al., 2021) is applied to provide 2D location\ninformation. With this backbone, the output feature map has a spatial shape that is half of that of the\ninput frame. This feature map is shared by the parallel attention network and the slot encoder as the\ninput.\nU-Net and mask transformer. The U-Net used by the parallel attention network follows a standard\narchitecture closely resembling that of MONet (Burgess et al., 2019). There are M blocks on either\nthe upsampling or downsampling path, with M taking the value of 5 for datasets MOVI-{A,B} and\n6 for datasets MOVI-{C,D,E}. Each block comprises the following layers in order: (a) a 3 × 3 bias-\nfree convolution with a stride of 1 and a padding of 1, (b) an instance normalization (Ulyanov et al.,\n2016) layer with learnable affine parameters, (c) a ReLU activation, and finally (d) a max pooling\nlayer of size 2 for the downsampling path and a nearest upsampling layer of factor 2 for the upsam-\npling path. No max pooling (upsampling) is applied to the first (last) block of the downsampling\n(upsampling) path. A skip connection is established between the output of the m-th downsampling\n13\nPublished as a conference paper at ICLR 2024\nblock and the output of the (M − m + 1)-th upsampling block, for each 2 ≤ m ≤ M. The non-skip\nbottleneck connection is an MLP with two layers of sizes (512, 512), both activated with ReLU. The\noutput of this MLP will be projected and reshaped to match the output of the downsampling path,\nconcatenated with it, and input to the upsampling path. After the upsampling path, a 1 × 1 convolu-\ntion is applied with an output channels of 1. The final output is a 2D map that retains the same spatial\ndimensions as the input. The values within this map represent unnormalized attention logits. For\nthe downsampling blocks, we set their output channels as {32, 64, 64, 128, 128} for MOVI-{A,B},\nwith an additional output channel number of 128 for MOVI-{C,D,E}.\nTo prepare the input to the U-Net for a slot’s attention mask, we first convolve its context vector\nacross the backbone feature locations, yielding a roughly estimated attention mask for that slot.\nThen its context vector is spatially broadcast to the feature locations, forming a location-invariant\ncontext map. Finally, the backbone feature map, the estimated attention mask, and the context map,\nare concatenated in the channel dimension to form the input to the U-Net.\nWhen there are K slots whose attention masks are being computed in parallel, right after the U-Net\nbottleneck MLP, we use a mask transformer to model the interaction among the K masks in the\nlatent space. The transformer consists of three decoder-only transformer blocks, and each block can\nbe briefly described as:\nu′ = v + MLP(LayerNorm(v)),\nv = u + MultiHeadAttn(LayerNorm(u)).\nwhere u represents the K mask latent vectors and u′ are the updated latent vectors. We configure\neach block with three attention heads. To maintain permutation invariance for the slots, no position\nencoding is incorporated in any of the transformer blocks.\nInitial slot latent.\nThe initial slot latent transformer in Eq. 5 shares a similar architecture with\nthe U-Net mask transformer, differing only in terms of layer sizes. It also has three decoder-only\ntransformer blocks and three attention heads per block. We set both the per-trajectory slot latent size\nand the per-frame slot latent size to be 128.\nVAE prior and posterior.\nTo compute the prior, again we use a transformer (Eq. 7) that has a\nsimilar architecture with the U-Net mask transformer, except for different layer sizes. The trans-\nformer has only two blocks with three attention heads in each block. Its output r′\nt,k is independently\nprojected (via an MLP of one hidden layer of size 128) to generate the mean and log variance of a\nGaussian as the prior distribution. The posterior is obtained by projecting the updated per-trajectory\nslot latent rt,k to generate its mean and log variance with a similar MLP.\nDecoder.\nSingh et al. (2022a) identified the mixture of components decoder in MONet as a weak\ndecoder, pinpointing two primary limitations: the slot-decoding dilemma and pixel independence.\nThese limitations pose challenges to the encoder’s capacity to achieve effective scene decomposi-\ntion. As a remedy, they proposed using a more powerful transformer-based decoder (Chen et al.,\n2020) instead. This decoder attends to all slots simultaneously and decodes the image in an auto-\nregressive manner. At a high level, the decoder is formulated as:\nPTransDec(x|z1, . . . , zK) =\nY\nm\nP(x(m)|z1, . . . , zK, x(1), . . . , x(m − 1)).\n(11)\nHere, x(m) represents the m-th patch on the image in the row-major order. Singh et al. (2022b)\napplied this decoder to learning objects from complex videos and achieved a notable performance\nenhancement.\nWe directly reused the transformer decoder implementation from the official code2 of STEVE (Singh\net al., 2022b). For the discrete VAE model, we re-implemented our own, but closely following the\nofficial implementation. All the decoder-related hyperparameters used in our experiments kept the\nsame with those of STEVE.\nSegmentation mask. For FG-ARI and mIoU evaluation, an integer segmentation mask is generated\nby consolidating the K slot attention masks mt,k (Eq. 3), where each mask contains a floating-point\nvalue within the range of [0, 1] at each pixel location. This value represents the probability of the\npixel being assigned to the corresponding slot. If the maximum probability at the pixel is smaller\nthan 0.3 (no slot is confident enough), we assign that pixel to a special “null” slot. Otherwise, the\n2https://github.com/singhgautam/steve\n14\nPublished as a conference paper at ICLR 2024\npixel is assigned to the slot with the maximum probability. As a result, each segmentation mask\ncontains at most K + 1 distinct integer values, where K is the number of pre-allocated slots.\nTraining from replayed video segments.\nTraining VONet on entire videos is impractical due\nto high memory usage caused by the large computational graph unrolled over time. Traditional\nvideo object learning methods, like Jiang et al. (2019); Kipf et al. (2021); Singh et al. (2022b),\ntrain models on short video segments, assuming that each mini-batch is initialized with fresh slots.\nDuring testing, they expect models to generalize to longer videos. VONet is also trained on short\nvideo segments, but it stores past states in a replay buffer, where each state includes per-trajectory\nslot latents {rt,k}K\nk=1 and a boolean flag indicating the initial frame of a video. When sampling a\nvideo segment from the replay buffer, we initiate the training of VONet with the state of the first\nstep of that segment, and only reset the state if the flag is true. Despite potential state obsolescence,\nthis replay-based training approach proves effective if a small replay buffer length is used. Using\na replay buffer allows training short video segments from past slot states, while still preserving the\ni.i.d. assumption with a random replay strategy. Without it, online training from previous slot states\nrequires sequential training through entire videos, introducing temporal correlation in the training\ndata.\nDuring training, we create a replay buffer with a length of 10k frames and a width of 16 videos. For\neach gradient update, we first unroll the same copy of VONet on each of 16 videos for 2 steps, store\nthe unrolled states in the replay buffer, and randomly extract a mini-batch of size B with a length\nof 3 (in total 3B frames) from the buffer to compute the gradient. After the update, the unrolling\ncontinues until the videos are finished (after 24\n2 = 12 updates), when we randomly select another\n16 videos from the dataset to replace them. We set B = 32 for MOVI-{A,B,C} while B = 24 for\nMOVI-{D,E}.\nA.3\nHYPERPARAMETERS\nWe provide a brief description of the key hyperparameters used for VONet in our experiments. For\nthe complete set of hyperparameters, we encourage the reader to refer to our code 3 .\nThe input video frame xt is resized to 128 × 128. Unlike some prior methods, we did not perform\nany data augmentation on the frames. We set the lengths of all the following vectors to 128:\ni) Noise vector ϵk (Eq. 5);\nii) Per-frame slot latent yt,k (Eq. 4);\niii) Per-trajectory slot latent rt,k (Eq. 4);\niv) Context vector ct,k (Eq. 4);\nv) VAE posterior embedding zt,k (Eq. 6).\n(We also explored a smaller length of 64 for these vectors, but obtained slightly worse results.) For\nany transformer, its model size (d_model), key size (d_k), and value size (d_v) are all set to be\nequal to the query size, whereas its hidden size (d_ff) is twice the query size.\nWe rolled out 16 workers in parallel for collecting video data in the replay buffer. Each worker\nutilizes the most recent VONet model for inference on 2 consecutive video frames, stores the inputs\nand states in the replay buffer, pauses for the trainer to perform a gradient update, and then resumes\ndata collection for another 2 frames, following this iterative process. The replay buffer length was\nset to 10k, resulting in a maximum number of time steps 16 × 10k = 160k in the buffer.\nWe trained 11 slots for MOVI-{A,B,C} while 16 slots for MOVI-{D,E}, reflecting the increased\nmaximum number of objects in the latter two datasets. Accordingly, to ensure consistent GPU\nmemory consumption across all training runs, we employed a mini-batch size4 of 32 for MOVI-\n{A,B,C} and 24 for MOVI-{D,E}. We sampled video segments of length 3 for training on all\ndatasets. This training configuration ensures that any training job can be executed on a system\nequipped with 4 NVIDIA GeForce RTX 3090 GPUs or an equivalent amount of GPU memory.\nFor the optimization, we set the KL balancing coefficient κ (Eq. 10) to 0.7. The KL loss weight β\n(Eq. 8) was linearly increased from 0 to 20\nD in the first 50k updates, where D = 128 is the posterior\n3https://github.com/hnyu/vonet\n4In our paper, when we refer to a sample within a mini-batch, we are referring to a video segment. Therefore,\nwhen we specify a mini-batch size of N, it indicates that the batch consists of N video segments.\n15\nPublished as a conference paper at ICLR 2024\ndimension. We used the Adam optimizer (Kingma & Ba, 2014) with a learning rate schedule as\nfollows. The learning rate was increased linearly from 10−5 to 10−4 in the first 5k training updates,\nmaintained the value until 100k updates, and finally was decreased linearly back to 10−5 in another\n50k updates. Thus in total, we trained each job for 150k gradient updates. In each update step, we\nnormalized the global parameter norm to 0.1 if the norm exceeds this threshold.\nA.4\nBASELINE DETAILS\nAll baseline methods except SIMONe (reason explained below), were trained with 11 slots for\nMOVI-{A,B,C} and 16 slots for MOVI-{C,D}.\nAll baseline methods, with the exception of\nSCALOR, employed the same methodology for obtaining segmentation masks as was utilized in\nVONet. SCALOR, on the other hand, possesses its own robust method for deriving segmentation\nmasks from attention maps, and thus we did not replace it. Below are their detailed training settings.\nSCALOR. We used the official implementation of SCALOR at https://github.com/\nJindongJiang/SCALOR. We followed the suggestions in SAVI (Kipf et al., 2021) to config-\nure its hyperparameters, with several exceptions as follows. The training video segment length was\nreduced from 10 to 6 to reduce memory consumption. We used an 8×8 grid instead of the 4×4 grid\nin SAVI as we found the former produced much better results on MOVI-{A,B}. We used a learning\nrate of 10−4 to speed up the training convergence.\nViMON. We used the official implementation of ViMON at https://github.com/\necker-lab/object-centric-representation-benchmark. We made adjustments to\nits default hyperparameters to adapt its training to the MOVI datasets, by setting the VAE latent dim\nto 64 and the training video seq length to 6. We also explored different values for γ, but found the\ndefault value 2 is already good enough. All other hyperparameters remain unchanged.\nSAVI. We made several changes to the official implementation of SAVI at https://github.\ncom/google-research/slot-attention-video for our experiments. First, SAVI was\ntrained in an entirely unsupervised manner to reconstruct RGB video frames only, without the object\nbounding box conditioning in the first video frame. The slots were initialized as unit Gaussian noises\nas in VONet. Second, to mitigate GPU memory usage, we halved the original training batch size,\nreducing it from 64 to 32. We also reduced the training video segment length from 6 to 3 on MOVI-\n{D,E}. Remarkably, these size adjustments did not yield any discernible impact on the training\nresults according to our observations on several trial training jobs. Finally, we conducted training\nwith a medium-sized SAVI model, featuring a 9-layer ResNet as the encoder backbone which has\na comparable size to the backbone of VONet. Specifically, the ResNet architecture was created by\nassigning a size (number of residual blocks) of 1 to each of the four ResNet stages, employing the\nclass ResNetWithBasicBlk implemented by SAVI.\nSIMONe.\nWe used the re-implementation of SIMONe at https://gitlab.com/\ngenerally-intelligent/simone, which successfully reproduced the performance on\nCATER (Girdhar & Ramanan, 2019). In contrast to other baseline methods and VONet, SIMONe\nimposes a strict requirement on the number of learnable slots. This number must be equal to the\nproduct of the height and width of the feature map resulting from its CNN encoder and transformer.\nConsequently, we employed a fixed slot number of 16 across all MOVI datasets for SIMONe. We set\nthe mini-batch size to 40 and the video segment length to 24 (full length). All other hyperparameters\nremained unaltered.\nSTEVE. Since STEVE has been extensively evaluated on MOVI-{D,E} by its authors, we largely\nretained its original hyperparameters. However, we explored one exception: experimenting with\ntwo different values for its slot size: 64 and 128. Our observation revealed that a larger slot size of\n128 consistently yielded no better or even sometimes slightly inferior results across all five datasets.\nConsequently, we settled on a slot size of 64 for STEVE in our final configuration.\nA.5\nDATASET DETAILS\nWe refer the reader to the official MOVI datasets website for details: https://github.\ncom/google-research/kubric/tree/main/challenges/movi.\nWe did not make\nany change to the five datasets. The official training/validation splits were used in our experiments.\n16\nPublished as a conference paper at ICLR 2024\nA.6\nMORE VISUALIZATION RESULTS\nFigure 10: Additional segmentation results of VONet. In these results, each video is presented with\nevery other frame displayed. The boundaries of object segments are marked with white curves. No\npost-processing was performed on the masks.\n17\nPublished as a conference paper at ICLR 2024\nFigure 11: Additional segmentation results of VONet. In these results, each video is presented with\nevery other frame displayed. The boundaries of object segments are marked with white curves. No\npost-processing was performed on the masks.\n18\nPublished as a conference paper at ICLR 2024\nFigure 12: Additional segmentation results of VONet. In these results, each video is presented with\nevery other frame displayed. The boundaries of object segments are marked with white curves. No\npost-processing was performed on the masks.\n19\n"
}