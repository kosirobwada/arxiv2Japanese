{
    "optim": "DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for\nMusic-to-Dance Synthesis\nXin Gao, Li Hu, Peng Zhang, Bang Zhang, Liefeng Bo\nInstitute for Intelligent Computing, Alibaba Group\n{zimu.gx, hooks.hl, futian.zp, zhangbang.zb, liefeng.bo}@alibaba-inc.com\nhttps://humanaigc.github.io/dance-meld/\nDance poses.\nGenerated dance sequence by our method.\nDance movements.\nFigure 1. In the field of choreography, dance poses composed of a series of basic meaningful body postures, while dance movements can\nreflect trends, rhythm and energy of the motion. Our method uses a hierarchical VQ-VAE to decouple dance poses and dance movements\nby representing them with bottom code and top code (short line for bottom code and long line for top code).\nAbstract\nIn the realm of 3D digital human applications, music-\nto-dance presents a challenging task.\nGiven the one-to-\nmany relationship between music and dance, previous meth-\nods have been limited in their approach, relying solely on\nmatching and generating corresponding dance movements\nbased on music rhythm. In the professional field of choreog-\nraphy, a dance phrase consists of several dance poses and\ndance movements. Dance poses composed of a series of ba-\nsic meaningful body postures, while dance movements can\nreflect dynamic changes such as the rhythm, melody, and\nstyle of dance. Taking inspiration from these concepts, we\nintroduce an innovative dance generation pipeline called\nDanceMeld, which comprising two stages, i.e., the dance\ndecouple stage and the dance generation stage. In the de-\ncouple stage, a hierarchical VQ-VAE is used to disentangle\ndance poses and dance movements in different feature space\nlevels, where the bottom code represents dance poses, and\nthe top code represents dance movements. In the genera-\ntion stage, we utilize a diffusion model as a prior to model\nthe distribution and generate latent codes conditioned on\nmusic features. We have experimentally demonstrated the\nrepresentational capabilities of top code and bottom code,\nenabling the explicit decoupling expression of dance poses\nand dance movements. This disentanglement not only pro-\nvides control over motion details, styles, and rhythm but\nalso facilitates applications such as dance style transfer and\ndance unit editing. Our approach has undergone qualita-\ntive and quantitative experiments on the AIST++ dataset,\ndemonstrating its superiority over other methods.\n1. Introduction\nVirtual characters have been popular in various domains,\nincluding virtual idols, virtual customer service, and vir-\n1\narXiv:2401.10242v1  [cs.OH]  30 Nov 2023\ntual assistants. Driving virtual characters to dance with mu-\nsic is an important application within the gaming industry,\nfilm production and AR/VR experiences. However, creating\ndance animations demands professional expertise to match\nintricate movements with diverse music styles, rhythm, and\nmelodies. As a result, automated dance generation holds\nsubstantial promise, offering a straightforward and cost-\neffective means for individuals to produce and edit dance\nsequences for 3D characters, reducing the need for exten-\nsive manual effort during the production process. Music-\ndriven dance generation remains an ongoing research chal-\nlenge owing to the intricate one-to-many relationships that\nexist between music and dance. Recent studies have aimed\nto address this challenge using machine learning techniques\nsuch as Generative Adversarial Networks (GANs) [20, 22],\nautoregressive models [14, 17, 22, 23, 35, 41], and diffu-\nsion models [1, 6, 24, 38]. Unfortunately, most previous\nmusic-to-dance methods treat the task as a general human\nmotion generation task, mapping frame-level dance motion\ndirectly to sequential music, without taking choreography\nknowledge into account.\nTo tackle this issue, we refer to expert knowledge of\nprofessional choreography [3, 28]. Dance phrase, which\nexpresses the complete meaning of a smallest dance se-\nquence, serves as the basic unit in choreography. To create\na dance phrase, the choreographer should: 1) conceive sev-\neral meaningful dance poses, 2) connect these dance poses\nwith appropriate dance movements in terms of space, shape,\ntime, energy and emotion from music. Therefore, dance\nposes and dance movements are two key elements in profes-\nsional choreography. Despite drawing inspiration from the\nknowledge mentioned above, it is still challenge to model\ndance poses and dance movements. In real dance motions,\nposes and movements do not exhibit a straightforward one-\nto-one mapping, they are intricately intertwined, which rep-\nresent artistic combinations that are influenced by the dance\ngenre, style, and the rhythm and melody of the music, mak-\ning it difficult to disentangle. Recently, Bailando [33] pro-\nposes to establish a dancing pose unit dictionary to learn\na dancing-style subspace. However, it still needs an extra\nactor-critic learning with a reward function to align between\nmotion tempo and music beats.\nIn this paper, we introduce a novel dance generation\npipeline called DanceMeld that comprising two stages: the\ndance decouple stage and the dance generation stage. In the\ndance decouple stage, inspired by previous works in text-\nto-image synthesis, we employ a hierarchical Vector Quan-\ntized Variational Autoencoder (VQ-VAE) [31, 39] to model\ndance poses and dance movements separately in different\nfeature space levels, where the bottom latent code aims to\nindicates a certain reasonable dance poses, and the top la-\ntent code energizes the representation of movement pro-\ncess which capturing aspects such as orientation, tendency,\nspeed and beat of the motion. In the dance generation stage,\nwe leverage a diffusion model as a prior to generate the se-\nquence of latent codes based on the given music. These\npredicted code sequences are decoded into dance sequences\nusing a motion decoder. In summary, our contributions are\nas follows:\n‚Ä¢ We propose DanceMeld, a two-stage music-to-dance\nframework that decouple dance poses and dance move-\nments by representing them with bottom code and top\ncode by a hierarchical VQ-VAE, experiments demon-\nstrate the interpretability of the latent codes.\n‚Ä¢ A diffusion model is used as a prior to represent the dis-\ntribution of latent codes and generate dance sequences\nconditioned on music, which enables various applications\nsuch as dance style transfer and dance unit editing.\n‚Ä¢ Our approach has been validated both qualitatively and\nquantitatively on the AIST++ dataset compared to the\nstate-of-the-art methods, demonstrating its effectiveness\nand superiority.\n2. Related Works\n2.1. Music-to-Dance\nMusic-to-dance is a sub-task within Human Motion Syn-\nthesis: the generation of human body movements that align\nwith the characteristics of the music, such as its genre,\nrhythm, and intensity. A classic approach involves using\nGraph-based Motion Synthesis methods [4, 10], which seg-\nment the motions in the dataset into various nodes. Edges\nbetween nodes are added based on their similarity. When\nprovided with input music, this task can be regarded as a\nretrieval task, where the corresponding motion nodes are\nselected by matching the speed and amplitude of character\nmovements in motion segments to the rhythm and intensity\nof the music. However, Graph-based methods have their\nlimitations, this cutting-and-pasting approach lacks diver-\nsity. To address this issue, [48] uses auto-regressive gen-\nerative model to generate 3D dance motions with high re-\nalism and diversity, [22, 23, 29, 35, 42] use transformer-\nbased architecture to generate more diversified dance mo-\ntion that are well-attuned to the input music.\nHowever,\nauto-regressive models suffer from the issue of error accu-\nmulation, making it prone to freezing when generating long\nsequences of dance motions. [2] try to generates long-term\nsequences of human motions by forming a global structure\nthat respects a specific dance genre. [14] proposes a cur-\nriculum learning strategy to alleviate error accumulation of\nauto-regressive models in long motion sequence generation.\nHuman choreographers design dance motions from mu-\nsic by firstly devising multiple choreographic dance units\nand then arranging the dance unit sequence according to\nmusic‚Äôs rhythm, melody and emotion [3, 28]. Inspired by\nthis, [20, 41] decompose a dance sequence into a series of\n2\nBottom\nEncoder\nTop\nEncoder\nTop\nDecoder\nBottom\nDecoder\nmotion ùë•\nBottom\nDecoder\nmusic features\nDiffusion\nPrior\nDiffusion\nPrior\ntop codebook\ntop code ùëí!\nbottom code ùëí\"\nbottom codebook\nDecouple Stage: Training Hierarchical VQ-VAE\nGeneration Stage: Training Diffusion Prior\nC\nreconstruction \nmotion ùë•#\nC\n‚Ñé\"\n‚Ñé!\nC\nConcatenate Operation\n‚Ñé!\n‚Ñé\"!\ntop codebook\ntop code ùëí!\nbottom code ùëí\"\nbottom codebook\n‚Ñé\"!\nreconstruction \nmotion ùë•#\nC\n‚Ñé#\nQuantize\nQuantize\nQuantize\nQuantize\nnoise\nFigure 2. Our method comprises two stages: the dance decouple stage and the dance generation stage. In the dance decouple stage, a\nhierarchical VQ-VAE is trained to decouple dance pose and dance movement by representing them with bottom code eb and top code et.\nIn the motion generation phase, a diffusion model is employed as a prior to model the distribution of latent codes. The latent codes are then\ndecoded into dance sequences by a motion decoder.\nbasic dance units and then compose a dance by organiz-\ning multiple basic dancing movements seamlessly accord-\ning to the input music. [33] utilizes VQ-VAE to encode\nand quantize 3D pose sequences and then employs a trans-\nformer based architecture to combine these discrete codes\nand generate corresponding motions.\n2.2. Motion Diffusion Models\nHuman motion generation, learned from motion capture\ndata, can be conditioned by any signal that describes the\nmotion, such as text and audio.\nFor the text-to-motion\ntask, [13, 36] represent motions as images and utilize the\nCLIP model [30] to align texts and images in latent space,\nenabling texts to control the generation of motions. The\nsuccess of generative models in the text-to-image task has\nspurred the exploration of diffusion-based models in motion\ngeneration task. [5, 18, 37, 43‚Äì45] employ diffusion mod-\nels to address the text-to-motion task, while [38, 46] use\ndiffusion models to tackle the music-to-dance task. Given\nthat text, audio, and music all serve as conditions for motion\ngeneration task, [6, 11, 27, 47] address these sub-tasks si-\nmultaneously by designing a unified generative framework.\nOne of the challenges in motion generation is the problem\nof generating long sequences. [38] generates choreogra-\nphies of any length by imposing temporal constraints on\nbatches of sequences. [32] generates arbitrarily-long se-\nquences with text control per interval using a fixed motion\ndiffusion prior. [21] generates long-term 3D human motion\nfrom multiple action labels.\n2.3. Hierarchical VQ-VAE\nVQ-VAE [39] is a generative model that leverages vec-\ntor quantization to efficiently encode and decode high-\ndimensional data. The encoder learns a discrete latent rep-\nresentation, and the auto-regressive decoder is learnt as a\nprior to generate images. To obtain higher-quality images,\ngenerative models often employ the coarse to fine strategy\nto generate images progressively. For example, StyleGAN\n[15, 16] uses a synthesis network to progressively increase\nthe feature resolutions and recovering high-resolution im-\nages from sampled noise.\n[7, 31] introduce a hierarchi-\ncal VQ-VAE structure comprising multiple nested encoders\nand decoders. In the field of music generation, [8, 9] also\nutilize the hierarchical VQ-VAE to represent more abstract\nmusical features. A key feature of this kind of model is\nits use of multiple discrete codebooks during the encoding\nand decoding processes. The lower-level codebook captures\nfine-grained details, while the top-level codebook captures\nhigher-level semantic information.\n3. Method\n3.1. Overview\nIn the professional field of choreography, a dance phrase\nconsists of several dance poses and dance movements.\nDance poses composed of a series of basic meaningful\nbody postures, while dance movements can reflect dynamic\nchanges such as the rhythm, melody, and style of dance.\nDespite drawing inspiration from the knowledge mentioned\n3\nabove, it is still a challenge to model dance poses and dance\nmovements. In real dance motions, poses and movements\ndo not exhibit a straightforward one-to-one mapping, they\nare intricately intertwined and difficult to disentangle. We\ntry to decouple and represent dance poses and dance move-\nments explicitly by carefully selecting appropriate model\nstructures and designing loss functions.\nIn Sec.3.3, a hierarchical VQ-VAE architecture is used\nto disentangle and model dance poses and dance move-\nments in different feature space levels, with the bottom la-\ntent code representing dance poses and the top latent code\nrepresenting dance movements. The relevant experiments\nin Sec.4.3 demonstrated the interpretability of latent codes.\nIn Sec.3.4, we design a prior for modeling and sampling the\nlatent codes of dance poses and dance movements by using\na diffusion model. Music features are incorporated as con-\nditions, ensuring that the generated dance motions match\nthe music‚Äôs type, rhythm, and intensity. The latent codes\nare fed into a decoder to reconstruct the dance motion se-\nquence.\n3.2. Data Representation\nDance motions are represented by sequences of 3D hu-\nman joints based on the SMPL model [25], which includes\nroot‚Äôs global positions and 6D rotation representation of 24\njoints, resulting in a total dimension of x ‚àà RN√ów, where\nw ‚àà R3+24‚àó6=147 is the dimension of motion features and\nN is the sequence number of frames. The music features\nhm are extracted from the jukebox encoder [8], which is a\nlarge generative model trained on 1M songs and the features\nshould have good representational ability for different types\nof music.\n3.3. Hierarchical VQ-VAE\nThe hierarchical VQ-VAE structure comprising multiple\nnested encoders and decoders. The lower-level features cap-\nture fine-grained details, while the top-level features capture\nhigher-level semantic information. For motion, its semantic\nmanifestation occurs in the temporal domain. Short-term\nmotion represents specific postures, while the continuous\nvariation of motion over a longer duration can reflect the se-\nmantic information. We use the hierarchical VQ-VAE and\nextract representations from intermediate layers to represent\ndance poses and dance movements. During the encoding\nprocess, the dimension of features gradually increases to ex-\npress more abstract characteristics. The temporal dimension\ncompresses, enabling the representation of a broader tempo-\nral range. Therefore, the low-level latent codes are used to\nrepresent dance poses, while the top-level latent codes are\nemployed to represent dance movements.\nSpecifically, the motion input x passes through encoders\nEb and Et to obtain bottom feature hb and top feature ht,\nrespectively. The top code et is obtained by the Quantize\noperation [39], which are quantized vectors based on their\ndistances to the prototype vectors in the codebook. Sim-\nilarly, the bottom code eb is the quantized results of h\n‚Ä≤\nb,\nwhere h\n‚Ä≤\nb is the concatenate results of hb and the result af-\nter passing through decoder Dt for et. eb and et are then\nconcatenated and fed into the decoder Db to get the recon-\nstructed result ÀÜx:\nhb = Eb(x), ht = Et(hb)\n(1)\net = Quantize(ht), h\n‚Ä≤\nb = Concat(Dt(et), hb)\n(2)\neb = Quantize(h\n‚Ä≤\nb), ÀÜx = Db(Concat(et, eb))\n(3)\nThe whole pipeline is shown in Fig.2. For training the hier-\narchical VQ-VAE, the reconstruction loss with the commit\nlosses are employeed:\nLV Q(x, ÀÜx) = ‚à•sg[h\n‚Ä≤\nb] ‚àí eb‚à•2\n2 + Œ±‚à•sg[eb] ‚àí h\n‚Ä≤\nb‚à•2\n2\n+ ‚à•sg[ht] ‚àí et‚à•2\n2 + Œ≤‚à•sg[et] ‚àí ht‚à•2\n2 + ‚à•x ‚àí ÀÜx‚à•2\n2\n(4)\nIn addition to using L2 loss for motion reconstruction to\ntrain encoders, the corresponding codebooks for the two\nVQ-VAE networks are also trained simultaneously. And\ntwo commit losses are used to ensure that the embedding\nresults of the encoders do not grow. Here the operator sg\nrefers to the stop-gradient operation that blocks gradients\nfrom flowing into its argument.\nTo improve physical realism in the absence of true sim-\nulation, we use four auxiliary losses: joint positions (Eq.5),\nvelocities (Eq.6), accelerates (Eq.7) and contact (Eq.8)\nlosses:\nLpos = ‚à•FK(x) ‚àí FK(ÀÜx)‚à•2\n2\n(5)\nLvel =\n1\nN ‚àí 1\nN‚àí1\nX\ni=1\n‚à•(xi+1 ‚àí xi) ‚àí (ÀÜxi+1 ‚àí ÀÜxi)‚à•2\n2\n(6)\nLacc =\n1\nN ‚àí 1\nN‚àí1\nX\ni=1\n‚à•(vi+1 ‚àí vi) ‚àí (ÀÜvi+1 ‚àí ÀÜvi)‚à•2\n2\n(7)\nLcontact =\n1\nN ‚àí 1\nN‚àí1\nX\ni=1\n‚à•(FK(ÀÜxi+1) ‚àí FK(ÀÜxi)).bi‚à•2\n2\n(8)\nwhere FK(¬∑) denotes the forward kinematic function which\nconverts joint angles into joint positions,v = xi+1‚àíxi, and\nbi is the binary foot contact label of the pose at each frame\ni. The overall auxiliary loss is as follows:\nLaux = Lpos + Œ≥Lvel + œïLacc + œàLcontact\n(9)\nIn addition, we also use a modality alignment loss LMA\nto align top code et which represents dance movements to\nhigh-level music features hm:\nLMA = ‚à•et ‚àí hm‚à•2\n2\n(10)\n4\nFinal training loss is the weighted sum of the above losses:\nL = LV Q + ŒªauxLaux + ŒªMALMA\n(11)\n3.4. Diffusion Model as Prior\nThe next step is to learn a prior over the latent codes, thus\nmotions can be generated from sampled latent codes con-\nditioned with music features. Recently, the success of gen-\nerative models in the text-to-image task has spurred the ex-\nploration of diffusion-based models in motion generation\ntask. Diffusion models can learn the distribution of the la-\ntent space, aligning similar textual or music conditions with\ncorresponding similar motion sequences. In addition to this,\nit has advantages in long-term dependency modeling and\nexhibits superior generation diversity. Based on the above\nanalysis of its advantages, we consider the transform based\ndiffusion model as the prior. Two choices can be chosen,\nwhether to use VQ-Diffusion [12] to model the distribution\nof the discrete latent codes p(eb, et|hm), or use traditional\ngaussian diffusion to model the distribution of the continu-\nous latent features p(h\n‚Ä≤\nb, ht|hm) while preserving the code-\nbooks. We‚Äôve tried both and opt for the latter because it\nyields better results.\nFor\nthe\nconditional\njoint\nprobability\ndistribu-\ntion\np(h\n‚Ä≤\nb, ht|hm),\nit\ncan\nbe\nfurther\nexpressed\nas\np(ht|hm)p(h\n‚Ä≤\nb|ht, hm).\nTherefore, we can use two\ndiffusion models to separately model the conditional\ndistribution p(ht|hm) and p(h\n‚Ä≤\nb|ht, hm), as shown in\nFig.3(a). Another approach is to directly predict the joint\nprobability distribution p(h\n‚Ä≤\nb, ht|hm) by concatenating ht\nand h‚Ä≤\nb, and then using a single diffusion model to model\nthe joint probability distribution, as shown in Fig.3(b). For\ndetailed comparative experiments and results, please refer\nto Sec.4.2. Here we choose the latter for better clarification.\nWe jointly model h\n‚Ä≤\nb and ht by concatenating them:\nh = Concat(h\n‚Ä≤\nb, ht), and follow the DDPM definition of\ndiffusion as a Markov noising process, {h(t)}\nT\nt=0, where\nh(0) is drawn from the data distributon and the forward\nprocess is defined as:\nq(h(t)|h(t‚àí1)) ‚àº N(\np\n1 ‚àí Œ≤th(t‚àí1), Œ≤tI)\n(12)\nwhere Œ≤t ‚àà (0, 1) and I is the standard normal distribu-\ntion. To approximate the distribution p(h(0)|hm), a trans-\nformer based neural network G is used to parameterize\nthe chained reverse diffusion process pG(h(t‚àí1)|h(t), hm)\nby directly predicting the clean sequence h(0) as ÀÜh(0) =\nG(h(t), t, hm). The generator G is optimized by the L2\nloss:\nL = Eh(0),t[‚à•h(0) ‚àí ÀÜh(0))‚à•]2\n2\n(13)\nIn the inference stage, given the extracted music fea-\nture hm as condition, we use DDIM to sample and grad-\nually denoise h(t) from noise, for timestep t ‚àí 1: ÀÜh(t‚àí1) ‚àº\n(b)\nDiffusion\nPrior-M\n‚Ñé\"!\n‚Ñé\"\"\n#\n‚Ñé$\n(a)\nDiffusion\nPrior-T\nDiffusion\nPrior-B\n‚Ñé\"!\n‚Ñé\"\"\n#\nnoise\nnoise\nnoise\n‚Ñé$\nFigure 3. Different prior model arthitecture. (a) Separatel model\np(ht|hm) and p(h\n‚Ä≤\nb|ht, hm).\n(b) Predict the joint probability\np(h\n‚Ä≤\nb, ht|hm) by concatenating ht and h‚Ä≤\nb.\nq(G(h(t), t, hm), t ‚àí 1). After predicting ÀÜh(0), the final\nmotion sequence can be obtained by the motion decoder.\n4. Experiments\nDataset.\nWe perform the training and evaluation on\nAIST++ dataset [23], which consists of 1,408 sequences of\n3D human dance motion and are paired to music. The dura-\ntions vary from 7.4 sec. to 48.0 sec. We re-use the train/test\nsplits provided by the original dataset.\nImplementation details.\nWe conduct pre-processing to\nsegment the dance sequence to 512, the FPS is set at 60,\nthe sliding window is 40. We use the features from the 36th\nlayer of the Jukebox encoder, and a small encoder is em-\nployed to reduce the feature dimension to align with the top\ncode. Same as motion input, the music feature‚Äôs frame rate\nis also set at 60 FPS. We employed a hierachical VQ-VAE\nwith a total of 54M parameters. The downsample rate of\nthe bottom encoder Eb and the top encoder Et is 4 and 2,\nrespectively. The dictionary size of the bottom codebook is\nset to 512, and 128 for the top codebook. The feature di-\nmensions for both bottom code and top code is set to 512.\nThe hyperparameters Œ± = Œ≤ = 0.02, Œ≥ = œï = œà = 1,\nŒªaux = 1, ŒªMA = 0.1. Adam Optimizer [19] is used with\nlr = 1e‚àí4. The network is trained for 1000 epochs with\na batch size of 64. The diffusion process is implemented\nusing DDPM with 1000 steps, by directly predicting h(0)\nwith a transformer encoder. The encoder has 8 layers, 8\n5\nMethods\nMotion Quality\nMotion Diversity\nBeat Align.‚Üë\nPFC‚Üì\nUser Study\nFIDk ‚Üì\nFIDg ‚Üì\nDivk ‚Üí\nDivg ‚Üí\nOurs WinRate\nGround Truth\n17.10\n10.60\n9.48\n7.32\n0.24\n1.33\n58.8%\nFACT [23]\n35.35\n22.11\n5.94\n6.18\n0.22\n2.25\n92.5%\nBailando [33]\n28.16\n9.62\n7.83\n6.34\n0.23\n1.75\n86.3%\nEDGE [38]\n-\n-\n10.68\n7.62\n0.27\n1.65\n64.6%\nDanceMeld (Ours)\n22.74\n9.18\n8.74\n7.89\n0.28\n1.72\nN/A\nTable 1. Numerical metrics of FACT [23], Bailando [33], EDGE [38], and our method on AIST++ dataset. ‚Üì indicates lower is better, ‚Üí\nindicates closer to ground truth is better. Our method outperforms other approaches in terms of generated quality, as evidenced by the FID\nmetric and validated by the User Study. In terms of diversity, our method remains on par with EDGE. Due to the incorporation of modality\nalignment loss and auxiliary losses, our method also exhibits advantages in terms of aligning with the beat of the music and PFC metrics.\nattention heads, and a latent dimension of 512. For train-\ning the transformer encoder, a learning rate of 4e‚àí4 is used\nwith the Adan optimizer [40] over 500 epochs. More details\nplease refer to the supplementary materials.\n4.1. Comparison to Existing Methods\nWe compare our method against three state-of-the-art ap-\nproaches: FACT [23], Bailando [33], and EDGE [38]. For\nevaluate metrics, we first extract kinetic feature and geo-\nmetric feature, following the same steps as in [23].\nWe\nevaluate the generated motion quality by calculating the dis-\ntribution distance between the generated and the ground-\ntruth motions using Fr¬¥echet Inception Distance (FID) on the\nextracted kinetic feature and geometric feature, FIDk and\nFIDg, respectively. For the motion diversity, we calculate\nthe average Euclidean distance in the feature space across\n40 generated motions on the AIST++ test set. The motion\ndiversity in the geometric feature space and in the kinetic\nfeature space are noted as Divk, Divg, respectively. In ad-\ndition, we also use Beat Alignment Score (BAS) [23] to\ncharacterize the alignment between the dance motions and\nthe musical rhythm. The Physical Foot Contact (PFC) score\n[38] is also introduced to evaluate physical plausibility. Just\nas discussed in [38], due to the lack of a proficient motion\nencoder to represent the motion features, the FID metrics\nmaybe unreliable as a measure of quality on this dataset.\nThus human evaluations are introduced as a supplementary\nmeasure to qualitatively evaluate from a visual perspective.\nWe hired 20 participants on an outsourcing platform for the\nUser Study. We randomly selected 100 segments, each last-\ning 10 seconds, and had each participant make judgments.\nEach sample had results from our method and a randomly\nselected comparison method (including ground truth), both\ngenerated using the same music. Participants were asked\nto determine which dance result exhibited better generation\nquality and better alignment with the music.\nThe quantitative results are as shown in Tab. 1. Quantita-\ntively, our method outperforms other approaches in terms of\ngenerated quality, as evidenced by the FID metric and vali-\ndated by the User Study. In terms of diversity, our method\nMethods\nMotion Quality\nMotion Diversity\nFIDk ‚Üì\nFIDg ‚Üì\nDivk ‚Üí\nDivg ‚Üí\nOurs (Best)\n22.74\n9.18\n8.74\n7.89\nVQ-Diffusion\n35.35\n22.11\n5.94\n6.18\nSingle VQ-VAE\n25.48\n11.75\n7.96\n6.12\nPrior-B+T\n28.16\n9.62\n7.83\n6.34\nTable 2. Ablation study on different network architectures. VQ-\nDiffusion predicts discrete latent codes distribution p(eb, et|hm).\nSingle VQ-VAE means not use hierarchical VQ-VAE. Prior-B+T\nrefers to the architecture that use two diffusion models.\nremains on par with EDGE, which also relies on diffusion-\nbased sampling. Due to the incorporation of modality align-\nment loss and auxiliary losses, our method also exhibits ad-\nvantages in terms of aligning with the rhythm of the music\nand maintaining the authenticity of footstep movements.\n4.2. Ablation Studies\nNetwork Selection.\nWe conduct relevant ablation studies\non the selection and design of network structures. Our op-\ntimal model utilizes a hierarchical VQ-VAE architecture,\nemploying the traditional gaussian diffusion to directly pre-\ndict the joint probability distribution p(h\n‚Ä≤\nb, ht|hm). When\nusing VQ-Diffusion to predict the discrete probability dis-\ntribution p(eb, et|hm), we observed a significant decrease\nin the metrics. We suspect that the mask-and-replace dif-\nfusion strategy on VQ-Diffusion still has some limitations\ncompared to gaussian diffusion process. Next, we replace\nthe hierarchical VQ-VAE model with a single VQ-VAE\nmodel and also observed some decrease in the metrics. Our\nthird comparative experiment involves using two diffusion\nmodels to predict conditional probability distributions for\np(ht|hm) and p(h\n‚Ä≤\nb|ht, hm), as shown in Fig.3(b), the re-\nsults were slightly inferior compared to directly predicting\nthe joint probability distribution. The corresponding results\nare shown in Tab. 2.\n6\nMethods\nMotion Quality\nBeat Align.‚Üë\nPFC‚Üì\nFIDk ‚Üì\nFIDg ‚Üì\nBaseline\n22.74\n9.18\n0.28\n1.72\nw/o Laux\n21.40\n13.63\n0.27\n2.16\nw/o LMA\n27.90\n8.91\n0.23\n1.84\nTable 3. The ablation study result of loss terms. The use of Laux\ncan improve physical realism and enhance temporal stability. The\nuse of LMA improves the BAS metric.\nLoss Terms.\nHere we discuss the impact of each loss term\non the generated results. As seen in Tab.3, when auxil-\niary losses are not used, the PFC metric significantly de-\nteriorates. Although FIDk shows a slight improvement, the\ngenerated motions exhibit minor fluctuations between time\nsteps. These results suggest that contact loss may contribute\nto maintaining the authenticity of footstep movements, and\nthe velocity and acceleration losses in the auxiliary losses\ncontribute to temporal stability.\nThe introduction of modality alignment loss enables the\nalignment of motion‚Äôs top code and music features in the\nhigh-level latent space, allowing the top code to better rep-\nresent music-related features, such as rhythm and melody.\nFrom Tab.3, it can be observed that when modality align-\nment loss is not used, the BAS metric significantly de-\ncreases. Additionally, we randomly selected some in the\nwild music and extracted beat information from both music\nand motion. The visual results in Fig.4 intuitively demon-\nstrate that the modality alignment loss enhances the align-\nment of music and motion in terms of rhythm.\ndance beats (w/o         )\ndance beats (with         )\nmusic beats\ndance velocity\ntime\nFigure 4.\nAfter using modality alignment loss, beats between\ndance motions and music are more synchronized.\n4.3. The Interpretability of Latent Codes\nIn our approach, bottom code can be explicitly represented\nas dance poses, while top code can characterize more ab-\nstract dance movements. We will validate the representa-\ntional capabilities of the bottom code and top code through\nseveral visual experiments.\nThe Interpretability of Bottom Code.\nWe design the fol-\nlowing experiment to validate that bottom code can repre-\nsent dance poses. By fixing the bottom code and changing\nFigure 5. We present dance poses for 12 distinct bottom codes,\ncombining each with a random selection of several top codes for\nvisualization. The results reveal that the poses are constrained\nwithin a fixed range.\nthe top code, we found that the generated motions were re-\nstrained within a fixed range. Fig.5 shows 12 examples,\neach example represents the overlapped visual results of a\nfixed bottom code with different top codes. It is evident\nthat the poses are restrained within a certain range. Differ-\nent bottom codes correspond to different dance poses with\nclear distinctions. More visual results please refer to the\nsupplementary materials.\n(a) Real dance motion\n(b) Same top codes as (a) with a fixed bottom code\nFigure 6. (a) A real dance motion sequence. (b) Replace bottom\ncode of (a) with a fixed value. Red arrows represent motion trends\n(dance movements). While the motion of (b) is constrained due to\nthe fixed bottom code, it still maintains the same dance movements\nas (a), e.g., opening legs in the second column and lifting hands\nhorizontally in the fifth column.\nThe Interpretability of Top Code.\nAs shown in Fig.6(a),\nwe first select a segment of dance motion sequence and dis-\nplay it in the time dimension. Next, we replace the bottom\ncode of this dance segment with a fixed value, as illustrated\nin Fig.6(b). Since the bottom code is replaced with a fixed\nvalue, the motion in Fig.6(b) is constrained within a certain\nrange. However, from the temporal perspective, the motion\ntrends (dance movements) of the two are consistent, e.g.,\nopening legs in the second column and lifting hands hor-\nizontally in the fifth column (red arrows represent motion\n7\nùëí!!\nùëí\"!\nùëí!!\nùëí\"\"\nùëí!\"\nùëí\"\"\nùëí!\"\nùëí\"!\nFigure 7. Results of transferring bottom code and top code. et1 and et2 represent two sets of top codes, eb1 and eb2 represent two sets of\nbottom codes. By modifying the top code (within the same row), the dance poses are retained, but the movements of the motion are altered.\nBy modifying the bottom code (within the same column), the specific dance poses change, but the motion movements preserved.\nInsert\nDelete\nExchange\nFigure 8. Generated dance can be edited by inserting, deleting,\nreplacing, or modifying the relative order of latent codes.\ntrends), which demonstrates that the top code can charac-\nterize dance movements. Please refer to the supplementary\nmaterials for more visual results.\n4.4. Applications\nThrough the above verification of the interpretability of top\ncode and bottom code, we‚Äôve confirmed that the bottom\ncode has the ability to represent dance poses, while the top\ncode can signify dance movements. Using a hierarchical\nVQ-VAE structure, we have successfully decoupled dance\npose and dance movement, enabling diverse applications.\nFor example, by transferring different top codes, we can\ncontrol the style, speed, and motion trends of the dance, as\nillustrated in Fig.7, et1 and et2 represent two sets of top\ncodes, eb1 and eb2 represent two sets of bottom codes. Dif-\nferent colors indicate different latent codes.By modifying\nthe top code (within the same row), the dance poses are\nretained, but the speed and movements of the motion are\naltered. By modifying the bottom code (within the same\ncolumn), the specific dance poses change, but the motion\ntrends and dance movements are preserved. Furthermore,\nwe can use this capability to achieve choreography editing.\nAs illustrated in Fig.8, since the latent code corresponds to\na dance phrase in a specific time segment, we can edit the\ngenerated dance by inserting, deleting, replacing, or modi-\nfying the relative order of latent codes.\n5. Limitations and Future Works\nGenerating long-term and temporally coherent dance se-\nquences remains a challenge. While our method can gen-\nerate dance sequences of any length by applying temporal\nconstraints to batches [38], there is still a need to explore\na way to model the global consistency like [2], reflecting\nthe concept of dance sections. In addition, the efficiency of\ndiffusion-based methods is relatively low, preventing real-\ntime generation. Consistency models[26, 34] may be a po-\ntential improvement to accelerate the generation process.\n6. Conclusions\nOur method draws inspiration from the domain of chore-\nography and introduces a two-stage generative framework\nnamed DanceMeld. In the decouple stage, a hierarchical\nVQ-VAE is trained to obtain the bottom code and top\ncode, representing dance pose and dance movement.\nIn\nthe generation stage, a diffusion model serves as a prior\nfor modeling and sampling latent codes.\nWe conduct\nseveral experiments to demonstrate the interpretability of\nbottom code and top code, demonstrating applications such\nas dance style transfer and dance editing.\nValidation on\nthe AIST++ dataset reveals state-of-the-art performance,\nsupported by both qualitative and quantitative results.\n8\nReferences\n[1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and\nGustav Eje Henter.\nListen, denoise, action! audio-driven\nmotion synthesis with diffusion models. ACM Transactions\non Graphics (TOG), 42(4):1‚Äì20, 2023. 2\n[2] Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman,\nDaniel Cohen-Or, Ariel Shamir, and Yiorgos Chrysanthou.\nRhythm is a dancer: Music-driven motion synthesis with\nglobal structure.\nIEEE Transactions on Visualization and\nComputer Graphics, 2022. 2, 8\n[3] Lynne Anne Blom and L Tarin Chaplin. The intimate act of\nchoreography. University of Pittsburgh Pre, 1982. 2\n[4] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan-\nChen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster:\nchoreography-oriented music-driven dance synthesis. ACM\nTransactions on Graphics (TOG), 40(4):1‚Äì13, 2021. 2\n[5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao\nChen, and Gang Yu. Executing your commands via motion\ndiffusion in latent space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 18000‚Äì18010, 2023. 3\n[6] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav\nGolyanik, and Christian Theobalt. Mofusion: A framework\nfor denoising-diffusion-based motion synthesis. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9760‚Äì9770, 2023. 2, 3\n[7] Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan.\nHierarchical autoregressive image models with auxiliary de-\ncoders. arXiv preprint arXiv:1903.04933, 2019. 3\n[8] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook\nKim, Alec Radford, and Ilya Sutskever. Jukebox: A gen-\nerative model for music. arXiv preprint arXiv:2005.00341,\n2020. 3, 4\n[9] Sander Dieleman, Aaron van den Oord, and Karen Si-\nmonyan. The challenge of realistic music generation: mod-\nelling raw audio at scale. Advances in neural information\nprocessing systems, 31, 2018. 3\n[10] Jibin Gao, Junfu Pu, Honglun Zhang, Ying Shan, and Wei-\nShi Zheng.\nPc-dance: Posture-controllable music-driven\ndance synthesis. In Proceedings of the 30th ACM Interna-\ntional Conference on Multimedia, pages 1261‚Äì1269, 2022.\n2\n[11] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zi-\nhang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang.\nTm2d: Bimodality driven 3d dance generation via music-text\nintegration. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 9942‚Äì9952, 2023. 3\n[12] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 10696‚Äì10706, 2022. 5\n[13] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang\nCai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. arXiv preprint\narXiv:2205.08535, 2022. 3\n[14] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang,\nand Daxin Jiang. Dance revolution: Long-term dance gen-\neration with music via curriculum learning. arXiv preprint\narXiv:2006.06119, 2020. 2\n[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401‚Äì4410, 2019. 3\n[16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 8110‚Äì8119, 2020. 3\n[17] Jinwoo Kim, Heeseok Oh, Seongjean Kim, Hoseok Tong,\nand Sanghoon Lee.\nA brand new dance partner: Music-\nconditioned pluralistic dancing controlled by multiple dance\ngenres.\nIn Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 3490‚Äì\n3500, 2022. 2\n[18] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free-\nform language-based motion synthesis & editing. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 8255‚Äì8263, 2023. 3\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 5\n[20] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun\nWang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz.\nDancing to music. Advances in neural information process-\ning systems, 32, 2019. 2\n[21] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Mul-\ntiact: Long-term 3d human motion generation from multiple\naction labels. In Proceedings of the AAAI Conference on Ar-\ntificial Intelligence, pages 1231‚Äì1239, 2023. 3\n[22] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance-\nformer: Music conditioned 3d dance generation with para-\nmetric motion transformer. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, pages 1272‚Äì1279, 2022. 2\n[23] Ruilong Li, Shan Yang, David A Ross, and Angjoo\nKanazawa. Ai choreographer: Music conditioned 3d dance\ngeneration with aist++. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 13401‚Äì\n13412, 2021. 2, 5, 6\n[24] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su,\nZeping Ren, Han Zhang, Yansong Tang, and Xiu Li.\nFinedance: A fine-grained choreography dataset for 3d full\nbody dance generation. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 10234‚Äì\n10243, 2023. 2\n[25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard\nPons-Moll, and Michael J Black. Smpl: A skinned multi-\nperson linear model. In Seminal Graphics Papers: Pushing\nthe Boundaries, Volume 2, pages 851‚Äì866. 2023. 4\n[26] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang\nZhao.\nLatent consistency models:\nSynthesizing high-\nresolution images with few-step inference, 2023. 8\n9\n[27] Jianxin Ma, Shuai Bai, and Chang Zhou. Pretrained diffusion\nmodels for unified human motion synthesis. arXiv preprint\narXiv:2212.02837, 2022. 3\n[28] Sandra Cerny Minton. Choreography: a basic approach us-\ning improvisation. Human Kinetics, 2017. 2\n[29] Junfu Pu and Ying Shan.\nMusic-driven dance regenera-\ntion with controllable key pose constraints. arXiv preprint\narXiv:2207.03682, 2022. 2\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748‚Äì8763. PMLR, 2021. 3\n[31] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener-\nating diverse high-fidelity images with vq-vae-2. Advances\nin neural information processing systems, 32, 2019. 2, 3\n[32] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H\nBermano.\nHuman motion diffusion as a generative prior.\narXiv preprint arXiv:2303.01418, 2023. 3\n[33] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang,\nChen Qian, Chen Change Loy, and Ziwei Liu. Bailando:\n3d dance generation by actor-critic gpt with choreographic\nmemory. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 11050‚Äì\n11059, 2022. 2, 3, 6\n[34] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya\nSutskever. Consistency models. 2023. 8\n[35] Jiangxin Sun, Chunyu Wang, Huang Hu, Hanjiang Lai, Zhi\nJin, and Jian-Fang Hu.\nYou never stop dancing: Non-\nfreezing dance generation via bank-constrained manifold\nprojection. Advances in Neural Information Processing Sys-\ntems, 35:9995‚Äì10007, 2022. 2\n[36] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano,\nand Daniel Cohen-Or. Motionclip: Exposing human motion\ngeneration to clip space. In European Conference on Com-\nputer Vision, pages 358‚Äì374. Springer, 2022. 3\n[37] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir,\nDaniel Cohen-Or, and Amit H Bermano. Human motion dif-\nfusion model. arXiv preprint arXiv:2209.14916, 2022. 3\n[38] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge:\nEditable dance generation from music. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 448‚Äì458, 2023. 2, 3, 6, 8\n[39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 2, 3, 4\n[40] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and\nShuicheng Yan.\nAdan: Adaptive nesterov momentum al-\ngorithm for faster optimizing deep models. arXiv preprint\narXiv:2208.06677, 2022. 6\n[41] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo\nMeng, and Yanfeng Wang. Choreonet: Towards music to\ndance synthesis with choreographic action unit. In Proceed-\nings of the 28th ACM International Conference on Multime-\ndia, pages 744‚Äì752, 2020. 2\n[42] Wenjie Yin, Hang Yin, Kim Baraka, Danica Kragic, and\nMÀöarten Bj¬®orkman.\nDance style transfer with cross-modal\ntransformer. In Proceedings of the IEEE/CVF Winter Confer-\nence on Applications of Computer Vision, pages 5058‚Äì5067,\n2023. 2\n[43] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou\nHong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif-\nfuse: Text-driven human motion generation with diffusion\nmodel. arXiv preprint arXiv:2208.15001, 2022. 3\n[44] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai,\nFangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu.\nRemodiffuse: Retrieval-augmented motion diffusion model.\narXiv preprint arXiv:2304.01116, 2023.\n[45] Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and\nNicu Sebe. Modiff: Action-conditioned 3d motion gener-\nation with denoising diffusion probabilistic models. arXiv\npreprint arXiv:2301.03949, 2023. 3\n[46] Zhuoran Zhao, Jinbin Bai, Delong Chen, Debang Wang,\nand Yubo Pan.\nTaming diffusion models for music-\ndriven conducting motion generation.\narXiv preprint\narXiv:2306.10065, 2023. 3\n[47] Zixiang Zhou and Baoyuan Wang.\nUde: A unified driv-\ning engine for human motion generation. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 5632‚Äì5641, 2023. 3\n[48] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang\nWang, Ming Shao, and Siyu Xia. Music2dance: Dancenet\nfor music-driven dance generation. ACM Transactions on\nMultimedia Computing, Communications, and Applications\n(TOMM), 18(2):1‚Äì21, 2022. 2\n10\n"
}