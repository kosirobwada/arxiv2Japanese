{
    "optim": "DanceMeld: Unraveling Dance Phrases with Hierarchical Latent Codes for Music-to-Dance Synthesis Xin Gao, Li Hu, Peng Zhang, Bang Zhang, Liefeng Bo Institute for Intelligent Computing, Alibaba Group {zimu.gx, hooks.hl, futian.zp, zhangbang.zb, liefeng.bo}@alibaba-inc.com https://humanaigc.github.io/dance-meld/ Dance poses. Generated dance sequence by our method. Dance movements. Figure 1. In the field of choreography, dance poses composed of a series of basic meaningful body postures, while dance movements can reflect trends, rhythm and energy of the motion. Our method uses a hierarchical VQ-VAE to decouple dance poses and dance movements by representing them with bottom code and top code (short line for bottom code and long line for top code). Abstract In the realm of 3D digital human applications, music- to-dance presents a challenging task. Given the one-to- many relationship between music and dance, previous meth- ods have been limited in their approach, relying solely on matching and generating corresponding dance movements based on music rhythm. In the professional field of choreog- raphy, a dance phrase consists of several dance poses and dance movements. Dance poses composed of a series of ba- sic meaningful body postures, while dance movements can reflect dynamic changes such as the rhythm, melody, and style of dance. Taking inspiration from these concepts, we introduce an innovative dance generation pipeline called DanceMeld, which comprising two stages, i.e., the dance decouple stage and the dance generation stage. In the de- couple stage, a hierarchical VQ-VAE is used to disentangle dance poses and dance movements in different feature space levels, where the bottom code represents dance poses, and the top code represents dance movements. In the genera- tion stage, we utilize a diffusion model as a prior to model the distribution and generate latent codes conditioned on music features. We have experimentally demonstrated the representational capabilities of top code and bottom code, enabling the explicit decoupling expression of dance poses and dance movements. This disentanglement not only pro- vides control over motion details, styles, and rhythm but also facilitates applications such as dance style transfer and dance unit editing. Our approach has undergone qualita- tive and quantitative experiments on the AIST++ dataset, demonstrating its superiority over other methods. 1. Introduction Virtual characters have been popular in various domains, including virtual idols, virtual customer service, and vir- 1 arXiv:2401.10242v1  [cs.OH]  30 Nov 2023 tual assistants. Driving virtual characters to dance with mu- sic is an important application within the gaming industry, film production and AR/VR experiences. However, creating dance animations demands professional expertise to match intricate movements with diverse music styles, rhythm, and melodies. As a result, automated dance generation holds substantial promise, offering a straightforward and cost- effective means for individuals to produce and edit dance sequences for 3D characters, reducing the need for exten- sive manual effort during the production process. Music- driven dance generation remains an ongoing research chal- lenge owing to the intricate one-to-many relationships that exist between music and dance. Recent studies have aimed to address this challenge using machine learning techniques such as Generative Adversarial Networks (GANs) [20, 22], autoregressive models [14, 17, 22, 23, 35, 41], and diffu- sion models [1, 6, 24, 38]. Unfortunately, most previous music-to-dance methods treat the task as a general human motion generation task, mapping frame-level dance motion directly to sequential music, without taking choreography knowledge into account. To tackle this issue, we refer to expert knowledge of professional choreography [3, 28]. Dance phrase, which expresses the complete meaning of a smallest dance se- quence, serves as the basic unit in choreography. To create a dance phrase, the choreographer should: 1) conceive sev- eral meaningful dance poses, 2) connect these dance poses with appropriate dance movements in terms of space, shape, time, energy and emotion from music. Therefore, dance poses and dance movements are two key elements in profes- sional choreography. Despite drawing inspiration from the knowledge mentioned above, it is still challenge to model dance poses and dance movements. In real dance motions, poses and movements do not exhibit a straightforward one- to-one mapping, they are intricately intertwined, which rep- resent artistic combinations that are influenced by the dance genre, style, and the rhythm and melody of the music, mak- ing it difficult to disentangle. Recently, Bailando [33] pro- poses to establish a dancing pose unit dictionary to learn a dancing-style subspace. However, it still needs an extra actor-critic learning with a reward function to align between motion tempo and music beats. In this paper, we introduce a novel dance generation pipeline called DanceMeld that comprising two stages: the dance decouple stage and the dance generation stage. In the dance decouple stage, inspired by previous works in text- to-image synthesis, we employ a hierarchical Vector Quan- tized Variational Autoencoder (VQ-VAE) [31, 39] to model dance poses and dance movements separately in different feature space levels, where the bottom latent code aims to indicates a certain reasonable dance poses, and the top la- tent code energizes the representation of movement pro- cess which capturing aspects such as orientation, tendency, speed and beat of the motion. In the dance generation stage, we leverage a diffusion model as a prior to generate the se- quence of latent codes based on the given music. These predicted code sequences are decoded into dance sequences using a motion decoder. In summary, our contributions are as follows: ‚Ä¢ We propose DanceMeld, a two-stage music-to-dance framework that decouple dance poses and dance move- ments by representing them with bottom code and top code by a hierarchical VQ-VAE, experiments demon- strate the interpretability of the latent codes. ‚Ä¢ A diffusion model is used as a prior to represent the dis- tribution of latent codes and generate dance sequences conditioned on music, which enables various applications such as dance style transfer and dance unit editing. ‚Ä¢ Our approach has been validated both qualitatively and quantitatively on the AIST++ dataset compared to the state-of-the-art methods, demonstrating its effectiveness and superiority. 2. Related Works 2.1. Music-to-Dance Music-to-dance is a sub-task within Human Motion Syn- thesis: the generation of human body movements that align with the characteristics of the music, such as its genre, rhythm, and intensity. A classic approach involves using Graph-based Motion Synthesis methods [4, 10], which seg- ment the motions in the dataset into various nodes. Edges between nodes are added based on their similarity. When provided with input music, this task can be regarded as a retrieval task, where the corresponding motion nodes are selected by matching the speed and amplitude of character movements in motion segments to the rhythm and intensity of the music. However, Graph-based methods have their limitations, this cutting-and-pasting approach lacks diver- sity. To address this issue, [48] uses auto-regressive gen- erative model to generate 3D dance motions with high re- alism and diversity, [22, 23, 29, 35, 42] use transformer- based architecture to generate more diversified dance mo- tion that are well-attuned to the input music. However, auto-regressive models suffer from the issue of error accu- mulation, making it prone to freezing when generating long sequences of dance motions. [2] try to generates long-term sequences of human motions by forming a global structure that respects a specific dance genre. [14] proposes a cur- riculum learning strategy to alleviate error accumulation of auto-regressive models in long motion sequence generation. Human choreographers design dance motions from mu- sic by firstly devising multiple choreographic dance units and then arranging the dance unit sequence according to music‚Äôs rhythm, melody and emotion [3, 28]. Inspired by this, [20, 41] decompose a dance sequence into a series of 2 Bottom Encoder Top Encoder Top Decoder Bottom Decoder motion ùë• Bottom Decoder music features Diffusion Prior Diffusion Prior top codebook top code ùëí! bottom code ùëí\" bottom codebook Decouple Stage: Training Hierarchical VQ-VAE Generation Stage: Training Diffusion Prior C reconstruction  motion ùë•# C ‚Ñé\" ‚Ñé! C Concatenate Operation ‚Ñé! ‚Ñé\"! top codebook top code ùëí! bottom code ùëí\" bottom codebook ‚Ñé\"! reconstruction  motion ùë•# C ‚Ñé# Quantize Quantize Quantize Quantize noise Figure 2. Our method comprises two stages: the dance decouple stage and the dance generation stage. In the dance decouple stage, a hierarchical VQ-VAE is trained to decouple dance pose and dance movement by representing them with bottom code eb and top code et. In the motion generation phase, a diffusion model is employed as a prior to model the distribution of latent codes. The latent codes are then decoded into dance sequences by a motion decoder. basic dance units and then compose a dance by organiz- ing multiple basic dancing movements seamlessly accord- ing to the input music. [33] utilizes VQ-VAE to encode and quantize 3D pose sequences and then employs a trans- former based architecture to combine these discrete codes and generate corresponding motions. 2.2. Motion Diffusion Models Human motion generation, learned from motion capture data, can be conditioned by any signal that describes the motion, such as text and audio. For the text-to-motion task, [13, 36] represent motions as images and utilize the CLIP model [30] to align texts and images in latent space, enabling texts to control the generation of motions. The success of generative models in the text-to-image task has spurred the exploration of diffusion-based models in motion generation task. [5, 18, 37, 43‚Äì45] employ diffusion mod- els to address the text-to-motion task, while [38, 46] use diffusion models to tackle the music-to-dance task. Given that text, audio, and music all serve as conditions for motion generation task, [6, 11, 27, 47] address these sub-tasks si- multaneously by designing a unified generative framework. One of the challenges in motion generation is the problem of generating long sequences. [38] generates choreogra- phies of any length by imposing temporal constraints on batches of sequences. [32] generates arbitrarily-long se- quences with text control per interval using a fixed motion diffusion prior. [21] generates long-term 3D human motion from multiple action labels. 2.3. Hierarchical VQ-VAE VQ-VAE [39] is a generative model that leverages vec- tor quantization to efficiently encode and decode high- dimensional data. The encoder learns a discrete latent rep- resentation, and the auto-regressive decoder is learnt as a prior to generate images. To obtain higher-quality images, generative models often employ the coarse to fine strategy to generate images progressively. For example, StyleGAN [15, 16] uses a synthesis network to progressively increase the feature resolutions and recovering high-resolution im- ages from sampled noise. [7, 31] introduce a hierarchi- cal VQ-VAE structure comprising multiple nested encoders and decoders. In the field of music generation, [8, 9] also utilize the hierarchical VQ-VAE to represent more abstract musical features. A key feature of this kind of model is its use of multiple discrete codebooks during the encoding and decoding processes. The lower-level codebook captures fine-grained details, while the top-level codebook captures higher-level semantic information. 3. Method 3.1. Overview In the professional field of choreography, a dance phrase consists of several dance poses and dance movements. Dance poses composed of a series of basic meaningful body postures, while dance movements can reflect dynamic changes such as the rhythm, melody, and style of dance. Despite drawing inspiration from the knowledge mentioned 3 above, it is still a challenge to model dance poses and dance movements. In real dance motions, poses and movements do not exhibit a straightforward one-to-one mapping, they are intricately intertwined and difficult to disentangle. We try to decouple and represent dance poses and dance move- ments explicitly by carefully selecting appropriate model structures and designing loss functions. In Sec.3.3, a hierarchical VQ-VAE architecture is used to disentangle and model dance poses and dance move- ments in different feature space levels, with the bottom la- tent code representing dance poses and the top latent code representing dance movements. The relevant experiments in Sec.4.3 demonstrated the interpretability of latent codes. In Sec.3.4, we design a prior for modeling and sampling the latent codes of dance poses and dance movements by using a diffusion model. Music features are incorporated as con- ditions, ensuring that the generated dance motions match the music‚Äôs type, rhythm, and intensity. The latent codes are fed into a decoder to reconstruct the dance motion se- quence. 3.2. Data Representation Dance motions are represented by sequences of 3D hu- man joints based on the SMPL model [25], which includes root‚Äôs global positions and 6D rotation representation of 24 joints, resulting in a total dimension of x ‚àà RN√ów, where w ‚àà R3+24‚àó6=147 is the dimension of motion features and N is the sequence number of frames. The music features hm are extracted from the jukebox encoder [8], which is a large generative model trained on 1M songs and the features should have good representational ability for different types of music. 3.3. Hierarchical VQ-VAE The hierarchical VQ-VAE structure comprising multiple nested encoders and decoders. The lower-level features cap- ture fine-grained details, while the top-level features capture higher-level semantic information. For motion, its semantic manifestation occurs in the temporal domain. Short-term motion represents specific postures, while the continuous variation of motion over a longer duration can reflect the se- mantic information. We use the hierarchical VQ-VAE and extract representations from intermediate layers to represent dance poses and dance movements. During the encoding process, the dimension of features gradually increases to ex- press more abstract characteristics. The temporal dimension compresses, enabling the representation of a broader tempo- ral range. Therefore, the low-level latent codes are used to represent dance poses, while the top-level latent codes are employed to represent dance movements. Specifically, the motion input x passes through encoders Eb and Et to obtain bottom feature hb and top feature ht, respectively. The top code et is obtained by the Quantize operation [39], which are quantized vectors based on their distances to the prototype vectors in the codebook. Sim- ilarly, the bottom code eb is the quantized results of h ‚Ä≤ b, where h ‚Ä≤ b is the concatenate results of hb and the result af- ter passing through decoder Dt for et. eb and et are then concatenated and fed into the decoder Db to get the recon- structed result ÀÜx: hb = Eb(x), ht = Et(hb) (1) et = Quantize(ht), h ‚Ä≤ b = Concat(Dt(et), hb) (2) eb = Quantize(h ‚Ä≤ b), ÀÜx = Db(Concat(et, eb)) (3) The whole pipeline is shown in Fig.2. For training the hier- archical VQ-VAE, the reconstruction loss with the commit losses are employeed: LV Q(x, ÀÜx) = ‚à•sg[h ‚Ä≤ b] ‚àí eb‚à•2 2 + Œ±‚à•sg[eb] ‚àí h ‚Ä≤ b‚à•2 2 + ‚à•sg[ht] ‚àí et‚à•2 2 + Œ≤‚à•sg[et] ‚àí ht‚à•2 2 + ‚à•x ‚àí ÀÜx‚à•2 2 (4) In addition to using L2 loss for motion reconstruction to train encoders, the corresponding codebooks for the two VQ-VAE networks are also trained simultaneously. And two commit losses are used to ensure that the embedding results of the encoders do not grow. Here the operator sg refers to the stop-gradient operation that blocks gradients from flowing into its argument. To improve physical realism in the absence of true sim- ulation, we use four auxiliary losses: joint positions (Eq.5), velocities (Eq.6), accelerates (Eq.7) and contact (Eq.8) losses: Lpos = ‚à•FK(x) ‚àí FK(ÀÜx)‚à•2 2 (5) Lvel = 1 N ‚àí 1 N‚àí1 X i=1 ‚à•(xi+1 ‚àí xi) ‚àí (ÀÜxi+1 ‚àí ÀÜxi)‚à•2 2 (6) Lacc = 1 N ‚àí 1 N‚àí1 X i=1 ‚à•(vi+1 ‚àí vi) ‚àí (ÀÜvi+1 ‚àí ÀÜvi)‚à•2 2 (7) Lcontact = 1 N ‚àí 1 N‚àí1 X i=1 ‚à•(FK(ÀÜxi+1) ‚àí FK(ÀÜxi)).bi‚à•2 2 (8) where FK(¬∑) denotes the forward kinematic function which converts joint angles into joint positions,v = xi+1‚àíxi, and bi is the binary foot contact label of the pose at each frame i. The overall auxiliary loss is as follows: Laux = Lpos + Œ≥Lvel + œïLacc + œàLcontact (9) In addition, we also use a modality alignment loss LMA to align top code et which represents dance movements to high-level music features hm: LMA = ‚à•et ‚àí hm‚à•2 2 (10) 4 Final training loss is the weighted sum of the above losses: L = LV Q + ŒªauxLaux + ŒªMALMA (11) 3.4. Diffusion Model as Prior The next step is to learn a prior over the latent codes, thus motions can be generated from sampled latent codes con- ditioned with music features. Recently, the success of gen- erative models in the text-to-image task has spurred the ex- ploration of diffusion-based models in motion generation task. Diffusion models can learn the distribution of the la- tent space, aligning similar textual or music conditions with corresponding similar motion sequences. In addition to this, it has advantages in long-term dependency modeling and exhibits superior generation diversity. Based on the above analysis of its advantages, we consider the transform based diffusion model as the prior. Two choices can be chosen, whether to use VQ-Diffusion [12] to model the distribution of the discrete latent codes p(eb, et|hm), or use traditional gaussian diffusion to model the distribution of the continu- ous latent features p(h ‚Ä≤ b, ht|hm) while preserving the code- books. We‚Äôve tried both and opt for the latter because it yields better results. For the conditional joint probability distribu- tion p(h ‚Ä≤ b, ht|hm), it can be further expressed as p(ht|hm)p(h ‚Ä≤ b|ht, hm). Therefore, we can use two diffusion models to separately model the conditional distribution p(ht|hm) and p(h ‚Ä≤ b|ht, hm), as shown in Fig.3(a). Another approach is to directly predict the joint probability distribution p(h ‚Ä≤ b, ht|hm) by concatenating ht and h‚Ä≤ b, and then using a single diffusion model to model the joint probability distribution, as shown in Fig.3(b). For detailed comparative experiments and results, please refer to Sec.4.2. Here we choose the latter for better clarification. We jointly model h ‚Ä≤ b and ht by concatenating them: h = Concat(h ‚Ä≤ b, ht), and follow the DDPM definition of diffusion as a Markov noising process, {h(t)} T t=0, where h(0) is drawn from the data distributon and the forward process is defined as: q(h(t)|h(t‚àí1)) ‚àº N( p 1 ‚àí Œ≤th(t‚àí1), Œ≤tI) (12) where Œ≤t ‚àà (0, 1) and I is the standard normal distribu- tion. To approximate the distribution p(h(0)|hm), a trans- former based neural network G is used to parameterize the chained reverse diffusion process pG(h(t‚àí1)|h(t), hm) by directly predicting the clean sequence h(0) as ÀÜh(0) = G(h(t), t, hm). The generator G is optimized by the L2 loss: L = Eh(0),t[‚à•h(0) ‚àí ÀÜh(0))‚à•]2 2 (13) In the inference stage, given the extracted music fea- ture hm as condition, we use DDIM to sample and grad- ually denoise h(t) from noise, for timestep t ‚àí 1: ÀÜh(t‚àí1) ‚àº (b) Diffusion Prior-M ‚Ñé\"! ‚Ñé\"\" # ‚Ñé$ (a) Diffusion Prior-T Diffusion Prior-B ‚Ñé\"! ‚Ñé\"\" # noise noise noise ‚Ñé$ Figure 3. Different prior model arthitecture. (a) Separatel model p(ht|hm) and p(h ‚Ä≤ b|ht, hm). (b) Predict the joint probability p(h ‚Ä≤ b, ht|hm) by concatenating ht and h‚Ä≤ b. q(G(h(t), t, hm), t ‚àí 1). After predicting ÀÜh(0), the final motion sequence can be obtained by the motion decoder. 4. Experiments Dataset. We perform the training and evaluation on AIST++ dataset [23], which consists of 1,408 sequences of 3D human dance motion and are paired to music. The dura- tions vary from 7.4 sec. to 48.0 sec. We re-use the train/test splits provided by the original dataset. Implementation details. We conduct pre-processing to segment the dance sequence to 512, the FPS is set at 60, the sliding window is 40. We use the features from the 36th layer of the Jukebox encoder, and a small encoder is em- ployed to reduce the feature dimension to align with the top code. Same as motion input, the music feature‚Äôs frame rate is also set at 60 FPS. We employed a hierachical VQ-VAE with a total of 54M parameters. The downsample rate of the bottom encoder Eb and the top encoder Et is 4 and 2, respectively. The dictionary size of the bottom codebook is set to 512, and 128 for the top codebook. The feature di- mensions for both bottom code and top code is set to 512. The hyperparameters Œ± = Œ≤ = 0.02, Œ≥ = œï = œà = 1, Œªaux = 1, ŒªMA = 0.1. Adam Optimizer [19] is used with lr = 1e‚àí4. The network is trained for 1000 epochs with a batch size of 64. The diffusion process is implemented using DDPM with 1000 steps, by directly predicting h(0) with a transformer encoder. The encoder has 8 layers, 8 5 Methods Motion Quality Motion Diversity Beat Align.‚Üë PFC‚Üì User Study FIDk ‚Üì FIDg ‚Üì Divk ‚Üí Divg ‚Üí Ours WinRate Ground Truth 17.10 10.60 9.48 7.32 0.24 1.33 58.8% FACT [23] 35.35 22.11 5.94 6.18 0.22 2.25 92.5% Bailando [33] 28.16 9.62 7.83 6.34 0.23 1.75 86.3% EDGE [38] - - 10.68 7.62 0.27 1.65 64.6% DanceMeld (Ours) 22.74 9.18 8.74 7.89 0.28 1.72 N/A Table 1. Numerical metrics of FACT [23], Bailando [33], EDGE [38], and our method on AIST++ dataset. ‚Üì indicates lower is better, ‚Üí indicates closer to ground truth is better. Our method outperforms other approaches in terms of generated quality, as evidenced by the FID metric and validated by the User Study. In terms of diversity, our method remains on par with EDGE. Due to the incorporation of modality alignment loss and auxiliary losses, our method also exhibits advantages in terms of aligning with the beat of the music and PFC metrics. attention heads, and a latent dimension of 512. For train- ing the transformer encoder, a learning rate of 4e‚àí4 is used with the Adan optimizer [40] over 500 epochs. More details please refer to the supplementary materials. 4.1. Comparison to Existing Methods We compare our method against three state-of-the-art ap- proaches: FACT [23], Bailando [33], and EDGE [38]. For evaluate metrics, we first extract kinetic feature and geo- metric feature, following the same steps as in [23]. We evaluate the generated motion quality by calculating the dis- tribution distance between the generated and the ground- truth motions using Fr¬¥echet Inception Distance (FID) on the extracted kinetic feature and geometric feature, FIDk and FIDg, respectively. For the motion diversity, we calculate the average Euclidean distance in the feature space across 40 generated motions on the AIST++ test set. The motion diversity in the geometric feature space and in the kinetic feature space are noted as Divk, Divg, respectively. In ad- dition, we also use Beat Alignment Score (BAS) [23] to characterize the alignment between the dance motions and the musical rhythm. The Physical Foot Contact (PFC) score [38] is also introduced to evaluate physical plausibility. Just as discussed in [38], due to the lack of a proficient motion encoder to represent the motion features, the FID metrics maybe unreliable as a measure of quality on this dataset. Thus human evaluations are introduced as a supplementary measure to qualitatively evaluate from a visual perspective. We hired 20 participants on an outsourcing platform for the User Study. We randomly selected 100 segments, each last- ing 10 seconds, and had each participant make judgments. Each sample had results from our method and a randomly selected comparison method (including ground truth), both generated using the same music. Participants were asked to determine which dance result exhibited better generation quality and better alignment with the music. The quantitative results are as shown in Tab. 1. Quantita- tively, our method outperforms other approaches in terms of generated quality, as evidenced by the FID metric and vali- dated by the User Study. In terms of diversity, our method Methods Motion Quality Motion Diversity FIDk ‚Üì FIDg ‚Üì Divk ‚Üí Divg ‚Üí Ours (Best) 22.74 9.18 8.74 7.89 VQ-Diffusion 35.35 22.11 5.94 6.18 Single VQ-VAE 25.48 11.75 7.96 6.12 Prior-B+T 28.16 9.62 7.83 6.34 Table 2. Ablation study on different network architectures. VQ- Diffusion predicts discrete latent codes distribution p(eb, et|hm). Single VQ-VAE means not use hierarchical VQ-VAE. Prior-B+T refers to the architecture that use two diffusion models. remains on par with EDGE, which also relies on diffusion- based sampling. Due to the incorporation of modality align- ment loss and auxiliary losses, our method also exhibits ad- vantages in terms of aligning with the rhythm of the music and maintaining the authenticity of footstep movements. 4.2. Ablation Studies Network Selection. We conduct relevant ablation studies on the selection and design of network structures. Our op- timal model utilizes a hierarchical VQ-VAE architecture, employing the traditional gaussian diffusion to directly pre- dict the joint probability distribution p(h ‚Ä≤ b, ht|hm). When using VQ-Diffusion to predict the discrete probability dis- tribution p(eb, et|hm), we observed a significant decrease in the metrics. We suspect that the mask-and-replace dif- fusion strategy on VQ-Diffusion still has some limitations compared to gaussian diffusion process. Next, we replace the hierarchical VQ-VAE model with a single VQ-VAE model and also observed some decrease in the metrics. Our third comparative experiment involves using two diffusion models to predict conditional probability distributions for p(ht|hm) and p(h ‚Ä≤ b|ht, hm), as shown in Fig.3(b), the re- sults were slightly inferior compared to directly predicting the joint probability distribution. The corresponding results are shown in Tab. 2. 6 Methods Motion Quality Beat Align.‚Üë PFC‚Üì FIDk ‚Üì FIDg ‚Üì Baseline 22.74 9.18 0.28 1.72 w/o Laux 21.40 13.63 0.27 2.16 w/o LMA 27.90 8.91 0.23 1.84 Table 3. The ablation study result of loss terms. The use of Laux can improve physical realism and enhance temporal stability. The use of LMA improves the BAS metric. Loss Terms. Here we discuss the impact of each loss term on the generated results. As seen in Tab.3, when auxil- iary losses are not used, the PFC metric significantly de- teriorates. Although FIDk shows a slight improvement, the generated motions exhibit minor fluctuations between time steps. These results suggest that contact loss may contribute to maintaining the authenticity of footstep movements, and the velocity and acceleration losses in the auxiliary losses contribute to temporal stability. The introduction of modality alignment loss enables the alignment of motion‚Äôs top code and music features in the high-level latent space, allowing the top code to better rep- resent music-related features, such as rhythm and melody. From Tab.3, it can be observed that when modality align- ment loss is not used, the BAS metric significantly de- creases. Additionally, we randomly selected some in the wild music and extracted beat information from both music and motion. The visual results in Fig.4 intuitively demon- strate that the modality alignment loss enhances the align- ment of music and motion in terms of rhythm. dance beats (w/o         ) dance beats (with         ) music beats dance velocity time Figure 4. After using modality alignment loss, beats between dance motions and music are more synchronized. 4.3. The Interpretability of Latent Codes In our approach, bottom code can be explicitly represented as dance poses, while top code can characterize more ab- stract dance movements. We will validate the representa- tional capabilities of the bottom code and top code through several visual experiments. The Interpretability of Bottom Code. We design the fol- lowing experiment to validate that bottom code can repre- sent dance poses. By fixing the bottom code and changing Figure 5. We present dance poses for 12 distinct bottom codes, combining each with a random selection of several top codes for visualization. The results reveal that the poses are constrained within a fixed range. the top code, we found that the generated motions were re- strained within a fixed range. Fig.5 shows 12 examples, each example represents the overlapped visual results of a fixed bottom code with different top codes. It is evident that the poses are restrained within a certain range. Differ- ent bottom codes correspond to different dance poses with clear distinctions. More visual results please refer to the supplementary materials. (a) Real dance motion (b) Same top codes as (a) with a fixed bottom code Figure 6. (a) A real dance motion sequence. (b) Replace bottom code of (a) with a fixed value. Red arrows represent motion trends (dance movements). While the motion of (b) is constrained due to the fixed bottom code, it still maintains the same dance movements as (a), e.g., opening legs in the second column and lifting hands horizontally in the fifth column. The Interpretability of Top Code. As shown in Fig.6(a), we first select a segment of dance motion sequence and dis- play it in the time dimension. Next, we replace the bottom code of this dance segment with a fixed value, as illustrated in Fig.6(b). Since the bottom code is replaced with a fixed value, the motion in Fig.6(b) is constrained within a certain range. However, from the temporal perspective, the motion trends (dance movements) of the two are consistent, e.g., opening legs in the second column and lifting hands hor- izontally in the fifth column (red arrows represent motion 7 ùëí!! ùëí\"! ùëí!! ùëí\"\" ùëí!\" ùëí\"\" ùëí!\" ùëí\"! Figure 7. Results of transferring bottom code and top code. et1 and et2 represent two sets of top codes, eb1 and eb2 represent two sets of bottom codes. By modifying the top code (within the same row), the dance poses are retained, but the movements of the motion are altered. By modifying the bottom code (within the same column), the specific dance poses change, but the motion movements preserved. Insert Delete Exchange Figure 8. Generated dance can be edited by inserting, deleting, replacing, or modifying the relative order of latent codes. trends), which demonstrates that the top code can charac- terize dance movements. Please refer to the supplementary materials for more visual results. 4.4. Applications Through the above verification of the interpretability of top code and bottom code, we‚Äôve confirmed that the bottom code has the ability to represent dance poses, while the top code can signify dance movements. Using a hierarchical VQ-VAE structure, we have successfully decoupled dance pose and dance movement, enabling diverse applications. For example, by transferring different top codes, we can control the style, speed, and motion trends of the dance, as illustrated in Fig.7, et1 and et2 represent two sets of top codes, eb1 and eb2 represent two sets of bottom codes. Dif- ferent colors indicate different latent codes.By modifying the top code (within the same row), the dance poses are retained, but the speed and movements of the motion are altered. By modifying the bottom code (within the same column), the specific dance poses change, but the motion trends and dance movements are preserved. Furthermore, we can use this capability to achieve choreography editing. As illustrated in Fig.8, since the latent code corresponds to a dance phrase in a specific time segment, we can edit the generated dance by inserting, deleting, replacing, or modi- fying the relative order of latent codes. 5. Limitations and Future Works Generating long-term and temporally coherent dance se- quences remains a challenge. While our method can gen- erate dance sequences of any length by applying temporal constraints to batches [38], there is still a need to explore a way to model the global consistency like [2], reflecting the concept of dance sections. In addition, the efficiency of diffusion-based methods is relatively low, preventing real- time generation. Consistency models[26, 34] may be a po- tential improvement to accelerate the generation process. 6. Conclusions Our method draws inspiration from the domain of chore- ography and introduces a two-stage generative framework named DanceMeld. In the decouple stage, a hierarchical VQ-VAE is trained to obtain the bottom code and top code, representing dance pose and dance movement. In the generation stage, a diffusion model serves as a prior for modeling and sampling latent codes. We conduct several experiments to demonstrate the interpretability of bottom code and top code, demonstrating applications such as dance style transfer and dance editing. Validation on the AIST++ dataset reveals state-of-the-art performance, supported by both qualitative and quantitative results. 8 References [1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG), 42(4):1‚Äì20, 2023. 2 [2] Andreas Aristidou, Anastasios Yiannakidis, Kfir Aberman, Daniel Cohen-Or, Ariel Shamir, and Yiorgos Chrysanthou. Rhythm is a dancer: Music-driven motion synthesis with global structure. IEEE Transactions on Visualization and Computer Graphics, 2022. 2, 8 [3] Lynne Anne Blom and L Tarin Chaplin. The intimate act of choreography. University of Pittsburgh Pre, 1982. 2 [4] Kang Chen, Zhipeng Tan, Jin Lei, Song-Hai Zhang, Yuan- Chen Guo, Weidong Zhang, and Shi-Min Hu. Choreomaster: choreography-oriented music-driven dance synthesis. ACM Transactions on Graphics (TOG), 40(4):1‚Äì13, 2021. 2 [5] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18000‚Äì18010, 2023. 3 [6] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: A framework for denoising-diffusion-based motion synthesis. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9760‚Äì9770, 2023. 2, 3 [7] Jeffrey De Fauw, Sander Dieleman, and Karen Simonyan. Hierarchical autoregressive image models with auxiliary de- coders. arXiv preprint arXiv:1903.04933, 2019. 3 [8] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: A gen- erative model for music. arXiv preprint arXiv:2005.00341, 2020. 3, 4 [9] Sander Dieleman, Aaron van den Oord, and Karen Si- monyan. The challenge of realistic music generation: mod- elling raw audio at scale. Advances in neural information processing systems, 31, 2018. 3 [10] Jibin Gao, Junfu Pu, Honglun Zhang, Ying Shan, and Wei- Shi Zheng. Pc-dance: Posture-controllable music-driven dance synthesis. In Proceedings of the 30th ACM Interna- tional Conference on Multimedia, pages 1261‚Äì1269, 2022. 2 [11] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zi- hang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9942‚Äì9952, 2023. 3 [12] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec- tor quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 10696‚Äì10706, 2022. 5 [13] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. Avatarclip: Zero-shot text- driven generation and animation of 3d avatars. arXiv preprint arXiv:2205.08535, 2022. 3 [14] Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, and Daxin Jiang. Dance revolution: Long-term dance gen- eration with music via curriculum learning. arXiv preprint arXiv:2006.06119, 2020. 2 [15] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 4401‚Äì4410, 2019. 3 [16] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improv- ing the image quality of stylegan. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8110‚Äì8119, 2020. 3 [17] Jinwoo Kim, Heeseok Oh, Seongjean Kim, Hoseok Tong, and Sanghoon Lee. A brand new dance partner: Music- conditioned pluralistic dancing controlled by multiple dance genres. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3490‚Äì 3500, 2022. 2 [18] Jihoon Kim, Jiseob Kim, and Sungjoon Choi. Flame: Free- form language-based motion synthesis & editing. In Pro- ceedings of the AAAI Conference on Artificial Intelligence, pages 8255‚Äì8263, 2023. 3 [19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 5 [20] Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz. Dancing to music. Advances in neural information process- ing systems, 32, 2019. 2 [21] Taeryung Lee, Gyeongsik Moon, and Kyoung Mu Lee. Mul- tiact: Long-term 3d human motion generation from multiple action labels. In Proceedings of the AAAI Conference on Ar- tificial Intelligence, pages 1231‚Äì1239, 2023. 3 [22] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Dance- former: Music conditioned 3d dance generation with para- metric motion transformer. In Proceedings of the AAAI Con- ference on Artificial Intelligence, pages 1272‚Äì1279, 2022. 2 [23] Ruilong Li, Shan Yang, David A Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision, pages 13401‚Äì 13412, 2021. 2, 5, 6 [24] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. Finedance: A fine-grained choreography dataset for 3d full body dance generation. In Proceedings of the IEEE/CVF In- ternational Conference on Computer Vision, pages 10234‚Äì 10243, 2023. 2 [25] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi- person linear model. In Seminal Graphics Papers: Pushing the Boundaries, Volume 2, pages 851‚Äì866. 2023. 4 [26] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao. Latent consistency models: Synthesizing high- resolution images with few-step inference, 2023. 8 9 [27] Jianxin Ma, Shuai Bai, and Chang Zhou. Pretrained diffusion models for unified human motion synthesis. arXiv preprint arXiv:2212.02837, 2022. 3 [28] Sandra Cerny Minton. Choreography: a basic approach us- ing improvisation. Human Kinetics, 2017. 2 [29] Junfu Pu and Ying Shan. Music-driven dance regenera- tion with controllable key pose constraints. arXiv preprint arXiv:2207.03682, 2022. 2 [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748‚Äì8763. PMLR, 2021. 3 [31] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Gener- ating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. 2, 3 [32] Yonatan Shafir, Guy Tevet, Roy Kapon, and Amit H Bermano. Human motion diffusion as a generative prior. arXiv preprint arXiv:2303.01418, 2023. 3 [33] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11050‚Äì 11059, 2022. 2, 3, 6 [34] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. 2023. 8 [35] Jiangxin Sun, Chunyu Wang, Huang Hu, Hanjiang Lai, Zhi Jin, and Jian-Fang Hu. You never stop dancing: Non- freezing dance generation via bank-constrained manifold projection. Advances in Neural Information Processing Sys- tems, 35:9995‚Äì10007, 2022. 2 [36] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Com- puter Vision, pages 358‚Äì374. Springer, 2022. 3 [37] Guy Tevet, Sigal Raab, Brian Gordon, Yonatan Shafir, Daniel Cohen-Or, and Amit H Bermano. Human motion dif- fusion model. arXiv preprint arXiv:2209.14916, 2022. 3 [38] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448‚Äì458, 2023. 2, 3, 6, 8 [39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information pro- cessing systems, 30, 2017. 2, 3, 4 [40] Xingyu Xie, Pan Zhou, Huan Li, Zhouchen Lin, and Shuicheng Yan. Adan: Adaptive nesterov momentum al- gorithm for faster optimizing deep models. arXiv preprint arXiv:2208.06677, 2022. 6 [41] Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, and Yanfeng Wang. Choreonet: Towards music to dance synthesis with choreographic action unit. In Proceed- ings of the 28th ACM International Conference on Multime- dia, pages 744‚Äì752, 2020. 2 [42] Wenjie Yin, Hang Yin, Kim Baraka, Danica Kragic, and MÀöarten Bj¬®orkman. Dance style transfer with cross-modal transformer. In Proceedings of the IEEE/CVF Winter Confer- ence on Applications of Computer Vision, pages 5058‚Äì5067, 2023. 2 [43] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondif- fuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 3 [44] Mingyuan Zhang, Xinying Guo, Liang Pan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei Yang, and Ziwei Liu. Remodiffuse: Retrieval-augmented motion diffusion model. arXiv preprint arXiv:2304.01116, 2023. [45] Mengyi Zhao, Mengyuan Liu, Bin Ren, Shuling Dai, and Nicu Sebe. Modiff: Action-conditioned 3d motion gener- ation with denoising diffusion probabilistic models. arXiv preprint arXiv:2301.03949, 2023. 3 [46] Zhuoran Zhao, Jinbin Bai, Delong Chen, Debang Wang, and Yubo Pan. Taming diffusion models for music- driven conducting motion generation. arXiv preprint arXiv:2306.10065, 2023. 3 [47] Zixiang Zhou and Baoyuan Wang. Ude: A unified driv- ing engine for human motion generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5632‚Äì5641, 2023. 3 [48] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet for music-driven dance generation. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(2):1‚Äì21, 2022. 2 10 "
}