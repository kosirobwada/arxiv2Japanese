{
    "optim": "Exploring descriptors for titanium microstructure via digital\nfingerprints from variational autoencoders\nMichael D. White1,∗, Gowtham Nimmal Haribabu1,3, Jeyapriya Thimukonda\nJegadeesan3, Bikramjit Basu3, Philip J. Withers1,2 and Chris P. Race2,4\n1Department of Materials, University of Manchester, Manchester, UK, M13 9PL\n2Henry Royce Institute, University of Manchester, Manchester, UK, M13 9PL\n3Materials Research Centre, Indian Institute of Science, Bangalore, India, 560012\n4Department of Materials Science and Engineering, University of Sheffield, Sheffield, UK, S1 3JD\n23rd January 2024\nAbstract\nMicrostructure is key to controlling and understanding the properties of metallic mater-\nials, but traditional approaches to describing microstructure capture only a small number\nof features. To enable data-centric approaches to materials discovery, to allow efficient stor-\nage of microstructural data and to assist in quality control in metals processing, we require\nmore complete descriptors of microstructure. The concept of microstructural fingerprinting,\nusing machine learning (ML) to develop quantitative, low-dimensional descriptors of micro-\nstructures, has recently attracted significant attention. However, it is difficult to interpret\nconclusions drawn by ML algorithms, which are often referred to as “black boxes”. For\nexample, convolutional neural networks (CNNs) can be trained to make predictions about\na material from a set of microstructural image data, but the feature space that is learned is\noften used uncritically and adopted without any validation.\nHere we explore the use of variational autoencoders (VAEs), comprising a pair of CNNs,\nwhich can be trained to produce microstructural fingerprints in a continuous latent space.\nThe VAE architecture also permits the reconstruction of images from fingerprints, allowing\nus to explore how key features of microstructure are encoded in the latent space of fin-\ngerprints. We develop a VAE architecture based on ResNet18 and train it on two classes\nof Ti-6Al-4V optical micrographs (bimodal and lamellar) as an example of an industrially\nimportant alloy where microstructural control is critical to performance. The latent/feature\nspace of fingerprints learned by the VAE is explored in several ways, including by supplying\ninterpolated and randomly perturbed fingerprints to the trained decoder and via dimension-\nality reduction to explore the distribution and correlation of microstructural features within\nthe latent space of fingerprints.\nWe show that the fingerprints generated via the trained VAE exhibit smooth, interpol-\nable behaviour with stability to local perturbations, supporting their suitability as general\n∗Corresponding author\nEmail address: michael.white-3@postgrad.manchester.ac.uk (M.D. White)\n1\narXiv:2401.11967v1  [cond-mat.mtrl-sci]  22 Jan 2024\npurpose descriptors for microstructure. We also show that key properties of the microstruc-\ntures (volume fraction and grain size) are strongly correlated with position in the encoded\nfeature space, supporting the use of VAE fingerprints for quantitative exploration of process-\nstructure-property relationships.\n2\n1\nIntroduction\nIn the field of metallurgy and materials science, process-structure-property (PSP) linkages are\ninstrumental in guiding material design for targeted applications [1, 2]. Despite the importance\nof PSP linkages, a rigorous mathematical framework is not currently available for systematic\nanalysis in this context [3]. The key problem is that, whilst compositional processing parameters\nand measured properties are inherently described by numbers, microstructure lacks a compre-\nhensive numerical descriptor. A central impediment is that the characteristic microstructural\nfeatures exhibit heterogeneity over a wide range of size, spatial, and temporal scales. From the\napplication standpoint, it is important to identify a subset of salient measures of internal struc-\nture that can be tracked through a material’s processing history, that capture the dominant\ninfluences on the targeted properties. In conventional microstructural analysis, some quantitat-\nive methods are used, but these generally rely on metrics applied to image data, such as grain\nsize, phase fraction and correlation coefficients. This can provide some crucial information, but\nthe metrics which are suitable in each case will depend on the morphology of the microstruc-\nture. For example, interlamellar spacing is useful to describe lamellar microstructures, but is\nredundant when considering an equiaxed microstructure. In any case, such measures embody\nonly a tiny fraction of the information contained in a microstructural image. A quantitative\ndescription of microstructure that is independent of morphology (a microstructural fingerprint)\nand embodies a full range of features would enable direct comparisons between microstructures\nwith different morphologies, and provide new methods for constructing PSP relationships [3, 4],\nor for quantifying the deviation of a given microstructure from some ideal standard in a quality\ncontrol procedure.\nIn the past two decades, reasonable progress has been achieved in the quantification and\nlow-dimensional representation of microstructures, over larger processing and material com-\npositional windows [5, 6, 7]. These advances have been possible with the use of concepts and\ntoolsets from data science and informatics [8, 9]. The first potential benefit is realised with\nthe use of automated feature engineering of the hierarchical material structures, e.g., estab-\nlishing low-dimensional representations that provide maximum value in capturing high-fidelity\nPSP linkages. A systematic and comprehensive quantification of the material structure is now\npossible, for example by performing statistical analysis of image features [10] or by combining\nthe physics-inspired framework of n-point spatial correlations (n-point statistics) with machine\nlearning approaches, such as principal component analysis (PCA) [11].\nA machine learning approach that has not yet been fully explored, in the context of mi-\ncrostructural fingerprinting, is variational autoencoders (VAEs), which comprise a pair of con-\nvolutional neural networks (CNNs), referred to individually as the encoder and the decoder.\nVAEs were first introduced by Kingma and Welling [12]. Initial applications focussed on gen-\nerating images of individual objects, particularly human faces, utilising datasets such as the\nCelebA dataset [13]. More recently, attention has shifted towards machine learning applications\nin materials science, such as quantification of microstructure and the development of new PSP\nrelationships. The key difference is that microstructural image data are such that the entire\nimage field contains potentially meaningful information, rather than an image of a foreground\nobject of interest and some arbitrary background. Attempts have been made to construct VAEs\n3\nfor texture embedding. One example is TextureVAE, which consists of a VGG19 encoder and\na decoder comprising 4 convolution blocks [14].\nThe network was tested on various micro-\nstructures and latent dimensions were artificially varied to visualise their effect on the resulting\nreconstructions. Further exploration of the encoded space has also been shown to carry the\npotential for material property prediction [15, 16]. Conversely, the ability to generate synthetic\nmicrostructures from specified properties was demonstrated [17]. Dimensionality reduction of\nthe encoded space has also been shown to provide meaningful visualisation of the space, enabling\nproperty prediction from microstructural image data [18] and identification of new processing\nroutes for target orientation distributions [19].\nHere, we employ a deep residual block VAE architecture, based on ResNet18 [20], to encode\noptical micrograph data from two classes of Ti-6Al-4V; a lamellar microstructure and a bimodal\nequiaxed microstructure. The encoded space is explored via paths and Gaussian permutations,\nas a tool for generating artificial microstructures. The latent space in further explored through\ndimensionality reduction via t-SNE, with analysis of morphological metric distributions across\nthe space. Support vector regression (SVR) is then applied to correlate fingerprints contained\nin the latent space with the same morphological metrics to quantify the trends visualised by\nthe t-SNE.\nOur titanium alloy dataset is representative of the microstructures of an important class\nof industrial materials in which control of microstructure is key to delivering the required in-\nservice performance and in which quality assurance of material (and hence microstructure) is\na critical part of manufacturing processes. Furthermore, the nature of the dataset allows us to\nevaluate the performance of the VAE with respect to several aspects common to a broad range\nof metallurgical challenges:\n1. the descriptors should transparently handle a range of microstructures (here we have\nwidely varying grain morphology);\n2. the space of descriptors (the latent space of the VAE) should be well-behaved, in the sense\nthat the fingerprints should vary smoothly with changes to the microstructure and vice\nversa;\n3. the representation in the latent space should be interpretable in terms of key features of\nthe microstructure (or properties of the material), which is to say that the fingerprints\nshould show statistical correlation with features and properties.\nWe explore these aspects of the VAE performance for our titanium dataset.\n2\nMaterials and Methods\n2.1\nDataset\nThe LightForm Ti-6Al-4V alloy bimodal/lamellar dataset (LFTi64BL) is an open access dataset,\ncurated within the LightForm group at the University of Manchester, and can be accessed via\nZenodo [21]. The dataset comprises 40 optical 8 bit micrographs of resolution 1292×968 pixels,\ncontaining equal numbers of bimodal equiaxed and lamellar microstructures. An example from\neach classification is shown in Figure 1.\n4\n(a) Bimodal\n(b) Lamellar\nFigure 1: Representative optical micrographs of Ti-6Al-4V from LFTi64BL dataset.\nThis dataset contains a relatively small number of images for machine learning purposes.\nTo expand the dataset, patches were extracted with random rotations and reflections applied.\nThe expanded LFTi64BL dataset contains 1,000 patches from each image, resulting in 40,000\npatches of resolution 256 × 256 pixels.\n2.2\nGreyscale Normalisation\nEach image was white balanced by clipping greyscale intensities outside the 90th percentile and\nremapping to the range [0, 1]. Figure 2 shows distributions of mean greyscale intensities across\nthe bimodal and lamellar datasets within LFTi64BL separately, before and after normalisation.\n(a) Bimodal\n(b) Lamellar\nFigure 2: Distribution of mean greyscale intensities across the LFTi64BL dataset before and\nafter normalisation, for bimdoal and lamellar microstructures separately.\nPrior to white-balancing, the variation in mean greyscale intensity is heavily influenced\nby fluctuations in lighting conditions during image capture. However, after white-balancing,\ngreyscale intensity is more normally distributed across the dataset. This is now indicative of\nthe distribution of phase fractions, with lower mean greyscale intensity corresponding to a higher\nvolume fraction of the β phase.\n5\n2.3\nVariational Autoencoders (VAEs)\nVariational autoencoders (VAEs) are a tool for encoding image data into a compressed format (or\nfingerprint) that preserves morphological features. They comprise a pair of convolutional neural\nnetworks (CNNs), referred to individually as the encoder and decoder. Fingerprints output by\nthe trained encoder can be fed into downstream tasks such as image classification and property\nprediction. Figure 3 provides a visual interpretation of the general VAE architecture.\nFigure 3: Schematic of general VAE architecture that takes an image x as input, encodes the\nimage into a fingerprint e(x) and is trained to reconstruct the input image with the mapping\nd(e(x)).\nSuppose we have an image, x ∈ Rm1×m2. The encoder takes the image x as an input and\ncomputes a compressed representation z ∈ RD, such that D ≪ m1m2, where z = e(x) and D is\na tunable parameter that denoted the dimensionality of the encoded space. The decoder then\ntakes the encoded vector z as an input and aims to reconstruct the input image, x. We denote\nthe reconstruction as ˆx = d(e(x)), where ˆx ∈ Rm1×m2. The encoded space learned is continuous\nand normally distributed.\nTo train a VAE, we must have a set of images, X = {x1, x2, . . . , xN}, where N is the total\nnumber of images in the dataset. The dataset is split into two subsets, Xtrain ∈ RNtrain×m1×m2\nand Xeval ∈ RNeval×m1×m2, where Xtrain is used to train the VAE, Xeval is used for evaluating\nthe VAE on unseen data, Ntrain + Neval = N, Xtrain ∪ Xeval ≡ X and Xtrain ∩ Xeval ≡ ∅. The\ntraining set is split into batches and fed into the VAE, one batch at a time. Ultimately, d(e(xi))\nfor xi ∈ Xtrain is computed and some loss function is used as a metric for assessing the encoded\nrepresentation and reconstruction quality. The loss function is then used as an input to an\noptimiser. The Adam optimiser [22] is currently the state of the art and is utilised throughout\nall models discussed herein. The optimiser updates weights and biases in both the encoder and\ndecoder from a single loss function after each batch operation. The model can then be evaluated\non Xeval to measure the potential for transfer learning, but metrics calculated on these images in\nthe evaluation set are not utilised for updating any weights or biases. Encoded representations,\ngenerated from the trained VAE on microstructural image data, can be considered as a signature,\n6\nor microstructural fingerprint (as described in [3]), and will be referred to as such throughout\nthis paper.\n2.4\nLoss Functions\nDuring VAE training, a loss function is periodically computed on the model, to determine\nperformance and provide the inputs for the optimiser to update the weights and biases in the\nnetworks, with the aim of minimising the loss function. This is typically a combination between\na measure in the spacial domain of the reconstruction accuracy and imposing a restriction on\nthe encoded space to be normally distributed [12]. To measure how normally distributed the\nencoded space is, the Kullback-Leibler (KL) divergence, DKL, can be determined between each\nencoded representation and the unit normal distribution N(0, 1) [23].\nThe reconstruction accuracy can be measured in several ways. A popular method is the\nmean squared error (MSE) [24], which is a distance metric in the spacial domain, given by\nMSE = 1\nm\nm\nX\nj=1\n(xj − ˆxj)2,\nwhere m = m1m2 is the total number of pixels in the image and the xj, ˆxj denote pixels in the\ninput and reconstruction, respectively.\nAnother metric that operates in the spacial domain is the binary cross-entropy (BCE),\ndenoted here as Lb, which is a measure of negative log likelihood and is given by\nLb = −\nm\nX\ni=1\nxi log ˆxi −\nm\nX\ni=1\n(1 − xi) log(1 − ˆxi).\nThe issue with minimising these loss functions is that they generally result in blurry re-\nconstructions. With the aim of minimising blur, a loss function on the frequency domain was\nproposed in [25]. This requires calculating the 2D Fourier transform of both the input and\nreconstruction. The spectral loss, Lf(x, ˆx), can then be given by the MSE between the 2D\nFourier transforms, i.e.,\nLf(x, ˆx) = 1\nm\nm\nX\nj=1\n\u0000(Im{F(x)j} − Im{F(ˆx)j})2 + (Re{F(x)j} − Re{F(ˆx)j})2\u0001\n,\nwhere F denotes the 2D FFT, Im{F} denotes the imaginary part of F and Re{F} the real\npart. The total loss function to be minimised by the optimiser is then given by\nL = αLb(x, ˆx) + (1 − α)Lf(x, ˆx),\n(1)\nwhere α ∈ [0, 1] is a tunable hyperparameter.\n2.5\nResNet18-VAE\nResNet [20] is a deep CNN composed of residual blocks and was initially proposed for classific-\nation of the ImageNet dataset [26], which is a dataset containing over 1 million natural images\nwith 1000 classifications. As such, the final layer of a standard ResNet is a 1000-dimensional\nfully connected layer, where the output from each node corresponds to a probability for each\n7\nImageNet classification. The depth of the network can be controlled by varying the number of\nlayers within each block and the total number of blocks. Here, we consider ResNet18, which\ncontains 8 residual blocks, each comprised of 2 convolution layers with subsequent ReLU ac-\ntivation and batch normalisation. Each block has the potential to be effectively skipped by\nsumming the input received at each block with the output from that block, after convolution.\nTo convert ResNet18 into an encoder, it was modified by replacing the average pool and\n1000-dimensional fully connected layers at the end of the network with a flattening of the final\nconvolution output. This was followed by a 512-dimensional fully connected layer with tanh\nactivation, which is then simultaneously fed into two separate fully connected layers. Each fully\nconnected layer consists of |z| neurons, where |z| is the specified dimension of the encoded space.\nThis is somewhat arbitrary, but 256 was used to generate the results presented in this paper.\nOne of these fully connected layers is utilised as a set of means, µ, whilst the other is treated\nas a set of standard deviations, σ. These are then combined into the output z as\nz = µ + exp(σN(0, 1)),\nwhere N(0, 1) denotes the standard normal distribution, with mean 0 and standard deviation\n1. This branching into µ and σ, with subsequent combination of the two, is what sets vari-\national autoencoders apart from standard autoencoders. The decoder then essentially mirrors\nthe encoder with transpose convolution layers replacing the standard convolution layers. The\narchitecture for this ResNet18 VAE is provided in Figure 4.\n2.6\nMorphological Analysis\nMorphological measurements were automated for the LFTi64BL dataset as metrics for correl-\nation with fingerprints produced by the VAE. These metrics can then also be plotted as colour\nmaps over across a dimensionality reduction of the fingerprint space to visualise how such fea-\ntures are distributed. Each metric requires the images to be binarised prior to measurement.\n2.6.1\nImage Binarisation\nBinarised images are required to compute certain metrics on the images, such as phase fraction\nand grain size.\nA high-pass Gaussian filter was applied to each image, in the Fourier do-\nmain, to normalise illumination across the images. The images were then binarised with Otsu’s\nthresholding method [27], before applying area closing to remove noise from all images and αs\nlaths from the bimodal images. Pixels corresponding to αp grains are assigned a label, whilst\nall other pixels are labelled as 0. Figure 5 provides representative examples of the resulting\nbinary images. The binarised lamellar microstructures are highly accurate, but there is some\nretention of αs laths in the bimodal microstructures due to connectivity with the αp grains.\n2.6.2\nPhase Fraction\nDue to the way in which the images were binarised (described in Section 2.6.1), the phase\nfraction is simply determined as the sum of all the pixel values in the binary image divided by\nthe total number of pixels in the image.\n8\n(a) Encoder\n(b) Decoder\nFigure 4: ResNet18 VAE architecture for the 256 × 256 inputs used in the present study.\n2.6.3\nLamellae Direction\nFor the lamellar images in the LFTi64BL dataset, another descriptor we can consider is the\ndominant direction, or orientation, of the lamellae, relative to the bottom edge of the image.\nThis can be quantified as the mean angle subtended between the elongation direction of each\nlamella and the bottom edge of the image. An erosion is applied on the binary image to isolate\noverlapping grains before each grain is labelled with a unique integer. Watershed segmentation\nis then applied with markers taken from the labelled image and the initial binary image as a\n9\n(a) Original lamellar\n(b) Binary lamellar\n(c) Original bimodal\n(d) Binary bimodal\nFigure 5: Representative optical micrograph images patches of Ti-6Al-4V before and after\nbinarisation.\nmask. Each grain in the watershed image is isolated and eigenvectors are computed for each\ngrain. The primary eigenvector describes the direction of each lamella. The angle between the\nprimary eigenvector and the bottom edge of the image is then calculated for each grain and\naveraged to provide the metric for lamellae direction of a given image. Figure 6 shows the\nerosion and watershed method applied to a representative micrograph and Figure 7 shows the\ncorresponding eigenvectors for an individual grain.\n(a)\nOriginal\nmicro-\ngraph\n(b) Binary with area\nclosing\n(c) Eroded and labelled\n(d)\nWatershed\nseg-\nmentation\nFigure 6: Erosion and watershed methods applied to a representative micrograph for grain\nisolation.\nFigure 7: Eigenvectors plotted on an individual lamella for direction measurement, with the\nprimary eigenvector shown in red and the secondary eigenvector plotted in blue.\n10\n2.6.4\nBimodal Grain Size\nAs a metric for quantifying the bimodal microstructures in LFTi64BL, average grain size meas-\nurements were automated following ASTM E1382-97 [28]. Random line scans are applied on\nthe binarised bimodal micrographs and peaks in the derivative of the profile along the line scans\nare used to detect grain edges. The distance in pixels between peaks in the derivative of the\nprofile line is then converted into a distance in µm.\n2.7\nSupport Vector Regression (SVR)\nOnce fingerprints have been extracted from the ResNet18 VAE, regression algorithms can be\ntrained to predict morphological metrics. Here, we use support vector regression (SVR), which\nis an extension to support vector machines for continuous variables [29].\nGaussian process\nregression is a popular alternative to SVR, but relies on relatively low-dimensional inputs com-\npared to the dimensionality of the ResNet18 VAE fingerprints. Fingerprints are randomly split\ninto a training set containing 90% of the fingerprints and a test set containing 10% for input\ninto the SVR. This is repeated 10 times with random splits to perform 10-fold cross-validation.\n2.8\nt-Stochastic Neighbour Embedding (t-SNE)\nThe encoded space learned is high-dimensional (256 dimensions for ResNet18-VAE architecture,\noutlined in Section 2.5). This makes it difficult to visualise the encoded space in its entirety.\nTo aid in visualising this high-dimensional space, we can perform dimensionality reduction\ndown to 2 or 3 dimensions, which allows us to plot the encoded space and to assess how the\nmicrostructures in our training set are distributed. Here, we consider t-stochastic neighbour\nembedding (t-SNE) [30].\nThe fingerprints are represented as a similarity matrix, S, where entries, Si,j, denote the\nprobability that zj is a nearest neighbour of zi. The aim is then to minimise the distance between\nSi,j and Sj,i [31]. Standard SNE utilises a Gaussian distribution to determine similarity between\nfingerprints, whereas t-SNE relies on the Student’s t distribution, hence its name. Due to the\nlonger tail of the t distribution, relative to a Gaussian distribution, the use of the t distribution\nresults in more separated embeddings and helps alleviate the crowding issue often encountered\nwith SNE [32]. Principal component analysis (PCA) is used to initialise the t-SNE.\n2.9\nTraversing the Encoded Space\nOne way in which the encoded space can be traversed is through construction of a specific path.\nHere, we consider a linear path. Two images, x1 and x2, are randomly selected and e(x1), e(x2)\nare computed. A linear path is then constructed from e(x1) to e(x2) in the encoded space,\naccording to the following equation.\nen(x1, x2) = e(x1) + n + 1\nN\n(e(x2)),\nn = 1, . . . , N,\nwhere en(x1, x2) denotes each fingerprint along the path and N is the number of steps along the\npath. Fingerprints along the path are then supplied to the decoder to output reconstructions.\n11\nAnother method for generating potentially valid encoded representations of microstructure\nis to form a random cloud, centered at a known valid encoded representation, e(x), for some\nimage x. This can be achieved by sampling random Gaussian noise from N(0, 1) and summing\nwith e(x), i.e.,\nen(x) = e(x) + γN(µ, σ),\nn = 1, . . . , N,\nwhere en(x) is a random neighbour of e(x), γ is a tunable scale factor that controls the noise\nlevel, µ, σ ∈ RD denote the element-wise mean and standard deviation for each dimension in\nthe encoded space and N is the number of neighbouring fingerprints to be output.\n3\nResults\n3.1\nReconstruction Accuracy\nThe ResNet18 VAE was trained under two separate regimes. The first included exclusively\neither lamellar or bimodal microstructures and the second included the full LFTi64BL dataset,\ncontaining both lamellar and bimodal microstructures. Figure 8 shows some example recon-\nstructions from the lamellar dataset after 1000 epochs.\nWhen training is restricted to the\nlamellar dataset, grain boundaries are accurately identified and reconstructed, but there is\nsome smoothing evident in the reconstructions and the interlamellar β appears coarsened. This\nbecomes clearer upon inspection of the morphological metric distributions shown in Figure 9.\nThe αp volume fraction is slightly reduced in the reconstructions, with some anomalies between\n30% and 50%. The mean lamellae direction, relative to the bottom edge of each image patch,\nis consistent. This confirms that the αp grains are oriented correctly in the reconstructions,\nbut there is an increase in the mean lamellae width, which is likely due to the smoothing effect\nremoving smaller grains. There is also an increase in the mean lamellae aspect ratio, owed to\nthe coarsening of the interlamellar β in the reconstructions which results in αp grains appearing\nmore elongated.\nFigure 8: Representative examples of (a) 256 × 256 patches from the lamellar microstructures\nand (b) their corresponding reconstructions from the ResNet18 VAE architecture, after 1000\nepochs.\nTraining the VAE on exclusively bimodal microstructures results in a similar smoothing\neffect as with the lamellar images, although this appears more pronounced in the bimodal case\n12\nFigure 9: Morphological metric distributions across the original image patches and their cor-\nresponding reconstructions for the lamellar dataset.\ndue to the nature of the fine scale features present in this case. Figure 10 shows some example\nreconstructions for the bimodal micrographs. All αs laths retained in the prior β grains are\ncompletely absent from the reconstructions and the outputs are effectively a mask for the αp\ngrains. This is confirmed to be a feature of the entire set of reconstructions in Figure 11, which\nshows a drastic increase in both the αp volume fraction and mean αp grain size.\n3.2\nTraversing the Encoded Space\nThe encoded space was explored with the methods discussed in Section 2.9. Figure 12 shows\nreconstructions along a linear path between a pair of fingerprints from the training set. A linear\npath between the fingerprints was constructed and 8 equispaced fingerprints were determined\nalong the path. These fingerprints were supplied to the trained decoder to generate the images\nin Figure 12.\nThe fingerprints along this path show a smooth transition along the linear path, which is a\ndirect result of the continuity of the latent space learned by the VAE. These microstructures are\nsynthetic and are not included in the training set, although they do have the same characteristics\nas the lamellar microstructures in the training set.\nFigure 13 shows the output from the same method applied to the full LFTi64BL dataset,\n13\nFigure 10: Representative examples of (a) 256 × 256 patches from the bimodal microstructures\nand (b) their corresponding reconstructions from the ResNet18 VAE architecture, after 1000\nepochs.\nFigure 11: Morphological metric distributions across the original image patches and their cor-\nresponding reconstructions for the bimodal dataset.\nFigure 12: Reconstructions along linear path through the encoded space learned during training\non exclusively lamellar image data from LFTi64BL.\n14\nwith the linear path defined between a lamellar and a bimodal microstructure. The same smooth\ntransition between the input microstructures is observed, however, intermediate microstructures\nalong the path stray considerably away from the training set, particularly towards the centre of\nthe path. This is to be expected, but confirms that, for VAEs to generate convincing artificial\nmicrographs, it is crucial that the training set be cohesive and not contain drastic variations\nin microstructure. Otherwise, the latent space constructed is likely to contain microstructures\nthat are not representative of the training set.\nFigure 13: Reconstructions along linear path through the encoded space learned during training\non the full LFTi64BL dataset.\nTo visualise neighbouring microstructures localised around an individual fingerprint, a sample\nmicrostructure was randomly selected and supplied to the trained encoder. The fingerprint ob-\ntained was perturbed with a small amount of Gaussian noise, as described in Section 2.9 with\nγ = 0.2, and provided to the trained decoder to generate a synthetic microstructure. This\nprocess was repeated and Figure 14 shows 10 realisations of microstructures produced.\nEach artificial microstructure possesses similar features to the input micrographs in terms of\ngrain morphology and direction of lamellae with respect to the bottom edge of the image. This\nshows that local fingerprints within the latent space are likely to possess similar microstructural\nfeatures. Figure 15 shows example micrographs constructed with increasing values of γ. As\nγ increases, images generated are no longer representative of the training dataset. PCA was\ntrained on the fingerprints constructed by the VAE and then used to transform the noise-\nperturbed fingerprints alongside the original fingerprints (see Figure 16). This shows the noise-\nperturbed fingerprints emanating from a single point, which corresponds to the fingerprint to\nwhich the noise was applied, and we can see that noise added with a scale factor of γ > 0.5 are\nall completely outside the encoded space learned by the VAE.\n3.3\nMetric Distributions Across the Encoded Space\nDimensionality reduction, in the form of t-SNE (described in Section 2.8), was also applied to\nthe original 256-dimensional fingerprints to reduce them down to 2-dimensional vectors. This\n15\nFigure 14: Synthetic microstructures generated from the addition of unit Gaussian noise, with\na scale factor of γ = 0.2, to a known valid encoded representation, from which morphology is\ninherited.\n(a) Original\n(b) γ = 0.05\n(c) γ = 0.1\n(d) γ = 0.2\n(e) γ = 0.5\n(f) γ = 1\nFigure 15: Synthetic micrograph examples generated via the Gaussian cloud method with\nvarious γ values, compared with original micrograph.\nFigure 16: PCA applied to fingerprints leanred by the VAE alongside fingerprints constructed\nwith the Gaussian cloud method. Original fingerprints are shown in grey and the colour map\ndenotes the γ values applied to generate the artifical fingerprints.\nenables fingerprints to be plotted in a 2-dimensional scatter plot to visualise the entire latent\nspace and distribution of metrics across the space. Figure 17 shows the full LFTi64BL dataset\nreduced to 2-dimensions.\nThe colour map in this figure that illustrates the microstructure\nclassification, with 0 denoting a bimodal microstructure and 1 denoting lamellar. There is a\n16\nstrong clustering of each class, with only a small overlap between them, allowing fingerprints\nfrom each class to be easily separated. Training an SVM with 90% of the fingerprints allocated\nfor training and 10% reserved for testing yields a mean classification accuracy of 99.9% ± 0.001\nafter 10-fold cross-validation.\nFigure 17: t-SNE with 2 components applied to encoded representations, where the colour map\ndenotes classifications, with bimodal microstructures in red and lamellar microstructures in\nblue.\nThe same methodology for dimensionality reduction was applied to the lamellar and bimodal\nfingerprints separately to map distributions of various microstructural features. Figures 18a\nand 18b shows greyscale intensity from the lamellar micrographs, before and after normalisation.\nPrior to normalisation, there is a strong clustering between two groups of images, heavily\ninfluenced by illumination during image capture. After normalisation, greyscale intensity is more\nclosely linked to volume fraction and is normally distributed across the dataset. Dimensionality\nreduction then shows a smooth gradient of greyscale intensity across the latent space. The plots\nwith volume fraction yield similar results, although less pronounced. Figures 18c and 18d shows\nplots of the encoded spaces for bimodal and lamellar microstructures, trained separately.\nFinally, we look at directionality (discussed in Section 2.6.3) for lamellar microstructures\nand grain size (Section 2.6.4) for bimodal microstructures as morphological features of interest.\nFigure 18e shows the t-SNE plot with a colour map corresponding to lamellae direction.\nThis appears to show random scatter across the encoded space, in contrast to the Gaussian\ncloud for image generation that seems to reconstruct images with similar direction, when per-\nturbing fingerprints with a small amount of Gaussian noise. Nearest neighbours are shown to\nhave similar directionality, but this is contained within small regions of the latent space and not\nuniversal. This random distribution of directionality may be useful in practice, though, as this\nimplies that when encoding the microstructural information in such a manner, directionality\ncan be effectively ignored and sample orientation when imaging would not be of any concern.\nFigure 18f shows the t-SNE plot for the bimodal fingerprints with a colour map denoting\ngrain size. Here, we see a gradient of grain size across the latent space, suggesting that nearest\nneighbours will share similar morphologies.\nIn each t-SNE reduction, there is a small group of fingerprints that are separated away from\nthe main cluster. There are no discernible differences between the microstructures that result\n17\n(a) Lamellar greyscale prior to normalisation\n(b) Lamellar greyscale after normalisation\n(c) Lamellar αp volume fraction\n(d) Bimodal αp volume fraction\n(e) Lamellae directionality.\n(f) Bimodal mean αp grain size.\nFigure 18: t-SNE with 2 components applied to encoded representations of the LFTi64BL\ndataset, with colour maps denoting various morphological metrics.\nin these fingerprints and those that form the main cluster. Despite these microstructures being\nincluded in the training set, their reconstructions after training are effectively just noise. This is\na caveat of the VAE training process and not the dimensionality reduction. Removing these out-\nliers from the set of fingerprints and rerunning the t-SNE with the remaining fingerprints results\nin retention of the main cluster and a complete absence of the secondary cluster. Removing the\nimages that reside in the secondary cluster from the training set and retraining the VAE from\nscratch results in a new cluster forming, with a different morphology, that contains a different\n18\nset of images. Nonetheless, we are able to explore the encoded space of valid reconstructions,\nusing the t-SNE as a guide to identify suitably encoded microstructures.\n3.4\nMetric Predictions from Encoded Representations\nIf an approach to microstructural fingerprinting is successfully encoding the essence of the\nmicrostructure, then we might reasonably expect to be able to recover key features of the mi-\ncrostructure, such as grain size, from the encoded fingerprint. A strong correlation between the\nfingerprint and a given property also opens the possibility of reversing the inferential process and\nasking what fingerprints (or which regions of latent space) would correspond to a microstructure\nexhibiting a given property of interest (with the possibility with a VAE of then reconstructing\nan image of the corresponding microstructure).\nThe predictability of morphological metrics across the encoded space was validated with\nSupport Vector Regression (SVR), described in Section 2.7. The fingerprints output from the\nRestNet18 VAE are randomly split into 90% training data and 10% test data. This is repeated\n10 times to perform 10-fold cross-validation. The SVR is then trained on the training set and\noutputs for the test set are compared with the true values to measure the prediction accuracy.\nFigure 19 shows scatter plots for the SVR predictions across the 10 train-test splits combined,\nagainst the true values, with the line y = x plotted in red to illustrate deviation from exact\npredictions. Table 1 then shows percentage error and standard deviation of the predictions\nrelative to the true values.\nClass\nMetric\nMean Percentage Error\nLamellar\nGreyscale Intensity\n0.60 % ± 0.01\nVolume Fraction αp (%)\n1.83 % ± 0.03\nDirectionality (◦)\n148 % ± 29\nBimodal\nGreyscale Intensity\n0.75 % ± 0.01\nVolume Fraction αp (%)\n2.50 % ± 0.03\nGrain Size (µm)\n7.91 % ± 0.12\nTable 1: Quantitative analysis of error for SVR predictions.\n4\nDiscussion\nReconstructions from the proposed ResNet18 VAE were shown to accurately identify grain\nboundaries, albeit with some smoothing. This information loss is acceptable for correlating with\nthe morphological features discussed, as these features are still captured. However, prediction\nof properties that rely upon small scale features, such as fine αs laths, may become hindered.\nThis could be alleviated with higher resolution images and patches with a higher magnification.\nThe encoded space learned by the VAE is continuous and contains valid microstructural\nfingerprints that are not included in the training set.\nTwo methods were used to explore\nthe encoded space as a tool for generating artificial microstructures. The linear path method\nillustrates the continuity of the space and shows a smooth transition between microstructures,\n19\n(a) Lamellar greyscale intensity\n(b) Bimodal greyscale intensity\n(c) Lamellar volume fraction αp\n(d) Bimodal volume fraction αp\n(e) Lamellae direction\n(f) Bimodal grain size\nFigure 19: Scatter plots to illustrate combined 10-fold cross validation SVR predictions of\nvarious morphological metrics with the line y = x added to highlight where true predictions\nshould lie.\nwhen linearly interpolating between two known fingerprints. With a training set containing\nonly lamellar microstructures, reconstructions along this path are completely artificial but still\nresemble lamellar microstructures. Once the training set is expanded to include both bimodal\nand lamellar microstructures, the linear interpolation between a bimodal fingerprint and a\nlamellar fingerprint yields a blend between the two morphologies. These can still be perceived\n20\nas valid microstructures, but it is important to note that these are not representative of any\ndata in the training set and VAEs trained in such a way should be interpolated carefully.\nGaussian noise was also used to explore the encoded space more locally around an individual\nfingerprint. Iteratively perturbing a learned fingerprint and supplying the output to the trained\ndecoder yields another set of artificially generated microstructures. In this case, the decoder\ngenerates microstructures with similar features to the input image, including the lamellae thick-\nness and direction relative to the bottom of the image, provided that a suitably small amount\nof noise is applied. This behaviour is only observed locally for some features, such as lamellae\ndirection, which is randomly distributed across the encoded space. This may be due to the\nfact that the VAE was trained on patches with random rotations applied, making the VAE\nrotationally invariant in this case. This removes any bias towards sample orientation during\nimage capture. Adding large amounts of noise would result in variations of such features.\nDimensionality reduction via t-SNE was performed to reduce the fingerprints to 2 dimen-\nsions. This enables them to be plotted in a 2-dimensional scatter plot to visualise the dis-\ntribution of morphological features across the encoded space. The fingerprint position in the\nencoded space appears to dependent significantly on the volume fraction of αp, relative to other\nmorphological metrics. This was confirmed by SVR predictions, which was trained to predict\nvolume fraction with an average percentage error of 1.83 % ± 0.03 for the lamellar dataset and\n2.50 % ± 0.03 for the bimodal dataset. Grain size was also highly correlated with the encoded\nrepresentations and SVR was able to predict grain size with reasonable accuracy, giving an\naverage percentage error of 7.91 % ± 0.12. Directionality of lamellae appeared to be distributed\nrandomly across the encoded space from the t-SNE plots and this was also confirmed with SVR,\nwhich was unable to learn any trends in the data and repeatedly predicted the mean direction\nfor each fingerprint, resulting in an average percentage error of 148 % ± 29. In practice, the\nrandom nature of the distribution of lamellae direction could be a useful feature, as this im-\nplies that sample orientation under the microscope can be effectively ignored when capturing a\ndataset for VAE without any bias being introduced.\n5\nConclusions\nThe latent space of microstructural fingerprints generated from variational autoencoders (VAEs)\nhas been explored to further our understanding of how VAEs encode feature information from\nmicrostructural image data.\n• We show that a variational autoencoder (VAE) architecture based on ResNet18 is able to\nproduce accurate reconstructions of microstructures (up to some smoothing).\n• Interpolation of fingerprints along a linear path in latent space and random perturbations\nabout fingerprints of input microstructures resulted in the generation of plausible synthetic\nmicrostructures, demonstrating the suitability of the VAE for smooth representation of\nmicrostructure.\n• Fingerprints constructed by the trained VAE are shown to correlate with morphological\nfeatures, including αp volume fraction and grain size, using support vector regression\n(SVR) with 10-fold cross-validation.\n21\n• Principal component analysis (PCA) is shown to provide useful insight into the amount of\nnoise that can be added to known fingerprints before the perturbed fingerprints become\ndisconnected from the learned latent space.\nOur study based on a set of micrographs of titanium alloy microstructure demonstrates that\na VAE can encode material microstructures to produce fingerprints that exhibit several of the\nkey features required in a general purpose fingerprint. Important next steps will be to test this\napproach on a broader class of microstructures and to apply it to datasets exhibiting variation in\nboth microstructure and measured properties, to allow exploration of the suitability of VAEs for\npredicting microstructure-process-property relationships in an explainable framework. We also\nnote that generative adversarial networks (GANs) are found to generate high-fidelity synthetic\nmicrostructures, with strong statistical similarity to the training data, even in a limited data\nregime [33]. It would be interesting to consider synthetic images output from a GAN as inputs\nto the VAE to determine how the generated images are distributed through the encoded space\nand assess their potential for bolstering morphology prediction.\nAcknowledgements\nThis research was supported through funding from the Scheme for Promotion of Academic\nand Research Collaboration (SPARC) grant MHRD-18-0015.\nMDW was supported by the\nUniversity of Manchester and Rolls-Royce plc under the Engineering and Physical Sciences\nResearch Council (EPSRC) grant EP/S022635/1 and Science Foundation Ireland (SFI) grant\n18/EPSRC-CDT/3584. CPR was supported by a University Research Fellowship of the Royal\nSociety. PJW was supported by the Henry Royce Institute for Advanced Materials, funded\nthrough EPSRC grants EP/R00661X/1, EP/S019367/1, EP/P025021/1 and EP/P025498/1.\nReferences\n[1] B. Basu, N. H. Gowtham, Y. Xiao, S. R. Kalidindi, K. W. Leong, Biomaterialomics:\nData science-driven pathways to develop fourth-generation biomaterials, Acta Biomateri-\nalia (2022). doi:10.1016/j.actbio.2022.02.027.\n[2] E. A. Holm, R. Cohn, N. Gao, A. R. Kitahara, T. P. Matson, B. Lei, S. R. Yarasi,\nOverview:\nComputer vision and machine learning for microstructural characterization\nand analysis, Metallurgical and Materials Transactions A 51 (2020) 5985–5999.\ndoi:\n10.1007/s11661-020-06008-4.\n[3] T. L. Burnett, P. J. Withers, Completing the picture through correlative characterization,\nNature Materials 18 (2019) 1041–1049. doi:10.1038/s41563-019-0402-8.\n[4] B. L. DeCost, E. A. Holm, A computer vision approach for automated analysis and classific-\nation of microstructural image data, Computational Materials Science 110 (2015) 126–133.\ndoi:10.1016/j.commatsci.2015.08.011.\n22\n[5] M. I. Latypov, S. R. Kalidindi, Data-driven reduced order models for effective yield strength\nand partitioning of strain in multiphase materials, Journal of Computational Physics 346\n(2017) 242–261. doi:10.1016/j.jcp.2017.06.013.\n[6] Y. C. Yabansu, P. Steinmetz, J. H¨otzer, S. R. Kalidindi, B. Nestler, Extraction of reduced-\norder process-structure linkages from phase-field simulations, Acta Materialia 124 (2017)\n182–194. doi:10.1016/j.actamat.2016.10.071.\n[7] E. Popova, T. M. Rodgers, X. Gong, A. Cecen, J. D. Madison, S. R. Kalidindi, Process-\nstructure linkages using a data science approach: Application to simulated additive man-\nufacturing data, Integrating Materials and Manufacturing Innovation 6 (1) (3 2017).\ndoi:10.1007/s40192-017-0088-1.\n[8] K. Rajan, Materials informatics: The materials “gene” and big data, Annual Review of Ma-\nterials Research 45 (1) (2015) 153–169. doi:10.1146/annurev-matsci-070214-021132.\n[9] D. L. McDowell,\nR. A. LeSar,\nThe need for microstructure informatics in pro-\ncess–structure–property relations, MRS Bulletin 41 (8) (2016) 587–593.\ndoi:10.1557/\nmrs.2016.163.\n[10] M. D. White, A. Tarakanov, P. J. Withers, C. P. Race, K. J. H. Law, Digital fingerprinting\nof microstructures, Computational Materials Science 218 (2023) 111985. doi:10.1016/j.\ncommatsci.2022.111985.\n[11] D. T. Fullwood, S. R. Niezgoda, B. L. Adams, S. R. Kalidindi, Microstructure sensitive\ndesign for performance optimization, Progress in Materials Science 55 (6) (2010) 477–562.\ndoi:10.1016/j.pmatsci.2009.08.002.\n[12] D. P. Kingma, M. Welling, Auto-Encoding Variational Bayes, arXiv (May 2014). doi:\n10.48550/arXiv.1312.6114.\n[13] Z. Liu, P. Luo, X. Wang, X. Tang, Deep learning face attributes in the wild, in: Proceedings\nof International Conference on Computer Vision (ICCV), 2015.\n[14] A. Sardeshmukh, S. Reddy, B. P. Gautham, P. Bhattacharyya, TextureVAE: Learning\nInterpretable Representations of Material Microstructures Using Variational Autoencoders,\nin: Proceedings of the AAAI 2021 Spring Symposium on Combining Artificial Intelligence\nand Machine Learning with Physical Sciences, Stanford, CA, USA, March 22nd - to - 24th,\n2021, Vol. 2964 of CEUR Workshop Proceedings, CEUR-WS.org, 2021.\n[15] R. Cang, H. Li, H. Yao, Y. Jiao, Y. Ren, Improving direct physical properties predic-\ntion of heterogeneous materials from imaging data via convolutional neural network and a\nmorphology-aware generative model, Computational Materials Science 150 (2018) 212–221.\ndoi:10.1016/j.commatsci.2018.03.074.\n[16] Y. Kim, H. K. Park, J. Jung, P. Asghari-Rad, S. Lee, J. Y. Kim, H. G. Jung, H. S.\nKim, Exploration of optimal microstructure and mechanical properties in continuous mi-\ncrostructure space using a variational autoencoder, Materials & Design 202 (2021) 109544.\ndoi:10.1016/j.matdes.2021.109544.\n23\n[17] H. S. Stein, D. Guevarra, P. F. Newhouse, E. Soedarmadji, J. M. Gregoire, Machine learning\nof optical properties of materials – predicting spectra from images and images from spectra,\nChemical Science 10 (2019) 47–55. doi:10.1039/C8SC03077D.\n[18] Y. Pathak, K. S. Juneja, G. Varma, M. Ehara, U. D. Priyakumar, Deep learning enabled\ninorganic material generator, Physical Chemistry Chemical Physics 22 (2020) 26935–26943.\ndoi:10.1039/D0CP03508D.\n[19] S. Sundar, V. Sundararaghavan, Database development and exploration of microstructure\nversus process relationships using variational autoencoders, arXiv (2020). doi:10.48550/\nARXIV.2001.09171.\n[20] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition, arXiv\n(2015). doi:10.48550/ARXIV.1512.03385.\n[21] M. White, C. Daniel, J. Quinta da Fonseca, X. Zeng, N. Byers, B. Karnasiewicz, Lamellar\nand bi-modal ti-64 microstructure images (Nov. 2021). doi:10.5281/zenodo.5714384.\nURL https://zenodo.org/records/5714384\n[22] D. P. Kingma, J. Ba, Adam: A Method for Stochastic Optimization, arXiv (2014). doi:\n10.48550/ARXIV.1412.6980.\n[23] D. P. Kingma, M. Welling, An introduction to variational autoencoders, Foundations and\nTrends® in Machine Learning 12 (4) (2019) 307–392. doi:10.1561/2200000056.\n[24] R. Yu, A tutorial on VAEs: From Bayes’ rule to lossless compression, arXiv (2020). doi:\n10.48550/arXiv.2006.10273.\n[25] S. Bj¨ork, J. N. Myhre, T. H. Johansen, Simpler is better: spectral regularization and up-\nsampling techniques for variational autoencoders, arXiv (2022). doi:10.48550/ARXIV.\n2201.07544.\n[26] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, L. Fei-Fei, Imagenet: A large-scale hierarchical\nimage database, in: 2009 IEEE conference on computer vision and pattern recognition, Ieee,\n2009, pp. 248–255. doi:10.1109/CVPR.2009.5206848.\n[27] S. L. Bangare, A. Dubal, P. S. Bangare, S. T. Patil, Reviewing Otsu’s method for image\nthresholding, International Journal of Applied Engineering Research 10 (9) (2015) 21777–\n21783. doi:10.37622/IJAER/10.9.2015.21777-21783.\n[28] E04 Committee, Test Methods for Determining Average Grain Size Using Semiauto-\nmatic and Automatic Image Analysis, Tech. rep., ASTM International.\ndoi:10.1520/\nE1382-97R15.\n[29] M. Awad, R. Khanna, Support Vector Regression, In: Efficient Learning Machines: The-\nories, Concepts, and Applications for Engineers and System Designers, Apress, Berkeley,\nCA, 2015, pp. 67–80. doi:10.1007/978-1-4302-5990-9_4.\n24\n[30] R. Silva, P. Melo-Pinto, t-SNE: A study on reducing the dimensionality of hyperspectral\ndata for the regression problem of estimating oenological parameters, Artificial Intelligence\nin Agriculture 7 (2023) 58–68. doi:10.1016/j.aiia.2023.02.003.\n[31] G. E. Hinton, S. Roweis, Stochastic neighbor embedding, in: S. Becker, S. Thrun, K. Ober-\nmayer (Eds.), Advances in Neural Information Processing Systems, Vol. 15, MIT Press,\n2002.\n[32] L. van der Maaten, G. Hinton, Visualizing data using t-sne, Journal of Machine Learning\nResearch 9 (86) (2008) 2579–2605.\n[33] N. H. Gowtham, T. J. Jeyapriya, C. Bhattacharya, B. Basu, A deep adversarial approach\nfor the generation of synthetic titanium alloy microstructures with limited training data,\nComputational Materials Science 230 (2023) 112512. doi:10.1016/j.commatsci.2023.\n112512.\n25\n"
}