{
    "optim": "A Probabilistic Model to explain Self-Supervised Representation Learning Alice Bizeul 1 Bernhard Sch¨olkopf 2 Carl Allen 1 Abstract Self-supervised learning (SSL) learns represen- tations by leveraging an auxiliary unsupervised task, such as classifying semantically related sam- ples, e.g. different data augmentations or modali- ties. Of the many approaches to SSL, contrastive methods, e.g. SimCLR, CLIP and VicREG, have gained attention for learning representations that achieve downstream performance close to that of supervised learning. However, a theoretical under- standing of the mechanism behind these methods eludes. We propose a generative latent variable model for the data and show that several fami- lies of discriminative self-supervised algorithms, including contrastive methods, approximately in- duce its latent structure over representations, pro- viding a unifying theoretical framework. We also justify links to mutual information and the use of a projection head. Fitting our model generatively, as SimVAE, improves performance over previous VAE methods on common benchmarks (e.g. Fash- ionMNIST, CIFAR10, CelebA), narrows the gap to discriminative methods on content classifica- tion and, as our analysis predicts, outperforms them where style information is required, taking a step toward task-agnostic representations. 1. Introduction In self-supervised learning (SSL), a model is trained on an auxiliary task without class labels, and learns representa- tions of the data in the process. Of the many approaches to SSL (Ericsson et al., 2022; Balestriero et al., 2023), recent contrastive methods, such as InfoNCE (Oord et al., 2018), SimCLR (Chen et al., 2020), DINO (Caron et al., 2021) and CLIP (Radford et al., 2021), have gained attention for their remarkable performance on downstream tasks, approaching that of supervised learning. These methods exploit seman- 1Department of Computer Science & ETH AI Center, ETH Zurich, Z¨urich 2Max Planck Institute for Intelligent Systems, T¨ubingen. Correspondence to: Alice Bizeul <al- ice.bizeul@inf.ethz.ch>. Preliminary work. Copyright 2024 by the author(s). tically related observations, such as different parts (Oord et al., 2018; Mikolov et al., 2013), augmentations (Chen et al., 2020; Misra & Maaten, 2020), or modalities/views (Baevski et al., 2020; Radford et al., 2021; Arandjelovic & Zisserman, 2017) of the data. There is significant inter- est in understanding self-supervised methods (e.g. Wang & Isola, 2020; Zimmermann et al., 2021; Tian, 2022; Ben- Shaul et al., 2023), but a general mathematical mechanism justifying their performance remains unclear. At the same time, theoretically principled representations can be learned generatively by fitting a latent variable model by variational inference (e.g. Kingma & Welling, 2014), but they typically under-perform recent (discriminative) SSL methods. To address this, we draw an equivalence between discrim- inative and generative representation learning that treats Figure 1: Gen- erative model for SSL (J semanti- cally related sam- ples, parameters explained in Eq. 4) representations as latent variables and an encoder f : X → Z (mapping data x to representations z = f(x)) as a special case of the posterior p(z|x). We then propose a genera- tive model for self-supervised learn- ing (Fig. 1) and by considering its evidence lower bound (ELBO), show that families of discriminative SSL methods, including the InfoNCE ob- jective adopted by several algorithms (e.g. Chen et al., 2020; Radford et al., 2021), induce similar latent struc- ture. Notably, the common percep- tion that self-supervised learning ob- jectives “pull together” representa- tions of semantically related data and “push apart” others (e.g. Wang & Isola, 2020) is explained from first princi- ples by this model: the prior pulls and the reconstruction term pushes. Under the proposed model, representations (z) of semanti- cally related data (x) form clusters conditioned on shared latent content (y), with variation in z|y considered style. Our analysis predicts that these clusters form under previ- ous discriminative SSL methods, but “collapse”, losing style information that distinguishes related samples, e.g. colour, orientation. Any information loss may affect some down- stream tasks more than others, limiting the generality of 1 arXiv:2402.01399v1  [cs.LG]  2 Feb 2024 A Probabilistic Model to explain Self-Supervised Representation Learning Figure 2: Assessing the information in representations: original images (left cols) and reconstructions from representations learned by generative unsupervised learning (VAE, β-VAE, CR-VAE), generative SSL (our SimVAE) and discriminative SSL (SimCLR, VicREG) on MNIST (l), Fashion MNIST (r). Discriminative methods lose style information (e.g. orientation). representations. To verify this, we fit the latent variable model generatively by maximising the ELBO, termed Sim- VAE,1 to explicitly induce the proposed latent structure. While training a generative model is more challenging than training a discriminative one in general, our results show that SimVAE is competitive with or outperforms popular discrim- inative methods at downstream classification on common benchmark datasets (MNIST, FashionMNIST and CelebA). On more complex data (CIFAR10), SimVAE is less compet- itive for content classification but, as predicted by our analy- sis above, consistently outperforms discriminative methods on tasks requiring style information. Notably, SimVAE sig- nificantly outperforms other VAE-based generative models on all tasks. These results provide empirical support for the generative model as a mathematical basis for SSL; and suggest that generative representation learning is promising if distributions can be well modelled, particularly given its added benefits of uncertainty estimation from the posterior, a means to generate synthetic samples and to qualitatively assess the information captured by inspecting reconstruc- tions and naturally model groups (Fig. 2). To summarise our main contributions: • we propose a latent variable model that underpins and unifies popular families of self-supervised learning algo- rithms, including contrastive methods (Fig. 1); • we show that this model underpins the notion that SSL “pulls together”/“pushes apart” representations, ratio- nalises the link to mutual information and justifies the common use of a projection head (§3); • we fit the latent variable model generatively, termed Sim- VAE, and show clear improvement (>15% on CIFAR10) on benchmark (content) classification tasks over previous generative (VAE-based) methods, including one tailored to SSL (Sinha & Dieng, 2021) (§5); and • we show that SimVAE captures more style information than discriminative methods as our analysis predicts (§5). 1So-named as it encodes the latent structure of SimCLR (Chen et al., 2020) in the prior of a VAE (Kingma & Welling, 2014). 2. Background and Related Work Representation Learning aims to learn an encoder f : X → Z that maps data x∈X to (often lower-dimensional) repre- sentations z =f(x)∈Z that perform well on downstream tasks. Representation learning is not “well defined” in the sense that downstream tasks can be arbitrary and a represen- tation that performs well on one task may perform poorly on another (Zhang et al., 2022). For instance, unsupervised image representations are commonly evaluated by predict- ing semantic class labels, but the downstream task could be to detect lighting, position or orientation, which a repre- sentation useful for class prediction may not capture. This suggests that general-purpose unsupervised representations should capture as much information about the data as possi- ble. Recent works support this by evaluating on a variety of downstream tasks (e.g. Balaˇzevi´c et al., 2023). Self-Supervised Learning (SSL) learns representations by leveraging an auxiliary task. The many approaches to SSL can be categorised in various ways (e.g. Balestriero et al., 2023). We define those we focus on as follows: Instance Discrimination (Dosovitskiy et al., 2014; Wu et al., 2018) treats each sample xi, along with any augmentations, as a distinct class i. A softmax classifier is trained to predict the “class” and encoder outputs are used as representations. Latent Clustering performs clustering on representations. Song et al. (2013); Xie et al. (2016); Yang et al. (2017) apply K-means or similar to the hidden layer of a standard auto-encoder. DeepCluster (Caron et al., 2020) iteratively clusters ResNet representations by K-means, and predicts the cluster assignments as “pseudo-labels”. DINO (Caron et al., 2021), a transformer-based model, can be interpreted as clustering in the latent space (Balestriero et al., 2023). Contrastive Learning encourages representations of seman- tically related data (positive samples) to be “close” in con- trast to those sampled at random (negative samples). Early SSL approaches include energy-based models (Chopra et al., 2005; Hadsell et al., 2006); and word2vec (Mikolov et al., 2013) that predicts co-occurring words, thereby capturing their pointwise mutual information (PMI) in its embed- dings (Levy & Goldberg, 2014; Allen & Hospedales, 2019). 2 A Probabilistic Model to explain Self-Supervised Representation Learning InfoNCE (Oord et al., 2018; Sohn, 2016) extends word2vec to other data domains. Its widely used objective, for a pos- itive pair of semantically related samples (x, x+) and ran- domly selected negative samples X−={x− k }K k=1, is defined LINCE(x, x+, X−) = log e sim(z,z+) P x′∈{x+}∪X− e sim(z,z′) , (1) where sim(·, ·) is a similarity function, e.g. dot product. Eq. 1 is minimised if sim(z, z′) = PMI(x, x′)+c, for constant c (Oord et al., 2018). Many works build on InfoNCE, e.g. SimCLR (Chen et al., 2020) uses synthetic augmentations and CLIP (Radford et al., 2021) uses different modalities as positive samples; DIM (Hjelm et al., 2019) takes other encoder parameters as representations; and MoCo (He et al., 2020), BYOL (Grill et al., 2020) and VicREG (Bardes et al., 2022) find alternative strategies to negative sampling to prevent representations from collapsing. We do not address all SSL algorithms in this work, in par- ticular those with regression-based auxiliary tasks such as reconstructing data from perturbed versions (e.g. He et al., 2022; Xie et al., 2022); or predicting perturbations, such as rotation angle (e.g. Gidaris et al., 2018). For clarity, we refer to the methods we address as Predictive SSL. Variational Auto-Encoder (VAE): For a generative model z → x, parameters θ of pθ(x) = R z pθ(x|z)pθ(z) can be learned by maximising the evidence lower bound (ELBO) Ex \u0002 log pθ(x) \u0003 ≥ Ex \u0002Z z qϕ(z|x) log pθ(x|z)p(z) qϕ(z|x) \u0003 , (2) where qϕ(z|x) learns to approximate the posterior pθ(z|x). Latent variables z can be used as representations (see §3.1). A VAE (Kingma & Welling, 2014) maximises the ELBO, with pθ, qϕ modelled as Gaussians parameterised by neural networks. A β-VAE (Higgins et al., 2017) weights ELBO terms to increase disentanglement of latent factors. A CR- VAE (Sinha & Dieng, 2021) considers semantically related samples through an additional regularisation term. Further relevant VAE variants are summarised in Appendix A.1. Variational Classification (VC): Dhuliawala et al. (2023) define a latent variable model for classification of labels y, p(y|x)= R z q(z|x) p(z|y)p(y) p(z) , that generalises softmax neu- ral network classifiers, interpreting the encoder as param- eterising q(z|x); and the softmax layer as encoding p(y|z) by Bayes’ rule (VC-A). For data in continuous domains X, e.g. images, q(z|x) of a softmax classifier can overfit to a single delta-distribution for all samples of a class, meaning representations of a class collapse together (termed neural collapse by Papyan et al., 2020).2 This loses semantic and probabilistic information distinguishing class samples and harms properties such as calibration and robustness (VC-B). 2In practice, constraints such as l2 regularisation and early stopping arbitrarily restrict the described optimum being attained. Prior theoretical analysis: There has been considerable in- terest in understanding the mathematical mechanism behind self-supervised learning (Arora et al., 2019; Tsai et al., 2020; Wang & Isola, 2020; Zimmermann et al., 2021; Lee et al., 2021; Von K¨ugelgen et al., 2021; HaoChen et al., 2021; Wang et al., 2021; Saunshi et al., 2022; Tian, 2022; Sansone & Manhaeve, 2022; Nakamura et al., 2023; Shwartz-Ziv et al., 2023; Ben-Shaul et al., 2023), as summarised by HaoChen et al. (2021) and Saunshi et al. (2022). A thread of works (Arora et al., 2019; Tosh et al., 2021; Lee et al., 2021; HaoChen et al., 2021; Saunshi et al., 2022) aims to prove that auxiliary task performance translates to downstream classification accuracy, but Saunshi et al. (2022) show this can not apply to typical datasets, and model architecture must be considered. Several works propose an informa- tion theoretic basis for SSL (Hjelm et al., 2019; Bachman et al., 2019; Tsai et al., 2020; Shwartz-Ziv et al., 2023), e.g. maximising mutual information between representa- tions, but Tschannen et al. (2020); McAllester & Stratos (2020); Tosh et al. (2021) raise doubts with this. We show that the relationship to mutual information is justified more fundamentally by our latent variable model (Fig. 1). 3. Self-Supervised Representation Learning We consider datasets X = SN i=1 xi, where each subset xi ={xj i}j contains semantically related data xj i ∈X, con- sidered to share semantic content and vary in style.3 For example, xi may contain different augmentations, modali- ties or snippets of a given observation (e.g. an image of a triangle and its mirror image, where the shape is content and orientation is style). We assume a generative process where yi ∼p(y) is sampled, determining semantic content, then zi = {zj i }j are sampled conditionally independently zj i ∼p(z|yi), reflecting the same content but varying in style. Finally data points xj i ∈xi are sampled xj i ∼p(x|zj i ). Fig. 1 shows the generative model for SSL, under which p(xi|yi) = Z zi \u0010Y j p(xj i|zj i ) \u0011\u0010Y j p(zj i |yi) \u0011 . (3) Note that yi is not semantically meaningful, only an observe indicator reflecting that xj i ∈xi are related. If distributions in Eq. 3 are modelled parametrically, their parameters can be learned by maximising ELBOSSL, a lower bound on log p(x|y) for J semantically related data x={xj}J j=1 ⊆xi, Ex,y h J X j=1 Z zjqϕ(zj|xj) \u0000log pθ(xj|zj) − log qϕ(zj|xj) \u0001 + Z z qϕ(z|x)log pψ(z|y) i , (4) where the approximate posterior is assumed to factorise as q(z|x) ≈ Q q(zj|xj).4 A derivation of Eq. 4 is given in 3|xi| may vary and domains X j ∋xj i can differ with modality. 4Expected to be reasonable for zj that carry high information 3 A Probabilistic Model to explain Self-Supervised Representation Learning Appendix A.2. As in the generic ELBO (Eq. 2), ELBOSSL comprises: a reconstruction term, the approximate posterior entropy and the (now conditional) log prior over all zj ∈z. Note that if all p(z|yi) are concentrated relative to p(z), i.e. Var[z|yi]≪Var[z], then latent variables of semantically re- lated data z∈ zi are clustered, or closer on average than those of random samples – as SSL representations are often described. Indeed, our claim is that the generative model in Fig. 1, fitted by maximising Eq. 4, underpins discriminative self-supervised learning algorithms, including instance dis- crimination, deep clustering and contrastive methods (pre- dictive SSL, §2). We first clearly define how discriminative and generative methods relate and, from that, show that the loss functions of discriminative methods reflect Eq. 4. 3.1. Discriminative vs Generative Learning Representation learning aims to train an encoder f : X →Z so that representations z = f(x) are arranged usefully for downstream tasks, e.g. clustered so that a class of interest is easily identified. The predictive self-supervised approaches we consider (§2) train f under a loss function without a gen- erative model, so are discriminative. Representations can also be learned generatively: if latent variables z, under a generative model z→x, determine underlying properties of the data x, then the posterior p(z|x) is expected to estimate semantic properties of x, so approximating it (by q(z|x) parameterised by f) gives semantically meaningful repre- sentations. These approaches are actually closely related: Gen-Discrim Equivalence: A deterministic encoder f can be viewed as a concentrated posterior distribution pf(z|x)=δz−f(x). Together with p(x), this defines the joint pf(x, z) = pf(z|x)p(x) and marginal pf(z) .= R x pf(x, z). Thus, an encoder can be trained to map the data distribu- tion to a “useful” target distribution (or distribution family) p∗(z): • generatively by optimising the ELBO with prior p∗(z); or • discriminatively if there exists a loss function that when optimised induces pf(z)≈p∗(z). The takeaway is that both approaches have an equivalent aim. In particular, our claim can now be restated as: pre- dictive SSL methods (§2) induce a distribution pf(z) that approximates p(z) under the graphical model in Fig. 1. 3.2. Inducing p∗(z): discriminatively or generatively? If an encoder can be trained to give a useful distribution p∗(z) over representations discriminatively or generatively then, all else equal, the former may seem preferable to avoid modelling p(x|z). However, while the ELBO gives a prin- cipled generative objective to learn representations that fit p∗(z), a principled discriminative loss that minimises with w.r.t. xj, such that observing related xk or its representation zk pro- vides negligible extra information, i.e. p(zj|xj, xk)≈p(zj|xj). pf(z)=p∗(z) is less clear. We therefore consider the effect each ELBO term has on the optimal posterior/encoder to understand what a discriminative, or “p(x|z)-free”, objec- tive needs to achieve to induce a given distribution over representations. Entropy: discriminative methods use a deterministic map- ping q(z|x)=δz−f(x). We treat this as a posterior with very small but non-zero fixed variance, hence H[q] is a constant and dropped from the SSL objective. Prior: this term is optimal w.r.t. q iff related samples x∈xi each map to a mode of p(z|yi). For uni-modal p(z|yi) this means all representations z ∈zi “collapse” to a point, losing information distinguishing x∈xi.5 Reconstruction: this term is maximised w.r.t. q iff each x maps to a distinct representation z that p(x|z) maps back to x. This requires all z to be distinct and all information to be retained, countering the prior to avoid collapse. In summary, we consider a discriminative objective to emu- late the ELBO if it combines the log prior with a p(x|z)-free substitute for the reconstruction term (denoted Rec(·)) that should, inter alia, avoid representation collapse both be- tween and within subsets zi, i.e. Z z q(z|x)(log p(z|y) + Rec(z, x)) . (5) From the analysis above, maximising Eq. 5 can be seen to fit the notion that SSL objectives “pull together” representa- tions of related data and “push apart” others (e.g. Wang & Isola, 2020), the prior pulls (representations are attracted to modes) and the reconstruction pushes or “avoids collapse” (representations are made distinct). Our claim is that Eq. 5, and by extension ELBOSSL, is the rationale for that notion, i.e. SSL objectives of the form of Eq. 5 emulate ELBOSSL and so approximately induce the latent structure p(z) in Fig. 1. The true reconstruction term not only avoids col- lapse but is also integral to modelling the data distribution and posterior to give meaningful representations. As we shall see, approximation by Rec(·) can impact representa- tions and their performance in downstream tasks. 3.3. Discriminative Self-Supervised Learning We now consider examples of instance discrimination, la- tent clustering and contrastive SSL methods (§2) and their relationship to the latent variable model in Fig. 1 via Eq. 5. Instance Discrimination (ID) (Dosovitskiy et al., 2014; Wu et al., 2018) trains a softmax classifier on sample-index pairs {(xj i, yi =i)}i,j, which VC-A (Dhuliawala et al., 2023, §2) interprets, under Fig. 1, as maximising the RHS of: log p(y|x) ≥ Z z qϕ(z|x) log p(y|z) (6) 5Assuming classes xi are distinct, as is the case for empirical self-supervised learning datasets of interest. 4 A Probabilistic Model to explain Self-Supervised Representation Learning = Z z qϕ(z|x)(log p(z|y) − log p(z)) + c. (7) Eq. 7 matches Eq. 5 with J =1 and Rec(·) = H[p(z)], the entropy of p(z). Intuitively, maximising entropy helps avoid collapse, but under a softmax loss, while representations of distinct classes do spread apart, those of the same class z ∈zi collapse together (VC-B, Dhuliawala et al., 2023). Deep Clustering (DC) (Caron et al., 2018) iteratively as- signs temporary labels yi to data xi by (K-means) clus- tering representations of a ResNet encoder and updating the encoder to predict those labels with a softmax head. While semantically related subsets xi are now defined by the ResNet’s “inductive bias”, the same loss is used as in ID (Eq. 7) and representation clusters z ∈zi collapse together. Contrastive Learning stems from the InfoNCE objective (Eq 1). A variational lower bound under Fig. 1 (comparable to Eq. 6) is optimised when sim(z, z′)=PMI(z, z′)+c (§2). Combining this, we see that InfoNCE maximises: LNCE(xi, X′ ={x+ i , x− i1, ..., x− ik}) = EX h Z z qϕ(Z|X) log sim(zi,z+ i ) P z′∈Z′ sim(zi,z′) i opt → EX h Z Z qϕ(Z|X) log p(z+ i |zi)/p(z+ i ) P z′∈Z′ p(z′|zi)/p(z′) i ≤ Ex h Z z qϕ(z|x) log p(zi,z+ i ) p(zi)p(z+ i ) i + log 1 k − 1 k→∞ → Ex h Z z qϕ(z|x)(log p(z|yi) − X j log p(zj i )) i (8) (A full derivation is given in Appendix A.3.) Thus the InfoNCE objective again approximates ELBOSSL using entropy Rec(·)=H[p(z)], as in Eq. 7 but with J =2. Role of J: ID and DC consider J = 1 sample xj i at a time and compute p(zi|yi; ψi) from saved parameters ψi (which could be memory intensive). For J ≥2 samples, one can es- timate p(zi|yi)= R ψi p(ψi) Q j p(zj i |yi; ψi)=s(zi), where ψi integrates out (see Appendix A.4 for an example). Since yi now defines no parameters, joint distributions over {zir}r depend only on whether {xir}r are semantically related: p(zi1, ..., zik)= ( s(zi) if ir =i ∀r (i.e. xir∈xi) Qk r=1 p(zir) if ir ̸=is ∀r, s InfoNCE implicitly applies a similar “trick” with sim(·, ·). Mutual Information (MI): InfoNCE is known to optimise a lower bound on MI, I(x, x′) = E[log p(x,x′) p(x)p(x′)] (Oord et al., 2018) and Eq. 8 shows it also lower bounds I(z, z′). As a result, maximising MI has at times been argued to underpin contrastive learning (Hjelm et al., 2019; Ozsoy et al., 2022). However, our analysis suggests that MI is not the fundamental explanation for contrastive learning, rather MI arises from substituting the reconstruction term in ELBOSSL. This is supported by Tschannen et al. (2020); McAllester & Stratos (2020); Poole et al. (2019) who show that better MI estimators do not give better representations, MI approximation is noisy and the InfoNCE estimator is ar- bitrarily upper-bounded. Further, for disjoint xi, pointwise mutual information (PMI) values span an unbounded range, [−∞, k], k>1, yet are commonly modelled by the bounded cosine similarity sim(z, z′) = z⊤z′ ∥z∥∥z′∥ ∈ [−1, 1] (e.g. Chen et al., 2020), making MI estimation worse (see §A.5). In Appendix A.5, we show that this common constraint leads to representations comparable to those learned by softmax cross entropy (cf ID), but without class parameters ψi.6 Notably, representations of related data again collapse. Summary: We have considered discriminative SSL meth- ods from instance discrimination, latent clustering and con- trastive learning, and shown that, despite considerable differ- ences, each approximates ELBOSSL and so approximately induces the prior of our model for SSL (Fig. 1). The number of SSL methods, even within the predictive subset we focus on, makes an exhaustive analysis infeasible. However, many methods adopt or approximate aspects analysed above. For example, SimCLR (Chen et al., 2020) and CLIP (Radford et al., 2021) use the InfoNCE objective. MoCo (He et al., 2020), BYOL (Grill et al., 2020) and VicREG (Bardes et al., 2022) replace negative sampling with a momentum encoder, stop gradients or (co-)variance terms, but nonetheless “pull together” representations of semantically related data mod- elling the prior; and “push apart” others, or “avoid collapse”, by those mechanisms that mimic the effect of reconstruc- tion. DINO (Caron et al., 2021) assigns representations of semantically related data pairs to a common cluster as in DC (Balestriero et al., 2023), but with J =2. The predictive SSL methods analysed above each replace the reconstruction term of ELBOSSL with entropy H[p(z)]. As such, representations of semantically related data form distinct clusters, but those clusters collapse, losing style information since representations z ∈zi become indistin- guishable (e.g. to a downstream classifier). Style informa- tion is important in many real-world tasks, e.g. detecting facial expression or voice sentiment; and is the focus of representation learning elsewhere (e.g. Higgins et al., 2017; Karras et al., 2019). Thus, counter to general representa- tion learning, discriminative SSL may over-fit to content- based tasks (see Appendix A.6 for further discussion.) Projection Head: Our analysis suggests a plausible expla- nation for the practice of adding layers, or a projection head, to an encoder and using its input as representations rather than its output z used in the loss function. Since the ELBO prevents both inter- and intra-cluster collapse, any approxi- 6We note that Wang & Isola (2020) reach a similar conclusion but less rigorously, e.g. the known PMI minimiser is not addressed. 5 A Probabilistic Model to explain Self-Supervised Representation Learning mation should also. Near-final encoder layers are found to exhibit similar clustering to z, but with higher intra-cluster variance (Gupta et al., 2022), as also seen in supervised soft- max classification (Wang et al., 2022). We conjecture that representations from near-final layers are preferable to z because their intra-cluster variance gives an overall distribu- tion closer to that under Fig. 1. Conversely, the simple fact that any other representations outperform z suggests that discriminative objectives do not learn ideal representations, supporting our claim that they are an approximation. 3.4. Generative Self-Supervised Learning (SimVAE) The proposed model for SSL (Fig. 1) has been shown to justify: (i) the training objectives of predictive SSL meth- ods; (ii) the notion that SSL “pulls together”/“pushes apart” representations; (iii) the connection to mutual information; and (iv) the use of a projection head. We now aim to add em- pirical support by validating the following predictions: [H1] maximising ELBOSSL achieves self-supervised learning, where distributions can be well modelled; [H2] maximising ELBOSSL retains more style information than discrimina- tive objectives; [H3] the generative model (Fig. 1) gives better representations than a range of VAE alternatives. To test these hypotheses, we maximise ELBOSSL (Eq. 4), effectively taking a generative approach to self-supervised learning, termed SimVAE. SimVAE can be considered a VAE with a mixture prior p(z)=P y p(z|y)p(y), where represen- tations z ∈z of semantically related samples are conditioned on the same y. We assume that p(x|z) and q(z|x) of Eq. 4 are Gaussians parameterised by neural networks, as in a stan- dard VAE; that conditionals p(z|y = i; ψi) = N(z; ψi, σ2) are Gaussian with small fixed variance σ2; and p(ψ) is uniform (over a suitable space). Integrating out ψ (as in Appendix A.4) gives: p(z|y) ∝ exp{− 1 2σ2 X z∈z (z − ¯z)2} , (9) a Gaussian centred around the mean representation ¯z = 1 J P j zj. Maximising this, as a component of ELBOSSL, “pinches together” representations of semantically related samples. Whereas contrastive methods typically compare pairs of related representations (i.e. J = 2), Eq. 4 allows any number J to be processed. In practice a balance is struck between better approximating p(z|y) and preserving diversity in a mini-batch. Algorithm 1 in Appendix A.7.1 details the steps to optimise Eq. 4 under these assumptions. 4. Experimental Setup Datasets and Evaluation Metrics We test our hypotheses by evaluating SimVAE representations on four benchmark datasets including two with natural images: MNIST (LeCun, 1998), FashionMNIST (Xiao et al., 2017), CelebA (Liu et al., 2015) and CIFAR10 (Krizhevsky et al., 2009). We augment images following the SimCLR protocol (Chen et al., 2020) which includes cropping and flipping, and colour jitter for natural images. Frozen pre-trained representations are eval- uated by (unsupervised) clustering under a gaussian mixture model and (supervised) training a non-linear MLP probe, a k-nearest neighbors (kNN) estimator (Cover & Hart, 1967) and a linear probe on classification tasks (Chen et al., 2020; Caron et al., 2020). Downstream performance is measured in terms of classification accuracy (Acc). Generative quality is evaluated by FID score (Heusel et al., 2017) and recon- struction error (see Appendix A.8). For further experimental details see Appendices A.7.2, A.7.3, A.7.6 and A.8. Baselines methods We compare SimVAE to other VAE- based models including the vanilla VAE (Kingma & Welling, 2014), β-VAE (Higgins et al., 2017) and CR-VAE (Sinha & Dieng, 2021), as well as to state-of-the-art self-supervised discriminative methods including SimCLR (Chen et al., 2020), VicREG (Bardes et al., 2022), and MoCo (He et al., 2020). As a lower bound, we also provide results for a ran- domly initialized encoder. For fair comparison, the augmen- tation strategy, representation dimensionality, batch size, and encoder-decoder architectures are invariant across meth- ods. To enable a qualitative comparison of representations, decoder networks were trained for each discriminative base- line on top of frozen representations using the reconstruction error. See Appendices A.7.4 and A.7.5 for further details on training baselines and decoder models. Implementation Details We use MLP and Resnet18 (He et al., 2016) network architectures for simple and natural image datasets respectively. The dimension of represen- tations z is set to 10 for MNIST, FashionMNIST, and 64 for CelebA and CIFAR10 datasets. For all generative ap- proaches, we adopt Gaussian posteriors, q(z|x), priors, p(z), and likelihoods, p(x|z), with diagonal covariance matrices (Kingma & Welling, 2014). For SimVAE, we adopt Gaus- sian p(z|y) as described in §3.4. SimVAE allows J related observations to be simultaneously incorporated. Based on early experiments, we fix the number of augmentations to J = 10 (see Fig. 7 for an ablation). Ablations were per- formed for all sensitive hyperparameters for each method and parameter values were selected based on the best av- erage MLP Acc across datasets. Further details regarding hyperparameters and computational resources can be found in Appendices A.7.4 and A.7.5. 5. Results Content classification: Table 1 reports the downstream classification across datasets using benchmark class labels (i.e., content). Tables 1 and 5 show that SimVAE is compara- ble to or outperforms generative baselines on supervised and unsupervised learning metrics on simple datasets. On nat- 6 A Probabilistic Model to explain Self-Supervised Representation Learning Acc-MP Acc-KNN Random 38.1 ± 3.8 46.1 ± 2.5 SimCLR 97.2 ± 0.0 97.2 ± 0.1 MoCo 94.6 ± 0.4 94.6 ± 0.3 VicREG 96.7 ± 0.0 97.0 ± 0.0 VAE 97.8 ± 0.1 98.0 ± 0.1 β-VAE 98.0 ± 0.0 98.3 ± 0.0 CR-VAE 97.7 ± 0.0 98.0 ± 0.0 MNIST SimVAE 98.4 ± 0.0 98.5 ± 0.0 Random 49.8 ± 0.8 66.5 ± 0.4 SimCLR 74.9 ± 0.2 76.0 ± 0.1 MoCo 71.2 ± 0.1 76.9 ± 0.2 VicREG 73.2 ± 0.1 76.0 ± 0.1 VAE 80.2 ± 0.3 83.7 ± 0.2 β-VAE 82.2 ± 0.1 86.1 ± 0.0 CR-VAE 82.6 ± 0.0 86.4 ± 0.0 FashionMNIST SimVAE 82.1 ± 0.0 86.5 ± 0.0 Acc-MP Acc-KNN Random 83.5 ± 1.0 80.0 ± 0.9 SimCLR 93.7 ± 0.4 91.6 ± 0.3 MoCo 89.7 ± 1.0 88.6 ± 1.0 VicREG 94.7 ± 0.1 92.7 ± 0.4 VAE 89.0 ± 0.5 86.9 ± 0.7 β-VAE 93.4 ± 0.4 92.0 ± 0.1 CR-VAE 93.1 ± 0.4 91.6 ± 0.6 CelebA SimVAE 95.6 ± 0.4 93.2 ± 0.1 Random 16.3 ± 0.4 13.1 ± 0.6 SimCLR 67.4 ± 0.1 64.0 ± 0.0 MoCo 56.4 ± 1.6 54.0 ± 2.0 VicREG 69.7 ± 0.0 68.3 ± 0.0 VAE 30.3 ± 0.4 25.6 ± 0.5 β-VAE 36.6 ± 0.1 28.5 ± 0.1 CR-VAE 36.8 ± 0.0 28.1 ± 0.1 CIFAR10 SimVAE 51.8 ± 0.0 47.1 ± 0.0 Table 1: Top-1% self-supervised Acc (↑) for MNIST, FashionMNIST, CIFAR10, and CelebA (gender classification) using a MLP probe (MP) and k-Nearest Neighbors (KNN) classification methods; We report mean and standard errors over three runs; Bold indicate best scores in each method class: generative (teal), discriminative methods (red). Figure 3: Assessing the information in representations: original images (left cols) and reconstructions from representations learned by generative unsupervised learning (VAE, β-VAE, CR-VAE), generative SSL (our SimVAE) and discriminative SSL (SimCLR, VicREG) on Cifar10 (l), CelebA (r). Discriminative methods lose style information (e.g. orientation/colour). ural image datasets, we observe a significant improvement in performance over all VAE methods including the self- supervised approach, CR-VAE (+2.5% for CelebA, +15% for CIFAR10), supporting H3. Table 1 also shows that representations learned by SimVAE materially reduces the performance gap (∆) with respect to representations learned by popular discriminative methods, by approximately half for CIFAR10 (∆ = 32.8% → ∆ = 17.6%). SimVAE outperforms all baselines on the CelebA dataset. The results in Table 1 thus empirically support H1 and demonstrate that SimVAE achieves self-supervised, particularly where distributions can be well-modelled. Style classification: We further investigate the downstream performance on style-related features (e.g., colour, position, and orientation) using the CelebA multi-attribute dataset. Figure 4 (left) shows that SimVAE outperforms both gener- ative and discriminative baselines on predicting an attribute dependant on style (hair colour). We also explore image reconstructions to gain qualitative insights into the infor- mation captured by the representation. Figures 2 and 3 shows a loss of orientation and colour information in rep- resentations learned by discriminative methods across four datasets. These findings support our hypothesis (H2) that discriminative methods lose style information, which may be important in some downstream tasks, and confirms the ability of generative methods to retain both content and style information (Figure 4, middle). Importantly, Figure 4 (right) shows that SimVAE learns more task-agnostic rep- resentations, surpassing all generative and discriminative baselines on average across the prediction of all (20) CelebA attributes. An overview of downstream performances for individual CelebA features are reported in Appendix A.8. Image Generation: While generative quality is not relevant to our main hypotheses, out of interest and perhaps as a future benchmark, we show randomly generated SimVAE images and generative quality metrics in Appendix A.8. We observe small but significant FID score and reconstruction error improvements relative to previous VAE methods on MNIST, FashionMNIST, CelebA and CIFAR10 datasets. 7 A Probabilistic Model to explain Self-Supervised Representation Learning CelebA Acc-MP Random 56.2 ± 0.8 SimCLR 52.0 ± 0.2 VicREG 52.8 ± 0.4 VAE 64.4 ± 0.3 β-VAE 66.4 ± 0.4 CR-VAE 66.2 ± 0.4 SimVAE 67.5 ± 0.3 Figure 4: Style prediction on CelebA (Acc-MP ↑): (left) hair colour prediction (mean and standard error over three runs, best overall results indicated in bold); (middle) content vs. style prediction (gender vs hair colour), best performance in top-right; (right) Performance increase of SimVAE relative to baselines across all 20 CelebA attributes. 6. Discussion & Conclusion Representations learned by SSL achieve impressive perfor- mance on downstream tasks, nearing that of fully supervised learning (e.g. Chen et al., 2020; Caron et al., 2021; Bardes et al., 2022), and can transfer well between datasets (Erics- son et al., 2022). This performance has spurred considerable interest in understanding the theoretical mechanism behind SSL (§2). Several works aim to explain how auxiliary task performance transfers to a downstream task but have been disproved for typical datasets (Saunshi et al., 2022). It has also been argued that SSL relies on maximising mutual in- formation between views/augmentations (Hjelm et al., 2019) or pulling representations of semantically similar data closer than those of random samples (Wang & Isola, 2020). We propose a hierarchical latent variable model for predic- tive SSL (Fig. 1) in which representations (z) of semanti- cally related data (x) are conditioned on latent variable (y) determining semantic content, and z ∼p(z|y) governs style. We show that our model justifies loss functions of predictive SSL methods, including contrastive learning, by reference to its ELBO; and underpins the common notion that SSL “pulls together” representations of semantically related data and “pushes apart” others. Predictive SSL methods are seen to maximise entropy H[p(z)] in place of the ELBO’s re- construction term, thereby maximising mutual information between representations. This seems intuitive appealing, but in fact causes representations of semantically related data to collapse together and lose information about the data, reducing the generality of representations. Our analysis also justifies the common use of a projection head (§3.3), which itself suggests caution in (over-)analysing representations that are ultimately discarded. We show that fitting the model generatively, as SimVAE, learns representations that perform comparably, or demon- strably reduce the performance gap, to discriminative meth- ods at content prediction, while outperforming where style information is required, taking a step towards task-agnostic representations. SimVAE outperforms previous generative VAE-based approaches, including CR-VAE tailored to SSL. Regarding limitations, a full explanation of SSL should con- nect learning an auxiliary task to downstream task perfor- mance. This requires understanding: (i) what the auxiliary task requires of the encoder; (ii) how the encoder achieves it; and (iii) how (i-ii) relate to the downstream task. Our work tackles (i). It is unclear if we currently have sufficient under- standing of neural networks to resolve (ii), required for (iii) (Saunshi et al., 2022). However, continuity, smoothness and inductive bias of the encoder seem key: as representations of semantically related data come together under the auxiliary task, representations of other related samples, “close” in X, are brought together in Z. By addressing (i), we hope to facilitate future work in (ii) and (iii). Learning representations generatively is currently challeng- ing, but we believe the connection drawn to popular SSL methods in our analysis and supported empirically justifies further research. Particularly given the prospect of uncer- tainty estimation from the posterior, of generating novel samples and of learning task-agnostic representations that disentangle style information rather than lose it (Higgins et al., 2017). Preliminary analysis (c.f., Figure 7 in Ap- pendix A.8) also suggests that SimVAE is less sensitive to hyperparameters, notably in the augmentation strategy. Even while generative representation learning remains a challenge, understanding the mechanism behind SSL offers a guide for designing and interpreting future discriminative methods. Our latent variable model also recasts SSL in line with other latent variable paradigms, such as unsupervised VAEs (Kingma & Welling, 2014) and supervised Variational Classification (Dhuliawala et al., 2023), which may facilitate a principled unified learning regime. 8 A Probabilistic Model to explain Self-Supervised Representation Learning 7. Broader Impact This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here. Specifically, we focus on a theoretical understanding of existing self-supervised learn- ing methods with the potential impact of improving their interpretability and future algorithm design. References Allen, C. and Hospedales, T. Analogies explained: Towards understanding word embeddings. In ICML, 2019. Arandjelovic, R. and Zisserman, A. Look, listen and learn. In ICCV, 2017. Arora, S., Khandeparkar, H., Khodak, M., Plevrakis, O., and Saunshi, N. A theoretical analysis of contrastive unsupervised representation learning. In ICML, 2019. Bachman, P., Hjelm, R. D., and Buchwalter, W. Learning representations by maximizing mutual information across views. In NeurIPS, 2019. Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In NeurIPS, 2020. Balaˇzevi´c, I., Steiner, D., Parthasarathy, N., Arandjelovi´c, R., and H´enaff, O. J. Towards in-context scene under- standing. In NeurIPS, 2023. Balestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar, S., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian, Y., et al. A cookbook of self-supervised learning. arXiv preprint arXiv:2304.12210, 2023. Bardes, A., Ponce, J., and LeCun, Y. Vicreg: Variance- invariance-covariance regularization for self-supervised learning. In ICLR, 2022. Ben-Shaul, I., Shwartz-Ziv, R., Galanti, T., Dekel, S., and LeCun, Y. Reverse engineering self-supervised learning. In NeurIPS, 2023. Caron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep clustering for unsupervised learning of visual features. In ECCV, 2018. Caron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P., and Joulin, A. Unsupervised learning of visual features by contrasting cluster assignments. In NeurIPS, 2020. Caron, M., Touvron, H., Misra, I., J´egou, H., Mairal, J., Bojanowski, P., and Joulin, A. Emerging properties in self-supervised vision transformers. In ICCV, 2021. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. A simple framework for contrastive learning of visual rep- resentations. In ICML, 2020. Chopra, S., Hadsell, R., and LeCun, Y. Learning a sim- ilarity metric discriminatively, with application to face verification. In CVPR, 2005. Cover, T. and Hart, P. Nearest neighbor pattern classification. In IEEE Transactions on Information Theory, 1967. Dhuliawala, S., Sachan, M., and Allen, C. Variational Clas- sification. In TMLR, 2023. Dosovitskiy, A., Springenberg, J. T., Riedmiller, M., and Brox, T. Discriminative unsupervised feature learning with convolutional neural networks. In NeurIPS, 2014. Edwards, H. and Storkey, A. Towards a neural statistician. In ICLR, 2016. Ericsson, L., Gouk, H., Loy, C. C., and Hospedales, T. M. Self-supervised representation learning: Introduc- tion, advances, and challenges. IEEE Signal Processing Magazine, 39(3):42–62, 2022. Gidaris, S., Singh, P., and Komodakis, N. Unsupervised rep- resentation learning by predicting image rotations. arXiv preprint arXiv:1803.07728, 2018. Grill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P., Buchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z., Gheshlaghi Azar, M., et al. Bootstrap your own latent-a new approach to self-supervised learning. In NeurIPS, 2020. Gupta, K., Ajanthan, T., Hengel, A. v. d., and Gould, S. Understanding and improving the role of projec- tion head in self-supervised learning. arXiv preprint arXiv:2212.11491, 2022. Hadsell, R., Chopra, S., and LeCun, Y. Dimensionality reduction by learning an invariant mapping. In CVPR, 2006. HaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable guarantees for self-supervised deep learning with spectral contrastive loss. In NeurIPS, 2021. He, J., Gong, Y., Marino, J., Mori, G., and Lehrmann, A. Variational autoencoders with jointly optimized latent dependency structure. In ICLR, 2018. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In CVPR, 2016. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo- mentum contrast for unsupervised visual representation learning. In CVPR, 2020. 9 A Probabilistic Model to explain Self-Supervised Representation Learning He, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick, R. Masked autoencoders are scalable vision learners. In CVPR, 2022. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, 2017. Higgins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X., Botvinick, M., Mohamed, S., and Lerchner, A. beta- vae: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017. Hjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal, K., Bachman, P., Trischler, A., and Bengio, Y. Learning deep representations by mutual information estimation and maximization. In ICLR, 2019. Karras, T., Laine, S., and Aila, T. A style-based genera- tor architecture for generative adversarial networks. In CVPR, 2019. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In ICLR, 2014. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. LeCun, Y. The mnist database of handwritten digits. http://yann. lecun. com/exdb/mnist/, 1998. Lee, J. D., Lei, Q., Saunshi, N., and Zhuo, J. Predicting what you already know helps: Provable self-supervised learning. In NeurIPS, 2021. Levy, O. and Goldberg, Y. Neural word embedding as implicit matrix factorization. In NeurIPS, 2014. Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. In ICCV, 2015. McAllester, D. and Stratos, K. Formal limitations on the measurement of mutual information. In AIStats, 2020. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. Distributed representations of words and phrases and their compositionality. In NeurIPS, 2013. Misra, I. and Maaten, L. v. d. Self-supervised learning of pretext-invariant representations. In CVPR, 2020. Nakamura, H., Okada, M., and Taniguchi, T. Representa- tion uncertainty in self-supervised learning as variational inference. In ICCV, 2023. Oord, A. v. d., Li, Y., and Vinyals, O. Representation learn- ing with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. Ozsoy, S., Hamdan, S., Arik, S., Yuret, D., and Erdogan, A. Self-supervised learning with an information maximiza- tion criterion. In NeurIPS, 2022. Papyan, V., Han, X., and Donoho, D. L. Prevalence of neural collapse during the terminal phase of deep learn- ing training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020. Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker, G. On variational bounds of mutual information. In ICML, 2019. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Ranganath, R., Tran, D., and Blei, D. Hierarchical varia- tional models. In ICML, 2016. Rolfe, J. T. Discrete variational autoencoders. In ICLR, 2017. Sansone, E. and Manhaeve, R. Gedi: Generative and dis- criminative training for self-supervised learning. arXiv preprint arXiv:2212.13425, 2022. Saunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora, S., Kakade, S., and Krishnamurthy, A. Understanding contrastive learning requires incorporating inductive bi- ases. In ICML, 2022. Shwartz-Ziv, R., Balestriero, R., Kawaguchi, K., Rudner, T. G., and LeCun, Y. An information-theoretic perspec- tive on variance-invariance-covariance regularization. In NeurIPS, 2023. Sinha, A., Song, J., Meng, C., and Ermon, S. D2c: Diffusion-decoding models for few-shot conditional gen- eration. In NeurIPS, 2021. Sinha, S. and Dieng, A. B. Consistency regularization for variational auto-encoders. In NeurIPS, 2021. Sohn, K. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS, 2016. Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and Winther, O. Ladder variational autoencoders. In NIPS, 2016. Song, C., Liu, F., Huang, Y., Wang, L., and Tan, T. Auto- encoder based data clustering. In Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, 2013. Tian, Y. Understanding deep contrastive learning via coordinate-wise optimization. In NeurIPS, 2022. 10 A Probabilistic Model to explain Self-Supervised Representation Learning Tosh, C., Krishnamurthy, A., and Hsu, D. Contrastive learning, multi-view redundancy, and linear models. In Algorithmic Learning Theory, pp. 1179–1206. PMLR, 2021. Tsai, Y.-H. H., Wu, Y., Salakhutdinov, R., and Morency, L.- P. Self-supervised learning from a multi-view perspective. In ICLR, 2020. Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S., and Lucic, M. On mutual information maximization for representation learning. In ICLR, 2020. Valpola, H. From neural pca to deep unsupervised learning. In Advances in Independent Component Analysis and learning machines, pp. 143–171. Elsevier, 2015. Von K¨ugelgen, J., Sharma, Y., Gresele, L., Brendel, W., Sch¨olkopf, B., Besserve, M., and Locatello, F. Self- supervised learning with data augmentations provably isolates content from style. In NeurIPS, 2021. Wang, T. and Isola, P. Understanding contrastive represen- tation learning through alignment and uniformity on the hypersphere. In ICML, 2020. Wang, Y., Zhang, Q., Wang, Y., Yang, J., and Lin, Z. Chaos is a ladder: A new theoretical understanding of con- trastive learning via augmentation overlap. In ICLR, 2021. Wang, Y., Tang, S., Zhu, F., Bai, L., Zhao, R., Qi, D., and Ouyang, W. Revisiting the transferability of supervised pretraining: an mlp perspective. In CVPR, 2022. Wu, C., Pfrommer, J., Zhou, M., and Beyerer, J. Generative- contrastive learning for self-supervised latent represen- tations of 3d shapes from multi-modal euclidean input. arXiv preprint arXiv:2301.04612, 2023. Wu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised fea- ture learning via non-parametric instance discrimination. In CVPR, 2018. Xiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017. Xie, J., Girshick, R., and Farhadi, A. Unsupervised deep embedding for clustering analysis. In ICML, 2016. Xie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q., and Hu, H. Simmim: A simple framework for masked image modeling. In CVPR, 2022. Yang, B., Fu, X., Sidiropoulos, N. D., and Hong, M. To- wards k-means-friendly spaces: Simultaneous deep learn- ing and clustering. In ICML, 2017. Zhang, M., Xiao, T. Z., Paige, B., and Barber, D. Improv- ing vae-based representation learning. arXiv preprint arXiv:2205.14539, 2022. Zimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M., and Brendel, W. Contrastive learning inverts the data generating process. In ICML, 2021. 11 A Probabilistic Model to explain Self-Supervised Representation Learning A. Appendix A.1. Background: Relevant VAE architectures The proposed hierarchical latent variable model for self-supervised learning (Fig. 1) is fitted to the data distribution by maximising the ELBOSSL and can be viewed as a VAE with a hierarchical prior. VAEs have been extended to model hierarchical latent structure (e.g. Valpola, 2015; Ranganath et al., 2016; Rolfe, 2017; He et al., 2018; Sønderby et al., 2016; Edwards & Storkey, 2016), which our work relates to. Notably, Edwards & Storkey (2016) propose the same graphical model as Fig. 1, but methods differ in how posteriors are factorised, which is a key aspect for learning informative representations that depend only on the sample they represent. Wu et al. (2023) and Sinha et al. (2021) combine aspects of VAEs and contrastive learning but do not propose a latent variable model for SSL. Nakamura et al. (2023) look to explain SSL methods via the ELBO, but with a second posterior approximation not a generative model. A.2. Derivation of ELBOSSL and SimVAE Objective Let x = {x1, ..., xJ} be a set of J semantically related samples, ω = {θ, ψ, π} be parameters of the generative model for SSL (Fig. 1) and ϕ parameterise the approximate posterior qϕ(z|x). We derive the Evidence Lower Bound (ELBOSSL) that underpins the training objectives of (projective) discriminative SSL methods and is used to train SimVAE (§3.4). min ω DKL[ p(x|y) ∥ pω(x|y) ] = max ω E x,y \u0002 log pω(x|y) \u0003 = max ω,ϕ E x,y hZ z qϕ(z|x) log pω(x|y) i = max ω,ϕ E x,y hZ z qϕ(z|x) log pθ(x|z)pψ(z|y) pω(z|x,y) qϕ(z|x) qϕ(z|x) i = max ω,ϕ E x,y hZ z qϕ(z|x) log pθ(x|z)pψ(z)|y qϕ(z|x) i + DKL[ qϕ(z|x) ∥ pω(z|x, y) ] ≥ max ω,ϕ E x,y hZ z qϕ(z|x) n log pθ(x|z) qϕ(z|x) + log pψ(z|y) oi (cf Eq. 2) = max ω,ϕ E x,y hX j nZ zj qϕ(zj|xj) log pθ(xj|zj) qϕ(zj|xj) o + Z z qϕ(z|x) log pψ(z|y) i (= Eq. 4) Terms of ELBOSSL are analogous to those of the standard ELBO: reconstruction error, entropy of the approximate posterior H(qϕ(z|x)) and the (conditional) prior. Algorithm 1 provides an overview of the computational steps required to maximise ELBOssl under Gaussian assumptions described in §3.4, referred to as SimVAE. As our experimental setting considers augmentations as semantically related samples, Algorithm 1 incorporates a preliminary step to augment data samples. A.3. Detailed derivation of InfoNCE Objective For data sample xi0 ∈xi0, let x′ i0 ∈xi0 be a semantically related positive sample and {x′ ir}k r=1 be random negative samples. Denote by x = {xi0, x′ i0} the positive pair, by X− ={x′ i1, ..., x′ ik} all negative samples, and by X = x ∪ X− all samples. The InfoNCE objective is derived as follows (by analogy to Oord et al. (2018)). EX \u0002 log p(y = 0|X) \u0003 (predict y = index of positive sample in X−) = EX \u0002 log Z Z p(y = 0|Z)q(Z|X) \u0003 (introduce latent variables Z: Y → Z → X) ≥ EX \u0002 Z Z q(Z|X) log p(y = 0|Z) \u0003 (by Jensen’s inequality) = EX h Z Z q(Z|X) log p(Z′|zi0,y=0)p(y = 0) Pk r=0 p(Z′|zi0,y=r)p(y = r) i (Bayes rule, note p(y=r)= 1 k, ∀r) = EX h Z Z q(Z|X) log p(z′ i0|zi0) Q s̸=0 p(z′ is) Pk r=0 p(z′ ir |zi0) Q s̸=r p(z′ is) i (from sample similarity/independence) = EX h Z Z q(Z|X) log p(z′ i0|zi0)/p(z′ i0) Pk r=0 p(z′ ir |zi0)/p(z′ ir ) i (divide through by Qk s=0 p(z′ is)) 12 A Probabilistic Model to explain Self-Supervised Representation Learning = EX h Z Z q(Z|X) log p(z′ i0,zi0)/p(z′ i0) Pk r=0 p(z′ ir ,zi0)/p(z′ ir ) i (10) The final expression is parameterised using a similarity function sim(z, z′) to give the objective. −LINCE .= EX h Z Z q(Z|X) log sim(z′ i0,zi0) Pk r=0 sim(z′ ir ,zi0) i Oord et al. (2018) show, and Poole et al. (2019) confirm, that this loss is a lower bound on the mutual information, which improves with the number of negative samples k. −LINCE = EX h Z Z q(Z|X) log p(z′ i0,zi0)/p(z′ i0) Pk r=0 p(z′ ir ,zi0)/p(z′ ir ) i (multiply Eq. 10 through by p(zi0)) = EX h Z Z q(Z|X) log p(zi0|z′ i0) − log \u0000p(zi0|z′ i0) + k X r=1 p(zi0|z′ ir) \u0001i = − EX h Z Z q(Z|X) log \u00001 + k X r=1 p(zi0|z′ ir ) p(zi0|z′ i0) \u0001i (divide through by p(zi0|z′ i0)) ≈ − Ex h Z z q(z|x) log \u00001 + (k − 1)Ex′ j \u0002 Z z′ j q(z′ j|x′ j) p(zi0|z′ j) p(zi0|z′ i0) \u0001\u0003i = − Ex h Z z q(z|x) log \u00001 + (k − 1) p(zi0) p(zi0|z′ i0) \u0001i ≤ − Ex h Z z q(z|x) log(k − 1) p(zi0) p(zi0|z′ i0) i = Ex h Z z q(z|x) log p(zi0,z′ i0) p(zi0)p(z′ i0) i + log 1 (k − 1) k→∞ → Ex h Z z qϕ(z|x) \u0000log p(z|yi0) − X j log p(zj i0) \u0001i In the last step, we revert to the terminology used in the main paper for ease of reference. A.4. Derivation of parameter-free p(zi|yi) = s(zi) Instance Discrimination methods consider J =1 sample xi at a time, labelled by its index yi =i, and computes p(xi|y=i; θi) from stored instance-specific parameters θi. This requires parameters proportional to the dataset size, which could be prohibitive, whereas parameter number is often independent of the dataset size, or grows slowly. We show that contrastive methods (approximately) optimise the same objective, but without parameters, and here explain how that is possible. Recall that the “label” i is semantically meaningless and simply identifies samples of a common distribution p(x|y=i) .=p(x|yi). For J ≥ 2 semantically related samples xi = {xj i}J j=1, xj i ∼ p(x|yi), their latent variables are conditionally independent, hence p(zi|yi) = R ψ p(ψi)p(zi|yi; ψi) = R ψ p(ψi) Q j p(zj i |yi; ψi) = s(zi), a function of the latent variables that non- parametrically approximates the joint distribution of latent variables of semantically related data. (Note that unsemantically related data are independent and the joint distribution over their latent variables is a product of marginals). We assume a Gaussian prior p(ψi)=N(ψi; 0, γ2I) and class-conditionals p(zj i |ψi)=N(zj i ; ψi, σ2) (for fixed variance σ2). p(zi|yi) = Z ψi p(zi|ψi)p(ψi) = Z ψi p(ψi) Y j p(zj i |ψi) ∝ Z ψi exp{− 1 2γ2 ψ2 i } Y j exp{− 1 2σ2 (zj i − ψi)2} = Z ψi exp{− 1 2σ2 ( σ2 γ2 ψ2 i + X j (zj i − ψi)2)} 13 A Probabilistic Model to explain Self-Supervised Representation Learning = Z ψi exp{− 1 2σ2 (( X j zj2 i ) − 2( X j zj i )ψi + ( σ2 γ2 + J)ψ2 i )} = Z ψi exp{− σ2/γ2+J 2σ2 (ψi − 1 (σ2/γ2+J) X j zj i )2} + exp{− 1 2σ2 ( X j zj2 i − 1 σ2/γ2+J ( X j zj i )2)} (*) ∝ exp{− 1 2σ2 ( X j zj2 i − 1 σ2/γ2+J ( X j zj2 i + X j̸=k zj i zk i ))} = exp{− 1 2σ2 ((1 − 1 σ2/γ2+J ) X j zj2 i + 1 σ2/γ2+J X j̸=k zj i zk i ))} ∝ exp{− 1 2σ2(σ2/γ2+J) X j̸=k zj i zk i )} (if ∥z∥2 = 1) The result can be rearranged into a Gaussian form (a well known result when all distributions are Gaussian), but the last line also shows that, under the common practice of setting embeddings to unit length (∥z∥2 =1), s(·) can be calculated directly from dot products, or cosine similarities (up to a proportionality constant, which does not affect optimisation). If we instead assume a uniform prior, we can take the limit of the line marked (*) as γ →∞: exp{− 1 2σ2 (( X j zj2 i ) − 1 σ2/γ2+J ( X j zj i )2)} → exp{− 1 2σ2 (( X j zj2 i ) − 1 J ( X j zj i )2)} = exp{− 1 2σ2 (( X j zj2 i ) − J ¯zi 2)} = exp{− 1 2σ2 (( X j zj2 i ) − 2J ¯zi 2 + J ¯zi 2)} = exp{− 1 2σ2 (( X j zj2 i ) − 2¯zi( X j zj i ) + X j ¯zi 2)} = exp{− 1 2σ2 X j (zj2 i − 2zj i ¯zi + ¯zi 2)} = exp{− 1 2σ2 X j (zj i − ¯zi)2} (11) A.5. Relationship between InfoNCE Representations and PMI For data sampled x∼p(x) and augmentations x′ ∼ pτ(x′|x) sampled under a synthetic augmentation strategy, Oord et al. (2018) show that the InfoNCE objective for a sample x is optimised if their respective representations z, z′ satisfy exp{sim(z, z′)} = c p(x,x′) p(x)p(x′), (12) where sim(·, ·) is the similarity function (e.g. dot product), and c is a proportionality constant, specific to x. Since c may differ arbitrarily with x it can be considered an arbitrary function of x, but for simplicity we consider a particular x and fixed c. Further, c > 0 is strictly positive since it is a ratio between positive (exponential) and non-negative (probability ratio) terms. Accordingly, representations satisfy sim(z, z′) = PMI(x, x′) + c′, (13) where c′ = log c ∈ R and PMI(x, x′) is the pointwise mutual information between samples x and x′. Pointwise mutual in- formation (PMI) is an information theoretic term that reflects the probability of events occurring jointly versus independently. For an arbitrary sample and augmentation this is given by: PMI(x, x′) .= log p(x, x′) p(x)p(x′) = logpτ(x′|x) p(x′) . (14) 14 A Probabilistic Model to explain Self-Supervised Representation Learning We note that pτ(x′|x)=0 if x can not be augmented to produce x′; and that, in a continuous domain, such as images, two augmentations are identical with probability zero. Thus augmentations of different samples are expected to not overlap and the marginal is given by p(x′)= R x pτ(x′|x)p(x)=pτ(x′|x∗)p(x∗), where x∗ is the sample augmented to give x′. Thus pτ(x′|x) p(x′) = pτ(x′|x) pτ(x′|x∗)p(x∗) = ( 1/p(x∗) if x∗ =x (i.e. x′ is an augmentation of x) 0 otherwise; (15) and PMI(x, x′) = − log p(x) ≥ k > 0 or PMI(x, x′) = −∞, respectively. Here k=− log arg maxx p(x) is a finite value based on the most likely sample. For typical datasets, this can be approximated empirically by 1 N where N is the size of the original dataset (since that is how often the algorithm observes each sample), hence k = log N, often of the order 5 − 10 (depending on the dataset). If the main objective were to accurately approximate PMI (subject to a constant c′) in Eq. 13, e.g. to approximate mutual information, or if representation learning depended on it, then, at the very least, the domain of sim(·, ·) must span its range of values, seen above as from −∞ for negative samples to a small positive value (e.g. 5-10) for positive samples. Despite this, the popular bounded cosine similarity function (cossim(z, z′) = zT z ||z||2||z′||2 ∈ [−1, 1]) is found to outperform the unbounded dot product, even though the cosine similarity function necessarily cannot span the range required to reflect true PMI values, while the dot product can. This strongly suggests that representation learning does not require representations to specifically learn PMI, or for the overall loss function to approximate mutual information. Instead, with the cosine similarity constraint, the InfoNCE objective is as optimised as possible if representations of a data sample and its augmentations are fully aligned (cossim(z, z′) = 1) and representations of dissimilar data are maximally misaligned cossim(z, z′) = −1, since these minimise the error from the true PMI values for positive and negative samples (described above). Constraints, such as the dimensionality of the representation space vs the number of samples, may prevent these revised theoretical optima being fully achieved, but the loss function is optimised by clustering representations of a sample and its augmentations and spreading apart those clusters. Note that this is the same geometric structure as induced under softmax cross-entropy loss (Dhuliawala et al., 2023). We note that our theoretical justification for representations not capturing PMI is supported by the empirical observation that closer approximations of mutual information do not appear to improve representations (Tschannen et al., 2020). Also, more recent contrastive self-supervised methods increase the cosine similarity between semantically related data but spread apart representation the without negative sampling of InfoNCE, yet outperform the InfoNCE objective despite having no obvious relationship to PMI (Grill et al., 2020; Bardes et al., 2022). A.6. Information Loss due to Representaiton Collapse: a discussion While it may seem appealing to lose information by way of representation collapse, e.g. to obtain representations invariant to nuisance factors, this is a problematic notion from the perspective of general-purpose representation learning, where the downstream task is unknown or there may be many, since what is noise for one task may be of use in another. For example, “blur” is often considered noise, but a camera on an autonomous vehicle may be better to detect blur (e.g. from soiling) than be invariant to it and eventually fail when blurring becomes too severe. We note that humans can observe a scene including many irrelevant pieces of information, e.g. we can tell when an image is blurred or that we are looking through a window, and “disentangle” that from the rest of the image. This suggests that factors can be preserved and disentangled. To stress the point that representation collapse is not desirable in and of itself, we note that collapsing together representations of semantically related data xi would be problematic if subsets xi overlap. For example, in the discrete case of word2vec, words are considered semantically related if they co-occur within a fixed window. Representation collapse, here, would mean that co-occurring words belonging to the same xi would have the same representation, which is clearly undesirable. 15 A Probabilistic Model to explain Self-Supervised Representation Learning A.7. Experimental Details A.7.1. SIMVAE ALGORITHM Algorithm 1 SimVAE Require: data {xk}M k=1; batch size N; data dim D; latent dim L; augmentation set T ; num views J; encoder fϕ; decoder gθ; Var(z|y) σ2; for randomly sampled mini-batch {xk}N k=1 do for augmentation tj ∼ T do xj k = tj(xk); # augment samples µj k, Σj k = fϕ(xj k); # forward pass: z ∼ pϕ(z|x) zj k ∼ N(µj k, Σj k); ˜xj k = gθ(zj k); # ˜x = E[x|z; θ] end for Lk rec = 1 D PJ j=1 ||xj k − ˜xj k||2 2 # minimize loss Lk H = 1 2 PJ j=1 log(|Σj k|) Lk prior = 1 2 PJ j=1 ||(zj k − 1 J PJ j=1 zj k)/σ||2 2 min(PN k=1 Lk rec + Lk H + Lk prior) w.r.t. ϕ, θ by SGD; end for return ϕ, θ; A.7.2. DATASETS MNIST The MNIST dataset (LeCun, 1998) gathers 60’000 training and 10’000 testing images representing digits from 0 to 9 in various caligraphic styles. Images were kept to their original 28x28 pixel resolution and were binarized. The 10-class digit classification task was used for evaluation. FashionMNIST The FashionMNIST dataset (Xiao et al., 2017) is a collection of 60’000 training and 10’000 test images depicting Zalando clothing items (i.e., t-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags and ankle boots). Images were kept to their original 28x28 pixel resolution. The 10-class clothing type classification task was used for evaluation. CIFAR10 The CIFAR10 dataset (Krizhevsky et al., 2009) offers a compact dataset of 60,000 (50,000 training and 10,000 testing images) small, colorful images distributed across ten categories including objects like airplanes, cats, and ships, with various lighting conditions. Images were kept to their original 32x32 pixel resolution. CelebA The CelebA dataset (Liu et al., 2015) comprises a vast collection of celebrity facial images. It encompasses a diverse set of 183’000 high-resolution images (i.e., 163’000 training and 20’000 test images), each depicting a distinct individual. The dataset showcases a wide range of facial attributes and poses and provides binary labels for 40 facial attributes including hair & skin colour, presence or absence of attributes such as eyeglasses and facial hair. Each image was cropped and resized to a 64x64 pixel resolution. Attributes referring to hair colour were aggregated into a 5-class attribute (i.e., bald, brown hair, blond hair, gray hair, black hair). Images with missing or ambiguous hair colour information were discarded at evaluation. All datasets were sourced from Pytorch’s dataset collection. A.7.3. DATA AUGMENTATION STRATEGY Taking inspiration from SimCLR’s (Chen et al., 2020) augmentation strategy which highlights the importance of random image cropping and colour jitter on downstream performance, our augmentation strategy includes random image cropping, random image flipping and random colour jitter. The colour augmentations are only applied to the non gray-scale datasets (i.e., CIFAR10 (Krizhevsky et al., 2009) & CelebA dataset (Liu et al., 2015)). Due to the varying complexity of the datasets we explored, hyperparameters such as the cropping strength were adapted to each dataset to ensure that semantically meaningful features remained after augmentation. The augmentation strategy hyperparameters used for each dataset are detailed in table 2. 16 A Probabilistic Model to explain Self-Supervised Representation Learning Dataset Crop Vertical Flip colour Jitter scale ratio prob. b-s-c hue prob. MNIST 0.4 [0.75,1.3] 0.5 - - - Fashion 0.4 [0.75,1.3] 0.5 - - - CIFAR10 0.6 [0.75,1.3] 0.5 0.8 0.2 0.8 CelebA 0.6 [0.75,1.3] 0.5 0.8 0.2 0.8 Table 2: Data augmentation strategy for each dataset: (left to right) cropping scale, cropping ratio, probability of vertical/horizontal flip, brightness-saturation-contrast jitter, hue jitter, probability of colour jitter A.7.4. TRAINING IMPLEMENTATION DETAILS This section contains all details regarding the architectural and optimization design choices used to train SimVAE and all baselines. Method-specific hyperparameters are also reported below. Network Architectures The encoder network architectures used for SimCLR, MoCo, VicREG, and VAE-based approaches including SimVAE for simple (i.e., MNIST, FashionMNIST ) and complex datasets (i.e., CIFAR10, CelebA) are detailed in Table 3a, Table 4a respectively. Generative models which include all VAE-based methods also require decoder networks for which the architectures are detailed in Table 3b and Table 4b. The latent dimensionality for MNIST and FashionMNIST is fixed at 10 and increased to 64 for the CelebA and CIFAR10 datasets. The encoder and decoder architecture networks are kept constant across methods including the latent dimensionality to ensure a fair comparison. Layer Name Output Size Block Parameters fc1 500 784x500 fc, relu fc2 500 500x500 fc, relu fc3 2000 500x2000 fc, relu fc4 10 2000x10 fc (a) Encoder Layer Name Output Size Block Parameters fc1 2000 10x2000 fc, relu fc2 500 2000x500 fc, relu fc3 500 500x500 fc, relu fc4 784 500x784 fc (b) Decoder Table 3: Multi-layer perceptron network architectures used for MNIST & FashionMNIST training Layer Name Output Block Parameters conv1 32x32 4x4, 16, stride 1 batchnorm, relu 3x3 maxpool, stride 2 conv2 x 32x32 3x3, 32, stride 1 3x3, 32, stride 1 conv3 x 16x16 3x3, 64, stride 2 3x3, 64, stride 1 conv4 x 8x8 3x3, 128, stride 2 3x3, 128, stride 1 conv5 x 4x4 3x3, 256, stride 2 3x3, 256, stride 1 fc 64 4096x64 fc (a) Encoder Layer Name Output Block Parameters fc 256x4x4 64x4096 fc conv1 x 8x8 3x3, 128, stride 2 3x3, 128, stride 1 conv2 x 16x16 3x3, 64, stride 2 3x3, 64, stride 1 conv3 x 32x32 3x3, 32, stride 2 3x3, 32, stride 1 conv4 x 64x64 3x3, 16, stride 2 3x3, 16, stride 1 conv5 64x64 5x5, 3, stride 1 (b) Decoder Table 4: Resnet18 network architectures used for CIFAR10 & CelebA datasets Optimisation & Hyper-parameter tuning All methods were trained using an Adam optimizer until training loss conver- gence. The batch size was fixed to 128. Hyper-parameter tuning was performed based on the downstream MLP classification accuracy across datasets. The final values of hyperparameters were selected to reach the best average downstream perfor- mance across datasets. While we observed stable performances across datasets for the VAE family of models, VicREG and 17 A Probabilistic Model to explain Self-Supervised Representation Learning MoCo, SimCLR is more sensitive, leading to difficulties when having to define a unique set of parameters across datasets. For VAEs, the learning rate was set to 8e−5, and the likelihood probability, p(x|z), variance parameter was set to 0.02 for β-VAE, CR-VAE and SimVAE. CR-VAE’s λ parameter was set to 0.1. SimVAE’s prior probability, p(z|y), variance was set to 0.15 and the number of augmentations to 10. VicREG’s parameter µ was set to 25 and learning rate to 1e-4. SimCLR’s temperature parameter, τ, was set to 0.7 and learning rates were adapted for each dataset due to significant performance variations across datasets ranging from 8e−5 to 1e−3. A.7.5. EVALUATION IMPLEMENTATION DETAILS Following common practices (Chen et al., 2020), downstream performance is assessed using a linear probe, a multi-layer perceptron probe, a k-nearest neighbors (kNN) algorithm, and a Gaussian mixture model (GMM). The linear probe consists of a fully connected layer whilst the mlp probe consists of two fully connected layers with a relu activation for the intermediate layer. Both probes were trained using an Adam optimizer with a learning rate of 3e−4 for 200 epochs with batch size fixed to 128. Scikit-learn’s Gaussian Mixture model with a full covariance matrix and 200 initialization was fitted to the representations using the ground truth cluster number. The kNN algorithm from Python’s Scikit-learn library was used with k spanning from 1 to 15 neighbors. The best performance was chosen as the final performance measurement. No augmentation strategy was used at evaluation. A.7.6. GENERATION PROTOCOL Here we detail the image generation protocol and the quality evaluation of generated samples. Ad-hoc decoder training VAE-based approaches, including SimVAE, are fundamentally generative methods aimed at approximating the logarithm of the marginal likelihood distribution, denoted as log p(x). In contrast, most traditional self-supervised methods adopt a discriminative framework without a primary focus on accurately modeling p(x). However, for the purpose of comparing representations, and assessing the spectrum of features present in z, we intend to train a decoder model for SimCLR & VicREG models. This decoder model is designed to reconstruct images from the fixed representations initially trained with these approaches. To achieve this goal, we train decoder networks using the parameter configurations specified in Tables 3b and 4b, utilizing the mean squared reconstruction error as the loss function. The encoder parameters remain constant, while we update the decoder parameters using an Adam optimizer with a learning rate of 1e−4 until a minimal validation loss is achieved (i.e. ∼ 10-80 epochs). Conditional Image Generation To allow for a fair comparison, all images across all methods are generated by sampling z from a multivariate Gaussian distribution fitted to the training samples’ representations. More precisely, each Gaussian distribution is fitted to z conditioned on a label y. Scikit-Learn Python library Gaussian Mixture model function (with full covariance matrix) is used. A.8. Additional Results & Ablations Content classification evaluation with linear & gaussian mixture model prediction heads Table 5 reports the top-1% self-supervised classification accuracy using a linear prediction head and a gaussian mixture model. From Table 5, we draw similar conclusion as with Table 1: SimVAE significantly bridges the gap between discriminative and generative self-supervised learning methods when considering a supervised linear predictor and fully unsupervised methods for downstream prediction. Table 6 report the normalized mutual information (NMI) and adjusted rank index (ARI) for the fitting of the GMM prediction head. Content & Style classification Figure 5 reports average classification accuracy using a MLP probe (over 3 runs) for the prediction of 20 CelebA facial attributes for SimVAE, generative and discriminative baselines. Augmentation protocol strength ablation Figure 6 reports the downstream classification accuracy across methods for various augmentations stategy. More precisely, we progressively increase the cropping scale and colour jitter amplitude. Unsurprinsingly (Chen et al., 2020), discriminative methods exhibit high sensitivity to the augmentation strategy with stronger disruption leading to improved content prediction. The opposite trend is observed with vanilla generative methods where reduced variability amongst the data leads to increased downstream performance. Interestingly, SimVAE is robust to augmentation protocol and performs comparably across settings. # of augmentation ablation Figure 7 reports the downstream classification accuracy for increasing numbers of augmentations considered simultaneously during the training of SimVAE for MNIST and CIFAR10 datasets. On average, a larger 18 A Probabilistic Model to explain Self-Supervised Representation Learning Figure 5: CelebA 20 facial attributes prediction using a MP. Average scores and standard errors are reported across 3 random seeds. 19 A Probabilistic Model to explain Self-Supervised Representation Learning Acc-LP Acc-GMM Random 39.7 ± 2.4 42.2 ± 1.2 SimCLR 96.8 ± 0.1 83.7 ± 0.6 MoCo 88.6 ± 1.7 70.5 ± 4.0 VicREG 96.7 ± 0.0 79.8 ± 0.6 VAE 97.2 ± 0.2 96.3 ± 0.4 β-VAE 97.8 ± 0.0 96.2 ± 0.2 CR-VAE 97.5 ± 0.0 96.9 ± 0.0 MNIST SimVAE 98.0 ± 0.0 96.6 ± 0.0 Random 51.2 ± 0.6 48.6 ± 0.2 SimCLR 73.0 ± 0.3 53.6 ± 0.3 MoCo 65.0 ± 1.3 56.6 ± 1.1 VicREG 71.7 ± 0.1 60.2 ± 1.1 VAE 79.0 ± 0.5 57.9 ± 0.8 β-VAE 79.6 ± 0.0 68.0 ± 0.3 CR-VAE 79.7 ± 0.0 63.4 ± 0.4 FashionMNIST SimVAE 80.0 ± 0.0 71.1 ± 0.0 Acc-LP Acc-GMM Random 64.4 ± 0.9 59.2 ± 0.3 SimCLR 94.2 ± 0.2 71.6 ± 0.6 MoCo VicREG 94.3 ± 0.3 53.9 ± 0.2 VAE 81.5 ± 1.0 58.8 ± 0.2 β-VAE 81.9 ± 0.2 59.5 ± 0.6 CR-VAE 81.6 ± 0.3 58.9 ± 0.4 CelebA SimVAE 87.1 ± 0.3 58.4 ± 0.6 Random 15.7 ± 0.9 13.1 ± 0.6 SimCLR 65.4 ± 0.1 28.2 ± 0.2 MoCo 53.3 ± 1.3 52.4 ± 0.3 VicREG 68.2 ± 0.0 35.0 ± 2.8 VAE 24.7 ± 0.4 23.4 ± 0.0 β-VAE 26.9 ± 0.0 31.2 ± 0.1 CR-VAE 26.8 ± 0.0 30.3 ± 0.0 CIFAR10 SimVAE 40.1 ± 0.0 39.3 ± 0.0 Table 5: Top-1% self-supervised Acc (↑) for MNIST, FashionMNIST, CIFAR10, and CelebA (gender classification) using a linear probe (LP) and Gaussian Mixture Model (GMM) classification methods; We report mean and standard errors over three runs; Bold indicate best scores in each method class: generative (teal), discriminative methods (red). Dataset VAE β-VAE CR-VAE SimVAE MoCo VicREG SimCLR MNIST ARI 89.0 ± 1.0 93.3 ± 0.3 94.0 ± 0.0 93.1 ± 0.0 58.3 ± 3.8 72.0 ± 0.7 77.4 ± 0.2 NMI 94.9 ± 0.4 96.7 ± 0.2 96.9 ± 0.0 96.6 ± 0.0 71.4 ± 2.5 86.8 ± 0.4 89.6 ± 0.1 Fashion ARI 44.3 ± 0.9 53.3 ± 0.4 47.6 ± 0.4 56.8 ± 0.0 30.9 ± 0.5 41.2 ± 0.5 33.2 ± 0.3 NMI 69.1 ± 0.6 75.6 ± 0.1 72.6 ± 0.1 77.1 ± 0.0 50.4 ± 0.6 66.9 ± 0.3 62.1 ± 0.2 CelebA ARI 5.7 ± 0.2 6.2 ± 0.7 6.6 ± 0.9 2.6 ± 0.7 − 18.7 ± 0.8 0.0 ± 0.1 NMI 3.9 ± 0.2 4.7 ± 0.9 5.0 ± 0.7 2.9 ± 0.7 − 24.3 ± 0.3 0.0 ± 0.0 CIFAR10 ARI 0.6 ± 0.0 2.9 ± 0.1 2.0 ± 0.0 12.2 ± 0.1 27.2 ± 1.0 25.7 ± 0.2 52.2 ± 0.1 NMI 31.7 ± 0.0 33.5 ± 0.1 32.4 ± 0.0 42.8 ± 0.1 16.5 ± 0.4 55.3 ± 0.1 21.7 ± 0.1 Table 6: Normalized mutual information (NMI) and Adjusted Rank Index (ARI) for all methods and datasets; Average scores and standard errors are computed across three runs number of augmentations result in a performance increase. Further exploration is needed to understand how larger sets of augmentations can be effectively leveraged potentially by allowing for batch size increase. From Figure 7, we fix our number of augmentations to 10 across datasets. Likelihood p(x|z) variance ablation We explore the impact of the likelihood, p(x|z), variance, σ2, across each pixel dimension on the downstream performance using the MNIST and CIFAR10 datasets. Figure 8 highlights how the predictive performance is inversely correlated with the σ2 on the variance range considered for the CIFAR10 dataset. A similar ablation was performed on all VAE-based models and led to a similar conclusion. We therefore fixed σ2 to 0.02 for β-VAE, CR-VAE and SimVAE across datasets. Computational Resources Models for MNIST, FashionMNIST and CIFAR10 were trained on a RTX2080ti GPU with 11G RAM. Models for CelebA were trained on an RTX3090 GPU with 24G RAM. We observe that while the family of generative models requires more time per iteration, the loss convergence occurs faster while discriminative methods converge at a slower rate when considering the optimal set of hyperparameters. As a consequence, generative baselines and SimVAE were trained for 400 epochs while discriminative methods were trained for 600 to 800 epochs. Image Generation We report the quality of images generated by SimVAE and considered baselines through visualisations (VAE-based approaches only) and quantitative measurements. 20 A Probabilistic Model to explain Self-Supervised Representation Learning Figure 6: Ablation experiment across the number of augmentations considered during training of the SimVAE model using the MNIST (left) and FashionMNIST (right) datasets. Two, four, six and eight augmentations were considered. The average and standard deviation of the downstream classification accuracy using KNN and GMM probes are reported across three seeds. Generated Images Figure 9 report examples of randomly generated images for each digit class and clothing item using the SimVAE trained on MNIST FashionMNIST, CIFAR10 and CelebA respectively. Generative quality Table 7 reports the FID scores, reconstruction error for all generative baselines and SimVAE. MSE (↓) FID (↓) MNIST VAE 0.029 ± 0.0 150.1 ± 0.2 β-VAE 0.029 ± 0.0 155.3 ± 0.5 CR-VAE 0.030 ± 0.0 153.0 ± 0.9 SimVAE 0.026 ± 0.0 152.7 ± 0.3 Fashion VAE 0.012 ± 0.0 99.4 ± 0.6 β-VAE 0.008 ± 0.0 99.9 ± 0.7 CR-VAE 0.008 ± 0.0 98.7 ± 0.0 SimVAE 0.009 ± 0.0 96.1 ± 1.0 CelebA VAE 0.016 ± 0.0 162.9 ± 2.8 β-VAE 0.005 ± 0.0 163.8 ± 2.3 CR-VAE 0.005 ± 0.0 159.3 ± 5.4 SimVAE 0.004 ± 0.0 157.8 ± 2.3 CIFAR10 VAE 0.008 ± 0.0 365.4 ± 3.3 β-VAE 0.004 ± 0.0 376.7 ± 1.7 CR-VAE 0.004 ± 0.0 374.4 ± 0.4 SimVAE 0.003 ± 0.0 349.9 ± 2.1 Table 7: Generation quality evaluated by: mean squared reconstruction error (RE), fr´echet inception distance (FID). Mean and standard errors are reported across three runs. 21 A Probabilistic Model to explain Self-Supervised Representation Learning Figure 7: Ablation experiment across the number of augmentations considered during training of the SimVAE model using the MNIST (left) and CIFAR10 (right) datasets. Two, four, six, eight and 10 augmentations were considered. The average and standard deviation of the downstream classification accuracy using Linear, MLP probes and a KNN & GMM estimators are reported across three seeds. Batch size of 128 for all reported methods and number of augmentations. Means and standard errors are reported for three runs. Figure 8: Ablation experiment across the likelihood p(x|z) variance considered during training of the SimVAE model using the MNIST (left) and CIFAR10 (right) datasets. The average and standard deviation of the downstream classification accuracy using Linear, MLP probes and a KNN & GMM estimators are reported across three seeds. Means and standard errors are reported for three runs. 22 A Probabilistic Model to explain Self-Supervised Representation Learning Figure 9: Samples generated from SimVAE model using MNIST, FashionMNIST, Cifar10 and CelebA training datasets 23 "
}