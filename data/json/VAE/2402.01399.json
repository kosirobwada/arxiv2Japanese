{
    "optim": "A Probabilistic Model to explain Self-Supervised Representation Learning\nAlice Bizeul 1\nBernhard Sch¨olkopf 2\nCarl Allen 1\nAbstract\nSelf-supervised learning (SSL) learns represen-\ntations by leveraging an auxiliary unsupervised\ntask, such as classifying semantically related sam-\nples, e.g. different data augmentations or modali-\nties. Of the many approaches to SSL, contrastive\nmethods, e.g. SimCLR, CLIP and VicREG, have\ngained attention for learning representations that\nachieve downstream performance close to that of\nsupervised learning. However, a theoretical under-\nstanding of the mechanism behind these methods\neludes. We propose a generative latent variable\nmodel for the data and show that several fami-\nlies of discriminative self-supervised algorithms,\nincluding contrastive methods, approximately in-\nduce its latent structure over representations, pro-\nviding a unifying theoretical framework. We also\njustify links to mutual information and the use of\na projection head. Fitting our model generatively,\nas SimVAE, improves performance over previous\nVAE methods on common benchmarks (e.g. Fash-\nionMNIST, CIFAR10, CelebA), narrows the gap\nto discriminative methods on content classifica-\ntion and, as our analysis predicts, outperforms\nthem where style information is required, taking\na step toward task-agnostic representations.\n1. Introduction\nIn self-supervised learning (SSL), a model is trained on an\nauxiliary task without class labels, and learns representa-\ntions of the data in the process. Of the many approaches to\nSSL (Ericsson et al., 2022; Balestriero et al., 2023), recent\ncontrastive methods, such as InfoNCE (Oord et al., 2018),\nSimCLR (Chen et al., 2020), DINO (Caron et al., 2021) and\nCLIP (Radford et al., 2021), have gained attention for their\nremarkable performance on downstream tasks, approaching\nthat of supervised learning. These methods exploit seman-\n1Department of Computer Science & ETH AI Center,\nETH Zurich, Z¨urich\n2Max Planck Institute for Intelligent\nSystems, T¨ubingen.\nCorrespondence to:\nAlice Bizeul <al-\nice.bizeul@inf.ethz.ch>.\nPreliminary work. Copyright 2024 by the author(s).\ntically related observations, such as different parts (Oord\net al., 2018; Mikolov et al., 2013), augmentations (Chen\net al., 2020; Misra & Maaten, 2020), or modalities/views\n(Baevski et al., 2020; Radford et al., 2021; Arandjelovic\n& Zisserman, 2017) of the data. There is significant inter-\nest in understanding self-supervised methods (e.g. Wang\n& Isola, 2020; Zimmermann et al., 2021; Tian, 2022; Ben-\nShaul et al., 2023), but a general mathematical mechanism\njustifying their performance remains unclear. At the same\ntime, theoretically principled representations can be learned\ngeneratively by fitting a latent variable model by variational\ninference (e.g. Kingma & Welling, 2014), but they typically\nunder-perform recent (discriminative) SSL methods.\nTo address this, we draw an equivalence between discrim-\ninative and generative representation learning that treats\nFigure 1:\nGen-\nerative model for\nSSL (J semanti-\ncally related sam-\nples,\nparameters\nexplained in Eq. 4)\nrepresentations as latent variables and\nan encoder f : X → Z (mapping\ndata x to representations z = f(x))\nas a special case of the posterior\np(z|x). We then propose a genera-\ntive model for self-supervised learn-\ning (Fig. 1) and by considering its\nevidence lower bound (ELBO), show\nthat families of discriminative SSL\nmethods, including the InfoNCE ob-\njective adopted by several algorithms\n(e.g. Chen et al., 2020; Radford et al.,\n2021), induce similar latent struc-\nture. Notably, the common percep-\ntion that self-supervised learning ob-\njectives “pull together” representa-\ntions of semantically related data and\n“push apart” others (e.g. Wang & Isola,\n2020) is explained from first princi-\nples by this model: the prior pulls and\nthe reconstruction term pushes.\nUnder the proposed model, representations (z) of semanti-\ncally related data (x) form clusters conditioned on shared\nlatent content (y), with variation in z|y considered style.\nOur analysis predicts that these clusters form under previ-\nous discriminative SSL methods, but “collapse”, losing style\ninformation that distinguishes related samples, e.g. colour,\norientation. Any information loss may affect some down-\nstream tasks more than others, limiting the generality of\n1\narXiv:2402.01399v1  [cs.LG]  2 Feb 2024\nA Probabilistic Model to explain Self-Supervised Representation Learning\nFigure 2: Assessing the information in representations: original images (left cols) and reconstructions from representations\nlearned by generative unsupervised learning (VAE, β-VAE, CR-VAE), generative SSL (our SimVAE) and discriminative SSL\n(SimCLR, VicREG) on MNIST (l), Fashion MNIST (r). Discriminative methods lose style information (e.g. orientation).\nrepresentations. To verify this, we fit the latent variable\nmodel generatively by maximising the ELBO, termed Sim-\nVAE,1 to explicitly induce the proposed latent structure.\nWhile training a generative model is more challenging than\ntraining a discriminative one in general, our results show that\nSimVAE is competitive with or outperforms popular discrim-\ninative methods at downstream classification on common\nbenchmark datasets (MNIST, FashionMNIST and CelebA).\nOn more complex data (CIFAR10), SimVAE is less compet-\nitive for content classification but, as predicted by our analy-\nsis above, consistently outperforms discriminative methods\non tasks requiring style information. Notably, SimVAE sig-\nnificantly outperforms other VAE-based generative models\non all tasks. These results provide empirical support for\nthe generative model as a mathematical basis for SSL; and\nsuggest that generative representation learning is promising\nif distributions can be well modelled, particularly given its\nadded benefits of uncertainty estimation from the posterior,\na means to generate synthetic samples and to qualitatively\nassess the information captured by inspecting reconstruc-\ntions and naturally model groups (Fig. 2).\nTo summarise our main contributions:\n• we propose a latent variable model that underpins and\nunifies popular families of self-supervised learning algo-\nrithms, including contrastive methods (Fig. 1);\n• we show that this model underpins the notion that SSL\n“pulls together”/“pushes apart” representations, ratio-\nnalises the link to mutual information and justifies the\ncommon use of a projection head (§3);\n• we fit the latent variable model generatively, termed Sim-\nVAE, and show clear improvement (>15% on CIFAR10)\non benchmark (content) classification tasks over previous\ngenerative (VAE-based) methods, including one tailored\nto SSL (Sinha & Dieng, 2021) (§5); and\n• we show that SimVAE captures more style information\nthan discriminative methods as our analysis predicts (§5).\n1So-named as it encodes the latent structure of SimCLR (Chen\net al., 2020) in the prior of a VAE (Kingma & Welling, 2014).\n2. Background and Related Work\nRepresentation Learning aims to learn an encoder f : X →\nZ that maps data x∈X to (often lower-dimensional) repre-\nsentations z =f(x)∈Z that perform well on downstream\ntasks. Representation learning is not “well defined” in the\nsense that downstream tasks can be arbitrary and a represen-\ntation that performs well on one task may perform poorly\non another (Zhang et al., 2022). For instance, unsupervised\nimage representations are commonly evaluated by predict-\ning semantic class labels, but the downstream task could\nbe to detect lighting, position or orientation, which a repre-\nsentation useful for class prediction may not capture. This\nsuggests that general-purpose unsupervised representations\nshould capture as much information about the data as possi-\nble. Recent works support this by evaluating on a variety of\ndownstream tasks (e.g. Balaˇzevi´c et al., 2023).\nSelf-Supervised Learning (SSL) learns representations by\nleveraging an auxiliary task. The many approaches to SSL\ncan be categorised in various ways (e.g. Balestriero et al.,\n2023). We define those we focus on as follows:\nInstance Discrimination (Dosovitskiy et al., 2014; Wu et al.,\n2018) treats each sample xi, along with any augmentations,\nas a distinct class i. A softmax classifier is trained to predict\nthe “class” and encoder outputs are used as representations.\nLatent Clustering performs clustering on representations.\nSong et al. (2013); Xie et al. (2016); Yang et al. (2017)\napply K-means or similar to the hidden layer of a standard\nauto-encoder. DeepCluster (Caron et al., 2020) iteratively\nclusters ResNet representations by K-means, and predicts\nthe cluster assignments as “pseudo-labels”. DINO (Caron\net al., 2021), a transformer-based model, can be interpreted\nas clustering in the latent space (Balestriero et al., 2023).\nContrastive Learning encourages representations of seman-\ntically related data (positive samples) to be “close” in con-\ntrast to those sampled at random (negative samples). Early\nSSL approaches include energy-based models (Chopra et al.,\n2005; Hadsell et al., 2006); and word2vec (Mikolov et al.,\n2013) that predicts co-occurring words, thereby capturing\ntheir pointwise mutual information (PMI) in its embed-\ndings (Levy & Goldberg, 2014; Allen & Hospedales, 2019).\n2\nA Probabilistic Model to explain Self-Supervised Representation Learning\nInfoNCE (Oord et al., 2018; Sohn, 2016) extends word2vec\nto other data domains. Its widely used objective, for a pos-\nitive pair of semantically related samples (x, x+) and ran-\ndomly selected negative samples X−={x−\nk }K\nk=1, is defined\nLINCE(x, x+, X−) = log\ne sim(z,z+)\nP\nx′∈{x+}∪X− e sim(z,z′) ,\n(1)\nwhere sim(·, ·) is a similarity function, e.g. dot product. Eq.\n1 is minimised if sim(z, z′) = PMI(x, x′)+c, for constant\nc (Oord et al., 2018). Many works build on InfoNCE, e.g.\nSimCLR (Chen et al., 2020) uses synthetic augmentations\nand CLIP (Radford et al., 2021) uses different modalities\nas positive samples; DIM (Hjelm et al., 2019) takes other\nencoder parameters as representations; and MoCo (He et al.,\n2020), BYOL (Grill et al., 2020) and VicREG (Bardes et al.,\n2022) find alternative strategies to negative sampling to\nprevent representations from collapsing.\nWe do not address all SSL algorithms in this work, in par-\nticular those with regression-based auxiliary tasks such as\nreconstructing data from perturbed versions (e.g. He et al.,\n2022; Xie et al., 2022); or predicting perturbations, such\nas rotation angle (e.g. Gidaris et al., 2018). For clarity, we\nrefer to the methods we address as Predictive SSL.\nVariational Auto-Encoder (VAE): For a generative model\nz → x, parameters θ of pθ(x) =\nR\nz pθ(x|z)pθ(z) can be\nlearned by maximising the evidence lower bound (ELBO)\nEx\n\u0002\nlog pθ(x)\n\u0003\n≥ Ex\n\u0002Z\nz\nqϕ(z|x) log pθ(x|z)p(z)\nqϕ(z|x)\n\u0003\n,\n(2)\nwhere qϕ(z|x) learns to approximate the posterior pθ(z|x).\nLatent variables z can be used as representations (see §3.1).\nA VAE (Kingma & Welling, 2014) maximises the ELBO,\nwith pθ, qϕ modelled as Gaussians parameterised by neural\nnetworks. A β-VAE (Higgins et al., 2017) weights ELBO\nterms to increase disentanglement of latent factors. A CR-\nVAE (Sinha & Dieng, 2021) considers semantically related\nsamples through an additional regularisation term. Further\nrelevant VAE variants are summarised in Appendix A.1.\nVariational Classification (VC): Dhuliawala et al. (2023)\ndefine a latent variable model for classification of labels y,\np(y|x)=\nR\nz q(z|x) p(z|y)p(y)\np(z)\n, that generalises softmax neu-\nral network classifiers, interpreting the encoder as param-\neterising q(z|x); and the softmax layer as encoding p(y|z)\nby Bayes’ rule (VC-A). For data in continuous domains X,\ne.g. images, q(z|x) of a softmax classifier can overfit to a\nsingle delta-distribution for all samples of a class, meaning\nrepresentations of a class collapse together (termed neural\ncollapse by Papyan et al., 2020).2 This loses semantic and\nprobabilistic information distinguishing class samples and\nharms properties such as calibration and robustness (VC-B).\n2In practice, constraints such as l2 regularisation and early\nstopping arbitrarily restrict the described optimum being attained.\nPrior theoretical analysis: There has been considerable in-\nterest in understanding the mathematical mechanism behind\nself-supervised learning (Arora et al., 2019; Tsai et al., 2020;\nWang & Isola, 2020; Zimmermann et al., 2021; Lee et al.,\n2021; Von K¨ugelgen et al., 2021; HaoChen et al., 2021;\nWang et al., 2021; Saunshi et al., 2022; Tian, 2022; Sansone\n& Manhaeve, 2022; Nakamura et al., 2023; Shwartz-Ziv\net al., 2023; Ben-Shaul et al., 2023), as summarised by\nHaoChen et al. (2021) and Saunshi et al. (2022). A thread of\nworks (Arora et al., 2019; Tosh et al., 2021; Lee et al., 2021;\nHaoChen et al., 2021; Saunshi et al., 2022) aims to prove\nthat auxiliary task performance translates to downstream\nclassification accuracy, but Saunshi et al. (2022) show this\ncan not apply to typical datasets, and model architecture\nmust be considered. Several works propose an informa-\ntion theoretic basis for SSL (Hjelm et al., 2019; Bachman\net al., 2019; Tsai et al., 2020; Shwartz-Ziv et al., 2023),\ne.g. maximising mutual information between representa-\ntions, but Tschannen et al. (2020); McAllester & Stratos\n(2020); Tosh et al. (2021) raise doubts with this. We show\nthat the relationship to mutual information is justified more\nfundamentally by our latent variable model (Fig. 1).\n3. Self-Supervised Representation Learning\nWe consider datasets X = SN\ni=1 xi, where each subset\nxi ={xj\ni}j contains semantically related data xj\ni ∈X, con-\nsidered to share semantic content and vary in style.3 For\nexample, xi may contain different augmentations, modali-\nties or snippets of a given observation (e.g. an image of a\ntriangle and its mirror image, where the shape is content\nand orientation is style). We assume a generative process\nwhere yi ∼p(y) is sampled, determining semantic content,\nthen zi = {zj\ni }j are sampled conditionally independently\nzj\ni ∼p(z|yi), reflecting the same content but varying in style.\nFinally data points xj\ni ∈xi are sampled xj\ni ∼p(x|zj\ni ). Fig. 1\nshows the generative model for SSL, under which\np(xi|yi) =\nZ\nzi\n\u0010Y\nj\np(xj\ni|zj\ni )\n\u0011\u0010Y\nj\np(zj\ni |yi)\n\u0011\n.\n(3)\nNote that yi is not semantically meaningful, only an observe\nindicator reflecting that xj\ni ∈xi are related. If distributions\nin Eq. 3 are modelled parametrically, their parameters can\nbe learned by maximising ELBOSSL, a lower bound on\nlog p(x|y) for J semantically related data x={xj}J\nj=1 ⊆xi,\nEx,y\nh\nJ\nX\nj=1\nZ\nzjqϕ(zj|xj)\n\u0000log pθ(xj|zj) − log qϕ(zj|xj)\n\u0001\n+\nZ\nz\nqϕ(z|x)log pψ(z|y)\ni\n,\n(4)\nwhere the approximate posterior is assumed to factorise as\nq(z|x) ≈ Q q(zj|xj).4 A derivation of Eq. 4 is given in\n3|xi| may vary and domains X j ∋xj\ni can differ with modality.\n4Expected to be reasonable for zj that carry high information\n3\nA Probabilistic Model to explain Self-Supervised Representation Learning\nAppendix A.2. As in the generic ELBO (Eq. 2), ELBOSSL\ncomprises: a reconstruction term, the approximate posterior\nentropy and the (now conditional) log prior over all zj ∈z.\nNote that if all p(z|yi) are concentrated relative to p(z), i.e.\nVar[z|yi]≪Var[z], then latent variables of semantically re-\nlated data z∈ zi are clustered, or closer on average than\nthose of random samples – as SSL representations are often\ndescribed. Indeed, our claim is that the generative model in\nFig. 1, fitted by maximising Eq. 4, underpins discriminative\nself-supervised learning algorithms, including instance dis-\ncrimination, deep clustering and contrastive methods (pre-\ndictive SSL, §2). We first clearly define how discriminative\nand generative methods relate and, from that, show that the\nloss functions of discriminative methods reflect Eq. 4.\n3.1. Discriminative vs Generative Learning\nRepresentation learning aims to train an encoder f : X →Z\nso that representations z = f(x) are arranged usefully for\ndownstream tasks, e.g. clustered so that a class of interest is\neasily identified. The predictive self-supervised approaches\nwe consider (§2) train f under a loss function without a gen-\nerative model, so are discriminative. Representations can\nalso be learned generatively: if latent variables z, under a\ngenerative model z→x, determine underlying properties of\nthe data x, then the posterior p(z|x) is expected to estimate\nsemantic properties of x, so approximating it (by q(z|x)\nparameterised by f) gives semantically meaningful repre-\nsentations. These approaches are actually closely related:\nGen-Discrim Equivalence:\nA deterministic encoder f\ncan be viewed as a concentrated posterior distribution\npf(z|x)=δz−f(x). Together with p(x), this defines the joint\npf(x, z) = pf(z|x)p(x) and marginal pf(z) .=\nR\nx pf(x, z).\nThus, an encoder can be trained to map the data distribu-\ntion to a “useful” target distribution (or distribution family)\np∗(z):\n• generatively by optimising the ELBO with prior p∗(z); or\n• discriminatively if there exists a loss function that when\noptimised induces pf(z)≈p∗(z).\nThe takeaway is that both approaches have an equivalent\naim. In particular, our claim can now be restated as: pre-\ndictive SSL methods (§2) induce a distribution pf(z) that\napproximates p(z) under the graphical model in Fig. 1.\n3.2. Inducing p∗(z): discriminatively or generatively?\nIf an encoder can be trained to give a useful distribution\np∗(z) over representations discriminatively or generatively\nthen, all else equal, the former may seem preferable to avoid\nmodelling p(x|z). However, while the ELBO gives a prin-\ncipled generative objective to learn representations that fit\np∗(z), a principled discriminative loss that minimises with\nw.r.t. xj, such that observing related xk or its representation zk pro-\nvides negligible extra information, i.e. p(zj|xj, xk)≈p(zj|xj).\npf(z)=p∗(z) is less clear. We therefore consider the effect\neach ELBO term has on the optimal posterior/encoder to\nunderstand what a discriminative, or “p(x|z)-free”, objec-\ntive needs to achieve to induce a given distribution over\nrepresentations.\nEntropy: discriminative methods use a deterministic map-\nping q(z|x)=δz−f(x). We treat this as a posterior with very\nsmall but non-zero fixed variance, hence H[q] is a constant\nand dropped from the SSL objective.\nPrior: this term is optimal w.r.t. q iff related samples x∈xi\neach map to a mode of p(z|yi). For uni-modal p(z|yi) this\nmeans all representations z ∈zi “collapse” to a point, losing\ninformation distinguishing x∈xi.5\nReconstruction: this term is maximised w.r.t. q iff each x\nmaps to a distinct representation z that p(x|z) maps back to\nx. This requires all z to be distinct and all information to be\nretained, countering the prior to avoid collapse.\nIn summary, we consider a discriminative objective to emu-\nlate the ELBO if it combines the log prior with a p(x|z)-free\nsubstitute for the reconstruction term (denoted Rec(·)) that\nshould, inter alia, avoid representation collapse both be-\ntween and within subsets zi, i.e.\nZ\nz\nq(z|x)(log p(z|y) + Rec(z, x)) .\n(5)\nFrom the analysis above, maximising Eq. 5 can be seen to\nfit the notion that SSL objectives “pull together” representa-\ntions of related data and “push apart” others (e.g. Wang &\nIsola, 2020), the prior pulls (representations are attracted to\nmodes) and the reconstruction pushes or “avoids collapse”\n(representations are made distinct). Our claim is that Eq. 5,\nand by extension ELBOSSL, is the rationale for that notion,\ni.e. SSL objectives of the form of Eq. 5 emulate ELBOSSL\nand so approximately induce the latent structure p(z) in\nFig. 1. The true reconstruction term not only avoids col-\nlapse but is also integral to modelling the data distribution\nand posterior to give meaningful representations. As we\nshall see, approximation by Rec(·) can impact representa-\ntions and their performance in downstream tasks.\n3.3. Discriminative Self-Supervised Learning\nWe now consider examples of instance discrimination, la-\ntent clustering and contrastive SSL methods (§2) and their\nrelationship to the latent variable model in Fig. 1 via Eq. 5.\nInstance Discrimination (ID) (Dosovitskiy et al., 2014;\nWu et al., 2018) trains a softmax classifier on sample-index\npairs {(xj\ni, yi =i)}i,j, which VC-A (Dhuliawala et al., 2023,\n§2) interprets, under Fig. 1, as maximising the RHS of:\nlog p(y|x) ≥\nZ\nz\nqϕ(z|x) log p(y|z)\n(6)\n5Assuming classes xi are distinct, as is the case for empirical\nself-supervised learning datasets of interest.\n4\nA Probabilistic Model to explain Self-Supervised Representation Learning\n=\nZ\nz\nqϕ(z|x)(log p(z|y) − log p(z)) + c. (7)\nEq. 7 matches Eq. 5 with J =1 and Rec(·) = H[p(z)], the\nentropy of p(z). Intuitively, maximising entropy helps avoid\ncollapse, but under a softmax loss, while representations\nof distinct classes do spread apart, those of the same class\nz ∈zi collapse together (VC-B, Dhuliawala et al., 2023).\nDeep Clustering (DC) (Caron et al., 2018) iteratively as-\nsigns temporary labels yi to data xi by (K-means) clus-\ntering representations of a ResNet encoder and updating\nthe encoder to predict those labels with a softmax head.\nWhile semantically related subsets xi are now defined by\nthe ResNet’s “inductive bias”, the same loss is used as in ID\n(Eq. 7) and representation clusters z ∈zi collapse together.\nContrastive Learning stems from the InfoNCE objective\n(Eq 1). A variational lower bound under Fig. 1 (comparable\nto Eq. 6) is optimised when sim(z, z′)=PMI(z, z′)+c (§2).\nCombining this, we see that InfoNCE maximises:\nLNCE(xi, X′ ={x+\ni , x−\ni1, ..., x−\nik})\n= EX\nh Z\nz\nqϕ(Z|X) log\nsim(zi,z+\ni )\nP\nz′∈Z′ sim(zi,z′)\ni\nopt\n→ EX\nh Z\nZ\nqϕ(Z|X) log\np(z+\ni |zi)/p(z+\ni )\nP\nz′∈Z′ p(z′|zi)/p(z′)\ni\n≤ Ex\nh Z\nz\nqϕ(z|x) log\np(zi,z+\ni )\np(zi)p(z+\ni )\ni\n+ log\n1\nk − 1\nk→∞\n→ Ex\nh Z\nz\nqϕ(z|x)(log p(z|yi) −\nX\nj\nlog p(zj\ni ))\ni\n(8)\n(A full derivation is given in Appendix A.3.) Thus the\nInfoNCE objective again approximates ELBOSSL using\nentropy Rec(·)=H[p(z)], as in Eq. 7 but with J =2.\nRole of J: ID and DC consider J = 1 sample xj\ni at a time\nand compute p(zi|yi; ψi) from saved parameters ψi (which\ncould be memory intensive). For J ≥2 samples, one can es-\ntimate p(zi|yi)=\nR\nψi p(ψi) Q\nj p(zj\ni |yi; ψi)=s(zi), where\nψi integrates out (see Appendix A.4 for an example). Since\nyi now defines no parameters, joint distributions over {zir}r\ndepend only on whether {xir}r are semantically related:\np(zi1, ..., zik)=\n(\ns(zi)\nif ir =i ∀r (i.e. xir∈xi)\nQk\nr=1 p(zir)\nif ir ̸=is ∀r, s\nInfoNCE implicitly applies a similar “trick” with sim(·, ·).\nMutual Information (MI): InfoNCE is known to optimise\na lower bound on MI, I(x, x′) = E[log\np(x,x′)\np(x)p(x′)] (Oord\net al., 2018) and Eq. 8 shows it also lower bounds I(z, z′).\nAs a result, maximising MI has at times been argued to\nunderpin contrastive learning (Hjelm et al., 2019; Ozsoy\net al., 2022). However, our analysis suggests that MI is\nnot the fundamental explanation for contrastive learning,\nrather MI arises from substituting the reconstruction term in\nELBOSSL. This is supported by Tschannen et al. (2020);\nMcAllester & Stratos (2020); Poole et al. (2019) who show\nthat better MI estimators do not give better representations,\nMI approximation is noisy and the InfoNCE estimator is ar-\nbitrarily upper-bounded. Further, for disjoint xi, pointwise\nmutual information (PMI) values span an unbounded range,\n[−∞, k], k>1, yet are commonly modelled by the bounded\ncosine similarity sim(z, z′) =\nz⊤z′\n∥z∥∥z′∥ ∈ [−1, 1] (e.g. Chen\net al., 2020), making MI estimation worse (see §A.5). In\nAppendix A.5, we show that this common constraint leads\nto representations comparable to those learned by softmax\ncross entropy (cf ID), but without class parameters ψi.6\nNotably, representations of related data again collapse.\nSummary: We have considered discriminative SSL meth-\nods from instance discrimination, latent clustering and con-\ntrastive learning, and shown that, despite considerable differ-\nences, each approximates ELBOSSL and so approximately\ninduces the prior of our model for SSL (Fig. 1). The number\nof SSL methods, even within the predictive subset we focus\non, makes an exhaustive analysis infeasible. However, many\nmethods adopt or approximate aspects analysed above. For\nexample, SimCLR (Chen et al., 2020) and CLIP (Radford\net al., 2021) use the InfoNCE objective. MoCo (He et al.,\n2020), BYOL (Grill et al., 2020) and VicREG (Bardes et al.,\n2022) replace negative sampling with a momentum encoder,\nstop gradients or (co-)variance terms, but nonetheless “pull\ntogether” representations of semantically related data mod-\nelling the prior; and “push apart” others, or “avoid collapse”,\nby those mechanisms that mimic the effect of reconstruc-\ntion. DINO (Caron et al., 2021) assigns representations of\nsemantically related data pairs to a common cluster as in\nDC (Balestriero et al., 2023), but with J =2.\nThe predictive SSL methods analysed above each replace\nthe reconstruction term of ELBOSSL with entropy H[p(z)].\nAs such, representations of semantically related data form\ndistinct clusters, but those clusters collapse, losing style\ninformation since representations z ∈zi become indistin-\nguishable (e.g. to a downstream classifier). Style informa-\ntion is important in many real-world tasks, e.g. detecting\nfacial expression or voice sentiment; and is the focus of\nrepresentation learning elsewhere (e.g. Higgins et al., 2017;\nKarras et al., 2019). Thus, counter to general representa-\ntion learning, discriminative SSL may over-fit to content-\nbased tasks (see Appendix A.6 for further discussion.)\nProjection Head: Our analysis suggests a plausible expla-\nnation for the practice of adding layers, or a projection head,\nto an encoder and using its input as representations rather\nthan its output z used in the loss function. Since the ELBO\nprevents both inter- and intra-cluster collapse, any approxi-\n6We note that Wang & Isola (2020) reach a similar conclusion\nbut less rigorously, e.g. the known PMI minimiser is not addressed.\n5\nA Probabilistic Model to explain Self-Supervised Representation Learning\nmation should also. Near-final encoder layers are found to\nexhibit similar clustering to z, but with higher intra-cluster\nvariance (Gupta et al., 2022), as also seen in supervised soft-\nmax classification (Wang et al., 2022). We conjecture that\nrepresentations from near-final layers are preferable to z\nbecause their intra-cluster variance gives an overall distribu-\ntion closer to that under Fig. 1. Conversely, the simple fact\nthat any other representations outperform z suggests that\ndiscriminative objectives do not learn ideal representations,\nsupporting our claim that they are an approximation.\n3.4. Generative Self-Supervised Learning (SimVAE)\nThe proposed model for SSL (Fig. 1) has been shown to\njustify: (i) the training objectives of predictive SSL meth-\nods; (ii) the notion that SSL “pulls together”/“pushes apart”\nrepresentations; (iii) the connection to mutual information;\nand (iv) the use of a projection head. We now aim to add em-\npirical support by validating the following predictions: [H1]\nmaximising ELBOSSL achieves self-supervised learning,\nwhere distributions can be well modelled; [H2] maximising\nELBOSSL retains more style information than discrimina-\ntive objectives; [H3] the generative model (Fig. 1) gives\nbetter representations than a range of VAE alternatives.\nTo test these hypotheses, we maximise ELBOSSL (Eq. 4),\neffectively taking a generative approach to self-supervised\nlearning, termed SimVAE. SimVAE can be considered a VAE\nwith a mixture prior p(z)=P\ny p(z|y)p(y), where represen-\ntations z ∈z of semantically related samples are conditioned\non the same y. We assume that p(x|z) and q(z|x) of Eq. 4\nare Gaussians parameterised by neural networks, as in a stan-\ndard VAE; that conditionals p(z|y = i; ψi) = N(z; ψi, σ2)\nare Gaussian with small fixed variance σ2; and p(ψ) is\nuniform (over a suitable space). Integrating out ψ (as in\nAppendix A.4) gives:\np(z|y) ∝ exp{−\n1\n2σ2\nX\nz∈z\n(z − ¯z)2} ,\n(9)\na Gaussian centred around the mean representation ¯z =\n1\nJ\nP\nj zj. Maximising this, as a component of ELBOSSL,\n“pinches together” representations of semantically related\nsamples. Whereas contrastive methods typically compare\npairs of related representations (i.e. J = 2), Eq. 4 allows\nany number J to be processed. In practice a balance is\nstruck between better approximating p(z|y) and preserving\ndiversity in a mini-batch. Algorithm 1 in Appendix A.7.1\ndetails the steps to optimise Eq. 4 under these assumptions.\n4. Experimental Setup\nDatasets and Evaluation Metrics We test our hypotheses\nby evaluating SimVAE representations on four benchmark\ndatasets including two with natural images: MNIST (LeCun,\n1998), FashionMNIST (Xiao et al., 2017), CelebA (Liu et al.,\n2015) and CIFAR10 (Krizhevsky et al., 2009). We augment\nimages following the SimCLR protocol (Chen et al., 2020)\nwhich includes cropping and flipping, and colour jitter for\nnatural images. Frozen pre-trained representations are eval-\nuated by (unsupervised) clustering under a gaussian mixture\nmodel and (supervised) training a non-linear MLP probe, a\nk-nearest neighbors (kNN) estimator (Cover & Hart, 1967)\nand a linear probe on classification tasks (Chen et al., 2020;\nCaron et al., 2020). Downstream performance is measured\nin terms of classification accuracy (Acc). Generative quality\nis evaluated by FID score (Heusel et al., 2017) and recon-\nstruction error (see Appendix A.8). For further experimental\ndetails see Appendices A.7.2, A.7.3, A.7.6 and A.8.\nBaselines methods We compare SimVAE to other VAE-\nbased models including the vanilla VAE (Kingma & Welling,\n2014), β-VAE (Higgins et al., 2017) and CR-VAE (Sinha &\nDieng, 2021), as well as to state-of-the-art self-supervised\ndiscriminative methods including SimCLR (Chen et al.,\n2020), VicREG (Bardes et al., 2022), and MoCo (He et al.,\n2020). As a lower bound, we also provide results for a ran-\ndomly initialized encoder. For fair comparison, the augmen-\ntation strategy, representation dimensionality, batch size,\nand encoder-decoder architectures are invariant across meth-\nods. To enable a qualitative comparison of representations,\ndecoder networks were trained for each discriminative base-\nline on top of frozen representations using the reconstruction\nerror. See Appendices A.7.4 and A.7.5 for further details on\ntraining baselines and decoder models.\nImplementation Details We use MLP and Resnet18 (He\net al., 2016) network architectures for simple and natural\nimage datasets respectively. The dimension of represen-\ntations z is set to 10 for MNIST, FashionMNIST, and 64\nfor CelebA and CIFAR10 datasets. For all generative ap-\nproaches, we adopt Gaussian posteriors, q(z|x), priors, p(z),\nand likelihoods, p(x|z), with diagonal covariance matrices\n(Kingma & Welling, 2014). For SimVAE, we adopt Gaus-\nsian p(z|y) as described in §3.4. SimVAE allows J related\nobservations to be simultaneously incorporated. Based on\nearly experiments, we fix the number of augmentations to\nJ = 10 (see Fig. 7 for an ablation). Ablations were per-\nformed for all sensitive hyperparameters for each method\nand parameter values were selected based on the best av-\nerage MLP Acc across datasets. Further details regarding\nhyperparameters and computational resources can be found\nin Appendices A.7.4 and A.7.5.\n5. Results\nContent classification: Table 1 reports the downstream\nclassification across datasets using benchmark class labels\n(i.e., content). Tables 1 and 5 show that SimVAE is compara-\nble to or outperforms generative baselines on supervised and\nunsupervised learning metrics on simple datasets. On nat-\n6\nA Probabilistic Model to explain Self-Supervised Representation Learning\nAcc-MP\nAcc-KNN\nRandom\n38.1 ± 3.8\n46.1 ± 2.5\nSimCLR\n97.2 ± 0.0\n97.2 ± 0.1\nMoCo\n94.6 ± 0.4\n94.6 ± 0.3\nVicREG\n96.7 ± 0.0\n97.0 ± 0.0\nVAE\n97.8 ± 0.1\n98.0 ± 0.1\nβ-VAE\n98.0 ± 0.0\n98.3 ± 0.0\nCR-VAE\n97.7 ± 0.0\n98.0 ± 0.0\nMNIST\nSimVAE\n98.4 ± 0.0\n98.5 ± 0.0\nRandom\n49.8 ± 0.8\n66.5 ± 0.4\nSimCLR\n74.9 ± 0.2\n76.0 ± 0.1\nMoCo\n71.2 ± 0.1\n76.9 ± 0.2\nVicREG\n73.2 ± 0.1\n76.0 ± 0.1\nVAE\n80.2 ± 0.3\n83.7 ± 0.2\nβ-VAE\n82.2 ± 0.1\n86.1 ± 0.0\nCR-VAE\n82.6 ± 0.0\n86.4 ± 0.0\nFashionMNIST\nSimVAE\n82.1 ± 0.0\n86.5 ± 0.0\nAcc-MP\nAcc-KNN\nRandom\n83.5 ± 1.0\n80.0 ± 0.9\nSimCLR\n93.7 ± 0.4\n91.6 ± 0.3\nMoCo\n89.7 ± 1.0\n88.6 ± 1.0\nVicREG\n94.7 ± 0.1\n92.7 ± 0.4\nVAE\n89.0 ± 0.5\n86.9 ± 0.7\nβ-VAE\n93.4 ± 0.4\n92.0 ± 0.1\nCR-VAE\n93.1 ± 0.4\n91.6 ± 0.6\nCelebA\nSimVAE\n95.6 ± 0.4\n93.2 ± 0.1\nRandom\n16.3 ± 0.4\n13.1 ± 0.6\nSimCLR\n67.4 ± 0.1\n64.0 ± 0.0\nMoCo\n56.4 ± 1.6\n54.0 ± 2.0\nVicREG\n69.7 ± 0.0\n68.3 ± 0.0\nVAE\n30.3 ± 0.4\n25.6 ± 0.5\nβ-VAE\n36.6 ± 0.1\n28.5 ± 0.1\nCR-VAE\n36.8 ± 0.0\n28.1 ± 0.1\nCIFAR10\nSimVAE\n51.8 ± 0.0\n47.1 ± 0.0\nTable 1: Top-1% self-supervised Acc (↑) for MNIST, FashionMNIST, CIFAR10, and CelebA (gender classification) using a\nMLP probe (MP) and k-Nearest Neighbors (KNN) classification methods; We report mean and standard errors over three\nruns; Bold indicate best scores in each method class: generative (teal), discriminative methods (red).\nFigure 3: Assessing the information in representations: original images (left cols) and reconstructions from representations\nlearned by generative unsupervised learning (VAE, β-VAE, CR-VAE), generative SSL (our SimVAE) and discriminative\nSSL (SimCLR, VicREG) on Cifar10 (l), CelebA (r). Discriminative methods lose style information (e.g. orientation/colour).\nural image datasets, we observe a significant improvement\nin performance over all VAE methods including the self-\nsupervised approach, CR-VAE (+2.5% for CelebA, +15%\nfor CIFAR10), supporting H3.\nTable 1 also shows that representations learned by SimVAE\nmaterially reduces the performance gap (∆) with respect to\nrepresentations learned by popular discriminative methods,\nby approximately half for CIFAR10 (∆ = 32.8% → ∆ =\n17.6%). SimVAE outperforms all baselines on the CelebA\ndataset. The results in Table 1 thus empirically support\nH1 and demonstrate that SimVAE achieves self-supervised,\nparticularly where distributions can be well-modelled.\nStyle classification: We further investigate the downstream\nperformance on style-related features (e.g., colour, position,\nand orientation) using the CelebA multi-attribute dataset.\nFigure 4 (left) shows that SimVAE outperforms both gener-\native and discriminative baselines on predicting an attribute\ndependant on style (hair colour). We also explore image\nreconstructions to gain qualitative insights into the infor-\nmation captured by the representation. Figures 2 and 3\nshows a loss of orientation and colour information in rep-\nresentations learned by discriminative methods across four\ndatasets. These findings support our hypothesis (H2) that\ndiscriminative methods lose style information, which may\nbe important in some downstream tasks, and confirms the\nability of generative methods to retain both content and\nstyle information (Figure 4, middle). Importantly, Figure 4\n(right) shows that SimVAE learns more task-agnostic rep-\nresentations, surpassing all generative and discriminative\nbaselines on average across the prediction of all (20) CelebA\nattributes. An overview of downstream performances for\nindividual CelebA features are reported in Appendix A.8.\nImage Generation: While generative quality is not relevant\nto our main hypotheses, out of interest and perhaps as a\nfuture benchmark, we show randomly generated SimVAE\nimages and generative quality metrics in Appendix A.8. We\nobserve small but significant FID score and reconstruction\nerror improvements relative to previous VAE methods on\nMNIST, FashionMNIST, CelebA and CIFAR10 datasets.\n7\nA Probabilistic Model to explain Self-Supervised Representation Learning\nCelebA\nAcc-MP\nRandom\n56.2 ± 0.8\nSimCLR\n52.0 ± 0.2\nVicREG\n52.8 ± 0.4\nVAE\n64.4 ± 0.3\nβ-VAE\n66.4 ± 0.4\nCR-VAE\n66.2 ± 0.4\nSimVAE\n67.5 ± 0.3\nFigure 4: Style prediction on CelebA (Acc-MP ↑): (left) hair colour prediction (mean and standard error over three runs,\nbest overall results indicated in bold); (middle) content vs. style prediction (gender vs hair colour), best performance in\ntop-right; (right) Performance increase of SimVAE relative to baselines across all 20 CelebA attributes.\n6. Discussion & Conclusion\nRepresentations learned by SSL achieve impressive perfor-\nmance on downstream tasks, nearing that of fully supervised\nlearning (e.g. Chen et al., 2020; Caron et al., 2021; Bardes\net al., 2022), and can transfer well between datasets (Erics-\nson et al., 2022). This performance has spurred considerable\ninterest in understanding the theoretical mechanism behind\nSSL (§2). Several works aim to explain how auxiliary task\nperformance transfers to a downstream task but have been\ndisproved for typical datasets (Saunshi et al., 2022). It has\nalso been argued that SSL relies on maximising mutual in-\nformation between views/augmentations (Hjelm et al., 2019)\nor pulling representations of semantically similar data closer\nthan those of random samples (Wang & Isola, 2020).\nWe propose a hierarchical latent variable model for predic-\ntive SSL (Fig. 1) in which representations (z) of semanti-\ncally related data (x) are conditioned on latent variable (y)\ndetermining semantic content, and z ∼p(z|y) governs style.\nWe show that our model justifies loss functions of predictive\nSSL methods, including contrastive learning, by reference\nto its ELBO; and underpins the common notion that SSL\n“pulls together” representations of semantically related data\nand “pushes apart” others. Predictive SSL methods are seen\nto maximise entropy H[p(z)] in place of the ELBO’s re-\nconstruction term, thereby maximising mutual information\nbetween representations. This seems intuitive appealing, but\nin fact causes representations of semantically related data\nto collapse together and lose information about the data,\nreducing the generality of representations. Our analysis also\njustifies the common use of a projection head (§3.3), which\nitself suggests caution in (over-)analysing representations\nthat are ultimately discarded.\nWe show that fitting the model generatively, as SimVAE,\nlearns representations that perform comparably, or demon-\nstrably reduce the performance gap, to discriminative meth-\nods at content prediction, while outperforming where style\ninformation is required, taking a step towards task-agnostic\nrepresentations. SimVAE outperforms previous generative\nVAE-based approaches, including CR-VAE tailored to SSL.\nRegarding limitations, a full explanation of SSL should con-\nnect learning an auxiliary task to downstream task perfor-\nmance. This requires understanding: (i) what the auxiliary\ntask requires of the encoder; (ii) how the encoder achieves it;\nand (iii) how (i-ii) relate to the downstream task. Our work\ntackles (i). It is unclear if we currently have sufficient under-\nstanding of neural networks to resolve (ii), required for (iii)\n(Saunshi et al., 2022). However, continuity, smoothness and\ninductive bias of the encoder seem key: as representations of\nsemantically related data come together under the auxiliary\ntask, representations of other related samples, “close” in X,\nare brought together in Z. By addressing (i), we hope to\nfacilitate future work in (ii) and (iii).\nLearning representations generatively is currently challeng-\ning, but we believe the connection drawn to popular SSL\nmethods in our analysis and supported empirically justifies\nfurther research. Particularly given the prospect of uncer-\ntainty estimation from the posterior, of generating novel\nsamples and of learning task-agnostic representations that\ndisentangle style information rather than lose it (Higgins\net al., 2017). Preliminary analysis (c.f., Figure 7 in Ap-\npendix A.8) also suggests that SimVAE is less sensitive\nto hyperparameters, notably in the augmentation strategy.\nEven while generative representation learning remains a\nchallenge, understanding the mechanism behind SSL offers\na guide for designing and interpreting future discriminative\nmethods. Our latent variable model also recasts SSL in line\nwith other latent variable paradigms, such as unsupervised\nVAEs (Kingma & Welling, 2014) and supervised Variational\nClassification (Dhuliawala et al., 2023), which may facilitate\na principled unified learning regime.\n8\nA Probabilistic Model to explain Self-Supervised Representation Learning\n7. Broader Impact\nThis paper presents work whose goal is to advance the field\nof Machine Learning. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here. Specifically, we focus on a\ntheoretical understanding of existing self-supervised learn-\ning methods with the potential impact of improving their\ninterpretability and future algorithm design.\nReferences\nAllen, C. and Hospedales, T. Analogies explained: Towards\nunderstanding word embeddings. In ICML, 2019.\nArandjelovic, R. and Zisserman, A. Look, listen and learn.\nIn ICCV, 2017.\nArora, S., Khandeparkar, H., Khodak, M., Plevrakis, O.,\nand Saunshi, N. A theoretical analysis of contrastive\nunsupervised representation learning. In ICML, 2019.\nBachman, P., Hjelm, R. D., and Buchwalter, W. Learning\nrepresentations by maximizing mutual information across\nviews. In NeurIPS, 2019.\nBaevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec\n2.0: A framework for self-supervised learning of speech\nrepresentations. In NeurIPS, 2020.\nBalaˇzevi´c, I., Steiner, D., Parthasarathy, N., Arandjelovi´c,\nR., and H´enaff, O. J. Towards in-context scene under-\nstanding. In NeurIPS, 2023.\nBalestriero, R., Ibrahim, M., Sobal, V., Morcos, A., Shekhar,\nS., Goldstein, T., Bordes, F., Bardes, A., Mialon, G., Tian,\nY., et al. A cookbook of self-supervised learning. arXiv\npreprint arXiv:2304.12210, 2023.\nBardes, A., Ponce, J., and LeCun, Y. Vicreg: Variance-\ninvariance-covariance regularization for self-supervised\nlearning. In ICLR, 2022.\nBen-Shaul, I., Shwartz-Ziv, R., Galanti, T., Dekel, S., and\nLeCun, Y. Reverse engineering self-supervised learning.\nIn NeurIPS, 2023.\nCaron, M., Bojanowski, P., Joulin, A., and Douze, M. Deep\nclustering for unsupervised learning of visual features. In\nECCV, 2018.\nCaron, M., Misra, I., Mairal, J., Goyal, P., Bojanowski, P.,\nand Joulin, A. Unsupervised learning of visual features\nby contrasting cluster assignments. In NeurIPS, 2020.\nCaron, M., Touvron, H., Misra, I., J´egou, H., Mairal, J.,\nBojanowski, P., and Joulin, A. Emerging properties in\nself-supervised vision transformers. In ICCV, 2021.\nChen, T., Kornblith, S., Norouzi, M., and Hinton, G. A\nsimple framework for contrastive learning of visual rep-\nresentations. In ICML, 2020.\nChopra, S., Hadsell, R., and LeCun, Y. Learning a sim-\nilarity metric discriminatively, with application to face\nverification. In CVPR, 2005.\nCover, T. and Hart, P. Nearest neighbor pattern classification.\nIn IEEE Transactions on Information Theory, 1967.\nDhuliawala, S., Sachan, M., and Allen, C. Variational Clas-\nsification. In TMLR, 2023.\nDosovitskiy, A., Springenberg, J. T., Riedmiller, M., and\nBrox, T. Discriminative unsupervised feature learning\nwith convolutional neural networks. In NeurIPS, 2014.\nEdwards, H. and Storkey, A. Towards a neural statistician.\nIn ICLR, 2016.\nEricsson, L., Gouk, H., Loy, C. C., and Hospedales,\nT. M. Self-supervised representation learning: Introduc-\ntion, advances, and challenges. IEEE Signal Processing\nMagazine, 39(3):42–62, 2022.\nGidaris, S., Singh, P., and Komodakis, N. Unsupervised rep-\nresentation learning by predicting image rotations. arXiv\npreprint arXiv:1803.07728, 2018.\nGrill, J.-B., Strub, F., Altch´e, F., Tallec, C., Richemond, P.,\nBuchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,\nGheshlaghi Azar, M., et al. Bootstrap your own latent-a\nnew approach to self-supervised learning. In NeurIPS,\n2020.\nGupta, K., Ajanthan, T., Hengel, A. v. d., and Gould,\nS.\nUnderstanding and improving the role of projec-\ntion head in self-supervised learning.\narXiv preprint\narXiv:2212.11491, 2022.\nHadsell, R., Chopra, S., and LeCun, Y. Dimensionality\nreduction by learning an invariant mapping. In CVPR,\n2006.\nHaoChen, J. Z., Wei, C., Gaidon, A., and Ma, T. Provable\nguarantees for self-supervised deep learning with spectral\ncontrastive loss. In NeurIPS, 2021.\nHe, J., Gong, Y., Marino, J., Mori, G., and Lehrmann, A.\nVariational autoencoders with jointly optimized latent\ndependency structure. In ICLR, 2018.\nHe, K., Zhang, X., Ren, S., and Sun, J. Deep residual\nlearning for image recognition. In CVPR, 2016.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In CVPR, 2020.\n9\nA Probabilistic Model to explain Self-Supervised Representation Learning\nHe, K., Chen, X., Xie, S., Li, Y., Doll´ar, P., and Girshick,\nR. Masked autoencoders are scalable vision learners. In\nCVPR, 2022.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. Gans trained by a two time-scale update\nrule converge to a local nash equilibrium. In NeurIPS,\n2017.\nHiggins, I., Matthey, L., Pal, A., Burgess, C., Glorot, X.,\nBotvinick, M., Mohamed, S., and Lerchner, A. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework. In ICLR, 2017.\nHjelm, R. D., Fedorov, A., Lavoie-Marchildon, S., Grewal,\nK., Bachman, P., Trischler, A., and Bengio, Y. Learning\ndeep representations by mutual information estimation\nand maximization. In ICLR, 2019.\nKarras, T., Laine, S., and Aila, T. A style-based genera-\ntor architecture for generative adversarial networks. In\nCVPR, 2019.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In ICLR, 2014.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nLeCun, Y.\nThe mnist database of handwritten digits.\nhttp://yann. lecun. com/exdb/mnist/, 1998.\nLee, J. D., Lei, Q., Saunshi, N., and Zhuo, J. Predicting\nwhat you already know helps: Provable self-supervised\nlearning. In NeurIPS, 2021.\nLevy, O. and Goldberg, Y. Neural word embedding as\nimplicit matrix factorization. In NeurIPS, 2014.\nLiu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face\nattributes in the wild. In ICCV, 2015.\nMcAllester, D. and Stratos, K. Formal limitations on the\nmeasurement of mutual information. In AIStats, 2020.\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\nDean, J. Distributed representations of words and phrases\nand their compositionality. In NeurIPS, 2013.\nMisra, I. and Maaten, L. v. d. Self-supervised learning of\npretext-invariant representations. In CVPR, 2020.\nNakamura, H., Okada, M., and Taniguchi, T. Representa-\ntion uncertainty in self-supervised learning as variational\ninference. In ICCV, 2023.\nOord, A. v. d., Li, Y., and Vinyals, O. Representation learn-\ning with contrastive predictive coding. arXiv preprint\narXiv:1807.03748, 2018.\nOzsoy, S., Hamdan, S., Arik, S., Yuret, D., and Erdogan, A.\nSelf-supervised learning with an information maximiza-\ntion criterion. In NeurIPS, 2022.\nPapyan, V., Han, X., and Donoho, D. L. Prevalence of\nneural collapse during the terminal phase of deep learn-\ning training. Proceedings of the National Academy of\nSciences, 117(40):24652–24663, 2020.\nPoole, B., Ozair, S., Van Den Oord, A., Alemi, A., and\nTucker, G. On variational bounds of mutual information.\nIn ICML, 2019.\nRadford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G.,\nAgarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J.,\net al. Learning transferable visual models from natural\nlanguage supervision. In ICML, 2021.\nRanganath, R., Tran, D., and Blei, D. Hierarchical varia-\ntional models. In ICML, 2016.\nRolfe, J. T. Discrete variational autoencoders. In ICLR,\n2017.\nSansone, E. and Manhaeve, R. Gedi: Generative and dis-\ncriminative training for self-supervised learning. arXiv\npreprint arXiv:2212.13425, 2022.\nSaunshi, N., Ash, J., Goel, S., Misra, D., Zhang, C., Arora,\nS., Kakade, S., and Krishnamurthy, A. Understanding\ncontrastive learning requires incorporating inductive bi-\nases. In ICML, 2022.\nShwartz-Ziv, R., Balestriero, R., Kawaguchi, K., Rudner,\nT. G., and LeCun, Y. An information-theoretic perspec-\ntive on variance-invariance-covariance regularization. In\nNeurIPS, 2023.\nSinha, A., Song, J., Meng, C., and Ermon, S.\nD2c:\nDiffusion-decoding models for few-shot conditional gen-\neration. In NeurIPS, 2021.\nSinha, S. and Dieng, A. B. Consistency regularization for\nvariational auto-encoders. In NeurIPS, 2021.\nSohn, K. Improved deep metric learning with multi-class\nn-pair loss objective. In NeurIPS, 2016.\nSønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and\nWinther, O. Ladder variational autoencoders. In NIPS,\n2016.\nSong, C., Liu, F., Huang, Y., Wang, L., and Tan, T. Auto-\nencoder based data clustering. In Progress in Pattern\nRecognition, Image Analysis, Computer Vision, and\nApplications, 2013.\nTian, Y.\nUnderstanding deep contrastive learning via\ncoordinate-wise optimization. In NeurIPS, 2022.\n10\nA Probabilistic Model to explain Self-Supervised Representation Learning\nTosh, C., Krishnamurthy, A., and Hsu, D.\nContrastive\nlearning, multi-view redundancy, and linear models. In\nAlgorithmic Learning Theory, pp. 1179–1206. PMLR,\n2021.\nTsai, Y.-H. H., Wu, Y., Salakhutdinov, R., and Morency, L.-\nP. Self-supervised learning from a multi-view perspective.\nIn ICLR, 2020.\nTschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S.,\nand Lucic, M. On mutual information maximization for\nrepresentation learning. In ICLR, 2020.\nValpola, H. From neural pca to deep unsupervised learning.\nIn Advances in Independent Component Analysis and\nlearning machines, pp. 143–171. Elsevier, 2015.\nVon K¨ugelgen, J., Sharma, Y., Gresele, L., Brendel, W.,\nSch¨olkopf, B., Besserve, M., and Locatello, F.\nSelf-\nsupervised learning with data augmentations provably\nisolates content from style. In NeurIPS, 2021.\nWang, T. and Isola, P. Understanding contrastive represen-\ntation learning through alignment and uniformity on the\nhypersphere. In ICML, 2020.\nWang, Y., Zhang, Q., Wang, Y., Yang, J., and Lin, Z. Chaos\nis a ladder: A new theoretical understanding of con-\ntrastive learning via augmentation overlap.\nIn ICLR,\n2021.\nWang, Y., Tang, S., Zhu, F., Bai, L., Zhao, R., Qi, D., and\nOuyang, W. Revisiting the transferability of supervised\npretraining: an mlp perspective. In CVPR, 2022.\nWu, C., Pfrommer, J., Zhou, M., and Beyerer, J. Generative-\ncontrastive learning for self-supervised latent represen-\ntations of 3d shapes from multi-modal euclidean input.\narXiv preprint arXiv:2301.04612, 2023.\nWu, Z., Xiong, Y., Yu, S. X., and Lin, D. Unsupervised fea-\nture learning via non-parametric instance discrimination.\nIn CVPR, 2018.\nXiao, H., Rasul, K., and Vollgraf, R. Fashion-mnist: a\nnovel image dataset for benchmarking machine learning\nalgorithms. arXiv preprint arXiv:1708.07747, 2017.\nXie, J., Girshick, R., and Farhadi, A. Unsupervised deep\nembedding for clustering analysis. In ICML, 2016.\nXie, Z., Zhang, Z., Cao, Y., Lin, Y., Bao, J., Yao, Z., Dai, Q.,\nand Hu, H. Simmim: A simple framework for masked\nimage modeling. In CVPR, 2022.\nYang, B., Fu, X., Sidiropoulos, N. D., and Hong, M. To-\nwards k-means-friendly spaces: Simultaneous deep learn-\ning and clustering. In ICML, 2017.\nZhang, M., Xiao, T. Z., Paige, B., and Barber, D. Improv-\ning vae-based representation learning. arXiv preprint\narXiv:2205.14539, 2022.\nZimmermann, R. S., Sharma, Y., Schneider, S., Bethge, M.,\nand Brendel, W. Contrastive learning inverts the data\ngenerating process. In ICML, 2021.\n11\nA Probabilistic Model to explain Self-Supervised Representation Learning\nA. Appendix\nA.1. Background: Relevant VAE architectures\nThe proposed hierarchical latent variable model for self-supervised learning (Fig. 1) is fitted to the data distribution by\nmaximising the ELBOSSL and can be viewed as a VAE with a hierarchical prior.\nVAEs have been extended to model hierarchical latent structure (e.g. Valpola, 2015; Ranganath et al., 2016; Rolfe, 2017; He\net al., 2018; Sønderby et al., 2016; Edwards & Storkey, 2016), which our work relates to. Notably, Edwards & Storkey\n(2016) propose the same graphical model as Fig. 1, but methods differ in how posteriors are factorised, which is a key aspect\nfor learning informative representations that depend only on the sample they represent. Wu et al. (2023) and Sinha et al.\n(2021) combine aspects of VAEs and contrastive learning but do not propose a latent variable model for SSL. Nakamura\net al. (2023) look to explain SSL methods via the ELBO, but with a second posterior approximation not a generative model.\nA.2. Derivation of ELBOSSL and SimVAE Objective\nLet x = {x1, ..., xJ} be a set of J semantically related samples, ω = {θ, ψ, π} be parameters of the generative model for\nSSL (Fig. 1) and ϕ parameterise the approximate posterior qϕ(z|x). We derive the Evidence Lower Bound (ELBOSSL) that\nunderpins the training objectives of (projective) discriminative SSL methods and is used to train SimVAE (§3.4).\nmin\nω DKL[ p(x|y) ∥ pω(x|y) ] = max\nω\nE\nx,y\n\u0002\nlog pω(x|y)\n\u0003\n= max\nω,ϕ\nE\nx,y\nhZ\nz\nqϕ(z|x) log pω(x|y)\ni\n= max\nω,ϕ\nE\nx,y\nhZ\nz\nqϕ(z|x) log pθ(x|z)pψ(z|y)\npω(z|x,y)\nqϕ(z|x)\nqϕ(z|x)\ni\n= max\nω,ϕ\nE\nx,y\nhZ\nz\nqϕ(z|x) log pθ(x|z)pψ(z)|y\nqϕ(z|x)\ni\n+ DKL[ qϕ(z|x) ∥ pω(z|x, y) ]\n≥ max\nω,ϕ\nE\nx,y\nhZ\nz\nqϕ(z|x)\nn\nlog pθ(x|z)\nqϕ(z|x) + log pψ(z|y)\noi\n(cf Eq. 2)\n= max\nω,ϕ\nE\nx,y\nhX\nj\nnZ\nzj qϕ(zj|xj) log pθ(xj|zj)\nqϕ(zj|xj)\no\n+\nZ\nz\nqϕ(z|x) log pψ(z|y)\ni\n(= Eq. 4)\nTerms of ELBOSSL are analogous to those of the standard ELBO: reconstruction error, entropy of the approximate posterior\nH(qϕ(z|x)) and the (conditional) prior. Algorithm 1 provides an overview of the computational steps required to maximise\nELBOssl under Gaussian assumptions described in §3.4, referred to as SimVAE. As our experimental setting considers\naugmentations as semantically related samples, Algorithm 1 incorporates a preliminary step to augment data samples.\nA.3. Detailed derivation of InfoNCE Objective\nFor data sample xi0 ∈xi0, let x′\ni0 ∈xi0 be a semantically related positive sample and {x′\nir}k\nr=1 be random negative samples.\nDenote by x = {xi0, x′\ni0} the positive pair, by X− ={x′\ni1, ..., x′\nik} all negative samples, and by X = x ∪ X− all samples.\nThe InfoNCE objective is derived as follows (by analogy to Oord et al. (2018)).\nEX\n\u0002\nlog p(y = 0|X)\n\u0003\n(predict y = index of positive sample in X−)\n= EX\n\u0002\nlog\nZ\nZ\np(y = 0|Z)q(Z|X)\n\u0003\n(introduce latent variables Z: Y → Z → X)\n≥ EX\n\u0002 Z\nZ\nq(Z|X) log p(y = 0|Z)\n\u0003\n(by Jensen’s inequality)\n= EX\nh Z\nZ\nq(Z|X) log\np(Z′|zi0,y=0)p(y = 0)\nPk\nr=0 p(Z′|zi0,y=r)p(y = r)\ni\n(Bayes rule, note p(y=r)= 1\nk, ∀r)\n= EX\nh Z\nZ\nq(Z|X) log\np(z′\ni0|zi0) Q\ns̸=0 p(z′\nis)\nPk\nr=0 p(z′\nir |zi0) Q\ns̸=r p(z′\nis)\ni\n(from sample similarity/independence)\n= EX\nh Z\nZ\nq(Z|X) log\np(z′\ni0|zi0)/p(z′\ni0)\nPk\nr=0 p(z′\nir |zi0)/p(z′\nir )\ni\n(divide through by Qk\ns=0 p(z′\nis))\n12\nA Probabilistic Model to explain Self-Supervised Representation Learning\n= EX\nh Z\nZ\nq(Z|X) log\np(z′\ni0,zi0)/p(z′\ni0)\nPk\nr=0 p(z′\nir ,zi0)/p(z′\nir )\ni\n(10)\nThe final expression is parameterised using a similarity function sim(z, z′) to give the objective.\n−LINCE .= EX\nh Z\nZ\nq(Z|X) log\nsim(z′\ni0,zi0)\nPk\nr=0 sim(z′\nir ,zi0)\ni\nOord et al. (2018) show, and Poole et al. (2019) confirm, that this loss is a lower bound on the mutual information, which\nimproves with the number of negative samples k.\n−LINCE\n= EX\nh Z\nZ\nq(Z|X) log\np(z′\ni0,zi0)/p(z′\ni0)\nPk\nr=0 p(z′\nir ,zi0)/p(z′\nir )\ni\n(multiply Eq. 10 through by p(zi0))\n= EX\nh Z\nZ\nq(Z|X) log p(zi0|z′\ni0) − log\n\u0000p(zi0|z′\ni0) +\nk\nX\nr=1\np(zi0|z′\nir)\n\u0001i\n= − EX\nh Z\nZ\nq(Z|X) log\n\u00001 +\nk\nX\nr=1\np(zi0|z′\nir )\np(zi0|z′\ni0)\n\u0001i\n(divide through by p(zi0|z′\ni0))\n≈ − Ex\nh Z\nz\nq(z|x) log\n\u00001 + (k − 1)Ex′\nj\n\u0002 Z\nz′\nj\nq(z′\nj|x′\nj)\np(zi0|z′\nj)\np(zi0|z′\ni0)\n\u0001\u0003i\n= − Ex\nh Z\nz\nq(z|x) log\n\u00001 + (k − 1)\np(zi0)\np(zi0|z′\ni0)\n\u0001i\n≤ − Ex\nh Z\nz\nq(z|x) log(k − 1)\np(zi0)\np(zi0|z′\ni0)\ni\n= Ex\nh Z\nz\nq(z|x) log\np(zi0,z′\ni0)\np(zi0)p(z′\ni0)\ni\n+ log\n1\n(k − 1)\nk→∞\n→ Ex\nh Z\nz\nqϕ(z|x)\n\u0000log p(z|yi0) −\nX\nj\nlog p(zj\ni0)\n\u0001i\nIn the last step, we revert to the terminology used in the main paper for ease of reference.\nA.4. Derivation of parameter-free p(zi|yi) = s(zi)\nInstance Discrimination methods consider J =1 sample xi at a time, labelled by its index yi =i, and computes p(xi|y=i; θi)\nfrom stored instance-specific parameters θi. This requires parameters proportional to the dataset size, which could be\nprohibitive, whereas parameter number is often independent of the dataset size, or grows slowly. We show that contrastive\nmethods (approximately) optimise the same objective, but without parameters, and here explain how that is possible. Recall\nthat the “label” i is semantically meaningless and simply identifies samples of a common distribution p(x|y=i) .=p(x|yi).\nFor J ≥ 2 semantically related samples xi = {xj\ni}J\nj=1, xj\ni ∼ p(x|yi), their latent variables are conditionally independent,\nhence p(zi|yi) =\nR\nψ p(ψi)p(zi|yi; ψi) =\nR\nψ p(ψi) Q\nj p(zj\ni |yi; ψi) = s(zi), a function of the latent variables that non-\nparametrically approximates the joint distribution of latent variables of semantically related data. (Note that unsemantically\nrelated data are independent and the joint distribution over their latent variables is a product of marginals).\nWe assume a Gaussian prior p(ψi)=N(ψi; 0, γ2I) and class-conditionals p(zj\ni |ψi)=N(zj\ni ; ψi, σ2) (for fixed variance σ2).\np(zi|yi) =\nZ\nψi\np(zi|ψi)p(ψi) =\nZ\nψi\np(ψi)\nY\nj\np(zj\ni |ψi)\n∝\nZ\nψi\nexp{− 1\n2γ2 ψ2\ni }\nY\nj\nexp{−\n1\n2σ2 (zj\ni − ψi)2}\n=\nZ\nψi\nexp{−\n1\n2σ2 ( σ2\nγ2 ψ2\ni +\nX\nj\n(zj\ni − ψi)2)}\n13\nA Probabilistic Model to explain Self-Supervised Representation Learning\n=\nZ\nψi\nexp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) − 2(\nX\nj\nzj\ni )ψi + ( σ2\nγ2 + J)ψ2\ni )}\n=\nZ\nψi\nexp{− σ2/γ2+J\n2σ2\n(ψi −\n1\n(σ2/γ2+J)\nX\nj\nzj\ni )2} + exp{−\n1\n2σ2 (\nX\nj\nzj2\ni\n−\n1\nσ2/γ2+J (\nX\nj\nzj\ni )2)}\n(*)\n∝ exp{−\n1\n2σ2 (\nX\nj\nzj2\ni\n−\n1\nσ2/γ2+J (\nX\nj\nzj2\ni\n+\nX\nj̸=k\nzj\ni zk\ni ))}\n= exp{−\n1\n2σ2 ((1 −\n1\nσ2/γ2+J )\nX\nj\nzj2\ni\n+\n1\nσ2/γ2+J\nX\nj̸=k\nzj\ni zk\ni ))}\n∝ exp{−\n1\n2σ2(σ2/γ2+J)\nX\nj̸=k\nzj\ni zk\ni )}\n(if ∥z∥2 = 1)\nThe result can be rearranged into a Gaussian form (a well known result when all distributions are Gaussian), but the last line\nalso shows that, under the common practice of setting embeddings to unit length (∥z∥2 =1), s(·) can be calculated directly\nfrom dot products, or cosine similarities (up to a proportionality constant, which does not affect optimisation).\nIf we instead assume a uniform prior, we can take the limit of the line marked (*) as γ →∞:\nexp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) −\n1\nσ2/γ2+J (\nX\nj\nzj\ni )2)}\n→ exp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) − 1\nJ (\nX\nj\nzj\ni )2)}\n= exp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) − J ¯zi\n2)}\n= exp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) − 2J ¯zi\n2 + J ¯zi\n2)}\n= exp{−\n1\n2σ2 ((\nX\nj\nzj2\ni ) − 2¯zi(\nX\nj\nzj\ni ) +\nX\nj\n¯zi\n2)}\n= exp{−\n1\n2σ2\nX\nj\n(zj2\ni\n− 2zj\ni ¯zi + ¯zi\n2)}\n= exp{−\n1\n2σ2\nX\nj\n(zj\ni − ¯zi)2}\n(11)\nA.5. Relationship between InfoNCE Representations and PMI\nFor data sampled x∼p(x) and augmentations x′ ∼ pτ(x′|x) sampled under a synthetic augmentation strategy, Oord et al.\n(2018) show that the InfoNCE objective for a sample x is optimised if their respective representations z, z′ satisfy\nexp{sim(z, z′)} = c\np(x,x′)\np(x)p(x′),\n(12)\nwhere sim(·, ·) is the similarity function (e.g. dot product), and c is a proportionality constant, specific to x. Since c may\ndiffer arbitrarily with x it can be considered an arbitrary function of x, but for simplicity we consider a particular x and\nfixed c. Further, c > 0 is strictly positive since it is a ratio between positive (exponential) and non-negative (probability\nratio) terms. Accordingly, representations satisfy\nsim(z, z′) = PMI(x, x′) + c′,\n(13)\nwhere c′ = log c ∈ R and PMI(x, x′) is the pointwise mutual information between samples x and x′. Pointwise mutual in-\nformation (PMI) is an information theoretic term that reflects the probability of events occurring jointly versus independently.\nFor an arbitrary sample and augmentation this is given by:\nPMI(x, x′) .= log p(x, x′)\np(x)p(x′) = logpτ(x′|x)\np(x′) .\n(14)\n14\nA Probabilistic Model to explain Self-Supervised Representation Learning\nWe note that pτ(x′|x)=0 if x can not be augmented to produce x′; and that, in a continuous domain, such as images, two\naugmentations are identical with probability zero. Thus augmentations of different samples are expected to not overlap and\nthe marginal is given by p(x′)=\nR\nx pτ(x′|x)p(x)=pτ(x′|x∗)p(x∗), where x∗ is the sample augmented to give x′. Thus\npτ(x′|x)\np(x′)\n=\npτ(x′|x)\npτ(x′|x∗)p(x∗) =\n(\n1/p(x∗)\nif x∗ =x (i.e. x′ is an augmentation of x)\n0\notherwise;\n(15)\nand PMI(x, x′) = − log p(x) ≥ k > 0 or PMI(x, x′) = −∞, respectively. Here k=− log arg maxx p(x) is a finite value\nbased on the most likely sample. For typical datasets, this can be approximated empirically by 1\nN where N is the size of the\noriginal dataset (since that is how often the algorithm observes each sample), hence k = log N, often of the order 5 − 10\n(depending on the dataset).\nIf the main objective were to accurately approximate PMI (subject to a constant c′) in Eq. 13, e.g. to approximate mutual\ninformation, or if representation learning depended on it, then, at the very least, the domain of sim(·, ·) must span its range\nof values, seen above as from −∞ for negative samples to a small positive value (e.g. 5-10) for positive samples. Despite\nthis, the popular bounded cosine similarity function (cossim(z, z′) =\nzT z\n||z||2||z′||2 ∈ [−1, 1]) is found to outperform the\nunbounded dot product, even though the cosine similarity function necessarily cannot span the range required to reflect true\nPMI values, while the dot product can. This strongly suggests that representation learning does not require representations\nto specifically learn PMI, or for the overall loss function to approximate mutual information.\nInstead, with the cosine similarity constraint, the InfoNCE objective is as optimised as possible if representations of a data\nsample and its augmentations are fully aligned (cossim(z, z′) = 1) and representations of dissimilar data are maximally\nmisaligned cossim(z, z′) = −1, since these minimise the error from the true PMI values for positive and negative samples\n(described above). Constraints, such as the dimensionality of the representation space vs the number of samples, may\nprevent these revised theoretical optima being fully achieved, but the loss function is optimised by clustering representations\nof a sample and its augmentations and spreading apart those clusters. Note that this is the same geometric structure as\ninduced under softmax cross-entropy loss (Dhuliawala et al., 2023).\nWe note that our theoretical justification for representations not capturing PMI is supported by the empirical observation that\ncloser approximations of mutual information do not appear to improve representations (Tschannen et al., 2020). Also, more\nrecent contrastive self-supervised methods increase the cosine similarity between semantically related data but spread apart\nrepresentation the without negative sampling of InfoNCE, yet outperform the InfoNCE objective despite having no obvious\nrelationship to PMI (Grill et al., 2020; Bardes et al., 2022).\nA.6. Information Loss due to Representaiton Collapse: a discussion\nWhile it may seem appealing to lose information by way of representation collapse, e.g. to obtain representations invariant\nto nuisance factors, this is a problematic notion from the perspective of general-purpose representation learning, where the\ndownstream task is unknown or there may be many, since what is noise for one task may be of use in another. For example,\n“blur” is often considered noise, but a camera on an autonomous vehicle may be better to detect blur (e.g. from soiling) than\nbe invariant to it and eventually fail when blurring becomes too severe. We note that humans can observe a scene including\nmany irrelevant pieces of information, e.g. we can tell when an image is blurred or that we are looking through a window,\nand “disentangle” that from the rest of the image. This suggests that factors can be preserved and disentangled.\nTo stress the point that representation collapse is not desirable in and of itself, we note that collapsing together representations\nof semantically related data xi would be problematic if subsets xi overlap. For example, in the discrete case of word2vec,\nwords are considered semantically related if they co-occur within a fixed window. Representation collapse, here, would\nmean that co-occurring words belonging to the same xi would have the same representation, which is clearly undesirable.\n15\nA Probabilistic Model to explain Self-Supervised Representation Learning\nA.7. Experimental Details\nA.7.1. SIMVAE ALGORITHM\nAlgorithm 1 SimVAE\nRequire: data {xk}M\nk=1; batch size N; data dim D; latent dim L; augmentation set T ; num views J; encoder fϕ; decoder\ngθ; Var(z|y) σ2;\nfor randomly sampled mini-batch {xk}N\nk=1 do\nfor augmentation tj ∼ T do\nxj\nk = tj(xk);\n# augment samples\nµj\nk, Σj\nk = fϕ(xj\nk);\n# forward pass: z ∼ pϕ(z|x)\nzj\nk ∼ N(µj\nk, Σj\nk);\n˜xj\nk = gθ(zj\nk);\n# ˜x = E[x|z; θ]\nend for\nLk\nrec = 1\nD\nPJ\nj=1 ||xj\nk − ˜xj\nk||2\n2\n# minimize loss\nLk\nH = 1\n2\nPJ\nj=1 log(|Σj\nk|)\nLk\nprior = 1\n2\nPJ\nj=1 ||(zj\nk − 1\nJ\nPJ\nj=1 zj\nk)/σ||2\n2\nmin(PN\nk=1 Lk\nrec + Lk\nH + Lk\nprior) w.r.t. ϕ, θ by SGD;\nend for\nreturn ϕ, θ;\nA.7.2. DATASETS\nMNIST The MNIST dataset (LeCun, 1998) gathers 60’000 training and 10’000 testing images representing digits from 0 to\n9 in various caligraphic styles. Images were kept to their original 28x28 pixel resolution and were binarized. The 10-class\ndigit classification task was used for evaluation.\nFashionMNIST The FashionMNIST dataset (Xiao et al., 2017) is a collection of 60’000 training and 10’000 test images\ndepicting Zalando clothing items (i.e., t-shirts, trousers, pullovers, dresses, coats, sandals, shirts, sneakers, bags and ankle\nboots). Images were kept to their original 28x28 pixel resolution. The 10-class clothing type classification task was used for\nevaluation.\nCIFAR10 The CIFAR10 dataset (Krizhevsky et al., 2009) offers a compact dataset of 60,000 (50,000 training and 10,000\ntesting images) small, colorful images distributed across ten categories including objects like airplanes, cats, and ships, with\nvarious lighting conditions. Images were kept to their original 32x32 pixel resolution.\nCelebA The CelebA dataset (Liu et al., 2015) comprises a vast collection of celebrity facial images. It encompasses a diverse\nset of 183’000 high-resolution images (i.e., 163’000 training and 20’000 test images), each depicting a distinct individual.\nThe dataset showcases a wide range of facial attributes and poses and provides binary labels for 40 facial attributes including\nhair & skin colour, presence or absence of attributes such as eyeglasses and facial hair. Each image was cropped and resized\nto a 64x64 pixel resolution. Attributes referring to hair colour were aggregated into a 5-class attribute (i.e., bald, brown hair,\nblond hair, gray hair, black hair). Images with missing or ambiguous hair colour information were discarded at evaluation.\nAll datasets were sourced from Pytorch’s dataset collection.\nA.7.3. DATA AUGMENTATION STRATEGY\nTaking inspiration from SimCLR’s (Chen et al., 2020) augmentation strategy which highlights the importance of random\nimage cropping and colour jitter on downstream performance, our augmentation strategy includes random image cropping,\nrandom image flipping and random colour jitter. The colour augmentations are only applied to the non gray-scale datasets\n(i.e., CIFAR10 (Krizhevsky et al., 2009) & CelebA dataset (Liu et al., 2015)). Due to the varying complexity of the datasets\nwe explored, hyperparameters such as the cropping strength were adapted to each dataset to ensure that semantically\nmeaningful features remained after augmentation. The augmentation strategy hyperparameters used for each dataset are\ndetailed in table 2.\n16\nA Probabilistic Model to explain Self-Supervised Representation Learning\nDataset\nCrop\nVertical Flip\ncolour Jitter\nscale\nratio\nprob.\nb-s-c\nhue\nprob.\nMNIST\n0.4\n[0.75,1.3]\n0.5\n-\n-\n-\nFashion\n0.4\n[0.75,1.3]\n0.5\n-\n-\n-\nCIFAR10\n0.6\n[0.75,1.3]\n0.5\n0.8\n0.2\n0.8\nCelebA\n0.6\n[0.75,1.3]\n0.5\n0.8\n0.2\n0.8\nTable 2: Data augmentation strategy for each dataset: (left to right)\ncropping scale, cropping ratio, probability of vertical/horizontal flip,\nbrightness-saturation-contrast jitter, hue jitter, probability of colour jitter\nA.7.4. TRAINING IMPLEMENTATION DETAILS\nThis section contains all details regarding the architectural and optimization design choices used to train SimVAE and all\nbaselines. Method-specific hyperparameters are also reported below.\nNetwork Architectures The encoder network architectures used for SimCLR, MoCo, VicREG, and VAE-based approaches\nincluding SimVAE for simple (i.e., MNIST, FashionMNIST ) and complex datasets (i.e., CIFAR10, CelebA) are detailed in\nTable 3a, Table 4a respectively. Generative models which include all VAE-based methods also require decoder networks for\nwhich the architectures are detailed in Table 3b and Table 4b. The latent dimensionality for MNIST and FashionMNIST is\nfixed at 10 and increased to 64 for the CelebA and CIFAR10 datasets. The encoder and decoder architecture networks are\nkept constant across methods including the latent dimensionality to ensure a fair comparison.\nLayer Name\nOutput Size\nBlock Parameters\nfc1\n500\n784x500 fc, relu\nfc2\n500\n500x500 fc, relu\nfc3\n2000\n500x2000 fc, relu\nfc4\n10\n2000x10 fc\n(a) Encoder\nLayer Name\nOutput Size\nBlock Parameters\nfc1\n2000\n10x2000 fc, relu\nfc2\n500\n2000x500 fc, relu\nfc3\n500\n500x500 fc, relu\nfc4\n784\n500x784 fc\n(b) Decoder\nTable 3: Multi-layer perceptron network architectures used for MNIST & FashionMNIST training\nLayer Name\nOutput\nBlock Parameters\nconv1\n32x32\n4x4, 16, stride 1\nbatchnorm, relu\n3x3 maxpool, stride 2\nconv2 x\n32x32\n3x3, 32, stride 1\n3x3, 32, stride 1\nconv3 x\n16x16\n3x3, 64, stride 2\n3x3, 64, stride 1\nconv4 x\n8x8\n3x3, 128, stride 2\n3x3, 128, stride 1\nconv5 x\n4x4\n3x3, 256, stride 2\n3x3, 256, stride 1\nfc\n64\n4096x64 fc\n(a) Encoder\nLayer Name\nOutput\nBlock Parameters\nfc\n256x4x4\n64x4096 fc\nconv1 x\n8x8\n3x3, 128, stride 2\n3x3, 128, stride 1\nconv2 x\n16x16\n3x3, 64, stride 2\n3x3, 64, stride 1\nconv3 x\n32x32\n3x3, 32, stride 2\n3x3, 32, stride 1\nconv4 x\n64x64\n3x3, 16, stride 2\n3x3, 16, stride 1\nconv5\n64x64\n5x5, 3, stride 1\n(b) Decoder\nTable 4: Resnet18 network architectures used for CIFAR10 & CelebA datasets\nOptimisation & Hyper-parameter tuning All methods were trained using an Adam optimizer until training loss conver-\ngence. The batch size was fixed to 128. Hyper-parameter tuning was performed based on the downstream MLP classification\naccuracy across datasets. The final values of hyperparameters were selected to reach the best average downstream perfor-\nmance across datasets. While we observed stable performances across datasets for the VAE family of models, VicREG and\n17\nA Probabilistic Model to explain Self-Supervised Representation Learning\nMoCo, SimCLR is more sensitive, leading to difficulties when having to define a unique set of parameters across datasets.\nFor VAEs, the learning rate was set to 8e−5, and the likelihood probability, p(x|z), variance parameter was set to 0.02 for\nβ-VAE, CR-VAE and SimVAE. CR-VAE’s λ parameter was set to 0.1. SimVAE’s prior probability, p(z|y), variance was set\nto 0.15 and the number of augmentations to 10. VicREG’s parameter µ was set to 25 and learning rate to 1e-4. SimCLR’s\ntemperature parameter, τ, was set to 0.7 and learning rates were adapted for each dataset due to significant performance\nvariations across datasets ranging from 8e−5 to 1e−3.\nA.7.5. EVALUATION IMPLEMENTATION DETAILS\nFollowing common practices (Chen et al., 2020), downstream performance is assessed using a linear probe, a multi-layer\nperceptron probe, a k-nearest neighbors (kNN) algorithm, and a Gaussian mixture model (GMM). The linear probe consists\nof a fully connected layer whilst the mlp probe consists of two fully connected layers with a relu activation for the\nintermediate layer. Both probes were trained using an Adam optimizer with a learning rate of 3e−4 for 200 epochs with\nbatch size fixed to 128. Scikit-learn’s Gaussian Mixture model with a full covariance matrix and 200 initialization was fitted\nto the representations using the ground truth cluster number. The kNN algorithm from Python’s Scikit-learn library was\nused with k spanning from 1 to 15 neighbors. The best performance was chosen as the final performance measurement. No\naugmentation strategy was used at evaluation.\nA.7.6. GENERATION PROTOCOL\nHere we detail the image generation protocol and the quality evaluation of generated samples.\nAd-hoc decoder training VAE-based approaches, including SimVAE, are fundamentally generative methods aimed at\napproximating the logarithm of the marginal likelihood distribution, denoted as log p(x). In contrast, most traditional\nself-supervised methods adopt a discriminative framework without a primary focus on accurately modeling p(x). However,\nfor the purpose of comparing representations, and assessing the spectrum of features present in z, we intend to train a\ndecoder model for SimCLR & VicREG models. This decoder model is designed to reconstruct images from the fixed\nrepresentations initially trained with these approaches. To achieve this goal, we train decoder networks using the parameter\nconfigurations specified in Tables 3b and 4b, utilizing the mean squared reconstruction error as the loss function. The\nencoder parameters remain constant, while we update the decoder parameters using an Adam optimizer with a learning rate\nof 1e−4 until a minimal validation loss is achieved (i.e. ∼ 10-80 epochs).\nConditional Image Generation To allow for a fair comparison, all images across all methods are generated by sampling\nz from a multivariate Gaussian distribution fitted to the training samples’ representations. More precisely, each Gaussian\ndistribution is fitted to z conditioned on a label y. Scikit-Learn Python library Gaussian Mixture model function (with full\ncovariance matrix) is used.\nA.8. Additional Results & Ablations\nContent classification evaluation with linear & gaussian mixture model prediction heads Table 5 reports the top-1%\nself-supervised classification accuracy using a linear prediction head and a gaussian mixture model. From Table 5, we\ndraw similar conclusion as with Table 1: SimVAE significantly bridges the gap between discriminative and generative\nself-supervised learning methods when considering a supervised linear predictor and fully unsupervised methods for\ndownstream prediction. Table 6 report the normalized mutual information (NMI) and adjusted rank index (ARI) for the\nfitting of the GMM prediction head.\nContent & Style classification Figure 5 reports average classification accuracy using a MLP probe (over 3 runs) for the\nprediction of 20 CelebA facial attributes for SimVAE, generative and discriminative baselines.\nAugmentation protocol strength ablation Figure 6 reports the downstream classification accuracy across methods for\nvarious augmentations stategy. More precisely, we progressively increase the cropping scale and colour jitter amplitude.\nUnsurprinsingly (Chen et al., 2020), discriminative methods exhibit high sensitivity to the augmentation strategy with\nstronger disruption leading to improved content prediction. The opposite trend is observed with vanilla generative methods\nwhere reduced variability amongst the data leads to increased downstream performance. Interestingly, SimVAE is robust to\naugmentation protocol and performs comparably across settings.\n# of augmentation ablation Figure 7 reports the downstream classification accuracy for increasing numbers of augmentations\nconsidered simultaneously during the training of SimVAE for MNIST and CIFAR10 datasets. On average, a larger\n18\nA Probabilistic Model to explain Self-Supervised Representation Learning\nFigure 5: CelebA 20 facial attributes prediction using a MP. Average scores and standard errors are reported across 3 random\nseeds.\n19\nA Probabilistic Model to explain Self-Supervised Representation Learning\nAcc-LP\nAcc-GMM\nRandom\n39.7 ± 2.4\n42.2 ± 1.2\nSimCLR\n96.8 ± 0.1\n83.7 ± 0.6\nMoCo\n88.6 ± 1.7\n70.5 ± 4.0\nVicREG\n96.7 ± 0.0\n79.8 ± 0.6\nVAE\n97.2 ± 0.2\n96.3 ± 0.4\nβ-VAE\n97.8 ± 0.0\n96.2 ± 0.2\nCR-VAE\n97.5 ± 0.0\n96.9 ± 0.0\nMNIST\nSimVAE\n98.0 ± 0.0\n96.6 ± 0.0\nRandom\n51.2 ± 0.6\n48.6 ± 0.2\nSimCLR\n73.0 ± 0.3\n53.6 ± 0.3\nMoCo\n65.0 ± 1.3\n56.6 ± 1.1\nVicREG\n71.7 ± 0.1\n60.2 ± 1.1\nVAE\n79.0 ± 0.5\n57.9 ± 0.8\nβ-VAE\n79.6 ± 0.0\n68.0 ± 0.3\nCR-VAE\n79.7 ± 0.0\n63.4 ± 0.4\nFashionMNIST\nSimVAE\n80.0 ± 0.0\n71.1 ± 0.0\nAcc-LP\nAcc-GMM\nRandom\n64.4 ± 0.9\n59.2 ± 0.3\nSimCLR\n94.2 ± 0.2\n71.6 ± 0.6\nMoCo\nVicREG\n94.3 ± 0.3\n53.9 ± 0.2\nVAE\n81.5 ± 1.0\n58.8 ± 0.2\nβ-VAE\n81.9 ± 0.2\n59.5 ± 0.6\nCR-VAE\n81.6 ± 0.3\n58.9 ± 0.4\nCelebA\nSimVAE\n87.1 ± 0.3\n58.4 ± 0.6\nRandom\n15.7 ± 0.9\n13.1 ± 0.6\nSimCLR\n65.4 ± 0.1\n28.2 ± 0.2\nMoCo\n53.3 ± 1.3\n52.4 ± 0.3\nVicREG\n68.2 ± 0.0\n35.0 ± 2.8\nVAE\n24.7 ± 0.4\n23.4 ± 0.0\nβ-VAE\n26.9 ± 0.0\n31.2 ± 0.1\nCR-VAE\n26.8 ± 0.0\n30.3 ± 0.0\nCIFAR10\nSimVAE\n40.1 ± 0.0\n39.3 ± 0.0\nTable 5: Top-1% self-supervised Acc (↑) for MNIST, FashionMNIST, CIFAR10, and CelebA (gender classification) using a\nlinear probe (LP) and Gaussian Mixture Model (GMM) classification methods; We report mean and standard errors over\nthree runs; Bold indicate best scores in each method class: generative (teal), discriminative methods (red).\nDataset\nVAE\nβ-VAE\nCR-VAE\nSimVAE\nMoCo\nVicREG\nSimCLR\nMNIST\nARI\n89.0 ± 1.0\n93.3 ± 0.3\n94.0 ± 0.0\n93.1 ± 0.0\n58.3 ± 3.8\n72.0 ± 0.7\n77.4 ± 0.2\nNMI\n94.9 ± 0.4\n96.7 ± 0.2\n96.9 ± 0.0\n96.6 ± 0.0\n71.4 ± 2.5\n86.8 ± 0.4\n89.6 ± 0.1\nFashion\nARI\n44.3 ± 0.9\n53.3 ± 0.4\n47.6 ± 0.4\n56.8 ± 0.0\n30.9 ± 0.5\n41.2 ± 0.5\n33.2 ± 0.3\nNMI\n69.1 ± 0.6\n75.6 ± 0.1\n72.6 ± 0.1\n77.1 ± 0.0\n50.4 ± 0.6\n66.9 ± 0.3\n62.1 ± 0.2\nCelebA\nARI\n5.7 ± 0.2\n6.2 ± 0.7\n6.6 ± 0.9\n2.6 ± 0.7\n−\n18.7 ± 0.8\n0.0 ± 0.1\nNMI\n3.9 ± 0.2\n4.7 ± 0.9\n5.0 ± 0.7\n2.9 ± 0.7\n−\n24.3 ± 0.3\n0.0 ± 0.0\nCIFAR10\nARI\n0.6 ± 0.0\n2.9 ± 0.1\n2.0 ± 0.0\n12.2 ± 0.1\n27.2 ± 1.0\n25.7 ± 0.2\n52.2 ± 0.1\nNMI\n31.7 ± 0.0\n33.5 ± 0.1\n32.4 ± 0.0\n42.8 ± 0.1\n16.5 ± 0.4\n55.3 ± 0.1\n21.7 ± 0.1\nTable 6: Normalized mutual information (NMI) and Adjusted Rank Index (ARI) for all methods and datasets; Average\nscores and standard errors are computed across three runs\nnumber of augmentations result in a performance increase. Further exploration is needed to understand how larger sets\nof augmentations can be effectively leveraged potentially by allowing for batch size increase. From Figure 7, we fix our\nnumber of augmentations to 10 across datasets.\nLikelihood p(x|z) variance ablation We explore the impact of the likelihood, p(x|z), variance, σ2, across each pixel\ndimension on the downstream performance using the MNIST and CIFAR10 datasets. Figure 8 highlights how the predictive\nperformance is inversely correlated with the σ2 on the variance range considered for the CIFAR10 dataset. A similar ablation\nwas performed on all VAE-based models and led to a similar conclusion. We therefore fixed σ2 to 0.02 for β-VAE, CR-VAE\nand SimVAE across datasets.\nComputational Resources Models for MNIST, FashionMNIST and CIFAR10 were trained on a RTX2080ti GPU with\n11G RAM. Models for CelebA were trained on an RTX3090 GPU with 24G RAM. We observe that while the family\nof generative models requires more time per iteration, the loss convergence occurs faster while discriminative methods\nconverge at a slower rate when considering the optimal set of hyperparameters. As a consequence, generative baselines and\nSimVAE were trained for 400 epochs while discriminative methods were trained for 600 to 800 epochs.\nImage Generation We report the quality of images generated by SimVAE and considered baselines through visualisations\n(VAE-based approaches only) and quantitative measurements.\n20\nA Probabilistic Model to explain Self-Supervised Representation Learning\nFigure 6: Ablation experiment across the number of augmentations considered during training of the SimVAE model using\nthe MNIST (left) and FashionMNIST (right) datasets. Two, four, six and eight augmentations were considered. The average\nand standard deviation of the downstream classification accuracy using KNN and GMM probes are reported across three\nseeds.\nGenerated Images Figure 9 report examples of randomly generated images for each digit class and clothing item using the\nSimVAE trained on MNIST FashionMNIST, CIFAR10 and CelebA respectively.\nGenerative quality Table 7 reports the FID scores, reconstruction error for all generative baselines and SimVAE.\nMSE (↓)\nFID (↓)\nMNIST\nVAE\n0.029 ± 0.0\n150.1 ± 0.2\nβ-VAE\n0.029 ± 0.0\n155.3 ± 0.5\nCR-VAE\n0.030 ± 0.0\n153.0 ± 0.9\nSimVAE\n0.026 ± 0.0\n152.7 ± 0.3\nFashion\nVAE\n0.012 ± 0.0\n99.4 ± 0.6\nβ-VAE\n0.008 ± 0.0\n99.9 ± 0.7\nCR-VAE\n0.008 ± 0.0\n98.7 ± 0.0\nSimVAE\n0.009 ± 0.0\n96.1 ± 1.0\nCelebA\nVAE\n0.016 ± 0.0\n162.9 ± 2.8\nβ-VAE\n0.005 ± 0.0\n163.8 ± 2.3\nCR-VAE\n0.005 ± 0.0\n159.3 ± 5.4\nSimVAE\n0.004 ± 0.0\n157.8 ± 2.3\nCIFAR10\nVAE\n0.008 ± 0.0\n365.4 ± 3.3\nβ-VAE\n0.004 ± 0.0\n376.7 ± 1.7\nCR-VAE\n0.004 ± 0.0\n374.4 ± 0.4\nSimVAE\n0.003 ± 0.0\n349.9 ± 2.1\nTable 7: Generation quality evaluated by: mean squared reconstruction error (RE),\nfr´echet inception distance (FID). Mean and standard errors are reported across three\nruns.\n21\nA Probabilistic Model to explain Self-Supervised Representation Learning\nFigure 7: Ablation experiment across the number of augmentations considered during training of the SimVAE model using\nthe MNIST (left) and CIFAR10 (right) datasets. Two, four, six, eight and 10 augmentations were considered. The average\nand standard deviation of the downstream classification accuracy using Linear, MLP probes and a KNN & GMM estimators\nare reported across three seeds. Batch size of 128 for all reported methods and number of augmentations. Means and\nstandard errors are reported for three runs.\nFigure 8: Ablation experiment across the likelihood p(x|z) variance considered during training of the SimVAE model\nusing the MNIST (left) and CIFAR10 (right) datasets. The average and standard deviation of the downstream classification\naccuracy using Linear, MLP probes and a KNN & GMM estimators are reported across three seeds. Means and standard\nerrors are reported for three runs.\n22\nA Probabilistic Model to explain Self-Supervised Representation Learning\nFigure 9: Samples generated from SimVAE model using MNIST, FashionMNIST, Cifar10 and CelebA training datasets\n23\n"
}