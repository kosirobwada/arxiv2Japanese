{
    "optim": "1\nThis paper has been submitted to IEEE Transactions on Communications. Copyright may change without notice.\nEffective Communication with Dynamic Feature\nCompression\nPietro Talli, Student Member, IEEE, Francesco Pase, Graduate Student Member, IEEE,\nFederico Chiariotti, Member, IEEE, Andrea Zanella, Senior Member, IEEE, and Michele Zorzi, Fellow, IEEE\nAbstract—The remote wireless control of industrial systems\nis one of the major use cases for 5G and beyond systems: in\nthese cases, the massive amounts of sensory information that\nneed to be shared over the wireless medium may overload even\nhigh-capacity connections. Consequently, solving the effective\ncommunication problem by optimizing the transmission strategy\nto discard irrelevant information can provide a significant advan-\ntage, but is often a very complex task. In this work, we consider\na prototypal system in which an observer must communicate\nits sensory data to a robot controlling a task (e.g., a mobile\nrobot in a factory). We then model it as a remote Partially\nObservable Markov Decision Process (POMDP), considering the\neffect of adopting semantic and effective communication-oriented\nsolutions on the overall system performance. We split the commu-\nnication problem by considering an ensemble Vector Quantized\nVariational Autoencoder (VQ-VAE) encoding, and train a Deep\nReinforcement Learning (DRL) agent to dynamically adapt the\nquantization level, considering both the current state of the\nenvironment and the memory of past messages. We tested\nthe proposed approach on the well-known CartPole reference\ncontrol problem, obtaining a significant performance increase\nover traditional approaches.\nIndex Terms—Effective communication, Networked control,\nSemantic communication, Information bottleneck\nI. INTRODUCTION\nT\nHE main goal of classical communication theory is to\nbuild reliable systems for the accurate transmission of\narbitrary data through a constrained communication channel\nwhile using as few symbols as possible. However, in the pref-\nace to Shannon’s seminal work [1], Warren Weaver already en-\nvisioned two more complex Levels of communication beyond\nthe simple transmission of bits. Classical communications are\nthen included in Level A, or the technical problem, which\nconcerns itself with the accurate and efficient transmission\nof arbitrary raw data. Level B, or the semantic problem, is\nto find the best way to convey the meaning of the message,\neven when irrelevant details are lost or misunderstood, while\nLevel C, also called the effectiveness problem, deals with the\nresulting behavior of the receiver [2]: as long as the receiver\ntakes the optimal decision, the effectiveness problem is solved,\nregardless of the quality of the received information. The Level\nB and C problems are tightly intertwined, as defining the\nPietro Talli (corresponding author, pietro.talli@phd.unipd.it), Francesco\nPase (pasefrance@dei.unipd.it), Federico Chiariotti (chiariot@dei.unipd.it),\nAndrea Zanella (zanella@dei.unipd.it) and Michele Zorzi (zorzi@dei.unipd.it)\nare with the Department of Information Engineering, University of Padova,\n35131 Padua, Italy.\nThis work was supported by the European Union under the Italian National\nRecovery and Resilience Plan of NextGenerationEU, under the partnership on\n“Telecommunications of the Future” (PE0000001 - program “RESTART”) and\nthe “SoE Young Researchers” grant REDIAL.\nmeaning of a message is often related to the intentions of\nthe receiver.\nWhile the Level B and C problems attracted limited at-\ntention for decades, the explosion of Industrial Internet of\nThings (IIoT) systems has drawn the research and indus-\ntrial communities toward semantic and effective communi-\ncation [3], optimizing remote control processes under severe\ncommunication constraints beyond Shannon’s limits on Level\nA performance [2]. In particular, the effectiveness problem is\nhighly relevant to robotic applications, in which independent\nmobile robots, such as drones or rovers, must operate based\non information from remote sensors. In this case the sensors\nand the cameras act as the transmitter in a communication\nproblem, while the robot is the receiver: by solving the Level\nC problem, the sensors can transmit the information that best\ndirects the robot’s actions toward the optimal policy [4]. We\ncan also consider a case in which the robot is the transmitter,\nwhile the receiver is a remote controller, which must get the\nmost relevant information to decide the control policy [5].\nThe rise of communication metrics that take the content\nof the message into account, such as the Value of Infor-\nmation (VoI) [6], represents an attempt to approach the\nproblem in practical scenarios, and analytical studies have\nexploited information theory to define a semantic accuracy\nmetric and minimize distortion [7]. In particular, information\nbottleneck theory [8] has been widely used to characterize\nLevel B optimization [9]. However, translating a practical\nsystem model into a semantic space is a non-trivial issue,\nand the semantic problem is a subject of active research [10],\n[11]. The effectiveness problem is even more complex, as it\nimplicitly depends on estimating the effect of communication\ndistortion on the control policy and, consequently, on its per-\nformance [12]. While the effect of simple scheduling policies\nis relatively easy to compute [13], and linear control systems\ncan be optimized explicitly [14], realistic control tasks are\nhighly complex, complicating an analytical approach to the\nLevel C problem. Pure learning-based solutions that consider\ncommunication as an action in a multi-agent DRL problem,\nsuch as emergent communication, also have limitations [15],\nas they can only deal with very simple scenarios due to\nsignificant convergence and training issues. In some cases,\nthe information bottleneck approach can also be exploited to\ndetermine state importance [16], but the existing literature on\noptimizing Level C communication is very sparse, and limited\nto simpler scenarios [17]. Another possible approach to the\nremote tracking of Markov sources is addressed in zero-delay\ncoding theory [18]. However, this theory considers the error on\nthe hidden state estimate as the objective of the optimization,\narXiv:2401.16236v1  [cs.LG]  29 Jan 2024\n2\nwhich is similar to what we could consider a semantic (or\nLevel B) approach. In this work, we show that it is possible\nto optimize the system with respect to other metrics which\ncannot be explicitly derived such as cumulative rewards in\nDRL: by accepting a higher distortion at the semantic Level\nwhen it is not relevant to the task, we can further reduce the\nrequired bitrate without sacrificing the control performance.\nIn this work, we consider a dual model which combines\nconcepts from DRL and semantic source coding: we consider\nan ensemble of VQ-VAE models [19], each of which learns\nto represent observations using a different codebook. A DRL\nagent can then select the codebook to be used for each\ntransmission, controlling the trade-off between accuracy and\ncompression. Depending on the task of the receiver, the reward\nto the DRL agent can be tuned to solve the Level A, B, and C\nproblems, optimizing the performance for each specific task.\nIn order to test the performance of the proposed framework\nin a relevant example scenario, we consider the well-known\nCartPole problem, whose state can be easily converted into\na semantic one, as its dynamics depend on a limited set of\nphysical quantities. The problem we selected is purposefully\nsimple, as this allows for a better explainability and an\neasier training, but the solution is not limited to the CartPole\nproblem, and can be adapted to more complex tasks. The main\ncontributions of this paper are then given by the following:\n• We model a remote-control system as a remote POMDP\nproblem and present an efficient solution for learning ef-\nfective communication through the dynamic compression\nof learnable features;\n• We show that dynamic codebook selection outperforms\nstatic strategies for all three Levels, and that considering\nthe Level C task can significantly improve the control\nperformance without increasing the bitrate;\n• We adopt an explainability framework to understand\nthe choices of the agent in this simple problem, and\nverify that the Level C dynamic compression captures\nthe receiver’s uncertainty in the state estimation and its\nimpact on the expected reward, transmitting only when\nnecessary.\nWe remark that the dynamic codebook selection policy is\nnot limited to the VQ-VAE ensemble we consider, but is\na general technique that can be applied to any compression\nalgorithm with adaptable quality parameters, helping deliver\nmore accurate information when it is relevant to do so. The\nresults and policy analysis lead to significant insights for the\ndesign of communication strategies for remote control. A par-\ntial version of this work was presented at the IEEE INFOCOM\nWiSARN 2023 workshop [20]. This paper includes a more\ncomplete theoretical characterization of the problem, as well as\nan updated learning architecture, additional results, and an in-\ndepth analysis of the DRL-based dynamic compression policy.\nThe rest of the paper is organized as follows: first, we\nanalyze the state of the art on semantic and effective commu-\nnication in Sec. II. Sec. III then presents the general system\nmodel and the three Levels of communication we consider.\nWe then describe the dynamic feature compression solution in\nSec. IV, which is evaluated by simulation in Sec. V. Finally,\nSec. VI concludes the paper.\nII. RELATED WORK\nThe specific requirements of distributed and remotely con-\ntrolled systems have focused the research community’s at-\ntention towards communication systems that must provide\nupdated information to enable real-time high-level tasks such\nas inference, tracking or control. Although metrics such as\nAge of Information (AoI) [6] represent a major improvement\nwith respect to latency and packet loss, they are still limited, as\nthey assume that the quality of the information available at the\nreceiver degrades deterministically with time, most commonly\n(but not necessarily [21]) in a linear fashion. However, more\nsophisticated systems can also take into consideration the\ncurrent state of the system in order to decide whether and\nwhen to update the status of the receiver. Metrics such as\nUrgency of Information (UoI) [14] and VoI [22] incorporate\nstate information in their definition and are thus aware of\nthe intrinsic value of potential updates. Other context-aware\nindices to measure the nonlinear time-varying importance and\nthe non-uniform context dependence of the status information\nhave also been proposed [14]. The authors of [23] considered\na system in which a transmitter monitors the status of a system\nand updates the controller, providing status information. Then\na constrained Markov decision process (MDP) is formulated to\nminimize the cost of actuation and simultaneously guarantee\na target communication rate. Both works show significant\nimprovements with respect to other metrics such as AoI.\nAt the same time, the development of learning-based coding\nschemes has allowed communication system designers to\nmove beyond packet error as the key coding performance met-\nric, exploiting semantic considerations. Joint source-channel\ncoding for wireless image transmission is implemented in [24],\n[25], and the encoder-decoder pair is parameterized by a neural\nnetwork (NN), whose architecture may vary. This approach\ncan be used to maintain the semantic information contained\nin the transmitted data, while improving the compression\nperformance.\nSemantic information at the receiver can be used to solve\ndifferent tasks. Effective communication [12] can be seen as\nan extension of this, in which the task involves the receiver\ntaking actions and possibly altering the information that the\ntransmitter is communicating. Effective communication differs\nfrom semantic communication mostly because the “semantic”\ncontent which has to be preserved in the communicated mes-\nsages is not explicit. Moreover, control tasks have a temporal\ncomponent that must be taken into account, as investigated\nin [12]. The scenario considered in the work is a two-agent\nPOMDP in which one agent communicates and the other agent\ninteracts with the environment, using DRL to solve the joint\nproblem and encoding the information. A distributed percep-\ntion scenario, in which multiple sensors communicate to a\nsingle robot, is considered in [26] and solved using multi-agent\nreinforcement learning (MARL), showing that joint training\nimproves the performance of the system, particularly when\ncommunication is severely constrained. While past works\naimed at specific scenarios and objectives, this paper proposes\n3\na novel DRL approach that combines status updates with an\nadaptive coding scheme and can be easily adapted to operate\non any of the three Levels of communication (A, B, or C).\nIII. SYSTEM MODEL\nThe recent interest in semantic and effective communica-\ntions from the research community has driven the develop-\nment of a wide array of models and conceptualizations, as\nhighlighted in the previous section. At the highest level of\nabstraction, our purpose is to define a model in which effective\ncommunication is meaningful, and the differences between the\nthree problems in Weaver’s formulation become clear.\nLet us then consider a simple example: we have a remote\nactuator performing a control task, while a camera observes\nthe results and transmits its observation through a wireless\nchannel. The actuator might have its own sensors, but it relies\non the video feed to improve its performance and maintain\nstable and efficient control. The classical, Level A approach\nto the problem would be to compress the video as efficiently\nas possible, minimizing the reconstruction error on frames by\nusing an appropriate codec. The difference between Level A\nand Level B solutions is then obvious: the former encodes new\nframes so that the reconstruction fidelity is preserved, while\nthe latter maps elements in the frame to their importance when\nestimating the physical state of the system. In some control\napplications, the state can be defined trivially, while in others\nit may be more complex, but in general, the translation of the\nvideo to the state space is unaffected by irrelevant information\n(such as, e.g., movements in the background).\nIf we consider Level C, we target control performance\ndirectly, and thus further restrict the definition of relevant\ninformation: while Level B concerns itself with estimating the\nsystem state correctly, a Level C solution only considers errors\nin the state estimation when they cause performance drops.\nIf the control action is the same over a wide set of states,\naccuracy then becomes unnecessary, as the actuator only needs\na rough estimate of the state to decide what to do; the same\nhappens if there are multiple actions with almost equivalent\nperformance, i.e., if the optimality gap caused by imperfect\ninformation remains small.\nThese natural observations represent the core concepts of\neffective communication, but implementing them in practical\nsystems is often complex as actions have long-term con-\nsequences, and state estimates are based on a history of\nobservations, so that transmitting a message may affect future\nperformance in complex ways. In the following, we provide an\nanalytical framework using the remote POMDP approach to\nobjectively evaluate these choices and implement a solution\nfor effective communication in cyber-physical systems. We\nwill denote random variables with capital letters, their possible\nvalues with lower-case letters, and sets with calligraphic or\nGreek capitals. Table I reports the main symbols we introduce\nin the following sections for the reader’s convenience.\nA. POMDP Definition and Solution\nIn the infinite horizon POMDP formulation [27], one agent\nneeds to optimally control a stochastic process defined by\nTABLE I: Main notation and definitions.\nSymbol\nDefinition\nS\nSet of system states\nA\nSet of feasible actions\nO\nSet of observations\nP\nState transition probability function\nω\nObservation function\nR\nReward function\nγ\nDiscount factor\nh\nHistory of stochastic observations\nh(r)\nHistory of received messages at the robot\nπ\nPolicy\nG\nExpected cumulative discounted reward\nΦ(·)\nSpace of probability distributions over a set\nξ\nBelief distribution over the state space S\nξ(o)\nBelief distribution at the observer\nξpri,(r)\nPrior belief distribution at the robot\nξ(r)\nBelief distribution at the robot\nM\nSet of messages\nΛ\nEncoding function\nm\nMessage communicated\nℓ(m)\nLength of message m\nζ\nVector quantizer\nP\nPicture space\nβ\nCommunication cost\na tuple ⟨S, A, O, P, ω, R, γ⟩, where S represents the set of\nsystem states, A is the set of feasible actions, and O is\nthe observation set. The function P\n: S × A → Φ(S),\nwhere Φ(·) represents the space of probability distributions\nover a set, gives the state transition probability function.\nWe denote the conditional probability distribution of the\nnext state, given the current state and the selected action,\nas P(s′|s, a) = Pr [St+1 = s′|St = s, At = a]. Then, ω :\nS\n→\nΦ(O) is an observation function representing the\nprobability of an observation conditioned on the system state:\nω(o|s) = Pr [Ot = o|St = s], i.e., the probability of receiving\nobservation o, given that the system is in state s. Finally,\nfunction R : S × A → R represents the expected reward\nreceived by the agent when taking action a in state s, denoted\nas R(s, a), and the scalar γ ∈ [0, 1) is a discount factor used\nto compute the long-term reward. We notice that functions P,\nR, and ω do not depend on the time instant t, thus focusing\non homogeneous Markov processes.\nThe POMDP proceeds in discrete steps indexed by t: at\neach step t, the agent can infer the system state st only\nfrom the partial information given by the history of stochastic\nobservations ht = (ot, ot−1, . . . , o1) ∈ Ot. Based on these\nobservations, and on its policy π : Ot → Φ(A), which outputs\na probability distribution over the action space for each pos-\nsible observation history, the agent interacts with the system\nby selecting an action At ∼ π(ht). The sampled action at is\nthen performed in the real environment, whose hidden state\nst is unknown to the agent, which then receives a feedback\nfrom the environment in the form of a (potentially stochastic)\nreward rt, with expected value Rt = R(st, at).1 The goal for\nthe agent is then to optimize its policy π to maximize the\nexpected cumulative discounted reward G = E\nh P\nt γtRt\ni\n.\n1As the reward signal is only provided during training, it cannot be used\nto infer the value of st.\n4\nHaving an optimal policy is equivalent to knowing the optimal\nstate-action values, also known as Q-values, and taking action\nat = π(ht) = arg max\na∈A\nQ(ht, a),\nwhere Q(ht, a) = E [P∞\nτ=t γτ−trτ|ht, a] is the expected\ncumulative reward starting from (ht, a).\nHowever, considering the full history of observations makes\nthe solution highly complex, as the length of ht is potentially\nunbounded. We then define an estimator ξ : Ot → Φ(S),\nwhich outputs the a posteriori belief distribution over the\nstate space. We can then recast the original POMDP as a\nstandard MDP, whose state space is S′ = Φ(S), i.e., the space\nof possible belief distributions. Solving the POMDP in this\nmodified belief space has been proved to be optimal in [28].\nThe policy over this modified MDP is then π : S′ → A,\nwhich can be optimized using standard tools [29]. We can also\ncompute the new transition probability and expected reward as\nin [28]. Given ξt(s) = Pr (St = s | ht), which represents the\nmaximum likelihood estimate of the state given all the history\nof observations ht, and At = a, we define the a priori belief\nover the state at time t + 1 as\nξpri\nt+1 (s|ξt, a) = Pr [St+1 = s | ξt, At = a]\n=\nX\ns′∈S\nξt(s′)P(s|s′, a).\nThe a posteriori belief ξt+1, which also includes the new\nobservation ot+1, can then be obtained by performing a\nBayesian update using the a priori belief as a prior:\nξt+1 (s|ξt, a, o) = Pr [St+1 = s | ξt, At = a, Ot+1 = o]\n= Pr\nh\nSt+1 = s | ξpri\nt+1 (s|ξt, a) , Ot+1 = o\ni\n=\nω(o|s)ξpri\nt+1(s|ξt, a)\nP\ns′∈S ξpri\nt+1(s′|ξt, a)ω(o|s′)\n.\n(1)\nThese update equations allow us to compute the modified\ntransition probability matrix P ′ for the belief MDP, which\nis then defined by the tuple ⟨Φ(S), A, P ′, R, γ⟩.\nB. The Remote POMDP\nIn this paper we consider a variant of the POMDP, that we\ndefine remote POMDP, in which two agents are involved in the\nprocess. The first agent, i.e., the observer, receives observation\nOt ∈ O, and needs to convey such information to a second\nagent, i.e., the robot, through a constrained communication\nchannel, which limits the number of bits the observer can send\nreliably. Consequently, the amount of information the observer\ncan send to the robot is limited. The robot then chooses\nand takes an action in the physical environment. This system\ncan formalize many control problems in future IIoT systems,\nas sensors and actuators may potentially be geographically\ndistributed, and the amount of information they can exchange\nto accomplish a task is limited by the shared wireless medium,\nwhich has to be allocated to the many devices installed in the\nfactory, as well as by the energy limitations of the sensors.\nSimilar systems have been analyzed in [12], [23]. We will\nnow analyze the problems for the two agents, considering\na case in which communication and control are designed\nseparately. Joint control and communication approaches [26]\ncan outperform separate approaches by tuning the two agents’\npolicies to each other, but they introduce additional training\ncomplexity, and will not be considered in this work. In the\nfollowing, we will refer to variable x related to the robot as\nx(r), while the corresponding variable on the observer side\nwill be denoted by x(o).\n1) The Robot-Side POMDP: We denote the message com-\nmunicated to the robot at time step t as mt ∈ M. The\nset of possible messages M forms the set of observations\nthat are available to the robot, and the history of these\nobservations is given by h(r)\nt\n= {mt, . . . , m1}, which is the\nsequence of messages received up to time t. We can then see\nthe robot as an agent with its own POMDP, in which the\nobservations are filtered by both the partial knowledge of the\nobserver and the further distortion produced by the fact that\nthese observations are encoded and communicated through a\nconstrained channel. The robot-side POMDP is then defined\nby the tuple\n\nS, A, M, P, π(o), R, γ\n\u000b\n, as observations depend\non the observer’s policy.\nThe message transmitted from the observer to the robot\nmodifies the belief distribution over the next state as a\nBayesian update. Let us define the distribution over the current\nstate, given that the message mt has been received, as ξ(r)\nt\n. For\nexample, if the communicated message mt contains the correct\nstate St = s, the belief distribution becomes deterministic,\ni.e., ξ(s′ | mt) = δs,s′, where δm,n is the Kronecker delta\nfunction, equal to 1 if the two arguments are the same\nand 0 otherwise. Ideally, an intelligent observer will allocate\nmore communication resources and thus provide more precise\nmessages if the a priori distribution of the robot is far from\nthe one estimated by the observer. The modified MDP is\nthen defined by the tuple\n\nΦ(S), A, P (r), R, γ\n\u000b\n, where P (r)\nrepresents the Bayesian update function. According to the\nprevious notation, we can express the optimal action at time\nt + 1 as\nat+1 = arg max\na∈A\nQ\n\u0010\nξ(r)\nt\n, a\n\u0011\n,\nwhere ξ(r)\nt\nis the current belief at the robot side (after message\nmt is received). The robot’s reward is simply given as the\nreward of the original POMDP, i.e., the control performance\nin the environment. The optimal policy can be reached by\nusing standard DRL tools.\n2) The Observer-Side POMDP: On the other side, the\nobserver needs to encode its belief ξ(o)\nt\nin a message mt ∈\nM and transmit it. We can then consider the observer-side\nPOMDP, in which the action set corresponds to the set of\nmessages M and the state space is represented by the belief\nfrom the observed results. The tuple defining this POMDP is\n\nS, M, O, P, ω, R(o), γ\n\u000b\n. We assume that the observer knows\nthe robot’s policy, i.e., it can know the actions that the\nrobot takes in the environment and use them to improve\nits estimate of the state. This can also be accomplished if\nthe robot transmits the actions it takes as feedback to the\nobserver. As described above for the robot-side problem, we\ncan transform this POMDP into the belief MDP given by\n5\n\nΦ(S) × Φ(S), M, P (o), R(o), γ\n\u000b\n, where P (o) is the Bayesian\nupdate given in (1). We highlight that the observer needs\nto keep track of both its own and the robot’s belief, as the\neffectiveness of communication depends on the difference\nbetween the two, and the state of the observer is given by\nD\nξ(o)\nt\n, ξpri,(r)\nt\nE\n.\nThe objective of the observer is to minimize channel usage,\ni.e., communicate as few bits as possible, while maintaining\nthe highest possible performance in the control task: the\nexpected reward R(o) depends on both components. We then\nconsider a simple linear combination approach: if it transmits\nmessage m, whose length in bits is ℓ(m), the observer then\ngets a penalty βℓ(m), where β ∈ R+ is a cost parameter. In\norder to optimize its policy, the observer also needs to have\na way to gauge the value of information, which is a complex\nproblem: information theory, and in particular rate-distortion\ntheory, have provided the fundamental limits when optimizing\nfor the technical problem, i.e., Level A, where the goal is\nto reconstruct the source signals with the highest fidelity [30].\nWe will discuss the definition of VoI in the following sections.\nMore complex modeling choices for the transmission cost are\nalso possible, e.g., considering energy constraints for a sensor\nwith energy harvesting capabilities, but are beyond the scope\nof this work.\nAs the complexity of the problem is massive, we restrict\nourselves to a smaller action space by making a simplifying\nassumption, which allows us to separate the problem: the\nobserver does not transmit the entire belief distribution, which\nmay be implicit, but rather the observation Ot. We then\nconsider the encoding function Λ : O × Φ(S) → M, which\nwill generate a message Mt = Λ\n\u0010\nOt | ξ(o)\nt\n\u0011\nto be sent to the\nrobot at each step t.\nC. Observer Reward in Remote POMDPs\nThe first and simplest way to solve the remote POMDP\nproblem is to blindly apply standard Level A rate-distortion\nmetrics to compress the sensor observations into messages to\nbe sent to the agent. As an example, in the CartPole problem\nanalyzed in this work (see Sec. V), one sensor observation\nis given by two consecutive 2D camera acquisitions. The\nobserver’s policy is then independent of the robot’s task, and\ncan be computed separately. The Level A reward function R(o)\nA\nis then given as follows:\nR(o)\nA\n\u0010D\nξ(o)\nt\n, ξpri,(r)\nt\nE\n, m\n\u0011\n= −dA(ot, ˆot) − βℓ(m).\n(2)\nIn the CartPole case, a natural distortion metric is the image\nPeak Signal to Noise Ratio (PSNR), an image quality metric\nproportional to the logarithm of the normalized Mean Square\nError (MSE) between the images. Naturally, encoding the\nobservation with a higher precision will require more bits, as\nthe set of messages needs to be bigger.\nThe Level B problem considers the projection of the raw\nobservations into a significantly smaller semantic space, over\nwhich we measure distortion using function dB, explicitly\ncapturing the error over the needed physical system informa-\ntion, e.g., the angular position and velocity of the pole in the\nCartPole problem. The Level B reward function R(o)\nB\nis then\ngiven as follows:\nR(o)\nB\n\u0010D\nξ(o)\nt\n, ξpri,(r)\nt\nE\n, m\n\u0011\n= −dB\n\u0010\nξ(o)\nt\n, ξ(r)\nt\n\u0011\n− βℓ(m). (3)\nIn our CartPole case, this may be simply represented by the\nMSE between the best estimate of the state at the transmitter\nand the receiver.\nFinally, we can consider the Level C system. In this case,\nthe distortion metric is not needed, as the control performance\ncan be used directly, and the reward R(o)\nC\nis:\nR(o)\nC\n\u0010D\nξ(o)\nt\n, ξpri,(r)\nt\nE\n, m\n\u0011\n= R\n\u0010\nξ(r)\nt\n, π(r) \u0010\nξ(r)\nt\n\u0011\u0011\n− βℓ(m).\n(4)\nThe VoI of message m, V\n\u0010\nξpri,(r)\nt\n, m\n\u0011\n, can then be given by\nthe difference between the expected performance of the robot\nwith and without this information:\nV\n\u0010\nξpri,(r)\nt\n, m\n\u0011\n=Q\n\u0010\nξ(r)\nt\n, π(r) \u0010\nξ(r)\nt\n\u0011\u0011\n− Q\n\u0010\nξpri,(r)\nt\n, π(r) \u0010\nξpri,(r)\nt\n\u0011\u0011\n.\n(5)\nThus, the optimal Level C observer policy π(o)\nC\nwill balance\nthe trade-off between the performance at the receiver and the\ncommunication cost not only in the current time step but also\nin the long term. This foresighted behavior is essential when\nconsidering that the belief distributions incorporate the mem-\nory of previously received messages. Providing information\nthat does not improve the expected reward in the next step\nmight still be worth the cost if it allows the robot to improve\nits estimate, reducing the need for future communication.\nIV. PROPOSED SOLUTION\nIn this section, we introduce the architecture we used to\nrepresent Λ, the VQ-VAE, and discuss the remote POMDP\nsolution. As the VQ-VAE model is not adaptive, we consider\nan ensemble model with different quantization levels, limiting\nthe choice of the observer to which VQ-VAE model to use\nin the transmission. As we mentioned, directly learning the\nencoding is highly complex, with a vast action space, and\ntechniques such as emergent communication that learn it\nexplicitly are limited to scenarios with very simple tasks and\nimmediate rewards. By restricting the problem to a smaller\naction space, we find a potentially sub-optimal solution, but\nwe can deal with much more complex problems.\nA. Deep VQ-VAE Encoding\nIn order to represent the encoding function Λ, and to restrict\nthe observer-side POMDP to a more manageable action space,\nthe observer exploits the VQ-VAE architecture introduced\nin [19]. The VQ-VAE is built on top of the more common\nVariational Autoencoder (VAE) model, with the additional\nfeature of finding an optimal discrete representation of the\nlatent space. The VAE is used to reduce the dimensionality\nof an input vector X ∈ RI, by mapping it into a stochastic\nlatent representation Z ∈ RL ∼ qν(Z|X), where L < I.\nThe stochastic encoding function qν(Z|X) is a parameterized\nprobability distribution represented by a neural network with\n6\nparameter vector ν. To find optimal latent representations Z,\nthe VAE jointly optimizes a decoding function pθ( ˆX|Z) that\naims to reconstruct X from a sample ˆX ∼ pθ( ˆX|Z). This way,\nthe parameter vectors ν and θ are usually jointly optimized to\nminimize the distortion d(X, ˆX) between the input and its\nreconstruction, given the constraint on Z, while reducing the\ndistance between qν(Z|X), and some prior q(Z) [31] used to\nimpose some structure or complexity budget.\nHowever, in practical scenarios, one needs to digitally\nencode the input X into a discrete latent representation. To\ndo this, the VQ-VAE quantizes the latent space by using N\nK-dimensional codewords z1, . . . , zN ∈ RK, forming a dictio-\nnary with N entries. Moreover, to better represent 3D inputs,\nthe VQ-VAE quantizes the latent representation Z using a set\nof F blocks, each quantizing one feature f(X) of the input,\nand chosen from a set of N possible codewords. We denote the\nset containing all the N F possible concatenated blocks with\nM(N), as it represents the set of all possible messages the\nobserver can use to convey to the robot the information on the\nobservation O, by using F discrete N-dimensional features.\nThe peculiarity of the VQ-VAE architecture is that it jointly\noptimizes the codewords in M(N) together with the stochastic\nencoding and decoding functions qν and pθ, instead of simply\napplying fixed vector quantization on top of learned continuous\nlatent variables Z. When the communication budget is fixed,\ni.e., the value of L is constant, the protocol to solve the remote\nPOMDP is rather simple: first, the observer trains the VQ-\nVAE with N = 2F −1L to minimize the technical, semantic,\nor effective distortion dα, depending on the problem; then, at\neach step t, the observer computes ˆm ∼ qν(·|ot), and finds\nmt = arg minm∈M(N) ∥m − ˆm∥2. The message mt is sent to\nthe robot, which can optimize its decision accordingly.\nB. Dynamic Feature Compression\nWe can then consider the architecture shown in Fig. 1, con-\nsisting of a set of VQ-VAEs V = {ζ∅, ζ1, . . . , ζV }, where each\nVQ-VAE ζv compresses each feature using v bits. We also\ninclude a null action ζ∅, which corresponds to not transmitting\nanything. As we only consider the communication side of the\nproblem, the actor is trained beforehand using the messages\nwith the finest-grained quantization, which are compressed\nwith the VQ-VAE ζV with the largest codebook. This choice\nensures that the actor can deal with finer-grained inputs, while\nstill being robust to lower-precision features. The robot can\nthen perform three different tasks, corresponding to the three\ncommunication problems: it can decode the observation (Level\nA) with the highest possible accuracy, using the decoder part\nof the VQ-VAE architecture; it can estimate the hidden state\n(Level B) using a supervised learning solution; or it can\nperform a control action based on the received information\nand observe its effects (Level C).\nIn all three cases, the dynamic compression is performed\nby the observer, based on the feedback from the robot. The\nobserver side of the remote POMDP, whose reward is given\nin (2)-(4), is restricted to the choice of ζv, i.e., to selecting\none of the possible codebooks learned by each VQ-VAE in\nthe ensemble model, or to avoid any transmission. As we\nObservation\nDRL agent\nEnsemble\nVQ-VAE\nΨ∅\nΨ1\n. . .\nΨV\nObserver\not\nat\nDecoder\nSemantic\nestimate\nDRL agent\nActor\nmt\nˆot\nˆst\nrt\nFig. 1: Dynamic feature compression architecture.\ndescribed in the previous section, the type of reward depends\non the communication problem that the observer is trying to\nsolve: at Levels A and B, the observer aims at minimizing\ndistortion in the observation and semantic space, respectively.\nAt Level C, the objective is to maximize the robot’s reward.\nWe remark that the only Level at which the decision of the\nreceiver matters is Level C. The semantic estimate describes\nthe physical process, which models the dynamics of the control\nprocess that the Observer can sense. Optimizing the transmitter\nto minimize the reconstruction error in the semantic estimate\ndoes not consider the decision of the actor (receiver). In\ngeneral, the physical state of the system can carry redundant\nor irrelevant information with respect to the agent’s decision,\nand is not equivalent to Level C optimization.\nIn all three cases, memory is important: representing snap-\nshots of the physical system in consecutive instants, subse-\nquent observations have high correlations, and the robot can\nglean a significant amount of information from past messages.\nThis is an important advantage of dynamic compression, as it\ncan adapt messages to the estimated knowledge at the receiver\nside.\nWhile the observer is adapting its transmissions to the\nrobot’s task, the robot’s algorithms are fixed. They could\nthemselves be adapted to the dynamic compression strategy,\nbut this joint training is significantly more complex, and we\nconsider it as a possible extension of this work.\nC. RL implementation\nThere are two policies in the considered system, one for\nthe observer and one for the robot and in both cases the\npolicies are learned through the Actor Critic algorithm. This\nmeans that an agent learns a parametric policy πλ and a Q-\nvalues estimator. Both the policy and the Q-values are neural\nnetworks. In order to take into account the past observations\nthe two networks share a Long Short-Term Memory (LSTM)\nlayer which estimates a latent state which is then given as input\nto both the policy and the values estimator. This architecture\navoids explicitly modeling the belief distribution which may\nbe complicated to treat in continuous settings like the one\nconsidered in this work. This practical choice is also useful\nto avoid decoding the latent features discovered by the VQ-\nVAE back in the observation space O or in the physical\n7\nTABLE II: Simulation Parameters.\nParameter\nValue\nDescription\nH × W\n160 × 360\nImage size\nV\n7\nNumber of quantizers\nF\n8\nNumber of latent features\nDemb\n8\nEmbedding dimension of features\nB\n256\nBatch size\nγ\n0.95\nDiscount factor\nT\n500\nMaximum number of steps for an episode\nαenc\n10−3\nVQ-VAE learning rate\nαReg\n10−4\nRegressor learning rate\nαA2C\n10−4\nA2C learning rate\nD\n5 × 104\nSize of the VQ-VAE training dataset\nTenc\n100\nEncoder training epochs\nErob\n2 × 104\nRobot policy training episodes\nEobs\n1 × 105\nObserver policy training episodes\nEtest\n1000\nNumber of test episodes\nstate space S, increasing the potential for errors. Indeed, the\nquantized features communicated with the message mt contain\na structured representation of the observation space which can\nbe used effectively by an LSTM to estimate the true state.\nThe training algorithm is the standard Advantage Actor Critic\n(A2C), but the replay buffer is appropriately modified to take\ninto account the history of previously received messages.\nV. SIMULATION SETTINGS AND RESULTS\nThe underlying use case analyzed in this work is the well-\nknown CartPole problem, as implemented in the OpenAI Gym\nlibrary.2 In this problem, a pole is installed on a cart, and the\ntask is to control the cart position and velocity to keep the\npole in equilibrium. The physical state of the system is fully\ndescribed by the cart position xt and velocity ˙xt, and the pole\nangle ψt and angular velocity ˙ψt. Consequently, the true state\nof the system is st = (xt, ˙xt, ψt, ˙ψt), and the semantic state\nspace is S ⊂ R4 (because of physical constraints, the range\nof each value does not actually span the whole real line). The\nmain simulation parameters are reported in Table II.\nAt each step t, the observer senses the system by taking\na black and white picture of the scene, which is in a space\nP = {0, . . . , 255}180×360. To take the temporal element into\naccount, an observation Ot includes two subsequent pictures,\nat times t − 1 and t, so that the observation space is O =\nP × P. An example of the transmission process is given in\nFig. 2, which shows the original version sensed by the observer\n(above) and the reconstructed version at the receiver (below)\nwhen using a trained VQ-VAE ζ6, i.e., an encoder trained with\n6 bits per feature, the maximum we consider in this study.\nIn the CartPole problem, the action space A contains\njust two actions Left and Right, which push the cart to\nthe left or to the right, respectively. At the end of each\nstep, depending on the true state st, and on the taken ac-\ntion at, the environment will return a deterministic reward\nRt = −x−1\nmax|xt| − ψ−1\nmax|ψt|, ∀t, where xmax = 4.8 m and\nψmax = 2π\n15 rad (equivalent to 24◦) are the maximum values\nfor the two quantities. If the angle or cart position go outside\nthe boundaries, the episode is over, and the agents do not\naccumulate any more reward. The goal for the two agents is\n2https://www.gymlibrary.dev/environments/classic control/cart pole/\n(a) Original.\n(b) Reconstructed.\nFig. 2: Example of the original and reconstructed observation.\n0\n20\n40\n60\n15\n20\n25\n30\n35\nEpoch\nPSNR [dB]\n(a) PSNR.\n0\n20\n40\n60\n0\n20\n40\n60\nEpoch\nPerplexity\nTraining perplexity\nMaximum perplexity\n(b) Perplexity.\nFig. 3: Training of the VQ-VAE model ζ6, with 6 bits per\nfeature.\nthus to maximize the cumulative discounted sum of the reward\nRt, while limiting the communication cost.\nA. The Coding and Decoding Functions\nAs mentioned in Sec. III-C, the observer can optimize its\ncoding function Λ according to different criteria depending\non the considered communication problem. However, as we\nexplained in Sec. IV, optimizing Λ without any parameters\nis usually not feasible due to the curse of dimensionality on\nthe action space. Consequently, we rely on a pre-trained set V\nof VQ-VAE models, whose codebooks are optimized to solve\nthe technical problem, i.e., minimizing the distortion on the\nobservation measured using the MSE: dA(o, ˆo) = MSE(o, ˆo).\nThe training performance of the VQ-VAE with ζ6, i.e., using\nthe maximum value V of 6 bits per feature, is shown in Fig. 3:\nthe encoder converges to a good reconstruction performance,\nwhich can be measured by its perplexity. The perplexity is\nsimply 2H(p), where H(p) is the entropy of the codeword\nselection frequency, and a perplexity equal to the number of\n8\ncodewords is the theoretical limit, which is only reached if\nall codewords are selected with the same probability. The\nperplexity at convergence is 54.97, which is close to the\ntheoretical limit for a real application.\nThe observer then uses DRL to foresightedly choose the\nbest VQ-VAE ζv at each time step, maximizing the expected\nlong-term reward for each communication problem. We train\nthe observer to solve each specific coding problem by de-\nsigning three different rewards, depending on the considered\ncommunication Level (A, B, or C):\n1) Level A (technical problem): The distortion metric for\nthe observer is dA(ot, ˆot) = −PSNR(ot, ˆot), as part of\nthe reward definition from (2). The PSNR is an image\nfidelity measure proportional to the logarithm of the\nnormalized MSE between the original and reconstructed\nimage;\n2) Level B (semantic problem): The distortion metric is\ndB(ˆs(o)\nt , ˆs(r)\nt ) = MSE(ˆs(o)\nt , ˆs(r)\nt ), as part of the reward\ndefined in (3), and the decoder needs to estimate the\nunderlying physical state st by minimizing the MSE, i.e.,\nthe distance between ˆs(o)\nt\nand ˆs(r)\nt\nin the semantic space.\nIn our case, the estimator used to obtain the estimates\nis a pre-trained supervised LSTM neural network;\n3) Level C (effective problem): In this case, there is no\ndirect distortion metric, and the control performance is\nused directly as in (4). The policy π(r) is given by an\nactor-critic agent implementing an LSTM architecture,\npre-trained using data with the highest available message\nquality (6 bits per feature).\nIn this case, the task depends on all the semantic features\ncontained in St. However, the 4 components of the state do not\ncarry the same amount of information to the robot: depending\non the system conditions, i.e., the state St, some pieces of\ninformation are more relevant than others.\nB. Neural Network Architecture and Training\nThe VQ-VAE architecture is made with Convolutional Neu-\nral Network (CNN) layers to extract latent features and it is\ntrained separately before the training of the control policy.\nTo this end, a dataset of observations is collected through\na random policy. We then train an encoding network, the\nvector quantization layer and the decoder jointly as in the\nstandard VQ-VAE [19]. The first vector quantization layer\nlearned contains the highest number of codewords. Finally,\nwe fix the encoder and the decoder and just train the other\nvector quantization layers, obtaining multiple quantizers over\nthe same latent space discovered by a common encoder.\nThe hyperparameters used to train the VQ-VAE are reported\nin Table II. After obtaining the V quantizers, we train the\npolicy using the standard A2C algorithm. Table III shows\nthe Encoder-Decoder layers of the VQ-VAE. In Table IV,\nthe layers of the implemented Regressor and the Actor-critic\nneural networks are reported. All the NNs are implemented\nthrough the Pytorch library. Once the robot policy has been\nobtained, we can train the observer policy. The observer learns\na policy through the same A2C algorithm, but in this case\nthe input to the policy are the features before quantization.\nTABLE III: Encoder-Decoder parameters.\nLayer type\nSize\nKernel size\nStride\nEncoder\nConv2d + ReLU\n64\n10 × 11\n8 × 9\nConv2d + ReLU\n64\n12 × 12\n10 × 10\nConv2d + ReLU\n128\n3 × 3\n1 × 1\nResidualStack\n2\n3 × 3\n1 × 1\nConv2d\n8\n3 × 3\n1 × 1\nDecoder\nResidualStack\n2\n3 × 3\n1 × 1\nConv2d + ReLU\n128\n3 × 3\n1 × 1\nConv2d + ReLU\n64\n12 × 12\n10 × 10\nConv2d\n64\n10 × 11\n8 × 9\nTABLE IV: Recurrent architectures.\nLayer type\nInputs\nOutputs\nDescription\nRegressor\nLSTM + ReLU\n64\n64\nSingle recurrent layer\nLinear + ReLU\n64\n128\nHidden layer\nLinear\n128\n1\nOutput layer\nActor-critic\nLSTM + ReLU\n64\n64\nSingle recurrent layer\nLinear + ReLU\n64\n128\nHidden layer\nLinear\n128\n1\nOutput layer (Value)\nLinear + softmax\n128\n|A|\nOutput layer (Policy)\nA unique observer policy is trained for different values of the\ntrade-off parameter β and for different communication Levels.\nFor further details on the implementation, training and testing\nprocess, we refer to the publicly available simulation code.3\nThe additional computational cost of the architecture on the\nobserver side is well within the computational capabilities of\neven relatively simple embedded devices [32], and even more\ncomplex problems can be dealt with by Edge devices. At the\nsame time, training the actor with compressed representations\nactually reduces its computational burden, as the feature\nextraction is performed by the sender. However, if the sender\nis significantly computationally constrained, replacing the VQ-\nVAE with a classical compression scheme such as JPEG might\nbe a good way to reduce the cost and still deliver the benefits\nof dynamic compression.\nC. Results\nWe assess the performance of the three different tasks in\nthe CartPole scenario by simulation, measuring the results over\n1000 episodes after convergence. Fig. 4 shows the performance\nof the various schemes over the three problems, compared\nwith a static VQ-VAE solution with a constant compression\nlevel. In the Level C evaluation, we also consider a static VQ-\nVAE solution in which the robot is not retrained for each\nv, but is only trained for v = 6 bits per feature (i.e., 48\nbits per message, as the VQ-VAE considers 8 features) as\nfor the dynamic scheme. We trained the dynamic schemes\nwith different values of the communication cost β, so as to\nprovide a full picture of the adaptation to the trade-off between\nperformance and compression. We also introduce the notion of\nPareto dominance: an n-dimensional tuple η = (η1, . . . , ηn)\n3https://www.github.com/pietro-talli/tmlcn code\n9\nLevel A\nLevel B\nLevel C\nStatic VQ-VAE\nStatic VQ-VAE (without retraining)\n1\n2\n3\n4\n5\n6\n20\n25\n30\n35\nAverage message length ¯ℓ [B]\nObs. PSNR [dB]\n(a) Technical problem.\n1\n2\n3\n4\n5\n6\n0\n0.05\n0.1\n0.15\n0.2\nAverage message length ¯ℓ [B]\nState MSE\n(b) Semantic problem.\n1\n2\n3\n4\n5\n6\n0\n200\n400\nAverage message length ¯ℓ [B]\nEpisode length [steps]\n(c) Effective problem.\nFig. 4: Performance of the communication schemes on the three Levels of the remote POMDP.\nPareto dominates η′ (which we denote as η ≻ η′) if and only\nif:\nη ≻ η′ ⇐⇒ ∃i : ηi > η′\ni ∧ ηj ≥ η′\nj ∀j.\n(6)\nWe can extend this to schemes with multiple possible config-\nurations. The definition of Pareto dominance for schemes x\nand y is: x ≻ y\n⇐⇒\n∃η(x) ≻ η(y) ∀η(y), i.e., for each\nconfiguration of scheme y, there is a setting of x that Pareto\ndominates it. In other words, we can always tune scheme x\nso that it outperforms any configuration of scheme y on all\nmetrics.\nWe first consider the technical problem performance, shown\nin Fig. 4a: as expected, the Level A dynamic compression\noutperforms all other solutions, and its performance is Pareto\ndominant with respect to static compression. Interestingly, the\nLevel B and Level C solutions perform worse than static\ncompression: by concentrating on features in the semantic\nspace or the task space, these solutions remove information\nthat could be useful to reconstruct the full observation, but is\nmeaningless for the specified task.\nIn the semantic problem, shown in Fig. 4b, a lower MSE\non the reconstructed state is better, and the Level B solution\nis Pareto dominant with respect to all others. The Level A\nsolution also Pareto dominates static compression, while the\nLevel C solution only outperforms it for higher compression\nlevels, i.e., on the left side of the graph.\nFinally, Fig. 4c shows the performance of the effectiveness\nproblem (Level C), summarized by how long the CartPole\nsystem manages to remain within the position and angle limits.\nThe Level C solution significantly outperforms all others, but\nis not strictly Pareto dominant: when the communication con-\nstraint is very tight, setting a static compression and retraining\nthe robot to deal with the specific VQ-VAE used may provide\na slight performance advantage. In general, almost perfect\ncontrol can be achieved with less than half of the average\nbitrate of the static compressor, which can only reach a similar\nperformance at a much higher communication cost. We also\nnote that, in this case, the Level B solution performs worst:\nchoosing the solution that minimizes the semantic distortion\nis not always matched to the task, as it considers the state\nvariables as having equal weight, while a higher precision\nmay be required when the quantization error affects the robot’s\naction.\nAnother analysis is conducted on the way the CartPole is\ncontrolled with the different communication policies. Fig. 5\nshows the Angular Root Mean Squared Deviation (RMSD)\n(Fig. 5a) and the Position RMSD (Fig. 5b), defined as:\nRMSD(x) =\nv\nu\nu\nt1\nt\nt\nX\ni=1\n(xi − xtarget)2,\nwhere xtarget is the desired value of the controlled process\nand xi is the recorded value of the process at time step i.\nBoth RMSD are computed with respect to the central and\nvertical position of the CartPole: xtarget = 0 and ψtarget = 0.\nThese results help to evaluate how well the control dynamics\nkeep the CartPole near the optimal central position and to\nassess the smoothness of the resulting pole oscillations. It\nis possible to see that, in general, a higher rate allows to\nkeep the angular RMSD smaller. In particular, in the Level C\nsystem, the values are the smallest. However, this comes at the\ncost of deviating more from the central position, as shown in\nthe figure. The policy prioritizes the stabilization of the pole\noscillations, though this requires deviating from the central\n10\nLevel A\nLevel B\nLevel C\nStatic VQ-VAE\nStatic VQ-VAE (without retraining)\n1\n2\n3\n4\n5\n6\n0\n0.01π\n0.02π\n0.03π\n0.04π\nAverage message length ¯ℓ [B]\nRMSD(ϕ)[rad]\n(a) Angular RMSD from the central position.\n1\n2\n3\n4\n5\n6\n0\n2\n4\nAverage message length ¯ℓ [B]\nRMSD(x) [m]\n(b) Position RMSD from the central position.\nFig. 5: Other performance metrics relative to the CartPole control problem.\nLevel A\nLevel B\nLevel C\nCompressAI (retrained)\nCompressAI\nJPEG\n0\n0.5\n1\n1.5\n2\n200\n400\nlog10(ℓ) [B]\nEpisolde length [steps]\nFig. 6: Comparison with other methods.\nposition. This is because swings in the pole’s angle are harder\nto control due to the instability of the inverted pendulum, and\nthere is a significant risk that the pole might go out of the\nacceptable range, ending the episode.\nD. Comparison with existing compression approaches\nIn this section, we compare the performance of our proposed\napproach to that of other methods. Specifically, we show how\ndigital compression techniques, such as JPEG, perform in the\nsame scenario. We also compare other NN-based compression\nmodels and evaluate their performance. For the digital com-\npression, we use different sets of parameters combining image\nresizing, the quality parameter of the JPEG standard, and the\nnumber of color grayscale levels. For learning-based compres-\nsion, we used the CompressAI library which implements the\nmodel proposed in [33].\nFig. 6 shows the performance of other methods with respect\nto the proposed dynamic feature compression method. It is\npossible to see that the digital compression scheme does not\nallow the actor to effectively control the system, as its stability\nis low even though the updates are bigger by two orders\nof magnitude. Obtaining a high control performance would\nrequire an extremely high bitrate. On the other hand, the neural\ncompression scheme achieves a higher performance with a\nlimited bitrate, but since the model is a general compression\ntechnique designed to compress a wide variety of images,\nit cannot reach extremely low bitrates. After retraining the\nscheme on CartPole pictures, it is possible to obtain lower\nbitrates while improving the resulting control performance.\nOur approach, which is directly trained on the CartPole task,\noutperforms all others; however, we do not claim that VQ-VAE\nis the best compression technique for all scenarios. The main\ncontribution of this work is not in the specific compression\nscheme, but rather in the dynamic and goal-oriented adaptation\nof the compression parameters for each transmitted update,\nwhich could be directly applied to different NN architectures\nand even JPEG.\nE. Analysis of the communication policy\nWe can then use an explainability approach to gain further\ninsights on how effective communication operates. Fig. 7\nshows the distribution of the quantization level selected by\nan observer trained for each communication Level (A, B, and\nC). We note that we adapted the scale of β for the three Levels,\nso as to obtain comparable results: as the reward process takes\nvalues in different ranges (e.g., the PSNR is in dB while the\nreward of the MDP is between −1 and 1), using the same\ntransmission cost would result in very different outcomes.\nWe then chose to rescale the transmission cost parameter to\nhave the full range of average bitrates at each communication\nLevel. The similarity in the compression level distributions\nat the three Levels is striking. For lower values of β, the\nobserver selects ζ∅, which corresponds to no transmission,\nmore often. As β decreases, the communication cost becomes\nlower, and thus the observer chooses longer messages more\noften. Another common pattern is that quantizing features\nusing 1, 2 or 3 bits is a rare choice. This shows that the\nmemory implemented implicitly in the system through the\nLSTM is powerful enough to obtain adequate beliefs based\non past messages, so that the observer can rely on it and not\nsend anything. A roughly quantized update has a relatively low\nvalue, as its novelty is limited, and transmitting intermittent\nupdates at a higher quality results in a better performance.\nHowever, the real difference between the three policies is\ngiven by when they decide not to transmit. Therefore, we\npropose an analysis based on the visualization of the observer\npolicy and the receiver policy. In Fig. 8, four colormaps\nshow different policies projected in the same domain: the pole\nangle on the x-axis and the cart velocity on the y-axis. More\n11\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\nPr [ℓt]\n(a) Level A, β = 2.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(b) Level A, β = 1.5.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(c) Level A, β = 1.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(d) Level A, β = 0.5.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(e) Level A, β = 0.1.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\nPr [ℓt]\n(f) Level B, β = 0.01.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(g) Level B, β = 0.0075.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(h) Level B, β = 0.005.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(i) Level B, β = 0.0025.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(j) Level B, β = 0.001.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\nPr [ℓt]\n(k) Level C, β = 0.15.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(l) Level C, β = 0.1.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(m) Level C, β = 0.07.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(n) Level C, β = 0.05.\n0\n2\n4\n6\n0\n0.2\n0.4\n0.6\n0.8\n1\nℓt\n(o) Level C, β = 0.01.\nFig. 7: Distribution of the selected compression levels.\nspecifically, we quantize the projected state into cells and show\nthe policy of the robot and the observer in each cell. Fig. 8a\nshows the robot actions, averaged among 106 samples. Since\nin the CartPole problem the actions are binary, we represent\nthe probability of choosing action Right in a range between\n0 and 1. Fig. 8b depicts the entropy of the robot policy. We\nconsidered the action probability in the previous figure and\ncompute the action entropy as follows:\nH(a) = −p(a=0) log2 (p(a=0)) − p(a=1) log2 (p(a=1)) ,\nwhere p(a) is empirically estimated by counting the number of\ntimes each action is chosen when the state is in the projected\ncell. Fig. 8c and Fig. 8d show the average length of the update\npackets, i.e., the number of Bytes transmitted in each cell when\noptimizing for Levels A and C, respectively. This can be seen\nas the average number of bits that the transmitter allocates for\neach projected slice of the state space. Fig. 9 shows the same\nresults but for a different physical state projection, mapping\nthe angle ψ on the x-axis and the pole angular velocity ˙ψ on\nthe y-axis.\nIn both figures, there is a strong correspondence between\nthe states where the robot entropy is higher and the states\nwhere the Level C policy allocates a higher number of bits.\nThis confirms that an effective observer policy manages to\ndiscriminate the uncertainty at the robot side. In regions of\nthe state space where it is more difficult to retrieve the correct\naction, i.e., the action entropy is higher, the observer will\nprovide the robot with more precise information by sending\nlonger messages. There are regions where the robot action is\nalways the same, e.g., whenever the cart is moving fast and\nthe tip of the pole is pointing to the same side the cart is\nmoving towards. In these cases, the entropy is extremely low,\nand the transmitter can avoid sending new updates to the robot.\nThis is due to the fact that, even if the estimated state at the\nreceiver differs from the observed one, the action to perform\nremains the same and will be to push further the cart to try\nto get the pole more vertical. Recalling (5), we note that if\nQ\n\u0010\nξ(r)\nt\n, π(r)\u0010\nξ(r)\nt\n\u0011\u0011\nis very sensitive to small variations in\nξ(r)\nt\n, then the gap in (5) is going to be significant, leading the\nobserver to choose to send precise information. In principle, a\nLevel C transmitter could reduce the message length or even\navoid transmission as long as the robot is able to choose the\ncorrect actions, even though its belief is incorrect. An optimal\ncommunication scheme approximately follows\nℓt ∝ V\n\u0010\nξpri,(r)\nt\n, m\n\u0011\n,\nwhich means that the message length is roughly proportional\nto VoI. This concept might be used when defining a heuristic\npolicy, which behaves similarly to the effective communication\npolicy but is much simpler to design and implement. Note that\nthis condition includes two separate cases in which a Level\nC observer chooses not to transmit, while Level A and B\ntransmitters would send precise data:\n• The action corresponding to the prior belief is the same\nas the one after the updating message. In this case, the\nVoI of the communicated message is low and thus we\ncan lower ℓt;\n• The action is different after the communicated message,\nbut the long-term rewards are close enough that the robot\nis not going to benefit too much from choosing the other\n12\nLeft action\nRight action\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙x [m/s]\n0\n0.2 0.4 0.6 0.8\n1\n(a) Probability of taking action Right.\nH(at)\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙x [m/s]\n0\n0.2 0.4 0.6 0.8\n1\n(b) Entropy of the policy.\n¯ℓ\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙x [m/s]\n0\n2\n4\n6\n(c) Level A bitrate.\n¯ℓ\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙x [m/s]\n0\n2\n4\n6\n(d) Level C bitrate.\nFig. 8: Analysis of the transmission policy as a function of the pole angle ψ and cart linear velocity ˙x. The bitrate is measured\nin Bytes per transmission.\nLeft\nRight\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙ψ [rad/s]\n0\n0.2 0.4 0.6 0.8\n1\n(a) Probability of taking action Right.\nH(at)\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙ψ [rad/s]\n0\n0.2 0.4 0.6 0.8\n1\n(b) Entropy of the policy.\n¯ℓ\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙ψ [rad/s]\n0\n2\n4\n6\n(c) Level A bitrate.\n¯ℓ\n−0.2−0.1\n0\n0.1\n0.2\n−1\n0\n1\nψ [rad]\n˙ψ [rad/s]\n0\n2\n4\n6\n(d) Level C bitrate.\nFig. 9: Analysis of the transmission policy as a function of the pole angle ψ and angular velocity ˙ψ. The bitrate is measured\nin Bytes per transmission.\naction. Even in this case, sending less information is not\ngoing to affect the control performance significantly.\nThese cases cannot be taken into account in Levels A and\nB. Indeed, the Level A policy shown in Fig. 8c tends to\nallocate communication resources in the states where the\npicture is changing more rapidly, so that the memory available\nto the robot is less useful to estimate the current observation,\nregardless of the correct action. As the cart speed ˙x increases\nalong the y-axis, the number of bits increases too. The same\nreasoning can be applied to the results in Fig. 9.\nAnother general principle that we can deduce for an effec-\ntive policy is that it should be aware of variations of the value\nfunction with respect to the belief. If the value function is\nstrongly affected by small perturbations of the belief, then\nthe effective policy should communicate more information\nin order to reduce the discrepancy between ξ(o)\nt\nand ξ(r)\nt\n.\nThis reasoning can be intuitively understood by looking at the\ndifferential of the robot’s value function Q\n\u0010\nξ(r)\nt\n, π(r)\u0010\nξ(r)\nt\n\u0011\u0011\nwith respect to changes in its belief distribution ξ(r)\nt\n. When\nthis value is big, an inaccurate estimation of the state would\ncause a poor estimation of the value function, which may in\nturn cause the robot to choose a low-quality action.\nIn Fig. 10, we provide an analysis of the communication\nstrategy with respect to different AoI values. This allows to\nshow how the memory of the robot and of the observer plays\na crucial role on the communication decisions. In particular,\nwe consider five values of the AoI: AoI = 0 indicates that a\nmessage of any length was transmitted in the previous time\nstep. AoI = ∆ with ∆ ∈ {1, 2, 3, 4} means that no messages\nhave been received by the observer for ∆ time steps since\nthe last received message. This is a measure of how up to\ndate the memory of the robot is, allowing us to evaluate\nthe next choice of the observer for a given age. We then\nconsider the distribution of the observer actions (y-axis) with\nrespect to different ranges of the robot actions entropy (x-\naxis). This means that, for each entropy interval, we count the\nnumber of times each action is performed, in order to obtain\nan empirical distribution. The columns are normalized so that\neach cell shows the probability that the observer chooses a\nspecific ℓt whenever the robot action entropy falls within the\ncorresponding interval, for different values of the AoI.\n13\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\nℓt\n(a) β = 0.15, AoI=0\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(b) β = 0.15, AoI=1.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(c) β = 0.15, AoI=2.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(d) β = 0.15, AoI=3.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n0\n0.2\n0.4\n0.6\n0.8\n1\nPMF\n(e) β = 0.15, AoI=4.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\nℓt\n(f) β = 0.1, AoI=0.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(g) β = 0.1, AoI=1.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(h) β = 0.1, AoI=2.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(i) β = 0.1, AoI=3.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n0\n0.2\n0.4\n0.6\n0.8\n1\nPMF\n(j) β = 0.1, AoI=4.\nFig. 10: Level C observer action distribution for different robot action entropy levels and values of β. The color of each cell\nrepresents the empirical probability of choosing a packet length ℓt for a given entropy H(at) and AoI. Each column of each\nsubfigure then represents a conditional Probability Mass Function (PMF).\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\nℓt\n(a) AoI=0.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(b) AoI=1.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(c) AoI=2.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n(d) AoI=3.\n0 0.20.40.60.8 1\n0\n2\n4\n6\nH(at)\n0\n0.2\n0.4\n0.6\n0.8\n1\nPMF\n(e) AoI=4.\nFig. 11: Level A observer action distribution for different robot action entropy levels with β = 1.\nFig. 10a clearly shows that, if there was a transmission in\nthe previous time step (AoI = 0), it is very unlikely that the\nsystem is going to be updated again in the current time step.\nAs remarked above, Figs. 10a-e show the case with β = 0.15,\nfor which the observer almost always picks ζ4 to transmit.\nOn the other hand, Figs. 10f-j show the case with β = 0.1,\nin which the agent sometimes selects other codebooks due to\nthe lower transmission costs. The observer often chooses to\ncommunicate if AoI = 1, with an exception if the system is\nin a very low entropy state, in which case the probability of\ncommunicating using ζ4 is similar to the one corresponding to\naction ζ∅. If we look at the behavior for higher values of the\nAoI, we can notice a general trend: communication is more\nlikely to happen in higher entropy states than in lower entropy\nones. This shows that the observer policy understands the cases\nwhere the state has to be precisely estimated by the robot\nto choose its action correctly. Additionally, the probability\nof transmitting an update actually decreases when the AoI\nreaches higher values. The observer will only skip several\nconsecutive transmission opportunities in two cases: either the\nsystem state is highly predictable, and the actor can rely on\nits past knowledge to get a precise estimate of it, or the two\nactions are almost equivalent, e.g., if the pole is balanced\nvertically. The former case is more likely to be a low-entropy\nstate, while the latter is high-entropy, but has a small difference\nbetween the rewards for the two actions. We can see that the\ntrend holds for different values of β by looking at Figs. 10f-j:\nif we decrease the value of β, the observer tends to transmit\nmore often, and use higher message lengths when it transmits,\nbut the general tendency to transmit more whenever the robot\naction entropy is high clearly holds. This final analysis allows\nus to get an easy heuristic for effective communication when\nthe value function is not available or cannot be learned.\nFig. 11 shows that the Level A policy allocates communica-\ntion resources without considering the entropy of the control\nactions. While Fig. 10 showed a clear monotonic trend in the\nprobability of selecting ζ4, which increased as the entropy\nincreased, the pattern is much weaker in this case. The value\nof β for this was chosen to get a similar overall bitrate (and, as\nwe discussed, a similar overall action distribution) to the Level\n14\nC case with β = 0.15. As we discussed above, there is a weak\ncorrelation between the action entropy and features such as the\nangular and cart velocities, but it is the latter that the Level A\npolicy considers: as the difficulty of accurately reconstructing\nthe image increases with the speed of the CartPole system,\nmore unstable states have more frequent transmissions.\nVI. CONCLUSION\nIn this work, we presented a dynamic feature compression\nscheme that can exploit an ensemble VQ-VAE to solve the\nsemantic and effective communication problems. The dynamic\nscheme outperforms fixed quantization, and can be trained\nautomatically with limited feedback, unlike emergent commu-\nnication models that are unable to deal with complex tasks.\nThe choices made by the observer are clearly tied to the control\npolicy of the robot it aims to help, significantly outperforming\na simpler optimization that does not take into account the\nsemantic and effective problems. We also analyzed the optimal\npolicies to draw insights on their decisions, showing that the\nLevel C optimization indeed considers the robot’s policy.\nA natural extension of this model is to consider more com-\nplex tasks and wider communication channels, correspond-\ning to realistic control scenarios, or scenarios with multiple\ntransmitters with partial information about each other and the\nrobot. Considering a more realistic channel model, which has\na loss probability and time-varying statistics in addition to\nthe transmission cost, would also be an interesting direction,\njoining our model with Joint Source Channel Coding (JSCC)\ntheory. Dynamically adapting JSCC parameters with the goal\nof helping a remote DRL agent would be a natural extension of\nour proposed approach for more realistic wireless scenarios.\nAnother interesting direction for future work is to consider\njoint training of the robot and the observer, or cases with\npartial information available at both transmitter and receiver.\nREFERENCES\n[1] C. E. Shannon and W. Weaver, The mathematical theory of communi-\ncation.\nUniversity of Illinois Press, Sep. 1949.\n[2] D. G¨und¨uz, Z. Qin, I. E. Aguerri, H. S. Dhillon, Z. Yang, A. Yener,\nK. K. Wong, and C.-B. Chae, “Beyond transmitting bits: Context, seman-\ntics, and task-oriented communications,” IEEE J. Sel. Areas Commun.,\nvol. 41, pp. 5–41, Jan. 2023.\n[3] P. Popovski, O. Simeone, F. Boccardi, D. G¨und¨uz, and O. Sahin,\n“Semantic-effectiveness filtering and control for post-5G wireless con-\nnectivity,” J. Indian Inst. Sci., vol. 100, no. 2, pp. 435–443, Apr. 2020.\n[4] P. A. Stavrou and M. Kountouris, “A rate distortion approach to goal-\noriented communication,” in Int. Symp. Inform. Theory (ISIT).\nIEEE,\nJun. 2022, pp. 590–595.\n[5] S. Wan, Z. Gu, and Q. Ni, “Cognitive computing and wireless commu-\nnications on the edge for healthcare service robots,” Comput. Commun.,\nvol. 149, pp. 99–106, Jan. 2020.\n[6] R. D. Yates, Y. Sun, D. Richard Brown, S. K. Kaul, E. Modiano, and\nS. Ulukus, “Age of Information: An Introduction and Survey,” IEEE J.\nSel. Areas Commun., vol. 39, no. 5, pp. 1183–1210, Mar. 2021.\n[7] Y. Shao, Q. Cao, and D. G¨und¨uz, “A Theory of Semantic Communica-\ntion,” arXiv:2212.01485, Dec. 2022.\n[8] E. Beck, C. Bockelmann, and A. Dekorsy, “Semantic communication:\nAn information bottleneck view,” arXiv:2204.13366, Apr. 2022.\n[9] J. Shao, Y. Mao, and J. Zhang, “Learning task-oriented communication\nfor edge inference: An information bottleneck approach,” IEEE J. Sel.\nAreas Commun., vol. 40, no. 1, pp. 197–211, Jan. 2022.\n[10] E. Uysal, O. Kaya, A. Ephremides, J. Gross, M. Codreanu, P. Popovski,\nM. Assad, G. Liva, A. Munari, B. Soret et al., “Semantic communi-\ncations in networked systems: A data significance perspective,” IEEE\nNetwork, vol. 36, no. 4, pp. 233–240, Oct. 2022.\n[11] F. Pase, S. Kobus, D. G¨und¨uz, and M. Zorzi, “Semantic Communication\nof Learnable Concepts,” Int. Symp. Inform. Theory (ISIT), Jun. 2023.\n[12] T.-Y. Tung, S. Kobus, J. P. Roig, and D. G¨und¨uz, “Effective commu-\nnications: A joint learning and communication framework for multi-\nagent reinforcement learning over noisy channels,” IEEE J. Sel. Areas\nCommun., vol. 39, no. 8, pp. 2590–2603, Jun. 2021.\n[13] D. Kim, S. Moon, D. Hostallero, W. J. Kang, T. Lee, K. Son, and Y. Yi,\n“Learning to schedule communication in multi-agent reinforcement\nlearning,” in 7th Int. Conf. Learning Repr. (ICLR), May 2019.\n[14] X. Zheng, S. Zhou, and Z. Niu, “Urgency of Information for Context-\nAware Timely Status Updates in Remote Control Systems,” IEEE T.\nWireless Commun., vol. 19, no. 11, pp. 7237–7250, Jul. 2020.\n[15] J. N. Foerster, Y. M. Assael, N. de Freitas, and S. Whiteson, “Learning\nto communicate with deep multi-agent reinforcement learning,” in 30th\nInt. Conf. Neural Inf. Proc. Sys. (NeurIPS), Dec. 2016.\n[16] A. Goyal, R. Islam, D. Strouse, Z. Ahmed, H. Larochelle, M. Botvinick,\nS. Levine, and Y. Bengio, “Transfer and exploration via the information\nbottleneck,” in 7th Int. Conf. Learning Repr. (ICLR), May 2019.\n[17] F. Pase, D. G¨und¨uz, and M. Zorzi, “Rate-constrained remote contextual\nbandits,” IEEE J. Sel. Areas Inform. Theory, vol. 3, no. 4, pp. 789–802,\nDec. 2022.\n[18] E. Akyol, K. B. Viswanatha, K. Rose, and T. A. Ramstad, “On zero-\ndelay source-channel coding,” IEEE T. Inform. Th., vol. 60, no. 12, pp.\n7473–7489, Dec. 2014.\n[19] A. Van Den Oord, O. Vinyals et al., “Neural discrete representation\nlearning,” in 31st Int. Conf. Neural Inf. Proc. Sys. (NeurIPS), Dec. 2017.\n[20] P. Talli, F. Pase, F. Chiariotti, A. Zanella, and M. Zorzi, “Semantic\nand effective communication for remote control tasks with dynamic\nfeature compression,” in 16th Int. Worksh. Wireless Sensing Actuating\nRob. Networks (INFOCOM WiSARN).\nIEEE, May 2023.\n[21] A. Kosta, N. Pappas, A. Ephremides, and V. Angelakis, “The age of\ninformation in a discrete time queue: Stationary distribution and non-\nlinear age mean analysis,” IEEE J. Sel. Areas Commun., vol. 39, no. 5,\npp. 1352–1364, May 2021.\n[22] Z. Wang, M.-A. Badiu, and J. P. Coon, “A framework for characterizing\nthe value of information in hidden Markov models,” IEEE T. Inform.\nTheory, vol. 68, no. 8, pp. 5203–5216, Aug. 2022.\n[23] E. Fountoulakis, N. Pappas, and M. Kountouris, “Goal-oriented poli-\ncies for cost of actuation error minimization in wireless autonomous\nsystems,” IEEE Commun. Lett. (Early Access), Jun. 2023.\n[24] E. Bourtsoulatze, D. Burth Kurka, and D. G¨und¨uz, “Deep joint source-\nchannel coding for wireless image transmission,” IEEE T. Cognitive\nCommun. & Networking, vol. 5, no. 3, pp. 567–579, May 2019.\n[25] H. Xie, Z. Qin, G. Y. Li, and B.-H. Juang, “Deep learning enabled\nsemantic communication systems,” IEEE T. Signal Proc., vol. 69, pp.\n2663–2675, Apr. 2021.\n[26] F. Mason, F. Chiariotti, A. Zanella, and P. Popovski, “Multi-agent\nreinforcement learning for pragmatic communication and control,”\narXiv:2302.14399, Feb. 2023.\n[27] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and\nacting in partially observable stochastic domains,” Artif. Intell., vol. 101,\nno. 1, pp. 99–134, May 1998.\n[28] E. J. Sondik, “The optimal control of partially observable Markov\nprocesses over the infinite horizon: Discounted costs,” Oper. Res.,\nvol. 26, no. 2, pp. 282–304, Apr. 1978.\n[29] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction.\nMIT press, Nov. 2018.\n[30] T. M. Cover and J. A. Thomas, Elements of Information Theory, Second\nEdition.\nWiley-Interscience, Sep. 2006.\n[31] D. P. Kingma and M. Welling, “Auto-encoding variational Bayes,” in\n2nd Int. Conf. Learning Repr. (ICLR), May 2014.\n[32] P. Kang and J. Jo, “Benchmarking modern edge devices for AI applica-\ntions,” IEICE T. Inf. & Sys., vol. 104, no. 3, pp. 394–403, Mar. 2021.\n[33] Z. Cheng, H. Sun, M. Takeuchi, and J. Katto, “Learned image com-\npression with discretized Gaussian mixture likelihoods and attention\nmodules,” in Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), Jun. 2020.\n[34] V. Mnih, K. Kavukcuoglu, D. Silver et al., “Human-level control through\ndeep reinforcement learning,” Nature, vol. 518, no. 7540, pp. 529–533,\nFeb. 2015.\nAPPENDIX\nTHE INFORMATION BOTTLENECK VIEW\nWe can also consider another perspective on the observer’s\nchoices, using information bottleneck theory. We define a\n15\nsufficient statistic i(s) of any given state s ∈ S, which is\nenough to determine the robot’s performance in that state.\nDenoting the number of bits required to represent a realization\nof random variable X as b(X), we consider a case in which:\nb(i(S)) < b(S) < b(O).\nIndeed, the observation may contain much more information\nthan needed to estimate the state [30], and lossily compressing\nthe message to preserve the relevant information, removing\nredundant or irrelevant details, can ease communication re-\nquirements without any performance loss. We can also observe\nthat i(S) → S → O is a Markov chain. The random quantity\ni(S) represents the minimal description of the system with\nrespect to the robot’s task, i.e., no additional data computed\nfrom S adds meaningful information for the robot’s policy. The\nstate S may also include task-irrelevant physical information\non the system. However, both S and i(S) are unknown\nquantities, as the observer only receives a noisy and high-\ndimensional representation of S through O. This is a well-\nknown issue in DRL: in the original paper presenting the Deep\nQ-Network (DQN) architecture [34], the agent could only\nobserve the screen while playing classic arcade videogames,\nand did not have access to the much more compact and\nprecise internal state representation of the game. Introducing\ncommunication and dynamic encoding adds another layer of\ncomplexity.\nWe can then consider the case in which communication\nis limited to a maximum length of L bits, i.e., to 2L+1 − 1\nmessages, considering all possible lengths lower than or equal\nto L, including no communication. Naturally, this assumes\nthat the receiver has a way to discriminate between messages\nof different lengths, e.g., through a MAC layer header. The\nchannel is ideal, i.e., instantaneous and error-free, but it\nincludes a constant cost per bit β, as in the observer reward\nwe gave in the previous section. Consequently, the problem\nintroduces an information bottleneck between the observation\nOt and the estimate ˆot that the robot can make, based on\nthe message Mt conveyed through the channel. If we define a\ndistortion measure over the observation space dA : O2 → R+,\nany communication introduces a non-zero distortion dA(o, ˆo)\nwhenever b(o) > L, whose theoretical asymptotic limits\nare given by rate-distortion theory [30]. If we also consider\nmemory, i.e., the use of past messages in the estimation of ˆo,\nthe mutual information between o and the previous messages\ncan be used to reduce the distortion, improving the quality of\nthe estimate.\nIn the semantic problem, the aim is to extrapolate the\nreal physical state of the system St from the compressed\nobservation Mt, which can be a complex stochastic function.\nIn general, the real state lies in a low-dimensional semantic\nspace S. The term semantic is motivated by fact that, in\nthis case, the observer is not just transmitting pure sensory\ndata, but some meaningful piece of physical information about\nthe system. Consequently, the distortion to be considered in\nthis case can be represented by a measure dB : S2 → R\nover the semantic space, so that the distortion dB(ˆs(o)\nt , ˆs(r)\nt )\nis computed between the observer’s best estimate of the\nstate and the one performed by the robot based on Mt, and\non its memory of past messages. Finally, to be even more\nefficient and specific with respect to the task, the observer may\noptimize the message Mt to minimize a distortion measure\ndC\n\u0010\ni\n\u0010\nξ(o)\nt\n\u0011\n, i\n\u0010\nξ(r)\nt\n\u0011\u0011\nbetween the effective representation\nof the observer’s belief on the state, which contains only the\ntask-specific information, and the knowledge available to the\nrobot. Naturally, any message instance mt ∈ M must be at\nmost L bits long, in order to respect the constraint. However,\ndefining the sufficient statistic i\n\u0010\nξ(o)\nt\n\u0011\nmay be highly complex\nand problem-dependent, and using the robot’s reward as a\ndirect performance measure is significantly more direct, with\nthe same guarantees.\n"
}