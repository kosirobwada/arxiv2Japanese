{
    "optim": "ScripTONES: Sentiment-Conditioned Music\nGeneration for Movie Scripts\nVishruth Veerendranath, Vibha Masti, Utkarsh Gupta, Hrishit Chaudhuri, Gowri Srinivasa\nPES Center for Pattern Recognition, Department of Computer Science and Engineering\nPES University\n{vishruthnath, vsmasti, utkarsh348, hrishitchaudhuri}@gmail.com,\ngsrinivasa@pes.edu\nAbstract\nFilm scores are considered an essential part of the film cinematic experience, but\nthe process of film score generation is often expensive and infeasible for small-scale\ncreators. Automating the process of film score composition would provide useful\nstarting points for music in small projects. In this paper, we propose a two-stage\npipeline for generating music from a movie script. The first phase is the Sentiment\nAnalysis phase where the sentiment of a scene from the film script is encoded into\nthe valence-arousal continuous space. The second phase is the Conditional Music\nGeneration phase which takes as input the valence-arousal vector and conditionally\ngenerates piano MIDI music to match the sentiment. We study the efficacy of\nvarious music generation architectures by performing a qualitative user survey and\npropose methods to improve sentiment-conditioning in VAE architectures.\n1\nIntroduction\nFilm scores are a critical part of the cinematic experience and in high-production environments,\nthey require a significant amount of skill and time to produce. The challenge lies in being able to\ncorrectly identify abstract musical themes and motifs. The current state of musical score composition\nrelies on human knowledge of these properties, with the pipeline often starting with the composers\nseeing a rough cut of the script, analyzing the mood of the scene, and applying this knowledge of\nmotifs to compose music that matches the sentiment of the scene. The music is then ornamented with\ninstrumentalization, and the score is edited to match the film shots.\nIn this paper, we present ScripTONES (Script TO Notes with Emotional Signals), an automated\ntwo-stage pipeline to conditionally generate music for movie scenes. Unlike [34], the goal of\nScripTONES is to generate music for a short movie scene, and not to retrieve music to fit the\nsentiment of a book chapter. The first stage of the pipeline is the Sentiment Analysis phase where the\nsentiment of the movie scene is extracted from the script text. After preprocessing the scene text,\nwe capture its sentiment by extracting its Valence and Arousal [30] values according to the NRC\nVAD lexicon [22]. The second stage is conditional music generation. Here, we generate polyphonic\npiano MIDI music to match the valence and arousal of the scene extracted in the first stage. We\nexplore the current transformer-based [17, 14] and VAE-based [32, 39, 19] methodologies that allow\nfor sentiment-conditioned music generation. To allow for finer control from the creator in inspiring\nthe music generated, we propose regularisation losses for sentiment modification. We also compute\nattribute vectors for each quadrant of the Valence-Arousal space (henceforth VA space) and perform\n© Veerendranath et.al & ACM, 2024. This is the author’s version of the work. It is posted here for your\npersonal use. Not for redistribution. The Version of Record is to be published in \"The Third International\nConference on Artificial Intelligence and Machine Learning Systems Proceedings\", https://doi.org/10.\n1145/3639856.3639891\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2401.07084v1  [cs.MM]  13 Jan 2024\nScreenPy\nParser\nVA sentiment\nextractor\nMovie Script Scene\nEXT. THE SOLAR\nSYSTEM\nSpace, infinite and\nempty. But then, slowly\nall nine planets of ...\nParsed Scene\n[\n   {\n      \"head_type\": \"heading\",\n      \"text\": \"Space, infinite and   \n                  empty. But then ...\",\n      \"head_text\": { ... },\n  }, ...\n]\nMovie script scene\nParsed script scene\nVAi\n(V, A) tuple\nLatent vector\nof conditioned\nmusic\nMusicVAE\ndecoder\nVA Quadrant Number\nEMOPIA-CWT\nMIDI output\n1\nSentiment Analysis of Script\nConditional Music Generation\n2.a MusicVAE +\nAttribute Vector Arithmetic\n2.b Transformer-based EMOPIA-CWT\n2\nAttribute\nVector\nArithmetic\n+\nMusic piece latent\nrepresentation\n Random latent vector or\nencoded music MIDI\nFigure 1: Illustration of the ScripTONES pipeline, which consists of 2 major stages - Sentiment\nAnalysis of Scripts and Conditional Music Generation. Music Generation is achieved either with\nMusicVAE with attribute vector arithmetic or EMOPIA-CWT\nattribute vector arithmetic to modify the sentiment of a music piece. The VAE would also allow for\ninterpolation to match the changes in the sentiment of a scene. A detailed discussion of related work\nand data used can be found in Appendices D and E respectively.\n2\nScripTONES Pipeline\nThis section describes the pipeline of score generation process which has been illustrated in Fig. 1.\n2.1\nSentiment Analysis of Scripts\nA movie script contains dialogue as well as additional information that is used to produce and direct\nthe movie. We parse the script to only extract meaningful words that contribute to the sentiment of\nthe scene, as well as segment a script into individual scenes. For this, we use the ScreenPy parser\n[40] which relies on formal grammar rules that conform to the Hollywood Standard format.\nFor sentiment analysis, we use the NRC VAD [22] lexicon to find the unweighted average of valence\nand arousal values from every single word in the parsed movie scene. The value obtained is in\nthe range [0, 1], and further normalized to be in the range [-1, 1]. The movie scene Si is thus\nassociated with a Valence-Arousal (VA) tuple value VAi = (V, A), where −1 ≤ V, A ≤ 1.\n2.2\nConditional Music Generation\nBelow we describe the two models used in the pipeline. We propose and experiment with ways\n(regularization losses) to improve conditional generation with VAEs in Appendix A and C.1.\n2.2.1\nEMOPIA-CWT (Transformer-based)\nWe experimented with the Compound Word Transformer (CWT) trained on EMOPIA [17] for music\ngeneration (henceforth referred to as EMOPIA-CWT). This phase is labelled 2.b in Fig. 1. Here,\n2\nsentiment is not represented as a continuous (V , A) tuple, but as a quadrant number Q of the valence-\narousal space in the range [1, 4]. The EMOPIA-CWT generates music that fits the sentiment of the\ncorresponding quadrant.\nWhile the discrete quadrant method does not allow for latent space sampling, the music generated is\nquite realistic, polyphonic and fits the sentiment of movie scenes rather well.\n2.2.2\nMusicVAE tuned with Attribute Vector Arithmetic\nInspired by [32], we extend the idea of attribute vector arithmetic in VAEs to the sentiment attributes\nof valence and arousal in music. To formalize this, we define 4 emotional attribute vectors, namely\nHigh Valence (zvh), Low Valence (zvl), High Arousal (zah) and Low Arousal (zal) by averaging\nlatent vectors z (encoded using MusicVAE [32]) of all MIDI samples in the EMOPIA dataset [17].\nTo conditionally generate the music, we scale the attribute vectors based on threshold value α defined\non the V & A values extracted from the script (empirically α = -0.25), as per Eqn. 1. This results in\nan Emotionally Conditioned latent vector zec, which is then decoded back to music as a MIDI file.\nThis methodology enables conditioning music in a continuous space.\nzec =\n\n\n\n\n\n\n\n|V | ∗ zvh + |A| ∗ zah\n(V ≥ 0, A ≥ α)\n|V | ∗ zvh + |A| ∗ zal\n(V ≥ 0, A < α)\n|V | ∗ zvl + |A| ∗ zah\n(V < 0, A ≥ α)\n|V | ∗ zvl + |A| ∗ zal\n(V < 0, A < α)\n(1)\nWe introduce stochasticity by randomly sampling a point in the latent space (zr) and tuning the\nrandom point based on our emotional condition vector as per ztuned = zr + zec. We further enable\nfiner control for artists by replacing zr with the encoding for artist’s inspiration music piece (zinsp).\n3\nEvaluation and Results\nAdditional experiments are detailed in Appendix C. Demonstrations & samples are on our webpage *.\n3.1\nAttribute Vector Arithmetic\nTo demonstrate the validity of the attribute vector arithmetic, we show an example of conditional\nmusic generation with the pre-trained MusicVAE [32] model with attrbite vector arithmetic. A sample\nfrom the EMOPIA dataset [17] (Fig 2(a)) has been passed in as an inspiration piece and is encoded to\nlatent vector z. The emotional characteristics of this piece have then been modified using the High\nValence (zvh) and High Arousal (zah) attribute vectors. When zvh is added to z and then converted\nback to MIDI, the density of notes, as seen in Fig 2(b), is much higher than the original. Similarly,\nadding zah to z and converting back to MIDI (Fig 2(c)) causes the music to follow a similar note\nsequence as the original inspiration music, but intersperses it with some fast-paced staccato notes\nbetween the original note structure.\n(a) Original music\n(b) Modified with increased valence (c) Modified with increased arousal\nFigure 2: Original music (a) modified with Increased Valence (b) and Increased Arousal (c)\n*bit.ly/scriptones-script-music\n3\n(a) Discrete Latent Regularization\n100\n200\n300\n400\n500\nEpoch\n0.45\n0.50\n0.55\n0.60\n0.65\nDiscrete Reg Loss\n(b) Regularization loss during finetuning\nFigure 3: Discrete Regularization and loss plot while finetuning FIGARO on EMOPIA data\nTable 1: Average ratings of match between\ngenerated music and film scene\nAttribute Rated\nE-CWT\nMVAE\nValence\n2.62\n1.96\nArousal\n2.44\n1.92\nOverall Mood Fit\n2.48\n1.86\nTable 2: User evaluated scene-wise overall\nmood fit ratings on a scale of 1-4\nScene Number\nE-CWT\nMVAE\nScene 1\n2.52\n1.58\nScene 2\n1.87\n2.17\nScene 3\n3.06\n1.84\n3.2\nUser Study Evaluation of ScripTONES\nA group of 31 users (with varying music knowledge as seen in Fig. 4(b)) were surveyed to compare the\nquality of music generated by the different models for 3 movie scenes. For each scene, two different\nmusic pieces were played – one generated by MusicVAE and the other generated by EMOPIA-CWT\nas the music generators in the ScripTONES pipeline. For each piece of music, on a scale of 1-4, we\nrate the valence/positivity, the arousal/excitement and the overall mood fit of the music with respect\nto the mood of the scene.\nEMOPIA-\nCWT\nValence\nEMOPIA-\nCWT\nArousal\nEMOPIA-\nCWT\nOverall\nMusicVAE\nValence\nMusicVAE\nArousal\nMusicVAE\nOverall\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n(a) Box-plot of user ratings for E-CWT\n& MVAE models\n1\n2\n3\n4\nMusic Knowledge Rating\n0\n2\n4\n6\n8\nCount of Survey subjects\n(b) Music Knowledge rating of survey sub-\njects\nFigure 4: User preferences of music generation model and their music knowledge\nTable 1 presents the average valence, arousal and overall mood fit ratings (on a 4-point Likert scale),\nacross all users and all scenes in the study. A detailed box-plot of the ratings are shown in Fig. 4(a).\nThe EMOPIA-CWT model receives a better rating across all attributes. This can be attributed to the\ncompound word architecture of EMOPIA-CWT, which can generate more coherent and nuanced\nmusic with polyphony.\nWe analyse the scene-wise consensus (details in Appendix B) among users by averaging the overall\nmood fit ratings for each scene across all users in Table 2. We find that user’s prefer EMOPIA-CWT in\nscenes that require nuanced and sophisticated music with polyphony whereas MusicVAE is preferred\nin negative and action scenes.\n4\n3.3\nDiscrete Regularization\nTo improve sentiment-conditioning further in VAEs we propose a Discrete Regularization loss as per\nEqn. 2. An illustration can be seen in Fig. 3(a) and the regularization process is described in more\ndetail in Appendix A.2.2. We use this loss to finetune FIGARO’s [38] pretrained VAE model, on the\nEMOPIA dataset [17]. This helps FIGARO’s VAE model learn a better latent space representation\nfor sentiment as seen in Fig. 3(b) and detailed further in Appendix C.1.\nLregdisc = BCE(vpred, vgt) + BCE(apred, agt)\n(2)\n4\nConclusion and Future Work\nIn this paper, we present ScripTONES, which introduces new directions for research on sentiment-\nconditioned music generation for movie scripts. We obtain the sentiment encoding of movie scripts\nusing the NRC VAD lexicon for English words and use it to conditionally generate music using\nboth VAE and Transformer architectures, which we qualitatively evaluate by surveying users. We\npresent the ideas of attribute vector arithmetic and latent regularisation for sentiment modifications\nin the latent space of the VAE. In future work, variations of encoder-decoder architectures (GANs,\nGRU-VAEs) can be explored to improve the music generation capabilities of the model.\nReferences\n[1] Andrea Agostinelli, Timo I Denk, Zalán Borsos, Jesse Engel, Mauro Verzetti, Antoine Caillon,\nQingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al. Musiclm: Generating\nmusic from text. arXiv preprint arXiv:2301.11325, 2023.\n[2] Dmitry Bogdanov, Xavier Lizarraga Seijas, Pablo Alonso-Jiménez, and Xavier Serra. Musav: a\ndataset of relative arousal-valence annotations for validation of audio models. 2022.\n[3] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy\nBengio. Generating sentences from a continuous space. arXiv preprint arXiv:1511.06349,\n2015.\n[4] Sven Buechel and Udo Hahn. EmoBank: Studying the impact of annotation perspective and\nrepresentation format on dimensional emotion analysis. In Proceedings of the 15th Conference\nof the European Chapter of the Association for Computational Linguistics: Volume 2, Short\nPapers, pages 578–585, Valencia, Spain, April 2017. Association for Computational Linguistics.\n[5] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade,\nand Sujith Ravi. Goemotions: A dataset of fine-grained emotions. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pages 4040–4054, 2020.\n[6] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[7] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya\nSutskever. Jukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\n[8] Paul Ekman. An argument for basic emotions. Cognition & emotion, 6(3-4):169–200, 1992.\n[9] Jesse Engel, Matthew Hoffman, and Adam Roberts. Latent constraints: Learning to generate\nconditionally from unconditional generative models. arXiv preprint arXiv:1711.05772, 2017.\n[10] Lucas N Ferreira and Jim Whitehead. Learning to generate music with sentiment. arXiv preprint\narXiv:2103.06125, 2021.\n[11] Nathan Fradet, Jean-Pierre Briot, Fabien Chhel, Amal El Fallah Seghrouchni, and Nicolas\nGutowski. MidiTok: A python package for MIDI file tokenization. In Extended Abstracts\nfor the Late-Breaking Demo Session of the 22nd International Society for Music Information\nRetrieval Conference, 2021.\n5\n[12] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick,\nShakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a\nconstrained variational framework. In International conference on learning representations,\n2017.\n[13] Wen-Yi Hsiao, Jen-Yu Liu, Yin-Cheng Yeh, and Yi-Hsuan Yang. Compound word transformer:\nLearning to compose full-song music over dynamic directed hypergraphs. In Proceedings of the\nAAAI Conference on Artificial Intelligence, volume 35, pages 178–186, 2021.\n[14] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis\nHawthorne, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck. Music\ntransformer. arXiv preprint arXiv:1809.04281, 2018.\n[15] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua Lin. Movienet: A holistic\ndataset for movie understanding. In Computer Vision–ECCV 2020: 16th European Conference,\nGlasgow, UK, August 23–28, 2020, Proceedings, Part IV 16, pages 709–727. Springer, 2020.\n[16] Yu-Siang Huang and Yi-Hsuan Yang. Pop music transformer: Beat-based modeling and\ngeneration of expressive pop piano compositions. In Proceedings of the 28th ACM International\nConference on Multimedia, 2020.\n[17] Hsiao-Tzu Hung, Joann Ching, Seungheon Doh, Nabin Kim, Juhan Nam, and Yi-Hsuan Yang.\nEmopia: A multi-modal pop piano dataset for emotion recognition and emotion-based music\ngeneration. arXiv preprint arXiv:2108.01374, 2021.\n[18] IMSDb. The internet movie script database (imsdb).\n[19] Junyan Jiang, Gus G Xia, Dave B Carlton, Chris N Anderson, and Ryan H Miyakawa. Trans-\nformer vae: A hierarchical model for structure-aware and interpretable music representation\nlearning. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 516–520. IEEE, 2020.\n[20] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[21] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting\nof the association for computational linguistics: Human language technologies, pages 142–150,\n2011.\n[22] Saif M. Mohammad. Obtaining reliable human ratings of valence, arousal, and dominance\nfor 20,000 english words. In Proceedings of The Annual Conference of the Association for\nComputational Linguistics (ACL), Melbourne, Australia, 2018.\n[23] Saif M Mohammad and Peter D Turney. Crowdsourcing a word–emotion association lexicon.\nComputational intelligence, 29(3):436–465, 2013.\n[24] Surabhi S Nath, Vishaal Udandarao, and Jainendra Shukla. It’s levasa not leviosa! latent\nencodings for valence-arousal structure alignment. In Proceedings of the 3rd ACM India Joint\nInternational Conference on Data Science & Management of Data (8th ACM IKDD CODS &\n26th COMAD), pages 238–242, 2021.\n[25] Pedro Neves, Jose Fornari, and João Florindo.\nGenerating music with sentiment using\ntransformer-gans. arXiv preprint arXiv:2212.11134, 2022.\n[26] Jianmo Ni, Jiacheng Li, and Julian McAuley. Justifying recommendations using distantly-\nlabeled reviews and fine-grained aspects. In Proceedings of the 2019 conference on empirical\nmethods in natural language processing and the 9th international joint conference on natural\nlanguage processing (EMNLP-IJCNLP), pages 188–197, 2019.\n[27] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan. This time with\nfeeling: Learning expressive musical performance. Neural Computing and Applications, 2018.\n6\n[28] Ashis Pati and Alexander Lerch. Attribute-based regularization of latent spaces for variational\nauto-encoders. Neural Computing and Applications, 33:4429–4444, 2021.\n[29] Robert Plutchik. The emotions. University Press of America, 1991.\n[30] Jonathan Posner, James A Russell, and Bradley S Peterson. The circumplex model of affect:\nAn integrative approach to affective neuroscience, cognitive development, and psychopathology.\nDevelopment and psychopathology, 17(3):715–734, 2005.\n[31] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified\ntext-to-text transformer. The Journal of Machine Learning Research, 21(1):5485–5551, 2020.\n[32] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck. A hierarchical\nlatent vector model for learning long-term structure in music. In International conference on\nmachine learning, pages 4364–4373. PMLR, 2018.\n[33] Klaus R Scherer and Harald G Wallbott. Evidence for universality and cultural variation\nof differential emotion response patterning. Journal of personality and social psychology,\n66(2):310, 1994.\n[34] Jaidev Shriram, Makarand Tapaswi, and Vinoo Alluri. Sonus texere! automated dense sound-\ntrack construction for books using movie adaptations.\n[35] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y\nNg, and Christopher Potts. Recursive deep models for semantic compositionality over a\nsentiment treebank. In Proceedings of the 2013 conference on empirical methods in natural\nlanguage processing, pages 1631–1642, 2013.\n[36] Hao Hao Tan and Dorien Herremans. Music fadernets: Controllable music generation based on\nhigh-level features via low-level feature modelling. arXiv preprint arXiv:2007.15474, 2020.\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information\nprocessing systems, 30, 2017.\n[38] Dimitri von Rütte, Luca Biggio, Yannic Kilcher, and Thomas Hofmann. Figaro: Generating\nsymbolic music with fine-grained artistic control. arXiv preprint arXiv:2201.10936, 2022.\n[39] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Junbo Zhao, and Gus\nXia. Pianotree vae: Structured representation learning for polyphonic music. arXiv preprint\narXiv:2008.07118, 2020.\n[40] David R Winer and R Michael Young. Automated screenplay annotation for extracting story-\ntelling knowledge. In Thirteenth Artificial Intelligence and Interactive Digital Entertainment\nConference, 2017.\n[41] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. Advances in\nneural information processing systems, 32, 2019.\n[42] Yelp. Yelp open dataset.\n[43] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu. Musicbert: Symbolic\nmusic understanding with large-scale pre-training, 2021.\n7\nA\nImproving Sentiment-Conditioning of Music with VAEs\nWe propose a few methods to improve the affective characteristics of the music generated by a\nvariational autoencoder (VAE) to match the sentiment of the script more closely. These methods are\nenabled by three properties of VAEs, which we describe below.\nA.1\nAttribute Vector Arithmetic\nAs described in Section 2.2.2, we can extract the characteristics for each type of sentiment in the\nembedding space and use it to tune the music generated. While we demonstrate the use of attribute\nvector arithmetic with the four quadrants of the valence-arousal space as our distinct sentiments, this\ncan also be used for other distinct sentiment models like positive-negative (2 categories), Plutchik’s\nwheel of emotions 8 categories) [29] and Ekman’s model (6 categories) [8]. However, we cannot\nextract attribute vectors for all sentiments in a continuous sentiment model.\nA.2\nLatent Space Regularization\nTo handle continuous sentiment models, we consider the work done by Pati et al. [28], which extended\nthe ideas of latent space regularization [9] and disentanglement [12] for music. The regularization\nloss is capable of constraining a particular dimension of the latent space to reflect monotonicity\nin a particular attribute of the data. Regularizing one dimension of the latent space each for the\nattributes of valence and arousal will enable direct control of valence and arousal by simply increasing\nor decreasing the corresponding dimension. This is much like the faders described in [36] but for\nhigh-level sentiment attributes.\nWe propose regularizing the latent space in the following two ways.\nA.2.1\nContinuous Regularization\nThe first, which we call Continuous Regularization is closely related to [28] where we define\nregularization losses for valence and arousal as per the equations\nLregv = MSE(tanh(Dz1) − sgn(Dv))\n(3)\nLrega = MSE(tanh(Dz2) − sgn(Da))\n(4)\nand Lregcont = Lregv + Lrega, where Dz1 and Dz2 are the sum of differences in z1 (1st dimension\nfor valence) and z2 (2nd dimension for arousal) respectively, computed pairwise over all pairs of data\npoints in the mini-batch. Dv and Da are the attribute distances i.e. the sum of differences in the\nvalence and arousal values of the pairs of data points. sgn(.) refers to the sign (positive or negative).\nDue to the unavailability of a dataset of music annotated with continuous valence and arousal values,\ncomputation of the attribute distances is not directly possible. We suggest the use of the valence-\narousal regressors introduced in MusAV [2] to predict the valence and arousal values of each piece of\nmusic, thereby enabling the computation of Da and Dv. However, the valence-arousal predictions\nfrom the regressors can only be considered noisy labels (true labels in [2] are relative valence-arousal\nand not absolute values) and might lead to inaccurate regularization.\nA.2.2\nDiscrete Regularization\nTo use true labels instead of noisy predictions as labels for regularization, we propose a second\nmethod for regularization called Discrete Regularization, inspired by [24]. Here, we use the discrete\nVA quadrant annotations in the EMOPIA dataset [17] as an approximation for the continuous VA\nvalues.\nHere again, we regularize z1 for valence and z2 for arousal. This is done by passing the z1 and z2\nthrough a simple single hidden layer neural network with a single output neuron. Sigmoid activation\nis applied to the output neuron to predict the probability of whether the corresponding attribute\n(valence or arousal) is high (1) or low (0). The regularization loss is formulated as\nLregdisc = BCE(vpred, vgt) + BCE(apred, agt)\n(5)\n8\nwhere BCE(.) refers to Binary Cross Entropy Loss, vpred and apred are the sigmoid probabilies\nfor valence and arousal values. vgt and agt are ground truth values for valence and arousal based\non the quadrant annotations. For instance, for a music piece with annotation as quadrant 4, the\nvalence is high and the arousal is low. This implies vgt = 1 and agt = 0. An illustration of Discrete\nRegularization can be seen in Fig. 3(a).\nOnce either Discrete or Continuous regularization has been performed during training, the valence\nand arousal attributes can be manipulated by simply changing the values of the dimensions z1 and\nz2 for any piece of music. This is done either as per V Ai extracted from the script or a slider, by\nperforming a simple addition z1 = z1 + V and z2 = z2 + A before decoding the music.\nA.3\nLatent Space Interpolation\nSince attributes of the data are encoded within the latent space by the VAE, interpolation in the latent\nspace of the VAE would allow us to smoothly change the attributes in a piece of music. This would\nbe particularly useful when the sentiment changes over the course of a scene. For instance, the start\nof the scene could have low arousal while the end of the scene has high arousal. The changes in\nsentiment can be captured by extracting V A values at a sentence level and manipulating z for the\nstart and end of a scene using V Astart and V Aend which refer to the V A values for the first and last\nsentence of a scene respectively. The pieces of music for the other parts of the scene can be filled\nusing interpolation between zstart and zend, as can be seen in Fig. 5.\nFigure 5: Latent Space Interpolation\nB\nScene-Wise Analysis of User Survey\nWe analyse the scene-wise consensus among users by averaging the overall mood fit ratings for\neach scene across all users in Table 2. Scene 1 — a dramatic, high-impact and large-scale sequence\n— requires nuanced and sophisticated music to enhance the experience, which EMOPIA-CWT is\nable to do better with its polyphony. In Scene 2 — which is a negative, action-packed fight scene —\nMusicVAE does better, as EMOPIA-CWT is known to struggle with music with negative sentiment.\nAdditionally, the action in the scene is paralleled better in MusicVAE due to attribute vector arithmetic,\nwhere the addition of the high arousal vector (zah) induces fast-paced notes in music. EMOPIA-CWT\nagain does better in Scene 3 as it is an emotional scene with almost no action sequence.\nC\nAdditional Experiments\nC.1\nFinetuning VAE with Discrete Regularization\nFIGARO [38] is a recent work that uses VAEs to extract high-level \"descriptions\" at a bar-level,\nand then uses the sequence of descriptions to generate coherent music with a Seq2Seq Transformer\nmodel. We finetune FIGARO’s [38] pretrained VAE model, on the EMOPIA dataset [17]. This is\ndone by adding a discrete regularization component to the VAE training loss as per Eqn 5, to improve\nsentiment conditioning (described in 3.3). This helps FIGARO’s VAE model learn a better latent\nspace representation for sentiment, in addition to the reconstruction properties carried over from the\npretraining.\n9\nIn Fig. 3(b), the discrete regularization loss continues to decrease during finetuning which indicates\nthat the model is able to predict the Valence and Arousal values of the piece of music better after\nfinetuning. However, the finetuning process is very slow and takes many epochs to converge as the\ngradient from the discrete regularization loss component is much smaller relative to the reconstruction\nand KL divergence loss components of the VAE.\nC.2\nPolyphonic Recurrent VAE\nExperiments were conducted to train a simple Recurrent VAE baseline for polyphonic music with\na similar architecture to [3]. We use a bidirectional LSTM as the encoder and a vanilla LSTM as\nthe decoder, with one layer each. During training, the input music is fed into the bi-LSTM encoder.\nThe hidden vector of the last state of the bi-LSTM is mapped to the latent dimensions through a\nfully connected layer, which is then fed into the LSTM decoder to output music. We additionally\nconcatenate the latent vector z with the input embedding at every time step of the decoder.\nThe polyphonic recurrent VAE however suffers from posterior collapse, where the LSTM decoder\nsimply ignores the latent vector z and learns to output a repeating sequence of notes. This is especially\nthe case when teacher forcing rate † is set to 0 in the decoder. When the teacher forcing rate is set to\n1, the problem of repeating notes is solved, but the reconstruction for all pieces of music remains the\nsame due to posterior collapse.\nWe detail the results of all our experiments, as well as the hyperparameters chosen in the Supplemen-\ntary Material.\nD\nRelated Work\nD.1\nSentiment Analysis\nOne-dimensional sentiment polarity analysis, commonly referred to as sentiment analysis, refers\nto identifying whether textual data conveys a positive or negative sentiment. Many benchmark\ndatasets for sentence-level and phrase-level binary classification [35, 21, 42, 26] have been developed.\nPre-trained Transformer-based models [31, 6, 41, 20] have been fine-tuned on the benchmark datasets\nto obtain state-of-the-art binary sentiment classification.\nTo capture more complex sentiments, sentiment analysis techniques based on discrete sentiment\ncategories have been explored. Few public datasets exist for multi-emotion classification. EmoLex\n[23] maps English language words to 8 discrete emotion categories. ISEAR [33] and GoEmotions\n[5] are multi-emotion sentence classification datasets with 28 and 7 emotion classes respectively.\nThe NRC VAD [22] lexicon maps 20,000 English words to a continuous space of real-valued valence,\ndominance and arousal scores. For sentence-level continuous emotion regression, the EmoBank\n[4] dataset contains 10,000 sentences labelled with valence, arousal and dominance values. The\ndrawback of this dataset is that there is an imbalance wherein most sentences are labelled with VA\nvalues in the range of 2 to 3.\nD.2\nMusic Generation\nMusic generation has been approached in two major ways. The first is to generate music as raw\naudio (a .wav, for instance) [7, 1]. The second is to generate music in the symbolic space either as a\nprobability over all possible notes for each time step [32, 14] or as a sequence of tokens in one of\nmany tokenized representations of music [27, 16, 43, 13], which can then be converted to a MIDI\n(.mid) file. We discuss approaches for symbolic music generation due to the complexities associated\nwith raw audio domain music generation.\nVariational autoencoders for generating music were popularised by MusicVAE [32], a hierarchical\nvariational autoencoder with recurrent neural networks (RNNs) as encoder and decoder.\n†percentage of times ground truth is fed back as input to decoder instead of previous decoder output\n10\nTo decompress the latent representation while avoiding posterior collapse, a hierarchical RNN decoder\nwas used. However, the output produced by the MusicVAE architecture is limited to monophonic\nmusic and is unable to produce polyphonic music ‡.\nThe authors also proposed using attribute vector arithmetic in the latent space but limited it to simpler\nattributes like note density. More recent VAE approaches for music like [39, 19, 38] have handled\npolyphonic music in a VAE. In addition to VAEs, transformers [37] have also been used widely to\ngenerate music [14, 13].\nVarious approaches have recently been proposed specifically for sentiment-conditioned music gen-\neration. Lucas et al. [10] propose a combination of mLSTM and a logistic regression model. The\nMIDI files are represented as a series of events, relevant to features such as timbre, harmony, tempo,\netc. The sentiment is interpreted from these features to create the partially annotated VG-MIDI\ndataset (annotated with a valence-arousal pair). The generative mLSTM is trained on the unlabeled\ndataset and the logistic regressor uses the hidden state to encode MIDI phrases and predict statements.\nHowever, it fails to accurately generate music for negative sentiments and cannot handle inputs more\ndetailed than a given set of labels/emotions.\nHung et al. [17] introduced the EMOPIA dataset, a collection of labelled pop-piano midi files based\non the valence-arousal model of sentiment. The authors provided baselines for music classification\nand sentiment-conditioned music generation. The compound word transformer (CWT) [13] was the\nbaseline proposed as a sentiment-conditioned music generator. This however is limited to generating\nmusic conditioned only by a given quadrant of the VA space. .\nOther recent approaches to this problem include Transformer-GANs [25] and Music FaderNets [36].\nThe first proposes using an additional adversarial loss component to the usual negative log-likelihood\n(NLL) loss utilized to train transformers for music generation, as a way to prevent the degradation\nof quality in longer sequences of music. However, the conditioning control is again only limited\nto the quadrant number and does not allow for sentiment modifications to a given piece of music.\nMusic FaderNets aimed at bypassing the question of formalizing high-level notions like \"sentiment\"\nby trying to force the model to infer them from a dataset of low-level features using semi-supervised\nclustering. Currently, however, FaderNets are trained only to be conditioned on arousal.\nE\nData and Dataset Preprocessing\nFor movie scripts, we use the IMSDb [18] and MovieNet [15] databases to source movie scripts. For\nsentiment analysis of text, we use the NRC VAD [22] lexicon, a dataset of valence, dominance and\narousal human ratings. For music generation tasks, we use the AILabs Pop1k7 dataset [13] and the\nEMOPIA dataset [17] which contains MIDI piano pieces categorized into one of the 4 quadrants in\nthe valence-arousal space.\nWe used the MidiTok [11] library to tokenize the AILabs1k7 and EMOPIA datasets into REMI [16]\nrepresentations, and chunked them into single-bar representations.\n‡In monophonic music, only one note can be played at any time. In polyphonic music, multiple notes can be\nplayed together simultaneously (like chords)\n11\n"
}