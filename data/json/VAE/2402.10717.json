{
    "optim": "BioFusionNet: Deep Learning-Based Survival Risk\nStratification in ER+ Breast Cancer Through\nMultifeature and Multimodal Data Fusion\nRaktim Kumar Mondol1, Ewan K.A. Millar2, Arcot Sowmya1, and Erik Meijering1,*\n1School of Computer Science and Engineering, University of New South Wales, Sydney, Australia\n2Department of Anatomical Pathology, NSW Health Pathology, St. George Hospital\n*Correspondence: erik.meijering@unsw.edu.au\nAbstract\nBreast cancer is a significant health concern affecting millions of women worldwide. Accurate\nsurvival risk stratification plays a crucial role in guiding personalised treatment decisions and\nimproving patient outcomes. Here we present BioFusionNet, a deep learning framework that\nfuses image-derived features with genetic and clinical data to achieve a holistic patient profile\nand perform survival risk stratification of ER+ breast cancer patients. We employ multiple self-\nsupervised feature extractors, namely DINO and MoCoV3, pretrained on histopathology patches\nto capture detailed histopathological image features. We then utilise a variational autoencoder\n(VAE) to fuse these features, and harness the latent space of the VAE to feed into a self-attention\nnetwork, generating patient-level features. Next, we develop a co-dual-cross-attention mechanism\nto combine the histopathological features with genetic data, enabling the model to capture the\ninterplay between them. Additionally, clinical data is incorporated using a feed-forward network\n(FFN), further enhancing predictive performance and achieving comprehensive multimodal feature\nintegration. Furthermore, we introduce a weighted Cox loss function, specifically designed to\nhandle imbalanced survival data, which is a common challenge in the field. The proposed model\nachieves a mean concordance index (C-index) of 0.77 and a time-dependent area under the curve\n(AUC) of 0.84, outperforming state-of-the-art methods. It predicts risk (high versus low) with\nprognostic significance for overall survival (OS) in univariate analysis (HR=2.99, 95% CI: 1.88–4.78,\np<0.005), and maintains independent significance in multivariate analysis incorporating standard\nclinicopathological variables (HR=2.91, 95% CI: 1.80–4.68, p<0.005). The proposed multifeature\nand multimodal data fusion approach not only improves model performance but also addresses\na critical gap in handling imbalanced data. This advancement has the potential to substantially\ninfluence therapeutic decision-making, leading to improved clinical outcomes for patients.\nKeywords Multimodal Fusion · Breast Cancer · Whole Slide Images · Deep Neural Network · Survival Prediction\n1\nIntroduction\nBreast cancer poses a significant global health concern, with\na high incidence rate and substantial impact on morbidity and\nmortality [1, 2]. The incidence of breast cancer varies across\ndifferent regions and populations, with higher rates observed\nin developed countries [1]. The prevalence of breast cancer\nin Australia, affecting 1 in 8 women up to the age of 85, is a\ncause for concern due to its rising incidence rate over the past\ndecade [3]. This trend highlights the crucial need for accurately\npredicting survival risks to identify high-risk patients who may\nbenefit from more intensive treatment or monitoring, thereby\npotentially improving outcomes [2].\nIn breast cancer, the estrogen receptor (ER) status plays a critical\nrole in determining treatment strategies and predicting patient\nprognosis. ER+ breast cancer, which includes Luminal A and\nLuminal B subtypes, is distinguished by the presence of ERs on\ncancer cells, making it responsive to hormonal therapies such as\ntamoxifen [4]. While Luminal A tumours are usually low-grade\nwith a favorable prognosis (Ki-67<14%), Luminal B tumours\nare usually higher grade and pose a higher recurrence risk and\nworse outcome (Ki-67≥14%) [4, 5]. The inherent heterogeneity\nof breast cancer poses challenges for prediction of prognosis and\ntreatment decisions, particularly in post-menopausal ER+ breast\ncancer, with previous studies reporting conflicting results on the\nsurvival difference between Luminal A and B metastatic breast\ncancer patients [6, 7]. A common critical clinical dilemma is the\nselection of those early ER+ breast cancer patients at high risk of\nrecurrence who may benefit from the addition of chemotherapy\nto endocrine therapy. Therefore, accurate survival risk prediction\nmodels specifically tailored for ER+ breast cancer are essential\nfor personalised treatment decisions.\nTraditional methods for survival risk prediction often rely on\nclinicopathological risk factors (such as age, tumour size, grade,\nlymph node metastasis and clinical stage), which may fail to\nfully capture the complex biology of cancer [4, 7–10]. To ad-\ndress this issue, molecular markers and gene expression profiles\nhave been identified as potential prognostic factors that provide\nvaluable insights into tumour biology and potential therapeu-\ntic targets [11–15]. Over the past decade, the integration of\ngenomic testing into treatment decision-making processes has\nbeen enhanced by the utilisation of several commercial gene\npanels, such as Prosigna/PAM50, OncotypeDx and Endopredict\narXiv:2402.10717v1  [cs.CV]  16 Feb 2024\n2\nPatient Details\nMedical Tests\nTreatment\nDiagnosis\nAge Group: Post Menopausal\nGender: Female\nEthnicity: Caucasian\nClinical Exam\nImaging\nCore Biopsy\nBreast-Conserving Surgery\n(Lumpectomy)\nTumor Size: T2\nTumor Grade: G2 \nHormone Receptor Status: ER+\nHER2 Status: HER2-\nLymph Node Status: LN-\nCell Proliferation: Ki67 20%\nGenetic Testing\nPAM50/ROR, \nOncotypeDX etc. \nEndocrine Therapy &\nRadiotherapy\nHigh Risk\nLow Risk\nAdjuvant \nChemotherapy\nChemotherapy\nNot Required\nMultimodal Risk Prediction\nInvasive Nature: IDC\nEnsures Adequate \nTreatment\nAvoids Excessive \nTreatment & Toxicity\nFigure 1: Illustration of the clinical management pathway in treating breast cancer patients. This example concerns a postmenopausal\npatient who has been diagnosed with breast cancer, specifically invasive ductal carcinoma (IDC). The process begins with an initial\ndiagnosis through clinical examination, imaging and core biopsy. Following this, surgery is performed to completely excise the tumour,\nand postoperative tumour histopathological classification is performed to assess key factors including tumour size (e.g. T2), grade (e.g.\nG2), hormone receptor status (e.g. ER+), HER2 status (e.g. HER2-), lymph node status (e.g. LN-), and proliferation index (e.g. Ki67\n20%). Subsequent treatments may include hormone therapy and radiotherapy. Additional molecular tests, like genetic testing, are utilised\nto determine specific cancer molecular subtypes and further assess risk of recurrence. The proposed final step in this pathway is the\napplication of our BioFusionNet model. This model combines tumour characteristics, pathology and genetic testing to determine high and\nlow risk patients, thereby guiding personalised treatment decisions and efficiently preventing both under-treatment and over-treatment. For\nexample, low-risk patients might undergo lumpectomy with hormone therapy and radiotherapy, whereas high-risk patients are advised\nto have chemotherapy in addition to these treatments. Whilst this pathway mirrors current clinical practice, our study streamlines the\nintegration of all available critical data to derive an automated single risk prediction score.\namong others. In addition, histopathological imaging, which\noffers in-depth insights into the cellular and tissue characteristics\nof tumours, plays a vital role in both the diagnosis and prognosis\nof breast cancer [16]. However, to address the varied nature of\nthe disease effectively, it is essential to consider all available data\nmodalities. Therefore, integrating imaging, genetic and clini-\ncopathological information into a single risk prediction model\ncould potentially enhance risk prognostication in the clinic [9].\nIn this study, we propose a novel multimodal survival risk predic-\ntion model to enhance the prognosis of ER+ breast cancer. The\nmodel uniquely integrates histopathology images, genetic pro-\nfiles and clinical data, enabling the prediction of risk scores that\ncategorise patients into distinct risk groups aligned with their sur-\nvival outcomes. The model is thoroughly evaluated using metrics\nsuch as the concordance index (C-index) and time-dependent\narea under the curve (AUC) score, and its performance is com-\npared to existing methods to establish its efficacy. Additionally,\nwe analyse the interpretability of the model, providing insights\ninto how different genes and clinical factors influence the risk\nprediction, thereby enhancing our understanding of the underly-\ning predictive mechanisms.\n2\nBackground\nCancer risk prediction is of paramount importance due to its\npotential for guiding personalised screening, prevention and\ntreatment strategies, ultimately leading to improved patient out-\ncomes and reduced mortality (Fig. 1). This approach is espe-\ncially vital in the context of ER+ breast cancer, where accurate\nrisk prediction is essential for identifying individuals at higher\nrecurrence risk. These patients may benefit from more aggres-\nsive treatments like chemotherapy [17–20]. On the other hand,\nrisk models are equally critical in recognising lower-risk pa-\ntients, potentially sparing them from unnecessary treatments and\ntheir side effects [21]. Online algorithms such as Predict1 and\nAdjuvant2 are used clinically to estimate the risk of recurrence\nand the benefit of adding chemotherapy to endocrine therapy,\nwhich is a major treatment dilemma.\nDevelopments in deep learning, such as the Cox proportional\nhazards deep neural network, have revolutionised cancer re-\nsearch by improving survival data modeling and treatment rec-\nommendation systems [22]. Multimodal data fusion, which\ncombines information from diverse sources such as imaging, ge-\nnomics and clinical data, has gained attention in cancer research\ndue to its ability to provide a comprehensive understanding of\nthe disease and improve predictive outcomes [23–25]. Cross-\nattention transformer mechanisms that integrate histopatholog-\nical images and genomic data capture complementary infor-\nmation from different modalities, leading to improved survival\nprediction [26].\nIn this evolving landscape, models such as MultiDeepCox-\nSC [27], MCAT [28], MultiSurv[29], HFBSurv [30], Pathomic\nFusion [31], and TransSurv [32] exemplify significant progress\nin multimodal analysis. Pathomic Fusion, for instance, em-\nploys a gating-based attention mechanism that modulates the\nexpressiveness of features from each modality. This approach\nis particularly effective in mitigating the impact of noisy uni-\nmodal features, ensuring that the model integrates and empha-\nsises the most pertinent information from each data source [31].\nSimilarly, HFBSurv decomposes the fusion problem into differ-\nent levels and integrates and passes information progressively\n1https://breast.predict.nhs.uk/tool\n2https://oncoassist.com/adjuvant-tools/\n3\nfrom lower to higher levels. This hierarchical framework, with\nmodality-specific and cross-modality attentional factorised bi-\nlinear modules, captures and quantifies complex relations in\nmultimodal data while reducing computational complexity [30].\nMoreover, the significance of deep autoencoders in construct-\ning nonlinear mappings between 2D images and 3D poses has\nalso been emphasised, further highlighting the potential of these\nmethods in cancer research [33].\nDespite these advances, the integration of multimodal data,\nmarked by its inherent heterogeneity and dimensional variabil-\nity, remains a significant challenge, underscoring the need for\nadvanced methodologies to effectively integrate and leverage\ndiverse data sources for robust survival risk assessment [34–36].\nA previous study has addressed ER+ breast cancer risk stratifica-\ntion using pathology image analysis, focussing on Nottingham\ngrading components such as mitotic rates, nuclear pleomorphism\nand tubule formation [37]. However, multimodal approaches\nthat thoroughly integrate diverse data types to accurately predict\nsurvival risks for ER+ breast cancer are currently lacking. This\nresearch gap emphasises the need for novel multimodal fusion\ntechniques for risk prediction in ER+ breast cancer.\n3\nMaterials and Methods\n3.1\nData Collection\nOur study used hematoxylin-and-eosin-stained (H&E) formalin-\nfixed paraffin-embedded (FFPE) digital slides from The Can-\ncer Genome Atlas Breast Invasive Carcinoma (TCGA-BRCA).\nWhole-slide images (WSIs) from the TCGA-BRCA data collec-\ntion were downloaded from the GDC Portal (accessed 25 August\n2023). In this work, we chose a subset of 249 cases from the\nTCGA-BRCA dataset, specifically focussing on two molecular\nsubtypes: Luminal A comprising 149 samples, and Luminal B\nwith 100 samples. Additionally, we obtained transcriptome-wide\nRNA-sequencing data representing mRNA expression levels for\na total of 20,438 genes in the reference genome from the TCGA\ndataset. These data were processed using RNA-sequencing by\nexpectation maximisation (RSEM) and were downloaded from\nthe cBioPortal platform[38]. This dataset included a range of\nclinical information for each patient, such as tumour grade, tu-\nmour size, lymph node status, age at diagnosis and molecular\nsubtypes. Overall, patients who had WSIs, RNA-sequencing\nand clinical data available were included in the study.\n3.2\nData Preparation\n3.2.1\nSlide Annotation\nAn expert breast pathologist manually annotated the selected\nslides using QuPath [39]. The annotation was performed for\nlocalization of the tumour outline, excluding any necrosis but\nincluding stroma and tumour infiltrating lymphocytes (TILs).\nThe pathologist was blinded to any molecular or clinical features\nduring annotation.\n3.2.2\nImage Data Preparation\nThe images were first downsampled to 0.25 µm/pixel, corre-\nsponding to approximately 40× magnification. The annotated\ntumour regions were processed semi-automatically with QuPath\nto create 224×224-pixel patches, resulting in approximately\n500 nonoverlapping patches per sample. To address staining\ninconsistencies, vector-based color normalization was applied\n[40].\n3.2.3\nRNA-Sequencing Data Preparation\nFrom the extensive set of 20,438 genes, we selected genes\nfeatured in various commercial assays, namely Oncotype DX,\nMammaprint, Prosigna (PAM50), EndoPredict, BCI (Breast\nCancer Index), and Mammostrat [41–46], as these are the most\nrelevant genetic markers to our study’s objectives. This resulted\nin a subset of 138 genes.\n3.2.4\nClinical Data Preparation\nFrom the clinical data, we selected variables based on their es-\ntablished relevance in breast cancer prognosis and treatment\noutcomes [47–50]. Specifically, we included tumour grade (cat-\negorised as grade 1&2 versus grade 3), tumour size (>20 mm\nversus ≤20 mm), patient age (>55 versus ≤55) and lymph node\nstatus (positive versus negative).\n3.3\nProposed Model\nThe proposed deep learning model, which we call BioFusion-\nNet, is an innovative feature extraction and multimodal fusion\nframework designed to leverage and integrate diverse data types,\nincluding histology images, genomic features and clinical data\nfor enhanced cancer outcome prediction (Figs. 2 and 3). The\nessence of BioFusionNet lies in its capability to fuse these data\nmodalities into a cohesive tensor representation, effectively cap-\nturing both bimodal and trimodal interactions. This approach is\naimed at surpassing the performance of traditional unimodal and\nexisting multimodal representations in survival risk prediction.\n3.3.1\nFeature Extraction Using DINO and MoCoV3\nHistopathological images, rich in phenotypic information, are\npivotal for understanding cancer pathology.\nBioFusionNet\nutilises two advanced self-supervised learning models, DINO\n(self-DIstillation with NO labels) and MoCoV3 (Momentum\nContrast version 3), both based on the Vision Transformer (ViT)\narchitecture, to extract morphological features from histology\nimages crucial for identifying cancer-related patterns.\nDINO: The DINO framework employs a dual-network architec-\nture, consisting of a student and a teacher network, both being\nViTs. The student network learns by attempting to replicate the\noutput of the teacher network, which in turn is an exponential\nmoving average of the student’s parameters. The core process\ninvolves generating multiple augmented views (I1, I2, . . . , Ik) of\na given input image (I), which are then processed by these net-\nworks. The resultant feature vectors from the student (Fs) and\nteacher (Ft) networks are utilised to compute the distillation loss\nas follows:\nLD =\nk\nX\ni=1\nCrossEntropy\n \nFs(Ii), Softmax\n Ft(Ii)\nτ\n!!\n,\n(1)\nwhere τ represents the temperature scaling parameter. Notably,\nDINO is pretrained on a broad range of datasets, including\nBACH, CRC, MHIST, PatchCamelyon and CoNSeP, compris-\ning 33 million patches (DINO33M), and 2 million patches\nfrom TCGA-BRCA (DINO2M) [51, 52]. The output func-\ntions for DINO33M and DINO2M, denoted as fDINO33M(x)\nand fDINO2M(x) respectively, convert an input image x of size\n224 × 224 × 3 into a 1 × 384 feature vector. We utilised these\ntwo models for feature extraction: DINO33M trained on diverse\ndatasets providing a broad perspective and enabling the model to\nrecognise a wide array of general histopathological features, and\n4\n [Each Patient ×  Image Embedding (256)]\n. . .\n. . .\n. . .\nOutput Image Features\n(Patient Level)\n[No. of Patches (N) × Image Embedding (256)]\n500 × 3 × 224 ×224\nPreprocessed\nHistopathology Images\nSkip Connection \nImage Patches\n...\nKey \nQuery\nValue\nSoftmax\nAggregated Output\nSelf-Attention Module\nMoCoV3\nPretrained on\n15M Histo Patches\n(Multiple Datasets)\nDINO\nPretrained on\n2M Histo Patches\n(TCGA-BRCA)\nDINO\nPretrained on\n33M Histo Patches\n(Multiple Datasets)\n+\nEncoder\nDecoder\nσ\nμ \nLatent\nSpace (z)\nTotal Loss = Reconstruction Loss (MSE) + KL Divergence\n500 × 384\n500 × 384\n500 × 1152\n500 × 1152\n500 × 384\nVariational Autoencoder (VAE)\nSSL Base Model\nViT Small\n(Patch16) \n+ = Concatenation\n= Mean\n= Standard Deviation\n500 × 256\nFeed-Forward Network\nEncoder\nDecoder\n= Matrix Multiplication\nσ\nμ\nLegend:\nFigure 2: BioFusionNet Stage 1: The proposed model integrates self-supervised image feature extraction methods, namely DINO and\nMoCoV3, pretrained on three distinct datasets. Features are concatenated and fed to a Variational AutoEncoder (VAE). Subsequently, the\nlatent space of the VAE is utilised to feed a self-attention network, which aggregates patch-level features into a comprehensive patient-level\nrepresentation.\nDINO2M specifically trained on breast cancer data for more spe-\ncialised and precise feature extraction relevant to breast cancer\npathology.\nMoCoV3: The MoCoV3 framework, which incorporates ViTs,\nrepresents a significant advancement in self-supervised learning\nthrough its adoption of the momentum contrast (MoCo) ap-\nproach [53]. At the heart of this framework lies the contrastive\nlearning mechanism, designed to differentiate between positive\nand negative pairs, thereby enhancing the model’s feature learn-\ning capabilities. MoCoV3’s architecture is defined by two main\ncomponents: a query encoder that processes the current batch\nof images, and a key encoder updated via a momentum-based\nmoving average of the query encoder’s parameters:\nθk ← mθk + (1 − m)θq,\n(2)\nwhere θk and θq are the parameters of the key and query encoders\nrespectively and m is the momentum coefficient. This enables\nthe key encoder to maintain a queue of encoded keys represent-\ning previously seen images, thus enhancing the model’s ability\nto maximise agreement between differently augmented views\nof the same image (positive pairs) and minimise similarity with\nother images (negative pairs). The framework uses the InfoNCE\nloss [54]:\nLInfoNCE = − log\nexp(q · k+/τ)\nPK\ni=0 exp(q · ki/τ)\n,\n(3)\nwhere q and k+ are the query and positive key feature vectors,\nki are the negative key vectors, K is the number of negative\nkeys and τ is the temperature parameter. MoCoV3 has been\npretrained on an extensive collection of 15 million histology\npatches from over 30 thousand WSIs derived from the TCGA\nand Pathology AI Platform (PAIP) datasets, encompassing a\nwide variety of cancer types and histological features. This\nendows the model with a robust and versatile capability to extract\nmeaningful features from histopathological data [55]. Similar to\nDINO, the output function of MoCoV3, fMoCoV3(x), transforms\nan input image x of dimensions 224 × 224 × 3 into a 1 × 384\nfeature vector.\n3.3.2\nUnimodal Feature Integration\nExtracted features from DINO33M, DINO2M and MoCoV3\nare concatenated to form a comprehensive 1 × 1152 feature\nvector fcat(x) = fDINO33M(x) ⊕ fDINO2m(x) ⊕ fMoCoV3(x) (Fig. 2).\nFollowing this, we employ a VAE to encode the integrated\nimage features into a 256-dimensional feature in latent space.\nThe latent feature vector is then passed through a self-attention\nmodel and aggregated using sum pooling to generate patient-\nlevel features.\n3.3.3\nFeature Fusion Using Variational Autoencoding\nOur VAE consists of an encoder and a decoder. The encoder\nfunction fenc maps the concatenated feature vector fcat(x) to the\nlatent space by generating the mean µ and standard deviation σ\nof the latent representation:\n(µ, σ) = fenc(fcat(x)).\n(4)\nWith 500 patches per patient, the latent space is structured as a\nmatrix of size 500 × 256. This is achieved by sampling z using\n5\n [ Each Patient ×  Image Embedding (256)]\n [ Each Patient ×  No. of Genes (138)]\nFully-connected\n128 + 4\nRisk Score\n256\n32\n[ Each Patient × Clinical Features (4)]\nFully-connected\nFully-connected\n512\n...\n≈ ≈\n...\n≈ ≈\n...\n≈ ≈\n...\n≈ ≈\nTransformer \nEncoder\nCo Dual Cross Attention\nCo Attention \nDual Cross Attention\nMulti-Head \nSelf-Attention\nFeed-Forward \nNetwork (FFN)\nTransformer Encoder\nKey \nQuery\nValue\nSoftmax\nCross Attention\n+\nLayer Normalization\nDropout\nMatrix Multiplication\nConcatenation\n+\n+\n+\n+\nx1 \nx2 \nxout \nfin \nfout\nI\nG\nAIG\nAGI\nCIG\nCGI\nDIG\nDGI\nTcat\nFigure 3: BioFusionNet Stage 2: The proposed model fuses image embeddings generated from Stage 1 with genetic data through a\nco-dual-cross-attention mechanism. This fusion is subsequently combined with clinical data using a feed-forward network (FFN), leading to\nthe generation of the final risk score output.\nthe reparameterization trick:\nz = µ + σ · ϵ,\nϵ ∼ N(0, I),\nz ∈ R500×256.\n(5)\nThe decoder fdec then attempts to reconstruct the input from the\nlatent variable z:\nˆx = fdec(z).\n(6)\nThe VAE is optimised using a loss function that combines mean\nsquared error (MSE) for reconstruction accuracy and Kullback-\nLeibler (KL) divergence for distribution regularisation:\nLVAE = MSE(ˆx, x) + β · KL(N(µ, σ2)∥N(0, I)),\n(7)\nwhere β balances the reconstruction and regularisation terms.\nThe MSE term ensures that the reconstructed image closely\nresembles the original input, while the KL divergence term\nencourages the latent distribution to approximate a standard\nnormal distribution. This enables BioFusionNet to effectively\nblend the features from the different self-supervised models,\nenhancing the overall feature representation.\n3.3.4\nPatch-to-Patient Aggregation\nTo aggregate the patch-level features from the latent space of the\nVAE into a comprehensive patient-level representation capturing\nthe interdependencies among image patches, our model uses\nself-attention (Fig. 2). The self-attention module computes a\nweighted sum of the key (K), query (Q), and value (V) vectors:\nyi =\n500\nX\nj=1\nSoftmax\n\u0010\ns(Qi, K j)\n\u0011\nVj,\n(8)\nwhere s(Qi, Kj) is the attention function that determines the rel-\nevance between each query and key pair. This offers two signifi-\ncant benefits. First, by leveraging the VAE’s latent vectors, the\nmodel focusses on the most pertinent image features, resulting\nin a more precise feature representation. Second, by consid-\nering all latent representations, the self-attention mechanism\ncontextualises each patch within the broader histopathology of\nthe patient.\n3.3.5\nMultimodal Fusion Using Co Dual Cross Attention\nTo integrate patient-level image embeddings with genetic fea-\nture data, our model uses a co-dual-cross-attention mechanism\n(Fig. 3). This is achieved using a complex architecture compris-\ning co-attention and dual-cross-attention modules.\nThe co-attention module applies linear transformations to the\nimage embeddings I and genetic features G, yielding their re-\nspective query (Q), key (K), and value (V) vectors, and computes\nthe co-attention scores between them as:\nAIG = Softmax\n\u0010\nQIKT\nG\n\u0011\nVG,\n(9)\nAGI = Softmax\n\u0010\nQGKT\nI\n\u0011\nVI,\n(10)\n6\nwhere AIG represents the attention from images to genetic fea-\ntures, and AGI the attention from genetic features to images. The\nSoftmax function normalises these scores, facilitating an effec-\ntive weighting of feature importance in the fusion process. This\nbidirectional attention prepares the ground for more complex in-\nteractions in the subsequent stage of the co-dual-cross-attention\nmechanism.\nSubsequently, the dual-cross-attention module further refines\nthe integration of the image and genetic features in two distinct\nstages. The first stage concerns the interaction between the co-\nattended image features (AIG) and co-attended genetic features\n(AGI). The cross attention is computed as:\nCIG = Softmax\n\u0010\nAIG(AGI)T\u0011\nAIG,\n(11)\nCGI = Softmax\n\u0010\nAGI(AIG)T\u0011\nAGI,\n(12)\nwhere CIG represents the cross-attention output when image fea-\ntures attend to genetic features, and CGI the reverse. This stage\nis crucial for enhancing each modality by integrating contextu-\nally relevant information from the other. In the second stage,\nthe outputs from the first stage are further refined by reapplying\nthem to their respective original features. This enhances the\ndepth of the multimodal integration:\nDIG = Softmax\n\u0010\nCIGIT\u0011\nI,\n(13)\nDGI = Softmax\n\u0010\nCGIGT\u0011\nG,\n(14)\nwhere DIG and DGI denote the refined cross-attention outputs,\nfurther enhancing the original image and genetic data with addi-\ntional contextual insights.\nBy sequentially processing the attended features, the model\nachieves a richer and more contextually informed representation\nof the fused image and genetic data, suited for complex tasks\nlike risk prediction. The concatenated output Tcat = DIG ⊕ DGI\nis fed to a Transformer Encoder, which employs multiple layers\nof self-attention and a feed-forward network (FFN) to achieve\na deeper assimilation and transformation of the fused features.\nThe output of the Transformer Encoder is then processed by\nfour fully-connected layers, the third of which also integrates\nthe clinical information (Fig. 3). The integration of clinical\nvariables at this stage is crucial due to their dimensionality and\ncharacteristics. Clinical data, comprising only four features,\nmay be overshadowed by the higher-dimensional features from\nhistopathological images and genetic profiles if introduced ear-\nlier in the model. By embedding these variables in a later layer,\ntheir impact is more directly and effectively mapped onto the\nmodel’s output, preserving their significant influence on survival\nrisk prediction.\nThis multimodal integration results in a holistic representation\nof both the phenotypic and genotypic information of ER+ breast\ncancer. Finally, the network employs a linear output layer that\npredicts the survival risk score.\n3.4\nProposed Loss Function\nFor training BioFusionNet, we propose a novel loss function\ntermed the weighted Cox loss (computed by Algorithm 1), which\nis tailored to address the challenges of imbalanced survival data\n(a common issue in survival analysis):\nLWCox = −\n1\nPN\ni=1 wiei\nN\nX\ni=1\nwiei(ri − log(Hwi)),\n(15)\nAlgorithm 1 Weighted Cox Loss\nRequire: r: risks (log hazard ratios), e: events, w: weights\nEnsure: LWCox: negative log likelihood loss\n1: Compute Ew = PN\ni=1 wiei as total weighted events\n2: Sort samples by descending r and align e, w\n3: for i ∈ 1, . . . , N do\n▷ N is the number of samples\n4:\nCompute hazard ratio hi = exp(ri)\n5: end for\n6: Init weighted cumulative hazard Hw0 = 0\n7: for i ∈ 1, . . . , N do\n8:\nUpdate cumulative sum Hwi = Hwi−1 + wihi\n9: end for\n10: for i ∈ 1, . . . , N do\n11:\nCompute uncensored log likelihood\nui = wi(ri − log(Hwi))\n12: end for\n13: Compute c = u ⊙ e\n▷ ⊙ is the element-wise product\n14: Compute loss LWCox = − 1\nEw\nPN\ni=1 ci\n15: return LWCox\nwhere ei denotes the event occurrence, wi the assigned weight, ri\nthe log hazard ratio, Hwi the weighted cumulative hazard and N\nthe number of samples. Unlike the traditional Cox proportional\nhazards loss (LCox) [22], the proposed loss uses weighting to\nmitigate the effects of uneven distribution of events within the\ndataset. In this work we used wi = 3, considering that censored\ndata (denoted as ‘0’) is almost three times as prevalent as event\ndata (denoted as ‘1’), and thus the sensitivity of the loss function\nto the latter should be enhanced accordingly, mitigating the bias\ntowards censored data.\n3.5\nModel Training\nThe training of BioFusionNet is divided into two distinct stages\nas follows (Figs. 2 and 3):\nStage 1: Feature Extraction: The self-supervised pretrained\nmodels DINO33M, MoCoV3 and DINO2M extract features\nfrom histopathology image patches, which are concatenated\nand then fed into a VAE to produce embeddings, which in turn\nare processed by a self-attention module to produce a patient-\nlevel feature vector (Fig. 2). The VAE was optimised using\nAdamW with a learning rate of 0.0001 and a batch size of 12.\nThe employed loss function is a combination of MSE and KL\ndivergence, targetting the construction of an advanced latent\nspace to generate detailed patient-level features.\nStage 2: Risk Prediction: The proposed co-dual-cross-attention\nmechanism, followed by multiple FFNs and a final output node\nthat uses a linear activation function, predicts the patient-level\nrisk from the image-based, genetic and clinical information\n(Fig. 3). Here, training was performed using the proposed\nweighted Cox loss (LWCox), which was optimised using the\nAdam algorithm with a learning rate of 0.001 and a batch size of\n12. To mitigate overfitting, an early stopping mechanism based\non the validation loss was implemented. This involved halting\nthe training process after a patience period of 10 epochs if no\nimprovement was observed.\nBoth training stages used a dataset comprising 199 training sam-\nples and 50 validation samples within a five-fold cross-validation\nframework. The trained model predicts a continuous risk score\nfor every patient within each validation fold. For survival analy-\n7\nTable 1: Performance comparison of multimodal and unimodal models for cancer risk prediction using C-index.\nFold\nImaging+Genetic+Clinical\nImaging+Genetic\nImaging\nClinical\nGenetic+Clinical\nBioFusionNet\nBioFusionNet\nBioFusionNet\nCoxPH\nRSF\nCoxPH\nRSF\n1\n0.78\n0.73\n0.58\n0.60\n0.63\n0.60\n0.74\n2\n0.71\n0.72\n0.69\n0.69\n0.37\n0.58\n0.34\n3\n0.72\n0.69\n0.61\n0.65\n0.68\n0.52\n0.32\n4\n0.81\n0.75\n0.70\n0.72\n0.67\n0.62\n0.55\n5\n0.82\n0.65\n0.69\n0.70\n0.65\n0.64\n0.64\nMean ± Std\n0.77 ± 0.05\n0.71 ± 0.04\n0.67 ± 0.04\n0.58 ± 0.11\n0.60 ± 0.13\n0.59 ± 0.04\n0.52 ± 0.18\nsis, we employed the median risk score θopt, derived from each\ntraining set, as a threshold to classify patients in the validation\nset into two categories: high risk (risk score > θopt) and low risk\n(risk score < θopt).\n3.6\nEvaluation Metrics\nTo quantitatively evaluate survival risk score prediction, we\nemployed the concordance index (C-index) and the area under\nthe curve (AUC) as our primary metrics. The C-index assesses\nthe concordance between predicted survival times and observed\noutcomes, especially in the presence of censored data:\nC-index =\nnP\ni=1\nnP\nj=1\nI(yi < y j, δi = 1)I( ˆf(xi) < ˆf(x j))\nnP\ni=1\nnP\nj=1\nI(yi < yj, δi = 1)\n.\n(16)\nwhere n is the number of patients, yi and y j denote the observed\nsurvival times, δi indicates whether the event was observed (not\ncensored), ˆf(xi) represents the predicted risk for the ith patient,\nand I is the indicator function that returns 1 when its condition is\ntrue and 0 otherwise. The time-dependent AUC offers a dynamic\nview of the model accuracy over time t and incorporates weights\nωi, and is calculated using the following formula:\nAUC(t) =\nnP\ni=1\nnP\nj=1\nωiI(yi ≤ t)I(y j > t)I( ˆf(xj) ≤ ˆf(xi))\nnP\ni=1\nωiI(yi ≤ t)\nnP\ni=1\nI(yi > t)\n.\n(17)\nBoth the C-index and AUC values range from 0 to 1, with higher\nvalues indicating better performance.\n4\nExperimental Results\n4.1\nComparison Across Modalities\nThe effectiveness of BioFusionNet in cancer risk prediction\nwas evaluated by comparing its C-index performance across\ndifferent modality configurations. The results (Table 1) show\nthat the mean performance in the cross-validation experiments\nconsistently increased from using only imaging data, to using\nimaging and genetic data, to combining imaging, genetic and\nclinical data. Two traditional methods, namely Cox Proportional\nHazards (CoxPH) and Random Survival Forests (RSF), were\nalso evaluated, showing no consistent advantage in using genetic\nand clinical data as opposed to only clinical data, and performing\nworse than BioFusionNet.\n4.2\nComparison With State-of-the-Art Fusion Methods\nThe performance of BioFusionNet was compared with several\nstate-of-the-art multimodal fusion methods for cancer risk pre-\ndiction in terms of both C-index and AUC (Table 2). For this ex-\nperiment we included methods using concatenation (MultiSurv),\nhierarchical attention (HFBSurv), gating attention (Pathomic-\nFusion), co-attention (MCAT) and cross attention (TransSurv).\nThe AUC was calculated using average values over 5-year and\n10-year periods. The proposed model consistently outperformed\nall these previous methods, showing substantial improvements\nin both metrics.\n4.3\nEvaluation of Loss Functions\nThe performance of two different loss functions was compared\nusing the C-index across five cross-validation folds for Bio-\nFusionNet and MoCoV3 (Table 3). The results show that the\nmean performance improved for both methods when using the\nTable 2: Performance comparison of multimodal fusion methods for\ncancer risk prediction.\nMethod\nFold\nC-index\nAUC\nValue\nMean ± Std\nValue\nMean ± Std\nMultiSurv [29, 56, 57]\n1\n0.71\n0.63 ± 0.07\n0.74\n0.63 ± 0.09\n2\n0.59\n0.52\n3\n0.60\n0.61\n4\n0.69\n0.70\n5\n0.54\n0.57\nHFBSurv [30]\n1\n0.58\n0.54 ± 0.07\n0.51\n0.49 ± 0.04\n2\n0.47\n0.51\n3\n0.56\n0.45\n4\n0.45\n0.44\n5\n0.62\n0.53\nPathomicFusion [31]\n1\n0.63\n0.52 ± 0.08\n0.68\n0.47 ± 0.23\n2\n0.43\n0.10\n3\n0.56\n0.54\n4\n0.50\n0.63\n5\n0.46\n0.38\nMCAT [28]\n1\n0.71\n0.70 ± 0.04\n0.70\n0.71 ± 0.04\n2\n0.69\n0.67\n3\n0.64\n0.65\n4\n0.70\n0.69\n5\n0.76\n0.72\nTransSurv [32][26]\n1\n0.70\n0.69 ± 0.04\n0.68\n0.66 ± 0.04\n2\n0.61\n0.60\n3\n0.69\n0.65\n4\n0.69\n0.67\n5\n0.74\n0.72\nBioFusionNet (Proposed)\n1\n0.78\n0.77 ± 0.05\n0.82\n0.84 ± 0.05\n2\n0.71\n0.93\n3\n0.72\n0.79\n4\n0.81\n0.81\n5\n0.82\n0.83\n8\n(a) CoxPH (Clinical)\n(b) MCAT (Multimodal)\n(c) BioFusionNet (Multimodal)\n(d) HFBSurv (Multimodal)\n(e) MultiSurv (Multimodal)\n(f) PathomicFusion (Multimodal)\nFigure 4: Performance comparison of BioFusionNet and other methods using Kaplan-Meier survival curves.\nTable 3: Performance comparison of loss functions for cancer risk\nprediction using two different methods.\nLoss\nMethod\nC-index (Fold)\nMean ± Std\n1\n2\n3\n4\n5\nLCox\nBioFusionNet\n0.69\n0.54\n0.59\n0.75\n0.80\n0.67 ± 0.10\nMoCoV3\n0.66\n0.57\n0.57\n0.78\n0.80\n0.67 ± 0.11\nLWCox (Proposed)\nBioFusionNet\n0.78\n0.71\n0.72\n0.81\n0.82\n0.77 ± 0.05\nMoCoV3\n0.70\n0.66\n0.66\n0.77\n0.72\n0.70 ± 0.04\nweighted Cox loss (LWCox) proposed in this paper, compared to\nthe traditional Cox loss (LCox).\n4.4\nUnivariate and Multivariate Hazard Analysis\nA comprehensive hazard analysis was conducted to evaluate the\noverall survival (OS) in the TCGA dataset of ER+ patients. Both\nunivariate and multivariate analyses were performed (Table 4).\nThe analysis encompassed various parameters, including tumour\ngrade, tumour size, age, lymph node (LN) status, subtype and\nthe risk predictions made by BioFusionNet. In the multivariate\nanalysis, positive LN status was associated with a hazard ratio\n(HR) of 1.87 (95% CI: 1.32–2.64), demonstrating a significant\neffect on survival (p < 0.005). Additionally, patients over the\nage of 55 had a HR of 1.77 (95% CI: 1.07–2.91), also showing\na significant impact on survival (p = 0.03). However, no signif-\nicant associations were found between tumour grade, size, or\nsubtype and survival outcomes in this analysis. Notably, the\nBioFusionNet-predicted risk group (high vs. low) demonstrated\na significant correlation with OS, with a HR of 2.91 (95% CI:\n1.80–4.68) (p < 0.005). Univariate analysis indicated that tumour\ngrade, size, age, and subtype were not statistically significant,\nwhereas LN status (HR of 1.84, 95% CI: 1.33–2.55, p < 0.005)\nand BioFusionNet risk group (HR of 2.99, 95% CI: 1.88–4.78,\np < 0.005) were significant predictors of survival. We note that\nthe LN status had 51 missing values, which were imputed using\na fixed value of 2. Kaplan-Meier survival analysis further sup-\nported the results, showing a significant difference in survival\nprobabilities between the high- and low-risk groups as predicted\nby BioFusionNet (log-rank test p = 6.45e-7) (Fig. 4c).\n4.5\nAblation Study\nWe also evaluated the performance of various versions of Bio-\nFusionNet for ER+ breast cancer risk stratification. The re-\nsults (Table 5) show that the base model, BioFusionNet-B0\n(MoCoV3, ViT Small) with LWCox, achieved the lowest C-\nindex, and incorporating single cross-attention (SCA), dual\ncross-attention (DCA), or co-attention (CoA) yielded slight im-\nprovements, as did BioFusionNet-B1 (DINO33M, ViT Small)\nand BioFusionNet-B2 (DINO2M, ViT Small), both with DCA\nand LWCox. Combining BioFusionNet-B0, B1, and B2 with just\nLWCox also resulted in slightly better performance than the base\nmodel, as did the inclusion of SCA and VAE. More substantial\nimprovements of the combined model were obtained with the\ninclusion of DCA or CoA instead of SCA. The best performance\nwas achieved by the combined model using VAE, CoA and DCA\nwith LWCox, which clearly outperformed the same model using\nthe traditional Cox loss LCox.\n4.6\nInterpretability of BioFusionNet\nBioFusionNet utilises a self-attention mechanism to analyse\nhistopathological image patches, identifying regions of high and\nlow attention within both high-risk and low-risk patient pro-\nfiles. Visual inspection of the results (Fig. 5) reveals that regions\nwith high attention contain distinct cellular patterns crucial for\nsynthesising features from patch-level to patient-level, whereas\n9\nTable 4: Univariate and multivariate analysis for overall survival (OS) in the TCGA dataset of ER+ patients.\nParameter\nRisk Group Cutoff\n#Patients/Group\nMultivariate (n=249)\nUnivariate (n=249)\nHR\n95% CI\np\nHR\n95% CI\np\nTumour Grade\n3 vs. 1 & 2\n64 vs. 185\n0.83\n0.46–1.49\n0.54\n1.13\n0.67–1.91\n0.65\nTumour Size\n>20 vs. ≤20 (mm)\n167 vs. 82\n1.45\n0.88–2.37\n0.14\n1.37\n0.86–2.19\n0.19\nAge\n>55 vs. ≤55\n159 vs. 90\n1.77\n1.07–2.91\n0.03\n1.47\n0.91–2.36\n0.11\nLN Status*\npos. vs. neg.\n110 vs. 88\n1.87\n1.32–2.64\n<0.005\n1.84\n1.33–2.55\n<0.005\nSubtype\nlum B vs. A\n100 vs. 149\n1.43\n0.88–2.34\n0.15\n1.38\n0.88–2.18\n0.16\nBioFusionNet\nhigh vs. low\n132 vs. 117\n2.91\n1.80–4.68\n<0.005\n2.99\n1.88–4.78\n<0.005\n*LN Status had 51 missing values which were imputed with a fixed value for both multivariate and univariate analysis.\nTable 5: Ablation study of BioFusionNet.\nModel\nC-index\nBioFusionNet-B0 (MoCoV3, ViT Small) + LWCox\n0.65 ± 0.05\nBioFusionNet-B0 (MoCoV3, ViT Small) + SCA + LWCox\n0.69 ± 0.04\nBioFusionNet-B0 (MoCoV3, ViT Small) + DCA + LWCox\n0.70 ± 0.03\nBioFusionNet-B0 (MoCoV3, ViT Small) + CoA + LWCox\n0.70 ± 0.04\nBioFusionNet-B1 (DINO33M, ViT Small) + DCA + LWCox\n0.68 ± 0.02\nBioFusionNet-B2 (DINO2M, ViT Small) + DCA + LWCox\n0.67 ± 0.03\nBioFusionNet-Concat(B0+B1+B2) + LWCox\n0.67 ± 0.04\nBioFusionNet-Concat(B0+B1+B2) + SCA + LWCox\n0.69 ± 0.03\nBioFusionNet-Concat(B0+B1+B2) + VAE + SCA + LWCox\n0.68 ± 0.04\nBioFusionNet-Concat(B0+B1+B2) + VAE + DCA + LWCox\n0.75 ± 0.04\nBioFusionNet-Concat(B0+B1+B2) + VAE + CoA + LWCox\n0.70 ± 0.03\nBioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + LCox\n0.67 ± 0.10\nBioFusionNet-Concat(B0+B1+B2) + VAE + CoA + DCA + LWCox\n0.77 ± 0.03\nareas of low attention typically exhibit less cellular atypia. This\nshows the model capacity to pinpoint clinically relevant features\nwithin tissue morphology. Additionally, SHAP analysis (Fig. 6)\nreveals the influence of individual genes on the model predic-\ntions, ranked from high to low, providing interpretability of the\nrisk assessment process. From this analysis, gene SLC39A6 (an\nestrogen regulated Zinc transporter protein with a role in epithe-\nlial to mesenchymal transition (EMT)) was identified as the most\nimportant predictor, with high expression levels producing high\nSHAP values, indicating positive impact on the model’s cancer\nrisk prediction. Other influential genes include ERBB2 (the\ngene for HER2), ESR1 (the gene for ER), with low expression\nlevels producing high SHAP values therefore positive impact\non the model. Moreover, the distribution of SHAP values for\nclinical features (Fig. 7) indicates that higher values of clinical\nparameters—such as positive LN status, higher tumour grade,\nincreased tumour size, and postmenopausal age group—tend to\nhave a positive impact on the model’s output. In this context, a\n‘positive impact’ implies that the model associates these values\nwith a higher likelihood of predicting patients at high risk.\n5\nDiscussion and Conclusion\nAs demonstrated by the experimental results, the proposed Bio-\nFusionNet is highly effective for cancer risk prediction, show-\ning superior performance compared to alternative approaches.\nClearly, the multimodal fusion of imaging, genetic and clinical\ndata allows the model to achieve substantially higher C-index\nscores compared to unimodal and dual-modal configurations, as\nwell as compared to the traditional Cox Proportional Hazards\nand Random Survival Forests methods. Furthermore, BioFu-\nsionNet outperforms existing multimodal fusion methods such\nas MultiSurv, HFBSurv, PathomicFusion, MCAT and TransSurv,\nand achieves the highest mean C-index (0.77 ± 0.05) and AUC\n(0.84 ± 0.05). Partly, the superior performance of BioFusionNet\nis due to the introduction of the proposed weighted Cox loss\nfunction instead of using the traditional Cox loss. Univariate\nand multivariate analyses showed the significant impact of age\nand BioFusionNet predictions on survival outcomes, while other\nclinical parameters such as tumour grade, size, lymph node sta-\ntus and subtype did not exhibit a significant correlation with\nsurvival outcomes. Kaplan-Meier analysis revealed a distinct\nseparation in survival probabilities between the high-risk and\nlow-risk groups identified by BioFusionNet. In addition, the\nresults of the ablation experiment confirmed the importance of\nattention mechanisms in improving prediction accuracy, with\nthe combined model configuration utilising VAE, CoA and DCA\nand the weighted Cox loss showing the highest performance.\nBioFusionNet also presents a significant advancement in the\ninterpretation of histopathological images, leveraging a self-\nattention mechanism to distinguish critical regions in patient\nprofiles. A key contribution is the model’s ability to align high-\nattention areas with distinct cellular patterns, crucial for tran-\nsitioning from patch-level to patient-level analysis, thereby en-\nhancing the diagnostic process. SHAP analysis amplifies this\nby clarifying the influence of specific genes and clinical fea-\ntures on the model’s predictions. We observed that elevated\nSLC39A6 gene expression correlates with a high-risk prediction\nin ER+ breast cancers, where previous studies have shown con-\nflicting findings, associating high SLC39A6 levels with good\nprognosis [58, 59], while others associated it with increased pro-\nliferation and lymph node involvement [60–62]. Similarly, our\nmodel identified high ESR1 expression as indicative of low risk,\naligning with literature that associates ESR1 positivity with en-\nhanced responsiveness to endocrine therapy and, consequently,\na better prognosis in ER+ breast cancer patients [63]. In con-\ntrast, our analysis revealed an unexpected association between\nERBB2 overexpression and a favorable prognosis in ER+ breast\ntumours, contrasting with the established view that ERBB2 over-\nexpression indicates a poor prognosis [64, 65]. As our study\nwas specifically tailored to analyze ER+ samples, excluding the\nHER2-enriched subtype (known for its high ERBB2 expression\nand aggressiveness) likely influenced the findings. Moreover,\nthe SHAP analysis for clinical factors (LN positivity, higher\ntumour grade and size, and postmenopausal age) significantly\ninfluences our model’s ability to identify patients at increased\nrisk, highlighting the critical role of these factors in breast cancer\nprognosis. Our analysis provides a transparent understanding of\n10\nLow Risk\nHigh Risk\nLuminal A\nLuminal B\nRisk Type\nHigh Attention\nLow Attention\nHigh Attention\nLow Attention\nTCGA-A7-A13H\nTCGA-AR-A2LK\nTCGA-B6-A0IN\nTCGA-A2-A259\nFigure 5: Visualisation of model-derived attention regions and associated risk types in Luminal A and Luminal B breast cancer patients.\nThe figure presents raw histopathological image patches processed with BioFusionNet, which identifies areas of high and low attention,\nsubsequently categorising patients into high and low risk.\nFigure 6: SHAP analysis of genetic features. The x-axis represents\nthe SHAP value; colour intensity indicates gene expression level.\nThe plot is sorted vertically by the features’ overall importance.\nFigure 7: SHAP value distribution of clinical features. In this distribu-\ntion, higher clinical values showing positive impact on the model, as\nindicated by its SHAP values.\nhow each gene and clinical feature contributes to the model’s\npredictions, providing actionable insights for clinical decision-\nmaking. While the risk assessment process mirrors current\nclinical practice, BioFusionNet streamlines the integration of all\navailable data (patient features, tumour features and molecular\nfeatures) to derive an automated single risk prediction score as a\npotential clinical oncology tool of the future.\nWhile insightful, this study has certain limitations. We primarily\nopted for OS as the key outcome measure, instead of disease-free\nsurvival (DFS). This choice was made because DFS presented\nchallenges such as a lower rate of events and a higher degree\nof data censorship, which could have limited the depth of the\nanalysis. While OS is a feasible choice, it potentially overlooks\ncritical insights into early-stage disease progression, typically\nhighlighted by DFS. Moreover, the study’s reliance on specific\ndatasets such as TCGA for ER+ patients may affect the broad\napplicability of our findings. Another shortcoming of this study\nis the inherent limitations of the clinical data, which, during\nunivariate analysis, identified tumour size, grade, and age as in-\nsignificant while only LN Status emerged as significant. Despite\nits limitations, the effectiveness of deep learning algorithms in\nanalysing this clinical data arises from their ability to uncover\ncomplex patterns and interactions within dataset. Future re-\nsearch should therefore aim to validate these findings across a\nwider range of datasets to bolster the model’s generalisability.\nIncorporating organ-level data, such as mammograms, could fur-\nther enhance the predictive accuracy of our model. Additionally,\nextending the application of BioFusionNet to other cancer types\nand clinical scenarios could yield more comprehensive insights,\nmaking the research more universally relevant and applicable.\nData Availability\nTCGA image data and clinical data are publicly available at\nhttps://portal.gdc.cancer.gov/.\nCode Availability\nOur work is fully reproducible and source code is publicly avail-\nable on GitHub at https://github.com/raktim-mondol/\nBioFusionNet.\n11\nAcknowledgement\nThis research was undertaken with the assistance of resources\nand services from the National Computational Infrastructure\n(NCI), which is supported by the Australian Government. Addi-\ntionally, data preprocessing was performed using the computa-\ntional cluster Katana, which is supported by Research Technol-\nogy Services at UNSW Sydney.\nReferences\n[1] Y. B. Shvetsov, L. R. Wilkens, K. K. White, M. Chong, A. Buyum,\nG. Badowski, R. T. L. Guerrero, and R. Novotny, “Prediction of\nbreast cancer risk among women of the Mariana Islands: the\nBRISK retrospective case–control study,” BMJ Open, vol. 12,\nno. 12, p. e061205, 2022.\n[2] W. Guo, W. Liang, Q. Deng, and X. Zou, “A multimodal affin-\nity fusion network for predicting the survival of breast cancer\npatients,” Frontiers in Genetics, vol. 12, p. 709027, 2021.\n[3] M. D. J. Peters, I. Ramsey, K. Kennedy, G. Sharplin, and M. Eck-\nert, “Culturally safe, high-quality breast cancer screening for\ntransgender people: a scoping review protocol,” Journal of Ad-\nvanced Nursing, vol. 78, no. 1, pp. 276–281, 2021.\n[4] C. Shuai, F. Yuan, Y. Liu, C. Wang, J. Wang, and H. He, “Estrogen\nreceptor—positive breast cancer survival prediction and analysis\nof resistance–related genes introduction,” PeerJ, vol. 9, p. e12202,\n2021.\n[5] K. Holli-Helenius, A. Salminen, I. Rinta-Kiikka, I. Koskivuo,\nN. Brück, P. Boström, and R. Parkkola, “MRI texture analysis in\ndifferentiating luminal A and luminal B breast cancer molecular\nsubtypes—a feasibility study,” BMC Medical Imaging, vol. 17,\np. 69, 2017.\n[6] H. Lindman, F. Wiklund, and K. K. Andersen, “Long-term treat-\nment patterns and survival in metastatic breast cancer by intrinsic\nsubtypes—an observational cohort study in Sweden,” BMC Can-\ncer, vol. 22, p. 1006, 2022.\n[7] Y. Han, J. Wang, and B. Xu, “Clinicopathological characteristics\nand prognosis of breast cancer with special histological types:\na surveillance, epidemiology, and end results database analysis,”\nThe Breast, vol. 54, pp. 114–120, 2020.\n[8] M. A. Han, E. C. Hwang, and J. H. Jung, “Prognostic factors\nof mortality in patients with cancer infected with COVID-19: a\nsystematic review protocol,” BMJ Open, vol. 13, no. 7, p. e071810,\n2023.\n[9] C. Nero, F. Ciccarone, A. Pietragalla, S. Duranti, G. Daniele,\nG. Scambia, and D. Lorusso, “Adjuvant treatment recommenda-\ntions in early-stage endometrial cancer: what changes with the\nintroduction of the integrated molecular-based risk assessment,”\nFrontiers in Oncology, vol. 11, p. 612450, 2021.\n[10] A. N. Zahari, N. S. Mohamad, and M. H. Mahmud, “Impacts\nof clinicopathological factors on metabolic parameters of 18F\nfluorodeoxyglucose PET/CT in the staging of breast cancer,” Jour-\nnal of Sustainability Science and Management, vol. 17, no. 12,\npp. 166–173, 2022.\n[11] X. Li, L. Liu, G. J. Goodall, A. Schreiber, T. Xu, J. Li, and T. D.\nLe, “A novel single-cell based method for breast cancer prognosis,”\nPLoS Computational Biology, vol. 16, no. 8, pp. 1–20, 2020.\n[12] M. I. Jaber, L. Beziaeva, C. W. Szeto, and S. C. Benz, “Deep\nlearning-based risk stratification for HER2-negative breast cancer\npatients,” bioRxiv, p. 10.1101/2021.05.26.445720, 2021.\n[13] M. Garutti, G. Griguolo, A. Botticelli, G. Buzzatti, C. D. Angelis,\nL. Gerratana, C. Molinelli, V. Adamo, G. Bianchini, L. Biganzoli,\nG. Curigliano, M. D. Laurentiis, A. Fabi, A. Frassoldati, A. Gen-\nnari, M. Scaltriti, F. Perrone, G. Viale, C. Zamagni, A. Zam-\nbelli, L. D. Mastro, S. D. Placido, V. Guarneri, P. Marchetti,\nand F. Puglisi, “Definition of high-risk early hormone-positive\nHER2-negative breast cancer: a consensus review,” Technology\nin Cancer Research & Treatment, vol. 14, no. 8, p. 1898, 2022.\n[14] B. Lu, E. Natarajan, H. R. B. Raghavendran, and U. D. Markan-\ndan, “Molecular classification, treatment, and genetic biomarkers\nin triple-negative breast cancer: a review,” Technology in Cancer\nResearch & Treatment, vol. 22, p. 15330338221145246, 2023.\n[15] L. Guo, D. Kong, J. Liu, L. Zhan, L. Luo, W. Zheng, Q. Zheng,\nC. Chen, and S. Sun, “Breast cancer heterogeneity and its implica-\ntion in personalized precision therapy,” Experimental Hematology\n& Oncology, vol. 12, no. 1, p. 3, 2023.\n[16] J. C. Wei, A. A. Suriawinata, B. Ren, Y. Zhang, M. Lisovsky, L. J.\nVaickus, C. R. Brown, M. J. Baker, N. Tomita, L. Torresani, J. Z.\nWei, and S. Hassanpour, “A petri dish for histopathology image\nanalysis,” Artificial Intelligence in Medicine, pp. 11–24, 2021.\n[17] E. Schaafsma, B. Zhang, M. Schaafsma, C. Tong, L. Zhang, and\nC. Cheng, “Impact of oncotype DX testing on ER+ breast cancer\ntreatment and survival in the first decade of use,” Breast Cancer\nResearch, vol. 23, no. 1, p. 74, 2021.\n[18] A. D. Caluwé, L. Buisseret, P. Poortmans, D. V. Gestel, R. Sal-\ngado, C. Sotiriou, D. Larsimont, M. Paesmans, L. Craciun, S. Dri-\nsis, C. Vandekerckhove, F. Reyal, I. Veys, D. Eiger, M. Piccart,\nE. Romano, and M. Ignatiadis, “Neo-CheckRay: radiation therapy\nand adenosine pathway blockade to increase benefit of immuno-\nchemotherapy in early stage luminal B breast cancer, a random-\nized phase II trial,” BMC Cancer, vol. 21, p. 899, 2021.\n[19] S. Paik, G. Tang, S. Shak, C. Kim, J. B. Baker, W. Kim, M. Cronin,\nF. L. Baehner, D. Watson, J. Bryant, J. P. Costantino, C. E. Geyer,\nD. L. Wickerham, and N. Wolmark, “Gene expression and benefit\nof chemotherapy in women with node-negative, estrogen recep-\ntor–positive breast cancer,” Journal of Clinical Oncology, vol. 24,\nno. 23, pp. 3726–3734, 2006.\n[20] K. Lee, S. H. Sim, E. J. Kang, J. H. Seo, H. D. Chae, K. S. Lee,\nJ. Y. Kim, J. S. Ahn, Y. H. Im, S. Park, Y. H. Park, and I. H.\nPark, “The role of chemotherapy in patients with HER2-negative\nisolated locoregional recurrence of breast cancer: a multicen-\nter retrospective cohort study,” Frontiers in Oncology, vol. 11,\np. 653243, 2021.\n[21] Y. Naoi, R. Tsunashima, K. Shimazu, and S. Noguchi, “The multi-\ngene classifiers 95GC/42GC/155GC for precision medicine in\nER-positive HER2-negative early breast cancer,” Cancer Science,\nvol. 112, no. 4, pp. 1369–1375, 2021.\n[22] J. Katzman, U. Shaham, A. Cloninger, J. Bates, T. Jiang, and\nY. Kluger, “DeepSurv: personalized treatment recommender sys-\ntem using a Cox proportional hazards deep neural network,” BMC\nMedical Research Methodology, vol. 18, p. 24, 2018.\n[23] Y. He, B. Hu, C. Zhu, W. Xu, X. Hao, B. Dong, X. Chen, Q. Dong,\nand X. Zhou, “A novel multimodal radiomics model for predicting\nprognosis of resected hepatocellular carcinoma,” Frontiers in\nOncology, vol. 12, p. 745258, 2022.\n[24] V. Subramanian, T. Syeda-Mahmood, and M. N. Do, “Multimodal\nfusion using sparse CCA for breast cancer survival prediction,”\nIEEE International Symposium on Biomedical Imaging, pp. 1429–\n1432, 2021.\n[25] R. Vanguri, J. Luo, A. Aukerman, J. V. Egger, C. J. Fong, N. Hor-\nvat, A. Pagano, J. d. A. B. Araújo-Filho, L. Geneslaw, H. Rizvi,\nR. E. Sosa, K. M. Boehm, S. Yang, F. M. Bodd, K. Ventura, T. J.\nHollmann, M. S. Ginsberg, J. Gao, M. D. Hellmann, J. L. Sauter,\nand S. P. Shah, “Multimodal integration of radiology, pathology\n12\nand genomics for prediction of response to PD-(L)1 blockade in\npatients with non-small cell lung cancer,” Nature Cancer, vol. 3,\nno. 10, pp. 1151–1164, 2022.\n[26] S. Deng, Y. Suo, S. Liu, X. Ma, H. Chen, X. Liao, J. Zhang,\nand W. W. Y. Ng, “MFCSA-CAT: a multimodal fusion method\nfor cancer survival analysis based on cross-attention transformer,”\nInternational Conference on Computer Information Science and\nArtificial Intelligence, vol. 12566, p. 1256608, 2023.\n[27] T. Wei, X. Yuan, R. Gao, L. J. Johnston, J. Zhou, Y. Wang,\nW. Kong, Y. Xie, Y. Zhang, D. Xu, and Z. Yu, “Survival predic-\ntion of stomach cancer using expression data and deep learning\nmodels with histopathological images,” Cancer Science, vol. 114,\nno. 2, pp. 690–701, 2022.\n[28] R. J. Chen, M. Y. Lu, W.-H. Weng, T. Y. Chen, D. F. Williamson,\nT. Manz, M. Shady, and F. Mahmood, “Multimodal co-attention\ntransformer for survival prediction in gigapixel whole slide im-\nages,” IEEE/CVF International Conference on Computer Vision,\npp. 3995–4005, 2021.\n[29] L. A. Vale-Silva and K. Rohr, “Long-term cancer survival predic-\ntion using multimodal deep learning,” Scientific Reports, vol. 11,\np. 13505, 2021.\n[30] R. Li, X. Wu, A. Li, and M. Wang, “HFBSurv: Hierarchical\nmultimodal fusion with factorized bilinear models for cancer\nsurvival prediction,” Bioinformatics, vol. 38, no. 9, pp. 2587–\n2594, 2022.\n[31] R. J. Chen, M. Y. Lu, J. Wang, D. F. Williamson, S. J. Rodig, N. I.\nLindeman, and F. Mahmood, “Pathomic Fusion: an integrated\nframework for fusing histopathology and genomic features for\ncancer diagnosis and prognosis,” IEEE Transactions on Medical\nImaging, vol. 41, no. 4, pp. 757–770, 2022.\n[32] Z. Lv, Y. Lin, R. Yan, Y. Wang, and F. Zhang, “TransSurv:\nTransformer-based survival analysis model integrating histopatho-\nlogical images and genomic data for colorectal cancer,”\nIEEE/ACM Transactions on Computational Biology and Bioinfor-\nmatics, vol. 20, no. 6, pp. 3411–3420, 2023.\n[33] J. Gao, P. Li, Z. Chen, and J. Zhang, “A survey on deep learning\nfor multimodal data fusion,” Neural Computation, vol. 32, no. 5,\npp. 829–864, 2020.\n[34] S. Garg, H. SS, and S. Kumar, “On-device document classifica-\ntion using multimodal features,” ACM India Joint International\nConference on Data Science & Management of Data, p. 203–207,\n2021.\n[35] K. Xu, Z. Lin, J. Zhao, P. Shi, W. Deng, and H. Wang, “Multi-\nmodal deep learning for social media popularity prediction with\nattention mechanism,” ACM International Conference on Multi-\nmedia, p. 4580–4584, 2020.\n[36] S. Qiu, X. Cui, Z. Ping, N. Shan, Z. Li, X. Bao, and X. Xu, “Deep\nlearning techniques in intelligent fault diagnosis and prognosis\nfor industrial systems: a review,” Sensors, vol. 23, no. 3, p. 1305,\n2023.\n[37] Y. Chen, H. Li, A. Janowczyk, P. Toro, G. Corredor, J. Whitney,\nC. Lu, C. F. Koyuncu, M. Mokhtari, C. Buzzy, S. Ganesan, M. D.\nFeldman, P. Fu, H. Corbin, A. Harbhajanka, H. Gilmore, L. J.\nGoldstein, N. E. Davidson, S. Desai, V. Parmar, and A. Madab-\nhushi, “Computational pathology improves risk stratification of a\nmulti-gene assay for early stage ER+ breast cancer,” npj Breast\nCancer, vol. 9, p. 40, 2023.\n[38] B. Li and C. N. Dewey, “RSEM: accurate transcript quantification\nfrom RNA-Seq data with or without a reference genome,” BMC\nBioinformatics, vol. 12, p. 323, 2011.\n[39] P. Bankhead, M. B. Loughrey, J. A. Fernández, Y. Dombrowski,\nD. G. McArt, P. D. Dunne, S. McQuaid, R. T. Gray, L. J. Murray,\nH. G. Coleman, J. A. James, M. Salto-Tellez, and P. W. Hamil-\nton, “QuPath: open source software for digital pathology image\nanalysis,” Scientific Reports, vol. 7, p. 16878, 2017.\n[40] M. Macenko, M. Niethammer, J. S. Marron, D. Borland, J. T.\nWoosley, X. Guan, C. Schmitt, and N. E. Thomas, “A method\nfor normalizing histology slides for quantitative analysis,” IEEE\nInternational Symposium on Biomedical Imaging, pp. 1107–1110,\n2009.\n[41] C. Mazo, S. Barron, and C. Mooney, “Multi-gene prognostic\nsignatures and prediction of pathological complete response to\nneoadjuvant chemotherapy in ER-positive, HER2-negative breast\ncancer patients,” Cancers, vol. 12, no. 5, p. 1133, 2020.\n[42] P. Blanchette, D. Sivajohanathan, J. M. Bartlett, A. Eisen,\nH. Feilotter, R. C. Pezo, G. Turashvili, and P. E. Williams, “Clin-\nical utility of multigene profiling assays in early-stage invasive\nbreast cancer: an Ontario Health (Cancer Care Ontario) clinical\npractice guideline,” Current Oncology, vol. 29, no. 4, pp. 2599–\n2616, 2022.\n[43] K. Almstedt, S. Mendoza, M. Otto, M. Battista, J. Steetskamp,\nA. S. Heimes, S. Krajnak, A. Poplawski, A. Gerhold-Ay, A. Hasen-\nburg, C. Denkert, and M. Schmidt, “Endopredict® in early hor-\nmone receptor-positive, HER2-negative breast cancer,” Breast\nCancer Research and Treatment, vol. 182, pp. 137–146, 2020.\n[44] S. Jahn, A. Bösl, O. Tsybrovskyy, C. Gruber-Rossipal, R. Helf-\ngott, F. Fitzal, M. Knauer, M. Bali´c, Z. Jasarevic, F. Offner, and\nF. Moinfar, “Clinically high-risk breast cancer displays markedly\ndiscordant molecular risk predictions between the mammaprint\nand endopredict tests,” British Journal of Cancer, vol. 122, no. 12,\npp. 1744–1746, 2020.\n[45] S. P. Somashekhar, S. Zaveri, D. G. Vijay, P. S. Dattatreya, R. Ku-\nmar, F. Islahi, and C. Bahl, “Individualized chemotherapy benefit\nprediction by endopredict in patients with early breast cancer in\nan Indian cohort,” JCO Global Oncology, no. 6, pp. 1363–1369,\n2020.\n[46] R. Buus, I. Šestak, R. Kronenwett, S. Ferree, C. A. Schnabel,\nF. L. Baehner, E. Mallon, J. Cuzick, and M. Dowsett, “Molecular\ndrivers of Oncotype DX, Prosigna, EndoPredict, and the Breast\nCancer Index: a TransATAC study,” Journal of Clinical Oncology,\nvol. 39, no. 2, pp. 126–135, 2021.\n[47] J. Warwick, L. Tabár, B. Viták, and S. W. Duffy, “Time-dependent\neffects on survival in breast carcinoma,” Cancer, vol. 100, no. 7,\npp. 1331–1336, 2004.\n[48] C. Woo, H. Silberman, S. Nakamura, W. Ye, R. Sposto, W. J.\nColburn, J. Waisman, and M. J. Silverstein, “Lymph node status\ncombined with lymphovascular invasion creates a more powerful\ntool for predicting outcome in patients with invasive breast cancer,”\nAmerican Journal of Surgery, vol. 184, no. 4, pp. 337–340, 2002.\n[49] H. Bor, E. N. Maina, B. Nyambega, K. Patel, C. O. Ol-\nwal, W. Nalyanya, and Y. Gavamukulya, “The potential of\ndifferentiation-related gene-1 (DRG1) as a biomarker for metas-\ntasis of estrogen receptor-positive breast cancer,” Journal of Ad-\nvances in Medicine and Medical Research, pp. 162–169, 2021.\n[50] S. Naz, M. Siddiqui, A. I. Memon, A. M. Bhatti, Z. I. Hussain,\nand Iqra, “Analysis of breast cancer receptors status and molecular\nsubtypes among female population,” Pakistan Journal of Medical\nand Health Sciences, vol. 17, no. 1, pp. 656–658, 2023.\n[51] M. Kang, H. Song, S. Park, D. Yoo, and S. Pereira, “Bench-\nmarking self-supervised learning on diverse pathology datasets,”\nIEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 3344–3354, 2023.\n13\n[52] R. J. Chen and R. G. Krishnan, “Self-supervised vision transform-\ners learn visual concepts in histopathology,” Annual Conference\non Neural Information Processing Systems, 2021.\n[53] X. Chen, S. Xie, and K. He, “An empirical study of training\nself-supervised vision transformers,” CoRR, p. 2104.02057, 2021.\n[54] A. van den Oord, Y. Li, and O. Vinyals, “Representation learning\nwith contrastive predictive coding,” arXiv, p. 1807.03748, 2018.\n[55] X. Wang, S. Yang, J. Zhang, M. Wang, J. Zhang, W. Yang,\nJ. Huang, and X. Han, “Transformer-based unsupervised con-\ntrastive learning for histopathological image classification,” Medi-\ncal Image Analysis, vol. 81, p. 102559, 2022.\n[56] J. Venugopalan, L. Tong, H. R. Hassanzadeh, and M. D.\nWang, “Multimodal deep learning models for early detection\nof Alzheimer’s disease stage,” Scientific Reports, vol. 11, p. 3254,\n2021.\n[57] S. Steyaert, Y. L. Qiu, Y. Zheng, P. Mukherjee, H. Vogel, and\nO. Gevaert, “Multimodal deep learning to predict prognosis in\nadult and pediatric brain tumors,” Communications Medicine,\nvol. 3, no. 1, p. 44, 2023.\n[58] M. Althobiti, K. A. El-sharawy, C. Joseph, M. Aleskandarany,\nM. S. Toss, A. R. Green, and E. A. Rakha, “Oestrogen-regulated\nprotein SLC39A6: A biomarker of good prognosis in luminal\nbreast cancer,” Breast Cancer Research and Treatment, vol. 189,\nno. 3, pp. 621–630, 2021.\n[59] L. Liu, J. Yang, and C. Wang, “Analysis of the prognostic signifi-\ncance of solute carrier (SLC) family 39 genes in breast cancer,”\nBioscience Reports, vol. 40, no. 8, p. BSR20200764, 2020.\n[60] C. Hogstrand, P. Kille, M. L. Ackland, S. Hiscox, and K. M.\nTaylor, “A mechanism for epithelial-mesenchymal transition and\nanoikis resistance in breast cancer triggered by zinc channel ZIP6\nand STAT3 (signal transducer and activator of transcription 3),”\nBiochemical Journal, vol. 455, pp. 229–237, 10 2013.\n[61] S. U. Gerold, K. M. Taylor, I. A. Muraina, D. Brethour, T. Nim-\nmanon, S. Ziliotto, P. Kille, and C. Hogstrand, “Zinc transporter\nZIP10 forms a heteromer with ZIP6 which regulates embryonic\ndevelopment and cell migration,” Biochemical Journal, vol. 473,\nno. 16, pp. 2531–2544, 2016.\n[62] K. M. Taylor, H. E. Morgan, K. Smart, N. M. Zahari, S. Pumford,\nI. O. Ellis, J. F. Robertson, and R. I. Nicholson, “The emerging\nrole of the LIV-1 subfamily of zinc transporters in breast cancer,”\nMolecular Medicine, vol. 13, no. 7-8, pp. 396–406, 2007.\n[63] A. Sappok and U. Mahlknecht, “Ribavirin restores ESR1 gene\nexpression and tamoxifen sensitivity in ESR1 negative breast\ncancer cell lines,” Clinical Epigenetics, vol. 3, p. 8, 2011.\n[64] W. Xu, M. Marcu, X. Yuan, E. Mimnaugh, C. Patterson, and\nL. Neckers, “Chaperone-dependent E3 ubiquitin ligase CHIP\nmediates a degradative pathway for c-ErbB2Neu,” Cell Biology,\nvol. 99, pp. 12847–12852, 2002.\n[65] W. Xia, S. Bacus, P. Hegde, I. Husain, J. Strum, L. Liu,\nG. Paulazzo, L. Lyass, P. Trusk, J. Hill, J. Harris, and N. L.\nSpector, “A model of acquired autoresistance to a potent ErbB2\ntyrosine kinase inhibitor and a therapeutic strategy to prevent its\nonset in breast cancer,” Proceedings of the National Academy of\nSciences, vol. 103, no. 20, pp. 7795–7800, 2006.\nRaktim Kumar Mondol is a PhD candidate\nin Computer Science and Engineering, spe-\ncializing in computer vision and bioinformat-\nics. He completed his MEng in Engineering\nwith High Distinction from RMIT University,\nAustralia.\nMondol’s research interests in-\nclude histopathological image analysis, clini-\ncal prognosis prediction, and enhancing clini-\ncal understanding through the interpretability\nof computational models.\nEwan Millar is a Senior Staff Specialist\nHistopathologist with NSW Health Pathology\nat St George Hospital Sydney with expertise\nin breast cancer pathology and translational\nresearch and a strong interest in AI and digi-\ntal pathology applications.\nArcot Sowmya is Professor in the School of\nComputer Science and Engineering, UNSW.\nHer major research interest is in the area\nof Machine Learning for Computer Vision\nand includes learning object models, fea-\nture extraction, segmentation and recogni-\ntion based on computer vision, machine\nlearning and deep learning. In recent years,\napplications in the broader health area are a\nfocus, including biomedical informatics and\nrapid diagnostics in the real world. All of\nthese areas have been supported by com-\npetitive, industry and government funding.\nErik Meijering (Fellow, IEEE), is a Profes-\nsor of Biomedical Image Computing in the\nSchool of Computer Science and Engineer-\ning. His research focusses on the develop-\nment of innovative computer vision and ma-\nchine learning (in particular deep learning)\nmethods for automated quantitative analysis\nof biomedical imaging data.\n"
}