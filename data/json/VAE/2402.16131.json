{
    "optim": "A VAE-based Framework for Learning Multi-Level Neural Granger-Causal Connectivity Jiahe Lin Huitian Lei George Michailidis* Abstract Granger causality has been widely used in various application domains to capture lead-lag relationships amongst the components of complex dynamical systems, and the focus in extant literature has been on a single dynamical system. In certain applications in macroeconomics and neuroscience, one has access to data from a collection of related such systems, wherein the modeling task of interest is to extract the shared common structure that is embedded across them, as well as to identify the idiosyncrasies within individual ones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger- causal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems, and handles the aforementioned task in a principled way. The performance of the proposed framework is evaluated on several synthetic data settings and benchmarked against existing approaches designed for individual system learning. The method is further illustrated on a real dataset involving time series data from a neurophysiological experiment and produces interpretable results. Keywords: deep neural networks; variational autoencoder; joint-learning; Granger-causality 1 Introduction The concept of Granger causality introduced in Granger (1969) leverages the temporal ordering of time series data. It is defined in terms of predictability of future values of a time series; namely, whether the inclusion of past information (lag values) of other time series as well as its own (self lags) leads to a reduction in the variance of the prediction error of the time series under consideration. Since its introduction, it has become a widely-used approach in the analysis of economic (Stock and Watson, 2001), financial (Hong et al., 2009) and neuroimaging (Seth et al., 2015) time series data. The standard setting in these applications is that one is interested in estimating Granger causal relationships in a dynamical system (e.g., a national economy, a brain) comprising of p variables. Granger causality can also be expressed through the language of graphical models (Dahlhaus and Eichler, 2003; Eichler, 2012). The node set of the graph corresponds to the p variables at different time points; directed edges between nodes at past time points to those at the present time capture Granger causal re- lationships (for more details and a pictorial illustration see Section C.1). Traditionally, Granger causality was operationalized through linear vector autoregressive (VAR) models (Granger, 1969), in which case the entries of the estimated transition matrices correspond precisely to the edges of the Granger causal graph. More recent work has explored how Granger causal relationships can be learned through nonlinear models; e.g., see review paper Shojaie and Fox (2022) and references therein. In certain application domains, one has access to data from a collection of related dynamical systems. A motivating example is described next. Consider electroencephalography (EEG) recordings obtained from p electrodes placed on the scalp of a subject (e.g., a patient or an animal). The resulting time series data consti- tute measurements from a complex neurophysiological dynamical system (Stam, 2005). On many instances, *Corresponding Author. Department of Statistics and Data Science, UCLA. 〈gmichail@ucla.edu〉 Code repo: https://github.com/GeorgeMichailidis/vae-multi-level-neural-GC-official 1 arXiv:2402.16131v1  [cs.LG]  25 Feb 2024 one has access to such measurements for a collection of M related subjects (or “entities”, equivalently); for example, they may be performing the same cognitive task (e.g., visual counting, geometric figure rotation) or exhibit a similar neurological disorder (e.g., epilepsy, insomnia, dementia). In such a setting, one can always opt to perform separate analyses on each subject’s data; however, it would be useful to develop methodology that models the data from all subjects jointly, so as to simultaneously extract the embedded structure shared across subjects and identify the idiosyncrasies (heterogeneity) in any single one. In other words, if one views all subjects as belonging to a common group, the quantities of interest are the shared group-level connectivity structure (amongst nodes) and the entity-level ones. Conceptually, the above-mentioned modeling task is not difficult to accomplish in a linear setting where one can decompose the transition matrices into a “shared” component and an idiosyncratic (entity-specific) one, with some orthogonality-type constraint to enforce identifiability of the parameters (more details provided in Section C.3). However, the task becomes more challenging and involved in non-linear settings where one hopes to use flexible models to capture the underlying complex dynamics. In particular, a decomposition- based approach, which requires the exact specification of the functional form of the shared component or how the shared and the idiosyncratic components interact, would be rather restrictive. To this end, we adopt a generative model-based approach, which circumvents the issue by encoding the Granger causal relationships through graphs. By postulating a model with a hierarchical structure between the shared and entity-specific components, the problem can be addressed in a flexible, yet principled manner. Summary of contributions. We develop a two-layer Variational Autoencoder (VAE) based framework for estimating Granger-causal connections amongst nodes in a collection of related dynamical systems — jointly for the common group-level and the entity-level ones — in the presence of entity-specific heterogeneity. De- pending on the assumed connection type (continuous or binary) amongst the nodes, the proposed framework can accommodate the scenario accordingly by imposing a commensurate structure on the encoded/decoded distributions, leveraging conjugacy between pairs of distributions. The proposed model enables extracting the embedded common structure in a principled way, without resorting to any ad-hoc or post-hoc aggre- gation. Finally, the framework can be generalized to the case where multiple levels of nested groups are present and provides estimates of the group-level connectivity for all levels of groups. The remainder of the paper is organized as follows. In Section 2, we provide a review of related literature on Granger-causality estimation, with an emphasis on neural network-based methods. The main building block used in the proposed framework, namely, a multi-layer VAE is also briefly introduced. Section 3 describes in detail the proposed framework, including the encoder/decoder modules and the training/inference pro- cedure. In Section 4, model performance is assessed on synthetic datasets and benchmarked against several existing methods. An application to a real dataset involving EEG signals from 22 subjects is discussed in Section 5. Finally, Section 6 concludes the paper. 2 Related Work and Preliminaries In this section, we review related work on inferring Granger causality based on time series data, with an em- phasis on deep neural network-based approaches. Further, as the proposed framework relies on variational autoencoders (VAE) with a hierarchical structure, we also briefly review VAEs in the presence of multiple latent layers. 2.1 Inference of Granger causality Linear VAR models have historically been the most popular approach for identifying Granger causal rela- tionships. Within the linear setting, hypothesis testing frameworks with theoretical guarantees have been developed (Granger, 1980; Geweke, 1984), while more recently regularized approaches have enabled the estimation in the high-dimensional setting (Basu et al., 2015). Recent advances in neural network techniques 2 have facilitated capturing non-linear dynamics and identifying Granger causality accordingly, as discussed next. Note that estimation of Granger causality is an unsupervised task, in the sense that the connectivity as cap- tured by the underlying graph is not observed and thus cannot serve as the supervised learning target. De- pending on the model family that the associated estimation procedure falls into, existing approaches suitable for estimating Granger causality based on neural networks (Montalto et al., 2015; Nauta et al., 2019; Wu et al., 2020; Khanna and Tan, 2020; Tank et al., 2021; Marcinkeviˇcs and Vogt, 2021; L¨owe et al., 2022) can be broadly categorized into prediction-based and generative model-based ones. We selectively review some of them next. In the remainder of this subsection, we use xi,t to denote the value of node i at time t, xt := (x1,t, · · · , xp,t) the collection of node values of the dynamical system, and x := {x1, · · · , xT } the trajectory over time. Within the predictive modeling framework, recent representative works include Khanna and Tan (2020); Tank et al. (2021); Marcinkeviˇcs and Vogt (2021), where the Granger-causal relationship is inferred from coefficients that govern the dynamics of the time series, and the coefficients are learned by formulating prediction tasks that can be generically represented as xt = f(xt−1, ..., xt−q) + εt, with xt ∈ Rp being the multivariate time series signal and εt the noise term. In Tank et al. (2021), coordinates of the response are considered separately, that is, xi,t = fi(xt−1, ..., xt−q) + εi,t, and fi is parameterized using either multi-layer perceptrons (MLP) or LSTM (Hochreiter and Schmidhuber, 1997). In the case of an L-layer MLP, bxi,t = W LhL−1 t + bL; hl t = σ \u0010 W lhl−1 t + bl\u0011 , l = 2, · · · , L; h1 t = σ \u0010 Xq k=1 W 1kxt−k + b1\u0011 ; the Granger-causal connection from the jth node to the ith node is then encoded in some “summary” (e.g., Frobenius norm) of {W 11 :j , · · · , W 1q :j }, with each component corresponding to the first hidden layer weight of lags xj,t−1, · · · , xj,t−q. Various regularization schemes are considered and incorporated as penalty terms in the loss function, to encourage sparsity and facilitate the identification of Granger-causal connections. The case of LSTM-based parameterization is handled analogously. Marcinkeviˇcs and Vogt (2021) parameterizes f as an additive function of the lags, i.e., xt = Pq k=1 Ψk(xt−k)xt−k + εt; the output of Ψk : Rp 7→ Rp×p contains the generalized coefficients of xt−k, whose (i, j) entry corresponds to the impact of xj,t−k on xi,t and Ψk is parameterized through MLPs. The Granger causal connection between the jth node and the ith node is obtained by aggregating information from the coefficients of all lags {Ψk(xt−k)ij}, i.e., max1≤k≤q{medianq+1≤t≤T (|Ψk(xt−k)ij|)}. Finally, an additional stability-based procedure where the model is fit to the time series in the reverse order is performed for the final selection of the connections.1 It is worth noting that for both of the above-reviewed approaches, the ultimately desired node j to node i (∀ i, j ∈ {1, · · · , p}) Granger causal relationship is depicted by a scalar value, whereas in the modeling stage, such a connection is collectively captured by multiple “intermediate” quantities—{W 1k :j , k = 1, · · · q} in Tank et al. (2021) and {Ψk(xt−k)ij, k = 1, · · · , q} in Marcinkeviˇcs and Vogt (2021); hence, an information aggregation step becomes necessary to summarize the above-mentioned quantities to a single scalar value. For generative model-based approaches, the starting point is slightly different. Notable ones include L¨owe et al. (2022) that builds upon Kipf et al. (2018), and the focus is on relational inference. The postulated generative model assumes that the trajectories are collectively governed by an underlying latent graph z, which effectively encodes Granger-causal connections: p(x|z) = p({xT +1, · · · , x1}|z) = YT t=1 p(xt+1|xt, · · · , x1, z). Specifically, in their setting, xi,t ∈ Rd is vector-valued and zij corresponds to a categorical “edge type” be- tween nodes i and j. For example, it can be a binary edge type indicating presence/absence, or a more complex one having more categories. To simultaneously learn the edge types and the temporal dynam- ics, the model is formalized through a VAE that maximizes the evidence lower bound (ELBO), given by Eqϕ(z|x)(log pθ(x|z))−KL(qϕ(z|x) \r\r pθ(z)), where qϕ(z|x) is the probabilistic encoder, pθ(x|z) the decoder, and 1This stability-based step amounts to finding an optimal thresholding level for the “final” connections: the same model is fit to the time series in the reverse order, and “agreement” is sought between the Granger causal connections obtained respectively based on the original and the reverse time series, over a sequence of thresholding levels; the optimal one is determined by the one that maximizes the agreement measure. We refer interested readers to the original paper and references therein for more details. 3 pθ(z) the prior distribution. Concretely, the probabilistic encoder is given by qϕ(z|x) = softmax \u0000fenc,ϕ(x) \u0001 and it infers the type for each entry of z; the function fenc,ϕ is parameterized by neural networks. The de- coder pθ(x|z) = QT t=1 pθ(xt+1|z, xτ : τ ≤ t) projects the trajectory based on past values and z—specifically, the distributional parameters for each step forward. For example, if a Gaussian distribution is assumed, in the Markovian case, pθ(xt+1|xt, z) = N(mean, variance), where mean = f 1 dec,θ(xt, z), variance = f 2 dec,θ(xt, z) and f 1 dec,θ, f 2 dec,θ are parameterized by some neural networks. Finally, maximizing the ELBO loss can be alternatively done by minimizing −Eqϕ(z|x)(log pθ(x|z)) + KL(qϕ(z|x) \r\r pθ(z)) := negative log-likelikehood + H \u0000qϕ(z|x) \u0001 + const; the negative log-likelihood corresponds to the reconstruction error of the entire trajectory coming out of the decoder, and the KL divergence term boils down to the sum of entropies denoted by H(·) if the prior pθ(z) is assumed to be a uniform distribution over edge types. In summary, at the formulation level, generative model-based approaches treat Granger-causal connections (relationships) as a latent graph and learn it jointly with the dynamics, whereas predictive ones extract Granger-causal connections from the parameters that govern the dynamics in a post-hoc manner. The former can readily accommodate vector-valued nodes whereas for the latter, it becomes more involved and further complicates how the connections can be extracted/represented based on the model parameters. At the task level, to learn the model parameters, prediction-based approaches rely on tasks where the predicted values of the future one-step-ahead timestamp are of interest, whereas generative approaches amount to reconstructing the observed trajectories; prediction and reconstruction errors constitute part of the empirical risk minimization loss and the ELBO loss, respectively. 2.2 Multi-layer variational autoencoders With a slight abuse of notation, in this subsection, we use x to denote the observed variable and zl, l = 1, · · · , L the latent ones for L layers. A “shallow” VAE with one latent layer is considered in the seminal work of Kingma and Welling (2014), where the generative model is given by pθ(x, z1) = pθ(x|z1)pθ(z1), with pθ(z1) denoting the prior distribution. Later works (Kingma et al., 2014; Burda et al., 2016; Sønderby et al., 2016) consider the extension into multiple latent layers, where the generative model can be represented through a cascading structure as follows: pθ(x, {zl}L l=1) = pθ(x|z1) \u0010 YL−1 l=1 pθ(zl|zl+1) \u0011 pθ(zL); the corresponding inference model (encoder) is given by qϕ(z1, · · · , zL|x) = qϕ(z1|x) QL i=1 qϕ(zl|zl−1). The variational lower bound on log p(x) can be written as Eqϕ({z}L l=1|x) \u0010 log pθ(x|{z}L l=1) \u0011 − KL \u0010 qϕ({z}L l=1|x) \r\r pθ({z}L l=1) \u0011 , (1) with the first term corresponding to the reconstruction error. Conjugacy adjustment. Under the above multi-layer setting, Sønderby et al. (2016) considers an inference model that recursively merges information from the “bottom-up” encoding and “top-down” decoding steps. Concretely, in the case where each layer is specified by a Gaussian distribution, the original distribution at layer l after encoding is given by qϕ(zl|zl−1) ∼ N(µq,l, σ2 q,l) and the distribution at the same layer after decod- ing is given by pθ(zl|zl+1) ∼ N(µp,l, σ2 p,l). The adjustment amounts to a precision-weighted combination that combines information from the decoder distribution into the encoder one, that is, qϕ(zl|·) ∼ N \u0000˜µq,l, ˜σ2 q,l \u0001 , where ˜µq,l = (µq,lσ−2 q,l +µp,lσ−2 p,l )/(σ−2 q,l +σ−2 p,l ) and ˜σ2 q,l = 1/(σ−2 q,l +σ−2 p,l ). This information-sharing mechanism leads to richer latent representations and improved approximation of the log-likelihood function. A similar objective is also considered in Burda et al. (2016) and operationalized through importance weighting. 4 It is worth noting that the precision-weighted adjustment in Sønderby et al. (2016) is in the spirit of the conjugate analysis in Bayesian statistics. In particular, in Bayesian settings where the data likelihood is assumed Gaussian with a fixed variance parameter and the prior distribution is also assumed Gaussian, the posterior distribution possesses a closed-form Gaussian distribution (and hence conjugate w.r.t. the prior)2. For this reason, we term such an adjustment as the “conjugacy adjustment”, which will be used later in our technical development. Finally, utilizing multiple layers possessing a hierarchy as discussed above resembles the framework adopted in Bayesian hierarchical modeling. We provide a brief review of the topic in Section C.2. We also sketch in Section C.3 a modeling formulation under this framework for collection of linear VARs. 3 The Proposed Framework Given a collection of trajectories for the same set of p variables (nodes) from M dynamical systems (entities), we are interested in estimating the Granger causal connections amongst the nodes in each system (i.e., entity- level connections), as well as the common “backbone” connections amongst the nodes that are shared across the entities (i.e., group-level connections). To this end, we propose a two-layer VAE-based framework, wherein Granger-causal connections are treated as latent variables with a hierarchical structure, and they are learned jointly with the dynamics of the tra- jectories. In Section 3.1, we present the posited generative process that is suitable for the modeling task of interest, and give an overview of the proposed VAE-based formulation; the details of the components in- volved and their exact modeling considerations are discussed in Section 3.2. Section 3.3 provides a summary of the end-to-end training process and the inference tasks that can be performed based on the trained model. The generalization of the proposed framework to the case of multiple levels of grouping across entities is deferred to Appendix F, where the grand common and the group common structures can be simultaneously learned with those of the entities. 3.1 An overview of the formulation ¯z z[1] z[2] z[M−1]z[M] x[1] x[2] x[M−1]x[M] pθ⋆(z[m]|¯z) pθ⋆(x[m]|z[m]) Figure 1: Diagram for the postulated top-down gener- ative process. Consider a setting where there are M entities, each of them hav- ing the same set of p nodes, that evolve as a dynamical system. Let x[m] i,t denote the value of node i of entity m ∈ {1, · · · , M} at time t. It can be either scalar or vector-valued, with scalar node values being prevalent in traditional time-series settings; in the latter case, the nodes can be thought of as being characterized by vector-valued “features”3. Let x[m] t := (x[m] 1,t , · · · , x[m] p,t ) be the collection of node values at time t for entity m, and x[m] := {x[m] 1 , · · · , x[m] T } the cor- responding trajectory over time. Further, let z[m] ∈ Rp×p denote the Granger-causal connection matrix of entity m and ¯z := [¯zij] ∈ Rp×p the common structure embedded in z[1], · · · , z[M], and note that it does not necessarily correspond to the arithmetic mean of the z[m]’s. In the remainder of this paper, we may refer to these matrices as “graphs” interchangeably. Depending on the modeling scenario, the entity-level Granger-causal connections z[m] can either be binary or continuous. In the former case, it corresponds precisely to the aggregate Granger causal graph defined in Dahlhaus and Eichler (2003); 2Note that in the case where the data likelihood is Gaussian, but the variance is no longer a fixed parameter, the Normal-Normal conjugacy does not necessarily go through. 3For example, in the Springs experiment in Kipf et al. (2018) (which is also considered in this paper; see Appendix B.1), the features correspond to a 4-dimensional vector, with the first two coordinates being the 2D velocity and the last two being the 2D location 5 in the latter case, its (i, j)-entry (scalar value) reflects the strength of the relationship between the past value(s) of node j the present value of node i; see Appendix C.1 and Remark 6 for a detailed discussion. The posited generative process, whose true parameters are denoted by θ⋆, is given by: pθ⋆ \u0010 {x[m]}M m=1, {z[m]}M m=1, ¯z \u0011 = pθ⋆ \u0010 {x[m]}M m=1|{z[m]}M m=1 \u0011 · pθ⋆ \u0010 {z[m]}M m=1|¯z \u0011 · pθ⋆(¯z) = YM m=1 pθ⋆(x[m]|z[m]) YM m=1 pθ⋆(z[m]|¯z) Y 1≤i,j≤p pθ⋆(¯zij). (2) The decomposition is based on the following underlying assumptions (see also Figure 1 for a pictorial illus- tration): • conditional on the entity-specific graphs z[m], their trajectories x[m]’s are independent of the grand com- mon ¯z, and they are conditionally independent from each other given their respective entity-specific graphs z[m]’s • the entity-specific graphs z[m] are conditionally independent given the common graph ¯z • the prior distribution pθ⋆(¯z) factorizes over the edges. The proposed model creates a hierarchy between the common graph and the entity-specific ones, which in turn naturally provides a coupling mechanism amongst the latter. The grand common structure can be estimated as one learns all the latent components jointly with the dynamics of the system through a VAE. Let X := {x[1], · · · , x[m]}, Z := {¯z, z[1], · · · , z[m]}, qϕ(Z|X) denote the encoder, pθ(X|Z) the decoder and pθ(Z) the prior distribution. Then, the ELBO is given by Eqϕ(Z|X) \u0010 log pθ(X|Z) \u0011 − KL \u0010 qϕ(Z|X) \r\r pθ(Z) \u0011 , and serves as the objective function for the end-to-end encoding-decoding procedure as depicted in Figure 2. {x[m]} {z[m]}|{x[m]} sampled ¯z pθ(¯z) {z[m]} | · {ˆx[m]} qϕ(z[m]|xm) qϕ(¯z|{¯zm}) pθ({z[m]}|¯z) pθ({x[m]}|{z[m]}) (merge info) (merge info) (observed) encoding decoding (reconstructed) (prior) Figure 2: Diagram for the end-to-end encoding-decoding procedure. Solid paths with arrows denote modeling the corresponding distributions during the encoding/decoding process; dashed paths with arrows correspond to information merging based on (weighted) conjugacy adjustment. Quantities obtained after each step are given inside the circles/rectangles. {x[m]} is short for the collection {x[m]}M m=1; {z[m]} is analogously defined. Remark 1 (On the proposed formulation). (1) Depending on the assumption on the entity-level Granger- causal connections z[m]—either binary or continuous—encoder/decoder distributions can then be selected accordingly. In particular, distributions that form conjugate pairs (e.g., Gaussian-Gaussian for the continuous case and Beta-Bernoulli for the binary case) can facilitate computations. (2) The proposed framework natu- rally allows estimation of positive/negative connections in a principled way without resorting to ad-hoc ag- gregation schemes. It also enables incorporation of external information pertaining to the presence/absence of connections through the decoder. (3) In settings where a large collection of entities is available, but each entity has limited sample size, the joint learning framework can be advantageous over an individual entity learning one. 6 3.2 Modeling details Next, we provide details on the specification of the encoder and the decoder, the sampling steps, and the loss function calculations for model (2). 3.2.1 Encoder The goal of the encoder is to infer the latent graphs Z := {¯z, z[1], · · · , z[M]} based on the observed trajectories X := {x[1], · · · , x[M]}. Let ϕ denote the collection of parameters in the encoder qϕ(Z|X). To delineate the dependency between the trajectories and the graphs, the following assumptions are imposed: • conditioning on {z[m]}M m=1, ¯z is independent of {x[m]}M m=1 and the conditional probability qϕ \u0000¯z|{z[m]}M m=1 \u0001 factorizes across edges (i, j); • the entity-specific graphs are conditionally independent given their corresponding trajectories, i.e., qϕ \u0000{z[m]}M m=1|{x[m]}M m=1 \u0001 factorizes across entities. These assumptions are in line with the structure of the model in (2), in that the conditional dependencies posited in the generative model are respected during the “bottom-up” encoding process. Consequently, the encoder can be decomposed into the following product components: qϕ \u0000Z \f\fX \u0001 = qϕ \u0010 ¯z \f\f {z[m]}M m=1 \u0011 M Y m=1 qϕ \u0010 z[m]\f\fx[m]\u0011 = Y 1≤i,j≤p qϕ \u0010 ¯zij |{z[m] ij }M m=1 \u0011 M Y m=1 qϕ \u0010 z[m]\f\fx[m]\u0011 . There are two types of terms in the above expression: qϕ(z[m]|x[m]) that infers each entity’s latent graph based on its trajectory, and qϕ(¯zij|{z[m] ij }M m=1) that obtains the grand common based on the entity-level graphs, in an edge-wise manner. Note that for qϕ(¯zij|{z[m] ij }M m=1), together with modeling pθ(z[m] ij |¯zij), resembles prior- posterior calculations in Bayesian statistics using conjugate pairs of distributions; hence, depending on the underlying structural assumptions (continuous or binary) on the z[m]’s, one can choose emission heads (or equivalently, the output functional form) accordingly. At the high level, the encoder can be abstracted into 3 modules, parameterized through fx→h, fh→z and fz→¯z, respectively: (enc-a) trajectory to hidden representation x[m] → h[m] := fx→h(x[m]), with h[m] ij corresponding to the edge-specific one; (enc-b) hidden representation to the entity-specific graph: h[m] → z[m] := fh→z(h[m]); (enc-c) entity-level graphs to the grand common (edge-wise): {z[m] ij }M m=1 → ¯zij := fz→¯z({z[m] ij }M m=1). Modules (enc-a) and (enc-b) combined, model qϕ(z[m]|x[m]) and correspond to “Trajectory2Graph” opera- tions, while module (enc-c) models qϕ(¯zij|{z[m] ij }M m=1) and captures the “Entity2Common” one. On the other hand, given the above-mentioned conjugate pair consideration, the choices of fh→z and fz→¯z are considered jointly. Formally, for fx→h, we use a similar approach to that in Kipf et al. (2018), where fx→h entails message- passing operations that are widely adopted in the literature related to graph neural networks (Scarselli et al., 2008; Gilmer et al., 2017). At a high level, these operations entail “node2edge” (concatenating the representation of the node stubs) and “edge2node” (aggregating the representation of incoming edges) iteratively and non-linear functions (e.g., MLPs) in between. The operation ultimately leads to {h[m] ij }, with h[m] ij ∈ Rnhid being a nhid-dimensional hidden representation corresponding to z[m] ij . Full details are provided in Appendix A.1 wherein we also provide a pictorial illustration for the operations. 7 Once the h[m] ij ’s are obtained, subsequent modeling in modules (enc-b) and (enc-c) can be generically repre- sented as z[m] ij | h[m] ij ∼ qz(· ; δ[m] q,ij), and ¯zij |{z[m] ij } ∼ q¯z(· ; ¯δij), where qz(· ; δ[m] q,ij) is some distribution with parameter δ[m] q,ij := fh→z(h[m] ij ) being the function output of fh→z. Similarly, q¯z(· ; ¯δq,ij) is some distribution with parameter ¯δq,ij := fz→¯z({z[m] ij }) being the function output of fz→¯z. The exact choices for fh→z and fz→¯z bifurcate depending on the scenario: • Case 1, z[m]’s entries being continuous: in this case, we consider a Gaussian-Gaussian emission head pair. Consequently, δ[m] q,ij = {µ[m] q,ij, (σ[m])2 q,ij}, ¯δq,ij = {¯µq,ij, ¯σ2 q,ij}; qz ∼ N \u0010 µ[m] q,ij, (σ[m])2 q,ij \u0011 ; µ[m] q,ij := f 1 h→z(h[m] ij ), (σ[m])2 q,ij := f 2 h→z(h[m] ij ); (3) q¯z ∼ N \u0010 ¯µq,ij, ¯σ2 q,ij \u0011 ; ¯µq,ij := f 1 z→¯z({z[m] ij }), ¯σ2 q,ij := f 2 ¯z→z({z[m] ij }). (4) f 1 h→z, f 2 h→z are component functions of fh→z, each with an nhid-dimensional input and a scalar out- put; they can be simple linear functions with f 2 h→z having an additional softplus operation to ensure positivity. Similarly, f 1 z→¯z, f 2 z→¯z comprise fz→¯z, each with an m-dimensional input and a scalar output; in practice their functional form can be as simple as taking the sample mean and standard deviation, respectively. • Case 2, z[m]’s entries being binary: in this case, we consider a Beta-Bernoulli emission head pair, i.e., qz ∼ Ber \u0010 δ[m] q,ij \u0011 ; δ[m] q,ij := fh→z(h[m] ij ), (5) q¯z ∼ Beta \u0010 ¯αq,ij, ¯βq,ij \u0011 ; ¯αq,ij := f 1 z→¯z({z[m] ij }), ¯βq,ij := f 2 z→¯z({z[m] ij }). (6) The output of fh→z corresponds to the Bernoulli success probability and it is parameterized with an MLP with the last layer performing sigmoid activation to ensure that the output lies in (0, 1). f 1 z→¯z and f 2 z→¯z are component functions of fz→¯z. Similar to the Gaussian case, their choice need not be complicated and is chosen based on moment-matching. Note that the prior distribution pθ(¯zij) is also selected according to the underlying scenario, with a standard Normal distribution used in the continuous case and a Beta(1, 1) in the binary case. Once the distribution parameters for ¯zij are obtained based on (4) or (6), we apply conjugacy adjustment to incorporate also the information from the prior, before the sampling step takes place. 3.2.2 Decoder The goal of the decoder pθ(X|Z) is to reconstruct the trajectories based on the entity and group level graphs, and its components follow from the generative process described in (2), that is, pθ(X|Z) = pθ \u0010 {x[m]}M m=1|{z[m]}M m=1 \u0011 · pθ \u0010 {z[m]}M m=1|¯z \u0011 = YM m=1 pθ(x[m]|z[m]) YM m=1 pθ(z[m]|¯z), where θ denotes the collections of parameters in the decoder. The two components pθ(z[m]|¯z) and pθ(x[m]|z[m]), respectively capture the dependency between the entity-specific graphs z[m]’s and their grand common ¯z, and the evolution of the trajectories given z[m]. Consequently, the decoder can be broken into two modules, parameterized through g¯z→z and gz→x: (dec-a) pθ(z[m]|¯z), the grand common to entity-specific graphs z → z[m] := g¯z→z(¯z), with g¯z→z(·) acting on the sampled ¯z (edge-wise). Samples drawn from this distribution will be used to guide the evolution of the trajectories of the corresponding entity; (dec-b) pθ(x[m]|z[m]), graph to trajectory z[m] → xm; concretely, pθ(x[m]|z[m]) = pθ(x[m] 1 |z[m]) YT t=2 pθ \u0010 x[m] t | x[m] t−1, ..., x[m] 1 , z[m]\u0011 , 8 with pθ(x[m] t | x[m] t−1, ..., x[m] 1 , z[m]) modeled through gz→x(x[m] t−1, · · · , x[m] t−q, z[m]) assuming a fixed con- text length of q (or q-lag dependency, equivalently). We refer to these two modules as “Common2Entity” and “Graph2Trajectory”, respectively. Common2Entity. We consider a weighted conjugacy adjustment that merges the information from the encoder distribution into the decoder one, so that it contains both the grand common and the entity-specific information. Concretely, for some pre-specified weight ω ∈ [0, 1], • Case 1, in the continuous case, let pθ(z[m] ij |¯zij) ∼ N(µ[m] p,ij, (σ[m])2 p,ij) with µ[m] p,ij := g1 ¯z→z(¯z[m] ij ) and (σ[m])2 p,ij := g2 ¯z→z(¯z[m] ij ); g1 ¯z→z, g2 ¯z→z : R 7→ R are component functions of g¯z→z. This gives the “un- adjusted” distribution that contains only the grand common information. With µ[m] q,ij and (σ[m])2 q,ij obtained in (3), the weighted adjustment gives pθ(z[m] ij |·) ∼ N \u0010 ˜µ[m] p,ij, (˜σ[m])2 p,ij \u0011 , where ˜µ[m] p,ij := ωµ[m] q,ij(σ[m])−2 q,ij + (1 − ω)µ[m] p,ij(σ[m])−2 p,ij ω(σ[m])−2 q,ij + (1 − ω)(σ[m])−2 p,ij , (˜σ[m])2 p,ij := 1 ω(σ[m])−2 q,ij + (1 − ω)(σ[m])−2 p,ij . (7) • Case 2, in the binary case, let pθ(z[m] ij |¯zij) ∼ Ber(δ[m] p,ij), where δ[m] p,ij := g¯z→z(¯zij). With δ[m] q,ij obtained in (5), the weighted adjustment gives pθ(z[m] ij |·) ∼ Ber \u0010 ˜δ[m] p,ij \u0011 ; ˜δ[m] p,ij = 1 ω/δ[m] q,ij + (1 − ω)/δ[m] p,ij . (8) Similar to the function fz→¯z in the encoder, here g¯z→z corresponds to fz→¯z’s “reverse-direction” counterpart and its choice can be rather simple4. Remark 2 (On the role of ω). It governs the mixing percentage of the entity-specific and the common in- formation: when ω = 1, the “tilde” parameters of the post-adjustment distribution effectively collapse into the encoder ones (e.g., ˜δp,ij ≡ δ[m] q,ij and analogously for ˜µp,ij, ˜σ2 p,ij); correspondingly, samples drawn from pθ(z[m] ij |·) essentially ignore the sampled ¯z and hence they can be viewed as entirely entity-specific. At the other extreme, for ω = 0, the tilde parameters coincide with the unadjusted ones; therefore, apart from the grand common information carried in the sampled ¯z, no entity-specific one is passed onto the sampled z[m]. By varying ω between (0, 1), one effectively controls the level of heterogeneity and how strongly the sampled entity-specific graphs deviate from the grand common one. Graph2Trajectory. Module (dec-b) pertains to modeling the dynamics of the trajectory x[m] given the sam- pled z[m]. Here, we focus on one-step Markovian dependency, i.e., q = 1 and thus pθ(x[m] t | x[m] t−1, ..., x[m] 1 , z[m]) ≈ gz→x(x[m] t−1, z[m]). The extension to longer lag dependencies (q > 1) can be readily obtained by pre-processing the input accordingly, as discussed in Appendix A.2. We consider the following parameterization of gz→x. At the high level, given that z[m] ij corresponds to the Granger-causal connection from node j to node i, it should serve as a “gate” controlling the amount of information that can be passed from x[m] j,t−1 to x[m] i,t . To this end, each response coordinate x[m] i,t is modeled as follows: u[m],j i,t−1 := ˇx[m] j,t−1 ◦ z[m] ij (gating), u[m] i,t−1 = {u[m],1 i,t−1, · · · , u[m],p i,t−1}, and ˇu[m] i,t−1 := MLP(u[m] i,t−1); (9) x[m] i,t ∼ N \u0000µ[m] x,it, (σ[m])2 x,it \u0001 , where µ[m] x,it := Linear(ˇu[m] i,t−1), (σ[m])2 x,it = Softplus \u0000Linear(ˇu[m] i,t−1) \u0001 . (10) Note that in the gating operation in (9), we use ˇx[m] j,t−1 to denote the output after some potential numerical embedding step (e.g., Gorishniy et al. (2022)) of x[m] j,t−1; in the absence of such embedding, ˇx[m] j,t−1 ≡ x[m] j,t−1. 4In our experiments, we use an identity function and it has been effective across the settings considered. 9 Through the gating step5, x[m] j,t−1 exerts its impact on x[m] i,t entirely through u[m],j i,t−1. The continuous case and the binary case z[m] ij can be treated in a unified manner: in the former case, the value of z[m] ij corresponds to the strength; in the latter case, it performs masking. Subsequently, u[m] i,t−1 collects the u[m],j i,t−1’s of all nodes j = 1, · · · , p, and serves as the predictor for x[m] i,t . Finally, if one simply sums all u[m],j i,t−1’s to obtain the mean of x[m] i,t , then it effectively coincides with the operation in a linear VAR system, with z[m] ij corresponding precisely to the entries in the transition matrix. Remark 3. The above-mentioned choice of gz→x can be viewed as a “node-centric” one, wherein entries z[m] ij control the information passing directly through the nodes. As an alternative, one can consider an “edge- centric” one, which leverages the idea of message-passing in GNNs and entails “node2edge” and “edge2node” operations. This resembles the technology adopted in Kipf et al. (2018); L¨owe et al. (2022) that consider primarily having graph entries corresponding to categorical edge types, which, after some adaptation, can be used to handle the numerical case. In practice, we observe that the edge-centric graph2trajectory decoder can lead to instability for time series signals6. A more detailed comparison can be found in Appendix A.2, where additional illustrations are provided for the two. 3.2.3 Sampling Given the stochastic nature of the sampled quantities, drawing samples from the encoded/decoded distribu- tions requires special handling to enable the gradient to back propagate. Depending on whether entries of z[m] are continuous or binary, there are three possible types of distributions involved; for notational simplic- ity, here we use z to represent generically the random variable under consideration. • Normal z ∼ N(µ, σ2). In this case, the “standard” reparameterization trick (Kingma and Welling, 2014) can be used, that is, z = µ + σ ◦ ϵ, ϵ ∼ N(0, 1). • Bernoulli z ∼ Ber(δ). In this case, the discrete distribution is approximated by its continuous relaxation (Maddison et al., 2017). Concretely, z = softmax((log(π) + ϵ)/τ) where ϵ ∈ R2 whose coordinates are i.i.d. samples from Gumbel(0, 1), π = (1 − δ, δ) is the binary class probability and τ is the temperature. • Beta z ∼ Beta(α, β). In this case, implicit reparameterization of the gradients (Figurnov et al., 2018) is leveraged and the construction of the reparameterized samples becomes much more involved. We refer interested readers to Figurnov et al. (2018); Jankowiak and Obermeyer (2018) for an in-depth discussion on how parameterized random variables can be obtained and become differentiable. 3.2.4 Loss function The loss function is given by the negative ELBO, that is,7 −Eqϕ(Z|X) \u0010 log pθ(X|Z) \u0011 + KL \u0010 qϕ(Z|X) \r\r pθ(Z) \u0011 =: reconstruction error + KL; the first term corresponds to the reconstruction error that measures the deviation between the original trajectories and the reconstructed ones, while the KL term measures the “consistency” between the encoded and the decoded distributions, and can be viewed as a type of regularization. Let µ[m] x,t := (µ[m] x,1t, · · · , µ[m] x,pt)⊤ and Σx[m] t := diag((σ[m])2 x,1t, · · · , (σ[m])2 x,pt)⊤ with the components defined in (10). The reconstruction error is the negative Gaussian log-likelihood loss given by M X m=1 \u0010 T X t=2 \u0000x[m] t − µ[m] x,t \u0001⊤Σ−1 x[m] t \u0000x[m] t − µ[m] x,t \u0001 + log |Σx[m] t | \u0011 . (11) 5Note that z[m] ij is a scalar and is applied to all coordinates of ˇx[m] j,t−1 in the case the latter is a vector. 6to contrast with the physical system (e.g., Springs) considered in the experiments of Kipf et al. (2018). 7Recall that X:={x[m]; m = 1, · · · , M} and Z := {¯z, z[m]; m = 1, · · · , M}. 10 The KL term can be simplified after some algebra to (see Appendix A.3 calculation): Eqϕ(Z|X) h KL \u0010 qϕ(¯z|{z[m]}) \r\r pθ(¯z) \u0011i + Eqϕ(Z|X) h KL \u0010 qϕ({z[m]}|{x[m]}) \r\r pθ({z[m]}|¯z) \u0011i ; (12) both terms can be viewed as “consistency matching” terms that measure the divergence between the distribu- tions obtained in the encoder pass and that from the decoder pass. Finally, note that in the implementation, the quantities involved are replaced by their conjugacy adjusted counterparts wherever applicable, and this is similar to the treatment in Sønderby et al. (2016). 3.3 Training and inference The functions in the encoder (fx→h, fh→z and fz→¯z) and those in the decoder (g¯z→z and gz→x) are shared across all entities m = 1, · · · , M, and thus the model is trained based on the “pooled” data of all entities, while keeping track of the entity id that each data block is associated with. The steps involved in the end-to- end training under the proposed framework are summarized in Exhibit 1. Exhibit 1: Outline of steps for training under the two-layer VAE-based framework Input: observed trajectories {x[1], · · · , x[M]}, hyperparameters. Let ⟨M⟩ := {1, · · · , M}. – Forward pass, encoder: {x[m]} → {z[m]} → ¯z 0. [Traj2Graph] m ∈ ⟨M⟩: obtain the encoded distribution for entity-specific graphs qϕ(z[m]|x[m]); 1. m ∈ ⟨M⟩: sample z[m] from qϕ(z[m]|x[m]); 2. [Entity2Common] based on {z[m]}M m=1, obtain the encoded distribution for the common graph qϕ(¯z|{z[m]}); – Forward pass, decoder: ¯z → {z[m]} → {xm} 3. merge prior info pθ(¯z) into qϕ(¯z|{z[m]}) then sample ¯z; 4. [Common2Entity] m ∈ ⟨M⟩: obtain the decoded distribution for entity-specific graphs pθ(z[m]|¯z); 5. m ∈ ⟨M⟩: merge entity-specific encoded info qϕ(z[m]|x[m]) into pθ(z[m]|¯z), then sample (z[m]| ·); 6. [Graph2Traj] m ∈ ⟨M⟩: using z[m] and the lag info x[m] t−1, decode to get ˆx[m] t ; t = 2, · · · , T. – Loss calculation 7. calculate the ELBO loss by summing up (11) and (12); – Backward pass: update neural network parameters based on gradients (back-propagation) Output: Trained encoder and decoder Several pertinent remarks follow. (1) The data typically consist of “long” trajectories that contain all the available observations (time points); one needs to partition them to “short” ones of length T (that are typi- cally between 20-50), which constitute the samples used in model training. See Appendix A.5 for additional illustration. (2) In the case where one has external information regarding presence or absence of edges in the z[m]’s, it can be incorporated by enforcing the corresponding entries to zero after the former are sampled in Step 5. (3) Once the encoder (inference model) and the decoder (generative model) are trained, the latent graphs can be obtained by applying the trained encoder on the trajectories. For entity-specific graphs z[m]’s, the inference model gives the encoded distribution qϕ(z[m]|x[m])’s. In practice, the graph of interest is extracted by calculating the “mode” of the distribution; the grand common graph ¯z can be analogously handled. It is worth noting that for continuous z[m]’s, the proposed framework naturally provides signed es- timates and thus positive/negative Granger causal connections can be readily differentiated (see Appendix E for a detailed discussion). (4) The trained decoder can be utilized to quantify also the predictive strength of the Granger-causal connection, as discussed in Appendix A.4. 4 Synthetic Data Experiments We evaluate the performance of the proposed framework, together with benchmarking methods on several synthetic data settings. For all experiments, we start from a common graph that corresponds to ¯z, add 11 perturbations to it for individual entities to produce heterogeneous Granger-causal connections (i.e., the z[m]’s), then simulate trajectories {x[m]} corresponding to each entity based on their respective z[m]’s and the specified dynamics. The estimated entity-specific and grand common graphs are then evaluated against the underlying truth, for both the proposed and competing methods. Prediction model-based competitors8 include NGC (Tank et al., 2021), GVAR (Marcinkeviˇcs and Vogt, 2021) and TCDF (Nauta et al., 2019), and a regularized linear VAR model based estimator (Linear; e.g., Basu and Michailidis (2015)). For generative model-based ones, we consider variations of L¨owe et al. (2022). Note that the original paper and the accompanying code implementation only handles the case where each entry in the latent graph is a categorical variable denoting the “edge type”. Consequently, we adapt the method and make necessary modifications to the code, so that it can handle numerical values9. Besides using the edge-centric graph2trajectory decoder adopted in Kipf et al. (2018); L¨owe et al. (2022), we also consider another variant based on the proposed node-centric one. These two benchmarks are referred to as One-edge and One-node. Note that none of the above-mentioned methods readily handles the multi-entity setting where all graphs are estimated jointly; hence, for comparison purposes, the estimated grand common graph for the competitors is simply obtained by averaging the estimated entity ones. 4.1 Data generating mechanisms The data generating mechanisms used are based on: (1) a linear VAR, (2) a non-linear VAR, and (3) multi- species Lotka-Volterra systems. Two additional mechanisms corresponding to the Lorenz96 and the Springs systems are also considered; their description and results are presented in Appendix B. Consistent with extant notation, p denotes the number of nodes and M the number of entities. Linear VAR. The dynamics of a linear VAR(1) model are determined by xt = Axt−1 +εt, xt ∈ Rp, wherein A ∈ Rp×p is the transition matrix and coincides with the Granger-causal graph; for notational convenience, let ¯A := ¯z denote the grand common and A[m] := z[m] the entity-specific graphs. For this mechanism, we set p = 30 and M = 20, while the noise term εt has i.i.d entries drawn from a standard Gaussian distribution. We first discuss the generation of the “initial” common graph ¯A(0), whose skeleton S ¯ A(0) (i.e., support set) is determined by independent draws from Ber(0.1); nonzero entries are first drawn from Unif(−2, −1) ∪ (1, 2), then scaled so that the spectral radius (i.e., the maximum in absolute value eigenvalue) of ¯A(0) is 0.5. Next, we generate perturbations of ¯A(0) by “relocating” 10% of the entries (denote their index set by Sptrb) in S ¯ A(0) to random locations in the non-support set Sc ¯ A(0). This step generates the corresponding A[m]’s. Note that the perturbation mechanism ensures that Sptrb ⊂ S ¯ A(0). Further, the positions of the 10% of entries selected at random remain fixed for all M entities, and only the “new” locations are randomly selected and hence differ across the entities, thus inducing heterogeneity across the A[m]’s. As a result of the perturbation, for ¯A(0), entries in Sptrb are essentially “flipped” to zero, and this gives rise to the final grand common graph ¯A; see also Figure 3a. Non-Linear VAR. For this mechanism, we set p = 20 and M = 10. We first describe how ¯z and z[m] are generated, as they dictate the connections and determine how the dynamics are specified. First, let ¯z(0) be the “initial” common graph, set to a banded matrix that has non-zero entries on the diagonal and the adjacent upper and lower diagonals. Next, we perturb ¯z(0) as follows: for all rows not divisible by 3 (e.g., rows, 1, 2, 4, etc.), the two off-diagonal entries are relocated to other positions at random within the same row. This is repeated for all m’s to generate z[m]’s. The perturbation creates a zigzag pattern for the final ¯z, since whenever a perturbation is present, the original off-diagonal entries on the ±1 band are guaranteed to get flipped to zero – see Figure 3b for an illustration. Within any entity m, response nodes indexed by 8The selection of these competitors is based on the results reported in Marcinkeviˇcs and Vogt (2021). Specifically, we picked the ones that were demonstrated to be competitive. The code implementations for these competitors (except for the regularized Linear VAR) are directly taken from the repositories accompanying the papers. 9see Appendix A.2 for how the adaptation can be conducted. 12 i = 2, · · · , p − 1 have 3 parents; denote their indices by k1 i < k2 i < k3 i with subscript i corresponding to the response node id and superscript the parent id, and k2 i ≡ i by construction. The trajectories are generated as follows. For i = 2, · · · , p − 1, let xi,t = 0.25xi,t−1 + sin(xk1 i ,t−1 · xk3 i ,t−1) + cos(xk1 i ,t−1 + xk3 i ,t−1) + εi,t, εi,t ∼ N(0, 0.25). For the first node i and the last node p, their dynamics are slightly different given that they only have one “neighbor”10. The choice of such dynamics (in particular, using sine/cosine functions) is somewhat ad-hoc, but aim to induce non-linearities, while ensuring that the system is stable given that these functions are uniformly bounded. Finally, note that we omit the superscript [m] that indexes the entities, as the dynamic specification applies to the dynamical systems of all entities; the parent set for each response node i of entity m is dictated by row i of z[m]. Multi-species Lotka-Volterra system. It comprises of coupled ordinary different equations (ODE) that model the population dynamics of multiple predators and preys based on their interactions, specified by the corresponding Granger causal graphs. We consider p = 20 and M = 10. The p nodes are sep- arated equally into preys and predators (i.e., p 2 preys and predators each). Let xt := (u⊤ t , v⊤ t )⊤ with ut := (u1,t, · · · , up/2,t)⊤ ∈ Rp/2 and vt := (v1,t, · · · , vp/2,t)⊤ ∈ Rp/2 denoting the population size of the preys and the predators at time t, respectively; ui := {ui,t} corresponds to the continuous-time trajectory for the ith coordinate and vj is analogously defined. The dynamics for each coordinate are specified through the following ODE system: dui dt = αui − βui( X j∈Pi vj) − α(ui/η)2; dvj dt = δvj( X i∈Pj ui) − γvj. (13) The parameters are set to α = 1.1, β = 0.2, γ = 1.1, δ = 0.2 and η = 200. Once again, we omit superscript [m] as this specification applies to all m = 1, · · · , M. The heterogeneity at the entity level is contingent on their graphs z[m]’s that dictate the coupling mechanism; in particular, Pi and Pj are the parent set of nodes i and j, and are respectively dictated by the support set of the ith and jth rows of the corresponding z[m]. The generation mechanism of ¯z and z[m] are described next. The common graph ¯z is generated identically to the one considered in Marcinkeviˇcs and Vogt (2021), where the 20 nodes can be separated into 5 decoupled systems, each containing 2 predators and 2 preys. We add random perturbations to ¯z to arrive at the z[m]’s, by adding additional entries. These additional entries in the upper right/lower left blocks need to be symmetric w.r.t. the diagonal so that the predator-prey correspondence is respected, and they also provide coupling across the originally decoupled 5 × 4 systems – see also Figure 3c for an illustration. 4.2 Performance evaluation For all settings, we consider sample sizes of 10K. We run 5 data replicates and report the mean and standard deviation of the AUROC and AUPRC metrics for the competing methods considered. Given that the under- lying true Granger-causal graphs in the examined settings are sparse, we also report the best attainable F1 score for each method after thresholding the entries of the group and entity-specific graphs. Results for two other experimental settings, —the Lorenz96 and the Springs systems—, are presented in Appendix B.1. Additional metrics such as true positive rate (TPR), true negative rate (TNR) and accuracy (ACC) based on different thresholding levels are deferred to Appendix B.2, together with visual illustrations of the estimates obtained by good performing competitors. Table 1 displays the results for all methods. The proposed framework is referred to as Multi-node and Multi- edge, corresponding to the multi-entity joint learning approaches using the node- and edge-centric decoders, respectively; a visualization of the estimated ¯z and z[1], z[2] for illustration purposes is provided in Figure 3 for the former. The main findings are as follows: (1) the proposed joint-learning approach clearly outperforms its individual learning counterpart (e.g., Multi-node vs. One-node), both at the entity level and the group level (i.e., 10For i = 1, the dynamics is given by x1,t = 0.4x1,t−1 − 0.5x2,t−1 + ε1,t; for i = p, the dynamics is given by xp,t = 0.4xp,t−1 − 0.5xp−1,t−1 + εp,t 13 (a) Linear VAR; p = 30, M = 20. (b) Non-linear VAR; p = 20, M = 10. Note that as the non-linearity is induced via sinusoidal functions, we do not know the true sign of the cross lead-lag dependency; as such, the entries corresponding to edges that are present are colored in black. (c) Multi-species Lotka-Volterra; p = 20, M = 10. Figure 3: True (shaded panel on the left) and estimated (non-shaded panel on the right) Granger-causal connections using the proposed framework with node-centric decoder (Multi-node); from left to right: ¯z, z[1] and z[2] and their estimated counterparts.Nonzero entries in z[1], z[2] (and bz[1], bz[2], resp.) that overlap with those in ¯z (b¯z) have been grayed-out so that the idiosyncratic ones stand out. Table 1: Performance evaluation for the estimated ¯z and z[m]’s: “common” corresponds to ¯z and “entity(avg)” the z[m]’s after averaging the performance metric across m = 1, · · · , M. Numbers are in % and rounded to integers, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parenthesis. Generative model-based Prediction model-based Multi-node Multi-edge One-node One-edge NGC-cMLP GVAR TCDF Linear Linear VAR common AUROC 100(0.0) 100(0.0) 95(6.6) 98(4.8) 100(0.4) 100(0.0) 79(2.0) 100(0.0) AUPRC 100(0.0) 100(0.0) 83(20.4) 91(15.9) 99(1.3) 100(0.0) 50(7.6) 100(0.0) F1(best) 100(0.0) 100(0.0) 81(17.4) 88(15.9) 96(3.5) 100(0.0) 52(5.1) 100(0.0) entity AUROC 100(0.1) 99(0.6) 100(0.1) 100(0.1) 96(1.8) 100(0.0) 77(1.4) 100(0.0) (avg) AUPRC 99(0.3) 95(2.4) 99(0.2) 98(0.4) 86(4.4) 99(0.1) 36(5.5) 100(0.0) F1(best) 97(0.8) 90(3.5) 96(0.6) 95(1.0) 79(4.7) 99(0.4) 44(3.4) 100(0.0) Non-linear VAR common AUROC 99(0.2) 82(1.7) 97(0.2) 93(0.8) 90(0.7) 99(0.1) 75(1.0) 99(0.1) AUPRC 96(0.9) 58(1.1) 80(0.8) 80(8.0) 64(1.1) 98(0.2) 53(0.5) 98(0.1) F1(best) 94(0.6) 60(0.7) 74(1.0) 83(6.9) 61(0.9) 98(0.7) 56(1.2) 98(0.7) entity AUROC 98(0.3) 85(0.9) 94(0.4) 95(0.5) 94(0.5) 99(0.3) 73(0.9) 96(0.7) (avg) AUPRC 93(1.0) 75(0.8) 76(0.2) 89(0.6) 87(0.6) 96(0.6) 44(1.8) 96(0.7) F1(best) 86(1.5) 73(1.0) 70(0.3) 86(0.8) 82(0.4) 91(0.8) 50(1.5) 97(0.6) Lotka-Volterra common AUROC 100(0.0) 100(0.0) 97(1.1) 87(8.4) 100(0.0) 100(0.0) 79(0.8) 100(0.1) AUPRC 100(0.0) 100(0.1) 92(3.0) 73(10.5) 100(0.0) 100(0.0) 58(1.2) 100(0.4) F1(best) 100(0.7) 99(0.8) 87(5.4) 69(9.0) 100(0.4) 97(1.2) 53(1.4) 94(3.5) entity AUROC 89(1.0) 84(1.3) 83(1.6) 75(1.3) 92(1.0) 93(0.6) 72(0.8) 77(1.0) (avg) AUPRC 80(1.5) 70(2.0) 69(1.8) 51(2.6) 87(1.2) 89(1.0) 41(1.0) 71(1.2) F1(best) 74(1.4) 65(2.0) 63(1.4) 53(2.2) 84(0.8) 84(0.7) 46(0.3) 71(0.7) 14 the common graph). (2) The node-centric decoder consistently outperforms its edge-centric counterpart (e.g., Multi-node vs. Multi-edge). (3) If one focuses only on individual learning methods, the ones based on prediction models tend to exhibit superior performance (e.g., GVAR/NGC vs. One-node). In addition, despite the presence of non-linear dynamics, the regularized linear VAR model exhibits surprisingly good performance, especially for the common structure. (4) For practical purposes, post-hoc averaging of the entity-specific Granger causal graphs is reasonably effective for extracting the common structure. Remark 4 (On the robustness with respect to sample size). The proposed joint-learning framework is ade- quately robust to sample sizes. In particular, in the case where the training sample size reduces to 3000, Multi-node shows little degradation in its performance in recovering ¯z (within 1% across all settings in AUROC), and its performance degradation in recovering the entity-level z[m]’s are within 2% for the same metric. On the other hand, One-node shows a material deterioration in performance especially for the es- timated ¯z (as large as 5% for more challenging settings such as the Lotka-Volterra system), although at the individual entity level, the deterioration is of a smaller magnitude at around 3%. In Appendix B.4 additional comments on the “minimum sample size required” are provided from a practitioner’s perspective. Finally, we remark that GVAR exhibits consistently strong performance amongst the methods under consider- ation. On the other hand, it is observed during evaluation time that given the magnitude of the estimated entries, the quality of the graph skeleton is sensitive to the exact choice of the thresholding level, whereas the proposed framework is more robust. This has implications on the difficulty of choosing a good threshold in practice — see also Table 4 and additional discussion and remarks in Appendix B.2. 5 Application to a Multi-Subject EEG Dataset The dataset in consideration corresponds to electroencephalogram (EEG) measurements obtained from 72 active electrodes placed on the scalp of 22 subjects (entities), and they are publicly available; see Trujillo et al. (2017). Prior investigation on this dataset primarily centers around understanding the information provided by different connectivity measures that are available in the literature, rather than the connectivity patterns themselves. The EEG experiment pertains to a stimulus procedure performed on the subjects comprising of 1-min in- terleaved sessions with eyes open (EO) or closed (EC). Such experiments aim to provide insights into the brain’s functional segregation and integration (Barry et al., 2007; Rubinov and Sporns, 2010; Miraglia et al., 2016). Note that (1) the experiment is integrated, but the data are collated separately for the eyes-open and the eyes-closed interleaving sessions, which results in two datasets (EO and EC, respectively); and (2) due to the design of the experiment, the dynamics governing the data within the EO sessions (respectively, EC sessions) are stable and stay largely unchanged. We select to analyze the data from 31 specific EEG channels (and hence p = 31) located at the back of the scalp (see Figure 4), where the primary visual cortex is located. For both datasets, we restrict the analysis to entities that have at least 40000 observations (total number of time points)11, and the whole trajectory is further partitioned into training/validation data, with the latter having 2000 time points. Here the validation data is used to select the best hyperparameters such that the reconstruction or prediction error is minimum over the search grid, depending on the method. Four methods are considered, including the proposed joint- learning one with a node-centric decoder (Multi-node), its individual-learning counterpart (One-node) and prediction model-based GVAR and NGC. The estimated common Granger-causal connections based on Multi-node and GVAR are depicted in Figures 4 and 5, respectively. The results based on One-node and NCG are delegated to Appendix D12. For all methods, we threshold the raw estimates to remove very small entries; the thresholding values are chosen so that 11This restriction has reduced the number of entities to 21 for the EO dataset while the number of entities for the EC dataset remains at 22. 12Note that the Granger causal connections estimated by Multi-node and One-node are up to a “complete sign flip” (see, e.g., discussion in Appendix E); nonetheless, these methods are effective in distinguishing positive (negative) connections from negative (positive) ones. Further, NCG does not provide signed estimates (positive/negative) of the Granger causal connections, unlike the other three methods. 15 (a) Eyes Open (EO) (b) Eyes Closed (EC) Figure 4: Multi-node results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent thresholding at 0.15. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is propor- tional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in gray. (a) Eyes Open (EO) (b) Eyes Closed (EC) Figure 5: GVAR results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent thresh- olding at 0.05. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is proportional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in gray. each method has around 400 total number of edges for the EC session to facilitate comparisons across them. Results from these methods exhibit commonalities and differences, as discussed next. The following common observations are noted across most methods: (1) based on the results by Multi-node, GVAR and One-node, the overall Granger causal connectivity level is markedly higher for the EC session compared to the EO one; this is consistent with results in studies in the literature (Barry et al., 2007; Marx et al., 2004; Das et al., 2016; Trujillo et al., 2017), albeit using different connectivity measures. On the other hand, results from NCG show the reverse pattern, i.e., higher connectivity for the EO session compared to the EC session. (2) For both the EO and EC sessions, the in-degree of nodes in the mid-line channels (i.e. OZ, POZ, PZ, CPZ) tends to be higher than that of the nodes to the left and right parts of the brain. This is broadly comparable to results in the literature—see, e.g., Barry et al. (2007) for adult subjects and Barry et al. (2009) for children, though the problem under consideration and thus the analysis is different in their work. (3) As it is observed in Multi-node, One-Node, and NGC, the OZ channel exhibits different degree of connectivity for the EO and the EC sessions; in particular, it is Granger causal for many other channels in the former, i.e., being the emitter of edges and exhibit higher node out-degree; this becomes significantly less so in the latter—see also Hatton et al. (2023). The four methods also exhibit certain discrepancies in their results. (1) As mentioned above, the EO session exhibits an overall decrease in connectivity when compared against the EC session. The drop in connectivity, however, is not uniform across nodes on the left and the right parts of the brain. This is reported by generative model-based methods Multi-node and One-node, and a result also mentioned in the literature (Barry et al., 2007, 2009; Modarres et al., 2023). Such discrepancy—in terms of the differential change in connectivity level between the nodes on the left and those on the right—is significantly less pronounced in GVAR and NGC. (2) A strong bi-directional Granger causal link between channels M1 and M2 in both the EO and EC sessions is observed according to GVAR. This strong connection is somewhat harder to interpret, since these two 16 channels correspond to mastoid (behind the ears) locations and their connectivity is customarily modulated through the midline positioned ones (OZ, PZ, POZ, CPZ) (Das et al., 2022). (3) As a minor remark, for GVAR, we observe strong autoregressive connections (i.e., dominant diagonals in the estimates)13; for NGC, the overall connectivity level in the raw estimates is significantly higher and thus requires stronger thresholding. Both observations are also noted in selected synthetic data experiments. In summary, all methods with the exception of NGC are in agreement regarding the decrease in Granger causal connections from the EC to the EO session. There is also concordance across methods regarding the observation that this decrease is not uniform across the left and right parts of the brain. Both of these results are in accordance to previous ones in the literature, although based on different analysis techniques and connectivity measures. 6 Discussion This paper proposes a multi-layer VAE-based framework for jointly estimating the group and entity-level Granger-causal graphs, in the presence of connectivity heterogeneity across entities. The framework is based on a hierarchical generative structure that couples the group and entity-specific graphs. The model is learned via an end-to-end encoding-decoding procedure that minimizes the negative ELBO loss. The results of the numerical experiments show that the performance of the proposed framework is broadly robust to sample size, especially for the common graph. Further, the joint learning paradigm has a clear advantage over its “individual learning” generative model-based counterpart, which then leads to more accurate quantification for both the common connectivity patterns and the idiosyncratic ones. This advantage becomes more pro- nounced in settings where one has limited sample size and large collections of related systems. In addition, the joint learning paradigm can be useful in situations, where one may be interested in detecting “outlier” dynamical systems in the collection under consideration, or in identifying clusters of such systems. These tasks can be accomplished by close examination and analysis of the entity specific graphs. Although “prediction models plus post-hoc aggregation” heuristics can sometimes exhibit competitive per- formance, the embedded common structure across entities is completely neglected at the formulation level. In addition, existing models within this framework are also limited to scalar-valued nodes, partly due to their reliance on performing ad-hoc extraction/aggregation on intermediate quantities (e.g., neural network weight matrices during training) to infer the Granger causality. In the presence of non-linearity, a key advantage of generative model-based approaches is that the Granger- causal relationships are solely encoded through the latent graph that serves as the gateway for information propagation. This provides a clean way to model relationships between connectivity patterns — either statically or dynamically. The setting considered in this work is a static one, and the type of such relationship manifests as common-idiosyncratic connectivity patterns. A potential extension to the generative process under consideration, suitable for more complex real-world dynamical systems, is to allow for time-varying connectivity patterns. For example, Graber and Schwing (2020) extends the work in Kipf et al. (2018) to a dynamic setting. With appropriate modifications to the proposed approach, such as expanding the conditional relationship of the graphs dictated in (2) so that they also depend on their past, this modeling task can be handled in a straightforward manner. 13this is not shown in the plot (to avoid self-loops) for aesthetic purposes. Note also that visually, the edges are overall more “faint” in the plot, as a result of the dominant diagonals and the corresponding normalization. 17 A Additional Modeling and Implementation Details In this section, we provide a description for some additional modeling details. In Sections A.1 and A.2, we omit superscript [m] that indexes the entities whenever there is no ambiguity, as the descriptions therein apply to all m’s independently unless otherwise specified. A.1 Encoder We provide details for the encoder sub-module that is abstracted as fx→h, wherein based on the node tra- jectories, one obtains the hidden representations for the edges {hij} := fx→h(x); see also Section 3.2, module (enc-a). As the most basic building blocks of message-passing operations, “node2edge” and “edge2node” operate based off a complete graph, and can be generically represented as: eij ← concat(xi, xj) (node2edge); xi ← X j eij (edge2node), with xi denoting the node representation and eij the edge one. fx→h is then parameterized through the L passes of such operations: (init emb) : ˇx(0) i ← emb(xi), ∀i = 1, · · · , p ˇx → e : e(l) ij ← MLP \u0000node2edge(ˇx(l−1) i , ˇx(l−1) j ) \u0001 ; l = 1, · · · , L e → ˇx : ˇx(l−1) i ← MLP \u0000edge2node(e(l) ij ; j = 1, · · · , p) \u0001 ; l = 2, · · · , L Here xi corresponds to the trajectory of node i over time, that is, xi = (xi,1, · · · , xi,T ) and the final hidden representation is given by hij := e(L) ij , i, j = 1, · · · , p. Concretely, the embedding module can be as simple as entailing only Linear-ReLU type operations; the input trajectory xi of a node i, ∀i ∈ {1, · · · , p}, is processed via the following steps outlined in Figure 6: xi Linear ReLU Dropout Linear ˇx(0) i (flatten) embedding module Figure 6: Example for the embedding operation in the encoder according to MLP style. Blocks with trainable parameters are outlined in red. Note that the flattening step is only required when the nodes are vector-valued. Note that this also coincides with the LR-type embedding functions in Gorishniy et al. (2022). In regards to the MLP block, it is obtained by stacking the sub-blocks as illustrated in Figure 7. input Linear ReLU Dropout Linear ReLU Dropout Linear output MLP sub-block MLP sub-block Figure 7: An MLP block obtained by stacking the sub-blocks. Constituent blocks with trainable parameters are outlined in red. Figure 8 provides a pictorial illustration for the sequential operations entailed in the Trajectory2Graph en- coder module.14 Note that this is effectively the MLPEncoder used in Kipf et al. (2018) and the description is given here for the sake of completeness. We refer interested readers to Kipf et al. (2018) for some other encoders considered therein. 14In our experiments, all the MLP blocks used in the Trajectory2Graph operations are kept simple with only one single sub-block; the hidden dimension is set at 128 or 256, depending on the exact experiments. 18 x1 x2 xp−1 xp ˇx(0) 1 ˇx(0) 2 ˇx(0) p−1 ˇx(0) p h11 h12 · · · h1p h21 h22 · · · h2p ... ... ... ... hp1 hp2 · · · hpp z11 z12 · · · z1p z21 z22 · · · z2p ... ... ... ... zp1 zp2 · · · zpp node2edge GNN-MLP1 edge2node GNN-MLP2 node2edge GNN-MLP3 Embedding MLP GNN-style message passing Figure 8: Diagram for the Trajectory2Graph encoder operations. Blocks with trainable parameters are outlined in red. A.2 Decoder We divide this subsection into two parts, that respectively (1) discuss how the structure adopted in a node- centric Graph2trajectory module described in Section 3.2 can readily accommodate the presence of depen- dence on more than 1 lags; and (2) provide a brief discussion on how the original edge-centric decoder adopted in Kipf et al. (2018); L¨owe et al. (2022) can be revised to adapt to the case of a numerical graph, and compare it with the node-centric one, although architectural choices are not the focus of this paper. Extension to multiple lag dependency. The extension of a node-centric decoder to accommodate the presence of more than 1 lags (i.e., q > 1) is straightforward, largely due to the fact that the node value at time t − 1, denoted by xj,t−1 is not limited to be scalar-valued in the first place. In the case of q-lag dependency, one can simply replace xj,t−1 by concat(xj,t−1, · · · , xj,t−q) and proceed with the remainder of the operations as outlined in (9) and (10). In particular, with the presence of more lags, as an alternative to a (optional) numerical embedding step, one can instead consider 1D-CNN as a preprocessing module on the “new” xj,t−1, before an element-wise gate represented by zij is applied to control the information flow. Adaptation of the edge-centric decoder. The original edge-centric decoder adopted in Kipf et al. (2018) handles the case where each entry in zij corresponds to an edge type (categorical), and it entails the follow- ing operations: 1. node2edge for each time step, that is eij,t−1 := concat(xi,t−1, xj,t−1) to arrive at the edge representa- tion at time t − 1; 2. for each edge type of interest, run eij,t−1’s through its corresponding edge type-specific function (e.g., MLP) to get the “enriched” representation ˇeij,t−1; 3. aggregate the enriched edge representations back to nodes via an edge2node operation, giving rise to vi,t−1’s, i = 1, · · · , p; vi,t−1 then serves as the predictor for time-t response xi,t. In order for the above module to accommodate the case of a numeric zij, the following simple modification to step 2 is introduced: 2’ run eij,t−1’s through some function (e.g., MLP) to get the “enriched” representation ˇeij,t−1, and further update it through a gating mechanism as dictated by zij, that is, ˇeij,t−1 ← ˇeij,t−1 ◦ zij. The information propagation path from node j to i can be represented as: xj,t−1 node2edge → eij,t−1 MLP → ˇeij,t−1 gating → ˇeij,t−1 ◦ zij edge2node → vi,t−1 → xi,t; (14) one can easily verify that for zij = 0, there is no path from xj,t−1 to xi,t. As a final remark, for the node-centric decoder, the gating through zij directly operates on the node repre- sentation, and the path is given by xj,t−1 emb → ˇxj,t−1 gating → ˇxj,t−1 ◦ zij element of → ui,t−1 → xi,t; 19 see also Figure 9 for a pictorial illustration. x1 x2 xp−1 xp ˇx1 ˇx2 ˇxp−1 ˇxp zi1 zi2 zi(p−1) zip u1 i u2 i up−1 i up i MLP xi ◦ ◦ ◦ ◦ past time ≤ (t − 1) time t numerical embedding Figure 9: Diagram for the node-centric Graph2Trajectory Decoder operations, with node i being the response node in this illustration. The corresponding entries of z’s (shaded in gray, obtained by sampling and is fed into the Graph2Trajectory Decoder as input) perform the gating operation (denoted by ◦). Blocks with trainable parameters are outlined in red and are shared across all response nodes i = 1, · · · , p. To contrast, for the edge-centric decoder, as indicated in (14), entries in zij determine the lead-lag informa- tion passing from j → i via eij,t−1, and therefore such a gating mechanism is somewhat circumstantial. A.3 Loss calculation A derivation of (12) is given next. KL \u0010 qϕ(Z|X) \r\r pθ(Z) \u0011 = Eqϕ(Z|X) log hqϕ(Z|X) pθ(Z) i = Eqϕ(Z|X) h log qϕ(¯z|{z[m]}) pθ(¯z) + log qϕ({z[m]}|{x[m]}) pθ({z[m]}|¯z) i = ZZ qϕ \u0000¯z|{z[m]} \u0001 qϕ \u0000{z[m]}|{x[m]} \u0001 log hqϕ \u0000¯z|{z[m]} \u0001 pθ \u0000¯z \u0001 i d¯zd{z[m]} + ZZ qϕ \u0000¯z|{z[m]} \u0001 qϕ \u0000{z[m]}|{x[m]} \u0001 log hqϕ \u0000{z[m]}|{x[m]} \u0001 pθ \u0000{z[m]}|¯z \u0001 i d¯zd{z[m]} = Z qϕ({z[m]}|{x[m]}) n Z qϕ \u0000¯z|{z[m]} \u0001 log hqϕ \u0000¯z|{z[m]} \u0001 pθ \u0000¯z \u0001 i d¯z o | {z } KL \u0010 qϕ(¯z|{z[m]}) \r\r pθ(¯z) \u0011 d{z[m]} + Z qϕ \u0000¯z|{z[m]}, {x[m]} \u0001 n Z qϕ \u0000{z[m]}|{x[m]} \u0001 log hqϕ \u0000{z[m]}|{x[m]} \u0001 pθ \u0000{z[m]}|¯z \u0001 i d{z[m]} o | {z } KL \u0010 qϕ({z[m]}|{x[m]}) \r\r pθ({z[m]}|¯z) \u0011 d¯z (a) = Eqϕ({z[m]}|X) h KL \u0010 qϕ(¯z|{z[m]}) \r\r pθ(¯z) \u0011i + Eqϕ(¯z|X) h KL \u0010 qϕ({z[m]}|{x[m]}) \r\r pθ({z[m]}|¯z) \u0011i . For (a), the first term is straightforward, the second term goes through since Z p(x|y, z) n Z p(y|z) log p(y|z) q(y|x)dy o dx = ZZ p(y|z)p(x|y, z) log p(y|z) q(y|x)dxdy = EY |ZEX|Z,Y log p(y|z) q(y|x) = EY |ZEX|Z log p(y|z) q(y|x) = EX|Z h EY |Z log p(y|z) q(y|x) i ; and the last equality holds as a result of the Fubini-Tonelli theorem. A.4 Evaluating the predictive strength of Granger causal relationships Next, we briefly discuss how the trained decoder can be used to measure the predictive strength of the Granger causal connections. 20 Once the model is trained, using the inference procedure described in Section 3.3, one obtains estimates ˆz[m] for all entity-specific graphs. Further, a trained Graph2Trajectory module, abstracted as ˆgz→x, also becomes available. The predictive strength of any connection entry (i, j) — corresponding to the lead-lag relationship from j to i — can then be assessed by nullifying the corresponding entry. Throughout the remainder of the discussion, we omit superscript [m] for ease of presentation, as the procedure is applicable to an arbitrary entity of interest. Let ˜z(ij) be identical to ˆz except that the (i, j) entry is set to zero (nullified). The reconstructed trajectories, based on the estimated and the nullified graphs are given by ˆx = ˆgz→x(ˆz, x1)15 and ˜x(ij) = ˆgz→x(˜z(ij), x1), respectively. The predictive strength can then be evaluated based on the difference in the residual-sum- of-squares (RSS), with the latter obtained by evaluating the reconstructed trajectory against the observed values. Concretely, RSS(ˆx) can be obtained by 1 T −1 PT t=2 ∥xt − ˆxt∥2 and that for ˜x(ij) can be analogously obtained; the predictive strength of the (i, j) connection can then be calculated as RSS(ˆx) − RSS(˜x(ij)). This procedure can be generalized to a set of connections, where instead of nullifying a single entry, multiple entries are nullified simultaneously and the remainder of the evaluation follows. Note that the proposed procedure resembles that of testing for the presence/absence of Granger causality in linear VAR models, where an F-test is used (Geweke, 1984). The calculated difference RSS(ˆx) − RSS(˜x(ij)) also appears in the numerator of the aforementioned F-statistic. A.5 Construction of samples We briefly explain how samples are constructed from observed data trajectories. We omit the superscript [m] that corresponds to the entity ID, since the construction is generally applicable. The available data can either correspond to a collection of long trajectories (e.g., traditional time series set- ting where observations for different variables are collected over time), or to multiple collections of (long) trajectories, where each collection corresponds to temporal observations over time from repeated measure- ments (e.g., in the context of a neurophysiological experiment, a subject is exposed to a stimulus (eyes open or eyes close) a number of times). In both cases, the trajectories are parsed into shorter ones of length T, which is the context window considered in the modeling. Concretely, let {x0, x1, · · · , x e T } be the long trajec- tory, with eT denoting the total number of observations. The samples, indexed by n, are shorter trajectories of length T, with each consisting of observations X (n) := {xsn, xsn+1, · · · , xsn+T −1}, where s is the stride size that dictates the overlapping between samples with adjacent indices. A long trajectory of length eT gives rise to ⌊( eT − T)/s + 1⌋ samples, which are then used during mini-batch training. B Additional Synthetic Data Experiments and Results B.1 Lorenz96 and Springs5 experiments To explore the applicability of the proposed framework to selected special cases, there are two other settings considered in our synthetic data experiments: the Lorenz96 and the Springs5 systems. Unlike the settings presented in the numerical experiments in Section 4 wherein the entity-level heterogeneity manifests itself primarily in the form of perturbations to the skeleton of the shared common graph, for these two systems, the entity-specific skeletons are either identical across all M entities and only the magnitude of the entries changes (Lorenz96), or they manifest their heterogeneity through a probabilistic mechanism (Springs), as explained in the sequel. Similar to those presented earlier, for both settings, we run the experiments on 5 data replicates and re- port the metrics after averaging across the 5 runs, with their respective standard deviation included in the parentheses. 15Recall that throughout the main sections, we use x := {x1, · · · , xT } to denote the trajectory; here ˆx is effectively its “reconstructed” couterpart. 21 B.1.1 The Lorenz96 system The Lorenz96 system (Lorenz, 1996) has been previously investigated in Tank et al. (2021); Marcinkeviˇcs and Vogt (2021). The dynamics for a p-variable system evolve according to the following ODE: dxi dt = (xi+1 − xi−2)xi−1 − xi + F, i = 1, · · · , p, (15) Figure 10: Lorenz96: ¯z and z[0], showing only the skeleton. where xi := {xi,t} denotes the continuous time trajectory of node i with x0 := xp, x−1 := xp−1 and xp+1 := x1. Such a sys- tem corresponds to a Granger-causal structure shown in Fig- ure 10 that depicts its skeleton. The representation in (15) can be obtained from Kerin and Engler (2022): dxi dt = α(xi+1 − xi−2)xi−1 − βxi + γ, (16) by reparameterizing α = β, λ = α/β and setting F = αγ/β2. F is the forcing constant that controls the degree of non-linearity; in particular, given the relationship between (15) and (16), as F varies, the strength of the Granger-causality changes despite an invariant skeleton. In other words, to induce heterogeneity across entities, we can only change the parameter F that induces heterogeneity in the magnitudes of the Granger causal connections, while the skeleton of the Granger causal graph remains the same. We consider a setting with p = 20 and M = 5 entities, with the forces taking the following values: F ∈ {10.0, 17.5, 25.0, 32.5, 40.0}. Table 2: Performance evaluation for the estimated ¯z and z[m]’s for setting Lorenz96. Numbers are in % and rounded to integers, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses. Generative model-based Prediction model-based Multi-node Multi-edge One-node One-edge NGC-cMLP GVAR TCDF Linear common AUROC 100(0.1) 100(0.7) 100(0.1) 90(19.7) 97(0.0) 100(0.1) 82(0.9) 99(0.1) AUPRC 100(0.4) 99(1.6) 100(0.3) 82(32.5) 87(0.1) 100(0.2) 65(0.9) 97(0.5) F1(best) 97(1.5) 96(3.4) 97(1.3) 80(25.7) 87(0.8) 98(1.0) 59(1.3) 89(0.2) entity AUROC 95(1.3) 85(3.7) 96(1.0) 88(1.9) 96(0.1) 97(0.8) 79(0.8) 99(0.1) (avg) AUPRC 89(2.3) 76(4.6) 91(2.0) 78(2.9) 85(0.3) 90(1.5) 62(0.7) 96(0.3) F1(best) 82(3.2) 71(3.5) 84(2.6) 72(3.1) 83(0.4) 83(0.2) 58(0.5) 88(0.3) The results are shown in Table 2 and the main findings are: (1) consistent with the results in Section 4, the node-centric decoder outperforms the edge-centric one; (2) the proposed joint-learning approach Multi-node matches the performance of GVAR and outperforms all other competitors for the common graph; (3) for the entity-specific graphs, interestingly, the linear VAR exhibits a slight edge over all competing methods, while the performance of the proposed model is broadly on-par with the remaining competitors. Figure 11: Estimated ¯z and z[m]’s with different F ’s using the proposed joint-learning framework with a node-centric decoder (Multi-node). Finally, the common and the five entity-specific Granger causal graphs for the Multi-node method are de- picted in Figure 11. It can be seen that the performance deteriorates for systems with larger external force F. 22 B.1.2 Springs5 system Figure 12: Springs5: ¯z and z[0]. z[0] is binary (and sym- metric) with entries generated according to Bernoulli distri- butions. This setting is investigated in Kipf et al. (2018); L¨owe et al. (2022), and in this work we consider a “multi-entity” version of it. In the original setting, particles (i.e., nodes) are connected (pairwise) by springs at random with probability 0.5; in the case where the connection between particles i and j is present, they interact according to Hooke’s law Fij = −k(ri−rj), where Fij is the force applied to particle i by particle j, k is the spring constant and ri is the location vector of particle i in 2- dimensional space. With some initial location and velocity, the trajectories can be simulated by solving Newton’s equations of motion (see also Kipf et al. (2018), Appendix B for details). Crucially, (1) the Granger-causal graph is essentially a realiza- tion of the homogeneous Erd˝os-R´enyi graph (Erd˝os and R´enyi, 1959) with edge probability being 0.5, and (2) each node’s trajectory is multivariate with 4 dimensions, that is, xi,t ∈ Rd, d = 4; the first 2 dimensions correspond to the velocity and the last 2 to the location in the 2-dimensional space. The extension to the “multi-entity” case that is suitable for the setup considered in this paper is described next, and it differs primarily from the original one in how the Granger-causal connections across nodes are generated. Specifically, we start from ¯z, whose entries (i, j) in its upper-triangular part are generated independently from Beta(1, 1); then set ¯zji ≡ ¯zij, i < j so that it’s symmetric. For the z[m]’s, let z[m] ij ∼ Ber(¯zij), i < j, and then set z[m] ji ≡ z[m] ij , ∀ m = 1, · · · , M. Once z[m]’s are generated, they dictate the connections between nodes in their respective systems, and one can proceed with the same procedure as in the original setting to simulate the trajectories. Note that (1) each entity’s Granger-causal graph corresponds to a realization of a heterogeneous Erd˝os-R´enyi graph; the edge probability differs across node pairs and depends on the corresponding entry in ¯z that is a realization from the Beta distribution, and (2) the grand common structure possesses a “probabilistic” interpretation, in that it effectively captures the expectation of an edge being present/absent across all entities. In this experiment, we set p = 5 and M = 10. None of the competitors based on the prediction models can readily handle this setting16, and therefore we only present results for those based on generative models. Note that in this experiment, despite that the underlying true graphs are symmetric, we do not use this information during our estimation. Table 3 shows the results for the above-mentioned systems, using both the node- and the edge-centric de- coders. A visualization of the estimates is provided in Figure 13. Overall, the proposed joint learning framework outperforms individual learning for entity-level graphs, while the performance is largely compa- rable for the common graph estimate. Given the physics system nature of this dataset (vis-a-vis time series signals), the edge-centric decoder has a small advantage over the node-centric one; this is manifested by the fact that under the joint learning framework, the two decoders show comparable performance, whereas the edge-centric decoder is clearly superior in the case of single-entity separate learning. Note that this points to another potential advantage of the joint-learning framework, in that it is more robust and exhibits less volatility than individual learning. Table 3: Performance evaluation for the estimated ¯z (error in Frobenius norm) and z[m]’s (accuracy and F1 score after thresholding at 0.5, averaged across all entities) for the Springs5 system. quantity metric Multi-node Multi-edge One-node One-edge common ERR-fnorm 1.00(0.259) 0.92(0.294) 1.30(0.412) 0.79(0.217) entity(avg) ACC% 99.3(0.84) 99.3(0.76) 87.5(6.45) 96.3(3.99) entity(avg) F1Score% 99.5(0.79) 99.4(0.73) 88.2(7.45) 96.3(4.78) 16There are two issues that the prediction model-based competitors can not readily handle and would require major changes: (1) all of them assume that the Granger-causality to be estimated is numeric and therefore does not naturally handle the binary case, and (2) at any point in time, each node is assumed to have a scalar value, akin to classical time-series settings, whereas here each node is vector valued; consequently, the existing code does not readily handle it. 23 Figure 13: Estimated ¯z and z[m]’s (showing the first five) using the proposed framework with node-centric decoder (Multi-node). B.2 Additional performance evaluation results and their visualization Table 4 presents additional evaluation metrics (TPR, TNR and ACC) for the proposed method and its strong competitors, after the estimates of the Granger causal graphs are thresholded at various levels no greater than 0.5 (after normalization). We only show the results for the estimated common graph ¯z, since the results for the entity-level ones exhibit similar patterns. As briefly mentioned in Section 4, prediction model-based methods (NGC/GVAR) are more sensitive to the value of the threshold, manifested by a sudden jump in accuracy once the threshold exceeds a certain level. On the other hand, the change in ACC for the ones based on generative models is more gradual. Given that in practice it is common to use a moderate threshold to eliminate small entries of the initial estimates of the Granger causal graphs to determine their skeleton, the above-mentioned susceptibility can adversely impact the quality of the final estimate used for interpretation purposes and in downstream analytical tasks. Table 4: Performance evaluation for the support set of the estimated common graph ¯z at various threshold levels (left-most column). Numbers are in %, and correspond to the mean results based on 5 data replicates. Multi-node One-node NGC-cMLP GVAR Linear TPR TNR ACC TPR TNR ACC TPR TNR ACC TPR TNR ACC TPR TNR ACC Linear VAR 0.10 100 92.1 92.9 98.1 50.3 55.1 100 0.0 10.0 100 0.0 10.0 100 99.9 99.9 0.20 100 99.9 99.9 95.8 78.9 80.6 100 0.0 10.0 100 0.0 10.0 100 100 100 0.30 100 100 100 91.2 90.9 91.0 100 0.0 10.0 100 2.9 12.7 100 100 100 0.40 99.6 100 100 81.8 96.0 94.6 100 48.9 54.2 100 57.4 61.9 100 100 100 0.50 92.7 100 99.3 67.6 98.5 95.4 79.4 99.9 97.9 96.9 100 99.7 98.7 100 99.9 Non-linear VAR 0.10 100 74.2 76.7 100 59.3 63.1 100 0.0 9.5 100 0.0 9.5 99.5 57.1 61.1 0.20 98.4 89.2 90.0 100 82.9 84.5 100 0.0 9.5 100 0.0 9.5 97.4 99.8 99.5 0.30 94.7 91.7 92.0 96.3 89.3 90.0 100 0.0 9.5 100 85.4 86.8 92.1 100 99.2 0.40 89.5 99.4 98.5 72.1 91.8 89.9 99.5 47.9 52.8 71.1 100 97.2 68.9 100 97.0 0.50 73.2 100 97.5 60.5 95.6 92.2 47.4 95.7 91.2 61.1 100 96.3 60.5 100 96.2 Lotka-Volterra 0.05 100 72.8 76.8 99.0 40.5 49.3 100 58.4 64.7 34.0 100 90.1 33.3 100 90.0 0.10 100 97.4 97.8 96.3 73.9 77.3 99.7 100 100 33.3 100 90.0 33.3 100 90.0 0.15 99.3 99.8 99.8 90.0 92.4 92.0 90.7 100 98.6 33.3 100 90.0 33.3 100 90.0 0.30 67.0 100 95.0 50.3 100 92.5 33.7 100 90.0 33.3 100 90.0 33.3 100 90.0 0.50 33.3 100 90.0 33.3 100 90.0 33.3 100 90.0 33.3 100 90.0 33.3 100 90.0 Lorenz96 0.05 95.2 99.5 98.7 93.8 100 98.8 100 0.0 20.0 100 99.8 99.8 95.8 94.1 94.5 0.10 58.8 100 91.8 39.5 100 87.9 100 0.0 20.0 96.8 100 97.0 50.0 100 90.0 0.15 27.2 100 85.5 25.0 100 85.0 100 0.0 20.0 72.8 100 94.5 25.0 100 85.0 0.30 25.0 100 85.0 25.0 100 85.0 100 79.2 83.4 25.0 100 85.0 25.0 100 85.0 0.50 25.0 100 85.0 25.0 100 85.0 93.0 93.4 93.3 25.0 100 85.0 25.0 100 85.0 An illustration of the recovered Granger-causal connections (after “optimal” thresholding) is shown in Fig- ure 14. Note that NGC can only produce the “unsigned” version of the connections and hence all its estimates are shown as positive, whereas for other methods, the entries are “signed” with red denoting the positive and blue the negative ones. One interesting observation is that for the Lotka-Volterra system, all methods have incorrectly estimated the signs of the diagonals, in that the underlying true dependencies on their own lags are positive for the preys 24 (a) Linear VAR: estimated ¯z (or transition matrix ¯ A, equivalently) (b) Non-linear VAR: estimated ¯z (c) Lotka-Volterra. Top panel: estimated ¯z; bottom panel: estimated ¯z after suppressing the diagonals (d) Lorenz96: estimated ¯z Figure 14: Estimated ¯z (after normalization) for various methods. The displayed f1score corresponds to the best attainable one (after thresholding) for each method. Red:(+); blue:(−). Note that NGC does not produce signed estimates and hence all its estimates are shown in red, with the shades corresponding to the magnitude of the entries after normalization. and negative for the predators, whereas all methods fail to identify such discrepancy — although for the prediction model-based ones all dependencies show as positive and generative model-based ones have the opposite sign. This could be partially driven by the fact that during trajectory generation, the Runge–Kutta method (specifically, RK4) has been used and thus it renders the presence of a self-lag linear term with coefficient 1 in the recursion; in addition, a small noise term has also been injected. For this setting, given that the estimated diagonals have dominating magnitude for GVAR and Linear, we also provide a visual display of the estimates with the diagonals suppressed. Remark 5. A dichotomous behavior is observed between the unsigned and the signed estimates obtained from the code implementation of GVAR17, with the former typically being 5-10% better (in absolute values, for reported metrics such as AUC, ACC that are between 0-100%). In all the tables, we have reported the performance of the superior one (unsigned), whereas Figure 14 is produced based on the signed estimate to show the positive/negative recovery. The best attainable F1 scores after thresholding (corresponding to the result of the specific data replicate being displayed) for these signed estimates are labeled in the title of 17Repository for GVAR: https://github.com/i6092467/GVAR 25 the figures; e.g., 0.75 for the non-linear VAR setting, 0.95 and 0.86 for the Lotka-Volterra and the Lorenz96 setting, respectively. B.3 The impact of the degree of heterogeneity To evaluate the robustness and potential susceptibility of the proposed framework to the level of heterogene- ity present across entities, we conduct additional experiments based on the Linear VAR and Non-linear VAR settings described in Section 4.1. To recap, the following dynamics are considered for each individual system of p nodes, xt = (x1,t, · · · , xp,t)⊤ ∈ Rp: • Linear VAR: xt = Axt−1 + εt. The Granger-causal graph z coincides with A. • Non-linear VAR: each response coordinate 2 ≤ i ≤ (p − 1) depends on the lag of its own that of two other coordinates indexed by k1 i and k3 i , that is, xi,t = 0.25xk2 i ,t−1 +sin(xk1 i ,t−1 ·xk3 i ,t−1)+cos(xk1 i ,t−1 + xk3 i ,t−1) + εi,t, with k1 i < k2 i ≡ i < k3 i . The dynamics for the first and the last coordinates depend only on their respective adjacent coordinate, i.e., for i = 1, x1,t = 0.4x1,t−1 − 0.5x2,t−1 + ε1,t; for i = p, xp,t = 0.4xp,t−1 −0.5xp−1,t−1 +εp,t. The Granger-causal graph z dictates the exact locations of the k1 i ’s and k3 i ’s. For both settings, the Granger-causal graphs z[m]’s of the M entities are obtained by a “perturbation” with respect to the initial common Granger-causal graph ¯z(0) (or ¯A(0) equivalently, in a linear setting), and the magnitude of such perturbation determines the degree of heterogeneity across entities and the final common graph ¯z. The perturbation logic resembles the one described in Section 4.1. Specifically, for the linear VAR setting, we let the skeleton of ¯A(0) have 30% density, that is, the support set S ¯ A(0) is determined by independent draws from Ber(0.3), and the magnitude of the perturbation is controlled by the percentage of “relocated” entries. For the Non-Linear VAR setting, the magnitude of the perturbation is controlled by the number of rows whose off-diagonal entries are kept unchanged from those in the initial common Granger causal graph.18 The sub-settings (S1-S5 with increasing degree of heterogeneity, respec- tively for linear and non-linear VAR setups) are depicted in Figures 15 and 16, where the percentage of relocation and the unchanged entries, respectively, are given in the sub-captions. Note that under the non- linear VAR setup, S1 and S5 correspond to the two extreme cases: no entity-level heterogeneity and almost fully heterogeneous. We focus on generative model-based methods with a node-centric decoder, i.e., Multi-node (proposed framework) and One-node, and evaluate the performance of the estimates, obtained by training the model on different sample sizes. For the linear VAR setting, the sample size is set to 200 and 1000, while for the non-linear VAR setting to 500 and 2000. The selection of these sample sizes was based on the following three considerations: (1) non-linear dynamics are typically more challenging to learn and thus require larger networks and more samples to train; and (2) instead of choosing a “large” sample size where both methods perform well and thus little differentiation is shown, additional insights can be gained by assessing the per- formance of the model in settings where the available sample size is getting close to the information-theoretic limit (at the conceptual level). Tables 5 and 6 display the results of the Linear/Non-linear VAR settings based on the same set of metrics as in Section 4.2, and they correspond to the average of 3 data replicates with the standard deviations displayed in parentheses. Major observations are: (1) for Multi-node, the estimation of ¯z is reasonably robust to the varying degree of heterogeneity across sub-settings. In particular, little deterioration is observed across sub- settings, although for sub-setting S5, given the very few common entries, the presented metrics become not not particularly meaningful. (2) Regarding the quality of individual entity estimates, Multi-node exhibits some deterioration in the non-linear setting when the model is getting close to being mis-specified (S5 versus S1-S4). (3) In the settings under consideration, where the sample size starts becoming rather small, Multi-node starts exhibiting an advantage over One-node by a wide margin. Specifically, for the estimated ¯z, One-node shows performance degradation as the level of heterogeneity increases across sub-settings S1 18Recall, in the original settings presented in Section 4.1, for the linear VAR setting, the percentage of “relocated” entries is 10%; for the non-linear VAR setting, every 3rd row is left unchanged. 26 (a) relocation = 10% (b) relocation = 25% (c) relocation = 50% (d) relocation = 75% (e) relocation = 90% Figure 15: 30 × 30 Linear VAR system with a total number of M = 20 entities. Sub-settings are displayed vertically with increasing level of heterogeneity (from left to right). In the figure, only ¯z and z[1], z[2] are displayed. (a) fix everything (b) fix every other row (c) fix every third row (d) fix diagonals + corners (e) fix first + last rows Figure 16: 20 × 20 Non-linear VAR system with a total number of M = 10 entities. Sub-settings are displayed vertically with increasing level of heterogeneity (from left to right). In the figure, only ¯z and z[1], z[2] are displayed. Similar to those in Section 4, as the non-linearity is induced via sinusoidal functions, we do not know the true sign of the cross lead-lag dependency; as such, the entries corresponding to entries that are present are colored in black. to S5 (even for the linear case), and the overall performance is inferior to that of Multi-node. The latter is somewhat expected: Multi-node performs joint estimation over samples across all entities and thus borrows 27 Table 5: Performance evaluation for the estimated ¯z and z[m]’s under settings S1-S5 of Linear VAR. Numbers are in %, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses. Multi-node One-node S1 S1 S3 S4 S5 S1 S2 S3 S4 S5 Linear VAR; train size 200 common AUROC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 90(2.1) 90(0.6) 90(4.7) 83(4.3) 85(1.7) AUPRC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 85(1.4) 82(0.2) 77(8.1) 56(5.7) 44(10.1) F1(best) 100(0.3) 100(0.0) 100(0.2) 100(0.0) 100(0.0) 75(2.1) 72(1.3) 70(8.1) 53(6.1) 43(10.0) entity AUROC 94(1.4) 94(1.2) 94(1.4) 95(1.6) 95(1.6) 88(3.1) 89(2.4) 89(3.1) 88(3.4) 88(2.6) (avg) AUPRC 92(1.9) 92(1.8) 92(2.3) 92(2.3) 92(2.4) 83(4.2) 84(3.4) 84(4.6) 82(4.4) 83(3.4) F1(best) 84(2.4) 84(2.2) 84(2.9) 84(2.7) 84(2.9) 75(4.1) 76(3.0) 75(4.1) 74(4.1) 74(3.2) Linear VAR; train size 1000 common AUROC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 97(1.3) 96(0.6) 95(1.3) 91(1.2) 93(0.6) AUPRC 100(0.0) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 95(1.6) 93(0.3) 88(3.9) 72(4.1) 59(9.9) F1(best) 100(0.4) 100(0.0) 100(0.0) 100(0.0) 100(0.0) 89(1.7) 85(0.5) 80(6.5) 67(3.0) 57(8.6) entity AUROC 95(1.4) 95(1.3) 95(1.5) 95(1.7) 95(1.6) 94(1.4) 94(1.4) 94(1.5) 94(1.6) 94(1.6) (avg) AUPRC 92(2.0) 92(2.0) 92(2.4) 92(2.3) 92(2.3) 92(2.0) 92(2.1) 91(2.4) 92(2.3) 92(2.3) F1(best) 84(2.9) 84(2.4) 84(2.9) 85(2.6) 85(2.8) 84(2.7) 84(2.5) 84(3.1) 84(2.7) 84(2.9) Table 6: Performance evaluation for the estimated ¯z and z[m]’s under settings S1-S5 of Non-linear VAR. Numbers are in %, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses. Multi-node One-node S1 S1 S3 S4 S5 S1 S2 S3 S4 S5 Non-linear VAR; train size 500 common AUROC 98(0.4) 98(1.4) 96(0.4) 100(0.0) 100(0.0) 92(1.0) 84(1.3) 75(1.7) 98(0.2) 98(0.9) AUPRC 81(1.4) 84(10.9) 73(3.2) 98(0.4) 100(0.0) 69(0.1) 64(0.6) 49(9.0) 89(0.6) 46(31.5) F1(best) 84(3.3) 78(7.3) 69(1.8) 92(1.9) 100(0.0) 74(0.6) 69(0.0) 60(5.6) 90(2.1) 50(31.0) entity AUROC 97(0.1) 97(0.8) 92(0.3) 98(0.5) 77(1.0) 92(0.4) 74(1.3) 63(1.6) 71(2.9) 51(2.9) (avg) AUPRC 79(0.4) 82(6.0) 67(2.1) 88(1.9) 41(1.9) 68(0.6) 54(0.2) 39(3.4) 52(2.6) 18(0.9) F1(best) 79(0.6) 77(3.1) 68(1.1) 81(2.6) 55(1.3) 67(1.2) 53(0.3) 45(1.1) 51(1.4) 28(1.5) Non-linear VAR; train size 2000 common AUC 99(0.1) 100(0.2) 98(0.3) 100(0.0) 100(0.0) 95(0.1) 95(0.0) 95(0.4) 99(0.2) 100(0.0) AUPRC 94(0.5) 96(4.3) 84(1.9) 99(0.3) 100(0.0) 74(0.1) 75(0.1) 77(1.1) 92(1.0) 100(0.0) F1(best) 95(0.0) 96(1.7) 77(1.1) 96(1.9) 100(0.0) 77(0.0) 69(0.0) 72(2.2) 92(0.0) 100(0.0) entity AUROC 99(0.1) 99(0.2) 95(0.6) 99(0.1) 80(0.6) 95(0.3) 93(0.2) 90(0.3) 93(0.5) 73(1.1) (avg) AUPRC 92(0.4) 93(2.1) 82(0.7) 95(0.3) 53(2.9) 78(0.9) 71(0.3) 67(0.2) 71(1.1) 32(0.8) F1(best) 87(0.6) 88(0.9) 76(1.2) 89(0.9) 61(1.5) 76(0.4) 70(1.0) 63(0.4) 64(1.2) 47(0.5) information across; as such, it can rely on fewer number of samples19 to attain estimates of similar accuracy. Finally, note that in the proposed framework, at the decoder stage the Common2Entity step where the en- coder distribution is merged via weighted conjugacy adjustment, the hyper-parameter ω controls the mixing percentage between the common and the entity-specific information. Conceptually, its choice varies accord- ing to the degree of heterogeneity present across entities: in the extreme case where the common structure is de facto absent, ω = 1; the other end of the extreme corresponds to ω = 0 when there is no heterogeneity. It is worth noting that we have observed empirically that the proposed framework is not sensitive to the choice of ω, since in most cases its specific value makes little difference to the quality of the estimated ¯z and z[m]’s, as long as it was selected from a reasonable range (e.g., between [0.25, 0.75]). For example, in all the experiments above, we have fixed ω at 0.5. B.4 Some remarks on sample size We give a brief account of the performance of the proposed framework in small sample size regimes. Note that in practical settings, model performance hinges on multiple factors, such as sample size, the size of the problem—including both the number of nodes and the number of entities, given the joint learning strategy— and how complex the temporal dynamics of the underlying systems are. The goal of this section is to provide guidance on the “minimum number of samples required”—from a practitioner’s perspective—in settings of 19Here the number of samples is expressed in relative terms, that is, train size, corresponding to the number of trajectories used in training for each entity. 28 comparable size to the ones considered herein. Specifically, we focus on the same set of time series settings for systems with non-linear dynamics considered in Section 4 and Appendix B.1, namely, the Non-Linear VAR, multi-species Lotka-Volterra, and the Lorenz96. In all three settings there are 20 nodes in their respective entity-level dynamical systems, and the collection contains 5 or 10 entities. Recall that for the first setting the non-linear dynamics are induced through some sinusoidal function, while the other two settings are ODE-based systems. Table 7 presents the performance evaluation of the estimated ¯z and z[m]’s based on Multi-node, when training sample sizes are 3000, 1000 and 500, respectively. Table 7: Performance evaluation for b¯z and bz[m]’s based on Multi-node under different settings with various training sample sizes. Numbers are in %, and correspond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses. Non-linear VAR Lotka-Volterra Lorenz96 3000 1000 500 3000 1000 500 3000 1000 500 common AUROC 98(0.1) 97(0.2) 96(0.4) 100(0.0) 97(2.2) 90(8.0) 99(0.4) 95(1.3) 89(3.5) AUPRC 89(0.5) 80(1.3) 74(3.1) 100(0.2) 95(3.2) 81(7.9) 98(0.8) 92(2.0) 85(2.8) F1(best) 79(1.4) 75(1.3) 69(1.7) 100(0.4) 94(4.3) 78(4.6) 94(0.7) 87(2.3) 84(1.9) entity AUROC 96(0.5) 94(0.6) 92(0.7) 88(0.8) 81(2.1) 64(1.4) 92(1.5) 87(1.3) 85(1.3) (avg) AUPRC 86(0.4) 76(1.2) 69(2.3) 79(1.2) 66(3.2) 43(3.8) 85(2.4) 79(2.1) 76(2.8) F1(best) 78(0.3) 73(1.4) 69(1.6) 75(1.1) 64(4.0) 42(2.4) 78(2.3) 73(2.5) 70(3.6) The main observations are: (1) for the common graph ¯z, as sample size reduces from 3000 to 1000, the proposed method’s performance metrics stay above a reasonable range, even though a certain degradation is present, and its magnitude varies across settings. (2) For the entity-specific z[m]’s, the degradation in performance is more pronounced as the sample size reduces, and the model clearly suffers from not having access to an adequate number of samples.20 Based on these observations, we broadly conclude the following for practical settings of comparable size to the ones examined above: in the case where the primary focus is on the common graph ¯z, the proposed framework would likely yield reasonable recovery even with about 1000 samples. On the other hand, if indi- vidual entity-level estimates are also of interest, sample sizes below 3000 would become rather challenging for the method to exhibit a satisfactory performance. B.5 Lotka-Volterra with perturbation: some characterization We provide a characterization/justification for the “perturbed” Lotka-Volterra system, pertaining to how to validate a Lotka-Volterra system based on the “perturbed” interaction matrix being stable. The general form of p-multi-species Lotka-Volterra equations are given by dxi dt = rixi \u00001 + p X j=1 Aijxj \u0001 , (17) where ri > 0 is the inherent per-capita growth rate of species xi, i = 1, · · · , p and A ∈ Rp×p the species interaction matrix. The system considered in (13) can then be put in this canonical form, by assuming that the first p/2 species are preys and the last p/2 species predators. Specifically, for the preys the corresponding equation in the canonical form becomes dxi dt = αxi \u0014\u00001 − 1 η2 xi \u0001 − β/α X j∈Pprey i xj \u0015 , i = 1, · · · , p/2 20For these small sample size experiments, we use the same set of hyper-parameters as the ones in earlier experiments with much larger sample sizes (1e4). One can potentially expect improved performance with more carefully tuned hyper-parameters, although the improvement would likely be limited. 29 where ri = α, Aii = − 1 η2 , Aij = −β/α for all j ∈ Pprey i otherwise 0; Pprey i denotes the support set of the prey indexed by i. Analogously, for the predators the corresponding equation in the canonical form becomes dxi dt = −γxi \u00001 − δ/γ X j∈Ppredator i xj \u0001 , i = p/2 + 1, · · · , p where ri = −γ, Aii = 0, Aij = −δ/γ for all j ∈ Ppredator i otherwise 0; Ppredator i denotes the support set of the predator indexed by i. It can be seen that fixed points of the set of equations in (17) can be found by setting dxi/dt = 0 for all i, which translates to the vector equation r + Ax = 0, r ∈ Rp, x ∈ Rp, A ∈ Rp×p. Consequently, fixed points exist if A is invertible and are given by x = −A−1r. Note that xi = 0 is a trivial fixed point. Further, the fixed point may contain both positive and negative values, which implies that there is no stable attractor for which the populations of all species are positive. The eigenvalues of A determine the stability of the fixed point. By the stable manifold theorem, if its eigenvalues are less than 1, then the fixed point is stable. This can be easily verified once the “perturbed” Granger-causal matrix z’s (which determines the Pi’s and hence the corresponding A) are generated. C Granger Causality and Graphical Models, Bayesian Hierarchical Modeling, and Linear VARs This section comprises of three parts that provide background information on different topics mentioned in the main paper. Section C.1 illustrates how the framework of graphical models can be used to capture the concept of Granger causality. Section C.2 provides a brief overview of the Bayesian hierarchical modeling framework and outlines how it shares broad similarities to the modeling framework used in the paper. Finally, Section C.3 discusses possible ways of accomplishing the modeling task via a collection of linear VARs, either using a frequentist formulation, or a Bayesian hierarchical modeling one. C.1 Granger causality and graphical models Consider a dynamical system, comprising of a p-dimensional stationary time series xt := (x1,t, · · · , xp,t), with xi,t denoting the value of node i at time t. Further, let V = {x1, · · · , xp} denote the node set of the p nodes/time series of the system. A Granger causal time series graph (Dahlhaus and Eichler, 2003) has node set V = V × Z and edge set E ⊆ V × V , wherein an edge (xi, t − s) → (xj, t) ̸∈ E, if and only if s ≤ 0 or xi,t−s ⊥⊥ xj,t | Xt \\ xi,t−s, where Xt = {xt′, t′ < t} denotes the entire past process of the time series at time t, ⊥⊥ probabilistic independence and \\ the set difference operator. The above definition implies that the edge set E contains directed edges from past time points to present ones, only if xi,t−s and xj,t are dependent, conditioned on all other past nodes in V excluding xi,t−s. An aggregate Granger causal graph (Dahlhaus and Eichler, 2003) has vertex set V and edge set E, wherein an edge (xi → xj) ̸∈ E if and only if (xi, t − s) → (xj, t) ̸∈ E for all u > 0, t ∈ Z; i.e, absence of the edge (xi → xj) from the aggregate Granger causal graph implies absence of Granger causality from node (time series) xi to node xj, while presence of that edge implies that one or more time lags of node xi are Granger causal of node xj. Figure 17 illustrates pictorially both the Granger causal time series graph (Figure 17a), and the matrix representation (in the form of heatmaps) for the aggregate Granger causal graph (the right-most heatmap in Figure 17b). 30 1 2 3 4 5 1 2 3 4 5 1 2 3 4 5 time t − 2 time t − 1 time t (a) Example for a Granger causal graph with 2-lag dependency and V = {1, 2, 3, 4, 5}. For the edges in E, those originate from time t − 2 are denoted in dash and those from time t − 1 are in solid arrows, respectively. “or” t − 2 t t − 1 t past < t present t (b) Connection matrices corresponding to the graph in Figure 17a, where columns correspond to emitters (past) and rows corresponding to receivers (present). Colored cells denote the presence of a connection. The right-most matrix corresponds to the aggregate Granger-causal connection matrix that summarizes and indicates present-past dependencies. Figure 17: Pictorial illustration for Granger causal time series graph, aggregate Granger causal graph and their corresponding matrix representation. In the case of a linear VAR system of order q given by xt = Pq k=1 Akxt−k + et, the edge set of the Granger causal time series graph corresponds to E = {(Ak)ij | (Ak)ij ̸= 0, i, j ∈ V, k = 1, · · · , q}, while E = {Bij|Bij = 1(Pq k=1(abs(Ak)ij) ̸= 0), i, j ∈ V}, with 1(·) denoting the indicator function. The aggregate Granger causal graph with edge set E is an unweighted one, namely, its edges take values 0 (absence) or 1 (presence) and consequently reflect absence/presence of Granger causality between the time series. Remark 6 (On the estimated Granger-causal graph). Under the proposed framework, in the binary case, the Granger connectivity graph corresponds exactly to the aggregate Granger causal graph defined above (see Section 3 and Remark 1), which is also in the same spirit as how different edge types are modeled in Kipf et al. (2018). In the continuous case, it corresponds to a weighted version of the aggregate Granger causal graph, wherein the weights correspond to the size of the “gate” through which the information from the past flows to the present. Admittedly, in the presence of non-linear modules (such as MLP) after the gating operation in the decoder, the weights no longer correspond to the “predictive strength” as defined in the original paper by Granger (1969). Nonetheless, at the conceptual level, the weights reflect the “strength” of the underlying relationships, as measured through the “permissible information flow”. C.2 Bayesian hierarchical modeling Given the prevalent usage of hierarchical modeling in the case where observational units form a hierarchy— e.g., in our motivating example, the observed time series are at the entity level and the entities form a group—we briefly review the Bayesian hierarchical modeling framework next. Since its initial introduction in Lindley and Smith (1972) for linear models, the Bayesian hierarchical frame- work has been expanded and used for many other classes of statistical models. The book by Gelman et al. (2014) provides a description of the general framework and outlines the role of exchangeability for con- structing prior distributions for statistical models with hierarchical structure. The framework has been oper- ationalized and used for many statistical models, including regression and multilevel models (Gelman and Hill, 2006), time series (Berliner, 1996) and spatio-temporal models (Wikle et al., 1998), in causal analy- sis (Feller and Gelman, 2015), cluster analysis (Heller and Ghahramani, 2005), in nonparametric modeling (Teh and Jordan, 2010), and so forth. At the modeling level, the outline of the framework for a hierarchy comprising of two levels is as follows. Data for entities m = 1, · · · , M are generated according to some probability distribution p(x[m]; θ[m], ϕ) = p(x[m]|θ[m]) · p(θ[m]|ϕ) · p(ϕ). θ[m]’s are entity-specific parameters, and they are assumed to be generated exchangeably from a common population, whose distribution is governed by a common parameter ϕ, and can be specified as p(θ[m]|ϕ). The common parameter ϕ can be fairly complex (for an example, see Section C.3) and possesses a prior distribution p(ϕ), which depending on the nature of ϕ can be fairly involved. The prior distribution for the parameter (θ[m], ϕ) that governs the data generation mechanism for entity m jointly, can then be character- ized by p(θ[m], ϕ) = p \u0000θ[m]|ϕ \u0001 p(ϕ). Note that the above specification exhibits differences to the generative process of a multi-level VAE presented in Section 2.2. Specifically, in the VAE specification, there are observed and latent random variables, mod- 31 eled according to a probability distribution with fixed parameters θ⋆, whereas in the Bayesian hierarchical modeling formulation, the parameters of the data generating distribution are random variables themselves and respect a hierarchical specification as previously mentioned. C.3 Modeling via a collection of linear VARs We illustrate how the modeling task at hand can be handled when the dynamics are assumed linear. In particular, the dynamical systems can be characterized by a collection of linear VAR models; we show how the common structure can be modeled by decomposing the transition matrix or using hierarchical modeling, respectively in a frequentist and a Bayesian setting. For ease of exposition, in the sequel, we assume the collection of linear VAR models have lag of order 1, and they are given by x[m] t = A[m]x[m] t−1 + εt, m = 1, · · · , M. Frequentist formulation. Suppose that the transition matrices can be decomposed as A[m] = A0 + B[m], i.e., into a common component A0 and an entity-specific one B[m]. For model identifiability purposes, an “orthogonality” constraint is imposed; for example, in the form of A0B[m] = 0 ∈ Rp×p. In settings where the transition matrices A[m] are additionally assumed sparse (see, e.g., the numerical experiments in Section 4), such a constraint is typically in the form of support(A0) ∩ support(B[m]) = ∅, namely that the matrices A0 and B[m] do not share non-zero entries. Bayesian hierarchical modeling formulation. We consider a collection of linear VAR models as above. The probability distribution of the data is p \u0000{x[m] t }M m=1|{A[m]}M m=1, ϕ \u0001 , where ϕ is a vector of additional parameters specified next. To construct the prior distribution of the model parameters ({A[m]}, ϕ) we proceed as follows. Note that at the modeling level, a simple hierarchy is defined for each (i, j)-th element of the transition matrix across all M entities/models; i.e., we consider p2 such hierarchies independently. To use the Bayesian hierarchical modeling framework, let ⃗cij = (A[m] ij , · · · , A[M] ij )′ be an M-dimensional vector containing the (i, j)-th element of all M transition matrices. The following distributions are imposed on ⃗cij’s and the parameters associated with their priors: ⃗cij | (Ψ, τij) ∼ N(0, τijΨ); (18) τij ∼ Gamma(M + 1/2, λij), Ψ ∼ Inverse Wishart(S0, γ0). The prior distributions in (18) are independent over index (i, j); τij is an (i, j)-element specific scaling factor, and Ψ an M × M matrix that captures similarities between the M models. The parameters λij, S0, γ0 can be either fixed to some pre-specified values (e.g., a fixed S0 can reflect prior knowledge on the similarity between the M models), or equipped with diffuse prior distributions. Further, note that if Ψ ≡ I the identity matrix, then the above specification reduces to the Bayesian group lasso of Kyung et al. (2010). Based on the above exposition, it can be seen that ϕ := (Ψ, {τ}ij, i, j = 1, · · · , p). In summary, we have the following two-level modeling specification: at the first level, we have the data distribution, while at the second level the distribution on the elements of the transition matrices that are “coupled” across the M models through ϕ and its prior distribution specification. Obviously, more complicated prior specifications can be imposed, for example by “coupling” whole rows of the transition matrices A[m] across the entities. D Additional Results for the EEG Dataset The estimated common Granger-causal connections based on One-node and NGC are depicted in Figures 18 and 19, respectively. The increase in the overall Granger causal connectivity in the EC session compared to that in the EO session observed for Multi-node and GVAR is also present in the results from One-node, 32 (a) Eyes Open (EO) (b) Eyes Closed (EC) Figure 18: One-node results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent thresholding at 0.50. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is propor- tional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in gray. (a) Eyes Open (EO) (b) Eyes Closed (EC) Figure 19: NGC results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent thresholding at 0.45. All edges are colored gray, since NGC does not provide signed estimates of Granger causal connections. The transparency of the edges is proportional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in gray. whereas it is reversed in the results of the NCG. Further, the observed increase in the overall connectivity pattern between the EO session compared to the EC session, exhibits differences between the left and right parts of the brain, something also observed in the results of Multi-node. Further, note that NCG does not produce signed estimates and hence all Granger causal connections are colored grey in Figure 19. This limitation of the method can hinder scientific insights that could be obtained from the analysis of a dataset by NGC. E On Respecting the Sign Distinction of the Connections This section provides some explanation to how the proposed methodology (Multi-node)—modulo estimation error that can introduce inaccuracies—recovers the sign of the underlying truth up to a complete sign flip, that is, SIGN(ˆz) = (±)SIGN(z); (19) with SIGN(·) operating in an entry-wise fashion on z or ˆz. In (19), z generically refers either to the grand- common Granger-causal graph ¯z or entity specific ones z[m], and ˆz is the corresponding estimate. This is equivalent to saying that there is no guarantee that for each individual entry, sign(zij) = sign(bzij) always holds; however, all positive (negative) signed connections are identified as having the same sign. In this regard, the signs of the estimates obtained from the procedure can be interpreted in a meaningful way, in that the positive/negative connections can be differentiated; see Figure 20 for an illustration. As a result of (19), the following also readily holds for any two entries indexed by (i1, j1) and (i2, j2), 33 ✓ complete sign flip ✓ no sign flip est, no sign flip truth est, complete sign flip Figure 20: Pictorial illustration for the concept of “up to complete sign flip”. In both the no-sign-flip and the complete-sign-flip case, the estimate always “groups” the positive/negative connections together in a way that is in accordance with the truth. ∀ i1, j1, i2, j2 ∈ {1, · · · , p}: sign(zi1j1)sign(zi2j2) = sign(bzi1j1)sign(bzi2j2); i.e., if two connections have the same/opposite signs in z, they continue having the same/opposite signs in ˆz. We shall refer to this property as “respecting the sign distinction”. The goal of this section is to provide some intuition on how the above mentioned is enabled through the encoder-decoder learning—in particular, in the presence of non-linear modules. Note that the subsequent arguments do not constitute a formal end-to-end proof. In the sequel, we focus on the single-entity case and ignore modules related to the coupling between entity- level graphs and their grand-common counterpart, as these modules are not pertinent to this specific discus- sion. Concretely, the relevant modules in the ensuing discussion are: • q(z[m]|x[m]) as captured by (enc-a) and (enc-b) combined; i.e., the “Trajectory2Graph” encoder. • p(x[m]|z[m]) as captured by (dec-b); i.e., the “Graph2Trajectory” decoder. The superscript [m] will be omitted henceforth. Outline of the argument. The argument consists of two parts: 1. The decoder, by utilizing a shared MLP across all response coordinates, ensures that the sign distinction is respected across the rows, along any column. Specifically, see, e.g., equations (9) and (10), wherein the MLP and the subsequent operations (in particular, their parameters) are shared by all response coordinates i’s. 2. The encoder, in the case of supervised training, disallows any partial (row or column) sign flip. (1) and (2) jointly ensure that ˆz respects the sign of z up to a complete sign flip, and this is operationalized via the end-to-end training where the parameters are jointly learned and the data likelihood maximized. At the high level, the shared MLP mechanism in the decoder ensures that it will not generate estimates that show “row sign flip” relative to the underlying truth. Specifically, for any fixed column, if one looks at the estimates along the columns (i.e., vertically) and across the rows, the estimates would respect their sign distinction in a pairwise fashion. However, it does not preclude cases where along the rows (i.e., horizontally) and across the columns, signs in the estimates can be flipped (i.e., column sign flip). On the row sign flip × truth est (a) Example for row sign-flip: signs of the 2nd row is flipped (right versus. left) column sign flip ? truth est (b) Example for column sign-flips: signs of the 1st, 3rd and 4th columns are flipped (right versus. left) Figure 21: Pictorial illustration for the concept of “row sign flip” and “column sign flip”, picking the first two rows from Figure 20. Note that the former is prohibited by the shared MLP mechanism in the decoder construction. other hand, to generate estimates that recover the sign of the underlying truth up to a complete sign flip, both row and column sign flips need to be precluded. The latter is facilitated by the encoder module during the end-to-end training: when the decoder fixes the “sign-orientation” vertically across the rows, the encoder would favor estimates that do not exhibit any partial sign flip during learning. 34 The details for each component are given next. E.1 Decoder Claim: By using a shared MLP across all response coordinates, for any fixed column j ∈ {1, · · · , p}, the decoder respects the sign distinction across the rows of z, that is, sign(bzi1j)sign(bzi2j) ≡ sign(zi1j)sign(zi2j), ∀i1, i2 ∈ {1, · · · , p}. (20) The same cannot be guaranteed, however, if different MLPs are used for different response coordinates. For illustration purposes, we focus on the case where the feature dimension is 1 (i.e., classical time series setting). Consider a simple two-layer MLP whose hidden layer has h neurons. Let fMLP : Rp 7→ R be represented as fMLP(u) = W (2)σ \u0000W (1)u + b(1)\u0001 + b(2), u ∈ Rp; W (1) ∈ Rh×p, b(1) ∈ Rh×1, W (2) ∈ R1×h, b(2) ∈ R; σ(·) is some activation function. Specifically in the Graph2Trajectory decoder, the function input of the MLP is in the form of ui,t−1, whose jth coordinate is given by xj,t−1 ◦ zij, assuming the absence of any numerical embedding (see, e.g., expressions in (9) with superscript [m] dropped). To further simplify notation, we ignore subscript t−1, and let y = (y1, · · · , yp) ∈ Rp denote the time-t target. Effectively, at decoding time, an approximation of the following form is considered for all timestamps: yi ≈ fMLP(ui), ∀ i = 1, · · · , p = W (2)σ       W (1) 11 W (1) 12 · · · W (1) 1p ... ... ... ... W (1) h1 W (1) h2 · · · W (1) hp     x1 ◦ zi1 ... xp ◦ zip   + b(1)     + b(2), (21) where x1, · · · , xp are inputs directly available through training data, (zi1, · · · , zip)′ constitutes the ith row of matrix z. Crucially, fMLP is shared across all i’s. In the actual end-to-end learning, zij’s are sampled from a distribution whose parameters are dictated by the encoding step. The parameters of the encoders are jointly learned with those of the decoders, by minimizing the reconstruction error and the KL term. Here to further delineate the issue pertaining specifically to whether with the use of a shared MLP, the learned zij’s can respect the sign distinction, we ignore the encoding step, and simplifies the question as follows: Can the learning procedure—by minimizing the prediction error based on (21)— that jointly learns the W’s, b’s and entries of z’s give rise to learned bzij’s, such that the bzij’s respect the sign distinction? The answer is affirmative for any fixed column j = 1, · · · , p. To see this, expand the matrix product in (21), which gives (here we ignore approximation error and assume the model is well-specified): yi = h X s=1 W (2) s σ \u0010 p X j=1 \u0000W (1) sj ◦ zij \u0001 xj + b(1)\u0011 + b(2). The predicted byi is given by byi = h X s=1 c W (2) s σ \u0010 p X j=1 \u0000c W (1) sj ◦ bzij \u0001 xj + bb(1)\u0011 + bb(2), where c W (1), c W (2), bb(1) and bb(2) are estimated weights and bias terms. By minimizing the prediction error, byi is close to yi, for any values of x1, x2, · · · , xp and for all i’s. This amounts to having the estimated coefficients in front of the xj’s sufficiently close to the truth—in particular, modulo estimation error, the following holds: W (1) sj zij = c W (1) sj bzij, for all i = 1, · · · , p. (22) 35 This further gives (W (1) sj )2zi2jzi2j = (c W (1) sj )2bzi1jbzi2j, ∀ i1, i2 ∈ {1, · · · , p}, (23) and therefore (20) follows since (c W (1) sj )2 > 0. Note that in the case where different MLPs are used for different response coordinates, (23) becomes (W (i1,1) sj W (i2,1) sj )zi2jzi2j = (c W (i1,1) sj c W (i2,1) sj )bzi1jbzi2j, which no longer leads to (20). Toy data experiments. To verify this empirically, we consider a toy data example, where the trajectories are generated according to a 2-dimensional linear VAR system, that is, xt = Axt−1 + et, where A = \u0014 0.5 −0.25 −0.25 0.5 \u0015 ; (24) coordinates of et are drawn i.i.d. from N(0, 0.5). Note that given the linear setup, the transition matrix corresponds precisely to the true Granger-causal graph, and therefore z ≡ A. We run end-to-end training based on two configurations of the decoder: (a) a single MLP shared across all response coordinates; (b) separate MLPs for different response coordinates. In both configurations, the MLPs are 2-layer ones with a hidden layer of dimension 64. The experiment is run over a single data replicate but repeated using 10 independent seeds. (a) Normalized truth (b) Estimates based on Configuration (a) - a shared MLP (c) Estimates based on Configuration (b) - separate MLPs Figure 22: Toy data experiment decoder results: heatmaps for z (truth, normalized) and ˆz (estimates, normalized) under the shared and separate-MLP configurations. Panel (a) corresponds to z after normalization; panel (b) correspond to normalized ˆz (from runs with different seeds) obtained under Configuration (a); panel (c) correspond to normalized ˆz obtained under Configuration (b). Figure 22 displays the estimated z corresponding to 3 different seeds for each configuration. Amongst all 10 runs, Configuration(a) preserves the sign distinction at all times—in this particular case, diagonals in ˆz always have the same sign and anti-diagonals have the opposite. Note that results from run seed 324 (left-most figure in Figure 22b) correspond to the case where the estimate yields a complete sign flip of the underlying truth. For Configuration (b), it fails in 2 out of the 10 runs—showing 2 failures (seed 324 and 644) and 1 success (seed 764) in Figure 22c, as the estimates can fail to preserve the sign distinction amongst the edges. E.2 Encoder Claim: the encoder is able to perform “effective” learning based on labels up to a complete sign flip, but learning becomes problematic when the labels entail any partial sign flip. Similar to the case of the decoder, to delineate the issue pertaining to the encoder, instead of considering end-to-end training where the two models are jointly learned, we consider a simplified setting, where we use the encoder module for a supervised learning task, based on data whose true generating mechanism is associated with the Granger causal graph z. The question posed is the following: The true trajectories are generated based on z. For a supervised learning task where the training labels are provided and the learning is enabled by the encoder module, is the encoder able to perform “effective” learning, 36 1. when the label used during training is some partial (column or row) sign flip of z? 2. when the label used during training is a complete sign flip of z, namely −z? This is explored via synthetic data experiments, where the data generating mechanism is identical to the one considered in Section E.1. Concretely, let z♯ denote the quantity that is provided as the target (label) during the supervised training; note that the data is generated according to (24), with z ≡ A = \u0002 0.5 −0.25 −0.25 0.5 \u0003 , irrespective of the labels provided. The following four training scenarios are considered: (a) No sign flip: z♯ = \u0002 0.5 −0.25 −0.25 0.5 \u0003 , that is, z♯ = z; (b) Complete sign flip: z♯ = \u0002 −0.5 0.25 0.25 −0.5 \u0003 , that is, z♯ = −z; (c) Column sign flip: z♯ = \u0002 0.5 0.25 −0.25 −0.5 \u0003 , that is, z♯ :,1 = z:,1, z♯ :,2 = −z:,2; (d) Row sign flip: z♯ = \u0002 0.5 −0.25 0.25 −0.5 \u0003 , that is, z♯ 1,: = z1,:, z♯ 2,: = −z2,:. We run encoder-only training for the above four scenarios. Results21 are displayed in Figure 23, with the estimated z displayed in the top panel and the label z♯ used for supervision during training displayed in the bottom panel. (a) Scenario (a) (b) Scenario (b) - complete flip (c) Scenario (c) - column flip (d) Scenario (d) - row flip Figure 23: Toy data experiment encoder-only results: heatmaps for ˆz (estimates, top panel) and and z♯ (training label, bottom panel) for scenarios (a) to (d) respectively. Note that the underlying ground truth (i.e., the z that governs the dynamics of the trajectories) for all these experiments are identical to the one in Scenario (a). As the results show, the encoder learns almost perfectly (relative to the provided labels) in scenarios (a) and (b), despite the latter being a complete sign flip. On the other hand, it struggles to learn in the case of partial sign flips (i.e., Scenarios (c) and (d)), as manifested by the essentially-zero estimated values. This empirically corroborates our claim. Finally, it is worth noting that the claim examined in this subsection is under the supervised learning setup, namely, it establishes the fact that the encoder only permits no or complete sign flip, under a setting where the training target is explicitly provided. In practice, the learning is end-to-end, that is, there is no “real” supervision on the encoder available. As such, at the conceptual level, the learning relies on the decoder to fix the vertical sign-orientation as well as the encoder to preclude potential row sign flip—our experiment results in Section E.1 also corroborates this. 21Here we are displaying results for the test data; the results for training data lead to the same conclusion qualitatively. 37 F Generalization to Multiple Levels of Grouping We discuss the generalization of the proposed framework to the case where multiple levels of grouping are present and the corresponding group-common graphs at different levels of the hierarchy are of interest. Consider L-levels of nested grouping where the group assignments become increasingly granular as the level index increases. Specifically, there is a single level-0 group that encompasses all entities, and M (degenerate) level-L groups, with each group m having a singleton member being the entity m; all other levels are cases in between – see also Figure 24 for a pictorial illustration. Note that the case discussed in the main manuscript corresponds to the special case with L = 1. As an example for the case of L = 2 levels, consider the data analyzed in Section 5. Suppose that the subjects can be partitioned into 3 groups according to their ages — e.g., less than 30 years old, 30-60 years old, over 60. In such a setting, the single level-0 group comprises of all subjects; the level-1 groups correspond to subjects falling into different age strata; the level-2 groups are the subjects themselves. The quantities of interest are the connectivity patterns shared by subjects within their respective groups at all levels. Entities Level 2 Group Level 1 Group Level 0 Group Figure 24: Diagram for a 3-level grouping. Neurons corresponds to Gl k’s that collects the indices of the entities belonging to that group. Solid lines with arrows indicate how small groups from an upper level form larger groups at a lower level. Let Gl := {Gl 1, · · · , Gl |Gl|} denote the collection of groups of level l; each Gl k is the index set for the entities belonging to group k at level l and the group membership is non-overlapping, that is, Gl k1∩Gl k2 = ∅, ∀ k1, k2 ∈ {1, · · · , |Gl|}. The quantities of interest are the entity-specific graphs z[m], as well as the group-level common structure for all groups at all levels, that is ¯zGl k, denoting the group-common structure amongst all entities that belong to the kth group, with level-l grouping; l = 0, · · · , L − 1 indexes the group level; k = 1, · · · , |Gl| indexes the group id within each level. Finally, we let ¯z ≡ ¯zG0, which is consistent with its definition in the main text and it corresponds to the grand-common structure across all entities. Without getting into the details of each step, the end-to-end learning procedure can be summarized in Figure 25. Compared with the two-level case, the generalization amounts to additional intermediate en- coded/decoded distributions in the form of qϕ(z[Gl−1 k ]|z[Gl k]), pθ(z[Gl k]|z[Gl−1 k ]) and pθ(z[Gl k]|·) (post conjugacy adjustment/merging information); l = 2, · · · , L; k = 1, · · · , |Gl|. {x[m]} {z[m]}|{x[m]} {z[GL−1 k ]}|{z[m]} {z[G1 k]}|{z[G2 k]} sampled ¯z pθ(¯z) {ˆx[m]} {z[m]} | · {z[GL−1 k ]} | · {z[G1 k]}| · qϕ(z[m]|xm) qϕ(¯z|{¯z[G1 k]}) pθ({z[G1 k]}|¯z) pθ({x[m]}|{z[m]}) (merge info) (merge info) (merge info) (merge info) (observed) encoding decoding (reconstructed) (prior) Figure 25: Diagram for the end-to-end encoding-decoding procedure in the presence of multiple levels of grouping. 38 References Barry, R. J., A. R. Clarke, S. J. Johnstone, and C. R. Brown (2009). EEG differences in children between eyes-closed and eyes-open resting conditions. Clinical Neurophysiology 120(10), 1806–1811. Barry, R. J., A. R. Clarke, S. J. Johnstone, C. A. Magee, and J. A. Rushby (2007). EEG differences between eyes-closed and eyes-open resting conditions. Clinical Neurophysiology 118(12), 2765–2773. Basu, S. and G. Michailidis (2015). Regularized estimation in sparse high-dimensional time series models. The Annals of Statistics 43(4), 1535–1567. Basu, S., A. Shojaie, and G. Michailidis (2015). Network Granger causality with inherent grouping structure. The Journal of Machine Learning Research 16(1), 417–453. Berliner, L. M. (1996). Hierarchical Bayesian time series models. In Maximum Entropy and Bayesian Methods: Santa Fe, New Mexico, USA, 1995 Proceedings of the Fifteenth International Workshop on Maximum Entropy and Bayesian Methods, pp. 15–22. Springer. Burda, Y., R. Grosse, and R. Salakhutdinov (2016). Importance weighted autoencoders. In International Conference on Learning Representations. Dahlhaus, R. and M. Eichler (2003). Causality and graphical models in time series analysis. Oxford Statistical Science Series, 115–137. Das, A., A. Mandel, H. Shitara, T. Popa, S. G. Horovitz, M. Hallett, and N. Thirugnanasambandam (2022). Evaluating interhemispheric connectivity during midline object recognition using EEG. PloS One 17(8), e0270949. Das, R., E. Maiorana, and P. Campisi (2016). EEG biometrics using visual stimuli: A longitudinal study. IEEE Signal Processing Letters 23(3), 341–345. Eichler, M. (2012). Graphical modelling of multivariate time series. Probability Theory and Related Fields 153, 233–268. Erd˝os, P. and A. R´enyi (1959). On random graphs I. Publ. math. debrecen 6(290-297), 18. Feller, A. and A. Gelman (2015). Hierarchical models for causal effects. Emerging Trends in the Social and Behavioral Sciences: An interdisciplinary, searchable, and linkable resource, 1–16. Figurnov, M., S. Mohamed, and A. Mnih (2018). Implicit reparameterization gradients. Advances in Neural Information Processing Systems 31. Gelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (2014). Bayesian data analysis (3rd ed.). Chapman and Hall/CRC. Gelman, A. and J. Hill (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge University Press. Geweke, J. (1984). Inference and causality in economic time series models. Handbook of Econometrics 2, 1101–1144. Gilmer, J., S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl (2017). Neural message passing for quantum chemistry. In International Conference on Machine Learning, pp. 1263–1272. PMLR. Gorishniy, Y., I. Rubachev, and A. Babenko (2022). On embeddings for numerical features in tabular deep learning. Advances in Neural Information Processing Systems 35, 24991–25004. Graber, C. and A. Schwing (2020). Dynamic neural relational inference for forecasting trajectories. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 1018– 1019. Granger, C. W. (1969). Investigating causal relations by econometric models and cross-spectral methods. Econometrica: Journal of the Econometric Society, 424–438. Granger, C. W. (1980). Testing for causality: A personal viewpoint. Journal of Economic Dynamics and Control 2, 329–352. Hatton, S. L., S. Rathore, I. Vilinsky, and A. Stowasser (2023). Quantitative and qualitative representation of introductory and advanced EEG concepts: An exploration of different EEG setups. Journal of Undergradu- ate Neuroscience Education 21(2), A142. 39 Heller, K. A. and Z. Ghahramani (2005). Bayesian hierarchical clustering. In Proceedings of the 22nd Inter- national Conference on Machine learning, pp. 297–304. Hochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural Computation 9(8), 1735–1780. Hong, Y., Y. Liu, and S. Wang (2009). Granger causality in risk and detection of extreme risk spillover between financial markets. Journal of Econometrics 150(2), 271–287. Jankowiak, M. and F. Obermeyer (2018). Pathwise derivatives beyond the reparameterization trick. In International Conference on Machine Learning, pp. 2235–2244. PMLR. Kerin, J. and H. Engler (2022). On the Lorenz’96 model and some generalizations. Discrete and Continuous Dynamical Systems - B 27(2), 769–797. Khanna, S. and V. Y. F. Tan (2020). Economy statistical recurrent units for inferring nonlinear granger causality. In International Conference on Learning Representations. Kingma, D. P., S. Mohamed, D. Jimenez Rezende, and M. Welling (2014). Semi-supervised learning with deep generative models. Advances in Neural Information Processing Systems 27. Kingma, D. P. and M. Welling (2014). Auto-encoding variational Bayes. In International Conference on Learning Representations. Kipf, T., E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel (2018). Neural relational inference for interacting systems. In International Conference on Machine Learning, pp. 2688–2697. PMLR. Kyung, M., J. Gill, M. Ghosh, and G. Casella (2010). Penalized regression, standard errors, and Bayesian Lassos. Bayesian Analysis 5(2), 369–411. Lindley, D. V. and A. F. Smith (1972). Bayes estimates for the linear model. Journal of the Royal Statistical Society Series B: Statistical Methodology 34(1), 1–18. Lorenz, E. N. (1996). Predictability: A problem partly solved. In Proc. Seminar on Predictability, Volume 1. Reading. L¨owe, S., D. Madras, R. Zemel, and M. Welling (2022). Amortized causal discovery: Learning to infer causal graphs from time-series data. In Conference on Causal Learning and Reasoning, pp. 509–525. PMLR. Maddison, C. J., A. Mnih, and Y. W. Teh (2017). The concrete distribution: A continuous relaxation of discrete random variables. In International Conference on Learning Representations. Marcinkeviˇcs, R. and J. E. Vogt (2021). Interpretable models for Granger causality using self-explaining neural networks. In International Conference on Learning Representations. Marx, E., A. Deutschl¨ander, T. Stephan, M. Dieterich, M. Wiesmann, and T. Brandt (2004). Eyes open and eyes closed as rest conditions: impact on brain activation patterns. Neuroimage 21(4), 1818–1824. Miraglia, F., F. Vecchio, P. Bramanti, and P. M. Rossini (2016). EEG characteristics in “eyes-open” versus “eyes-closed” conditions: Small-world network architecture in healthy aging and age-related brain degen- eration. Clinical Neurophysiology 127(2), 1261–1268. Modarres, M., D. Cochran, D. N. Kennedy, and J. A. Frazier (2023). Comparison of comprehensive quanti- tative EEG metrics between typically developing boys and girls in resting state eyes-open and eyes-closed conditions. Frontiers in Human Neuroscience 17. Montalto, A., S. Stramaglia, L. Faes, G. Tessitore, R. Prevete, and D. Marinazzo (2015). Neural networks with non-uniform embedding and explicit validation phase to assess Granger causality. Neural Networks 71, 159–171. Nauta, M., D. Bucur, and C. Seifert (2019). Causal discovery with attention-based convolutional neural networks. Machine Learning and Knowledge Extraction 1(1), 19. Rubinov, M. and O. Sporns (2010). Complex network measures of brain connectivity: uses and interpreta- tions. Neuroimage 52(3), 1059–1069. Scarselli, F., M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini (2008). The graph neural network model. IEEE Transactions on Neural Networks 20(1), 61–80. Seth, A. K., A. B. Barrett, and L. Barnett (2015). Granger causality analysis in neuroscience and neuroimag- ing. Journal of Neuroscience 35(8), 3293–3297. 40 Shojaie, A. and E. B. Fox (2022). Granger causality: A review and recent advances. Annual Review of Statistics and Its Application 9, 289–319. Sønderby, C. K., T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther (2016). Ladder variational autoen- coders. Advances in Neural Information Processing Systems 29. Stam, C. J. (2005). Nonlinear dynamical analysis of EEG and MEG: review of an emerging field. Clinical Neurophysiology 116(10), 2266–2301. Stock, J. H. and M. W. Watson (2001). Vector autoregressions. Journal of Economic Perspectives 15(4), 101–115. Tank, A., I. Covert, N. Foti, A. Shojaie, and E. B. Fox (2021). Neural Granger causality. IEEE Transactions on Pattern Analysis and Machine Intelligence 44(8), 4267–4279. Teh, Y. W. and M. I. Jordan (2010). Hierarchical Bayesian nonparametric models with applications. Bayesian Nonparametrics 1, 158–207. Trujillo, L. T., C. T. Stanfield, and R. D. Vela (2017). The effect of electroencephalogram (EEG) reference choice on information-theoretic measures of the complexity and integration of EEG signals. Frontiers in Neuroscience 11, 425. Wikle, C. K., L. M. Berliner, and N. Cressie (1998). Hierarchical Bayesian space-time models. Environmental and Ecological Statistics 5, 117–154. Wu, T., T. Breuel, M. Skuhersky, and J. Kautz (2020). Discovering nonlinear relations with minimum pre- dictive information regularization. arXiv preprint arXiv:2001.01885. 41 "
}