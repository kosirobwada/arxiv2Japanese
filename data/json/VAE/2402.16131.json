{
    "optim": "A VAE-based Framework for Learning Multi-Level\nNeural Granger-Causal Connectivity\nJiahe Lin\nHuitian Lei\nGeorge Michailidis*\nAbstract\nGranger causality has been widely used in various application domains to capture lead-lag relationships\namongst the components of complex dynamical systems, and the focus in extant literature has been on a\nsingle dynamical system. In certain applications in macroeconomics and neuroscience, one has access to\ndata from a collection of related such systems, wherein the modeling task of interest is to extract the shared\ncommon structure that is embedded across them, as well as to identify the idiosyncrasies within individual\nones. This paper introduces a Variational Autoencoder (VAE) based framework that jointly learns Granger-\ncausal relationships amongst components in a collection of related-yet-heterogeneous dynamical systems,\nand handles the aforementioned task in a principled way. The performance of the proposed framework\nis evaluated on several synthetic data settings and benchmarked against existing approaches designed for\nindividual system learning. The method is further illustrated on a real dataset involving time series data\nfrom a neurophysiological experiment and produces interpretable results.\nKeywords: deep neural networks; variational autoencoder; joint-learning; Granger-causality\n1\nIntroduction\nThe concept of Granger causality introduced in Granger (1969) leverages the temporal ordering of time series\ndata. It is defined in terms of predictability of future values of a time series; namely, whether the inclusion\nof past information (lag values) of other time series as well as its own (self lags) leads to a reduction in the\nvariance of the prediction error of the time series under consideration. Since its introduction, it has become\na widely-used approach in the analysis of economic (Stock and Watson, 2001), financial (Hong et al., 2009)\nand neuroimaging (Seth et al., 2015) time series data. The standard setting in these applications is that one\nis interested in estimating Granger causal relationships in a dynamical system (e.g., a national economy, a\nbrain) comprising of p variables.\nGranger causality can also be expressed through the language of graphical models (Dahlhaus and Eichler,\n2003; Eichler, 2012). The node set of the graph corresponds to the p variables at different time points;\ndirected edges between nodes at past time points to those at the present time capture Granger causal re-\nlationships (for more details and a pictorial illustration see Section C.1). Traditionally, Granger causality\nwas operationalized through linear vector autoregressive (VAR) models (Granger, 1969), in which case the\nentries of the estimated transition matrices correspond precisely to the edges of the Granger causal graph.\nMore recent work has explored how Granger causal relationships can be learned through nonlinear models;\ne.g., see review paper Shojaie and Fox (2022) and references therein.\nIn certain application domains, one has access to data from a collection of related dynamical systems. A\nmotivating example is described next. Consider electroencephalography (EEG) recordings obtained from p\nelectrodes placed on the scalp of a subject (e.g., a patient or an animal). The resulting time series data consti-\ntute measurements from a complex neurophysiological dynamical system (Stam, 2005). On many instances,\n*Corresponding Author. Department of Statistics and Data Science, UCLA. 〈gmichail@ucla.edu〉\nCode repo: https://github.com/GeorgeMichailidis/vae-multi-level-neural-GC-official\n1\narXiv:2402.16131v1  [cs.LG]  25 Feb 2024\none has access to such measurements for a collection of M related subjects (or “entities”, equivalently); for\nexample, they may be performing the same cognitive task (e.g., visual counting, geometric figure rotation)\nor exhibit a similar neurological disorder (e.g., epilepsy, insomnia, dementia). In such a setting, one can\nalways opt to perform separate analyses on each subject’s data; however, it would be useful to develop\nmethodology that models the data from all subjects jointly, so as to simultaneously extract the embedded\nstructure shared across subjects and identify the idiosyncrasies (heterogeneity) in any single one. In other\nwords, if one views all subjects as belonging to a common group, the quantities of interest are the shared\ngroup-level connectivity structure (amongst nodes) and the entity-level ones.\nConceptually, the above-mentioned modeling task is not difficult to accomplish in a linear setting where one\ncan decompose the transition matrices into a “shared” component and an idiosyncratic (entity-specific) one,\nwith some orthogonality-type constraint to enforce identifiability of the parameters (more details provided\nin Section C.3). However, the task becomes more challenging and involved in non-linear settings where one\nhopes to use flexible models to capture the underlying complex dynamics. In particular, a decomposition-\nbased approach, which requires the exact specification of the functional form of the shared component or\nhow the shared and the idiosyncratic components interact, would be rather restrictive. To this end, we\nadopt a generative model-based approach, which circumvents the issue by encoding the Granger causal\nrelationships through graphs. By postulating a model with a hierarchical structure between the shared and\nentity-specific components, the problem can be addressed in a flexible, yet principled manner.\nSummary of contributions.\nWe develop a two-layer Variational Autoencoder (VAE) based framework for\nestimating Granger-causal connections amongst nodes in a collection of related dynamical systems — jointly\nfor the common group-level and the entity-level ones — in the presence of entity-specific heterogeneity. De-\npending on the assumed connection type (continuous or binary) amongst the nodes, the proposed framework\ncan accommodate the scenario accordingly by imposing a commensurate structure on the encoded/decoded\ndistributions, leveraging conjugacy between pairs of distributions. The proposed model enables extracting\nthe embedded common structure in a principled way, without resorting to any ad-hoc or post-hoc aggre-\ngation. Finally, the framework can be generalized to the case where multiple levels of nested groups are\npresent and provides estimates of the group-level connectivity for all levels of groups.\nThe remainder of the paper is organized as follows. In Section 2, we provide a review of related literature on\nGranger-causality estimation, with an emphasis on neural network-based methods. The main building block\nused in the proposed framework, namely, a multi-layer VAE is also briefly introduced. Section 3 describes\nin detail the proposed framework, including the encoder/decoder modules and the training/inference pro-\ncedure. In Section 4, model performance is assessed on synthetic datasets and benchmarked against several\nexisting methods. An application to a real dataset involving EEG signals from 22 subjects is discussed in\nSection 5. Finally, Section 6 concludes the paper.\n2\nRelated Work and Preliminaries\nIn this section, we review related work on inferring Granger causality based on time series data, with an em-\nphasis on deep neural network-based approaches. Further, as the proposed framework relies on variational\nautoencoders (VAE) with a hierarchical structure, we also briefly review VAEs in the presence of multiple\nlatent layers.\n2.1\nInference of Granger causality\nLinear VAR models have historically been the most popular approach for identifying Granger causal rela-\ntionships. Within the linear setting, hypothesis testing frameworks with theoretical guarantees have been\ndeveloped (Granger, 1980; Geweke, 1984), while more recently regularized approaches have enabled the\nestimation in the high-dimensional setting (Basu et al., 2015). Recent advances in neural network techniques\n2\nhave facilitated capturing non-linear dynamics and identifying Granger causality accordingly, as discussed\nnext.\nNote that estimation of Granger causality is an unsupervised task, in the sense that the connectivity as cap-\ntured by the underlying graph is not observed and thus cannot serve as the supervised learning target. De-\npending on the model family that the associated estimation procedure falls into, existing approaches suitable\nfor estimating Granger causality based on neural networks (Montalto et al., 2015; Nauta et al., 2019; Wu\net al., 2020; Khanna and Tan, 2020; Tank et al., 2021; Marcinkeviˇcs and Vogt, 2021; L¨owe et al., 2022)\ncan be broadly categorized into prediction-based and generative model-based ones. We selectively review\nsome of them next. In the remainder of this subsection, we use xi,t to denote the value of node i at time\nt, xt := (x1,t, · · · , xp,t) the collection of node values of the dynamical system, and x := {x1, · · · , xT } the\ntrajectory over time.\nWithin the predictive modeling framework, recent representative works include Khanna and Tan (2020);\nTank et al. (2021); Marcinkeviˇcs and Vogt (2021), where the Granger-causal relationship is inferred from\ncoefficients that govern the dynamics of the time series, and the coefficients are learned by formulating\nprediction tasks that can be generically represented as xt = f(xt−1, ..., xt−q) + εt, with xt ∈ Rp being the\nmultivariate time series signal and εt the noise term. In Tank et al. (2021), coordinates of the response are\nconsidered separately, that is, xi,t = fi(xt−1, ..., xt−q) + εi,t, and fi is parameterized using either multi-layer\nperceptrons (MLP) or LSTM (Hochreiter and Schmidhuber, 1997). In the case of an L-layer MLP,\nbxi,t = W LhL−1\nt\n+ bL;\nhl\nt = σ\n\u0010\nW lhl−1\nt\n+ bl\u0011\n, l = 2, · · · , L;\nh1\nt = σ\n\u0010 Xq\nk=1 W 1kxt−k + b1\u0011\n;\nthe Granger-causal connection from the jth node to the ith node is then encoded in some “summary” (e.g.,\nFrobenius norm) of {W 11\n:j , · · · , W 1q\n:j }, with each component corresponding to the first hidden layer weight of\nlags xj,t−1, · · · , xj,t−q. Various regularization schemes are considered and incorporated as penalty terms in\nthe loss function, to encourage sparsity and facilitate the identification of Granger-causal connections. The\ncase of LSTM-based parameterization is handled analogously. Marcinkeviˇcs and Vogt (2021) parameterizes\nf as an additive function of the lags, i.e., xt = Pq\nk=1 Ψk(xt−k)xt−k + εt; the output of Ψk : Rp 7→ Rp×p\ncontains the generalized coefficients of xt−k, whose (i, j) entry corresponds to the impact of xj,t−k on\nxi,t and Ψk is parameterized through MLPs. The Granger causal connection between the jth node and\nthe ith node is obtained by aggregating information from the coefficients of all lags {Ψk(xt−k)ij}, i.e.,\nmax1≤k≤q{medianq+1≤t≤T (|Ψk(xt−k)ij|)}. Finally, an additional stability-based procedure where the model\nis fit to the time series in the reverse order is performed for the final selection of the connections.1\nIt\nis worth noting that for both of the above-reviewed approaches, the ultimately desired node j to node i\n(∀ i, j ∈ {1, · · · , p}) Granger causal relationship is depicted by a scalar value, whereas in the modeling\nstage, such a connection is collectively captured by multiple “intermediate” quantities—{W 1k\n:j , k = 1, · · · q}\nin Tank et al. (2021) and {Ψk(xt−k)ij, k = 1, · · · , q} in Marcinkeviˇcs and Vogt (2021); hence, an information\naggregation step becomes necessary to summarize the above-mentioned quantities to a single scalar value.\nFor generative model-based approaches, the starting point is slightly different. Notable ones include L¨owe\net al. (2022) that builds upon Kipf et al. (2018), and the focus is on relational inference. The postulated\ngenerative model assumes that the trajectories are collectively governed by an underlying latent graph z,\nwhich effectively encodes Granger-causal connections:\np(x|z) = p({xT +1, · · · , x1}|z) =\nYT\nt=1 p(xt+1|xt, · · · , x1, z).\nSpecifically, in their setting, xi,t ∈ Rd is vector-valued and zij corresponds to a categorical “edge type” be-\ntween nodes i and j. For example, it can be a binary edge type indicating presence/absence, or a more\ncomplex one having more categories. To simultaneously learn the edge types and the temporal dynam-\nics, the model is formalized through a VAE that maximizes the evidence lower bound (ELBO), given by\nEqϕ(z|x)(log pθ(x|z))−KL(qϕ(z|x)\n\r\r pθ(z)), where qϕ(z|x) is the probabilistic encoder, pθ(x|z) the decoder, and\n1This stability-based step amounts to finding an optimal thresholding level for the “final” connections: the same model is fit to the\ntime series in the reverse order, and “agreement” is sought between the Granger causal connections obtained respectively based on the\noriginal and the reverse time series, over a sequence of thresholding levels; the optimal one is determined by the one that maximizes\nthe agreement measure. We refer interested readers to the original paper and references therein for more details.\n3\npθ(z) the prior distribution. Concretely, the probabilistic encoder is given by qϕ(z|x) = softmax\n\u0000fenc,ϕ(x)\n\u0001\nand it infers the type for each entry of z; the function fenc,ϕ is parameterized by neural networks. The de-\ncoder pθ(x|z) = QT\nt=1 pθ(xt+1|z, xτ : τ ≤ t) projects the trajectory based on past values and z—specifically,\nthe distributional parameters for each step forward. For example, if a Gaussian distribution is assumed, in\nthe Markovian case, pθ(xt+1|xt, z) = N(mean, variance), where mean = f 1\ndec,θ(xt, z), variance = f 2\ndec,θ(xt, z)\nand f 1\ndec,θ, f 2\ndec,θ are parameterized by some neural networks. Finally, maximizing the ELBO loss can be\nalternatively done by minimizing\n−Eqϕ(z|x)(log pθ(x|z)) + KL(qϕ(z|x)\n\r\r pθ(z)) := negative log-likelikehood + H\n\u0000qϕ(z|x)\n\u0001\n+ const;\nthe negative log-likelihood corresponds to the reconstruction error of the entire trajectory coming out of the\ndecoder, and the KL divergence term boils down to the sum of entropies denoted by H(·) if the prior pθ(z) is\nassumed to be a uniform distribution over edge types.\nIn summary, at the formulation level, generative model-based approaches treat Granger-causal connections\n(relationships) as a latent graph and learn it jointly with the dynamics, whereas predictive ones extract\nGranger-causal connections from the parameters that govern the dynamics in a post-hoc manner. The former\ncan readily accommodate vector-valued nodes whereas for the latter, it becomes more involved and further\ncomplicates how the connections can be extracted/represented based on the model parameters.\nAt the\ntask level, to learn the model parameters, prediction-based approaches rely on tasks where the predicted\nvalues of the future one-step-ahead timestamp are of interest, whereas generative approaches amount to\nreconstructing the observed trajectories; prediction and reconstruction errors constitute part of the empirical\nrisk minimization loss and the ELBO loss, respectively.\n2.2\nMulti-layer variational autoencoders\nWith a slight abuse of notation, in this subsection, we use x to denote the observed variable and zl, l =\n1, · · · , L the latent ones for L layers.\nA “shallow” VAE with one latent layer is considered in the seminal work of Kingma and Welling (2014), where\nthe generative model is given by pθ(x, z1) = pθ(x|z1)pθ(z1), with pθ(z1) denoting the prior distribution. Later\nworks (Kingma et al., 2014; Burda et al., 2016; Sønderby et al., 2016) consider the extension into multiple\nlatent layers, where the generative model can be represented through a cascading structure as follows:\npθ(x, {zl}L\nl=1) = pθ(x|z1)\n\u0010 YL−1\nl=1 pθ(zl|zl+1)\n\u0011\npθ(zL);\nthe corresponding inference model (encoder) is given by qϕ(z1, · · · , zL|x) = qϕ(z1|x) QL\ni=1 qϕ(zl|zl−1). The\nvariational lower bound on log p(x) can be written as\nEqϕ({z}L\nl=1|x)\n\u0010\nlog pθ(x|{z}L\nl=1)\n\u0011\n− KL\n\u0010\nqϕ({z}L\nl=1|x)\n\r\r pθ({z}L\nl=1)\n\u0011\n,\n(1)\nwith the first term corresponding to the reconstruction error.\nConjugacy adjustment.\nUnder the above multi-layer setting, Sønderby et al. (2016) considers an inference\nmodel that recursively merges information from the “bottom-up” encoding and “top-down” decoding steps.\nConcretely, in the case where each layer is specified by a Gaussian distribution, the original distribution at\nlayer l after encoding is given by qϕ(zl|zl−1) ∼ N(µq,l, σ2\nq,l) and the distribution at the same layer after decod-\ning is given by pθ(zl|zl+1) ∼ N(µp,l, σ2\np,l). The adjustment amounts to a precision-weighted combination that\ncombines information from the decoder distribution into the encoder one, that is, qϕ(zl|·) ∼ N\n\u0000˜µq,l, ˜σ2\nq,l\n\u0001\n,\nwhere ˜µq,l = (µq,lσ−2\nq,l +µp,lσ−2\np,l )/(σ−2\nq,l +σ−2\np,l ) and ˜σ2\nq,l = 1/(σ−2\nq,l +σ−2\np,l ). This information-sharing mechanism\nleads to richer latent representations and improved approximation of the log-likelihood function. A similar\nobjective is also considered in Burda et al. (2016) and operationalized through importance weighting.\n4\nIt is worth noting that the precision-weighted adjustment in Sønderby et al. (2016) is in the spirit of the\nconjugate analysis in Bayesian statistics. In particular, in Bayesian settings where the data likelihood is\nassumed Gaussian with a fixed variance parameter and the prior distribution is also assumed Gaussian, the\nposterior distribution possesses a closed-form Gaussian distribution (and hence conjugate w.r.t. the prior)2.\nFor this reason, we term such an adjustment as the “conjugacy adjustment”, which will be used later in our\ntechnical development.\nFinally, utilizing multiple layers possessing a hierarchy as discussed above resembles the framework adopted\nin Bayesian hierarchical modeling. We provide a brief review of the topic in Section C.2. We also sketch in\nSection C.3 a modeling formulation under this framework for collection of linear VARs.\n3\nThe Proposed Framework\nGiven a collection of trajectories for the same set of p variables (nodes) from M dynamical systems (entities),\nwe are interested in estimating the Granger causal connections amongst the nodes in each system (i.e., entity-\nlevel connections), as well as the common “backbone” connections amongst the nodes that are shared across\nthe entities (i.e., group-level connections).\nTo this end, we propose a two-layer VAE-based framework, wherein Granger-causal connections are treated\nas latent variables with a hierarchical structure, and they are learned jointly with the dynamics of the tra-\njectories. In Section 3.1, we present the posited generative process that is suitable for the modeling task\nof interest, and give an overview of the proposed VAE-based formulation; the details of the components in-\nvolved and their exact modeling considerations are discussed in Section 3.2. Section 3.3 provides a summary\nof the end-to-end training process and the inference tasks that can be performed based on the trained model.\nThe generalization of the proposed framework to the case of multiple levels of grouping across entities is\ndeferred to Appendix F, where the grand common and the group common structures can be simultaneously\nlearned with those of the entities.\n3.1\nAn overview of the formulation\n¯z\nz[1] z[2]\nz[M−1]z[M]\nx[1]\nx[2]\nx[M−1]x[M]\npθ⋆(z[m]|¯z)\npθ⋆(x[m]|z[m])\nFigure 1: Diagram for the postulated top-down gener-\native process.\nConsider a setting where there are M entities, each of them hav-\ning the same set of p nodes, that evolve as a dynamical system. Let\nx[m]\ni,t denote the value of node i of entity m ∈ {1, · · · , M} at time\nt. It can be either scalar or vector-valued, with scalar node values\nbeing prevalent in traditional time-series settings; in the latter case,\nthe nodes can be thought of as being characterized by vector-valued\n“features”3. Let x[m]\nt\n:= (x[m]\n1,t , · · · , x[m]\np,t ) be the collection of node\nvalues at time t for entity m, and x[m] := {x[m]\n1\n, · · · , x[m]\nT } the cor-\nresponding trajectory over time. Further, let z[m] ∈ Rp×p denote the\nGranger-causal connection matrix of entity m and ¯z := [¯zij] ∈ Rp×p\nthe common structure embedded in z[1], · · · , z[M], and note that it\ndoes not necessarily correspond to the arithmetic mean of the z[m]’s.\nIn the remainder of this paper, we may refer to these matrices as\n“graphs” interchangeably.\nDepending on the modeling scenario, the entity-level Granger-causal\nconnections z[m] can either be binary or continuous. In the former\ncase, it corresponds precisely to the aggregate Granger causal graph defined in Dahlhaus and Eichler (2003);\n2Note that in the case where the data likelihood is Gaussian, but the variance is no longer a fixed parameter, the Normal-Normal\nconjugacy does not necessarily go through.\n3For example, in the Springs experiment in Kipf et al. (2018) (which is also considered in this paper; see Appendix B.1), the features\ncorrespond to a 4-dimensional vector, with the first two coordinates being the 2D velocity and the last two being the 2D location\n5\nin the latter case, its (i, j)-entry (scalar value) reflects the strength of the relationship between the past\nvalue(s) of node j the present value of node i; see Appendix C.1 and Remark 6 for a detailed discussion.\nThe posited generative process, whose true parameters are denoted by θ⋆, is given by:\npθ⋆\n\u0010\n{x[m]}M\nm=1, {z[m]}M\nm=1, ¯z\n\u0011\n= pθ⋆\n\u0010\n{x[m]}M\nm=1|{z[m]}M\nm=1\n\u0011\n· pθ⋆\n\u0010\n{z[m]}M\nm=1|¯z\n\u0011\n· pθ⋆(¯z)\n=\nYM\nm=1 pθ⋆(x[m]|z[m])\nYM\nm=1 pθ⋆(z[m]|¯z)\nY\n1≤i,j≤p pθ⋆(¯zij).\n(2)\nThe decomposition is based on the following underlying assumptions (see also Figure 1 for a pictorial illus-\ntration):\n• conditional on the entity-specific graphs z[m], their trajectories x[m]’s are independent of the grand com-\nmon ¯z, and they are conditionally independent from each other given their respective entity-specific\ngraphs z[m]’s\n• the entity-specific graphs z[m] are conditionally independent given the common graph ¯z\n• the prior distribution pθ⋆(¯z) factorizes over the edges.\nThe proposed model creates a hierarchy between the common graph and the entity-specific ones, which\nin turn naturally provides a coupling mechanism amongst the latter. The grand common structure can be\nestimated as one learns all the latent components jointly with the dynamics of the system through a VAE. Let\nX := {x[1], · · · , x[m]}, Z := {¯z, z[1], · · · , z[m]}, qϕ(Z|X) denote the encoder, pθ(X|Z) the decoder and pθ(Z)\nthe prior distribution. Then, the ELBO is given by\nEqϕ(Z|X)\n\u0010\nlog pθ(X|Z)\n\u0011\n− KL\n\u0010\nqϕ(Z|X)\n\r\r pθ(Z)\n\u0011\n,\nand serves as the objective function for the end-to-end encoding-decoding procedure as depicted in Figure 2.\n{x[m]}\n{z[m]}|{x[m]}\nsampled ¯z\npθ(¯z)\n{z[m]} | ·\n{ˆx[m]}\nqϕ(z[m]|xm)\nqϕ(¯z|{¯zm})\npθ({z[m]}|¯z)\npθ({x[m]}|{z[m]})\n(merge info)\n(merge info)\n(observed)\nencoding\ndecoding\n(reconstructed)\n(prior)\nFigure 2: Diagram for the end-to-end encoding-decoding procedure. Solid paths with arrows denote modeling the corresponding distributions during the\nencoding/decoding process; dashed paths with arrows correspond to information merging based on (weighted) conjugacy adjustment. Quantities obtained\nafter each step are given inside the circles/rectangles. {x[m]} is short for the collection {x[m]}M\nm=1; {z[m]} is analogously defined.\nRemark 1 (On the proposed formulation). (1) Depending on the assumption on the entity-level Granger-\ncausal connections z[m]—either binary or continuous—encoder/decoder distributions can then be selected\naccordingly. In particular, distributions that form conjugate pairs (e.g., Gaussian-Gaussian for the continuous\ncase and Beta-Bernoulli for the binary case) can facilitate computations. (2) The proposed framework natu-\nrally allows estimation of positive/negative connections in a principled way without resorting to ad-hoc ag-\ngregation schemes. It also enables incorporation of external information pertaining to the presence/absence\nof connections through the decoder. (3) In settings where a large collection of entities is available, but each\nentity has limited sample size, the joint learning framework can be advantageous over an individual entity\nlearning one.\n6\n3.2\nModeling details\nNext, we provide details on the specification of the encoder and the decoder, the sampling steps, and the loss\nfunction calculations for model (2).\n3.2.1\nEncoder\nThe goal of the encoder is to infer the latent graphs Z := {¯z, z[1], · · · , z[M]} based on the observed trajectories\nX := {x[1], · · · , x[M]}.\nLet ϕ denote the collection of parameters in the encoder qϕ(Z|X). To delineate the dependency between the\ntrajectories and the graphs, the following assumptions are imposed:\n• conditioning on {z[m]}M\nm=1, ¯z is independent of {x[m]}M\nm=1 and the conditional probability qϕ\n\u0000¯z|{z[m]}M\nm=1\n\u0001\nfactorizes across edges (i, j);\n• the entity-specific graphs are conditionally independent given their corresponding trajectories, i.e.,\nqϕ\n\u0000{z[m]}M\nm=1|{x[m]}M\nm=1\n\u0001\nfactorizes across entities.\nThese assumptions are in line with the structure of the model in (2), in that the conditional dependencies\nposited in the generative model are respected during the “bottom-up” encoding process.\nConsequently, the encoder can be decomposed into the following product components:\nqϕ\n\u0000Z\n\f\fX\n\u0001\n= qϕ\n\u0010\n¯z\n\f\f {z[m]}M\nm=1\n\u0011\nM\nY\nm=1\nqϕ\n\u0010\nz[m]\f\fx[m]\u0011\n=\nY\n1≤i,j≤p\nqϕ\n\u0010\n¯zij |{z[m]\nij }M\nm=1\n\u0011\nM\nY\nm=1\nqϕ\n\u0010\nz[m]\f\fx[m]\u0011\n.\nThere are two types of terms in the above expression: qϕ(z[m]|x[m]) that infers each entity’s latent graph based\non its trajectory, and qϕ(¯zij|{z[m]\nij }M\nm=1) that obtains the grand common based on the entity-level graphs, in\nan edge-wise manner. Note that for qϕ(¯zij|{z[m]\nij }M\nm=1), together with modeling pθ(z[m]\nij |¯zij), resembles prior-\nposterior calculations in Bayesian statistics using conjugate pairs of distributions; hence, depending on the\nunderlying structural assumptions (continuous or binary) on the z[m]’s, one can choose emission heads (or\nequivalently, the output functional form) accordingly.\nAt the high level, the encoder can be abstracted into 3 modules, parameterized through fx→h, fh→z and\nfz→¯z, respectively:\n(enc-a) trajectory to hidden representation x[m] → h[m] := fx→h(x[m]), with h[m]\nij\ncorresponding to the\nedge-specific one;\n(enc-b) hidden representation to the entity-specific graph: h[m] → z[m] := fh→z(h[m]);\n(enc-c) entity-level graphs to the grand common (edge-wise): {z[m]\nij }M\nm=1 → ¯zij := fz→¯z({z[m]\nij }M\nm=1).\nModules (enc-a) and (enc-b) combined, model qϕ(z[m]|x[m]) and correspond to “Trajectory2Graph” opera-\ntions, while module (enc-c) models qϕ(¯zij|{z[m]\nij }M\nm=1) and captures the “Entity2Common” one. On the other\nhand, given the above-mentioned conjugate pair consideration, the choices of fh→z and fz→¯z are considered\njointly.\nFormally, for fx→h, we use a similar approach to that in Kipf et al. (2018), where fx→h entails message-\npassing operations that are widely adopted in the literature related to graph neural networks (Scarselli\net al., 2008; Gilmer et al., 2017). At a high level, these operations entail “node2edge” (concatenating the\nrepresentation of the node stubs) and “edge2node” (aggregating the representation of incoming edges)\niteratively and non-linear functions (e.g., MLPs) in between. The operation ultimately leads to {h[m]\nij }, with\nh[m]\nij\n∈ Rnhid being a nhid-dimensional hidden representation corresponding to z[m]\nij . Full details are provided\nin Appendix A.1 wherein we also provide a pictorial illustration for the operations.\n7\nOnce the h[m]\nij ’s are obtained, subsequent modeling in modules (enc-b) and (enc-c) can be generically repre-\nsented as\nz[m]\nij\n| h[m]\nij\n∼ qz(· ; δ[m]\nq,ij),\nand\n¯zij |{z[m]\nij } ∼ q¯z(· ; ¯δij),\nwhere qz(· ; δ[m]\nq,ij) is some distribution with parameter δ[m]\nq,ij := fh→z(h[m]\nij ) being the function output of fh→z.\nSimilarly, q¯z(· ; ¯δq,ij) is some distribution with parameter ¯δq,ij := fz→¯z({z[m]\nij }) being the function output of\nfz→¯z. The exact choices for fh→z and fz→¯z bifurcate depending on the scenario:\n• Case 1, z[m]’s entries being continuous: in this case, we consider a Gaussian-Gaussian emission head\npair. Consequently, δ[m]\nq,ij = {µ[m]\nq,ij, (σ[m])2\nq,ij}, ¯δq,ij = {¯µq,ij, ¯σ2\nq,ij};\nqz ∼ N\n\u0010\nµ[m]\nq,ij, (σ[m])2\nq,ij\n\u0011\n;\nµ[m]\nq,ij := f 1\nh→z(h[m]\nij ), (σ[m])2\nq,ij := f 2\nh→z(h[m]\nij );\n(3)\nq¯z ∼ N\n\u0010\n¯µq,ij, ¯σ2\nq,ij\n\u0011\n;\n¯µq,ij := f 1\nz→¯z({z[m]\nij }), ¯σ2\nq,ij := f 2\n¯z→z({z[m]\nij }).\n(4)\nf 1\nh→z, f 2\nh→z are component functions of fh→z, each with an nhid-dimensional input and a scalar out-\nput; they can be simple linear functions with f 2\nh→z having an additional softplus operation to ensure\npositivity. Similarly, f 1\nz→¯z, f 2\nz→¯z comprise fz→¯z, each with an m-dimensional input and a scalar output;\nin practice their functional form can be as simple as taking the sample mean and standard deviation,\nrespectively.\n• Case 2, z[m]’s entries being binary: in this case, we consider a Beta-Bernoulli emission head pair, i.e.,\nqz ∼ Ber\n\u0010\nδ[m]\nq,ij\n\u0011\n;\nδ[m]\nq,ij := fh→z(h[m]\nij ),\n(5)\nq¯z ∼ Beta\n\u0010\n¯αq,ij, ¯βq,ij\n\u0011\n;\n¯αq,ij := f 1\nz→¯z({z[m]\nij }), ¯βq,ij := f 2\nz→¯z({z[m]\nij }).\n(6)\nThe output of fh→z corresponds to the Bernoulli success probability and it is parameterized with an\nMLP with the last layer performing sigmoid activation to ensure that the output lies in (0, 1). f 1\nz→¯z\nand f 2\nz→¯z are component functions of fz→¯z. Similar to the Gaussian case, their choice need not be\ncomplicated and is chosen based on moment-matching.\nNote that the prior distribution pθ(¯zij) is also selected according to the underlying scenario, with a standard\nNormal distribution used in the continuous case and a Beta(1, 1) in the binary case. Once the distribution\nparameters for ¯zij are obtained based on (4) or (6), we apply conjugacy adjustment to incorporate also the\ninformation from the prior, before the sampling step takes place.\n3.2.2\nDecoder\nThe goal of the decoder pθ(X|Z) is to reconstruct the trajectories based on the entity and group level graphs,\nand its components follow from the generative process described in (2), that is,\npθ(X|Z) = pθ\n\u0010\n{x[m]}M\nm=1|{z[m]}M\nm=1\n\u0011\n· pθ\n\u0010\n{z[m]}M\nm=1|¯z\n\u0011\n=\nYM\nm=1 pθ(x[m]|z[m])\nYM\nm=1 pθ(z[m]|¯z),\nwhere θ denotes the collections of parameters in the decoder. The two components pθ(z[m]|¯z) and pθ(x[m]|z[m]),\nrespectively capture the dependency between the entity-specific graphs z[m]’s and their grand common ¯z,\nand the evolution of the trajectories given z[m]. Consequently, the decoder can be broken into two modules,\nparameterized through g¯z→z and gz→x:\n(dec-a) pθ(z[m]|¯z), the grand common to entity-specific graphs z → z[m] := g¯z→z(¯z), with g¯z→z(·) acting\non the sampled ¯z (edge-wise). Samples drawn from this distribution will be used to guide the\nevolution of the trajectories of the corresponding entity;\n(dec-b) pθ(x[m]|z[m]), graph to trajectory z[m] → xm; concretely,\npθ(x[m]|z[m]) = pθ(x[m]\n1\n|z[m])\nYT\nt=2 pθ\n\u0010\nx[m]\nt\n| x[m]\nt−1, ..., x[m]\n1\n, z[m]\u0011\n,\n8\nwith pθ(x[m]\nt\n| x[m]\nt−1, ..., x[m]\n1\n, z[m]) modeled through gz→x(x[m]\nt−1, · · · , x[m]\nt−q, z[m]) assuming a fixed con-\ntext length of q (or q-lag dependency, equivalently).\nWe refer to these two modules as “Common2Entity” and “Graph2Trajectory”, respectively.\nCommon2Entity.\nWe consider a weighted conjugacy adjustment that merges the information from the\nencoder distribution into the decoder one, so that it contains both the grand common and the entity-specific\ninformation. Concretely, for some pre-specified weight ω ∈ [0, 1],\n• Case 1, in the continuous case, let pθ(z[m]\nij |¯zij) ∼ N(µ[m]\np,ij, (σ[m])2\np,ij) with µ[m]\np,ij := g1\n¯z→z(¯z[m]\nij ) and\n(σ[m])2\np,ij := g2\n¯z→z(¯z[m]\nij ); g1\n¯z→z, g2\n¯z→z : R 7→ R are component functions of g¯z→z. This gives the “un-\nadjusted” distribution that contains only the grand common information. With µ[m]\nq,ij and (σ[m])2\nq,ij\nobtained in (3), the weighted adjustment gives pθ(z[m]\nij |·) ∼ N\n\u0010\n˜µ[m]\np,ij, (˜σ[m])2\np,ij\n\u0011\n, where\n˜µ[m]\np,ij := ωµ[m]\nq,ij(σ[m])−2\nq,ij + (1 − ω)µ[m]\np,ij(σ[m])−2\np,ij\nω(σ[m])−2\nq,ij + (1 − ω)(σ[m])−2\np,ij\n,\n(˜σ[m])2\np,ij :=\n1\nω(σ[m])−2\nq,ij + (1 − ω)(σ[m])−2\np,ij\n.\n(7)\n• Case 2, in the binary case, let pθ(z[m]\nij |¯zij) ∼ Ber(δ[m]\np,ij), where δ[m]\np,ij := g¯z→z(¯zij). With δ[m]\nq,ij obtained\nin (5), the weighted adjustment gives\npθ(z[m]\nij |·) ∼ Ber\n\u0010\n˜δ[m]\np,ij\n\u0011\n; ˜δ[m]\np,ij =\n1\nω/δ[m]\nq,ij + (1 − ω)/δ[m]\np,ij\n.\n(8)\nSimilar to the function fz→¯z in the encoder, here g¯z→z corresponds to fz→¯z’s “reverse-direction” counterpart\nand its choice can be rather simple4.\nRemark 2 (On the role of ω). It governs the mixing percentage of the entity-specific and the common in-\nformation: when ω = 1, the “tilde” parameters of the post-adjustment distribution effectively collapse into\nthe encoder ones (e.g., ˜δp,ij ≡ δ[m]\nq,ij and analogously for ˜µp,ij, ˜σ2\np,ij); correspondingly, samples drawn from\npθ(z[m]\nij |·) essentially ignore the sampled ¯z and hence they can be viewed as entirely entity-specific. At the\nother extreme, for ω = 0, the tilde parameters coincide with the unadjusted ones; therefore, apart from the\ngrand common information carried in the sampled ¯z, no entity-specific one is passed onto the sampled z[m].\nBy varying ω between (0, 1), one effectively controls the level of heterogeneity and how strongly the sampled\nentity-specific graphs deviate from the grand common one.\nGraph2Trajectory.\nModule (dec-b) pertains to modeling the dynamics of the trajectory x[m] given the sam-\npled z[m]. Here, we focus on one-step Markovian dependency, i.e., q = 1 and thus pθ(x[m]\nt\n| x[m]\nt−1, ..., x[m]\n1\n, z[m]) ≈\ngz→x(x[m]\nt−1, z[m]). The extension to longer lag dependencies (q > 1) can be readily obtained by pre-processing\nthe input accordingly, as discussed in Appendix A.2.\nWe consider the following parameterization of gz→x. At the high level, given that z[m]\nij\ncorresponds to the\nGranger-causal connection from node j to node i, it should serve as a “gate” controlling the amount of\ninformation that can be passed from x[m]\nj,t−1 to x[m]\ni,t . To this end, each response coordinate x[m]\ni,t is modeled as\nfollows:\nu[m],j\ni,t−1 := ˇx[m]\nj,t−1 ◦ z[m]\nij\n(gating), u[m]\ni,t−1 = {u[m],1\ni,t−1, · · · , u[m],p\ni,t−1}, and ˇu[m]\ni,t−1 := MLP(u[m]\ni,t−1);\n(9)\nx[m]\ni,t ∼ N\n\u0000µ[m]\nx,it, (σ[m])2\nx,it\n\u0001\n, where µ[m]\nx,it := Linear(ˇu[m]\ni,t−1), (σ[m])2\nx,it = Softplus\n\u0000Linear(ˇu[m]\ni,t−1)\n\u0001\n.\n(10)\nNote that in the gating operation in (9), we use ˇx[m]\nj,t−1 to denote the output after some potential numerical\nembedding step (e.g., Gorishniy et al. (2022)) of x[m]\nj,t−1; in the absence of such embedding, ˇx[m]\nj,t−1 ≡ x[m]\nj,t−1.\n4In our experiments, we use an identity function and it has been effective across the settings considered.\n9\nThrough the gating step5, x[m]\nj,t−1 exerts its impact on x[m]\ni,t entirely through u[m],j\ni,t−1. The continuous case and\nthe binary case z[m]\nij\ncan be treated in a unified manner: in the former case, the value of z[m]\nij\ncorresponds\nto the strength; in the latter case, it performs masking. Subsequently, u[m]\ni,t−1 collects the u[m],j\ni,t−1’s of all nodes\nj = 1, · · · , p, and serves as the predictor for x[m]\ni,t . Finally, if one simply sums all u[m],j\ni,t−1’s to obtain the mean of\nx[m]\ni,t , then it effectively coincides with the operation in a linear VAR system, with z[m]\nij\ncorresponding precisely\nto the entries in the transition matrix.\nRemark 3. The above-mentioned choice of gz→x can be viewed as a “node-centric” one, wherein entries z[m]\nij\ncontrol the information passing directly through the nodes. As an alternative, one can consider an “edge-\ncentric” one, which leverages the idea of message-passing in GNNs and entails “node2edge” and “edge2node”\noperations. This resembles the technology adopted in Kipf et al. (2018); L¨owe et al. (2022) that consider\nprimarily having graph entries corresponding to categorical edge types, which, after some adaptation, can\nbe used to handle the numerical case. In practice, we observe that the edge-centric graph2trajectory decoder\ncan lead to instability for time series signals6. A more detailed comparison can be found in Appendix A.2,\nwhere additional illustrations are provided for the two.\n3.2.3\nSampling\nGiven the stochastic nature of the sampled quantities, drawing samples from the encoded/decoded distribu-\ntions requires special handling to enable the gradient to back propagate. Depending on whether entries of\nz[m] are continuous or binary, there are three possible types of distributions involved; for notational simplic-\nity, here we use z to represent generically the random variable under consideration.\n• Normal z ∼ N(µ, σ2). In this case, the “standard” reparameterization trick (Kingma and Welling,\n2014) can be used, that is, z = µ + σ ◦ ϵ, ϵ ∼ N(0, 1).\n• Bernoulli z ∼ Ber(δ). In this case, the discrete distribution is approximated by its continuous relaxation\n(Maddison et al., 2017). Concretely, z = softmax((log(π) + ϵ)/τ) where ϵ ∈ R2 whose coordinates are\ni.i.d. samples from Gumbel(0, 1), π = (1 − δ, δ) is the binary class probability and τ is the temperature.\n• Beta z ∼ Beta(α, β). In this case, implicit reparameterization of the gradients (Figurnov et al., 2018)\nis leveraged and the construction of the reparameterized samples becomes much more involved. We\nrefer interested readers to Figurnov et al. (2018); Jankowiak and Obermeyer (2018) for an in-depth\ndiscussion on how parameterized random variables can be obtained and become differentiable.\n3.2.4\nLoss function\nThe loss function is given by the negative ELBO, that is,7\n−Eqϕ(Z|X)\n\u0010\nlog pθ(X|Z)\n\u0011\n+ KL\n\u0010\nqϕ(Z|X)\n\r\r pθ(Z)\n\u0011\n=: reconstruction error + KL;\nthe first term corresponds to the reconstruction error that measures the deviation between the original\ntrajectories and the reconstructed ones, while the KL term measures the “consistency” between the encoded\nand the decoded distributions, and can be viewed as a type of regularization.\nLet µ[m]\nx,t := (µ[m]\nx,1t, · · · , µ[m]\nx,pt)⊤ and Σx[m]\nt\n:= diag((σ[m])2\nx,1t, · · · , (σ[m])2\nx,pt)⊤ with the components defined\nin (10). The reconstruction error is the negative Gaussian log-likelihood loss given by\nM\nX\nm=1\n\u0010\nT\nX\nt=2\n\u0000x[m]\nt\n− µ[m]\nx,t\n\u0001⊤Σ−1\nx[m]\nt\n\u0000x[m]\nt\n− µ[m]\nx,t\n\u0001\n+ log |Σx[m]\nt\n|\n\u0011\n.\n(11)\n5Note that z[m]\nij\nis a scalar and is applied to all coordinates of ˇx[m]\nj,t−1 in the case the latter is a vector.\n6to contrast with the physical system (e.g., Springs) considered in the experiments of Kipf et al. (2018).\n7Recall that X:={x[m]; m = 1, · · · , M} and Z := {¯z, z[m]; m = 1, · · · , M}.\n10\nThe KL term can be simplified after some algebra to (see Appendix A.3 calculation):\nEqϕ(Z|X)\nh\nKL\n\u0010\nqϕ(¯z|{z[m]})\n\r\r pθ(¯z)\n\u0011i\n+ Eqϕ(Z|X)\nh\nKL\n\u0010\nqϕ({z[m]}|{x[m]})\n\r\r pθ({z[m]}|¯z)\n\u0011i\n;\n(12)\nboth terms can be viewed as “consistency matching” terms that measure the divergence between the distribu-\ntions obtained in the encoder pass and that from the decoder pass. Finally, note that in the implementation,\nthe quantities involved are replaced by their conjugacy adjusted counterparts wherever applicable, and this\nis similar to the treatment in Sønderby et al. (2016).\n3.3\nTraining and inference\nThe functions in the encoder (fx→h, fh→z and fz→¯z) and those in the decoder (g¯z→z and gz→x) are shared\nacross all entities m = 1, · · · , M, and thus the model is trained based on the “pooled” data of all entities,\nwhile keeping track of the entity id that each data block is associated with. The steps involved in the end-to-\nend training under the proposed framework are summarized in Exhibit 1.\nExhibit 1: Outline of steps for training under the two-layer VAE-based framework\nInput: observed trajectories {x[1], · · · , x[M]}, hyperparameters. Let ⟨M⟩ := {1, · · · , M}.\n– Forward pass, encoder: {x[m]} → {z[m]} → ¯z\n0. [Traj2Graph] m ∈ ⟨M⟩: obtain the encoded distribution for entity-specific graphs qϕ(z[m]|x[m]);\n1. m ∈ ⟨M⟩: sample z[m] from qϕ(z[m]|x[m]);\n2. [Entity2Common] based on {z[m]}M\nm=1, obtain the encoded distribution for the common graph qϕ(¯z|{z[m]});\n– Forward pass, decoder: ¯z → {z[m]} → {xm}\n3. merge prior info pθ(¯z) into qϕ(¯z|{z[m]}) then sample ¯z;\n4. [Common2Entity] m ∈ ⟨M⟩: obtain the decoded distribution for entity-specific graphs pθ(z[m]|¯z);\n5. m ∈ ⟨M⟩: merge entity-specific encoded info qϕ(z[m]|x[m]) into pθ(z[m]|¯z), then sample (z[m]| ·);\n6. [Graph2Traj] m ∈ ⟨M⟩: using z[m] and the lag info x[m]\nt−1, decode to get ˆx[m]\nt\n; t = 2, · · · , T.\n– Loss calculation\n7. calculate the ELBO loss by summing up (11) and (12);\n– Backward pass: update neural network parameters based on gradients (back-propagation)\nOutput: Trained encoder and decoder\nSeveral pertinent remarks follow. (1) The data typically consist of “long” trajectories that contain all the\navailable observations (time points); one needs to partition them to “short” ones of length T (that are typi-\ncally between 20-50), which constitute the samples used in model training. See Appendix A.5 for additional\nillustration. (2) In the case where one has external information regarding presence or absence of edges in\nthe z[m]’s, it can be incorporated by enforcing the corresponding entries to zero after the former are sampled\nin Step 5. (3) Once the encoder (inference model) and the decoder (generative model) are trained, the\nlatent graphs can be obtained by applying the trained encoder on the trajectories. For entity-specific graphs\nz[m]’s, the inference model gives the encoded distribution qϕ(z[m]|x[m])’s. In practice, the graph of interest\nis extracted by calculating the “mode” of the distribution; the grand common graph ¯z can be analogously\nhandled. It is worth noting that for continuous z[m]’s, the proposed framework naturally provides signed es-\ntimates and thus positive/negative Granger causal connections can be readily differentiated (see Appendix E\nfor a detailed discussion). (4) The trained decoder can be utilized to quantify also the predictive strength of\nthe Granger-causal connection, as discussed in Appendix A.4.\n4\nSynthetic Data Experiments\nWe evaluate the performance of the proposed framework, together with benchmarking methods on several\nsynthetic data settings. For all experiments, we start from a common graph that corresponds to ¯z, add\n11\nperturbations to it for individual entities to produce heterogeneous Granger-causal connections (i.e., the\nz[m]’s), then simulate trajectories {x[m]} corresponding to each entity based on their respective z[m]’s and the\nspecified dynamics. The estimated entity-specific and grand common graphs are then evaluated against the\nunderlying truth, for both the proposed and competing methods.\nPrediction model-based competitors8 include NGC (Tank et al., 2021), GVAR (Marcinkeviˇcs and Vogt, 2021)\nand TCDF (Nauta et al., 2019), and a regularized linear VAR model based estimator (Linear; e.g., Basu and\nMichailidis (2015)). For generative model-based ones, we consider variations of L¨owe et al. (2022). Note\nthat the original paper and the accompanying code implementation only handles the case where each entry\nin the latent graph is a categorical variable denoting the “edge type”. Consequently, we adapt the method\nand make necessary modifications to the code, so that it can handle numerical values9. Besides using the\nedge-centric graph2trajectory decoder adopted in Kipf et al. (2018); L¨owe et al. (2022), we also consider\nanother variant based on the proposed node-centric one. These two benchmarks are referred to as One-edge\nand One-node. Note that none of the above-mentioned methods readily handles the multi-entity setting\nwhere all graphs are estimated jointly; hence, for comparison purposes, the estimated grand common graph\nfor the competitors is simply obtained by averaging the estimated entity ones.\n4.1\nData generating mechanisms\nThe data generating mechanisms used are based on: (1) a linear VAR, (2) a non-linear VAR, and (3) multi-\nspecies Lotka-Volterra systems. Two additional mechanisms corresponding to the Lorenz96 and the Springs\nsystems are also considered; their description and results are presented in Appendix B. Consistent with extant\nnotation, p denotes the number of nodes and M the number of entities.\nLinear VAR.\nThe dynamics of a linear VAR(1) model are determined by xt = Axt−1 +εt, xt ∈ Rp, wherein\nA ∈ Rp×p is the transition matrix and coincides with the Granger-causal graph; for notational convenience,\nlet ¯A := ¯z denote the grand common and A[m] := z[m] the entity-specific graphs. For this mechanism, we set\np = 30 and M = 20, while the noise term εt has i.i.d entries drawn from a standard Gaussian distribution.\nWe first discuss the generation of the “initial” common graph ¯A(0), whose skeleton S ¯\nA(0) (i.e., support set) is\ndetermined by independent draws from Ber(0.1); nonzero entries are first drawn from Unif(−2, −1) ∪ (1, 2),\nthen scaled so that the spectral radius (i.e., the maximum in absolute value eigenvalue) of ¯A(0) is 0.5. Next,\nwe generate perturbations of ¯A(0) by “relocating” 10% of the entries (denote their index set by Sptrb) in S ¯\nA(0)\nto random locations in the non-support set Sc\n¯\nA(0). This step generates the corresponding A[m]’s. Note that\nthe perturbation mechanism ensures that Sptrb ⊂ S ¯\nA(0). Further, the positions of the 10% of entries selected\nat random remain fixed for all M entities, and only the “new” locations are randomly selected and hence\ndiffer across the entities, thus inducing heterogeneity across the A[m]’s. As a result of the perturbation, for\n¯A(0), entries in Sptrb are essentially “flipped” to zero, and this gives rise to the final grand common graph ¯A;\nsee also Figure 3a.\nNon-Linear VAR.\nFor this mechanism, we set p = 20 and M = 10. We first describe how ¯z and z[m] are\ngenerated, as they dictate the connections and determine how the dynamics are specified. First, let ¯z(0)\nbe the “initial” common graph, set to a banded matrix that has non-zero entries on the diagonal and the\nadjacent upper and lower diagonals. Next, we perturb ¯z(0) as follows: for all rows not divisible by 3 (e.g.,\nrows, 1, 2, 4, etc.), the two off-diagonal entries are relocated to other positions at random within the same\nrow. This is repeated for all m’s to generate z[m]’s. The perturbation creates a zigzag pattern for the final ¯z,\nsince whenever a perturbation is present, the original off-diagonal entries on the ±1 band are guaranteed\nto get flipped to zero – see Figure 3b for an illustration. Within any entity m, response nodes indexed by\n8The selection of these competitors is based on the results reported in Marcinkeviˇcs and Vogt (2021). Specifically, we picked the\nones that were demonstrated to be competitive. The code implementations for these competitors (except for the regularized Linear\nVAR) are directly taken from the repositories accompanying the papers.\n9see Appendix A.2 for how the adaptation can be conducted.\n12\ni = 2, · · · , p − 1 have 3 parents; denote their indices by k1\ni < k2\ni < k3\ni with subscript i corresponding to the\nresponse node id and superscript the parent id, and k2\ni ≡ i by construction.\nThe trajectories are generated as follows. For i = 2, · · · , p − 1, let xi,t = 0.25xi,t−1 + sin(xk1\ni ,t−1 · xk3\ni ,t−1) +\ncos(xk1\ni ,t−1 + xk3\ni ,t−1) + εi,t, εi,t ∼ N(0, 0.25). For the first node i and the last node p, their dynamics are\nslightly different given that they only have one “neighbor”10. The choice of such dynamics (in particular,\nusing sine/cosine functions) is somewhat ad-hoc, but aim to induce non-linearities, while ensuring that the\nsystem is stable given that these functions are uniformly bounded. Finally, note that we omit the superscript\n[m] that indexes the entities, as the dynamic specification applies to the dynamical systems of all entities; the\nparent set for each response node i of entity m is dictated by row i of z[m].\nMulti-species Lotka-Volterra system.\nIt comprises of coupled ordinary different equations (ODE) that\nmodel the population dynamics of multiple predators and preys based on their interactions, specified by\nthe corresponding Granger causal graphs.\nWe consider p = 20 and M = 10.\nThe p nodes are sep-\narated equally into preys and predators (i.e.,\np\n2 preys and predators each).\nLet xt := (u⊤\nt , v⊤\nt )⊤ with\nut := (u1,t, · · · , up/2,t)⊤ ∈ Rp/2 and vt := (v1,t, · · · , vp/2,t)⊤ ∈ Rp/2 denoting the population size of the\npreys and the predators at time t, respectively; ui := {ui,t} corresponds to the continuous-time trajectory for\nthe ith coordinate and vj is analogously defined. The dynamics for each coordinate are specified through\nthe following ODE system:\ndui\ndt = αui − βui(\nX\nj∈Pi\nvj) − α(ui/η)2;\ndvj\ndt = δvj(\nX\ni∈Pj\nui) − γvj.\n(13)\nThe parameters are set to α = 1.1, β = 0.2, γ = 1.1, δ = 0.2 and η = 200. Once again, we omit superscript\n[m] as this specification applies to all m = 1, · · · , M. The heterogeneity at the entity level is contingent on\ntheir graphs z[m]’s that dictate the coupling mechanism; in particular, Pi and Pj are the parent set of nodes\ni and j, and are respectively dictated by the support set of the ith and jth rows of the corresponding z[m].\nThe generation mechanism of ¯z and z[m] are described next. The common graph ¯z is generated identically to\nthe one considered in Marcinkeviˇcs and Vogt (2021), where the 20 nodes can be separated into 5 decoupled\nsystems, each containing 2 predators and 2 preys. We add random perturbations to ¯z to arrive at the z[m]’s, by\nadding additional entries. These additional entries in the upper right/lower left blocks need to be symmetric\nw.r.t. the diagonal so that the predator-prey correspondence is respected, and they also provide coupling\nacross the originally decoupled 5 × 4 systems – see also Figure 3c for an illustration.\n4.2\nPerformance evaluation\nFor all settings, we consider sample sizes of 10K. We run 5 data replicates and report the mean and standard\ndeviation of the AUROC and AUPRC metrics for the competing methods considered. Given that the under-\nlying true Granger-causal graphs in the examined settings are sparse, we also report the best attainable F1\nscore for each method after thresholding the entries of the group and entity-specific graphs. Results for\ntwo other experimental settings, —the Lorenz96 and the Springs systems—, are presented in Appendix B.1.\nAdditional metrics such as true positive rate (TPR), true negative rate (TNR) and accuracy (ACC) based on\ndifferent thresholding levels are deferred to Appendix B.2, together with visual illustrations of the estimates\nobtained by good performing competitors.\nTable 1 displays the results for all methods. The proposed framework is referred to as Multi-node and Multi-\nedge, corresponding to the multi-entity joint learning approaches using the node- and edge-centric decoders,\nrespectively; a visualization of the estimated ¯z and z[1], z[2] for illustration purposes is provided in Figure 3\nfor the former.\nThe main findings are as follows: (1) the proposed joint-learning approach clearly outperforms its individual\nlearning counterpart (e.g., Multi-node vs. One-node), both at the entity level and the group level (i.e.,\n10For i = 1, the dynamics is given by x1,t = 0.4x1,t−1 − 0.5x2,t−1 + ε1,t; for i = p, the dynamics is given by xp,t = 0.4xp,t−1 −\n0.5xp−1,t−1 + εp,t\n13\n(a) Linear VAR; p = 30, M = 20.\n(b) Non-linear VAR; p = 20, M = 10. Note that as the non-linearity is induced via sinusoidal functions, we do not know the true sign of the cross lead-lag dependency; as such, the entries corresponding to edges\nthat are present are colored in black.\n(c) Multi-species Lotka-Volterra; p = 20, M = 10.\nFigure 3: True (shaded panel on the left) and estimated (non-shaded panel on the right) Granger-causal connections using the proposed framework with\nnode-centric decoder (Multi-node); from left to right: ¯z, z[1] and z[2] and their estimated counterparts.Nonzero entries in z[1], z[2] (and bz[1], bz[2], resp.)\nthat overlap with those in ¯z (b¯z) have been grayed-out so that the idiosyncratic ones stand out.\nTable 1: Performance evaluation for the estimated ¯z and z[m]’s: “common” corresponds to ¯z and “entity(avg)” the z[m]’s after averaging the performance\nmetric across m = 1, · · · , M. Numbers are in % and rounded to integers, and correspond to the mean results based on 5 data replicates; standard\ndeviations are reported in the parenthesis.\nGenerative model-based\nPrediction model-based\nMulti-node\nMulti-edge\nOne-node\nOne-edge\nNGC-cMLP\nGVAR\nTCDF\nLinear\nLinear VAR\ncommon\nAUROC\n100(0.0)\n100(0.0)\n95(6.6)\n98(4.8)\n100(0.4)\n100(0.0)\n79(2.0)\n100(0.0)\nAUPRC\n100(0.0)\n100(0.0)\n83(20.4)\n91(15.9)\n99(1.3)\n100(0.0)\n50(7.6)\n100(0.0)\nF1(best)\n100(0.0)\n100(0.0)\n81(17.4)\n88(15.9)\n96(3.5)\n100(0.0)\n52(5.1)\n100(0.0)\nentity\nAUROC\n100(0.1)\n99(0.6)\n100(0.1)\n100(0.1)\n96(1.8)\n100(0.0)\n77(1.4)\n100(0.0)\n(avg)\nAUPRC\n99(0.3)\n95(2.4)\n99(0.2)\n98(0.4)\n86(4.4)\n99(0.1)\n36(5.5)\n100(0.0)\nF1(best)\n97(0.8)\n90(3.5)\n96(0.6)\n95(1.0)\n79(4.7)\n99(0.4)\n44(3.4)\n100(0.0)\nNon-linear VAR\ncommon\nAUROC\n99(0.2)\n82(1.7)\n97(0.2)\n93(0.8)\n90(0.7)\n99(0.1)\n75(1.0)\n99(0.1)\nAUPRC\n96(0.9)\n58(1.1)\n80(0.8)\n80(8.0)\n64(1.1)\n98(0.2)\n53(0.5)\n98(0.1)\nF1(best)\n94(0.6)\n60(0.7)\n74(1.0)\n83(6.9)\n61(0.9)\n98(0.7)\n56(1.2)\n98(0.7)\nentity\nAUROC\n98(0.3)\n85(0.9)\n94(0.4)\n95(0.5)\n94(0.5)\n99(0.3)\n73(0.9)\n96(0.7)\n(avg)\nAUPRC\n93(1.0)\n75(0.8)\n76(0.2)\n89(0.6)\n87(0.6)\n96(0.6)\n44(1.8)\n96(0.7)\nF1(best)\n86(1.5)\n73(1.0)\n70(0.3)\n86(0.8)\n82(0.4)\n91(0.8)\n50(1.5)\n97(0.6)\nLotka-Volterra\ncommon\nAUROC\n100(0.0)\n100(0.0)\n97(1.1)\n87(8.4)\n100(0.0)\n100(0.0)\n79(0.8)\n100(0.1)\nAUPRC\n100(0.0)\n100(0.1)\n92(3.0)\n73(10.5)\n100(0.0)\n100(0.0)\n58(1.2)\n100(0.4)\nF1(best)\n100(0.7)\n99(0.8)\n87(5.4)\n69(9.0)\n100(0.4)\n97(1.2)\n53(1.4)\n94(3.5)\nentity\nAUROC\n89(1.0)\n84(1.3)\n83(1.6)\n75(1.3)\n92(1.0)\n93(0.6)\n72(0.8)\n77(1.0)\n(avg)\nAUPRC\n80(1.5)\n70(2.0)\n69(1.8)\n51(2.6)\n87(1.2)\n89(1.0)\n41(1.0)\n71(1.2)\nF1(best)\n74(1.4)\n65(2.0)\n63(1.4)\n53(2.2)\n84(0.8)\n84(0.7)\n46(0.3)\n71(0.7)\n14\nthe common graph). (2) The node-centric decoder consistently outperforms its edge-centric counterpart\n(e.g., Multi-node vs. Multi-edge). (3) If one focuses only on individual learning methods, the ones based\non prediction models tend to exhibit superior performance (e.g., GVAR/NGC vs. One-node). In addition,\ndespite the presence of non-linear dynamics, the regularized linear VAR model exhibits surprisingly good\nperformance, especially for the common structure. (4) For practical purposes, post-hoc averaging of the\nentity-specific Granger causal graphs is reasonably effective for extracting the common structure.\nRemark 4 (On the robustness with respect to sample size). The proposed joint-learning framework is ade-\nquately robust to sample sizes. In particular, in the case where the training sample size reduces to 3000,\nMulti-node shows little degradation in its performance in recovering ¯z (within 1% across all settings in\nAUROC), and its performance degradation in recovering the entity-level z[m]’s are within 2% for the same\nmetric. On the other hand, One-node shows a material deterioration in performance especially for the es-\ntimated ¯z (as large as 5% for more challenging settings such as the Lotka-Volterra system), although at the\nindividual entity level, the deterioration is of a smaller magnitude at around 3%. In Appendix B.4 additional\ncomments on the “minimum sample size required” are provided from a practitioner’s perspective.\nFinally, we remark that GVAR exhibits consistently strong performance amongst the methods under consider-\nation. On the other hand, it is observed during evaluation time that given the magnitude of the estimated\nentries, the quality of the graph skeleton is sensitive to the exact choice of the thresholding level, whereas\nthe proposed framework is more robust. This has implications on the difficulty of choosing a good threshold\nin practice — see also Table 4 and additional discussion and remarks in Appendix B.2.\n5\nApplication to a Multi-Subject EEG Dataset\nThe dataset in consideration corresponds to electroencephalogram (EEG) measurements obtained from 72\nactive electrodes placed on the scalp of 22 subjects (entities), and they are publicly available; see Trujillo\net al. (2017). Prior investigation on this dataset primarily centers around understanding the information\nprovided by different connectivity measures that are available in the literature, rather than the connectivity\npatterns themselves.\nThe EEG experiment pertains to a stimulus procedure performed on the subjects comprising of 1-min in-\nterleaved sessions with eyes open (EO) or closed (EC). Such experiments aim to provide insights into the\nbrain’s functional segregation and integration (Barry et al., 2007; Rubinov and Sporns, 2010; Miraglia et al.,\n2016). Note that (1) the experiment is integrated, but the data are collated separately for the eyes-open and\nthe eyes-closed interleaving sessions, which results in two datasets (EO and EC, respectively); and (2) due\nto the design of the experiment, the dynamics governing the data within the EO sessions (respectively, EC\nsessions) are stable and stay largely unchanged.\nWe select to analyze the data from 31 specific EEG channels (and hence p = 31) located at the back of the\nscalp (see Figure 4), where the primary visual cortex is located. For both datasets, we restrict the analysis\nto entities that have at least 40000 observations (total number of time points)11, and the whole trajectory is\nfurther partitioned into training/validation data, with the latter having 2000 time points. Here the validation\ndata is used to select the best hyperparameters such that the reconstruction or prediction error is minimum\nover the search grid, depending on the method. Four methods are considered, including the proposed joint-\nlearning one with a node-centric decoder (Multi-node), its individual-learning counterpart (One-node) and\nprediction model-based GVAR and NGC.\nThe estimated common Granger-causal connections based on Multi-node and GVAR are depicted in Figures 4\nand 5, respectively. The results based on One-node and NCG are delegated to Appendix D12. For all methods,\nwe threshold the raw estimates to remove very small entries; the thresholding values are chosen so that\n11This restriction has reduced the number of entities to 21 for the EO dataset while the number of entities for the EC dataset remains\nat 22.\n12Note that the Granger causal connections estimated by Multi-node and One-node are up to a “complete sign flip” (see, e.g.,\ndiscussion in Appendix E); nonetheless, these methods are effective in distinguishing positive (negative) connections from negative\n(positive) ones. Further, NCG does not provide signed estimates (positive/negative) of the Granger causal connections, unlike the other\nthree methods.\n15\n(a) Eyes Open (EO)\n(b) Eyes Closed (EC)\nFigure 4: Multi-node results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent\nthresholding at 0.15. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is propor-\ntional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in\ngray.\n(a) Eyes Open (EO)\n(b) Eyes Closed (EC)\nFigure 5: GVAR results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent thresh-\nolding at 0.05. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is proportional to\nthe strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in gray.\neach method has around 400 total number of edges for the EC session to facilitate comparisons across them.\nResults from these methods exhibit commonalities and differences, as discussed next.\nThe following common observations are noted across most methods: (1) based on the results by Multi-node,\nGVAR and One-node, the overall Granger causal connectivity level is markedly higher for the EC session\ncompared to the EO one; this is consistent with results in studies in the literature (Barry et al., 2007; Marx\net al., 2004; Das et al., 2016; Trujillo et al., 2017), albeit using different connectivity measures. On the other\nhand, results from NCG show the reverse pattern, i.e., higher connectivity for the EO session compared to\nthe EC session. (2) For both the EO and EC sessions, the in-degree of nodes in the mid-line channels (i.e.\nOZ, POZ, PZ, CPZ) tends to be higher than that of the nodes to the left and right parts of the brain. This\nis broadly comparable to results in the literature—see, e.g., Barry et al. (2007) for adult subjects and Barry\net al. (2009) for children, though the problem under consideration and thus the analysis is different in their\nwork. (3) As it is observed in Multi-node, One-Node, and NGC, the OZ channel exhibits different degree of\nconnectivity for the EO and the EC sessions; in particular, it is Granger causal for many other channels in the\nformer, i.e., being the emitter of edges and exhibit higher node out-degree; this becomes significantly less so\nin the latter—see also Hatton et al. (2023).\nThe four methods also exhibit certain discrepancies in their results. (1) As mentioned above, the EO session\nexhibits an overall decrease in connectivity when compared against the EC session. The drop in connectivity,\nhowever, is not uniform across nodes on the left and the right parts of the brain. This is reported by generative\nmodel-based methods Multi-node and One-node, and a result also mentioned in the literature (Barry et al.,\n2007, 2009; Modarres et al., 2023). Such discrepancy—in terms of the differential change in connectivity\nlevel between the nodes on the left and those on the right—is significantly less pronounced in GVAR and NGC.\n(2) A strong bi-directional Granger causal link between channels M1 and M2 in both the EO and EC sessions\nis observed according to GVAR. This strong connection is somewhat harder to interpret, since these two\n16\nchannels correspond to mastoid (behind the ears) locations and their connectivity is customarily modulated\nthrough the midline positioned ones (OZ, PZ, POZ, CPZ) (Das et al., 2022). (3) As a minor remark, for GVAR,\nwe observe strong autoregressive connections (i.e., dominant diagonals in the estimates)13; for NGC, the\noverall connectivity level in the raw estimates is significantly higher and thus requires stronger thresholding.\nBoth observations are also noted in selected synthetic data experiments.\nIn summary, all methods with the exception of NGC are in agreement regarding the decrease in Granger\ncausal connections from the EC to the EO session. There is also concordance across methods regarding the\nobservation that this decrease is not uniform across the left and right parts of the brain. Both of these results\nare in accordance to previous ones in the literature, although based on different analysis techniques and\nconnectivity measures.\n6\nDiscussion\nThis paper proposes a multi-layer VAE-based framework for jointly estimating the group and entity-level\nGranger-causal graphs, in the presence of connectivity heterogeneity across entities. The framework is based\non a hierarchical generative structure that couples the group and entity-specific graphs. The model is learned\nvia an end-to-end encoding-decoding procedure that minimizes the negative ELBO loss. The results of the\nnumerical experiments show that the performance of the proposed framework is broadly robust to sample\nsize, especially for the common graph. Further, the joint learning paradigm has a clear advantage over its\n“individual learning” generative model-based counterpart, which then leads to more accurate quantification\nfor both the common connectivity patterns and the idiosyncratic ones. This advantage becomes more pro-\nnounced in settings where one has limited sample size and large collections of related systems. In addition,\nthe joint learning paradigm can be useful in situations, where one may be interested in detecting “outlier”\ndynamical systems in the collection under consideration, or in identifying clusters of such systems. These\ntasks can be accomplished by close examination and analysis of the entity specific graphs.\nAlthough “prediction models plus post-hoc aggregation” heuristics can sometimes exhibit competitive per-\nformance, the embedded common structure across entities is completely neglected at the formulation level.\nIn addition, existing models within this framework are also limited to scalar-valued nodes, partly due to\ntheir reliance on performing ad-hoc extraction/aggregation on intermediate quantities (e.g., neural network\nweight matrices during training) to infer the Granger causality.\nIn the presence of non-linearity, a key advantage of generative model-based approaches is that the Granger-\ncausal relationships are solely encoded through the latent graph that serves as the gateway for information\npropagation.\nThis provides a clean way to model relationships between connectivity patterns — either\nstatically or dynamically. The setting considered in this work is a static one, and the type of such relationship\nmanifests as common-idiosyncratic connectivity patterns. A potential extension to the generative process\nunder consideration, suitable for more complex real-world dynamical systems, is to allow for time-varying\nconnectivity patterns. For example, Graber and Schwing (2020) extends the work in Kipf et al. (2018)\nto a dynamic setting. With appropriate modifications to the proposed approach, such as expanding the\nconditional relationship of the graphs dictated in (2) so that they also depend on their past, this modeling\ntask can be handled in a straightforward manner.\n13this is not shown in the plot (to avoid self-loops) for aesthetic purposes. Note also that visually, the edges are overall more “faint”\nin the plot, as a result of the dominant diagonals and the corresponding normalization.\n17\nA\nAdditional Modeling and Implementation Details\nIn this section, we provide a description for some additional modeling details. In Sections A.1 and A.2, we\nomit superscript [m] that indexes the entities whenever there is no ambiguity, as the descriptions therein\napply to all m’s independently unless otherwise specified.\nA.1\nEncoder\nWe provide details for the encoder sub-module that is abstracted as fx→h, wherein based on the node tra-\njectories, one obtains the hidden representations for the edges {hij} := fx→h(x); see also Section 3.2,\nmodule (enc-a).\nAs the most basic building blocks of message-passing operations, “node2edge” and “edge2node” operate\nbased off a complete graph, and can be generically represented as:\neij ← concat(xi, xj) (node2edge);\nxi ←\nX\nj eij (edge2node),\nwith xi denoting the node representation and eij the edge one. fx→h is then parameterized through the L\npasses of such operations:\n(init emb) :\nˇx(0)\ni\n← emb(xi),\n∀i = 1, · · · , p\nˇx → e :\ne(l)\nij ← MLP\n\u0000node2edge(ˇx(l−1)\ni\n, ˇx(l−1)\nj\n)\n\u0001\n;\nl = 1, · · · , L\ne → ˇx :\nˇx(l−1)\ni\n← MLP\n\u0000edge2node(e(l)\nij ; j = 1, · · · , p)\n\u0001\n;\nl = 2, · · · , L\nHere xi corresponds to the trajectory of node i over time, that is, xi = (xi,1, · · · , xi,T ) and the final hidden\nrepresentation is given by hij := e(L)\nij , i, j = 1, · · · , p.\nConcretely, the embedding module can be as simple as entailing only Linear-ReLU type operations; the input\ntrajectory xi of a node i, ∀i ∈ {1, · · · , p}, is processed via the following steps outlined in Figure 6:\nxi\nLinear\nReLU\nDropout\nLinear\nˇx(0)\ni\n(flatten)\nembedding module\nFigure 6: Example for the embedding operation in the encoder according to MLP style. Blocks with trainable parameters are outlined in red. Note that the\nflattening step is only required when the nodes are vector-valued.\nNote that this also coincides with the LR-type embedding functions in Gorishniy et al. (2022). In regards to\nthe MLP block, it is obtained by stacking the sub-blocks as illustrated in Figure 7.\ninput\nLinear\nReLU\nDropout\nLinear\nReLU\nDropout\nLinear\noutput\nMLP sub-block\nMLP sub-block\nFigure 7: An MLP block obtained by stacking the sub-blocks. Constituent blocks with trainable parameters are outlined in red.\nFigure 8 provides a pictorial illustration for the sequential operations entailed in the Trajectory2Graph en-\ncoder module.14 Note that this is effectively the MLPEncoder used in Kipf et al. (2018) and the description\nis given here for the sake of completeness. We refer interested readers to Kipf et al. (2018) for some other\nencoders considered therein.\n14In our experiments, all the MLP blocks used in the Trajectory2Graph operations are kept simple with only one single sub-block; the\nhidden dimension is set at 128 or 256, depending on the exact experiments.\n18\nx1\nx2\nxp−1\nxp\nˇx(0)\n1\nˇx(0)\n2\nˇx(0)\np−1\nˇx(0)\np\nh11 h12 · · · h1p\nh21 h22 · · · h2p\n...\n...\n...\n...\nhp1 hp2 · · · hpp\nz11 z12 · · · z1p\nz21 z22 · · · z2p\n...\n...\n...\n...\nzp1 zp2 · · · zpp\nnode2edge\nGNN-MLP1\nedge2node\nGNN-MLP2\nnode2edge\nGNN-MLP3\nEmbedding\nMLP\nGNN-style message passing\nFigure 8: Diagram for the Trajectory2Graph encoder operations. Blocks with trainable parameters are outlined in red.\nA.2\nDecoder\nWe divide this subsection into two parts, that respectively (1) discuss how the structure adopted in a node-\ncentric Graph2trajectory module described in Section 3.2 can readily accommodate the presence of depen-\ndence on more than 1 lags; and (2) provide a brief discussion on how the original edge-centric decoder\nadopted in Kipf et al. (2018); L¨owe et al. (2022) can be revised to adapt to the case of a numerical graph,\nand compare it with the node-centric one, although architectural choices are not the focus of this paper.\nExtension to multiple lag dependency.\nThe extension of a node-centric decoder to accommodate the\npresence of more than 1 lags (i.e., q > 1) is straightforward, largely due to the fact that the node value\nat time t − 1, denoted by xj,t−1 is not limited to be scalar-valued in the first place. In the case of q-lag\ndependency, one can simply replace xj,t−1 by concat(xj,t−1, · · · , xj,t−q) and proceed with the remainder of\nthe operations as outlined in (9) and (10). In particular, with the presence of more lags, as an alternative to\na (optional) numerical embedding step, one can instead consider 1D-CNN as a preprocessing module on the\n“new” xj,t−1, before an element-wise gate represented by zij is applied to control the information flow.\nAdaptation of the edge-centric decoder.\nThe original edge-centric decoder adopted in Kipf et al. (2018)\nhandles the case where each entry in zij corresponds to an edge type (categorical), and it entails the follow-\ning operations:\n1. node2edge for each time step, that is eij,t−1 := concat(xi,t−1, xj,t−1) to arrive at the edge representa-\ntion at time t − 1;\n2. for each edge type of interest, run eij,t−1’s through its corresponding edge type-specific function (e.g.,\nMLP) to get the “enriched” representation ˇeij,t−1;\n3. aggregate the enriched edge representations back to nodes via an edge2node operation, giving rise to\nvi,t−1’s, i = 1, · · · , p; vi,t−1 then serves as the predictor for time-t response xi,t.\nIn order for the above module to accommodate the case of a numeric zij, the following simple modification\nto step 2 is introduced:\n2’ run eij,t−1’s through some function (e.g., MLP) to get the “enriched” representation ˇeij,t−1, and further\nupdate it through a gating mechanism as dictated by zij, that is, ˇeij,t−1 ← ˇeij,t−1 ◦ zij.\nThe information propagation path from node j to i can be represented as:\nxj,t−1\nnode2edge\n→\neij,t−1\nMLP\n→ ˇeij,t−1\ngating\n→ ˇeij,t−1 ◦ zij\nedge2node\n→\nvi,t−1 → xi,t;\n(14)\none can easily verify that for zij = 0, there is no path from xj,t−1 to xi,t.\nAs a final remark, for the node-centric decoder, the gating through zij directly operates on the node repre-\nsentation, and the path is given by\nxj,t−1\nemb\n→ ˇxj,t−1\ngating\n→ ˇxj,t−1 ◦ zij\nelement of\n→\nui,t−1 → xi,t;\n19\nsee also Figure 9 for a pictorial illustration.\nx1\nx2\nxp−1\nxp\nˇx1\nˇx2\nˇxp−1\nˇxp\nzi1\nzi2\nzi(p−1)\nzip\nu1\ni\nu2\ni\nup−1\ni\nup\ni\nMLP\nxi\n◦\n◦\n◦\n◦\npast time ≤ (t − 1)\ntime t\nnumerical embedding\nFigure 9: Diagram for the node-centric Graph2Trajectory Decoder operations, with node i being the response node in this illustration. The corresponding\nentries of z’s (shaded in gray, obtained by sampling and is fed into the Graph2Trajectory Decoder as input) perform the gating operation (denoted by ◦).\nBlocks with trainable parameters are outlined in red and are shared across all response nodes i = 1, · · · , p.\nTo contrast, for the edge-centric decoder, as indicated in (14), entries in zij determine the lead-lag informa-\ntion passing from j → i via eij,t−1, and therefore such a gating mechanism is somewhat circumstantial.\nA.3\nLoss calculation\nA derivation of (12) is given next.\nKL\n\u0010\nqϕ(Z|X)\n\r\r pθ(Z)\n\u0011\n= Eqϕ(Z|X) log\nhqϕ(Z|X)\npθ(Z)\ni\n= Eqϕ(Z|X)\nh\nlog qϕ(¯z|{z[m]})\npθ(¯z)\n+ log qϕ({z[m]}|{x[m]})\npθ({z[m]}|¯z)\ni\n=\nZZ\nqϕ\n\u0000¯z|{z[m]}\n\u0001\nqϕ\n\u0000{z[m]}|{x[m]}\n\u0001\nlog\nhqϕ\n\u0000¯z|{z[m]}\n\u0001\npθ\n\u0000¯z\n\u0001\ni\nd¯zd{z[m]}\n+\nZZ\nqϕ\n\u0000¯z|{z[m]}\n\u0001\nqϕ\n\u0000{z[m]}|{x[m]}\n\u0001\nlog\nhqϕ\n\u0000{z[m]}|{x[m]}\n\u0001\npθ\n\u0000{z[m]}|¯z\n\u0001\ni\nd¯zd{z[m]}\n=\nZ\nqϕ({z[m]}|{x[m]})\nn Z\nqϕ\n\u0000¯z|{z[m]}\n\u0001\nlog\nhqϕ\n\u0000¯z|{z[m]}\n\u0001\npθ\n\u0000¯z\n\u0001\ni\nd¯z\no\n|\n{z\n}\nKL\n\u0010\nqϕ(¯z|{z[m]})\n\r\r pθ(¯z)\n\u0011\nd{z[m]}\n+\nZ\nqϕ\n\u0000¯z|{z[m]}, {x[m]}\n\u0001 n Z\nqϕ\n\u0000{z[m]}|{x[m]}\n\u0001\nlog\nhqϕ\n\u0000{z[m]}|{x[m]}\n\u0001\npθ\n\u0000{z[m]}|¯z\n\u0001\ni\nd{z[m]}\no\n|\n{z\n}\nKL\n\u0010\nqϕ({z[m]}|{x[m]})\n\r\r pθ({z[m]}|¯z)\n\u0011\nd¯z\n(a)\n= Eqϕ({z[m]}|X)\nh\nKL\n\u0010\nqϕ(¯z|{z[m]})\n\r\r pθ(¯z)\n\u0011i\n+ Eqϕ(¯z|X)\nh\nKL\n\u0010\nqϕ({z[m]}|{x[m]})\n\r\r pθ({z[m]}|¯z)\n\u0011i\n.\nFor (a), the first term is straightforward, the second term goes through since\nZ\np(x|y, z)\nn Z\np(y|z) log p(y|z)\nq(y|x)dy\no\ndx =\nZZ\np(y|z)p(x|y, z) log p(y|z)\nq(y|x)dxdy\n= EY |ZEX|Z,Y log p(y|z)\nq(y|x) = EY |ZEX|Z log p(y|z)\nq(y|x) = EX|Z\nh\nEY |Z log p(y|z)\nq(y|x)\ni\n;\nand the last equality holds as a result of the Fubini-Tonelli theorem.\nA.4\nEvaluating the predictive strength of Granger causal relationships\nNext, we briefly discuss how the trained decoder can be used to measure the predictive strength of the\nGranger causal connections.\n20\nOnce the model is trained, using the inference procedure described in Section 3.3, one obtains estimates ˆz[m]\nfor all entity-specific graphs. Further, a trained Graph2Trajectory module, abstracted as ˆgz→x, also becomes\navailable. The predictive strength of any connection entry (i, j) — corresponding to the lead-lag relationship\nfrom j to i — can then be assessed by nullifying the corresponding entry. Throughout the remainder of the\ndiscussion, we omit superscript [m] for ease of presentation, as the procedure is applicable to an arbitrary\nentity of interest.\nLet ˜z(ij) be identical to ˆz except that the (i, j) entry is set to zero (nullified). The reconstructed trajectories,\nbased on the estimated and the nullified graphs are given by ˆx = ˆgz→x(ˆz, x1)15 and ˜x(ij) = ˆgz→x(˜z(ij), x1),\nrespectively. The predictive strength can then be evaluated based on the difference in the residual-sum-\nof-squares (RSS), with the latter obtained by evaluating the reconstructed trajectory against the observed\nvalues. Concretely, RSS(ˆx) can be obtained by\n1\nT −1\nPT\nt=2 ∥xt − ˆxt∥2 and that for ˜x(ij) can be analogously\nobtained; the predictive strength of the (i, j) connection can then be calculated as RSS(ˆx) − RSS(˜x(ij)). This\nprocedure can be generalized to a set of connections, where instead of nullifying a single entry, multiple\nentries are nullified simultaneously and the remainder of the evaluation follows. Note that the proposed\nprocedure resembles that of testing for the presence/absence of Granger causality in linear VAR models,\nwhere an F-test is used (Geweke, 1984). The calculated difference RSS(ˆx) − RSS(˜x(ij)) also appears in the\nnumerator of the aforementioned F-statistic.\nA.5\nConstruction of samples\nWe briefly explain how samples are constructed from observed data trajectories. We omit the superscript [m]\nthat corresponds to the entity ID, since the construction is generally applicable.\nThe available data can either correspond to a collection of long trajectories (e.g., traditional time series set-\nting where observations for different variables are collected over time), or to multiple collections of (long)\ntrajectories, where each collection corresponds to temporal observations over time from repeated measure-\nments (e.g., in the context of a neurophysiological experiment, a subject is exposed to a stimulus (eyes open\nor eyes close) a number of times). In both cases, the trajectories are parsed into shorter ones of length T,\nwhich is the context window considered in the modeling. Concretely, let {x0, x1, · · · , x e\nT } be the long trajec-\ntory, with eT denoting the total number of observations. The samples, indexed by n, are shorter trajectories\nof length T, with each consisting of observations X (n) := {xsn, xsn+1, · · · , xsn+T −1}, where s is the stride\nsize that dictates the overlapping between samples with adjacent indices. A long trajectory of length eT gives\nrise to ⌊( eT − T)/s + 1⌋ samples, which are then used during mini-batch training.\nB\nAdditional Synthetic Data Experiments and Results\nB.1\nLorenz96 and Springs5 experiments\nTo explore the applicability of the proposed framework to selected special cases, there are two other settings\nconsidered in our synthetic data experiments: the Lorenz96 and the Springs5 systems. Unlike the settings\npresented in the numerical experiments in Section 4 wherein the entity-level heterogeneity manifests itself\nprimarily in the form of perturbations to the skeleton of the shared common graph, for these two systems,\nthe entity-specific skeletons are either identical across all M entities and only the magnitude of the entries\nchanges (Lorenz96), or they manifest their heterogeneity through a probabilistic mechanism (Springs), as\nexplained in the sequel.\nSimilar to those presented earlier, for both settings, we run the experiments on 5 data replicates and re-\nport the metrics after averaging across the 5 runs, with their respective standard deviation included in the\nparentheses.\n15Recall that throughout the main sections, we use x := {x1, · · · , xT } to denote the trajectory; here ˆx is effectively its “reconstructed”\ncouterpart.\n21\nB.1.1\nThe Lorenz96 system\nThe Lorenz96 system (Lorenz, 1996) has been previously investigated in Tank et al. (2021); Marcinkeviˇcs\nand Vogt (2021). The dynamics for a p-variable system evolve according to the following ODE:\ndxi\ndt = (xi+1 − xi−2)xi−1 − xi + F,\ni = 1, · · · , p,\n(15)\nFigure 10: Lorenz96: ¯z and z[0], showing only the skeleton.\nwhere xi := {xi,t} denotes the continuous time trajectory of\nnode i with x0 := xp, x−1 := xp−1 and xp+1 := x1. Such a sys-\ntem corresponds to a Granger-causal structure shown in Fig-\nure 10 that depicts its skeleton. The representation in (15) can\nbe obtained from Kerin and Engler (2022):\ndxi\ndt = α(xi+1 − xi−2)xi−1 − βxi + γ,\n(16)\nby reparameterizing α = β, λ = α/β and setting F = αγ/β2. F\nis the forcing constant that controls the degree of non-linearity;\nin particular, given the relationship between (15) and (16), as\nF varies, the strength of the Granger-causality changes despite an invariant skeleton. In other words, to\ninduce heterogeneity across entities, we can only change the parameter F that induces heterogeneity in the\nmagnitudes of the Granger causal connections, while the skeleton of the Granger causal graph remains the\nsame. We consider a setting with p = 20 and M = 5 entities, with the forces taking the following values:\nF ∈ {10.0, 17.5, 25.0, 32.5, 40.0}.\nTable 2: Performance evaluation for the estimated ¯z and z[m]’s for setting Lorenz96. Numbers are in % and rounded to integers, and correspond to the\nmean results based on 5 data replicates; standard deviations are reported in the parentheses.\nGenerative model-based\nPrediction model-based\nMulti-node\nMulti-edge\nOne-node\nOne-edge\nNGC-cMLP\nGVAR\nTCDF\nLinear\ncommon\nAUROC\n100(0.1)\n100(0.7)\n100(0.1)\n90(19.7)\n97(0.0)\n100(0.1)\n82(0.9)\n99(0.1)\nAUPRC\n100(0.4)\n99(1.6)\n100(0.3)\n82(32.5)\n87(0.1)\n100(0.2)\n65(0.9)\n97(0.5)\nF1(best)\n97(1.5)\n96(3.4)\n97(1.3)\n80(25.7)\n87(0.8)\n98(1.0)\n59(1.3)\n89(0.2)\nentity\nAUROC\n95(1.3)\n85(3.7)\n96(1.0)\n88(1.9)\n96(0.1)\n97(0.8)\n79(0.8)\n99(0.1)\n(avg)\nAUPRC\n89(2.3)\n76(4.6)\n91(2.0)\n78(2.9)\n85(0.3)\n90(1.5)\n62(0.7)\n96(0.3)\nF1(best)\n82(3.2)\n71(3.5)\n84(2.6)\n72(3.1)\n83(0.4)\n83(0.2)\n58(0.5)\n88(0.3)\nThe results are shown in Table 2 and the main findings are: (1) consistent with the results in Section\n4, the node-centric decoder outperforms the edge-centric one; (2) the proposed joint-learning approach\nMulti-node matches the performance of GVAR and outperforms all other competitors for the common graph;\n(3) for the entity-specific graphs, interestingly, the linear VAR exhibits a slight edge over all competing\nmethods, while the performance of the proposed model is broadly on-par with the remaining competitors.\nFigure 11: Estimated ¯z and z[m]’s with different F ’s using the proposed joint-learning framework with a node-centric decoder (Multi-node).\nFinally, the common and the five entity-specific Granger causal graphs for the Multi-node method are de-\npicted in Figure 11. It can be seen that the performance deteriorates for systems with larger external force\nF.\n22\nB.1.2\nSprings5 system\nFigure 12: Springs5: ¯z and z[0]. z[0] is binary (and sym-\nmetric) with entries generated according to Bernoulli distri-\nbutions.\nThis setting is investigated in Kipf et al. (2018); L¨owe et al.\n(2022), and in this work we consider a “multi-entity” version of\nit. In the original setting, particles (i.e., nodes) are connected\n(pairwise) by springs at random with probability 0.5; in the\ncase where the connection between particles i and j is present,\nthey interact according to Hooke’s law Fij = −k(ri−rj), where\nFij is the force applied to particle i by particle j, k is the\nspring constant and ri is the location vector of particle i in 2-\ndimensional space. With some initial location and velocity, the\ntrajectories can be simulated by solving Newton’s equations of\nmotion (see also Kipf et al. (2018), Appendix B for details).\nCrucially, (1) the Granger-causal graph is essentially a realiza-\ntion of the homogeneous Erd˝os-R´enyi graph (Erd˝os and R´enyi,\n1959) with edge probability being 0.5, and (2) each node’s trajectory is multivariate with 4 dimensions, that\nis, xi,t ∈ Rd, d = 4; the first 2 dimensions correspond to the velocity and the last 2 to the location in the\n2-dimensional space.\nThe extension to the “multi-entity” case that is suitable for the setup considered in this paper is described\nnext, and it differs primarily from the original one in how the Granger-causal connections across nodes\nare generated. Specifically, we start from ¯z, whose entries (i, j) in its upper-triangular part are generated\nindependently from Beta(1, 1); then set ¯zji ≡ ¯zij, i < j so that it’s symmetric. For the z[m]’s, let z[m]\nij\n∼\nBer(¯zij), i < j, and then set z[m]\nji\n≡ z[m]\nij , ∀ m = 1, · · · , M. Once z[m]’s are generated, they dictate the\nconnections between nodes in their respective systems, and one can proceed with the same procedure as in\nthe original setting to simulate the trajectories. Note that (1) each entity’s Granger-causal graph corresponds\nto a realization of a heterogeneous Erd˝os-R´enyi graph; the edge probability differs across node pairs and\ndepends on the corresponding entry in ¯z that is a realization from the Beta distribution, and (2) the grand\ncommon structure possesses a “probabilistic” interpretation, in that it effectively captures the expectation of\nan edge being present/absent across all entities. In this experiment, we set p = 5 and M = 10.\nNone of the competitors based on the prediction models can readily handle this setting16, and therefore we\nonly present results for those based on generative models. Note that in this experiment, despite that the\nunderlying true graphs are symmetric, we do not use this information during our estimation.\nTable 3 shows the results for the above-mentioned systems, using both the node- and the edge-centric de-\ncoders.\nA visualization of the estimates is provided in Figure 13.\nOverall, the proposed joint learning\nframework outperforms individual learning for entity-level graphs, while the performance is largely compa-\nrable for the common graph estimate. Given the physics system nature of this dataset (vis-a-vis time series\nsignals), the edge-centric decoder has a small advantage over the node-centric one; this is manifested by the\nfact that under the joint learning framework, the two decoders show comparable performance, whereas the\nedge-centric decoder is clearly superior in the case of single-entity separate learning. Note that this points\nto another potential advantage of the joint-learning framework, in that it is more robust and exhibits less\nvolatility than individual learning.\nTable 3: Performance evaluation for the estimated ¯z (error in Frobenius norm) and z[m]’s (accuracy and F1 score after thresholding at 0.5, averaged across\nall entities) for the Springs5 system.\nquantity\nmetric\nMulti-node\nMulti-edge\nOne-node\nOne-edge\ncommon\nERR-fnorm\n1.00(0.259)\n0.92(0.294)\n1.30(0.412)\n0.79(0.217)\nentity(avg)\nACC%\n99.3(0.84)\n99.3(0.76)\n87.5(6.45)\n96.3(3.99)\nentity(avg)\nF1Score%\n99.5(0.79)\n99.4(0.73)\n88.2(7.45)\n96.3(4.78)\n16There are two issues that the prediction model-based competitors can not readily handle and would require major changes: (1) all\nof them assume that the Granger-causality to be estimated is numeric and therefore does not naturally handle the binary case, and (2)\nat any point in time, each node is assumed to have a scalar value, akin to classical time-series settings, whereas here each node is vector\nvalued; consequently, the existing code does not readily handle it.\n23\nFigure 13: Estimated ¯z and z[m]’s (showing the first five) using the proposed framework with node-centric decoder (Multi-node).\nB.2\nAdditional performance evaluation results and their visualization\nTable 4 presents additional evaluation metrics (TPR, TNR and ACC) for the proposed method and its strong\ncompetitors, after the estimates of the Granger causal graphs are thresholded at various levels no greater\nthan 0.5 (after normalization). We only show the results for the estimated common graph ¯z, since the results\nfor the entity-level ones exhibit similar patterns.\nAs briefly mentioned in Section 4, prediction model-based methods (NGC/GVAR) are more sensitive to the\nvalue of the threshold, manifested by a sudden jump in accuracy once the threshold exceeds a certain level.\nOn the other hand, the change in ACC for the ones based on generative models is more gradual. Given that\nin practice it is common to use a moderate threshold to eliminate small entries of the initial estimates of the\nGranger causal graphs to determine their skeleton, the above-mentioned susceptibility can adversely impact\nthe quality of the final estimate used for interpretation purposes and in downstream analytical tasks.\nTable 4: Performance evaluation for the support set of the estimated common graph ¯z at various threshold levels (left-most column). Numbers are in %,\nand correspond to the mean results based on 5 data replicates.\nMulti-node\nOne-node\nNGC-cMLP\nGVAR\nLinear\nTPR\nTNR\nACC\nTPR\nTNR\nACC\nTPR\nTNR\nACC\nTPR\nTNR\nACC\nTPR\nTNR\nACC\nLinear VAR\n0.10\n100\n92.1\n92.9\n98.1\n50.3\n55.1\n100\n0.0\n10.0\n100\n0.0\n10.0\n100\n99.9\n99.9\n0.20\n100\n99.9\n99.9\n95.8\n78.9\n80.6\n100\n0.0\n10.0\n100\n0.0\n10.0\n100\n100\n100\n0.30\n100\n100\n100\n91.2\n90.9\n91.0\n100\n0.0\n10.0\n100\n2.9\n12.7\n100\n100\n100\n0.40\n99.6\n100\n100\n81.8\n96.0\n94.6\n100\n48.9\n54.2\n100\n57.4\n61.9\n100\n100\n100\n0.50\n92.7\n100\n99.3\n67.6\n98.5\n95.4\n79.4\n99.9\n97.9\n96.9\n100\n99.7\n98.7\n100\n99.9\nNon-linear VAR\n0.10\n100\n74.2\n76.7\n100\n59.3\n63.1\n100\n0.0\n9.5\n100\n0.0\n9.5\n99.5\n57.1\n61.1\n0.20\n98.4\n89.2\n90.0\n100\n82.9\n84.5\n100\n0.0\n9.5\n100\n0.0\n9.5\n97.4\n99.8\n99.5\n0.30\n94.7\n91.7\n92.0\n96.3\n89.3\n90.0\n100\n0.0\n9.5\n100\n85.4\n86.8\n92.1\n100\n99.2\n0.40\n89.5\n99.4\n98.5\n72.1\n91.8\n89.9\n99.5\n47.9\n52.8\n71.1\n100\n97.2\n68.9\n100\n97.0\n0.50\n73.2\n100\n97.5\n60.5\n95.6\n92.2\n47.4\n95.7\n91.2\n61.1\n100\n96.3\n60.5\n100\n96.2\nLotka-Volterra\n0.05\n100\n72.8\n76.8\n99.0\n40.5\n49.3\n100\n58.4\n64.7\n34.0\n100\n90.1\n33.3\n100\n90.0\n0.10\n100\n97.4\n97.8\n96.3\n73.9\n77.3\n99.7\n100\n100\n33.3\n100\n90.0\n33.3\n100\n90.0\n0.15\n99.3\n99.8\n99.8\n90.0\n92.4\n92.0\n90.7\n100\n98.6\n33.3\n100\n90.0\n33.3\n100\n90.0\n0.30\n67.0\n100\n95.0\n50.3\n100\n92.5\n33.7\n100\n90.0\n33.3\n100\n90.0\n33.3\n100\n90.0\n0.50\n33.3\n100\n90.0\n33.3\n100\n90.0\n33.3\n100\n90.0\n33.3\n100\n90.0\n33.3\n100\n90.0\nLorenz96\n0.05\n95.2\n99.5\n98.7\n93.8\n100\n98.8\n100\n0.0\n20.0\n100\n99.8\n99.8\n95.8\n94.1\n94.5\n0.10\n58.8\n100\n91.8\n39.5\n100\n87.9\n100\n0.0\n20.0\n96.8\n100\n97.0\n50.0\n100\n90.0\n0.15\n27.2\n100\n85.5\n25.0\n100\n85.0\n100\n0.0\n20.0\n72.8\n100\n94.5\n25.0\n100\n85.0\n0.30\n25.0\n100\n85.0\n25.0\n100\n85.0\n100\n79.2\n83.4\n25.0\n100\n85.0\n25.0\n100\n85.0\n0.50\n25.0\n100\n85.0\n25.0\n100\n85.0\n93.0\n93.4\n93.3\n25.0\n100\n85.0\n25.0\n100\n85.0\nAn illustration of the recovered Granger-causal connections (after “optimal” thresholding) is shown in Fig-\nure 14. Note that NGC can only produce the “unsigned” version of the connections and hence all its estimates\nare shown as positive, whereas for other methods, the entries are “signed” with red denoting the positive\nand blue the negative ones.\nOne interesting observation is that for the Lotka-Volterra system, all methods have incorrectly estimated the\nsigns of the diagonals, in that the underlying true dependencies on their own lags are positive for the preys\n24\n(a) Linear VAR: estimated ¯z (or transition matrix ¯\nA, equivalently)\n(b) Non-linear VAR: estimated ¯z\n(c) Lotka-Volterra. Top panel: estimated ¯z; bottom panel: estimated ¯z after suppressing the diagonals\n(d) Lorenz96: estimated ¯z\nFigure 14: Estimated ¯z (after normalization) for various methods. The displayed f1score corresponds to the best attainable one (after thresholding)\nfor each method. Red:(+); blue:(−). Note that NGC does not produce signed estimates and hence all its estimates are shown in red, with the shades\ncorresponding to the magnitude of the entries after normalization.\nand negative for the predators, whereas all methods fail to identify such discrepancy — although for the\nprediction model-based ones all dependencies show as positive and generative model-based ones have the\nopposite sign. This could be partially driven by the fact that during trajectory generation, the Runge–Kutta\nmethod (specifically, RK4) has been used and thus it renders the presence of a self-lag linear term with\ncoefficient 1 in the recursion; in addition, a small noise term has also been injected.\nFor this setting, given that the estimated diagonals have dominating magnitude for GVAR and Linear, we\nalso provide a visual display of the estimates with the diagonals suppressed.\nRemark 5. A dichotomous behavior is observed between the unsigned and the signed estimates obtained\nfrom the code implementation of GVAR17, with the former typically being 5-10% better (in absolute values,\nfor reported metrics such as AUC, ACC that are between 0-100%). In all the tables, we have reported the\nperformance of the superior one (unsigned), whereas Figure 14 is produced based on the signed estimate\nto show the positive/negative recovery. The best attainable F1 scores after thresholding (corresponding to\nthe result of the specific data replicate being displayed) for these signed estimates are labeled in the title of\n17Repository for GVAR: https://github.com/i6092467/GVAR\n25\nthe figures; e.g., 0.75 for the non-linear VAR setting, 0.95 and 0.86 for the Lotka-Volterra and the Lorenz96\nsetting, respectively.\nB.3\nThe impact of the degree of heterogeneity\nTo evaluate the robustness and potential susceptibility of the proposed framework to the level of heterogene-\nity present across entities, we conduct additional experiments based on the Linear VAR and Non-linear VAR\nsettings described in Section 4.1. To recap, the following dynamics are considered for each individual system\nof p nodes, xt = (x1,t, · · · , xp,t)⊤ ∈ Rp:\n• Linear VAR: xt = Axt−1 + εt. The Granger-causal graph z coincides with A.\n• Non-linear VAR: each response coordinate 2 ≤ i ≤ (p − 1) depends on the lag of its own that of two\nother coordinates indexed by k1\ni and k3\ni , that is, xi,t = 0.25xk2\ni ,t−1 +sin(xk1\ni ,t−1 ·xk3\ni ,t−1)+cos(xk1\ni ,t−1 +\nxk3\ni ,t−1) + εi,t, with k1\ni < k2\ni ≡ i < k3\ni . The dynamics for the first and the last coordinates depend only\non their respective adjacent coordinate, i.e., for i = 1, x1,t = 0.4x1,t−1 − 0.5x2,t−1 + ε1,t; for i = p,\nxp,t = 0.4xp,t−1 −0.5xp−1,t−1 +εp,t. The Granger-causal graph z dictates the exact locations of the k1\ni ’s\nand k3\ni ’s.\nFor both settings, the Granger-causal graphs z[m]’s of the M entities are obtained by a “perturbation” with\nrespect to the initial common Granger-causal graph ¯z(0) (or ¯A(0) equivalently, in a linear setting), and the\nmagnitude of such perturbation determines the degree of heterogeneity across entities and the final common\ngraph ¯z. The perturbation logic resembles the one described in Section 4.1.\nSpecifically, for the linear VAR setting, we let the skeleton of ¯A(0) have 30% density, that is, the support set\nS ¯\nA(0) is determined by independent draws from Ber(0.3), and the magnitude of the perturbation is controlled\nby the percentage of “relocated” entries. For the Non-Linear VAR setting, the magnitude of the perturbation\nis controlled by the number of rows whose off-diagonal entries are kept unchanged from those in the initial\ncommon Granger causal graph.18 The sub-settings (S1-S5 with increasing degree of heterogeneity, respec-\ntively for linear and non-linear VAR setups) are depicted in Figures 15 and 16, where the percentage of\nrelocation and the unchanged entries, respectively, are given in the sub-captions. Note that under the non-\nlinear VAR setup, S1 and S5 correspond to the two extreme cases: no entity-level heterogeneity and almost\nfully heterogeneous.\nWe focus on generative model-based methods with a node-centric decoder, i.e., Multi-node (proposed\nframework) and One-node, and evaluate the performance of the estimates, obtained by training the model\non different sample sizes. For the linear VAR setting, the sample size is set to 200 and 1000, while for\nthe non-linear VAR setting to 500 and 2000. The selection of these sample sizes was based on the following\nthree considerations: (1) non-linear dynamics are typically more challenging to learn and thus require larger\nnetworks and more samples to train; and (2) instead of choosing a “large” sample size where both methods\nperform well and thus little differentiation is shown, additional insights can be gained by assessing the per-\nformance of the model in settings where the available sample size is getting close to the information-theoretic\nlimit (at the conceptual level).\nTables 5 and 6 display the results of the Linear/Non-linear VAR settings based on the same set of metrics as in\nSection 4.2, and they correspond to the average of 3 data replicates with the standard deviations displayed\nin parentheses. Major observations are: (1) for Multi-node, the estimation of ¯z is reasonably robust to the\nvarying degree of heterogeneity across sub-settings. In particular, little deterioration is observed across sub-\nsettings, although for sub-setting S5, given the very few common entries, the presented metrics become not\nnot particularly meaningful. (2) Regarding the quality of individual entity estimates, Multi-node exhibits\nsome deterioration in the non-linear setting when the model is getting close to being mis-specified (S5\nversus S1-S4). (3) In the settings under consideration, where the sample size starts becoming rather small,\nMulti-node starts exhibiting an advantage over One-node by a wide margin. Specifically, for the estimated\n¯z, One-node shows performance degradation as the level of heterogeneity increases across sub-settings S1\n18Recall, in the original settings presented in Section 4.1, for the linear VAR setting, the percentage of “relocated” entries is 10%; for\nthe non-linear VAR setting, every 3rd row is left unchanged.\n26\n(a) relocation = 10%\n(b) relocation = 25%\n(c) relocation = 50%\n(d) relocation = 75%\n(e) relocation = 90%\nFigure 15: 30 × 30 Linear VAR system with a total number of M = 20 entities. Sub-settings are displayed vertically with increasing level of heterogeneity\n(from left to right). In the figure, only ¯z and z[1], z[2] are displayed.\n(a) fix everything\n(b) fix every other row\n(c) fix every third row\n(d) fix diagonals + corners\n(e) fix first + last rows\nFigure 16: 20 × 20 Non-linear VAR system with a total number of M = 10 entities. Sub-settings are displayed vertically with increasing level of\nheterogeneity (from left to right). In the figure, only ¯z and z[1], z[2] are displayed. Similar to those in Section 4, as the non-linearity is induced via\nsinusoidal functions, we do not know the true sign of the cross lead-lag dependency; as such, the entries corresponding to entries that are present are\ncolored in black.\nto S5 (even for the linear case), and the overall performance is inferior to that of Multi-node. The latter is\nsomewhat expected: Multi-node performs joint estimation over samples across all entities and thus borrows\n27\nTable 5: Performance evaluation for the estimated ¯z and z[m]’s under settings S1-S5 of Linear VAR. Numbers are in %, and correspond to the mean results\nbased on 5 data replicates; standard deviations are reported in the parentheses.\nMulti-node\nOne-node\nS1\nS1\nS3\nS4\nS5\nS1\nS2\nS3\nS4\nS5\nLinear VAR; train size 200\ncommon\nAUROC\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n90(2.1)\n90(0.6)\n90(4.7)\n83(4.3)\n85(1.7)\nAUPRC\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n85(1.4)\n82(0.2)\n77(8.1)\n56(5.7)\n44(10.1)\nF1(best)\n100(0.3)\n100(0.0)\n100(0.2)\n100(0.0)\n100(0.0)\n75(2.1)\n72(1.3)\n70(8.1)\n53(6.1)\n43(10.0)\nentity\nAUROC\n94(1.4)\n94(1.2)\n94(1.4)\n95(1.6)\n95(1.6)\n88(3.1)\n89(2.4)\n89(3.1)\n88(3.4)\n88(2.6)\n(avg)\nAUPRC\n92(1.9)\n92(1.8)\n92(2.3)\n92(2.3)\n92(2.4)\n83(4.2)\n84(3.4)\n84(4.6)\n82(4.4)\n83(3.4)\nF1(best)\n84(2.4)\n84(2.2)\n84(2.9)\n84(2.7)\n84(2.9)\n75(4.1)\n76(3.0)\n75(4.1)\n74(4.1)\n74(3.2)\nLinear VAR; train size 1000\ncommon\nAUROC\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n97(1.3)\n96(0.6)\n95(1.3)\n91(1.2)\n93(0.6)\nAUPRC\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n95(1.6)\n93(0.3)\n88(3.9)\n72(4.1)\n59(9.9)\nF1(best)\n100(0.4)\n100(0.0)\n100(0.0)\n100(0.0)\n100(0.0)\n89(1.7)\n85(0.5)\n80(6.5)\n67(3.0)\n57(8.6)\nentity\nAUROC\n95(1.4)\n95(1.3)\n95(1.5)\n95(1.7)\n95(1.6)\n94(1.4)\n94(1.4)\n94(1.5)\n94(1.6)\n94(1.6)\n(avg)\nAUPRC\n92(2.0)\n92(2.0)\n92(2.4)\n92(2.3)\n92(2.3)\n92(2.0)\n92(2.1)\n91(2.4)\n92(2.3)\n92(2.3)\nF1(best)\n84(2.9)\n84(2.4)\n84(2.9)\n85(2.6)\n85(2.8)\n84(2.7)\n84(2.5)\n84(3.1)\n84(2.7)\n84(2.9)\nTable 6: Performance evaluation for the estimated ¯z and z[m]’s under settings S1-S5 of Non-linear VAR. Numbers are in %, and correspond to the mean\nresults based on 5 data replicates; standard deviations are reported in the parentheses.\nMulti-node\nOne-node\nS1\nS1\nS3\nS4\nS5\nS1\nS2\nS3\nS4\nS5\nNon-linear VAR; train size 500\ncommon\nAUROC\n98(0.4)\n98(1.4)\n96(0.4)\n100(0.0)\n100(0.0)\n92(1.0)\n84(1.3)\n75(1.7)\n98(0.2)\n98(0.9)\nAUPRC\n81(1.4)\n84(10.9)\n73(3.2)\n98(0.4)\n100(0.0)\n69(0.1)\n64(0.6)\n49(9.0)\n89(0.6)\n46(31.5)\nF1(best)\n84(3.3)\n78(7.3)\n69(1.8)\n92(1.9)\n100(0.0)\n74(0.6)\n69(0.0)\n60(5.6)\n90(2.1)\n50(31.0)\nentity\nAUROC\n97(0.1)\n97(0.8)\n92(0.3)\n98(0.5)\n77(1.0)\n92(0.4)\n74(1.3)\n63(1.6)\n71(2.9)\n51(2.9)\n(avg)\nAUPRC\n79(0.4)\n82(6.0)\n67(2.1)\n88(1.9)\n41(1.9)\n68(0.6)\n54(0.2)\n39(3.4)\n52(2.6)\n18(0.9)\nF1(best)\n79(0.6)\n77(3.1)\n68(1.1)\n81(2.6)\n55(1.3)\n67(1.2)\n53(0.3)\n45(1.1)\n51(1.4)\n28(1.5)\nNon-linear VAR; train size 2000\ncommon\nAUC\n99(0.1)\n100(0.2)\n98(0.3)\n100(0.0)\n100(0.0)\n95(0.1)\n95(0.0)\n95(0.4)\n99(0.2)\n100(0.0)\nAUPRC\n94(0.5)\n96(4.3)\n84(1.9)\n99(0.3)\n100(0.0)\n74(0.1)\n75(0.1)\n77(1.1)\n92(1.0)\n100(0.0)\nF1(best)\n95(0.0)\n96(1.7)\n77(1.1)\n96(1.9)\n100(0.0)\n77(0.0)\n69(0.0)\n72(2.2)\n92(0.0)\n100(0.0)\nentity\nAUROC\n99(0.1)\n99(0.2)\n95(0.6)\n99(0.1)\n80(0.6)\n95(0.3)\n93(0.2)\n90(0.3)\n93(0.5)\n73(1.1)\n(avg)\nAUPRC\n92(0.4)\n93(2.1)\n82(0.7)\n95(0.3)\n53(2.9)\n78(0.9)\n71(0.3)\n67(0.2)\n71(1.1)\n32(0.8)\nF1(best)\n87(0.6)\n88(0.9)\n76(1.2)\n89(0.9)\n61(1.5)\n76(0.4)\n70(1.0)\n63(0.4)\n64(1.2)\n47(0.5)\ninformation across; as such, it can rely on fewer number of samples19 to attain estimates of similar accuracy.\nFinally, note that in the proposed framework, at the decoder stage the Common2Entity step where the en-\ncoder distribution is merged via weighted conjugacy adjustment, the hyper-parameter ω controls the mixing\npercentage between the common and the entity-specific information. Conceptually, its choice varies accord-\ning to the degree of heterogeneity present across entities: in the extreme case where the common structure\nis de facto absent, ω = 1; the other end of the extreme corresponds to ω = 0 when there is no heterogeneity.\nIt is worth noting that we have observed empirically that the proposed framework is not sensitive to the\nchoice of ω, since in most cases its specific value makes little difference to the quality of the estimated ¯z and\nz[m]’s, as long as it was selected from a reasonable range (e.g., between [0.25, 0.75]). For example, in all the\nexperiments above, we have fixed ω at 0.5.\nB.4\nSome remarks on sample size\nWe give a brief account of the performance of the proposed framework in small sample size regimes. Note\nthat in practical settings, model performance hinges on multiple factors, such as sample size, the size of the\nproblem—including both the number of nodes and the number of entities, given the joint learning strategy—\nand how complex the temporal dynamics of the underlying systems are. The goal of this section is to provide\nguidance on the “minimum number of samples required”—from a practitioner’s perspective—in settings of\n19Here the number of samples is expressed in relative terms, that is, train size, corresponding to the number of trajectories used in\ntraining for each entity.\n28\ncomparable size to the ones considered herein.\nSpecifically, we focus on the same set of time series settings for systems with non-linear dynamics considered\nin Section 4 and Appendix B.1, namely, the Non-Linear VAR, multi-species Lotka-Volterra, and the Lorenz96.\nIn all three settings there are 20 nodes in their respective entity-level dynamical systems, and the collection\ncontains 5 or 10 entities. Recall that for the first setting the non-linear dynamics are induced through some\nsinusoidal function, while the other two settings are ODE-based systems.\nTable 7 presents the performance evaluation of the estimated ¯z and z[m]’s based on Multi-node, when\ntraining sample sizes are 3000, 1000 and 500, respectively.\nTable 7: Performance evaluation for b¯z and bz[m]’s based on Multi-node under different settings with various training sample sizes. Numbers are in %, and\ncorrespond to the mean results based on 5 data replicates; standard deviations are reported in the parentheses.\nNon-linear VAR\nLotka-Volterra\nLorenz96\n3000\n1000\n500\n3000\n1000\n500\n3000\n1000\n500\ncommon\nAUROC\n98(0.1)\n97(0.2)\n96(0.4)\n100(0.0)\n97(2.2)\n90(8.0)\n99(0.4)\n95(1.3)\n89(3.5)\nAUPRC\n89(0.5)\n80(1.3)\n74(3.1)\n100(0.2)\n95(3.2)\n81(7.9)\n98(0.8)\n92(2.0)\n85(2.8)\nF1(best)\n79(1.4)\n75(1.3)\n69(1.7)\n100(0.4)\n94(4.3)\n78(4.6)\n94(0.7)\n87(2.3)\n84(1.9)\nentity\nAUROC\n96(0.5)\n94(0.6)\n92(0.7)\n88(0.8)\n81(2.1)\n64(1.4)\n92(1.5)\n87(1.3)\n85(1.3)\n(avg)\nAUPRC\n86(0.4)\n76(1.2)\n69(2.3)\n79(1.2)\n66(3.2)\n43(3.8)\n85(2.4)\n79(2.1)\n76(2.8)\nF1(best)\n78(0.3)\n73(1.4)\n69(1.6)\n75(1.1)\n64(4.0)\n42(2.4)\n78(2.3)\n73(2.5)\n70(3.6)\nThe main observations are: (1) for the common graph ¯z, as sample size reduces from 3000 to 1000, the\nproposed method’s performance metrics stay above a reasonable range, even though a certain degradation\nis present, and its magnitude varies across settings. (2) For the entity-specific z[m]’s, the degradation in\nperformance is more pronounced as the sample size reduces, and the model clearly suffers from not having\naccess to an adequate number of samples.20\nBased on these observations, we broadly conclude the following for practical settings of comparable size to\nthe ones examined above: in the case where the primary focus is on the common graph ¯z, the proposed\nframework would likely yield reasonable recovery even with about 1000 samples. On the other hand, if indi-\nvidual entity-level estimates are also of interest, sample sizes below 3000 would become rather challenging\nfor the method to exhibit a satisfactory performance.\nB.5\nLotka-Volterra with perturbation: some characterization\nWe provide a characterization/justification for the “perturbed” Lotka-Volterra system, pertaining to how to\nvalidate a Lotka-Volterra system based on the “perturbed” interaction matrix being stable.\nThe general form of p-multi-species Lotka-Volterra equations are given by\ndxi\ndt = rixi\n\u00001 +\np\nX\nj=1\nAijxj\n\u0001\n,\n(17)\nwhere ri > 0 is the inherent per-capita growth rate of species xi, i = 1, · · · , p and A ∈ Rp×p the species\ninteraction matrix. The system considered in (13) can then be put in this canonical form, by assuming that\nthe first p/2 species are preys and the last p/2 species predators.\nSpecifically, for the preys the corresponding equation in the canonical form becomes\ndxi\ndt = αxi\n\u0014\u00001 − 1\nη2 xi\n\u0001\n− β/α\nX\nj∈Pprey\ni\nxj\n\u0015\n,\ni = 1, · · · , p/2\n20For these small sample size experiments, we use the same set of hyper-parameters as the ones in earlier experiments with much\nlarger sample sizes (1e4). One can potentially expect improved performance with more carefully tuned hyper-parameters, although the\nimprovement would likely be limited.\n29\nwhere ri = α, Aii = − 1\nη2 , Aij = −β/α for all j ∈ Pprey\ni\notherwise 0; Pprey\ni\ndenotes the support set of the prey\nindexed by i. Analogously, for the predators the corresponding equation in the canonical form becomes\ndxi\ndt = −γxi\n\u00001 − δ/γ\nX\nj∈Ppredator\ni\nxj\n\u0001\n,\ni = p/2 + 1, · · · , p\nwhere ri = −γ, Aii = 0, Aij = −δ/γ for all j ∈ Ppredator\ni\notherwise 0; Ppredator\ni\ndenotes the support set of the\npredator indexed by i.\nIt can be seen that fixed points of the set of equations in (17) can be found by setting dxi/dt = 0 for all i,\nwhich translates to the vector equation\nr + Ax = 0,\nr ∈ Rp, x ∈ Rp, A ∈ Rp×p.\nConsequently, fixed points exist if A is invertible and are given by x = −A−1r. Note that xi = 0 is a trivial\nfixed point. Further, the fixed point may contain both positive and negative values, which implies that there\nis no stable attractor for which the populations of all species are positive. The eigenvalues of A determine the\nstability of the fixed point. By the stable manifold theorem, if its eigenvalues are less than 1, then the fixed\npoint is stable. This can be easily verified once the “perturbed” Granger-causal matrix z’s (which determines\nthe Pi’s and hence the corresponding A) are generated.\nC\nGranger Causality and Graphical Models, Bayesian Hierarchical Modeling, and\nLinear VARs\nThis section comprises of three parts that provide background information on different topics mentioned in\nthe main paper. Section C.1 illustrates how the framework of graphical models can be used to capture the\nconcept of Granger causality. Section C.2 provides a brief overview of the Bayesian hierarchical modeling\nframework and outlines how it shares broad similarities to the modeling framework used in the paper.\nFinally, Section C.3 discusses possible ways of accomplishing the modeling task via a collection of linear\nVARs, either using a frequentist formulation, or a Bayesian hierarchical modeling one.\nC.1\nGranger causality and graphical models\nConsider a dynamical system, comprising of a p-dimensional stationary time series xt := (x1,t, · · · , xp,t),\nwith xi,t denoting the value of node i at time t. Further, let V = {x1, · · · , xp} denote the node set of the p\nnodes/time series of the system.\nA Granger causal time series graph (Dahlhaus and Eichler, 2003) has node set V = V × Z and edge set\nE ⊆ V × V , wherein an edge (xi, t − s) → (xj, t) ̸∈ E, if and only if s ≤ 0 or xi,t−s ⊥⊥ xj,t | Xt \\ xi,t−s, where\nXt = {xt′, t′ < t} denotes the entire past process of the time series at time t, ⊥⊥ probabilistic independence\nand \\ the set difference operator. The above definition implies that the edge set E contains directed edges\nfrom past time points to present ones, only if xi,t−s and xj,t are dependent, conditioned on all other past\nnodes in V excluding xi,t−s.\nAn aggregate Granger causal graph (Dahlhaus and Eichler, 2003) has vertex set V and edge set E, wherein\nan edge (xi → xj) ̸∈ E if and only if (xi, t − s) → (xj, t) ̸∈ E for all u > 0, t ∈ Z; i.e, absence of the edge\n(xi → xj) from the aggregate Granger causal graph implies absence of Granger causality from node (time\nseries) xi to node xj, while presence of that edge implies that one or more time lags of node xi are Granger\ncausal of node xj.\nFigure 17 illustrates pictorially both the Granger causal time series graph (Figure 17a), and the matrix\nrepresentation (in the form of heatmaps) for the aggregate Granger causal graph (the right-most heatmap\nin Figure 17b).\n30\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\n1\n2\n3\n4\n5\ntime t − 2\ntime t − 1\ntime t\n(a) Example for a Granger causal graph with 2-lag dependency and V\n=\n{1, 2, 3, 4, 5}. For the edges in E, those originate from time t − 2 are denoted\nin dash and those from time t − 1 are in solid arrows, respectively.\n“or”\nt − 2\nt\nt − 1\nt\npast < t\npresent t\n(b) Connection matrices corresponding to the graph in Figure 17a, where columns correspond to emitters (past) and rows\ncorresponding to receivers (present). Colored cells denote the presence of a connection. The right-most matrix corresponds\nto the aggregate Granger-causal connection matrix that summarizes and indicates present-past dependencies.\nFigure 17: Pictorial illustration for Granger causal time series graph, aggregate Granger causal graph and their corresponding matrix representation.\nIn the case of a linear VAR system of order q given by xt = Pq\nk=1 Akxt−k + et, the edge set of the Granger\ncausal time series graph corresponds to E = {(Ak)ij | (Ak)ij ̸= 0, i, j ∈ V, k = 1, · · · , q}, while E =\n{Bij|Bij = 1(Pq\nk=1(abs(Ak)ij) ̸= 0), i, j ∈ V}, with 1(·) denoting the indicator function. The aggregate\nGranger causal graph with edge set E is an unweighted one, namely, its edges take values 0 (absence) or 1\n(presence) and consequently reflect absence/presence of Granger causality between the time series.\nRemark 6 (On the estimated Granger-causal graph). Under the proposed framework, in the binary case, the\nGranger connectivity graph corresponds exactly to the aggregate Granger causal graph defined above (see\nSection 3 and Remark 1), which is also in the same spirit as how different edge types are modeled in Kipf\net al. (2018). In the continuous case, it corresponds to a weighted version of the aggregate Granger causal\ngraph, wherein the weights correspond to the size of the “gate” through which the information from the\npast flows to the present. Admittedly, in the presence of non-linear modules (such as MLP) after the gating\noperation in the decoder, the weights no longer correspond to the “predictive strength” as defined in the\noriginal paper by Granger (1969). Nonetheless, at the conceptual level, the weights reflect the “strength” of\nthe underlying relationships, as measured through the “permissible information flow”.\nC.2\nBayesian hierarchical modeling\nGiven the prevalent usage of hierarchical modeling in the case where observational units form a hierarchy—\ne.g., in our motivating example, the observed time series are at the entity level and the entities form a\ngroup—we briefly review the Bayesian hierarchical modeling framework next.\nSince its initial introduction in Lindley and Smith (1972) for linear models, the Bayesian hierarchical frame-\nwork has been expanded and used for many other classes of statistical models. The book by Gelman et al.\n(2014) provides a description of the general framework and outlines the role of exchangeability for con-\nstructing prior distributions for statistical models with hierarchical structure. The framework has been oper-\nationalized and used for many statistical models, including regression and multilevel models (Gelman and\nHill, 2006), time series (Berliner, 1996) and spatio-temporal models (Wikle et al., 1998), in causal analy-\nsis (Feller and Gelman, 2015), cluster analysis (Heller and Ghahramani, 2005), in nonparametric modeling\n(Teh and Jordan, 2010), and so forth. At the modeling level, the outline of the framework for a hierarchy\ncomprising of two levels is as follows. Data for entities m = 1, · · · , M are generated according to some\nprobability distribution\np(x[m]; θ[m], ϕ) = p(x[m]|θ[m]) · p(θ[m]|ϕ) · p(ϕ).\nθ[m]’s are entity-specific parameters, and they are assumed to be generated exchangeably from a common\npopulation, whose distribution is governed by a common parameter ϕ, and can be specified as p(θ[m]|ϕ).\nThe common parameter ϕ can be fairly complex (for an example, see Section C.3) and possesses a prior\ndistribution p(ϕ), which depending on the nature of ϕ can be fairly involved. The prior distribution for the\nparameter (θ[m], ϕ) that governs the data generation mechanism for entity m jointly, can then be character-\nized by p(θ[m], ϕ) = p\n\u0000θ[m]|ϕ\n\u0001\np(ϕ).\nNote that the above specification exhibits differences to the generative process of a multi-level VAE presented\nin Section 2.2. Specifically, in the VAE specification, there are observed and latent random variables, mod-\n31\neled according to a probability distribution with fixed parameters θ⋆, whereas in the Bayesian hierarchical\nmodeling formulation, the parameters of the data generating distribution are random variables themselves\nand respect a hierarchical specification as previously mentioned.\nC.3\nModeling via a collection of linear VARs\nWe illustrate how the modeling task at hand can be handled when the dynamics are assumed linear. In\nparticular, the dynamical systems can be characterized by a collection of linear VAR models; we show how\nthe common structure can be modeled by decomposing the transition matrix or using hierarchical modeling,\nrespectively in a frequentist and a Bayesian setting. For ease of exposition, in the sequel, we assume the\ncollection of linear VAR models have lag of order 1, and they are given by\nx[m]\nt\n= A[m]x[m]\nt−1 + εt,\nm = 1, · · · , M.\nFrequentist formulation.\nSuppose that the transition matrices can be decomposed as A[m] = A0 + B[m],\ni.e., into a common component A0 and an entity-specific one B[m]. For model identifiability purposes, an\n“orthogonality” constraint is imposed; for example, in the form of A0B[m] = 0 ∈ Rp×p. In settings where the\ntransition matrices A[m] are additionally assumed sparse (see, e.g., the numerical experiments in Section 4),\nsuch a constraint is typically in the form of support(A0) ∩ support(B[m]) = ∅, namely that the matrices A0\nand B[m] do not share non-zero entries.\nBayesian hierarchical modeling formulation.\nWe consider a collection of linear VAR models as above.\nThe probability distribution of the data is p\n\u0000{x[m]\nt\n}M\nm=1|{A[m]}M\nm=1, ϕ\n\u0001\n, where ϕ is a vector of additional\nparameters specified next. To construct the prior distribution of the model parameters ({A[m]}, ϕ) we proceed\nas follows. Note that at the modeling level, a simple hierarchy is defined for each (i, j)-th element of the\ntransition matrix across all M entities/models; i.e., we consider p2 such hierarchies independently. To use\nthe Bayesian hierarchical modeling framework, let ⃗cij = (A[m]\nij , · · · , A[M]\nij )′ be an M-dimensional vector\ncontaining the (i, j)-th element of all M transition matrices. The following distributions are imposed on ⃗cij’s\nand the parameters associated with their priors:\n⃗cij | (Ψ, τij) ∼ N(0, τijΨ);\n(18)\nτij ∼ Gamma(M + 1/2, λij),\nΨ ∼ Inverse Wishart(S0, γ0).\nThe prior distributions in (18) are independent over index (i, j); τij is an (i, j)-element specific scaling factor,\nand Ψ an M × M matrix that captures similarities between the M models. The parameters λij, S0, γ0 can\nbe either fixed to some pre-specified values (e.g., a fixed S0 can reflect prior knowledge on the similarity\nbetween the M models), or equipped with diffuse prior distributions. Further, note that if Ψ ≡ I the identity\nmatrix, then the above specification reduces to the Bayesian group lasso of Kyung et al. (2010). Based on\nthe above exposition, it can be seen that ϕ := (Ψ, {τ}ij, i, j = 1, · · · , p). In summary, we have the following\ntwo-level modeling specification: at the first level, we have the data distribution, while at the second level\nthe distribution on the elements of the transition matrices that are “coupled” across the M models through\nϕ and its prior distribution specification. Obviously, more complicated prior specifications can be imposed,\nfor example by “coupling” whole rows of the transition matrices A[m] across the entities.\nD\nAdditional Results for the EEG Dataset\nThe estimated common Granger-causal connections based on One-node and NGC are depicted in Figures 18\nand 19, respectively. The increase in the overall Granger causal connectivity in the EC session compared\nto that in the EO session observed for Multi-node and GVAR is also present in the results from One-node,\n32\n(a) Eyes Open (EO)\n(b) Eyes Closed (EC)\nFigure 18: One-node results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent\nthresholding at 0.50. Red edges correspond to positive connections and blue edges correspond to negative ones; the transparency of the edges is propor-\ntional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are colored in\ngray.\n(a) Eyes Open (EO)\n(b) Eyes Closed (EC)\nFigure 19: NGC results: estimated common Granger-causal connections for EO (left panel) and EC (right panel) after normalization and subsequent\nthresholding at 0.45. All edges are colored gray, since NGC does not provide signed estimates of Granger causal connections. The transparency of the edges\nis proportional to the strength of the connection. Larger node sizes correspond to higher in-degree (incoming connectivity), and the top 6 nodes are\ncolored in gray.\nwhereas it is reversed in the results of the NCG. Further, the observed increase in the overall connectivity\npattern between the EO session compared to the EC session, exhibits differences between the left and right\nparts of the brain, something also observed in the results of Multi-node. Further, note that NCG does not\nproduce signed estimates and hence all Granger causal connections are colored grey in Figure 19. This\nlimitation of the method can hinder scientific insights that could be obtained from the analysis of a dataset\nby NGC.\nE\nOn Respecting the Sign Distinction of the Connections\nThis section provides some explanation to how the proposed methodology (Multi-node)—modulo estimation\nerror that can introduce inaccuracies—recovers the sign of the underlying truth up to a complete sign flip,\nthat is,\nSIGN(ˆz) = (±)SIGN(z);\n(19)\nwith SIGN(·) operating in an entry-wise fashion on z or ˆz. In (19), z generically refers either to the grand-\ncommon Granger-causal graph ¯z or entity specific ones z[m], and ˆz is the corresponding estimate. This is\nequivalent to saying that there is no guarantee that for each individual entry, sign(zij) = sign(bzij) always\nholds; however, all positive (negative) signed connections are identified as having the same sign. In this\nregard, the signs of the estimates obtained from the procedure can be interpreted in a meaningful way, in\nthat the positive/negative connections can be differentiated; see Figure 20 for an illustration.\nAs a result of (19), the following also readily holds for any two entries indexed by (i1, j1) and (i2, j2),\n33\n✓\ncomplete sign flip\n✓\nno sign flip\nest, no sign flip\ntruth\nest, complete sign flip\nFigure 20: Pictorial illustration for the concept of “up to complete sign flip”. In both the no-sign-flip and the complete-sign-flip case, the estimate always\n“groups” the positive/negative connections together in a way that is in accordance with the truth.\n∀ i1, j1, i2, j2 ∈ {1, · · · , p}:\nsign(zi1j1)sign(zi2j2) = sign(bzi1j1)sign(bzi2j2);\ni.e., if two connections have the same/opposite signs in z, they continue having the same/opposite signs in\nˆz. We shall refer to this property as “respecting the sign distinction”.\nThe goal of this section is to provide some intuition on how the above mentioned is enabled through the\nencoder-decoder learning—in particular, in the presence of non-linear modules. Note that the subsequent\narguments do not constitute a formal end-to-end proof.\nIn the sequel, we focus on the single-entity case and ignore modules related to the coupling between entity-\nlevel graphs and their grand-common counterpart, as these modules are not pertinent to this specific discus-\nsion. Concretely, the relevant modules in the ensuing discussion are:\n• q(z[m]|x[m]) as captured by (enc-a) and (enc-b) combined; i.e., the “Trajectory2Graph” encoder.\n• p(x[m]|z[m]) as captured by (dec-b); i.e., the “Graph2Trajectory” decoder.\nThe superscript [m] will be omitted henceforth.\nOutline of the argument.\nThe argument consists of two parts:\n1. The decoder, by utilizing a shared MLP across all response coordinates, ensures that the sign distinction\nis respected across the rows, along any column. Specifically, see, e.g., equations (9) and (10), wherein\nthe MLP and the subsequent operations (in particular, their parameters) are shared by all response\ncoordinates i’s.\n2. The encoder, in the case of supervised training, disallows any partial (row or column) sign flip.\n(1) and (2) jointly ensure that ˆz respects the sign of z up to a complete sign flip, and this is operationalized\nvia the end-to-end training where the parameters are jointly learned and the data likelihood maximized.\nAt the high level, the shared MLP mechanism in the decoder ensures that it will not generate estimates\nthat show “row sign flip” relative to the underlying truth. Specifically, for any fixed column, if one looks\nat the estimates along the columns (i.e., vertically) and across the rows, the estimates would respect their\nsign distinction in a pairwise fashion.\nHowever, it does not preclude cases where along the rows (i.e.,\nhorizontally) and across the columns, signs in the estimates can be flipped (i.e., column sign flip).\nOn the\nrow sign flip\n×\ntruth\nest\n(a) Example for row sign-flip: signs of the 2nd row is flipped (right versus. left)\ncolumn sign flip\n?\ntruth\nest\n(b) Example for column sign-flips: signs of the 1st, 3rd and 4th columns are flipped (right versus. left)\nFigure 21: Pictorial illustration for the concept of “row sign flip” and “column sign flip”, picking the first two rows from Figure 20. Note that the former is\nprohibited by the shared MLP mechanism in the decoder construction.\nother hand, to generate estimates that recover the sign of the underlying truth up to a complete sign flip,\nboth row and column sign flips need to be precluded. The latter is facilitated by the encoder module during\nthe end-to-end training: when the decoder fixes the “sign-orientation” vertically across the rows, the encoder\nwould favor estimates that do not exhibit any partial sign flip during learning.\n34\nThe details for each component are given next.\nE.1\nDecoder\nClaim:\nBy using a shared MLP across all response coordinates, for any fixed column j ∈ {1, · · · , p}, the\ndecoder respects the sign distinction across the rows of z, that is,\nsign(bzi1j)sign(bzi2j) ≡ sign(zi1j)sign(zi2j),\n∀i1, i2 ∈ {1, · · · , p}.\n(20)\nThe same cannot be guaranteed, however, if different MLPs are used for different response coordinates.\nFor illustration purposes, we focus on the case where the feature dimension is 1 (i.e., classical time series\nsetting).\nConsider a simple two-layer MLP whose hidden layer has h neurons.\nLet fMLP : Rp 7→ R be\nrepresented as\nfMLP(u) = W (2)σ\n\u0000W (1)u + b(1)\u0001\n+ b(2),\nu ∈ Rp;\nW (1) ∈ Rh×p, b(1) ∈ Rh×1, W (2) ∈ R1×h, b(2) ∈ R; σ(·) is some activation function.\nSpecifically in the\nGraph2Trajectory decoder, the function input of the MLP is in the form of ui,t−1, whose jth coordinate is\ngiven by xj,t−1 ◦ zij, assuming the absence of any numerical embedding (see, e.g., expressions in (9) with\nsuperscript [m] dropped). To further simplify notation, we ignore subscript t−1, and let y = (y1, · · · , yp) ∈ Rp\ndenote the time-t target. Effectively, at decoding time, an approximation of the following form is considered\nfor all timestamps:\nyi ≈ fMLP(ui),\n∀ i = 1, · · · , p\n= W (2)σ\n\n\n\n\n\n\nW (1)\n11\nW (1)\n12\n· · ·\nW (1)\n1p\n...\n...\n...\n...\nW (1)\nh1\nW (1)\nh2\n· · ·\nW (1)\nhp\n\n\n\n\nx1 ◦ zi1\n...\nxp ◦ zip\n\n + b(1)\n\n\n\n + b(2),\n(21)\nwhere x1, · · · , xp are inputs directly available through training data, (zi1, · · · , zip)′ constitutes the ith row of\nmatrix z. Crucially, fMLP is shared across all i’s.\nIn the actual end-to-end learning, zij’s are sampled from a distribution whose parameters are dictated by the\nencoding step. The parameters of the encoders are jointly learned with those of the decoders, by minimizing\nthe reconstruction error and the KL term.\nHere to further delineate the issue pertaining specifically to\nwhether with the use of a shared MLP, the learned zij’s can respect the sign distinction, we ignore the\nencoding step, and simplifies the question as follows:\nCan the learning procedure—by minimizing the prediction error based on (21)— that jointly learns the W’s, b’s\nand entries of z’s give rise to learned bzij’s, such that the bzij’s respect the sign distinction?\nThe answer is affirmative for any fixed column j = 1, · · · , p. To see this, expand the matrix product in (21),\nwhich gives (here we ignore approximation error and assume the model is well-specified):\nyi =\nh\nX\ns=1\nW (2)\ns\nσ\n\u0010\np\nX\nj=1\n\u0000W (1)\nsj ◦ zij\n\u0001\nxj + b(1)\u0011\n+ b(2).\nThe predicted byi is given by\nbyi =\nh\nX\ns=1\nc\nW (2)\ns\nσ\n\u0010\np\nX\nj=1\n\u0000c\nW (1)\nsj ◦ bzij\n\u0001\nxj + bb(1)\u0011\n+ bb(2),\nwhere c\nW (1), c\nW (2), bb(1) and bb(2) are estimated weights and bias terms. By minimizing the prediction error, byi\nis close to yi, for any values of x1, x2, · · · , xp and for all i’s. This amounts to having the estimated coefficients\nin front of the xj’s sufficiently close to the truth—in particular, modulo estimation error, the following holds:\nW (1)\nsj zij = c\nW (1)\nsj bzij,\nfor all i = 1, · · · , p.\n(22)\n35\nThis further gives\n(W (1)\nsj )2zi2jzi2j = (c\nW (1)\nsj )2bzi1jbzi2j,\n∀ i1, i2 ∈ {1, · · · , p},\n(23)\nand therefore (20) follows since (c\nW (1)\nsj )2 > 0.\nNote that in the case where different MLPs are used for different response coordinates, (23) becomes\n(W (i1,1)\nsj\nW (i2,1)\nsj\n)zi2jzi2j = (c\nW (i1,1)\nsj\nc\nW (i2,1)\nsj\n)bzi1jbzi2j, which no longer leads to (20).\nToy data experiments.\nTo verify this empirically, we consider a toy data example, where the trajectories\nare generated according to a 2-dimensional linear VAR system, that is,\nxt = Axt−1 + et,\nwhere A =\n\u0014\n0.5\n−0.25\n−0.25\n0.5\n\u0015\n;\n(24)\ncoordinates of et are drawn i.i.d. from N(0, 0.5). Note that given the linear setup, the transition matrix\ncorresponds precisely to the true Granger-causal graph, and therefore z ≡ A.\nWe run end-to-end training based on two configurations of the decoder:\n(a) a single MLP shared across all response coordinates;\n(b) separate MLPs for different response coordinates.\nIn both configurations, the MLPs are 2-layer ones with a hidden layer of dimension 64. The experiment is\nrun over a single data replicate but repeated using 10 independent seeds.\n(a) Normalized truth\n(b) Estimates based on Configuration (a) - a shared MLP\n(c) Estimates based on Configuration (b) - separate MLPs\nFigure 22: Toy data experiment decoder results: heatmaps for z (truth, normalized) and ˆz (estimates, normalized) under the shared and separate-MLP\nconfigurations. Panel (a) corresponds to z after normalization; panel (b) correspond to normalized ˆz (from runs with different seeds) obtained under\nConfiguration (a); panel (c) correspond to normalized ˆz obtained under Configuration (b).\nFigure 22 displays the estimated z corresponding to 3 different seeds for each configuration. Amongst all\n10 runs, Configuration(a) preserves the sign distinction at all times—in this particular case, diagonals in\nˆz always have the same sign and anti-diagonals have the opposite. Note that results from run seed 324\n(left-most figure in Figure 22b) correspond to the case where the estimate yields a complete sign flip of the\nunderlying truth. For Configuration (b), it fails in 2 out of the 10 runs—showing 2 failures (seed 324 and\n644) and 1 success (seed 764) in Figure 22c, as the estimates can fail to preserve the sign distinction amongst\nthe edges.\nE.2\nEncoder\nClaim:\nthe encoder is able to perform “effective” learning based on labels up to a complete sign flip, but\nlearning becomes problematic when the labels entail any partial sign flip.\nSimilar to the case of the decoder, to delineate the issue pertaining to the encoder, instead of considering\nend-to-end training where the two models are jointly learned, we consider a simplified setting, where we\nuse the encoder module for a supervised learning task, based on data whose true generating mechanism is\nassociated with the Granger causal graph z. The question posed is the following:\nThe true trajectories are generated based on z. For a supervised learning task where the training labels are\nprovided and the learning is enabled by the encoder module, is the encoder able to perform “effective” learning,\n36\n1. when the label used during training is some partial (column or row) sign flip of z?\n2. when the label used during training is a complete sign flip of z, namely −z?\nThis is explored via synthetic data experiments, where the data generating mechanism is identical to the one\nconsidered in Section E.1.\nConcretely, let z♯ denote the quantity that is provided as the target (label) during the supervised training;\nnote that the data is generated according to (24), with z ≡ A =\n\u0002\n0.5\n−0.25\n−0.25\n0.5\n\u0003\n, irrespective of the labels\nprovided. The following four training scenarios are considered:\n(a) No sign flip: z♯ =\n\u0002\n0.5\n−0.25\n−0.25\n0.5\n\u0003\n, that is, z♯ = z;\n(b) Complete sign flip: z♯ =\n\u0002 −0.5 0.25\n0.25 −0.5\n\u0003\n, that is, z♯ = −z;\n(c) Column sign flip: z♯ =\n\u0002\n0.5\n0.25\n−0.25 −0.5\n\u0003\n, that is, z♯\n:,1 = z:,1, z♯\n:,2 = −z:,2;\n(d) Row sign flip: z♯ =\n\u0002 0.5 −0.25\n0.25 −0.5\n\u0003\n, that is, z♯\n1,: = z1,:, z♯\n2,: = −z2,:.\nWe run encoder-only training for the above four scenarios. Results21 are displayed in Figure 23, with the\nestimated z displayed in the top panel and the label z♯ used for supervision during training displayed in the\nbottom panel.\n(a) Scenario (a)\n(b) Scenario (b) - complete flip\n(c) Scenario (c) - column flip\n(d) Scenario (d) - row flip\nFigure 23: Toy data experiment encoder-only results: heatmaps for ˆz (estimates, top panel) and and z♯ (training label, bottom panel) for scenarios (a) to\n(d) respectively. Note that the underlying ground truth (i.e., the z that governs the dynamics of the trajectories) for all these experiments are identical to\nthe one in Scenario (a).\nAs the results show, the encoder learns almost perfectly (relative to the provided labels) in scenarios (a)\nand (b), despite the latter being a complete sign flip. On the other hand, it struggles to learn in the case of\npartial sign flips (i.e., Scenarios (c) and (d)), as manifested by the essentially-zero estimated values. This\nempirically corroborates our claim.\nFinally, it is worth noting that the claim examined in this subsection is under the supervised learning setup,\nnamely, it establishes the fact that the encoder only permits no or complete sign flip, under a setting where\nthe training target is explicitly provided. In practice, the learning is end-to-end, that is, there is no “real”\nsupervision on the encoder available. As such, at the conceptual level, the learning relies on the decoder to\nfix the vertical sign-orientation as well as the encoder to preclude potential row sign flip—our experiment\nresults in Section E.1 also corroborates this.\n21Here we are displaying results for the test data; the results for training data lead to the same conclusion qualitatively.\n37\nF\nGeneralization to Multiple Levels of Grouping\nWe discuss the generalization of the proposed framework to the case where multiple levels of grouping are\npresent and the corresponding group-common graphs at different levels of the hierarchy are of interest.\nConsider L-levels of nested grouping where the group assignments become increasingly granular as the level\nindex increases. Specifically, there is a single level-0 group that encompasses all entities, and M (degenerate)\nlevel-L groups, with each group m having a singleton member being the entity m; all other levels are cases in\nbetween – see also Figure 24 for a pictorial illustration. Note that the case discussed in the main manuscript\ncorresponds to the special case with L = 1. As an example for the case of L = 2 levels, consider the data\nanalyzed in Section 5. Suppose that the subjects can be partitioned into 3 groups according to their ages —\ne.g., less than 30 years old, 30-60 years old, over 60. In such a setting, the single level-0 group comprises of\nall subjects; the level-1 groups correspond to subjects falling into different age strata; the level-2 groups are\nthe subjects themselves. The quantities of interest are the connectivity patterns shared by subjects within\ntheir respective groups at all levels.\nEntities\nLevel 2 Group\nLevel 1 Group\nLevel 0 Group\nFigure 24: Diagram for a 3-level grouping. Neurons corresponds to Gl\nk’s that collects the indices of the entities belonging to that group. Solid lines with\narrows indicate how small groups from an upper level form larger groups at a lower level.\nLet Gl := {Gl\n1, · · · , Gl\n|Gl|} denote the collection of groups of level l; each Gl\nk is the index set for the entities\nbelonging to group k at level l and the group membership is non-overlapping, that is, Gl\nk1∩Gl\nk2 = ∅, ∀ k1, k2 ∈\n{1, · · · , |Gl|}. The quantities of interest are the entity-specific graphs z[m], as well as the group-level common\nstructure for all groups at all levels, that is ¯zGl\nk, denoting the group-common structure amongst all entities\nthat belong to the kth group, with level-l grouping; l = 0, · · · , L − 1 indexes the group level; k = 1, · · · , |Gl|\nindexes the group id within each level. Finally, we let ¯z ≡ ¯zG0, which is consistent with its definition in the\nmain text and it corresponds to the grand-common structure across all entities.\nWithout getting into the details of each step, the end-to-end learning procedure can be summarized in\nFigure 25. Compared with the two-level case, the generalization amounts to additional intermediate en-\ncoded/decoded distributions in the form of qϕ(z[Gl−1\nk\n]|z[Gl\nk]), pθ(z[Gl\nk]|z[Gl−1\nk\n]) and pθ(z[Gl\nk]|·) (post conjugacy\nadjustment/merging information); l = 2, · · · , L; k = 1, · · · , |Gl|.\n{x[m]}\n{z[m]}|{x[m]}\n{z[GL−1\nk\n]}|{z[m]}\n{z[G1\nk]}|{z[G2\nk]}\nsampled ¯z\npθ(¯z)\n{ˆx[m]}\n{z[m]} | ·\n{z[GL−1\nk\n]} | ·\n{z[G1\nk]}| ·\nqϕ(z[m]|xm)\nqϕ(¯z|{¯z[G1\nk]})\npθ({z[G1\nk]}|¯z)\npθ({x[m]}|{z[m]})\n(merge info)\n(merge info)\n(merge info)\n(merge info)\n(observed)\nencoding\ndecoding\n(reconstructed)\n(prior)\nFigure 25: Diagram for the end-to-end encoding-decoding procedure in the presence of multiple levels of grouping.\n38\nReferences\nBarry, R. J., A. R. Clarke, S. J. Johnstone, and C. R. Brown (2009). EEG differences in children between\neyes-closed and eyes-open resting conditions. Clinical Neurophysiology 120(10), 1806–1811.\nBarry, R. J., A. R. Clarke, S. J. Johnstone, C. A. Magee, and J. A. Rushby (2007). EEG differences between\neyes-closed and eyes-open resting conditions. Clinical Neurophysiology 118(12), 2765–2773.\nBasu, S. and G. Michailidis (2015). Regularized estimation in sparse high-dimensional time series models.\nThe Annals of Statistics 43(4), 1535–1567.\nBasu, S., A. Shojaie, and G. Michailidis (2015). Network Granger causality with inherent grouping structure.\nThe Journal of Machine Learning Research 16(1), 417–453.\nBerliner, L. M. (1996). Hierarchical Bayesian time series models. In Maximum Entropy and Bayesian Methods:\nSanta Fe, New Mexico, USA, 1995 Proceedings of the Fifteenth International Workshop on Maximum Entropy\nand Bayesian Methods, pp. 15–22. Springer.\nBurda, Y., R. Grosse, and R. Salakhutdinov (2016). Importance weighted autoencoders. In International\nConference on Learning Representations.\nDahlhaus, R. and M. Eichler (2003). Causality and graphical models in time series analysis. Oxford Statistical\nScience Series, 115–137.\nDas, A., A. Mandel, H. Shitara, T. Popa, S. G. Horovitz, M. Hallett, and N. Thirugnanasambandam (2022).\nEvaluating interhemispheric connectivity during midline object recognition using EEG. PloS One 17(8),\ne0270949.\nDas, R., E. Maiorana, and P. Campisi (2016). EEG biometrics using visual stimuli: A longitudinal study. IEEE\nSignal Processing Letters 23(3), 341–345.\nEichler, M. (2012). Graphical modelling of multivariate time series. Probability Theory and Related Fields 153,\n233–268.\nErd˝os, P. and A. R´enyi (1959). On random graphs I. Publ. math. debrecen 6(290-297), 18.\nFeller, A. and A. Gelman (2015). Hierarchical models for causal effects. Emerging Trends in the Social and\nBehavioral Sciences: An interdisciplinary, searchable, and linkable resource, 1–16.\nFigurnov, M., S. Mohamed, and A. Mnih (2018). Implicit reparameterization gradients. Advances in Neural\nInformation Processing Systems 31.\nGelman, A., J. B. Carlin, H. S. Stern, and D. B. Rubin (2014). Bayesian data analysis (3rd ed.). Chapman\nand Hall/CRC.\nGelman, A. and J. Hill (2006). Data analysis using regression and multilevel/hierarchical models. Cambridge\nUniversity Press.\nGeweke, J. (1984). Inference and causality in economic time series models. Handbook of Econometrics 2,\n1101–1144.\nGilmer, J., S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl (2017). Neural message passing for\nquantum chemistry. In International Conference on Machine Learning, pp. 1263–1272. PMLR.\nGorishniy, Y., I. Rubachev, and A. Babenko (2022). On embeddings for numerical features in tabular deep\nlearning. Advances in Neural Information Processing Systems 35, 24991–25004.\nGraber, C. and A. Schwing (2020).\nDynamic neural relational inference for forecasting trajectories.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pp. 1018–\n1019.\nGranger, C. W. (1969). Investigating causal relations by econometric models and cross-spectral methods.\nEconometrica: Journal of the Econometric Society, 424–438.\nGranger, C. W. (1980). Testing for causality: A personal viewpoint. Journal of Economic Dynamics and\nControl 2, 329–352.\nHatton, S. L., S. Rathore, I. Vilinsky, and A. Stowasser (2023). Quantitative and qualitative representation of\nintroductory and advanced EEG concepts: An exploration of different EEG setups. Journal of Undergradu-\nate Neuroscience Education 21(2), A142.\n39\nHeller, K. A. and Z. Ghahramani (2005). Bayesian hierarchical clustering. In Proceedings of the 22nd Inter-\nnational Conference on Machine learning, pp. 297–304.\nHochreiter, S. and J. Schmidhuber (1997). Long short-term memory. Neural Computation 9(8), 1735–1780.\nHong, Y., Y. Liu, and S. Wang (2009). Granger causality in risk and detection of extreme risk spillover\nbetween financial markets. Journal of Econometrics 150(2), 271–287.\nJankowiak, M. and F. Obermeyer (2018). Pathwise derivatives beyond the reparameterization trick. In\nInternational Conference on Machine Learning, pp. 2235–2244. PMLR.\nKerin, J. and H. Engler (2022). On the Lorenz’96 model and some generalizations. Discrete and Continuous\nDynamical Systems - B 27(2), 769–797.\nKhanna, S. and V. Y. F. Tan (2020).\nEconomy statistical recurrent units for inferring nonlinear granger\ncausality. In International Conference on Learning Representations.\nKingma, D. P., S. Mohamed, D. Jimenez Rezende, and M. Welling (2014). Semi-supervised learning with\ndeep generative models. Advances in Neural Information Processing Systems 27.\nKingma, D. P. and M. Welling (2014).\nAuto-encoding variational Bayes.\nIn International Conference on\nLearning Representations.\nKipf, T., E. Fetaya, K.-C. Wang, M. Welling, and R. Zemel (2018). Neural relational inference for interacting\nsystems. In International Conference on Machine Learning, pp. 2688–2697. PMLR.\nKyung, M., J. Gill, M. Ghosh, and G. Casella (2010). Penalized regression, standard errors, and Bayesian\nLassos. Bayesian Analysis 5(2), 369–411.\nLindley, D. V. and A. F. Smith (1972). Bayes estimates for the linear model. Journal of the Royal Statistical\nSociety Series B: Statistical Methodology 34(1), 1–18.\nLorenz, E. N. (1996). Predictability: A problem partly solved. In Proc. Seminar on Predictability, Volume 1.\nReading.\nL¨owe, S., D. Madras, R. Zemel, and M. Welling (2022). Amortized causal discovery: Learning to infer causal\ngraphs from time-series data. In Conference on Causal Learning and Reasoning, pp. 509–525. PMLR.\nMaddison, C. J., A. Mnih, and Y. W. Teh (2017). The concrete distribution: A continuous relaxation of\ndiscrete random variables. In International Conference on Learning Representations.\nMarcinkeviˇcs, R. and J. E. Vogt (2021). Interpretable models for Granger causality using self-explaining\nneural networks. In International Conference on Learning Representations.\nMarx, E., A. Deutschl¨ander, T. Stephan, M. Dieterich, M. Wiesmann, and T. Brandt (2004). Eyes open and\neyes closed as rest conditions: impact on brain activation patterns. Neuroimage 21(4), 1818–1824.\nMiraglia, F., F. Vecchio, P. Bramanti, and P. M. Rossini (2016). EEG characteristics in “eyes-open” versus\n“eyes-closed” conditions: Small-world network architecture in healthy aging and age-related brain degen-\neration. Clinical Neurophysiology 127(2), 1261–1268.\nModarres, M., D. Cochran, D. N. Kennedy, and J. A. Frazier (2023). Comparison of comprehensive quanti-\ntative EEG metrics between typically developing boys and girls in resting state eyes-open and eyes-closed\nconditions. Frontiers in Human Neuroscience 17.\nMontalto, A., S. Stramaglia, L. Faes, G. Tessitore, R. Prevete, and D. Marinazzo (2015). Neural networks\nwith non-uniform embedding and explicit validation phase to assess Granger causality. Neural Networks 71,\n159–171.\nNauta, M., D. Bucur, and C. Seifert (2019). Causal discovery with attention-based convolutional neural\nnetworks. Machine Learning and Knowledge Extraction 1(1), 19.\nRubinov, M. and O. Sporns (2010). Complex network measures of brain connectivity: uses and interpreta-\ntions. Neuroimage 52(3), 1059–1069.\nScarselli, F., M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini (2008). The graph neural network\nmodel. IEEE Transactions on Neural Networks 20(1), 61–80.\nSeth, A. K., A. B. Barrett, and L. Barnett (2015). Granger causality analysis in neuroscience and neuroimag-\ning. Journal of Neuroscience 35(8), 3293–3297.\n40\nShojaie, A. and E. B. Fox (2022). Granger causality: A review and recent advances. Annual Review of Statistics\nand Its Application 9, 289–319.\nSønderby, C. K., T. Raiko, L. Maaløe, S. K. Sønderby, and O. Winther (2016). Ladder variational autoen-\ncoders. Advances in Neural Information Processing Systems 29.\nStam, C. J. (2005). Nonlinear dynamical analysis of EEG and MEG: review of an emerging field. Clinical\nNeurophysiology 116(10), 2266–2301.\nStock, J. H. and M. W. Watson (2001). Vector autoregressions. Journal of Economic Perspectives 15(4),\n101–115.\nTank, A., I. Covert, N. Foti, A. Shojaie, and E. B. Fox (2021). Neural Granger causality. IEEE Transactions on\nPattern Analysis and Machine Intelligence 44(8), 4267–4279.\nTeh, Y. W. and M. I. Jordan (2010). Hierarchical Bayesian nonparametric models with applications. Bayesian\nNonparametrics 1, 158–207.\nTrujillo, L. T., C. T. Stanfield, and R. D. Vela (2017). The effect of electroencephalogram (EEG) reference\nchoice on information-theoretic measures of the complexity and integration of EEG signals. Frontiers in\nNeuroscience 11, 425.\nWikle, C. K., L. M. Berliner, and N. Cressie (1998). Hierarchical Bayesian space-time models. Environmental\nand Ecological Statistics 5, 117–154.\nWu, T., T. Breuel, M. Skuhersky, and J. Kautz (2020). Discovering nonlinear relations with minimum pre-\ndictive information regularization. arXiv preprint arXiv:2001.01885.\n41\n"
}