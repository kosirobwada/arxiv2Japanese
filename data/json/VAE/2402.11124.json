{
    "optim": "DISENTANGLEMENT IN IMPLICIT CAUSAL MODELS VIA SWITCH\nVARIABLE\nShayan Shirahmad Gale Bagi\nDepartment of Electrical and Computer Engineering\nUniversity of Waterloo\nWaterloo, ON N2L 3G1\nsshirahm@uwaterloo.ca\nZahra Gharaee\nDepartment of Systems Design Engineering\nUniversity of Waterloo\nWaterloo, ON N2L 3G1\nzahra.gharaee@uwaterloo.ca\nOliver Schulte\nSchool of Computing Science\nSimon Fraser University\nBurnaby, B.C. V5A 1S6\noschulte@cs.sfu.ca\nMark Crowley\nDepartment of Electrical and Computer Engineering\nUniversity of Waterloo\nWaterloo, ON N2L 3G1\nmcrowley@uwaterloo.ca\nABSTRACT\nLearning causal representations from observational and interventional data in the absence of known\nground-truth graph structures necessitates implicit latent causal representation learning. Implicitly\nlearning causal mechanisms typically involves two categories of interventional data: hard and\nsoft interventions. In real-world scenarios, soft interventions are often more realistic than hard\ninterventions, as the latter require fully controlled environments. Unlike hard interventions, which\ndirectly force changes in a causal variable, soft interventions exert influence indirectly by affecting\nthe causal mechanism. In this paper, we tackle implicit latent causal representation learning in a\nVariational Autoencoder (VAE) framework through soft interventions. Our approach models soft\ninterventions effects by employing a causal mechanism switch variable designed to toggle between\ndifferent causal mechanisms. In our experiments, we consistently observe improved learning of\nidentifiable, causal representations, compared to baseline approaches.\nKeywords Causality · Disentanglement · Soft Intervention · Variational Inference\n1\nIntroduction\nCausal representation learning provides the tantalizing possibility of elevating conventional deep learning models from\nblack boxes to gray boxes such that we can obtain robust, transferable, and explainable representations [1]. In contrast,\nin statistical representation learning, the representations do not generally have a meaningful structure. Although we can\nrepresent statistical models with a Directed Acyclic Graph (DAG), just as in causal models, the connections in such\na graph are volatile and cannot be generalized to different domains. Furthermore, in causal models we can perform\ninterventions and counterfactuals on the variables which can help the model to provide meaningful responses for\nsituations that it has never seen before.\nCausal models can be expressed by both graphical models and structural causal models (SCMs) [2]. A causal graphical\nmodel is a DAG in which there is an edge between variables such as X → Y if X is a parent of Y . We call a variable\nendogenous if it has a parent in the graph and exogenous if it has no parents. Exogenous variables usually correspond to\nuncertainties and noise in the data [3]. In graphical causal models we can only make inferences about variables which\nare causally related to each other, but we cannot make inferences about how they are related to each other. SCMs are a\nway to describe this relationship using functions.\nOne of the long-standing challenges in the field is how to recover the ground-truth causal graph of a system from\nobservations alone. This is known as the identifiablity of causal models problem. Without identifiablity, we cannot\nclaim the learnt representations are causal, since statistical models can also be represented by DAGs where the edges are\narXiv:2402.11124v1  [cs.LG]  16 Feb 2024\nSoftCD\nnot necessarily causal. To see this, let’s consider three random variables X, Y, Z whose joint distribution is factorized as\np(X, Y, Z) = p(Y |X)p(Z|X)p(X). According to the faithfulness assumption, the conditional independencies in the\njoint distribution of variables must also be entailed in its corresponding graph[4]. However, for this joint distribution, the\nconditional independence Z ⊥⊥ Y |X alone can be represented by three different graphs: Z → X → Y , Y → X → Z,\nand Y ← X → Z. With respect to this independence relation, these three graphical models belong to the same Markov\nEquivalence Class (MEC). Thus, the Markov condition in graphs is insufficient for identifying causal models [1] and\nwithout further assumptions or data, we can only learn a MEC of the causal model. Existing works have made different\nassumptions about availability of ground-truth causal variables labels [5], model parameters [6], availability of paired\ninterventional data [7], and availability of intervention targets [8] to ensure identifiability of causal models.\nIn a Variational AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning:\nExplicit Latent Causal Models (ELCMs) [5, 6, 9, 10, 8, 11] and Implicit Latent Causal Models (ILCMs) [7]. In ELCMs,\nthe latents are the causal variables and the adjacency matrix of the causal graph is parameterized and integrated in the\nprior of the latents such that the prior of latents is factorized according to the Causal Markov Condition [12]. This\napproach in causal representation learning is highly susceptible to being stuck in local minima as it is hard to learn\nrepresentations without knowing the graph, and it is hard to learn the graph without knowing the representations. ILCMs\n[7] were introduced to circumvent this “chicken-and-egg” problem by using solution functions, which can implicitly\nmodel edges in the causal graph rather than explicitly modeling the entire adjacency matrix of the causal model. In\nILCMs the latents are the exogenous variables and the there is no explicit parameterization for the graph.\nLet’s consider a simple SCM to understand the motivation behind solution functions:\nZ1 = f1(E1)\n&\nZ2 = f2(Z1, E2),\n(1)\nwhere E1 and E2 are exogenous variables, f1 and f2 are the actual causal mechanisms, and Z1 and Z2 are the causal\nvariables being modelled. This example SCM is a general case in which the causal variables are an arbitrary nonlinear\nfunction of parents and exogenous variables. We can now easily express all the causal variables as a function of\nexogenous variables:\n\u001a\nZ1 = f1(E1) = s1(E1, E2)\nZ2 = f2(f1(E1), E2) = s2(E1, E2),\n(2)\nwhere s1 and s2 are the solution functions. We see that s2 takes the two exogenous variables as input, as it must in order\nto map them to the appropriate causal variables. But note that s1 also takes this same form, even though in this case,\nE2 won’t be needed for modeling Z1. We leave it to the solution function to learn which of the exogenous variables\nare ancestral. Each solution function will thus take in all exogenous variables and learn to ignore some of them. This\nmakes solution functions simpler to define than a full adjacency matrix or even multiple predefined functions, and no\nknowledge of the graph structure is needed a priori. One might wonder why exogenous variables are chosen to be\nthe latents in ILCMs. The reason is that exogenous variables must be mutually independent in a SCM, hence, we can\nconveniently factorize the priors of the exogenous variables as p(E) = Πip(Ei). The observed variables are obtained as\nmixture function g of causal variables Z, i.e., X = g(Z).\nIn implicit causal representation learning, the task involves recovering the exogenous variables E from observed\nvariables X and learning solution functions. In [7], interventions are assumed to be hard, but this is unrealistic and\ndoes not align with real-world problems [13]. Hard interventions involve severing the connections between intervened\nvariables and their parents [3]. While in soft interventions, the causal variable is still affected by its parents, thus\nintervention can influence the conditional distribution p(Zi|Zpa) [13]. Soft interventions include hard interventions\nwithin them as a special case by using the intervention do(Zi = zi).\nIn this paper, we propose a novel approach for implicit causal representation learning with soft interventions.\nWe will introduce the causal mechanism switch variable as a way of modeling the effect of soft interventions and\nidentifying the causal variables. Our experiments on both synthetic and large real-world datasets, highlight the efficacy\nof proposed method in identifying causal variables and promising future directions in implicit causal representation\nlearning.\nOur key contributions can be summarized as follows:\n1. A novel approach for implicit causal representation learning with soft interventions.\n2. Employing causal mechanisms switch variable to model the effect of soft interventions.\n3. Theory for identifiability up to reparameterization and causal disentanglement from soft interventions.\n2\nSoftCD\n2\nRelated Work\n2.1\nCausal Representation Learning\nCausal representation learning has recently garnered significant attention [12, 14]. The primary challenge in this\nproblem lies in achieving identifiability beyond the Markov equivalence class [1]. Solely relying on observational data\nnecessitates additional assumptions regarding causal mechanisms, decoders, latent structure, and the availability of\ninterventional data [15, 16, 4, 17, 11, 6, 18, 19, 5]. Recent works have focused on identifying causal models from\ncollected interventional data instead of making strong assumptions about functions of the causal model. Interventional\ndata facilitates identifiability based on relatively weak assumptions [6, 20, 7, 21, 22]. This type of data can be further\ncategorized based on whether it involves soft or hard interventions, and whether the manipulated variables are observed\nand specified or latent. Our focus in this paper is on examining soft interventions, encompassing both observed and\nunobserved variables.\nTable 1: Comparison of proposed method with other recent related work on causal learning from interventional data\nMethods\nCausal Mechanisms\nMixing functions\nInterventions\nExplicit/Implicit\nIdentifiability\nCausalDiscrepancy [23]\nNonlinear\nFull row rank polynomial\nSoft\nExplicit\nPermutation and Affine\nCauCA [22]\nNonlinear\nDiffeomorphism\nSoft\nExplicit\nDifferent based on assumptions\nLinear-CD [24]\nLinear\nLinear\nHard\nExplicit\nPermutation\nScale-I [25]\nNonlinear\nLinear\nHard/Soft\nExplicit\nScale/Mixed\nILCM [7]\nNonlinear\nDiffeomorphism\nHard\nImplicit\nPermutation and reparameterization\ndVAE [26]\nNonlinear\nDiffeomorphism\nHard\nImplicit\nPermutation and reparameterization\nSoftCD (ours)\nNonlinear\nDiffeomorphism\nSoft\nImplicit\nReparameterization\n2.2\nExplicit models vs. Implicit models\nTable 1 presents a comparison of the assumptions and identifiability results between our proposed theory and other\nrelated works on causal representation learning with interventions. In causal representation learning with interventions,\none approach assumes a given causal graph and concentrates on identifying causal mechanisms and mixing functions.\nFor instance, Causal Component Analysis (CauCA) [22] explores soft interventions with a known graph. Alternatively,\nwhen the graph is not provided, explicit models seek to reconstruct it from interventional data [20, 8], potentially\nresulting in a chicken-and-egg problem in causal representation learning [7]. Current methods face the challenge of\nsimultaneously learning the causal graph and other network parameters, especially in the absence of information about\ncausal variables or the graph. Addressing these challenges, [7] recently introduced ILCM, which performs implicit\ncausal representation learning exclusively using hard intervention data. In contrast, our approach introduces a novel\nmethod for learning an implicit model from soft interventions. [7] describes methods for extracting a causal graph\nfrom a learned implicit model, which could be applied to our method as well. In our experiments, we will compare\nour method with ILCM and dvae [26], given their implicit nature and similar experimental settings and assumptions.\nAdditionally, to showcase the superiority of our method over explicit models, we will employ explicit causal model\ndiscovery methods like ENCO [27] and DDS [28], in conjunction with various variants of β-VAE.\n2.3\nHard interventions vs Soft interventions\nThe identification of explicit causal models from hard interventions has been extensively explored. [24] investigate\ncausal disentanglement in linear causal models with linear mixing functions under hard interventions. Similarly, [29]\nfocus on identifying causal models with linear causal mechanisms and nonlinear mixing functions, also utilizing hard\ninterventions. In a more general setting with non-parametric causal mechanisms and mixing functions, [30] examine\nthe identifiability of causal models, utilizing multi-environment data from unknown interventions. Similarly, [31]\nexplore identifiability of causal models using multi-environment data from unknown interventions. [25] investigate the\nidentifiability of causal models with nonlinear causal mechanisms and linear mixing functions, considering both hard\nand soft interventions.\nRecent papers have extended explicit hard interventions to soft interventions. [23], the authors indentify the causal\nmodel from soft interventions, using sparsity of the adjacency matrix as an inductive bias. For implicit models, however,\nsoft interventions give rise to new challenges. Identifiability is more difficult with soft interventions, since the effect\nof causal variables on the observed variables is not clear. Effects could be due either intervention or influences from\nparent variables on the causal variables. Also note that, as we are retaining implicit modeling, hence, there is no\nknowledge of parents. In [7], identifiability is theoretically proven for hard interventions. However, practical results for\ncomplex causal models with over 10 variables indicate increased ambiguity and confounding factors, making model\nidentification less straightforward. In subsequent sections, we’ll explore limitations of [7]’s identifiability theory with\nsoft interventions and propose using the causal mechanisms switch technique from [2, 1] to help the model identify true\ncausal relationships.\n3\nSoftCD\n3\nProposed Method\n3.1\nStructural Causal Models for Intervention Data\nA causal model is used to understand and describe the relationships between different variables and how they influence\neach other through causal mechanisms. A causal model in its most general form can be defined as follows.\nDefinition 3.1. (Structural Causal Models) A structural causal model (SCM) is a tuple C = (F, Z, E, G) with the\nfollowing components:\n• The domain of causal variables Z = Z1 × Z2 × . . . × Zn.\n• The domain of exogenous variables E = E1 × E2 × . . . × En.\n• a directed acyclic graph G(C) over the causal and exogenous variables.\n• causal mechanism fi ∈ F which maps an assignment of parent values for the parents Zpai plus an exogenous\nvariable value for Ei to a value of causal variable Zi.\nA decoder function g(z) = x maps a vector of causal values to observed values. The causal variables Z are unobserved\nand the goal is to infer them from interventional data. For each causal variable, a solution function si : E → Zi\ndeterministically maps the exogenous variables to a value for Zi. In implicit modeling, we learn the solution functions\nsi directly, rather than defining them through local mechanisms fi. We write s for the set of all solution functions si, so\ns(e) = z. Since exogenous variables are assumed to be mutually independent in a causal model, we can conveniently\nfactorize p(E) = Πip(Ei) whereas the priors of causal variables should be factorized according to the Causal Markov\ncondition [12] i.e., p(Z) = Πip(Zi|Zpai).\nIdentifying causal models from data can be complex and is often studied within classes of models such as those\nidentifiable up to affine transformations. For example, in the context of nonlinear Independent Component Analysis\n(ICA), the generative process also involves a mixture function g of latent causal variables Z ∈ Rn, resulting in\nobservations X ∈ Rn [11, 32]. However, a significant distinction between causal representation learning and nonlinear-\nICA is that in the former, the causal variables Z may have complex dependencies. Our objective in this paper is to\nrecover E from X and eventually map E to Z using solution functions.\nIdentifying a causal model from observational data is not trivial and requires assumptions on the parameters of the\nmodel [6]. Adding information about interventions in addition to observations, helps to identify causal variables by\nexhibiting the effect of changing a causal variable on the observed variables. An interventional data point (x, ˜x, i)\nincludes the pre-intervention observation x, the post-intervention observation ˜x, and intervention target i ∈ I where I\nis the set of intervention targets. The post-intervention data is generated by a soft intervention that targets one of the\ncausal variables Z. A soft intervention can be represented with causal mechanism switch variable V ∈ Rn. Adding\nswitch variables leads to the concept of an implicit augmented SCM.\nDefinition 3.2. (Implicit Augmented Structural Causal Models) The Augmented Structural causal models (ASCMs) are\ndefined as A = (F, Z, E, G, V) where V ∈ Rn is the causal mechanism switch variable which models the effect of soft\ninterventions on causal mechanisms F:\n∀i, ˜Zi = ˜si( ˜E/i, ˜Ei) = si(E/i, Ei, V),\nwhere ˜si is the new solution function resulted from the soft intervention, ˜Zpai is the altered set of parents due to\nintervention, and ˜Ei is the post-intervention exogenous variable.\nCausal mechanism switch variable:The intuition behind using V is to separate the effect of soft intervention into\ntwo: (1) The effect on causal mechanisms and parents (2) The effect on exogenous variable. Let’s consider a set\nof images containing objects where an action is performed on them (see images in Figure A5). We can say that\ncausal variables in these images are the objects’ attributes such as shape, color, and size and the actions change these\nattributes. Furthermore, it can be asserted that the camera angle within a given image may influence the shape of\nthe object. This could be conceptualized as a shift in the conditional distribution of object attributes conditioned on\ntheir respective parent elements, resulting from various actions. Consequently, the images can be seen as a result of\nsoft interventions where an action changes the intrinsic attributes of an object but also changes the causal variable’s\nconditional distribution. In this case, if we had a knowledge of how the camera angle is changed, then we could separate\nthe effect of soft intervention. In other words, if V is observed, then we can extract the effect of the intervention that we\nare interested in (i.e., the effect on the causal variable itself). Observability of V is given as an assumption below:\n4\nSoftCD\nThe new parent set of Zi, post intervention, will be g\nPAi = PAi ∪ V which is related to Zi by the following conditional\nprobability:\np(zi|f\npai) =\n\u001ap(zi|pai, v)\nif zi is intervened softly.\np(zi|pai)\nif no interventions.\n(3)\nIn Appendix A2, we discuss how V can also handle the change in the set of parents due to a soft intervention. The\nusage of V in soft interventions is analogous to augmented networks in [2] which were mainly designed for hard\ninterventions. Pearl even foresaw this possibility in the original paper, saying \"One advantage of the augmented\nnetwork representation is that it is applicable to any change in the functional relationship fi and not merely to the\nreplacement of fi by a constant.\"[2]\n3.2\nData Generating Process\nThe data generation process for an implicit SCM can be defined as follows.\nDefinition 3.3 (Data Generation Process). Consider a latent causal model where the underlying SCM is defined by\na continuous latent space E ∈ Rn with independent probabilities p(Ei) which admits a solution function s. The\npre-intervention data is generated as:\ne ∼ p(E),\nz = s(e),\nx = g(z).\n˜e ∼ p( ˜E),\n˜z = ˜s(˜e),\n˜x = g(˜z).\n(4)\nM\nV\nV =\n\"!\n⋮\n\"\"\nEncoder\n𝑿\nPre\nIntervention\nPost\nIntervention\nFC\nFC\nCausal \nMechanism \nSwitch\nLocation\nScale\nSolution Function (S)\nEncoder\n𝑿\" − 𝑿\n𝑋\"  − 𝑋\nM\nV\n! =\n!!\n⋮\n!\"\nM\nV\n!̃ =\n!̃!\n⋮\n!̃\"\nDecoder\nDecoder\n𝒁\"\n𝑋\n𝑋\"\n𝑿\"\nFigure 1: General overview of the SoftCD for learning an encoder, decoder, and solution function from interventional\ndata. FC represents a fully connected neural network, and S the solution function.\nAssumption 3.4. (Identical correspondence assumptions)\n1. Atomic Interventions: For every paired sample (x, ˜x), only one causal variable is targeted by an intervention.\n2. Known Targets: Targets of soft interventions are known.\n3. Post-intervention Exogenous Variables: The exogenous value changes only for the intervened causal variable,\nwhile the others maintain their pre-intervention values.\n\u001aei ̸= ˜ei\nif\ni ∈ I\nei = ˜ei\notherwise\n4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability [11].\nThese interventions should modify causal mechanisms to ensure non-overlapping conditional distributions of\ncausal variables (refer to Figure A2).\n5\nSoftCD\n𝑧̃!!\"\n𝑧̃!\n𝑧\n𝑧̃!!\"\n𝑧\"\nHard \nIntervention\n𝑰\n𝑧̃#!\n\"\n=\n𝑧̃#!\n\"\n𝑧̃#\n𝑓!\n𝜑\"!\n𝜑!!\n𝑰\n𝑓!!#\n𝜑\n𝑧\n(a)\n𝑧̃!!\"\n𝑧̃!\n𝑧\n𝑧̃!!\"\n𝑧\"\n𝑧\nSoft \nIntervention\n𝑰\n𝑰\n𝜑!!\n𝑧̃#!\n\"\n=\n𝑧̃#!\n\"\n𝜑\"!\n𝑓#!\n𝑓\"\n𝑓\"!#\n𝜑\n𝑓#!!#\n(b)\nFigure 2: Causal graph models in the presence of Hard (a) and Soft (b) interventions.\n5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss and avoids\nabrupt changes in the function’s image.\nWithout these assumptions, observed changes in ˜x become ambiguous, as multiple interventions or alterations in\nexogenous variables may result in overlapping effects on the observed variables. For instance, when examining the\ncombined impact of both heater and lights on room temperature, changing them simultaneously would obscure the\nprecise influence of each on the room temperature.\n3.3\nTheoretical Identifiability Analysis for Implicit SCMs with Soft Interventions\nSince our focus in this paper is to disentangle the causal variables using soft interventions, first we define causal\ndisentanglement and identifiability up to reparameterization.\nDefinition 3.5. (Component-wise Transformation) Let ϕ be a transformation (1-1 onto mapping) between product\nspaces ϕ : Πn\ni=1Xi → Πn\ni=1Yi. If there exist local transformations ϕi such that ∀i, ∀x, ϕ(x1, x2, ..., xn)i = ϕi(xi),\nthen ϕ is a component-wise transformation.\nDefinition 3.6. (Equivalence up to component-wise reparameterization)\nLet M = (A, X, g, I) and M′ =\n(A′, X, g′, I) be two Latent Causal Models (LCM) based on ASCMs A, A′ with shared observation space X, shared\ncausal variables Z, shared intervention targets I, and respective decoders g and g′.\nWe say that M and M′ are equivalent up to component-wise reparameterization M ∼r M′ if there exists a component-\nwise transformation ϕZ from the causal variables Z to the causal variables Z′ and a component-wise transformation\nϕE between E and E′ such that:\n1. Indices are preserved (i.e., ϕi(zi) = z′\ni and ϕi(ei) = e′\ni). Corresponding edges are preserved (i.e., Zi → Zj\nholds in G iff Z′\ni → Z′\nj holds in G′. Edges Ei → Zi should be preserved as well.)\n2. The exogenous transformation preserves the probability measure on exogenous variables pE′ = (ϕE)∗pE\n(Definition A2.2).\n3. The causal transformation preserves the probability measure on causal variables pZ′ = (ϕZ)∗pZ (Definition\nA2.2).\nDefinition 3.7 (Disentanglement [11]). Given a groundtruth LCM M∗, we say a learned LCM M is disentangled\nwhen M∗ and M are equivalent up to reparameterization.\nNow we can analyze how and where soft interventions violate identifiability in models that disentangle causal variables\nusing hard interventions.\nSoft intervention:Let’s consider two SCMs C and C′ and use o to denote all variables except i. The string diagrams for\nsoft and hard interventions in C and C′ are depicted in Figure 2. The major difference of soft intervention (Figure 2(b))\nwith hard intervention (Figure 2(a)) is that ˜Zi is no longer disconnected from its parents and its causal mechanism fi is\naffected by the intervention. Thus, with a hard intervention, we know the post-intervention parents of a node Zi (there\nare none), whereas with soft interventions, the parents themselves may not change. This is why identifying the causal\nmechanisms is more difficult for soft interventions. Soft intervention data yield fewer constraints on the causal graph\nstructure than hard intervention data.\n6\nSoftCD\nIn order for the causal model to be identifiable according to Definition 3.6, Z′\ni should only be the function of its\ncorresponding Zi and not Zo. Mathematically, we say that ϕ should be a component-wise transformation according to\nDefinition 3.5. This characteristic will be violated in the soft intervention case and ˜\nZ′\ni′ will no longer be a function of\n˜Zi alone. To address this and model the effect of soft interventions, we introduce the causal mechanism switch variable\nV. As soft interventions can also alter the set of parents of a causal variable, we propose to use a modulated form of V\nto model the soft intervention effects on each variable as an additive effect with a nonlinear function hi such that:\n∀i, ˜Zi = ˜si( ˜E/i, ˜Ei) = si(E/i, ˜Ei, V) ≈ si(E/i, ˜Ei) + hi(V).\n(5)\nAs the parental set for each causal variable is not known, the inclusion of V in Equation 5 enables the model to\nencompass variations in the parental sets of all causal variables in V. Therefore, there is a switch variable Vi for each\ncausal variable Zi. Subsequently, it can modulate the impact of a soft intervention on the intervened causal variable\nusing hi.\nAssumption 3.8. (Observability of modulated V) Given an intervention sample (x, ˜x, i) and linear decoders, we can\napproximate the soft intervention effects hi(V) as follows:\n˜z − z = ∆eA − h(v) using Equation 5\n˜x − x = g(˜z) − g(z) ≈ g(˜z − z) = g(∆eA − h(v)),\nwhere h(v) and ∆eA are the vectors indicating the change in causal mechanism and change in effect of ancestral\nexogenous variables of each causal variable, respectively. Note that elements of h(v) will be all zero except for the\nintervened causal variable. Consequently, with linear mixing functions and some pre-processing on observed samples\n(here subtraction), we can observe hi(v).\nTheorem 3.9. (Identifiability of augmented latent causal models. The proof is given in Appendix A2.)\nLet M =\n(A, X, g, I) and M′ = (A′, X, g′, I) be two LCMs with shared observation space X and shared intervention targets\nI. Suppose the following conditions are satisfied:\n1. Identical correspondence assumptions explained in 3.4.\n2. Soft interventions satisfy Assumption 3.8.\n3. The causal and exogenous variables are real-valued.\nThen the following statements are equivalent:\n• Two LCMs M and M′ assign the same likelihood to interventional and observational data i.e., pX\nM(x, ˜x) =\npX ′\nM′(x, ˜x).\n• M and M′ are disentangled, that is M ∼r M′ according to Definition 3.6.\nThe proof is in Appendix A2.\n3.4\nTraining Objective\nConsequently, there will be three latent variables in SoftCD:\n1. A causal mechanism switch variable V.\n2. The pre-intervention causal variables E.\n3. The post-intervention causal variables ˜E.\nAs the data log-likelihood log p(x, ˜x, x − ˜x) ≡ log p(x, ˜x) is intractable, we utilize an ELBO approximation as training\nobjective:\nlog p(x, ˜x) ≥Eq(e,˜e,v|x,˜x)\nh\nlog p(x, ˜x|e, ˜e, v)\ni\n− KLD(q(e, ˜e, v|x, ˜x)||p(e, ˜e, x))\n= Eq(v|˜x−x)·q(e|x)·q(˜e|˜x)\nh\nlog(p(x|e)p(˜x|˜e)p(˜x − x|v))\ni\n− KLD(q(v|˜x − x) · q(e|x) · q(˜e|˜x)||p(˜e|e, v)p(v)p(e))\n(6)\nThe observations are encoded and decoded independently. The KLD term regularizes the encodings to share the\nlatent intervention model p(˜e|e, v)p(v)p(e) that is shared across all data points. The components of this model can be\ninterpreted as follows:\n1. p(e) is the prior distribution over exogenous variables e.\n7\nSoftCD\n2. p(v) is the prior distribution over switch variables v.\n3. p(˜e|e, v) is a transition model that shows how the exogeneous variables change as a function of the intervention.\nWe factorize the posterior with a mean-field approximation q(v, e, ˜e|x, ˜x) = q(v|˜x − x) · q(e|x) · q(˜e|˜x) and, following\nour data generation model (Figure A1), the reconstruction probability as p(x, ˜x|e, ˜e, v) = p(x|e)p(˜x|˜e)p(˜x − x|v).\nThe prior over latent variables is factorized as p(˜e, e, v) = p(˜e|e, v)p(v)p(e)(Figure A1). As exogenous variables are\nmutually independent p(e) = Πip(ei) and we assume p(ei) and p(v) to be standard Gaussian (See Appendix A2).\nFurthermore, as we assume ei = ˜ei for all non-intervened variables, the p(˜e|e, v) will be as follows:\np(˜e|e, v) = Πi/∈Iδ(˜ei − ei)Πi∈Ip( ˜ei|ei, v) = Πi/∈Iδ( ˜ei − ei)Πi∈Ip( ˜zi|ei, v)\n\f\f\f\f\n∂ ˜zi\n∂ ˜ei\n\f\f\f\f\n(7)\nThe last equality is obtained from the Change of Variable Rule in probability theory, applied to the solution function\n˜zi = ˜si(˜ei; e, v) that maps the exogeneous variable ˜ei to the causal variable ˜zi. We implement the solution function\nusing a location-scale noise models [33] as also practiced in [7], which defines an invertible diffeomorphism. For\nsimplicity, in our experiments, we are only going to change the loc network in post-intervention. Therefore, hi(v) will\nbe used as:\n˜zi = ˜si(˜ei; e, v) = ˜ei − (loc(e/i) + hi(v))\nscale(e/i)\n,\n(8)\nwhere loc : Rn−1 → R and scale : Rn−1 → R are fully connected networks calculating the first and second moments,\nrespectively. We assume p(˜zi|ei, v) to be a Gaussian whose mean is determined by ei and v as ei is a parent of ˜zi and\nwe do not have knowledge about the other ancestors in implicit modeling. We also obtain v from ˜x − x to ensure that v\nshould capture the effects of soft intervention, since we assume the changes in ˜x are due to the soft intervention only.\nThe general overview of the model is illustrated in Figure 1.\n4\nExperiments and Results\nThe experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement to identify the true\ncausal graph from pairs of observations (x, ˜x), and (2) Action Inference to make supervised inferences about actions\ngenerated from the post-intervention samples using information about the values of the manipulated causal variables.\n4.1\nDatasets\n4.1.1\nSynthetic Dataset\nWe generate simple synthetic datasets with X = Z = Rn. For each value of n, we generate ten random DAGs, a\nrandom location-scale SCM, then a random dataset from the parameterized SCM. To generate random DAGs, each\nedge is sampled in a fixed topological order from a Bernoulli distribution with probability 0.5. The pre-intervention and\npost-intervention causal variables are obtained as:\nzi = scale(zpai)ei + loc(zpai)\nSoft-Intervention.\n−−−−−−−−−→ ˜zi = scale(zpai) ˜ei + f\nloc(zpai),\n(9)\nwhere the loc and scale networks are changed in post intervention. The pre-intervention loc and post-intervention f\nloc\nnetwork weights are initialized with samples drawn from N(0, 1) and N(3, 1), respectively. The scale is constant 1 for\nboth pre-intervention and post-intervention samples. Both ei and ˜ei are sampled from a standard Gaussian. The causal\nvariables are mapped to the data space through a randomly sampled SO(n) rotation. For each dataset, we generate\n100,000 training samples, 10,000 validation samples, and 10,000 test samples.\n4.1.2\nAction Datasets\nCausal-Triplet datasets are designed specifically for actionable counterfactuals [34] with paired images where few or\nmore global scene properties might change such as camera view and object occlusions. Therefore, the images can be\nseen as a result of soft intervention where an action is performed on an object but other subtle changes are also allowed.\nThese datasets [34] consist of: images obtained from a photo-realistic simulator of embodied agents, ProcTHOR [35],\nand the other contains images repurposed from a real-world video dataset of human-object interactions [36]. The former\none contains 100k images in which 7 types of actions manipulate 24 types of objects in 10k distinct ProcTHOR indoor\nenvironments. The latter consists of 2632 image pairs, collected under a similar setup from the Epic-Kitchens dataset\nwith 97 actions manipulating 277 objects.\n4.2\nMetrics\nFor the causal disentanglement task, we are going to use the DCI scores [37]. Causal disentanglement score quantifies\nthe degree to which Zi factorises or disentangles the Z∗. Causal disentanglement Di for Zi is calculated as Di =\n(1 − HK(Pi.)) = (1 + PK−1\nk=0 Pik logK Pik) where Pij =\nRij\nPK−1\nk=0 Rik and Rij denotes the probability of Zi being\nimportant for predicting Z∗\nj . Total causal disentanglement is the weighted average P\ni ρiDi where ρi =\nP\nj Rij\nP\nij Rij .\n8\nSoftCD\nCausal Completeness quantifies the degree to which each Z∗\ni is captured by a single Zi. Causal completeness is\ncalculated as Cj = (1 − HD( ˜P.j)) = (1 + PD−1\nd=0 ˜Pdj logD ˜Pij). D and K here are equal to the dimension of Z∗ and\nZ which is n. For the action inference task, we will use classification accuracy as a metric.\n5\nResults\n5.1\nCausal Disentanglement\nWe generated a dataset for the soft interventions and trained the models of SoftCD, ILCM, β-VAE and D-VAE for\n10 different seeds, which generated 10 different causal graphs. We selected 4 causal variables to encompass complex\ncausal structures, including forks, chains, and colliders. Table 2 displays the Causal Disentanglement and Causal\nCompleteness scores for all models, computed on the test data.\nTable 2: Comparison of identifiability results\nGraph\nCausal Disentanglement\nCausal Completeness\nModel\nName\nβ-VAE\nd-VAE\nILCM\nSoftCD\nβ-VAE\nd-VAE\nILCM\nSoftCD\nG1\n0.39\n0.54\n0.69\n0.81\n0.53\n0.68\n0.76\n0.87\nG2\n0.15\n0.72\n0.74\n0.83\n0.24\n0.77\n0.79\n0.87\nG3\n0.25\n0.51\n0.67\n0.97\n0.54\n0.56\n0.77\n0.97\nG4\n0.19\n0.50\n0.65\n0.69\n0.41\n0.69\n0.76\n0.79\nG5\n0.25\n0.44\n0.32\n0.42\n0.45\n0.54\n0.37\n0.50\nG6\n0.51\n0.62\n0.73\n0.98\n0.63\n0.69\n0.73\n0.98\nG7\n0.35\n0.49\n0.70\n0.76\n0.65\n0.74\n0.88\n0.90\nG8\n0.48\n0.54\n0.50\n0.60\n0.61\n0.63\n0.62\n0.69\nG9\n0.31\n0.68\n0.83\n0.86\n0.40\n0.76\n0.86\n0.87\nG10\n0.15\n0.39\n0.52\n0.32\n0.42\n0.56\n0.82\n0.50\nThe results in Table 2 indicate that our method SoftCD can identify the true causal graph in most cases. The worst\nresults are seen for graphs G5 and G10. As mentioned in [12, 17], causal graphs are sparse and in the G5 case, where\nthe graph is fully connected, the proposed method cannot identify the causal variables well. Furthermore, in the next\nexperiment we are going to examine the factors affecting causal disentanglement such as the number of edges in the\ngraph and the intensity of soft intervention effect. These findings can explain why SoftCD cannot identify causal\nvariables in G10 despite its sparsity.\nTable 3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets under different\nsettings. Z and zi show whether all causal variables (Z), or only the intervened casual variable (zi) are used for the\nprediction task. R64 and R32 denote images with resolutions 64 × 64 and 32 × 32, respectively.\nEpic-Kitchens\nProcTHOR\nAction Accuracy\nObject Accuracy\nAction Accuracy\nObject Accuracy\nMethod\nZ;R64\nzi;R64\nzi;R32\nZ;R64\nzi;R64\nzi;R32\nZ;R64\nzi;R64\nzi;R32\nZ;R64\nzi;R64\nzi;R32\nβ − V AE [38]\n0.30\n0.17\n0.13\n0.20\n0.05\n0.05\n0.32\n0.20\n0.22\n0.41\n0.35\n0.35\nd − V AE [26]\n0.19\n0.60\n0.46\n0.20\n0.14\n0.1\n0.27\n0.60\n0.63\n0.38\n0.77\n0.78\nILCM [7]\n0.17\n0.25\n0.27\n0.18\n0.06\n0.07\n0.21\n0.36\n0.35\n0.34\n0.54\n0.59\nSoftCD (ours)\n0.17\n0.63\n0.66\n0.21\n0.14\n0.16\n0.22\n0.75\n0.95\n0.36\n0.78\n0.81\n5.2\nFactors Affecting Causal Disentanglement\nIn this experiment, we consider the graph G3, which has the best identifiability, and change the intensity of soft\nintervention and number of edges in its data generation process. To change the intensity, the post-intervention f\nloc\n9\nSoftCD\nnetwork weights are initialized with samples drawn from N(1, 1) (almost similar to loc) and N(10, 1) (significantly\ndifferent from loc). To change the number of edges, we consider a chain and fully-connected graph.\nTable 4: SoftCD performance on different configurations of G5\nEdges\nPost-intervention\nCausal\nCausal\ncausal mechanism\nDisentanglement\nCompleteness\nChain\nDefault\n0.98\n0.98\nFull\nDefault\n0.89\n0.89\nDefault\nSignificantly different\n0.68\n0.73\nDefault\nAlmost similar\n0.85\n0.86\nTable 5: Action and object accuracy of two explicit models are compared with SoftCD. Experiments are conducted\napplying image with resolution of R64 as the input to the encoder with the intervened casual variable (zi).\nDatasets\nMethods\nAction Accuracy\nObject Accuracy\nEpic-Kitchens\nENCO [27]\n0.39\n0.07\nDDS [28]\n0.49\n0.09\nFixed-order\n0.34\n0.09\nSoftCD (ours)\n0.63\n0.14\nProcTHOR\nENCO [27]\n0.60\n0.64\nDDS [28]\n0.38\n0.44\nFixed-order\n0.43\n0.55\nSoftCD (ours)\n0.75\n0.78\nThe results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is much worse\nin the fully-connected graph than the default graph of G3. The result for significantly different post-intervention\ncausal mechanisms indicate that the switch variable cannot approximate intense effects of soft intervention and more\nsupervision is required to observe V. Similar post-intervention causal mechanisms also do not have sufficient variability\nto disentangle the causal variables as mentioned in Theory 3.9.\n5.3\nAction Inference\nIn this experiment, we show the performance of SoftCD in the real-world Causal-Triplet datasets. Based on the nature\nof actions in this dataset, the causal variables should represent attributes of objects such as shape and color. As the\ndataset consists of images we train all the methods with ResNet encoder and decoder. One of the constraints of ILCM\nand SoftCD is to have a diffeomorphic encoder.\nTo ensure this, we add the following inverse consistency constraint to the loss function to guarantee that decoder is the\ninverse of encoder as: P\ni Eei∼q(ei|x),ˆei∼q(ˆei|ˆx)[(ei − ˆei)2], where ˆx are the reconstructed samples. For the ProcThor\ndataset the number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from the\ndataset as 97 causal variables will be too complex in a VAE setup. The results are shown in Table 3.\nThe results in Table 3 indicate that when including all causal variables to predict actions, SoftCD performs at par with\nthe baseline methods. However, including all causal variables in the action or object inference may cause spurious\ncorrelations. Therefore, we have also experimented with including only the related causal variable in action and\nobject inference. In this setting, SoftCD significantly outperforms the baseline methods which means that it can better\ndisentangle the causal variables.\nWe have also compared SoftCD with explicit causal representation learning methods. ENCO [27] and DDS [28] have\nvariable topological order of causal variables during training. Furthermore, we have included a specific setting where\nthe topological order is fixed during training. As shown in Table 5, our proposed method has superior performance to\nexplicit models as well.\n6\nConclusion\nWe have presented SoftCD, a novel model, which improves implicit causal representation learning during soft interven-\ntions by introducing a causal mechanism switch variable. In evaluations on synthetic and real-world datasets, SoftCD\noutperforms state-of-the-art methods, demonstrating its effectiveness in practical applications. Theoretical contributions\ninclude articulating conditions for achieving causal disentanglement in implicit models through soft interventions. Our\nresults underscore SoftCD’s capacity to discern causal models from soft interventions, establishing it as a promising\navenue for future research.\n10\nSoftCD\nReferences\n[1] Bernhard Schölkopf. Causality for machine learning. CoRR, abs/1911.10500, 2019.\n[2] Judea Pearl. Causality, cambridge university press (2000). Artif. Intell., 169(2):174–179, 2005.\n[3] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal inference in statistics: A primer. John Wiley and\nSons, 2016.\n[4] Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, and Xindong Wu. Causality-based feature\nselection: Methods and evaluations. ACM Comput. Surv., 53(5), 2020. ISSN 0360-0300. doi:10.1145/3409382.\n[5] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang.\nCausalvae: Dis-\nentangled representation learning via neural structural causal models.\nIn IEEE Conference on Computer\nVision and Pattern Recognition, CVPR, pages 9593–9602. Computer Vision Foundation / IEEE, 2021.\ndoi:10.1109/CVPR46437.2021.00947.\n[6] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning.\nIn International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning\nResearch, pages 372–407. PMLR, 2023.\n[7] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation\nlearning. In NeurIPS, 2022.\n[8] Phillip Lippe, Sara Magliacane, Sindy Löwe, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS: causal\nidentifiability from temporal intervened sequences. In International Conference on Machine Learning, ICML,\nvolume 162 of Proceedings of Machine Learning Research, pages 13557–13603. PMLR, 2022.\n[9] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li.\nLearning causal representations\nfor robust domain adaptation. IEEE Transactions on Knowledge and Data Engineering, pages 1–1, 2021.\ndoi:10.1109/TKDE.2021.3119185.\n[10] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks. In\nProceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine\nLearning Research, pages 7154–7163. PMLR, 2019.\n[11] Sébastien Lachapelle, Pau Rodríguez, Yash Sharma, Katie Everett, Rémi Le Priol, Alexandre Lacoste, and Simon\nLacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In\n1st Conference on Causal Learning and Reasoning, CLeaR, volume 177 of Proceedings of Machine Learning\nResearch, pages 428–484. PMLR, 2022.\n[12] Bernhard Schölkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal,\nand Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612–634, 2021.\ndoi:10.1109/JPROC.2021.3058954.\n[13] Juan D. Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results. In\nHugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors,\nAdvances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing\nSystems, NeurIPS, 2020.\n[14] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A survey and\nopen problems. CoRR, abs/2206.15475, 2022. doi:10.48550/arXiv.2206.15475.\n[15] Chaochao Lu, Yuhuai Wu, José Miguel Hernández-Lobato, and Bernhard Schölkopf. Invariant causal repre-\nsentation learning for out-of-distribution generalization. In The Tenth International Conference on Learning\nRepresentations, ICLR, 2022.\n[16] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Weakly supervised disentangled\ngenerative causal representation learning. J. Mach. Learn. Res., 23:241:1–241:55, 2022.\n[17] Ronan Perry, Julius von Kügelgen, and Bernhard Schölkopf. Causal discovery in heterogeneous environments\nunder the sparse mechanism shift hypothesis. In NeurIPS, 2022.\n[18] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with NO TEARS: continuous optimization\nfor structure learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural\nInformation Processing Systems NeurIPS, pages 9492–9503, 2018.\n[19] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft\ninterventions with unknown targets: Characterization and learning. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020.\n11\nSoftCD\n[20] Gregory F. Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational data,\n2013.\n[21] Jiaqi Zhang, Chandler Squires, Kristjan H. Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline\nUhler. Identifiability guarantees for causal disentanglement from soft interventions. CoRR, abs/2307.06250, 2023.\ndoi:10.48550/arXiv.2307.06250.\n[22] Liang Wendong, Armin Keki´c, Julius von Kügelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and\nBernhard Schölkopf. Causal component analysis, 2023.\n[23] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline\nUhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023.\n[24] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via interventions,\n2023.\n[25] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based causal\nrepresentation learning with interventions, 2023.\n[26] Francesco Locatello, Ben Poole, Gunnar Rätsch, Bernhard Schölkopf, Olivier Bachem, and Michael Tschannen.\nWeakly-supervised disentanglement without compromises. In Proceedings of the 37th International Conference on\nMachine Learning,ICML, volume 119 of Proceedings of Machine Learning Research, pages 6348–6359. PMLR,\n2020.\n[27] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity constraints.\nIn The Tenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022.\n[28] Bertrand Charpentier, Simon Kibler, and Stephan Günnemann. Differentiable DAG sampling. In The Tenth\nInternational Conference on Learning Representations, ICLR. OpenReview.net, 2022.\n[29] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard Schölkopf, and Pradeep\nRavikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023.\n[30] Julius von Kügelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin Keki´c, Elias Bareinboim, David M.\nBlei, and Bernhard Schölkopf. Nonparametric identifiability of causal representations from unknown interventions,\n2023.\n[31] Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, and Mark Crowley. Generative causal representation\nlearning for out-of-distribution motion forecasting. In International Conference on Machine Learning, ICML,\nvolume 202 of Proceedings of Machine Learning Research, pages 31596–31612. PMLR, 2023.\n[32] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ICA: sparsity and beyond. In\nNeurIPS, 2022.\n[33] Alexander Immer, Christoph Schultheiss, Julia E. Vogt, Bernhard Schölkopf, Peter Bühlmann, and Alexander\nMarx. On the identifiability and estimation of causal location-scale noise models. In International Conference\non Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 14316–14332.\nPMLR, 2023.\n[34] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard Schölkopf, and Francesco\nLocatello. Causal triplet: An open challenge for intervention-centric causal representation learning. In Conference\non Causal Learning and Reasoning, CLeaR, volume 213 of Proceedings of Machine Learning Research, pages\n553–573. PMLR, 2023.\n[35] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve,\nAniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation.\nAdvances in Neural Information Processing Systems, 35:5982–5994, 2022.\n[36] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide\nMoltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection,\npipeline and challenges for EPIC-KITCHENS-100. Int. J. Comput. Vis., 130(1):33–55, 2022. doi:10.1007/s11263-\n021-01531-2.\n[37] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled\nrepresentations. In 6th International Conference on Learning Representations, ICLR, 2018.\n[38] Irina Higgins, Loïc Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir\nMohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational\nframework. In 5th International Conference on Learning Representations, ICLR, 2017.\n12\nSoftCD\nAppendix\nA1\nGenerative model\nFigure A1 shows our main generative model. It includes a data augmentation step that adds the intervention displacement\n˜x − x as an observed feature that directly represents the effect of an intervention in observation space.\n𝑒\n𝑥#\n𝑥\" − 𝑥\n𝑥\n𝑒̃\n𝑣\n𝑧\n𝑧̃\nFigure A1: Our generative model. x, ˜x represent low-level observed variables. e, ˜e represent higher-level latent\nexogenous variables, and z, ˜z represent higher-level latent causal variables (e.g. objects). v represents the causal\nmechanism switch variable.\nA2\nProof of Identifiability Theorem\nIn order to prove our model is identifiable we need a two additional definitions and some previously stated assumptions.\nDefinition A2.1. (Diffeomorphism) A diffeomorphism between smooth manifolds M and N is a bijective map f :\nM → N, which is smooth and has a smooth inverse.\nDiffeomorphisms preserve information as they are invertible transformations without discontinuous changes in their\nimage.\nDefinition A2.2. (Pushforward measure) Given a measurable function f : A → B between two measurable spaces A\nand B, and a measure p defined on A, the pushforward measure f∗p on B is defined for measurable sets E in B as:\n(f∗p)(E) = p(f −1(E))\nwhere ∗ denotes the pushforward operation.\nIn other words, the pushforward measure f∗p assigns a measure to a set in B by measuring the pre-image of that set\nunder f in the space A.\nTheorem A2.3. (Identifiability of augmented latent causal models) Let M = (A, X, g, I) and M ′ = (A′, X′, g′, I′)\nbe two Augmented Latent Causal Models (ALCM) with identical observation space X = X′ and identical intervention\ntargets I = I′.\nIf the following conditions are satisfied,\n• Assumptions for identical correspondence as explained in Assumption 3.4.\n• Some form of supervision on how the soft interventions affect the causal mechanisms or observability of V as\nexplained in Assumption 3.8.\n• The causal and exogenous variables are real-valued Zi = Z′\ni = Ei = E′\ni = R.\n• The causal mechanism switch variables are real valued V = Rn.\nthen the following statements are equivalent:\n1. The ALCMs assign the same likelihood to interventional and observational data i.e., pX\nM(x, ˜x) = pX\nM ′(x, ˜x).\n2. M ∼r M ′ according to Definition 3.6.\nProof We will proceed to prove the equivalence between statements 1 and 2 by showing the implication is true in each\ndirection.\nA2.1\nM ∼r M′ ⇒ pX\nM(x, ˜x) = pX\nM′(x, ˜x)\nThis direction is fairly straightforward. According to Definition 3.6, the fact that M ∼r M ′ implies that ϕE is measure\npreserving. Therefore, pE\nM′(e′, ˜e′) = (ϕE)∗pE\nM(e, ˜e). Furthermore, considering that ancestry is preserved, ϕZ is\nmeasure preserving, and that causal variables are obtained from their ancestral exogenous variables in implicit models,\n13\nSoftCD\nwe have pZ\nM′(z′, ˜z′) = (ϕZ)∗pZ\nM(z, ˜z). Since models are trained to maximize the log likelihood of p(x, ˜x, ˜x − x) and\nthe latent spaces in M and M ′ have the same distribution, the decoders should yield the same observational distributions\npX\nM(x, ˜x) = pX\nM′(x, ˜x).\nA2.2\npX\nM(x, ˜x) = pX\nM′(x, ˜x) ⇒ M ∼r M′\nLet’s define ϕE = g′−1 ◦ g : E → E′. Since we can express e = s−1(z), we can now define ϕZ as\nϕZ = s′ ◦ g′−1 ◦ g ◦ s−1 : Z → Z′.\n(10)\nTherefore, ϕE = s′−1 ◦ ϕZ ◦ s. Because g and g′ are diffeomorphisms, ϕE is a diffeomorphism as well. Furthermore,\nsince pX\nM = pX\nM′ and ϕE is a diffeomorphism, then pE\nM′ = (ϕE)∗pE\nM. Consequently, ϕE is measure-preserving.\nSimilarly, ϕE is measure-preserving as well since causal mechanisms are diffeomorphisms.\nStep 1: Identical correspondence of edges and nodes Let’s define the set U as U = {E × E|∀I, J ∈ I :\nsupppE,I\nM (e, ˜e|I)∩supppE,I\nM (e, ˜e|J)}. Then, assuming atomic interventions and counterfactual exogenous variables,\npE,I\nM (U|I) = pE,I\nM (U|J) = 0. Therefore, we can say that pE\nM(e, ˜e) = P\nI∈I pE,I\nM (e, ˜e|I)pI\nM(I) is a discrete mixture of\nnon-overlapping distributions pE,I\nM (e, ˜e|I). Similarly, we can say that pE\nM′(e, ˜e) is a discrete mixture of non-overlapping\ndistributions. It can be concluded that as ϕE must map between these distributions, there exists a bijection that also\ninduces a permutation ψ : [n] → [n]. Note: If we had non-atomic interventions or non-counterfactual exogenous\nvariables, then these distributions would have some overlapping. With overlapping distributions, we can no longer\nclaim there is a bijection mapping between these distributions.\nIn space Z, the interventions should also be sufficiently variable in order to have non-overlapping pZ,I\nM (z, ˜z|I)\ndistributions. In the case of soft interventions, ˜z is affected by all ancestral exogenous variables which could be\nancestors of other causal variables as well. Consequently, if the changes in causal mechanisms are not sufficient,\nthe effect of ancestral exogenous variables on causal variables will share some similarities and create overlapping\ndistributions. Similar to pE\nM(e, ˜e|I), we can say that there is a permutation between pZ\nM(z, ˜z|I) as well. Furthermore,\nas we assume the target of interventions are known we have:\n∀I ∈ I : pZ\nM(z, ˜z|I) = pZ\nM′(z, ˜z|I)\nConsequently, the permutation ψ is an identity transformation. The effect of soft intervention with known targets on\nthese conditional distributions is shown in Figure A2.\nStep 2: Component-wise ϕZ Assuming V is observed or g is linear, we can observe how soft intervention changes\nf → ˜f. The causal mechanism switch variable V can be seen as the new and possibly altered set of the parents of\ncausal variables which is why we choose p(v) to be same as p(e) which in our experiments are standard Gaussian.\nFurthermore, h is used to model the change in effect of the parents due to soft intervention. Each causal variable Zi has\nits own hi as the soft intervention effect on causal mechanisms may be different for each causal variable. By modeling\nsoft interventions this way, we can disentangle the effect of soft intervention in higher level representations:\n• Observed changes due to intervention effect of causal mechanisms and parents.\n• Observed changes due to intervention effect on causal variable itself which is integrated in ˜Ei.\nBy using V as an auxiliary variable we can model the changes in parent set and causal mechanisms. Since we also\nuse single encoder and decoder for e and ˜e, the model already sees the effect of e/i on the intervened causal variable\nand observed variables, hence, the interventional data will be informative about ˜ei. Let’s denote all Z except Zi as\nZo. The connection between the causal variables in M and M′ is shown in Figure 2. Assuming that ˜Zi = ˜Z′\ni = R\nand Lemma 2 in [7], it can be concluded that the transformation ˜Zi × Z → ˜Z′\ni is constant in Z. Using the diagram in\nleft hand side of the equality in Figure 2(b), it can be further concluded that ϕZi is constant in ˜\nZo. Therefore, ϕZi is a\ncomponent-wise transformation.\nStep 3: Component-wise ϕE\nUsing the result from previous step that ϕZ is a component-wise transformation, the string diagrams for connections\nbetween E and E′ will be as shown in Figure A4. ϕEi will only depend on EA, where A = anci is the ancestors\nof variable i, and ei. Because s(e)anci, s(e)i, and s′−1(z′)i only depend on ancestors and ϕZ is a component-wise\ntransformation. The first equality in Figure A4 follows from the definition of ϕEi. The second equality holds when\nwe first apply ϕZA and then apply the causal mechanisms. It can be concluded from the most right-hand side diagram\nin Figure A4 that the transformation from E′\ni × EA → E′\ni is constant in EA. Therefore, ϕEi is a component-wise\ntransformation.\n14\nSoftCD\n0.6 0.4 0.20.0 0.2 0.4 0.6 0.8 1.0\nX1\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nX2\nObserved samples\nIntervention on E1\nIntervention on E2\nZ1\n10 0\n10\n20\n30\n40\nZ2\n10\n0\n10\n20\n30\n40\nP\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nM\nM'\nPre-Intervention\n(a)\n(b)\nZ1\n10 0\n10\n20\n30\n40\nZ2\n10\n0\n10\n20\n30\n40\nP\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nM\nM'\nIntervention on Z1\nZ1\n10 0\n10\n20\n30\n40\nZ2\n10\n0\n10\n20\n30\n40\nP\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nM\nM'\nIntervention on Z2\n(c)\n(d)\nFigure A2: The distribution of observed and causal variables in two causal models M and M′, which belong to the\nequivalence class up to reparameterization. (a) There are 10 observed samples in which Z1 or Z2 has been intervened\non. (b) The distribution of causal variables when I = 0 (no intervention) is identical to each other but the range of value\nof causal variables are different and can be mapped to each other using ϕZ. (c) The intervention on Z1 (I = 1). (d) The\nintervention on Z2 (I = 2). For I = 1 and I = 2 the distributions are again identical to each other but are different for\ndifferent targets of intervention as soft interventions change the conditional distribution (condition on parents) of causal\nvariables. Also, for each value of I, the distributions of M and M′ should move in one direction as targets are known.\nA3\nSoft vs. Hard intervention\nIn a causal model, an intervention refers to a deliberate action taken to manipulate or change one or more variables in\norder to observe its impact on other variables within the causal model. Interventions help to study how changes in one\nvariable directly cause changes in another, thereby revealing causal relationships.\nBased on the levels of control and manipulation in a causal intervention, we can have soft vs. hard interventions. A\nhard intervention involves directly manipulating the variables of interest in a controlled manner such as Randomized\nControlled Trials (RCTs). In other words, a hard intervention sets the value of a causal variable Z to a certain value\ndenoted as do(Z = z) [3].\nOn the other hand, soft intervention involves more subtle or less controlled manipulation of variables and changes the\nconditional distribution of the causal variable p(Z|Zpa) → ˜p(Z|Zpa) which can be modeled as ˜zi = ˜fi(zpai, ˜ei) [13].\nLooking at interventions from a graphical standpoint, a hard intervention entails that the intervened node is solely\nimpacted by the intervention itself, with no influence coming from its ancestral nodes. Conversely, in the context of a\nsoft intervention, the representation of the intervened node can be influenced not only by the intervention but also by its\nparent nodes.\n15\nSoftCD\n𝑧̃!\n\"\n𝑧̃!\n𝑧\n𝑧̃!\n\"\n𝑧\n𝑰\n𝑰\n𝜑!!\n𝑧̃#\"\n=\n𝑧̃#\"\n𝜑!\"\n𝑓\"\n𝑓#\n𝑓#$\n𝜑!\n𝑓\"\n$\n𝑽\n𝑽\"\n(a)\n𝑧̃!\n\"\n𝑧̃!\n𝑧\n𝑧̃!\n\"\n𝑰\n𝑰\n𝜑!!\n=\n𝑓\"\n𝑓#\n𝑓\"\n$\n(b)\nFigure A3: (a) String diagram of the causal variables Z and Z′. The triangle indicates sampling I from its distribution.\nThe left-hand side diagram is when ϕZ is applied last and the right-hand side diagram is when ϕZ is applied first. I\nis the intervention which affects intervened causal variable’s mechanism variable. V is used to model the effect of\nintervention on mechanisms and parents. (b) String diagrams after discarding ˜\nZ′o and the disentangled effect of soft\nintervention on ˜Zi modeled by V .\n𝑓!\n\"#$\n𝑓!\n𝑠\"\n𝑓!\n\"#$\n𝑓!\n#\n𝑠\"\n=\n=\n𝜑$!\n𝜀!\n𝜀\"\n𝜀\"\n#\n𝜀!\n𝜀\"\n𝑧!\n𝑧\"\n𝜑%\"\n𝜑%!\n𝑧\"\n#\n𝑧!\n#\n𝜀\"\n#\n𝜀\"\n#\n𝑧\"\n#\n𝑧!\n#\n𝜀\"\n#\n𝜑%\" \n𝑧!\n𝜀!\nFigure A4: String diagrams for connections between E and E′. The triangle indicates sampling variables from their\ncorresponding distributions.\nAs an example, suppose we are trying to understand the causal relationship between different types of diets and weight\nloss. The soft intervention in this scenario could be a switch from a regular diet to a low-carb diet. Switching to a\nlow-carb diet is a voluntary choice made by the individual and there are no external forces or regulations compelling\nthem to make this change (non-coercive).\nThe intervention involves a modification of the individual’s diet rather than a complete disruption since they are\nadjusting the proportion of macronutrients (fats, proteins, and carbs) they consume, which is less disruptive than a\nradical change in eating habits (gradual modification). The individual has autonomy to choose and tailor their diet\naccording to their preferences and health goals so they are empowered to make informed decisions about their dietary\nchoices (behavioural empowerment).\nConversely, if the government or an authority were to intervene and enforce a mandatory low-carb diet through legal\nmeans, this would constitute a hard intervention. In this scenario, regulations would be implemented, prohibiting the\nconsumption of specific carbohydrate-containing foods. Regulatory agencies would be established to oversee and\nensure adherence to the low-carb diet mandate, taking actions such as removing prohibited foods from the market,\nrestricting their import and production, and so on. Individuals caught consuming banned foods would be subject to\nfines, legal repercussions, or other penalties.\nA4\nExperiments\nThis section contains additional details about SoftCD design architectures, datasets, and experiments settings.\n16\nSoftCD\n(a) Pre-Epic-Kitchens\n(b) Pre-Epic-Kitchens\n(c) Pre-Epic-Kitchens\n(d) Pre-Epic-Kitchens\n(a) Post: Valve-locked\n(b) Post: Bread-Inserted\n(c) Post: Clothes-Gathered\n(d) Post: Juice-Poured\n(e) Pre-ProcTHOR\n(f) Pre-ProcTHOR\n(g) Pre-ProcTHOR\n(h) Pre-ProcTHOR\n(e) Post: Cabinet-Open\n(f) Post: Box-Open\n(g) Post: TV-Broken\n(h) Post: TV-On\nFigure A5: In the Causal-Triplet dataset [34], visual representations capture both pre and post-intervention scenarios.\nThe first two rows showcase data samples from Epic-Kitchens, while the third and fourth rows feature samples from\nProcTHOR. Each image in the post-intervention condition is accompanied by labels specifying the corresponding\naction and intervened object. In the images in the first two rows, the agent is performing an action on an object but the\ncamera angle has also changed. So we can say that for example the distribution of causal variables conditioned on the\ncamera angle has been changed due to soft intervention.\nA4.1\nDatasets\nThe Causal-Triplet datasets are consisted of images containing objects in which an action is manipulating the objects\nshown in Figure A5. Examples of actions and objects in these datasets are given in Table A2.\nTable A1: Actions and objects present in the Causal-Triplet images (ProcTHOR Dataset).\nProcTHOR Dataset\nObject\nTelevision\nBed\nBed\nTelevision\nLaptop\nBook\nBox\nAction\nBreak\nClean\nDirty\nTurn off\nTurn on\nOpen\nClose\nBased on the actions and objects, we treat our causal variables as attributes of objects which can be changed by actions.\nTherefore, actions in these datasets are considered as interventions. Assume that z1 corresponds to the attributes of an\nobject, e.g. a door, the target of opening or closing (action’s target) is z1.\nWe use actions’ labels in these datasets to detect the targets of interventions to determine which causal variable has\nbeen intervened upon. Note that informing the model about the target of intervention is not same as informing about the\naction itself (See Table 3).\n17\nSoftCD\nTable A2: Actions and objects present in the Causal-Triplet images (Epic-Kitchens Dataset).\nEpic-Kitchens Dataset\nObject\nTofu\nRice\nHob\nBag\nCupboard\nGarlic\nTap\nWrap\nRice\nCheese\nAction\nInsert\nPour\nWash\nFold\nOpen\nPat\nMove\nCheck\nTransition\nStretch\nObject\nWrap\nSkin\nButton\nLid\nPlate\nEgg\nSponge\nOil\nWater\nDough\nAction\nFlip\nGather\nPress\nLock\nWrap\nDrop\nWater\nCarry\nSmell\nMark\nA4.2\nArchitecture Design\nBased on the SoftCD architecture depicted in Figure 1, we devised a solution function comprising three fully connected\nnetworks. These networks consist of two layers each, with 64 hidden units per layer and ReLU activation functions. For\nour synthetic dataset experiments, we employed a fully connected auto-encoder model, while for the realistic dataset,\nwe utilized ResNet-based networks.\nA5\nResults\nIn our synthetic dataset experiments, SoftCD achieved the highest average performance with a 4-variable causal model.\nWe employed various data generation seeds, resulting in diverse adjacency matrices, reflecting the influence of soft\ninterventions on causal mechanisms and parental impact.\nA5.1\nScalability\nWhile our primary research objective centered on addressing identifiability challenges in implicit causal models under\nsoft interventions, we also conducted an investigation into the scalability of our proposed model. To comprehensively\nassess its performance, we designed experiments covering a range of causal graphs, featuring 5 to 10 variables, with 10\ndifferent seeds for each variable, following a similar experimental setup as our 4-variable causal graph experiments. The\noutcomes of these experiments, comparing SoftCD and ILCM, are presented in Figure A6. By increasing the number of\nvariables in the graph, confounding factors and ambiguities of causal relations increase as well. Consequently, more\nsupervision on V is required to better separate the effect of causal variables themselves on the observed variables.\nD4\nD5\nD6\nD7\nD8\nD9\nD10\nCausal Variables\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nCausal Disentanglement Score\nMeans with Standard Deviations\nMean-SoftCD\nStd-SoftCD\nMean-ILCM\nStd-ILCM\nFigure A6: Causal disentanglement for different number of variables\n18\n"
}