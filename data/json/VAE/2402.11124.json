{
    "optim": "DISENTANGLEMENT IN IMPLICIT CAUSAL MODELS VIA SWITCH VARIABLE Shayan Shirahmad Gale Bagi Department of Electrical and Computer Engineering University of Waterloo Waterloo, ON N2L 3G1 sshirahm@uwaterloo.ca Zahra Gharaee Department of Systems Design Engineering University of Waterloo Waterloo, ON N2L 3G1 zahra.gharaee@uwaterloo.ca Oliver Schulte School of Computing Science Simon Fraser University Burnaby, B.C. V5A 1S6 oschulte@cs.sfu.ca Mark Crowley Department of Electrical and Computer Engineering University of Waterloo Waterloo, ON N2L 3G1 mcrowley@uwaterloo.ca ABSTRACT Learning causal representations from observational and interventional data in the absence of known ground-truth graph structures necessitates implicit latent causal representation learning. Implicitly learning causal mechanisms typically involves two categories of interventional data: hard and soft interventions. In real-world scenarios, soft interventions are often more realistic than hard interventions, as the latter require fully controlled environments. Unlike hard interventions, which directly force changes in a causal variable, soft interventions exert influence indirectly by affecting the causal mechanism. In this paper, we tackle implicit latent causal representation learning in a Variational Autoencoder (VAE) framework through soft interventions. Our approach models soft interventions effects by employing a causal mechanism switch variable designed to toggle between different causal mechanisms. In our experiments, we consistently observe improved learning of identifiable, causal representations, compared to baseline approaches. Keywords Causality Â· Disentanglement Â· Soft Intervention Â· Variational Inference 1 Introduction Causal representation learning provides the tantalizing possibility of elevating conventional deep learning models from black boxes to gray boxes such that we can obtain robust, transferable, and explainable representations [1]. In contrast, in statistical representation learning, the representations do not generally have a meaningful structure. Although we can represent statistical models with a Directed Acyclic Graph (DAG), just as in causal models, the connections in such a graph are volatile and cannot be generalized to different domains. Furthermore, in causal models we can perform interventions and counterfactuals on the variables which can help the model to provide meaningful responses for situations that it has never seen before. Causal models can be expressed by both graphical models and structural causal models (SCMs) [2]. A causal graphical model is a DAG in which there is an edge between variables such as X â†’ Y if X is a parent of Y . We call a variable endogenous if it has a parent in the graph and exogenous if it has no parents. Exogenous variables usually correspond to uncertainties and noise in the data [3]. In graphical causal models we can only make inferences about variables which are causally related to each other, but we cannot make inferences about how they are related to each other. SCMs are a way to describe this relationship using functions. One of the long-standing challenges in the field is how to recover the ground-truth causal graph of a system from observations alone. This is known as the identifiablity of causal models problem. Without identifiablity, we cannot claim the learnt representations are causal, since statistical models can also be represented by DAGs where the edges are arXiv:2402.11124v1  [cs.LG]  16 Feb 2024 SoftCD not necessarily causal. To see this, letâ€™s consider three random variables X, Y, Z whose joint distribution is factorized as p(X, Y, Z) = p(Y |X)p(Z|X)p(X). According to the faithfulness assumption, the conditional independencies in the joint distribution of variables must also be entailed in its corresponding graph[4]. However, for this joint distribution, the conditional independence Z âŠ¥âŠ¥ Y |X alone can be represented by three different graphs: Z â†’ X â†’ Y , Y â†’ X â†’ Z, and Y â† X â†’ Z. With respect to this independence relation, these three graphical models belong to the same Markov Equivalence Class (MEC). Thus, the Markov condition in graphs is insufficient for identifying causal models [1] and without further assumptions or data, we can only learn a MEC of the causal model. Existing works have made different assumptions about availability of ground-truth causal variables labels [5], model parameters [6], availability of paired interventional data [7], and availability of intervention targets [8] to ensure identifiability of causal models. In a Variational AutoEncoder (VAE) framework, there are generally two approaches for causal representation learning: Explicit Latent Causal Models (ELCMs) [5, 6, 9, 10, 8, 11] and Implicit Latent Causal Models (ILCMs) [7]. In ELCMs, the latents are the causal variables and the adjacency matrix of the causal graph is parameterized and integrated in the prior of the latents such that the prior of latents is factorized according to the Causal Markov Condition [12]. This approach in causal representation learning is highly susceptible to being stuck in local minima as it is hard to learn representations without knowing the graph, and it is hard to learn the graph without knowing the representations. ILCMs [7] were introduced to circumvent this â€œchicken-and-eggâ€ problem by using solution functions, which can implicitly model edges in the causal graph rather than explicitly modeling the entire adjacency matrix of the causal model. In ILCMs the latents are the exogenous variables and the there is no explicit parameterization for the graph. Letâ€™s consider a simple SCM to understand the motivation behind solution functions: Z1 = f1(E1) & Z2 = f2(Z1, E2), (1) where E1 and E2 are exogenous variables, f1 and f2 are the actual causal mechanisms, and Z1 and Z2 are the causal variables being modelled. This example SCM is a general case in which the causal variables are an arbitrary nonlinear function of parents and exogenous variables. We can now easily express all the causal variables as a function of exogenous variables: \u001a Z1 = f1(E1) = s1(E1, E2) Z2 = f2(f1(E1), E2) = s2(E1, E2), (2) where s1 and s2 are the solution functions. We see that s2 takes the two exogenous variables as input, as it must in order to map them to the appropriate causal variables. But note that s1 also takes this same form, even though in this case, E2 wonâ€™t be needed for modeling Z1. We leave it to the solution function to learn which of the exogenous variables are ancestral. Each solution function will thus take in all exogenous variables and learn to ignore some of them. This makes solution functions simpler to define than a full adjacency matrix or even multiple predefined functions, and no knowledge of the graph structure is needed a priori. One might wonder why exogenous variables are chosen to be the latents in ILCMs. The reason is that exogenous variables must be mutually independent in a SCM, hence, we can conveniently factorize the priors of the exogenous variables as p(E) = Î ip(Ei). The observed variables are obtained as mixture function g of causal variables Z, i.e., X = g(Z). In implicit causal representation learning, the task involves recovering the exogenous variables E from observed variables X and learning solution functions. In [7], interventions are assumed to be hard, but this is unrealistic and does not align with real-world problems [13]. Hard interventions involve severing the connections between intervened variables and their parents [3]. While in soft interventions, the causal variable is still affected by its parents, thus intervention can influence the conditional distribution p(Zi|Zpa) [13]. Soft interventions include hard interventions within them as a special case by using the intervention do(Zi = zi). In this paper, we propose a novel approach for implicit causal representation learning with soft interventions. We will introduce the causal mechanism switch variable as a way of modeling the effect of soft interventions and identifying the causal variables. Our experiments on both synthetic and large real-world datasets, highlight the efficacy of proposed method in identifying causal variables and promising future directions in implicit causal representation learning. Our key contributions can be summarized as follows: 1. A novel approach for implicit causal representation learning with soft interventions. 2. Employing causal mechanisms switch variable to model the effect of soft interventions. 3. Theory for identifiability up to reparameterization and causal disentanglement from soft interventions. 2 SoftCD 2 Related Work 2.1 Causal Representation Learning Causal representation learning has recently garnered significant attention [12, 14]. The primary challenge in this problem lies in achieving identifiability beyond the Markov equivalence class [1]. Solely relying on observational data necessitates additional assumptions regarding causal mechanisms, decoders, latent structure, and the availability of interventional data [15, 16, 4, 17, 11, 6, 18, 19, 5]. Recent works have focused on identifying causal models from collected interventional data instead of making strong assumptions about functions of the causal model. Interventional data facilitates identifiability based on relatively weak assumptions [6, 20, 7, 21, 22]. This type of data can be further categorized based on whether it involves soft or hard interventions, and whether the manipulated variables are observed and specified or latent. Our focus in this paper is on examining soft interventions, encompassing both observed and unobserved variables. Table 1: Comparison of proposed method with other recent related work on causal learning from interventional data Methods Causal Mechanisms Mixing functions Interventions Explicit/Implicit Identifiability CausalDiscrepancy [23] Nonlinear Full row rank polynomial Soft Explicit Permutation and Affine CauCA [22] Nonlinear Diffeomorphism Soft Explicit Different based on assumptions Linear-CD [24] Linear Linear Hard Explicit Permutation Scale-I [25] Nonlinear Linear Hard/Soft Explicit Scale/Mixed ILCM [7] Nonlinear Diffeomorphism Hard Implicit Permutation and reparameterization dVAE [26] Nonlinear Diffeomorphism Hard Implicit Permutation and reparameterization SoftCD (ours) Nonlinear Diffeomorphism Soft Implicit Reparameterization 2.2 Explicit models vs. Implicit models Table 1 presents a comparison of the assumptions and identifiability results between our proposed theory and other related works on causal representation learning with interventions. In causal representation learning with interventions, one approach assumes a given causal graph and concentrates on identifying causal mechanisms and mixing functions. For instance, Causal Component Analysis (CauCA) [22] explores soft interventions with a known graph. Alternatively, when the graph is not provided, explicit models seek to reconstruct it from interventional data [20, 8], potentially resulting in a chicken-and-egg problem in causal representation learning [7]. Current methods face the challenge of simultaneously learning the causal graph and other network parameters, especially in the absence of information about causal variables or the graph. Addressing these challenges, [7] recently introduced ILCM, which performs implicit causal representation learning exclusively using hard intervention data. In contrast, our approach introduces a novel method for learning an implicit model from soft interventions. [7] describes methods for extracting a causal graph from a learned implicit model, which could be applied to our method as well. In our experiments, we will compare our method with ILCM and dvae [26], given their implicit nature and similar experimental settings and assumptions. Additionally, to showcase the superiority of our method over explicit models, we will employ explicit causal model discovery methods like ENCO [27] and DDS [28], in conjunction with various variants of Î²-VAE. 2.3 Hard interventions vs Soft interventions The identification of explicit causal models from hard interventions has been extensively explored. [24] investigate causal disentanglement in linear causal models with linear mixing functions under hard interventions. Similarly, [29] focus on identifying causal models with linear causal mechanisms and nonlinear mixing functions, also utilizing hard interventions. In a more general setting with non-parametric causal mechanisms and mixing functions, [30] examine the identifiability of causal models, utilizing multi-environment data from unknown interventions. Similarly, [31] explore identifiability of causal models using multi-environment data from unknown interventions. [25] investigate the identifiability of causal models with nonlinear causal mechanisms and linear mixing functions, considering both hard and soft interventions. Recent papers have extended explicit hard interventions to soft interventions. [23], the authors indentify the causal model from soft interventions, using sparsity of the adjacency matrix as an inductive bias. For implicit models, however, soft interventions give rise to new challenges. Identifiability is more difficult with soft interventions, since the effect of causal variables on the observed variables is not clear. Effects could be due either intervention or influences from parent variables on the causal variables. Also note that, as we are retaining implicit modeling, hence, there is no knowledge of parents. In [7], identifiability is theoretically proven for hard interventions. However, practical results for complex causal models with over 10 variables indicate increased ambiguity and confounding factors, making model identification less straightforward. In subsequent sections, weâ€™ll explore limitations of [7]â€™s identifiability theory with soft interventions and propose using the causal mechanisms switch technique from [2, 1] to help the model identify true causal relationships. 3 SoftCD 3 Proposed Method 3.1 Structural Causal Models for Intervention Data A causal model is used to understand and describe the relationships between different variables and how they influence each other through causal mechanisms. A causal model in its most general form can be defined as follows. Definition 3.1. (Structural Causal Models) A structural causal model (SCM) is a tuple C = (F, Z, E, G) with the following components: â€¢ The domain of causal variables Z = Z1 Ã— Z2 Ã— . . . Ã— Zn. â€¢ The domain of exogenous variables E = E1 Ã— E2 Ã— . . . Ã— En. â€¢ a directed acyclic graph G(C) over the causal and exogenous variables. â€¢ causal mechanism fi âˆˆ F which maps an assignment of parent values for the parents Zpai plus an exogenous variable value for Ei to a value of causal variable Zi. A decoder function g(z) = x maps a vector of causal values to observed values. The causal variables Z are unobserved and the goal is to infer them from interventional data. For each causal variable, a solution function si : E â†’ Zi deterministically maps the exogenous variables to a value for Zi. In implicit modeling, we learn the solution functions si directly, rather than defining them through local mechanisms fi. We write s for the set of all solution functions si, so s(e) = z. Since exogenous variables are assumed to be mutually independent in a causal model, we can conveniently factorize p(E) = Î ip(Ei) whereas the priors of causal variables should be factorized according to the Causal Markov condition [12] i.e., p(Z) = Î ip(Zi|Zpai). Identifying causal models from data can be complex and is often studied within classes of models such as those identifiable up to affine transformations. For example, in the context of nonlinear Independent Component Analysis (ICA), the generative process also involves a mixture function g of latent causal variables Z âˆˆ Rn, resulting in observations X âˆˆ Rn [11, 32]. However, a significant distinction between causal representation learning and nonlinear- ICA is that in the former, the causal variables Z may have complex dependencies. Our objective in this paper is to recover E from X and eventually map E to Z using solution functions. Identifying a causal model from observational data is not trivial and requires assumptions on the parameters of the model [6]. Adding information about interventions in addition to observations, helps to identify causal variables by exhibiting the effect of changing a causal variable on the observed variables. An interventional data point (x, Ëœx, i) includes the pre-intervention observation x, the post-intervention observation Ëœx, and intervention target i âˆˆ I where I is the set of intervention targets. The post-intervention data is generated by a soft intervention that targets one of the causal variables Z. A soft intervention can be represented with causal mechanism switch variable V âˆˆ Rn. Adding switch variables leads to the concept of an implicit augmented SCM. Definition 3.2. (Implicit Augmented Structural Causal Models) The Augmented Structural causal models (ASCMs) are defined as A = (F, Z, E, G, V) where V âˆˆ Rn is the causal mechanism switch variable which models the effect of soft interventions on causal mechanisms F: âˆ€i, ËœZi = Ëœsi( ËœE/i, ËœEi) = si(E/i, Ei, V), where Ëœsi is the new solution function resulted from the soft intervention, ËœZpai is the altered set of parents due to intervention, and ËœEi is the post-intervention exogenous variable. Causal mechanism switch variable:The intuition behind using V is to separate the effect of soft intervention into two: (1) The effect on causal mechanisms and parents (2) The effect on exogenous variable. Letâ€™s consider a set of images containing objects where an action is performed on them (see images in Figure A5). We can say that causal variables in these images are the objectsâ€™ attributes such as shape, color, and size and the actions change these attributes. Furthermore, it can be asserted that the camera angle within a given image may influence the shape of the object. This could be conceptualized as a shift in the conditional distribution of object attributes conditioned on their respective parent elements, resulting from various actions. Consequently, the images can be seen as a result of soft interventions where an action changes the intrinsic attributes of an object but also changes the causal variableâ€™s conditional distribution. In this case, if we had a knowledge of how the camera angle is changed, then we could separate the effect of soft intervention. In other words, if V is observed, then we can extract the effect of the intervention that we are interested in (i.e., the effect on the causal variable itself). Observability of V is given as an assumption below: 4 SoftCD The new parent set of Zi, post intervention, will be g PAi = PAi âˆª V which is related to Zi by the following conditional probability: p(zi|f pai) = \u001ap(zi|pai, v) if zi is intervened softly. p(zi|pai) if no interventions. (3) In Appendix A2, we discuss how V can also handle the change in the set of parents due to a soft intervention. The usage of V in soft interventions is analogous to augmented networks in [2] which were mainly designed for hard interventions. Pearl even foresaw this possibility in the original paper, saying \"One advantage of the augmented network representation is that it is applicable to any change in the functional relationship fi and not merely to the replacement of fi by a constant.\"[2] 3.2 Data Generating Process The data generation process for an implicit SCM can be defined as follows. Definition 3.3 (Data Generation Process). Consider a latent causal model where the underlying SCM is defined by a continuous latent space E âˆˆ Rn with independent probabilities p(Ei) which admits a solution function s. The pre-intervention data is generated as: e âˆ¼ p(E), z = s(e), x = g(z). Ëœe âˆ¼ p( ËœE), Ëœz = Ëœs(Ëœe), Ëœx = g(Ëœz). (4) M V V = \"! â‹® \"\" Encoder ğ‘¿ Pre Intervention Post Intervention FC FC Causal  Mechanism  Switch Location Scale Solution Function (S) Encoder ğ‘¿\" âˆ’ ğ‘¿ ğ‘‹\"  âˆ’ ğ‘‹ M V ! = !! â‹® !\" M V !Ìƒ = !Ìƒ! â‹® !Ìƒ\" Decoder Decoder ğ’\" ğ‘‹ ğ‘‹\" ğ‘¿\" Figure 1: General overview of the SoftCD for learning an encoder, decoder, and solution function from interventional data. FC represents a fully connected neural network, and S the solution function. Assumption 3.4. (Identical correspondence assumptions) 1. Atomic Interventions: For every paired sample (x, Ëœx), only one causal variable is targeted by an intervention. 2. Known Targets: Targets of soft interventions are known. 3. Post-intervention Exogenous Variables: The exogenous value changes only for the intervened causal variable, while the others maintain their pre-intervention values. \u001aei Ì¸= Ëœei if i âˆˆ I ei = Ëœei otherwise 4. Sufficient Variability: Soft interventions alter causal mechanisms to introduce sufficient variability [11]. These interventions should modify causal mechanisms to ensure non-overlapping conditional distributions of causal variables (refer to Figure A2). 5 SoftCD ğ‘§Ìƒ!!\" ğ‘§Ìƒ! ğ‘§ ğ‘§Ìƒ!!\" ğ‘§\" Hard  Intervention ğ‘° ğ‘§Ìƒ#! \" = ğ‘§Ìƒ#! \" ğ‘§Ìƒ# ğ‘“! ğœ‘\"! ğœ‘!! ğ‘° ğ‘“!!# ğœ‘ ğ‘§ (a) ğ‘§Ìƒ!!\" ğ‘§Ìƒ! ğ‘§ ğ‘§Ìƒ!!\" ğ‘§\" ğ‘§ Soft  Intervention ğ‘° ğ‘° ğœ‘!! ğ‘§Ìƒ#! \" = ğ‘§Ìƒ#! \" ğœ‘\"! ğ‘“#! ğ‘“\" ğ‘“\"!# ğœ‘ ğ‘“#!!# (b) Figure 2: Causal graph models in the presence of Hard (a) and Soft (b) interventions. 5. Diffeomorphic decoder and causal mechanisms: Diffeomorphism guarantees no information loss and avoids abrupt changes in the functionâ€™s image. Without these assumptions, observed changes in Ëœx become ambiguous, as multiple interventions or alterations in exogenous variables may result in overlapping effects on the observed variables. For instance, when examining the combined impact of both heater and lights on room temperature, changing them simultaneously would obscure the precise influence of each on the room temperature. 3.3 Theoretical Identifiability Analysis for Implicit SCMs with Soft Interventions Since our focus in this paper is to disentangle the causal variables using soft interventions, first we define causal disentanglement and identifiability up to reparameterization. Definition 3.5. (Component-wise Transformation) Let Ï• be a transformation (1-1 onto mapping) between product spaces Ï• : Î n i=1Xi â†’ Î n i=1Yi. If there exist local transformations Ï•i such that âˆ€i, âˆ€x, Ï•(x1, x2, ..., xn)i = Ï•i(xi), then Ï• is a component-wise transformation. Definition 3.6. (Equivalence up to component-wise reparameterization) Let M = (A, X, g, I) and Mâ€² = (Aâ€², X, gâ€², I) be two Latent Causal Models (LCM) based on ASCMs A, Aâ€² with shared observation space X, shared causal variables Z, shared intervention targets I, and respective decoders g and gâ€². We say that M and Mâ€² are equivalent up to component-wise reparameterization M âˆ¼r Mâ€² if there exists a component- wise transformation Ï•Z from the causal variables Z to the causal variables Zâ€² and a component-wise transformation Ï•E between E and Eâ€² such that: 1. Indices are preserved (i.e., Ï•i(zi) = zâ€² i and Ï•i(ei) = eâ€² i). Corresponding edges are preserved (i.e., Zi â†’ Zj holds in G iff Zâ€² i â†’ Zâ€² j holds in Gâ€². Edges Ei â†’ Zi should be preserved as well.) 2. The exogenous transformation preserves the probability measure on exogenous variables pEâ€² = (Ï•E)âˆ—pE (Definition A2.2). 3. The causal transformation preserves the probability measure on causal variables pZâ€² = (Ï•Z)âˆ—pZ (Definition A2.2). Definition 3.7 (Disentanglement [11]). Given a groundtruth LCM Mâˆ—, we say a learned LCM M is disentangled when Mâˆ— and M are equivalent up to reparameterization. Now we can analyze how and where soft interventions violate identifiability in models that disentangle causal variables using hard interventions. Soft intervention:Letâ€™s consider two SCMs C and Câ€² and use o to denote all variables except i. The string diagrams for soft and hard interventions in C and Câ€² are depicted in Figure 2. The major difference of soft intervention (Figure 2(b)) with hard intervention (Figure 2(a)) is that ËœZi is no longer disconnected from its parents and its causal mechanism fi is affected by the intervention. Thus, with a hard intervention, we know the post-intervention parents of a node Zi (there are none), whereas with soft interventions, the parents themselves may not change. This is why identifying the causal mechanisms is more difficult for soft interventions. Soft intervention data yield fewer constraints on the causal graph structure than hard intervention data. 6 SoftCD In order for the causal model to be identifiable according to Definition 3.6, Zâ€² i should only be the function of its corresponding Zi and not Zo. Mathematically, we say that Ï• should be a component-wise transformation according to Definition 3.5. This characteristic will be violated in the soft intervention case and Ëœ Zâ€² iâ€² will no longer be a function of ËœZi alone. To address this and model the effect of soft interventions, we introduce the causal mechanism switch variable V. As soft interventions can also alter the set of parents of a causal variable, we propose to use a modulated form of V to model the soft intervention effects on each variable as an additive effect with a nonlinear function hi such that: âˆ€i, ËœZi = Ëœsi( ËœE/i, ËœEi) = si(E/i, ËœEi, V) â‰ˆ si(E/i, ËœEi) + hi(V). (5) As the parental set for each causal variable is not known, the inclusion of V in Equation 5 enables the model to encompass variations in the parental sets of all causal variables in V. Therefore, there is a switch variable Vi for each causal variable Zi. Subsequently, it can modulate the impact of a soft intervention on the intervened causal variable using hi. Assumption 3.8. (Observability of modulated V) Given an intervention sample (x, Ëœx, i) and linear decoders, we can approximate the soft intervention effects hi(V) as follows: Ëœz âˆ’ z = âˆ†eA âˆ’ h(v) using Equation 5 Ëœx âˆ’ x = g(Ëœz) âˆ’ g(z) â‰ˆ g(Ëœz âˆ’ z) = g(âˆ†eA âˆ’ h(v)), where h(v) and âˆ†eA are the vectors indicating the change in causal mechanism and change in effect of ancestral exogenous variables of each causal variable, respectively. Note that elements of h(v) will be all zero except for the intervened causal variable. Consequently, with linear mixing functions and some pre-processing on observed samples (here subtraction), we can observe hi(v). Theorem 3.9. (Identifiability of augmented latent causal models. The proof is given in Appendix A2.) Let M = (A, X, g, I) and Mâ€² = (Aâ€², X, gâ€², I) be two LCMs with shared observation space X and shared intervention targets I. Suppose the following conditions are satisfied: 1. Identical correspondence assumptions explained in 3.4. 2. Soft interventions satisfy Assumption 3.8. 3. The causal and exogenous variables are real-valued. Then the following statements are equivalent: â€¢ Two LCMs M and Mâ€² assign the same likelihood to interventional and observational data i.e., pX M(x, Ëœx) = pX â€² Mâ€²(x, Ëœx). â€¢ M and Mâ€² are disentangled, that is M âˆ¼r Mâ€² according to Definition 3.6. The proof is in Appendix A2. 3.4 Training Objective Consequently, there will be three latent variables in SoftCD: 1. A causal mechanism switch variable V. 2. The pre-intervention causal variables E. 3. The post-intervention causal variables ËœE. As the data log-likelihood log p(x, Ëœx, x âˆ’ Ëœx) â‰¡ log p(x, Ëœx) is intractable, we utilize an ELBO approximation as training objective: log p(x, Ëœx) â‰¥Eq(e,Ëœe,v|x,Ëœx) h log p(x, Ëœx|e, Ëœe, v) i âˆ’ KLD(q(e, Ëœe, v|x, Ëœx)||p(e, Ëœe, x)) = Eq(v|Ëœxâˆ’x)Â·q(e|x)Â·q(Ëœe|Ëœx) h log(p(x|e)p(Ëœx|Ëœe)p(Ëœx âˆ’ x|v)) i âˆ’ KLD(q(v|Ëœx âˆ’ x) Â· q(e|x) Â· q(Ëœe|Ëœx)||p(Ëœe|e, v)p(v)p(e)) (6) The observations are encoded and decoded independently. The KLD term regularizes the encodings to share the latent intervention model p(Ëœe|e, v)p(v)p(e) that is shared across all data points. The components of this model can be interpreted as follows: 1. p(e) is the prior distribution over exogenous variables e. 7 SoftCD 2. p(v) is the prior distribution over switch variables v. 3. p(Ëœe|e, v) is a transition model that shows how the exogeneous variables change as a function of the intervention. We factorize the posterior with a mean-field approximation q(v, e, Ëœe|x, Ëœx) = q(v|Ëœx âˆ’ x) Â· q(e|x) Â· q(Ëœe|Ëœx) and, following our data generation model (Figure A1), the reconstruction probability as p(x, Ëœx|e, Ëœe, v) = p(x|e)p(Ëœx|Ëœe)p(Ëœx âˆ’ x|v). The prior over latent variables is factorized as p(Ëœe, e, v) = p(Ëœe|e, v)p(v)p(e)(Figure A1). As exogenous variables are mutually independent p(e) = Î ip(ei) and we assume p(ei) and p(v) to be standard Gaussian (See Appendix A2). Furthermore, as we assume ei = Ëœei for all non-intervened variables, the p(Ëœe|e, v) will be as follows: p(Ëœe|e, v) = Î i/âˆˆIÎ´(Ëœei âˆ’ ei)Î iâˆˆIp( Ëœei|ei, v) = Î i/âˆˆIÎ´( Ëœei âˆ’ ei)Î iâˆˆIp( Ëœzi|ei, v) \f\f\f\f âˆ‚ Ëœzi âˆ‚ Ëœei \f\f\f\f (7) The last equality is obtained from the Change of Variable Rule in probability theory, applied to the solution function Ëœzi = Ëœsi(Ëœei; e, v) that maps the exogeneous variable Ëœei to the causal variable Ëœzi. We implement the solution function using a location-scale noise models [33] as also practiced in [7], which defines an invertible diffeomorphism. For simplicity, in our experiments, we are only going to change the loc network in post-intervention. Therefore, hi(v) will be used as: Ëœzi = Ëœsi(Ëœei; e, v) = Ëœei âˆ’ (loc(e/i) + hi(v)) scale(e/i) , (8) where loc : Rnâˆ’1 â†’ R and scale : Rnâˆ’1 â†’ R are fully connected networks calculating the first and second moments, respectively. We assume p(Ëœzi|ei, v) to be a Gaussian whose mean is determined by ei and v as ei is a parent of Ëœzi and we do not have knowledge about the other ancestors in implicit modeling. We also obtain v from Ëœx âˆ’ x to ensure that v should capture the effects of soft intervention, since we assume the changes in Ëœx are due to the soft intervention only. The general overview of the model is illustrated in Figure 1. 4 Experiments and Results The experiments conducted in this paper address two downstream tasks; (1) Causal Disentanglement to identify the true causal graph from pairs of observations (x, Ëœx), and (2) Action Inference to make supervised inferences about actions generated from the post-intervention samples using information about the values of the manipulated causal variables. 4.1 Datasets 4.1.1 Synthetic Dataset We generate simple synthetic datasets with X = Z = Rn. For each value of n, we generate ten random DAGs, a random location-scale SCM, then a random dataset from the parameterized SCM. To generate random DAGs, each edge is sampled in a fixed topological order from a Bernoulli distribution with probability 0.5. The pre-intervention and post-intervention causal variables are obtained as: zi = scale(zpai)ei + loc(zpai) Soft-Intervention. âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’â†’ Ëœzi = scale(zpai) Ëœei + f loc(zpai), (9) where the loc and scale networks are changed in post intervention. The pre-intervention loc and post-intervention f loc network weights are initialized with samples drawn from N(0, 1) and N(3, 1), respectively. The scale is constant 1 for both pre-intervention and post-intervention samples. Both ei and Ëœei are sampled from a standard Gaussian. The causal variables are mapped to the data space through a randomly sampled SO(n) rotation. For each dataset, we generate 100,000 training samples, 10,000 validation samples, and 10,000 test samples. 4.1.2 Action Datasets Causal-Triplet datasets are designed specifically for actionable counterfactuals [34] with paired images where few or more global scene properties might change such as camera view and object occlusions. Therefore, the images can be seen as a result of soft intervention where an action is performed on an object but other subtle changes are also allowed. These datasets [34] consist of: images obtained from a photo-realistic simulator of embodied agents, ProcTHOR [35], and the other contains images repurposed from a real-world video dataset of human-object interactions [36]. The former one contains 100k images in which 7 types of actions manipulate 24 types of objects in 10k distinct ProcTHOR indoor environments. The latter consists of 2632 image pairs, collected under a similar setup from the Epic-Kitchens dataset with 97 actions manipulating 277 objects. 4.2 Metrics For the causal disentanglement task, we are going to use the DCI scores [37]. Causal disentanglement score quantifies the degree to which Zi factorises or disentangles the Zâˆ—. Causal disentanglement Di for Zi is calculated as Di = (1 âˆ’ HK(Pi.)) = (1 + PKâˆ’1 k=0 Pik logK Pik) where Pij = Rij PKâˆ’1 k=0 Rik and Rij denotes the probability of Zi being important for predicting Zâˆ— j . Total causal disentanglement is the weighted average P i ÏiDi where Ïi = P j Rij P ij Rij . 8 SoftCD Causal Completeness quantifies the degree to which each Zâˆ— i is captured by a single Zi. Causal completeness is calculated as Cj = (1 âˆ’ HD( ËœP.j)) = (1 + PDâˆ’1 d=0 ËœPdj logD ËœPij). D and K here are equal to the dimension of Zâˆ— and Z which is n. For the action inference task, we will use classification accuracy as a metric. 5 Results 5.1 Causal Disentanglement We generated a dataset for the soft interventions and trained the models of SoftCD, ILCM, Î²-VAE and D-VAE for 10 different seeds, which generated 10 different causal graphs. We selected 4 causal variables to encompass complex causal structures, including forks, chains, and colliders. Table 2 displays the Causal Disentanglement and Causal Completeness scores for all models, computed on the test data. Table 2: Comparison of identifiability results Graph Causal Disentanglement Causal Completeness Model Name Î²-VAE d-VAE ILCM SoftCD Î²-VAE d-VAE ILCM SoftCD G1 0.39 0.54 0.69 0.81 0.53 0.68 0.76 0.87 G2 0.15 0.72 0.74 0.83 0.24 0.77 0.79 0.87 G3 0.25 0.51 0.67 0.97 0.54 0.56 0.77 0.97 G4 0.19 0.50 0.65 0.69 0.41 0.69 0.76 0.79 G5 0.25 0.44 0.32 0.42 0.45 0.54 0.37 0.50 G6 0.51 0.62 0.73 0.98 0.63 0.69 0.73 0.98 G7 0.35 0.49 0.70 0.76 0.65 0.74 0.88 0.90 G8 0.48 0.54 0.50 0.60 0.61 0.63 0.62 0.69 G9 0.31 0.68 0.83 0.86 0.40 0.76 0.86 0.87 G10 0.15 0.39 0.52 0.32 0.42 0.56 0.82 0.50 The results in Table 2 indicate that our method SoftCD can identify the true causal graph in most cases. The worst results are seen for graphs G5 and G10. As mentioned in [12, 17], causal graphs are sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify the causal variables well. Furthermore, in the next experiment we are going to examine the factors affecting causal disentanglement such as the number of edges in the graph and the intensity of soft intervention effect. These findings can explain why SoftCD cannot identify causal variables in G10 despite its sparsity. Table 3: Table comparing action and object accuracy across various methods on Causal-Triplet datasets under different settings. Z and zi show whether all causal variables (Z), or only the intervened casual variable (zi) are used for the prediction task. R64 and R32 denote images with resolutions 64 Ã— 64 and 32 Ã— 32, respectively. Epic-Kitchens ProcTHOR Action Accuracy Object Accuracy Action Accuracy Object Accuracy Method Z;R64 zi;R64 zi;R32 Z;R64 zi;R64 zi;R32 Z;R64 zi;R64 zi;R32 Z;R64 zi;R64 zi;R32 Î² âˆ’ V AE [38] 0.30 0.17 0.13 0.20 0.05 0.05 0.32 0.20 0.22 0.41 0.35 0.35 d âˆ’ V AE [26] 0.19 0.60 0.46 0.20 0.14 0.1 0.27 0.60 0.63 0.38 0.77 0.78 ILCM [7] 0.17 0.25 0.27 0.18 0.06 0.07 0.21 0.36 0.35 0.34 0.54 0.59 SoftCD (ours) 0.17 0.63 0.66 0.21 0.14 0.16 0.22 0.75 0.95 0.36 0.78 0.81 5.2 Factors Affecting Causal Disentanglement In this experiment, we consider the graph G3, which has the best identifiability, and change the intensity of soft intervention and number of edges in its data generation process. To change the intensity, the post-intervention f loc 9 SoftCD network weights are initialized with samples drawn from N(1, 1) (almost similar to loc) and N(10, 1) (significantly different from loc). To change the number of edges, we consider a chain and fully-connected graph. Table 4: SoftCD performance on different configurations of G5 Edges Post-intervention Causal Causal causal mechanism Disentanglement Completeness Chain Default 0.98 0.98 Full Default 0.89 0.89 Default Significantly different 0.68 0.73 Default Almost similar 0.85 0.86 Table 5: Action and object accuracy of two explicit models are compared with SoftCD. Experiments are conducted applying image with resolution of R64 as the input to the encoder with the intervened casual variable (zi). Datasets Methods Action Accuracy Object Accuracy Epic-Kitchens ENCO [27] 0.39 0.07 DDS [28] 0.49 0.09 Fixed-order 0.34 0.09 SoftCD (ours) 0.63 0.14 ProcTHOR ENCO [27] 0.60 0.64 DDS [28] 0.38 0.44 Fixed-order 0.43 0.55 SoftCD (ours) 0.75 0.78 The results in Table 4 further confirms the sparsity of causal graphs as the causal disentanglement is much worse in the fully-connected graph than the default graph of G3. The result for significantly different post-intervention causal mechanisms indicate that the switch variable cannot approximate intense effects of soft intervention and more supervision is required to observe V. Similar post-intervention causal mechanisms also do not have sufficient variability to disentangle the causal variables as mentioned in Theory 3.9. 5.3 Action Inference In this experiment, we show the performance of SoftCD in the real-world Causal-Triplet datasets. Based on the nature of actions in this dataset, the causal variables should represent attributes of objects such as shape and color. As the dataset consists of images we train all the methods with ResNet encoder and decoder. One of the constraints of ILCM and SoftCD is to have a diffeomorphic encoder. To ensure this, we add the following inverse consistency constraint to the loss function to guarantee that decoder is the inverse of encoder as: P i Eeiâˆ¼q(ei|x),Ë†eiâˆ¼q(Ë†ei|Ë†x)[(ei âˆ’ Ë†ei)2], where Ë†x are the reconstructed samples. For the ProcThor dataset the number of causal variables are 7. For the Epic-Kitchens dataset, we randomly chose 20 actions from the dataset as 97 causal variables will be too complex in a VAE setup. The results are shown in Table 3. The results in Table 3 indicate that when including all causal variables to predict actions, SoftCD performs at par with the baseline methods. However, including all causal variables in the action or object inference may cause spurious correlations. Therefore, we have also experimented with including only the related causal variable in action and object inference. In this setting, SoftCD significantly outperforms the baseline methods which means that it can better disentangle the causal variables. We have also compared SoftCD with explicit causal representation learning methods. ENCO [27] and DDS [28] have variable topological order of causal variables during training. Furthermore, we have included a specific setting where the topological order is fixed during training. As shown in Table 5, our proposed method has superior performance to explicit models as well. 6 Conclusion We have presented SoftCD, a novel model, which improves implicit causal representation learning during soft interven- tions by introducing a causal mechanism switch variable. In evaluations on synthetic and real-world datasets, SoftCD outperforms state-of-the-art methods, demonstrating its effectiveness in practical applications. Theoretical contributions include articulating conditions for achieving causal disentanglement in implicit models through soft interventions. Our results underscore SoftCDâ€™s capacity to discern causal models from soft interventions, establishing it as a promising avenue for future research. 10 SoftCD References [1] Bernhard SchÃ¶lkopf. Causality for machine learning. CoRR, abs/1911.10500, 2019. [2] Judea Pearl. Causality, cambridge university press (2000). Artif. Intell., 169(2):174â€“179, 2005. [3] Judea Pearl, Madelyn Glymour, and Nicholas P. Jewell. Causal inference in statistics: A primer. John Wiley and Sons, 2016. [4] Kui Yu, Xianjie Guo, Lin Liu, Jiuyong Li, Hao Wang, Zhaolong Ling, and Xindong Wu. Causality-based feature selection: Methods and evaluations. ACM Comput. Surv., 53(5), 2020. ISSN 0360-0300. doi:10.1145/3409382. [5] Mengyue Yang, Furui Liu, Zhitang Chen, Xinwei Shen, Jianye Hao, and Jun Wang. Causalvae: Dis- entangled representation learning via neural structural causal models. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 9593â€“9602. Computer Vision Foundation / IEEE, 2021. doi:10.1109/CVPR46437.2021.00947. [6] Kartik Ahuja, Divyat Mahajan, Yixin Wang, and Yoshua Bengio. Interventional causal representation learning. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 372â€“407. PMLR, 2023. [7] Johann Brehmer, Pim de Haan, Phillip Lippe, and Taco S. Cohen. Weakly supervised causal representation learning. In NeurIPS, 2022. [8] Phillip Lippe, Sara Magliacane, Sindy LÃ¶we, Yuki M. Asano, Taco Cohen, and Stratis Gavves. CITRIS: causal identifiability from temporal intervened sequences. In International Conference on Machine Learning, ICML, volume 162 of Proceedings of Machine Learning Research, pages 13557â€“13603. PMLR, 2022. [9] Shuai Yang, Kui Yu, Fuyuan Cao, Lin Liu, Hao Wang, and Jiuyong Li. Learning causal representations for robust domain adaptation. IEEE Transactions on Knowledge and Data Engineering, pages 1â€“1, 2021. doi:10.1109/TKDE.2021.3119185. [10] Yue Yu, Jie Chen, Tian Gao, and Mo Yu. DAG-GNN: DAG structure learning with graph neural networks. In Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 7154â€“7163. PMLR, 2019. [11] SÃ©bastien Lachapelle, Pau RodrÃ­guez, Yash Sharma, Katie Everett, RÃ©mi Le Priol, Alexandre Lacoste, and Simon Lacoste-Julien. Disentanglement via mechanism sparsity regularization: A new principle for nonlinear ICA. In 1st Conference on Causal Learning and Reasoning, CLeaR, volume 177 of Proceedings of Machine Learning Research, pages 428â€“484. PMLR, 2022. [12] Bernhard SchÃ¶lkopf, Francesco Locatello, Stefan Bauer, Nan Rosemary Ke, Nal Kalchbrenner, Anirudh Goyal, and Yoshua Bengio. Toward causal representation learning. Proceedings of the IEEE, 109(5):612â€“634, 2021. doi:10.1109/JPROC.2021.3058954. [13] Juan D. Correa and Elias Bareinboim. General transportability of soft interventions: Completeness results. In Hugo Larochelle, Marcâ€™Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. [14] Jean Kaddour, Aengus Lynch, Qi Liu, Matt J. Kusner, and Ricardo Silva. Causal machine learning: A survey and open problems. CoRR, abs/2206.15475, 2022. doi:10.48550/arXiv.2206.15475. [15] Chaochao Lu, Yuhuai Wu, JosÃ© Miguel HernÃ¡ndez-Lobato, and Bernhard SchÃ¶lkopf. Invariant causal repre- sentation learning for out-of-distribution generalization. In The Tenth International Conference on Learning Representations, ICLR, 2022. [16] Xinwei Shen, Furui Liu, Hanze Dong, Qing Lian, Zhitang Chen, and Tong Zhang. Weakly supervised disentangled generative causal representation learning. J. Mach. Learn. Res., 23:241:1â€“241:55, 2022. [17] Ronan Perry, Julius von KÃ¼gelgen, and Bernhard SchÃ¶lkopf. Causal discovery in heterogeneous environments under the sparse mechanism shift hypothesis. In NeurIPS, 2022. [18] Xun Zheng, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing. Dags with NO TEARS: continuous optimization for structure learning. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems NeurIPS, pages 9492â€“9503, 2018. [19] Amin Jaber, Murat Kocaoglu, Karthikeyan Shanmugam, and Elias Bareinboim. Causal discovery from soft interventions with unknown targets: Characterization and learning. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems, NeurIPS, 2020. 11 SoftCD [20] Gregory F. Cooper and Changwon Yoo. Causal discovery from a mixture of experimental and observational data, 2013. [21] Jiaqi Zhang, Chandler Squires, Kristjan H. Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions. CoRR, abs/2307.06250, 2023. doi:10.48550/arXiv.2307.06250. [22] Liang Wendong, Armin KekiÂ´c, Julius von KÃ¼gelgen, Simon Buchholz, Michel Besserve, Luigi Gresele, and Bernhard SchÃ¶lkopf. Causal component analysis, 2023. [23] Jiaqi Zhang, Chandler Squires, Kristjan Greenewald, Akash Srivastava, Karthikeyan Shanmugam, and Caroline Uhler. Identifiability guarantees for causal disentanglement from soft interventions, 2023. [24] Chandler Squires, Anna Seigal, Salil Bhate, and Caroline Uhler. Linear causal disentanglement via interventions, 2023. [25] Burak Varici, Emre Acarturk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score-based causal representation learning with interventions, 2023. [26] Francesco Locatello, Ben Poole, Gunnar RÃ¤tsch, Bernhard SchÃ¶lkopf, Olivier Bachem, and Michael Tschannen. Weakly-supervised disentanglement without compromises. In Proceedings of the 37th International Conference on Machine Learning,ICML, volume 119 of Proceedings of Machine Learning Research, pages 6348â€“6359. PMLR, 2020. [27] Phillip Lippe, Taco Cohen, and Efstratios Gavves. Efficient neural causal discovery without acyclicity constraints. In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022. [28] Bertrand Charpentier, Simon Kibler, and Stephan GÃ¼nnemann. Differentiable DAG sampling. In The Tenth International Conference on Learning Representations, ICLR. OpenReview.net, 2022. [29] Simon Buchholz, Goutham Rajendran, Elan Rosenfeld, Bryon Aragam, Bernhard SchÃ¶lkopf, and Pradeep Ravikumar. Learning linear causal representations from interventions under general nonlinear mixing, 2023. [30] Julius von KÃ¼gelgen, Michel Besserve, Liang Wendong, Luigi Gresele, Armin KekiÂ´c, Elias Bareinboim, David M. Blei, and Bernhard SchÃ¶lkopf. Nonparametric identifiability of causal representations from unknown interventions, 2023. [31] Shayan Shirahmad Gale Bagi, Zahra Gharaee, Oliver Schulte, and Mark Crowley. Generative causal representation learning for out-of-distribution motion forecasting. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 31596â€“31612. PMLR, 2023. [32] Yujia Zheng, Ignavier Ng, and Kun Zhang. On the identifiability of nonlinear ICA: sparsity and beyond. In NeurIPS, 2022. [33] Alexander Immer, Christoph Schultheiss, Julia E. Vogt, Bernhard SchÃ¶lkopf, Peter BÃ¼hlmann, and Alexander Marx. On the identifiability and estimation of causal location-scale noise models. In International Conference on Machine Learning, ICML, volume 202 of Proceedings of Machine Learning Research, pages 14316â€“14332. PMLR, 2023. [34] Yuejiang Liu, Alexandre Alahi, Chris Russell, Max Horn, Dominik Zietlow, Bernhard SchÃ¶lkopf, and Francesco Locatello. Causal triplet: An open challenge for intervention-centric causal representation learning. In Conference on Causal Learning and Reasoning, CLeaR, volume 213 of Proceedings of Machine Learning Research, pages 553â€“573. PMLR, 2023. [35] Matt Deitke, Eli VanderBilt, Alvaro Herrasti, Luca Weihs, Kiana Ehsani, Jordi Salvador, Winson Han, Eric Kolve, Aniruddha Kembhavi, and Roozbeh Mottaghi. Procthor: Large-scale embodied ai using procedural generation. Advances in Neural Information Processing Systems, 35:5982â€“5994, 2022. [36] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Rescaling egocentric vision: Collection, pipeline and challenges for EPIC-KITCHENS-100. Int. J. Comput. Vis., 130(1):33â€“55, 2022. doi:10.1007/s11263- 021-01531-2. [37] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In 6th International Conference on Learning Representations, ICLR, 2018. [38] Irina Higgins, LoÃ¯c Matthey, Arka Pal, Christopher P. Burgess, Xavier Glorot, Matthew M. Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. In 5th International Conference on Learning Representations, ICLR, 2017. 12 SoftCD Appendix A1 Generative model Figure A1 shows our main generative model. It includes a data augmentation step that adds the intervention displacement Ëœx âˆ’ x as an observed feature that directly represents the effect of an intervention in observation space. ğ‘’ ğ‘¥# ğ‘¥\" âˆ’ ğ‘¥ ğ‘¥ ğ‘’Ìƒ ğ‘£ ğ‘§ ğ‘§Ìƒ Figure A1: Our generative model. x, Ëœx represent low-level observed variables. e, Ëœe represent higher-level latent exogenous variables, and z, Ëœz represent higher-level latent causal variables (e.g. objects). v represents the causal mechanism switch variable. A2 Proof of Identifiability Theorem In order to prove our model is identifiable we need a two additional definitions and some previously stated assumptions. Definition A2.1. (Diffeomorphism) A diffeomorphism between smooth manifolds M and N is a bijective map f : M â†’ N, which is smooth and has a smooth inverse. Diffeomorphisms preserve information as they are invertible transformations without discontinuous changes in their image. Definition A2.2. (Pushforward measure) Given a measurable function f : A â†’ B between two measurable spaces A and B, and a measure p defined on A, the pushforward measure fâˆ—p on B is defined for measurable sets E in B as: (fâˆ—p)(E) = p(f âˆ’1(E)) where âˆ— denotes the pushforward operation. In other words, the pushforward measure fâˆ—p assigns a measure to a set in B by measuring the pre-image of that set under f in the space A. Theorem A2.3. (Identifiability of augmented latent causal models) Let M = (A, X, g, I) and M â€² = (Aâ€², Xâ€², gâ€², Iâ€²) be two Augmented Latent Causal Models (ALCM) with identical observation space X = Xâ€² and identical intervention targets I = Iâ€². If the following conditions are satisfied, â€¢ Assumptions for identical correspondence as explained in Assumption 3.4. â€¢ Some form of supervision on how the soft interventions affect the causal mechanisms or observability of V as explained in Assumption 3.8. â€¢ The causal and exogenous variables are real-valued Zi = Zâ€² i = Ei = Eâ€² i = R. â€¢ The causal mechanism switch variables are real valued V = Rn. then the following statements are equivalent: 1. The ALCMs assign the same likelihood to interventional and observational data i.e., pX M(x, Ëœx) = pX M â€²(x, Ëœx). 2. M âˆ¼r M â€² according to Definition 3.6. Proof We will proceed to prove the equivalence between statements 1 and 2 by showing the implication is true in each direction. A2.1 M âˆ¼r Mâ€² â‡’ pX M(x, Ëœx) = pX Mâ€²(x, Ëœx) This direction is fairly straightforward. According to Definition 3.6, the fact that M âˆ¼r M â€² implies that Ï•E is measure preserving. Therefore, pE Mâ€²(eâ€², Ëœeâ€²) = (Ï•E)âˆ—pE M(e, Ëœe). Furthermore, considering that ancestry is preserved, Ï•Z is measure preserving, and that causal variables are obtained from their ancestral exogenous variables in implicit models, 13 SoftCD we have pZ Mâ€²(zâ€², Ëœzâ€²) = (Ï•Z)âˆ—pZ M(z, Ëœz). Since models are trained to maximize the log likelihood of p(x, Ëœx, Ëœx âˆ’ x) and the latent spaces in M and M â€² have the same distribution, the decoders should yield the same observational distributions pX M(x, Ëœx) = pX Mâ€²(x, Ëœx). A2.2 pX M(x, Ëœx) = pX Mâ€²(x, Ëœx) â‡’ M âˆ¼r Mâ€² Letâ€™s define Ï•E = gâ€²âˆ’1 â—¦ g : E â†’ Eâ€². Since we can express e = sâˆ’1(z), we can now define Ï•Z as Ï•Z = sâ€² â—¦ gâ€²âˆ’1 â—¦ g â—¦ sâˆ’1 : Z â†’ Zâ€². (10) Therefore, Ï•E = sâ€²âˆ’1 â—¦ Ï•Z â—¦ s. Because g and gâ€² are diffeomorphisms, Ï•E is a diffeomorphism as well. Furthermore, since pX M = pX Mâ€² and Ï•E is a diffeomorphism, then pE Mâ€² = (Ï•E)âˆ—pE M. Consequently, Ï•E is measure-preserving. Similarly, Ï•E is measure-preserving as well since causal mechanisms are diffeomorphisms. Step 1: Identical correspondence of edges and nodes Letâ€™s define the set U as U = {E Ã— E|âˆ€I, J âˆˆ I : supppE,I M (e, Ëœe|I)âˆ©supppE,I M (e, Ëœe|J)}. Then, assuming atomic interventions and counterfactual exogenous variables, pE,I M (U|I) = pE,I M (U|J) = 0. Therefore, we can say that pE M(e, Ëœe) = P IâˆˆI pE,I M (e, Ëœe|I)pI M(I) is a discrete mixture of non-overlapping distributions pE,I M (e, Ëœe|I). Similarly, we can say that pE Mâ€²(e, Ëœe) is a discrete mixture of non-overlapping distributions. It can be concluded that as Ï•E must map between these distributions, there exists a bijection that also induces a permutation Ïˆ : [n] â†’ [n]. Note: If we had non-atomic interventions or non-counterfactual exogenous variables, then these distributions would have some overlapping. With overlapping distributions, we can no longer claim there is a bijection mapping between these distributions. In space Z, the interventions should also be sufficiently variable in order to have non-overlapping pZ,I M (z, Ëœz|I) distributions. In the case of soft interventions, Ëœz is affected by all ancestral exogenous variables which could be ancestors of other causal variables as well. Consequently, if the changes in causal mechanisms are not sufficient, the effect of ancestral exogenous variables on causal variables will share some similarities and create overlapping distributions. Similar to pE M(e, Ëœe|I), we can say that there is a permutation between pZ M(z, Ëœz|I) as well. Furthermore, as we assume the target of interventions are known we have: âˆ€I âˆˆ I : pZ M(z, Ëœz|I) = pZ Mâ€²(z, Ëœz|I) Consequently, the permutation Ïˆ is an identity transformation. The effect of soft intervention with known targets on these conditional distributions is shown in Figure A2. Step 2: Component-wise Ï•Z Assuming V is observed or g is linear, we can observe how soft intervention changes f â†’ Ëœf. The causal mechanism switch variable V can be seen as the new and possibly altered set of the parents of causal variables which is why we choose p(v) to be same as p(e) which in our experiments are standard Gaussian. Furthermore, h is used to model the change in effect of the parents due to soft intervention. Each causal variable Zi has its own hi as the soft intervention effect on causal mechanisms may be different for each causal variable. By modeling soft interventions this way, we can disentangle the effect of soft intervention in higher level representations: â€¢ Observed changes due to intervention effect of causal mechanisms and parents. â€¢ Observed changes due to intervention effect on causal variable itself which is integrated in ËœEi. By using V as an auxiliary variable we can model the changes in parent set and causal mechanisms. Since we also use single encoder and decoder for e and Ëœe, the model already sees the effect of e/i on the intervened causal variable and observed variables, hence, the interventional data will be informative about Ëœei. Letâ€™s denote all Z except Zi as Zo. The connection between the causal variables in M and Mâ€² is shown in Figure 2. Assuming that ËœZi = ËœZâ€² i = R and Lemma 2 in [7], it can be concluded that the transformation ËœZi Ã— Z â†’ ËœZâ€² i is constant in Z. Using the diagram in left hand side of the equality in Figure 2(b), it can be further concluded that Ï•Zi is constant in Ëœ Zo. Therefore, Ï•Zi is a component-wise transformation. Step 3: Component-wise Ï•E Using the result from previous step that Ï•Z is a component-wise transformation, the string diagrams for connections between E and Eâ€² will be as shown in Figure A4. Ï•Ei will only depend on EA, where A = anci is the ancestors of variable i, and ei. Because s(e)anci, s(e)i, and sâ€²âˆ’1(zâ€²)i only depend on ancestors and Ï•Z is a component-wise transformation. The first equality in Figure A4 follows from the definition of Ï•Ei. The second equality holds when we first apply Ï•ZA and then apply the causal mechanisms. It can be concluded from the most right-hand side diagram in Figure A4 that the transformation from Eâ€² i Ã— EA â†’ Eâ€² i is constant in EA. Therefore, Ï•Ei is a component-wise transformation. 14 SoftCD 0.6 0.4 0.20.0 0.2 0.4 0.6 0.8 1.0 X1 0.2 0.0 0.2 0.4 0.6 0.8 1.0 1.2 X2 Observed samples Intervention on E1 Intervention on E2 Z1 10 0 10 20 30 40 Z2 10 0 10 20 30 40 P 0.00 0.01 0.02 0.03 0.04 0.05 M M' Pre-Intervention (a) (b) Z1 10 0 10 20 30 40 Z2 10 0 10 20 30 40 P 0.00 0.01 0.02 0.03 0.04 0.05 M M' Intervention on Z1 Z1 10 0 10 20 30 40 Z2 10 0 10 20 30 40 P 0.00 0.01 0.02 0.03 0.04 0.05 M M' Intervention on Z2 (c) (d) Figure A2: The distribution of observed and causal variables in two causal models M and Mâ€², which belong to the equivalence class up to reparameterization. (a) There are 10 observed samples in which Z1 or Z2 has been intervened on. (b) The distribution of causal variables when I = 0 (no intervention) is identical to each other but the range of value of causal variables are different and can be mapped to each other using Ï•Z. (c) The intervention on Z1 (I = 1). (d) The intervention on Z2 (I = 2). For I = 1 and I = 2 the distributions are again identical to each other but are different for different targets of intervention as soft interventions change the conditional distribution (condition on parents) of causal variables. Also, for each value of I, the distributions of M and Mâ€² should move in one direction as targets are known. A3 Soft vs. Hard intervention In a causal model, an intervention refers to a deliberate action taken to manipulate or change one or more variables in order to observe its impact on other variables within the causal model. Interventions help to study how changes in one variable directly cause changes in another, thereby revealing causal relationships. Based on the levels of control and manipulation in a causal intervention, we can have soft vs. hard interventions. A hard intervention involves directly manipulating the variables of interest in a controlled manner such as Randomized Controlled Trials (RCTs). In other words, a hard intervention sets the value of a causal variable Z to a certain value denoted as do(Z = z) [3]. On the other hand, soft intervention involves more subtle or less controlled manipulation of variables and changes the conditional distribution of the causal variable p(Z|Zpa) â†’ Ëœp(Z|Zpa) which can be modeled as Ëœzi = Ëœfi(zpai, Ëœei) [13]. Looking at interventions from a graphical standpoint, a hard intervention entails that the intervened node is solely impacted by the intervention itself, with no influence coming from its ancestral nodes. Conversely, in the context of a soft intervention, the representation of the intervened node can be influenced not only by the intervention but also by its parent nodes. 15 SoftCD ğ‘§Ìƒ! \" ğ‘§Ìƒ! ğ‘§ ğ‘§Ìƒ! \" ğ‘§ ğ‘° ğ‘° ğœ‘!! ğ‘§Ìƒ#\" = ğ‘§Ìƒ#\" ğœ‘!\" ğ‘“\" ğ‘“# ğ‘“#$ ğœ‘! ğ‘“\" $ ğ‘½ ğ‘½\" (a) ğ‘§Ìƒ! \" ğ‘§Ìƒ! ğ‘§ ğ‘§Ìƒ! \" ğ‘° ğ‘° ğœ‘!! = ğ‘“\" ğ‘“# ğ‘“\" $ (b) Figure A3: (a) String diagram of the causal variables Z and Zâ€². The triangle indicates sampling I from its distribution. The left-hand side diagram is when Ï•Z is applied last and the right-hand side diagram is when Ï•Z is applied first. I is the intervention which affects intervened causal variableâ€™s mechanism variable. V is used to model the effect of intervention on mechanisms and parents. (b) String diagrams after discarding Ëœ Zâ€²o and the disentangled effect of soft intervention on ËœZi modeled by V . ğ‘“! \"#$ ğ‘“! ğ‘ \" ğ‘“! \"#$ ğ‘“! # ğ‘ \" = = ğœ‘$! ğœ€! ğœ€\" ğœ€\" # ğœ€! ğœ€\" ğ‘§! ğ‘§\" ğœ‘%\" ğœ‘%! ğ‘§\" # ğ‘§! # ğœ€\" # ğœ€\" # ğ‘§\" # ğ‘§! # ğœ€\" # ğœ‘%\"  ğ‘§! ğœ€! Figure A4: String diagrams for connections between E and Eâ€². The triangle indicates sampling variables from their corresponding distributions. As an example, suppose we are trying to understand the causal relationship between different types of diets and weight loss. The soft intervention in this scenario could be a switch from a regular diet to a low-carb diet. Switching to a low-carb diet is a voluntary choice made by the individual and there are no external forces or regulations compelling them to make this change (non-coercive). The intervention involves a modification of the individualâ€™s diet rather than a complete disruption since they are adjusting the proportion of macronutrients (fats, proteins, and carbs) they consume, which is less disruptive than a radical change in eating habits (gradual modification). The individual has autonomy to choose and tailor their diet according to their preferences and health goals so they are empowered to make informed decisions about their dietary choices (behavioural empowerment). Conversely, if the government or an authority were to intervene and enforce a mandatory low-carb diet through legal means, this would constitute a hard intervention. In this scenario, regulations would be implemented, prohibiting the consumption of specific carbohydrate-containing foods. Regulatory agencies would be established to oversee and ensure adherence to the low-carb diet mandate, taking actions such as removing prohibited foods from the market, restricting their import and production, and so on. Individuals caught consuming banned foods would be subject to fines, legal repercussions, or other penalties. A4 Experiments This section contains additional details about SoftCD design architectures, datasets, and experiments settings. 16 SoftCD (a) Pre-Epic-Kitchens (b) Pre-Epic-Kitchens (c) Pre-Epic-Kitchens (d) Pre-Epic-Kitchens (a) Post: Valve-locked (b) Post: Bread-Inserted (c) Post: Clothes-Gathered (d) Post: Juice-Poured (e) Pre-ProcTHOR (f) Pre-ProcTHOR (g) Pre-ProcTHOR (h) Pre-ProcTHOR (e) Post: Cabinet-Open (f) Post: Box-Open (g) Post: TV-Broken (h) Post: TV-On Figure A5: In the Causal-Triplet dataset [34], visual representations capture both pre and post-intervention scenarios. The first two rows showcase data samples from Epic-Kitchens, while the third and fourth rows feature samples from ProcTHOR. Each image in the post-intervention condition is accompanied by labels specifying the corresponding action and intervened object. In the images in the first two rows, the agent is performing an action on an object but the camera angle has also changed. So we can say that for example the distribution of causal variables conditioned on the camera angle has been changed due to soft intervention. A4.1 Datasets The Causal-Triplet datasets are consisted of images containing objects in which an action is manipulating the objects shown in Figure A5. Examples of actions and objects in these datasets are given in Table A2. Table A1: Actions and objects present in the Causal-Triplet images (ProcTHOR Dataset). ProcTHOR Dataset Object Television Bed Bed Television Laptop Book Box Action Break Clean Dirty Turn off Turn on Open Close Based on the actions and objects, we treat our causal variables as attributes of objects which can be changed by actions. Therefore, actions in these datasets are considered as interventions. Assume that z1 corresponds to the attributes of an object, e.g. a door, the target of opening or closing (actionâ€™s target) is z1. We use actionsâ€™ labels in these datasets to detect the targets of interventions to determine which causal variable has been intervened upon. Note that informing the model about the target of intervention is not same as informing about the action itself (See Table 3). 17 SoftCD Table A2: Actions and objects present in the Causal-Triplet images (Epic-Kitchens Dataset). Epic-Kitchens Dataset Object Tofu Rice Hob Bag Cupboard Garlic Tap Wrap Rice Cheese Action Insert Pour Wash Fold Open Pat Move Check Transition Stretch Object Wrap Skin Button Lid Plate Egg Sponge Oil Water Dough Action Flip Gather Press Lock Wrap Drop Water Carry Smell Mark A4.2 Architecture Design Based on the SoftCD architecture depicted in Figure 1, we devised a solution function comprising three fully connected networks. These networks consist of two layers each, with 64 hidden units per layer and ReLU activation functions. For our synthetic dataset experiments, we employed a fully connected auto-encoder model, while for the realistic dataset, we utilized ResNet-based networks. A5 Results In our synthetic dataset experiments, SoftCD achieved the highest average performance with a 4-variable causal model. We employed various data generation seeds, resulting in diverse adjacency matrices, reflecting the influence of soft interventions on causal mechanisms and parental impact. A5.1 Scalability While our primary research objective centered on addressing identifiability challenges in implicit causal models under soft interventions, we also conducted an investigation into the scalability of our proposed model. To comprehensively assess its performance, we designed experiments covering a range of causal graphs, featuring 5 to 10 variables, with 10 different seeds for each variable, following a similar experimental setup as our 4-variable causal graph experiments. The outcomes of these experiments, comparing SoftCD and ILCM, are presented in Figure A6. By increasing the number of variables in the graph, confounding factors and ambiguities of causal relations increase as well. Consequently, more supervision on V is required to better separate the effect of causal variables themselves on the observed variables. D4 D5 D6 D7 D8 D9 D10 Causal Variables 0.4 0.5 0.6 0.7 0.8 0.9 Causal Disentanglement Score Means with Standard Deviations Mean-SoftCD Std-SoftCD Mean-ILCM Std-ILCM Figure A6: Causal disentanglement for different number of variables 18 "
}