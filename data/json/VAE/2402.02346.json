{
    "optim": "Closed-Loop Unsupervised Representation Disentanglement with\nÎ²-VAE Distillation and Diffusion Probabilistic Feedback\nXin Jin1, *, Bohan Li1, 2, BAAO Xie1,\nWenyao Zhang1, 2, Jinming Liu1, 2, Ziqiang Li1, 2, Tao Yang3, Wenjun Zeng1,\n1Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China\n2Shanghai Jiao Tong University, Shanghai, China, 3Xiâ€™an Jiaotong University, Xiâ€™an, China\nCorresponding: jinxin@eitech.edu.cn\nAbstract\nRepresentation disentanglement may help AI fundamen-\ntally understand the real world and thus benefit both dis-\ncrimination and generation tasks. It currently has at least\nthree unresolved core issues: (i) heavy reliance on label\nannotation and synthetic data â€” causing poor generaliza-\ntion on natural scenarios; (ii) heuristic/hand-craft disen-\ntangling constraints make it hard to adaptively achieve an\noptimal training trade-off; (iii) lacking reasonable evalua-\ntion metric, especially for the real label-free data. To ad-\ndress these challenges, we propose a Closed-Loop unsu-\npervised representation Disentanglement approach dubbed\nCL-Dis. Specifically, we use diffusion-based autoencoder\n(Diff-AE) as a backbone while resorting to Î²-VAE as a co-\npilot to extract semantically disentangled representations.\nThe strong generation ability of diffusion model and the\ngood disentanglement ability of VAE model are complemen-\ntary. To strengthen disentangling, VAE-latent distillation\nand diffusion-wise feedback are interconnected in a closed-\nloop system for a further mutual promotion. Then, a self-\nsupervised Navigation strategy is introduced to identify in-\nterpretable semantic directions in the disentangled latent\nspace. Finally, a new metric based on content tracking is\ndesigned to evaluate the disentanglement effect. Experi-\nments demonstrate the superiority of CL-Dis on applica-\ntions like real image manipulation and visual analysis.\n1. Introduction\nDisentangled representation learning (DRL) [31] learns the\nunderlying explainable factors behind the observed data,\nwhere the different latent factors correspond to different\nproperties, respectively. This matches the way humans per-\nceive this world that we understand an object from its var-\nious properties (e.g., shape, size, color, etc.) [2, 20, 21, 60,\n91]. Thus, DRL is thought to be one of the possible ways\nfor AI to understand the world fundamentally, thus achiev-\n(a)   VAE-based DRL \n(b)   GAN-based DRL\n(c)    Diffusion-based DRL\n(d)   our Closed-loop DRL\nğ±\nğ³\nğ„ğ§ğœ\nğ’’ğ“(ğ’›|ğ’™)\nğ’‘ğœ½(ğ’™|ğ’›)\nğƒğğœ\nğ±â€²\nmin ğ‘«(ğ’’ğ“(ğ’›)||ğ’‘(ğ’›))\nInfer disentangled latent\nğ§\nğ³\nğ†ğğ§\nğ±â€²\nğƒğ¢ğ¬\nğ±\nNoise\nğ„ğ§ğœ\nğ³â€²\nReal/\nFake\nmin ğ‘°(ğ’›, ğ’›â€²)\nInfer disentangled latent\nğ±ğŸ\nğ±ğ­\nAdd \nnoise\nğ”ğ§ğğ­\nğ„ğ§ğœ\nğƒğğœ\nğ“\nğ„ğ§ğœ\nà·œğ±ğŸ\nDiffusion process\nà·œğ³\nğ³â€²\nInfer disentangled latent by \nhand-craft variant-invariant loss\nğ³\nğ±ğŸ\nâ€²\nğ±ğŸ\nğ±ğ­\nAdd \nnoise\nğ”ğ§ğğ­\nğ„ğ§ğœ\nğƒğğœ\nğ“\nà·œğ±ğŸ\nğ³\nğ±ğŸ\nâ€²\nmin ğ‘«(ğ’’ğ“(ğ’›)||ğ’‘(ğ’›)) automatically\nInfer disentangled latent\nDistillation\nFeedback\nDiffusion \nprocess\nGaussian\nFigure 1. The illustration of disentangled representation learn-\ning (DRL) frameworks:\n(a) VAE-based, (b) GAN-based, (c)\nDiffusion-based, and (d) our proposed Closed-loop approach. We\ncan intuitively see that previous works all relied on heuristic hand-\ncraft loss constraints to infer disentangled latent, only our method\nleverages two kinds of generative branches to build a cycle system\nfor mutually-promoting automatically disentangled learning.\ning Artificial General Intelligence (AGI) [3, 49, 82].\nMost existing DRL methods learn the disentangled rep-\nresentation based on generative models, such as VAE [4, 7,\n18, 30, 42, 43, 45, 47, 69], GAN [8, 25, 36, 54, 77, 98], and\neven recently popular diffusion [48, 85, 86, 89, 94]. Among\nthem, the VAE-based DRL methods always encourage the\ndisentanglement of the latent variables by forcing the vari-\national posterior to be closer to the factorized Gaussian\nprior (see Figure 1(a)), but they generally need to balance\na hard inherent trade-off between the disentangling ability\nand generating quality [7, 30, 42]. As shown in Figure 1(b),\nthe GAN-based DRL methods tend to use the mutual in-\nformation as disentanglement constraints, but they typically\nsuffer from the problem of reconstruction quality due to the\ngan-inversion difficulty [81, 94]. Besides, VAE- and GAN-\narXiv:2402.02346v1  [cs.CV]  4 Feb 2024\nbased generative models are prone to learn the factorized or\nconditioned statistics bias [32], which essentially conflicts\nwith the ultimate purpose of a generalizable disentangle-\nment that makes AI understand the world like humans.\nMoreover, given the recently popular diffusion proba-\nbilistic models (DPMs) show superiority in image gener-\nation quality [33, 73] and unsupervised controlling (i.e.,\nlearning meaningful representations) of images [64], re-\nsearchers have started to investigate the possibility of DRL\nin DPMs. Asyrp [48] proposes an asymmetric reverse pro-\ncess to discover the semantic latent space in frozen pre-\ntrained diffusion models, which reveals that DPMs already\nhave a semantic latent space and thus more suitable for\nlearning disentangled semantics. The co-current work of\nDisDiff [94] shown in Figure 1(c) uses the gradient field of\nfrozen DPMs to achieve a vector-wise disentangled repre-\nsentation learning. However, this work applies an external\nheuristic hand-craft variant-invariant loss [57, 94] to enforce\nthe disentanglement processes, resulting in reduced model\nflexibility and posing challenges for optimization.\nIn this paper, we propose to leverage two kinds of gen-\nerative branches of the diffusion model and VAE model,\nto build a cycle system for mutually-promoting DRL.\nAs shown in Figure 1(d)), we construct an unsupervised\nClosed-Loop Disentanglement framework dubbed CL-Dis,\nwhich is built upon a diffusion-based autoencoder (Diff-\nAE) backbone while combining with a co-pilot of Î²-VAE,\nto together learn the meaningful disentangled representa-\ntions. To collaborate two branches well and facilitate fea-\nture disentangling, we encourage Diff-AE and Î²-VAE to\ncomplement each other, achieving a win-win effect: we\nuse the pre-trained latent of VAE to guide a semantic-aware\nreverse diffusion process with a distillation loss. Then, the\ngradually increasing information capability during the dif-\nfusion process is taken as feedback, in turn, to progressively\nenforce the inner latent of VAE more disentangling. Next, a\nself-supervised Navigation strategy is introduced to clearly\nidentify each factorâ€™s semantic meanings, where shifting\nalong a disentangled semantic direction will result in con-\ntinuously changed generations. Finally, we further design\na label-free metric based on â€œchanges trackingâ€ to quan-\ntitatively measure the disentanglement of learned features,\nwherein we use optical flow to reflect image variation de-\ngree objectively. Our main contributions are summarized:\nâ€¢ We build a new unsupervised representation disentan-\nglement framework CL-Dis, which has a mutually-\npromoting closed-loop architecture driven by diffusion\nand VAE models. They complement each other with a\ndistillation loss and a new feedback loss, and thus em-\npower a more controllable and stronger DRL capability.\nâ€¢ We introduce a self-supervised method in CL-Dis to\nclearly identify each factorâ€™s semantic meanings by navi-\ngating directions in the learned disentangled latent space,\nleading to coherent generated variations. This makes the\ngeneration more explainable and fine-controllable.\nâ€¢ We design a new label-free metric to quantitatively mea-\nsure the interpretability and disentanglement of learned\nfeatures, and provide an example of immediate practical\nbenefit from our work. Namely, we experimentally show\nhow to exploit our CL-Dis for generative manipulation of\nreal images and visual discrimination tasks.\n2. Related Works\n2.1. Disentangled Representation Learning (DRL)\nThe concept of DRL was introduced by Bengio et al. [3] in\n2013 and was believed helpful in different tasks in practical\napplications like image generation [7, 8, 42, 43, 45, 47, 54,\n87], image editing/translation [24, 50, 56, 83], NLP [10,\n28, 84] and multimodal applications [34, 59, 78, 90, 97].\nDifferent from image generation or editing, the core of this\ntask is to fully understand the latent factors of the model to\nenhance its fine-grained controllability [7, 42, 43, 47, 87].\nMost VAE-based DRL approaches like Î²-VAE [30],\nDIP-VAE [47], FactorVAE [42], etc., achieve unsupervised\ndisentanglement by the direct constraints on probabilistic\ndistributions [7, 30, 42]. While straightforward, Locatello et\nal. [57] point out that is not enough and emphasize the need\nfor extra inductive bias. Similarly, Burgess [4] proposes to\nprogressively increase the information capacity of the latent\ncode in Î²-VAE. Yang et al. [92] use symmetry properties\nmodeled by group theory as inductive bias. Except for VAE,\nthere are also some DRL works based on GAN, e.g., lever-\naging mutual information [8], self-supervised contrastive\nregularizer [54], spatial constriction [98], and combining\nwith other pre-trained generative models [52, 66].\nFast-developing generative diffusion probabilistic mod-\nels (DPMs) have accelerated the DRL exploration. LCG-\nDM [44] uses a vanilla VAE in DPMs to extract semantics\ncodes for controllable generation, but ignores the interac-\ntion between these two structures.\nAsyrp [48] discovers\nthe semantic latent space in frozen pre-trained DPMs for\nimage manipulation. Co-current DisDiff [94] proposes a\nvector-wise method to express more information compared\nwith other scalar-based methods [4, 48, 54]. However, they\nuses gradient fields as an inductive bias at all time steps to\nachieve DRL, which relies on an external heuristic hand-\ncraft disentangling loss, lacking a self-driven learning abil-\nity and is hard to optimize. In contrast, our CL-Dis builds\na closed-loop DRL framework with a mutual-promoting\nmechanism driven by a two-branch complementary diffu-\nsion and Î²-VAE architecture, encouraging stronger disen-\ntangled feature auto-learning.\n2.2. Knowledge Distillation and Feature Feedback\nOur closed-loop CL-Dis is bridged by these two techniques.\nKnowledge distillation (KD) is originally developed for\nmodel compression and acceleration [11, 26, 80], or used\nin transfer learning [1, 38].\nFeature feedback [14, 15,\n41, 58, 63] enhances the robustness of learning systems\nby using explainable information, which is gaining more\nand more attention in AI alignment [37], RLHF [13, 46],\nand embodied AI [17, 67]. For instance, ControlVAE [69]\nachieves much better reconstruction quality with a variant\nof the proportional-integral-derivative (PID) control, using\nthe output KL-divergence as feedback. Following this idea,\nwe explore taking the strong generation capability of diffu-\nsion models as information-increasing feedback to automat-\nically and adaptively enhance disentangled feature learning.\n2.3. Latent Semantics Discovery (LSD)\nThis task emerges to move around the latent code (of\nGAN [12, 75, 79], Diffusion [48], etc), such that only one\nfactor varies during the traversal generation. They mostly\nrely on human annotations (i.e., segmentation masks, at-\ntribute categories, 3D priors, and text descriptions) to define\nthe semantic labels [9, 22, 53, 62, 71, 72]. Unsupervised\nLSD [70, 74, 79] often learn a set of directions or a classi-\nfier/matrix (e.g., Hessian, Jacobian regularization, etc.) to\nidentify latent semantics. Wu et al. [85] find semantics in\nthe stable diffusion model by partially changing the text em-\nbeddings. However, these methods are not strictly equiva-\nlent to DRL, most of them just rely on already disentangled\n(text) features to get coherent traversal generation results.\n3. Background and Motivation\n3.1. Understanding disentangling in Î²-VAE\nDue to the poor disentanglement of vanilla VAE on complex\ndata, Î²-VAE [30] adds explicit inductive bias to strengthen\nthe independence constraint of the variational posterior dis-\ntribution qÏ•(z|x) with a Î² penalty coefficient in ELBO:\nL(Î¸, Ï•) =EqÏ•(z|x)\n\u0002\nlog pÎ¸(x|z)\n\u0003\nâˆ’ Î²DKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\n,\n(1)\nwhen Î²=1, Î²-VAE degenerates to the original VAE, and\nlarger Î² encourages more disentangled representations but\nharms the performance of reconstruction. Unfortunately, it\nis practically intractable to obtain the optimal Î² to balance\nthe trade-off between reconstruction quality and the disen-\ntangling capability [7, 47]. Thus, Burgess et al. [4] under-\nstand Î²-VAE from the perspective of information bottleneck\ntheory, and propose improving disentangling in Î²-VAE with\ncontrolled information capacity increase:\nL(Î¸, Ï•) =EqÏ•(z|x) log pÎ¸(x|z) âˆ’ Î²\n\f\fDKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\nâˆ’ C\n\f\f,\n(2)\nwhere C gradually increases from 0 to a value large enough\nduring the training to guarantee the expressiveness of la-\ntent representations, thus improving disentangling. How-\never, such a hand-craft increasing strategy is hard to con-\ntrol and might hurt disentangling performance, especially\nfor various scenarios on different datasets. Thus, in our\nwork, we learn from this idea but upgrade it by exploiting\nthe essence of the Diffusion Probabilistic Model (DPM) in\nautomatically increasing information capacity [51, 61] for\ndisentangling enhancement.\n3.2. DPM and Diffusion Autoencoder (Diff-AE)\nHere, we first take DDPM [33] as a vanilla DPM example\nfor illustration, which adopts a sequence of fixed variance\ndistributions q(xt|xtâˆ’1) as the noise-adding forward pro-\ncess to collapse the image distribution p(x0) to N(0, I):\nq(xt|xtâˆ’1) = N(xt;\np\n1 âˆ’ Î²txtâˆ’1, Î²tI).\n(3)\nThe reverse process is fitting by other distributions of Î¸, and\nit is a progressively increasing process of information ca-\npacity (i.e., entropy decreasing) as we mentioned before:\npÎ¸(xtâˆ’1|xt) = N(xt; ÂµÎ¸(xt, t), ÏƒtI),\n(4)\nwhere ÂµÎ¸(xt, t) is parameterize by a Unet ÏµÎ¸(xt, t). Its opti-\nmization is to minimize the variational upper bound of neg-\native log-likelihood LÎ¸ = Ex0,t,Ïµ âˆ¥Ïµ âˆ’ ÏµÎ¸(xt, t)âˆ¥.\nFurthermore, Diff-AE [64] is proposed for representa-\ntion learning based on DPM and Autoencoder. It uses a\nlearnable semantic encoder for discovering the high-level\nmeaningful semantics zsem = Esem(x0), and a DPM-\nbased decoder p(xtâˆ’1|xt, zsem) that is conditioned on\nzsem for reconstruction. However, the latent representa-\ntion zsem learned by Diff-AE does not explicitly respond to\nthe underlying factors of the data, or say, its disentangling\npotential has not been fully unleashed.\n4. Methodology: Closed-Loop DRL (CL-Dis)\nBased on the analysis above, we propose to leverage VAE\nand diffusion to unleash their strengths for a mutually pro-\nmoting DRL. As shown in Figure 2, our CL-Dis is a dual-\nbranch cycle framework, involving multiple iterative phases\nwith (a) diffusion autoencoder (Diff-AE), (b) Î²-VAE, and\nthe core (c) knowledge distillation & feature feedback pro-\ncess. In Section 4.1, we first present the formulation of\nclosed-loop DRL and the overview of CL-Dis. After that,\nwe introduce different phases of CL-Dis, including Diff-AE\npre-training, Î²-VAE pre-training, and knowledge distilla-\ntion & feature feedback in Section 4.2. Then, we propose a\nself-supervised Navigation strategy to identify disentangled\nsemantic meanings and a new well-designed DRL metric\n(Section 4.3) for the final disentanglement measurement.\n4.1. Problem Formulation and Overview of CL-Dis\nGiven a practical dataset D derived by N underlying fac-\ntors C = {1, 2, . . . , N}, the target of disentanglement for\nForward \nDiffusion\nğ’™ğ‘‡\nğ’™ğ‘‡âˆ’1\nReal Data ğ‘ğ‘‘ğ‘ğ‘¡ğ‘(ğ±)\nProbabilistic \nEncoder\nğ‘ğœ™(ğ³|ğ±)\nğœ‡\nğœ\nReparameterization\nğ³ğğ¢ğ¬ğğ§\nProbabilistic \nDecoder\nğ‘ğœƒ(ğ±|ğ³)\nReconstructed Data ğ‘ğ‘”(ğ±)\nDisentangled \nLatent\n(c) Such increasing information capacity process can be \ntaken as an indicator to enhance disentangling\nSemantic \nEncoder\nğ¸ğ‘ ğ‘’ğ‘š\nğ³ğ¬ğğ¦\nğ’™0\nConditioning\nâ€¦\nSmile,\nAzimuth,\nâ€¦,\nHair Color\n(a) Diffusion Autoencoders\n(b) ğœ· âˆ’ ğ‘½ğ‘¨ğ‘¬\nâ€¦\nğ’™ğ‘‡âˆ’2\nğ’™0\nâ€²\nğ’™ğ‘‡âˆ’3\nğ’™1\nDenoising \nU-Net\nDenoising \nU-Net\nDenoising \nU-Net\nDenoising \nU-Net\nDenoising \nU-Net\nğ›½\nğ‘(ğ³)\nğ’™ğ‘‡âˆ’1\nReverse Denoising \nFigure 2. Pipeline of the proposed CL-Dis. We explore using (a) diffusion-based autoencoder (Diff-AE) as a backbone while resorting\nto (b) Î²-VAE as a co-pilot via knowledge distillation to help Esem extract a semantic disentangled representation zsem of an input image\nx0 (which is taken as condition for diffusion-based generation). To further facilitate feature disentangling, we further take (c) the reverse\ndiffusion process as an increasing information capacity indicator feedback to, in turn, enhance the disentanglement of feature zdisen in\nÎ²-VAE. As a result, Diff-AE and Î²-VAE are interconnected as a closed-loop cycle system with VAE-latent distillation and diffusion-wise\nfeedback controlling, achieving a win-win effect and well-disentangled fine-controllable representation learning.\nour CL-Dis is to learn a semantic encoder Esem, for each\nfactor c âˆˆ C, when varying its latent value, its reconstructed\ngeneration results will have the corresponding changes.\nIn Figure 2, our CL-Dis takes Diff-AE as the back-\nbone, which has exactly a semantic encoder Esem to en-\ncode the raw data x0 into the semantic representations\nzsem for conditioning the diffusion reverse distributions\np(xtâˆ’1|xt, zsem). To empower Esem disentangling capa-\nbility, we additionally construct a Î²-VAE as co-pilot to\nhelp zsem become a semantic disentangled representation.\nGiven a dataset X from a distribution pdata(x), a stan-\ndard Î²-VAE framework [30] is built and Ï•, Î¸ parametrize\nthe VAE encoder qÏ•(z|x) and the decoder pÎ¸(x|z), respec-\ntively. The prior p(z) is typically set to the isotropic unit\nGaussian N(0, 1), using a differentiable â€œreparametrization\ntrick (z = Âµ+Ïƒ)â€ [45]. With the objective of Eq. 1, we get a\npreliminary disentangled representations zdisen. Then, we\nuse a distillation loss Ldt to transfer such disentangling ca-\npability to the semantic latent zsem of Diff-AE,\nLdt = DKL\n\u0000zsem âˆ¥ zdisen\n\u0001\n=\nX\nzsem Â· log( zsem\nzdisen ), (5)\nAs discussed before, gradually increasing the information\ncapacity of the VAE latent with a controlled way could im-\nprove disentangling [4]. The reverse diffusion process of\nDiff-AE (Eq. 4) is exactly a procedure of information capa-\nbility increasing. Thus, we take it as an indicator (see Fig-\nure 2(c)) in turn to enhance Î²-VAE disentangling by adap-\ntively adjusting the objective of Eq. 2, where we change the\nhand-craft C to a variable that can automatically change in\nthe optimization (Eq. 8). Such a mutually promoting mech-\nanism forms a closed-loop optimization flow.\n4.2. Training of the Whole CL-Dis Framework\n4.2.1\nPhase-1: Diff-AE and Î²-VAE Pre-training\nFollowing [64], the entire Diff-AE model is pre-trained on\nthe various datasets [39, 40, 96] to cover more semantic fac-\ntors in the latent space of zsem. But at this phase, zsem is\nstill far from well-disentangled, which is just taken as a kind\nof coarse condition for guiding diffusion generation [94].\nMoreover, the co-pilot Î²-VAE is also pre-trained on the\ncorresponding datasets (i.e., if Diff-AE was trained on the\nfacial data, Î²-VAE did the same), following the objective\nof Eq. 1, its latent zdisen has already enabled a preliminary\ndisentangling capability, i.e., can distinguish facial features\nof smile, azimuth, hair color, etc (Figure 2) to some degree.\n4.2.2\nPhase-2: Knowledge Distillation & Diffusion Feedback\nTo transfer the preliminary disentangling capability of\nzdisen into zsem, we use a distillation loss shown in Eq. 5\nto smoothly bridge Diff-AE and Î²-VAE. But this interaction\nis currently one-way, which is still limited by the intrinsic\nshortcomings of Î²-VAE discussed after Eq. 1 that it is hard\nto balance a trade-off and thus hurt the final performance.\nTherefore, we propose a novel diffusion feedback-driven\nloss Lfd to further facilitate the disentanglement of the rep-\nresentations (zdisen, zsem) and break the optimization bot-\ntleneck of the entire CL-Dis framework. Basically, inspired\nby the work of understanding Î²-VAE [4] that modified the\nvanilla objective by adding a hand-craft increasing con-\ntroller C (as shown in Eq. 2), we propose to leverage the\nreverse diffusion of Diff-AE that adaptively increases infor-\nmation capacity to build constraints to replace the original\nformat. Formally, we make the original C auto-changeable,\nand control its value dynamically according to the informa-\ntion capacity of the reverse diffusion process as follows:\nCdyn = f(Ext) =\n(\nCbase Â· (\nEx0\nExt ),\nif 0 < f(Ext) < Cmax\nCmax,\nif f(Ext) â‰¥ Cmax\n(6)\nEx0 = âˆ’\nX\npx0 âˆˆR\npx0logpx0,\nExt = âˆ’\nX\npxt âˆˆR\npxtlogpxt,\n(7)\nwhere Ex0 and Ext denote the information entropy of the\nstatic target image (i.e., Ground Truth) x0 and the inter-\nmediate predicted diffusion denoising image result xt, re-\nspectively.\nThe probability values of px0, pxt are calcu-\nlated by flattening and probabilizing the values of the im-\nage matrices across the dimension of RCÃ—HÃ—W . Besides,\nCbase, Cmax are the original empirical start-point and the\nmaximum upper-bound according to [4], and their influ-\nences are studied in supplementary. We can intuitively see\nthat with the denoising diffusion processing, the entropy\nExt of the predicted image result xt will gradually drop\n(i.e., information capacity increasing), and thus the ratio of\nEx0\nExt will increase to raise the influence of Cdyn. So, the ob-\njective of Î²-VAE changes from Eq. 2 to the following Lfd,\nLfd(Î¸, Ï•, Cdyn; x, z) =EqÏ•(z|x) log pÎ¸(x|z)âˆ’\nÎ²\n\f\fDKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\nâˆ’ Cdyn\n\f\f,\n(8)\nthe above physical meaning is that: when KL-divergence\ndrops to a certain extent (i.e., disentangling ability encoun-\nters bottleneck in a naive VAE latent), the dynamic con-\ntroller Cdyn will counteract this change by increasing the\nvalue of the second item of Eq. 8. Intuitively, Cdyn that\nis controlled by diffusion feedback gradually highlights the\npenalty for KL-divergence, thus improving the latent disen-\ntangling. Under this closed-loop optimization flow (see Fig-\nure 2(c)), the gradually increasing information capacity of\ndiffusion adaptively enhances the disentanglement of latent\nzdisen, so as to further benefit the semantic code of zsem.\nPlease see supplementary for more theoretical analysis.\n4.2.3\nPhase-3: Semantics Directions Navigation\nAfter getting the well-disentangled representations of zsem,\nthe next step is to identify its semantic meanings. Shifting a\nDiff-AE\nGenerator\nShifter\nğ‘€(ğœ…, ğ›¿)\nWell-disentangled ğ³ğ¬ğğ¦\nğ³ğ¬ğğ¦\nâ€²\n= ğ³ğ¬ğğ¦+ ğ‘€(ğœ…, ğ›¿)\nShift\nPredictor\nğ‘ƒ\nğœ…, ğ›¿\nğœ…â€², ğ›¿â€²\nShifting a dimension of ğ’›ğ’”ğ’†ğ’ causes \nonly one factor (i.e., eyes) changes.\nFigure 3.\nA self-supervised Navigation strategy is to discover\nthe interpretable shifts in the well-disentangled space of zsem. A\ntraining sample consists of two images, produced by the Diff-AE\nwith original and shifted inputs. The images are given to a predic-\ntor P that predicts the direction index k and shift magnitude Î´.\ncertain dimension of the well-disentangled condition zsem\nwill result in a generated result that only the corresponding\nproperty/factor changes. Thus, we adopt an outer naviga-\ntion branch to identify the traversal directions in a global\nview. Specifically, we employ a learnable matrix M (see\nFigure 3) to manually append a shift on zsem. The shifted\ncode zâ€²\nsem, together with the original zsem, is fed into our\nDiff-AE decoder to generate a pair of images, following the\nreverse conditional diffusion process of p(xtâˆ’1|xt, zsem).\nA learnable predictor P is then devised to predict the shift\n(i.e., semantic direction) according to such generated paired\nimages, with a reconstruction loss. In this way, we clearly\nidentify the inner disentangled semantics directions in our\nCL-Dis for further fine-controllable generations.\n4.3. New Disentanglement Metric\nThe conditional diffusion-based generation in our CL-Dis\ndefines a clear mapping between the disentangled represen-\ntations and the generated image results, so we can quantita-\ntively measure the performance of disentanglement with the\ngeneration difference. Following this idea, we quantify this\ndifference by using an optical flow tracking technique [76]:\nGiven a well-disentangled representation za\nsem and its shift\nvariant zaâ€²\nsem, we calculate the optical flow map F between\ntheir corresponding generated images. Then, we use a con-\ntinuously changed hyper-parameter as a dynamic threshold\nto distinguish these â€œchanged characteristicsâ€ caused by\nshifted factors from the rest â€œfrozen onesâ€. The average ra-\ntio of these two parts actually reflects the disentanglement\nperformance, the smaller the better. Due to the space limi-\ntations, more operation details are in supplementary.\n5. Experiment\n5.1. Implementation Detail and Experiment Setting\nFor the backbone of Diff-AE, we take pre-trained weights\nin [64] as the initial parameters with Adam optimizer and\nthe learning rate of 1e-4.\nFor the co-pilot Î²-VAE, we\nadopt the same Adam optimizer and learning rate.\nFor\nthe navigation branch, we refer to the GAN-based editing\nmethods [12, 79] for a fine-controllable generation. Dif-\nferently, our CL-Dis has already in advance got the well-\ndisentangled latent zsem. The whole CL-Dis framework is\ntrained on 2 NVIDIA V100 GPUs with a batch size of 32.\nIn inference, the co-pilot Î²-VAE is discarded for efficiency.\nThe dimension of zsem is set to 512 in our framework. To\nevaluate disentanglement, we conduct experiments on both\ngeneration & perception tasks, on synthetic & real data.\nDatasets & Baselines. For the image generation task, we\nconduct experiments on FFHQ [40], CelebA [39], LSUN\nHorse [96], LSUN Cars [96], Shape3D [42]. We compare\nour proposed CL-Dis with VAE-based, GAN-based, and\nDiffusion-based baselines, including Î²-VAE [30], Factor-\nVAE [42], Beta-TCVAE [7], GAN Edit v1 [79], GAN Edit\nv2 [12], InfoGAN-CR [54], Asyrp [48], GANspace (GS)\n[27], NaviNeRF [88], DisCo [66], Diff-AE [64] and vector-\nbased DisDiff [94].\nFor the perception task, we just choose the classic face\nrecognition for instantiation due to limited space and con-\nduct experiments on CASIA-Webface [95] and VGGFace2\n[5] in comparison with the baseline of LFW [35].\nEvaluation Metrics. To alleviate errors, we have multiple\nruns for each method. We use 4 representative metrics of\nFactor-VAE score [42], DCI [19], FID [29] and classifica-\ntion accuracy in face recognition, and our additionally pro-\nposed new metric for validation. Note that our CL-Dis be-\nlongs to dimension-/scalar-based DRL methods, so we just\ndiscuss but do not directly compare with many vector-based\nones [6, 16, 55, 77, 93] in the next experiment sections.\n5.2. Comparisons with SOTA DRL Methods\nTable 1.\nQuantitative comparison re-\nsults on CelebA dataset.\nMethods\nFID\nVAE [45]\n132.80\nÎ²-VAE [30]\n136.23\nÎ²-TCVAE [7]\n139.07\nFactorVAE [42]\n134.52\nCL-Dis (Ours)\n6.54\nTable 2.\nQuan-\ntitative comparison\nresults\non\nFFHQ\ndataset.\nMethods\nFID\nNaviNeRF [88]\n13.00\nCL-Dis (Ours)\n12.15\nQuantitative Results. Table 1 and Table 2 report the re-\nsults of Frechet Inception Distance (FID) [29] to measure\nthe quality of the generated images. We see that our CL-\nDis outperforms other typical disentangled methods in gen-\neration quality, especially on the real-world face images of\nCelebA.\nBesides, the quantitative comparison results of disentan-\nglement under different metrics are shown in Table 3. As\nshown in the table, CL-Dis outperforms other competitors,\ndemonstrating the methodâ€™s superior disentanglement abil-\nity. Compared with the VAE-based methods, these methods\nsuffer from the the trade-off between generation and dis-\nentanglement [4, 30], but CL-Dis does not have this prob-\nlem due to its automatic optimization mechanism. As for\nthe GAN-based methods are typically limited by the latent\nspace of GAN [94]. The diffusion-based DisDiff is a vector-\nTable 3. Comparisons of disentanglement on the FactorVAE score\nand DCI disentanglement metrics (mean Â± std, higher is better).\nWe can see that our scalar-based CL-Dis even achieves compa-\nrable results with the co-current factor-based vector-wise DisD-\niff [94].\nMethod\nShapes3D\nFactorVAE score\nDCI\nVAE-based:\nFactorVAE [42]\n0.840 Â± 0.066\n0.611 Â± 0.082\nÎ²-TCVAE [7]\n0.873 Â± 0.074\n0.613 Â± 0.114\nGAN-based:\nInfoGAN-CR [54]\n0.587 Â± 0.058\n0.478 Â± 0.055\nPre-trained\nGAN-based:\nGAN Edit v1 [79]\n0.805 Â± 0.064\n0.380 Â± 0.062\nGANspace [27]\n0.788 Â± 0.091\n0.284 Â± 0.034\nDisCo [66]\n0.877 Â± 0.031\n0.708 Â± 0.048\nDiffusion-based:\nDisDiff [94]\n0.902 Â± 0.043\n0.723 Â± 0.013\nCL-Dis (Ours)\n0.952 Â± 0.017\n0.731 Â± 0.045\nSmile\nEyes\nEarrings\nTeeth\nFigure 4. Fine-grained Disentanglement Results. The qualitative\nresults present the results of attribute manipulation, which demon-\nstrate the semantic manipulation on the FFHQ dataset. To clearly\nshow the isolated and â€œdisentangledâ€ effects, we use red circles to\nmark the corresponding changed regions.\nwise method, in which representations {zc} could express\nmore information than our scalar-based ones zsem, but our\nCL-Dis still outperforms DisDiff on Shape3D. And, our\nscalar-based disentangled representations are more light,\ngeneric, robust, and easy to apply to different scenarios.\nQualitative Results. Unlike most previous papers that only\ntest on the toy synthetic datasets [7, 30, 42], our CL-Dis\ncan be naturally applied to real images. Figure 4 shows the\neffectiveness of our method on various datasets. To clearly\nshow the isolated and â€œdisentangledâ€ effects, we use red cir-\ncles to mark the corresponding changed regions. We can ob-\nserve that CL-Dis can disentangle and change the isolated\nattributes continuously in an unsupervised manner, such as\nserious expression â†’ smile, closing eyes â†’ opening eyes,\nw/o earrings â†’ w/ earrings, etc.\nMoreover, to validate the superiority of CL-Dis in the\ndisentanglement capability, Figure 5 also provides results\nfor changing human facial attributes to different character-\nOurs\nGAN Edit v1 \nAsyrp\nGAN Edit v2\nMouth\nOurs\nGAN Edit v1\nAsyrp\nGAN Edit v2\nEyes\nOurs\nGAN Edit v1 \nAsyrp\nGAN Edit v2\nHair\nFigure 5. Comparison of the â€œmouthâ€, â€œeyesâ€ and â€œhairâ€ attributes\nwith different methods on the FFHQ dataset. Since different meth-\nods pre-train on different datasets, we thus compare the disentan-\nglement with different images according to the previous methods\n[12, 48, 79].\nistics along certain directions like â€œMouthâ€, â€œEyesâ€ and\nâ€œhairâ€, in comparison with three baselines of GAN Edit\nv1 [79], GAN Edit v2 [12], and Asyrp [48]. We can see\nthat other methods like Asyrp are prone to generate arti-\nfacts (bottom row of Figure 5), and their editing results are\nnot well-disentangled. For example, the whole face content\nhas changed for the schemes of GAN Edit v1 [79] and GAN\nEdit v2 [12]). In contrast, our method consistently keeps\nthe identity while just changing the disentangled properties,\nachieving an â€œisolated and disentangledâ€ effect.\nTo demonstrate the generalization ability of our CL-\nDis, we also use different datasets and different backbones\n(supplementary) for validation.\nAs shown in Figure 6,\neven for cars, horses, and buildings, CL-Dis still obtains\na group of fine-grained disentanglement results, where only\nthe â€œcolorâ€ attribute changes across the traversal direction,\nproving the disentanglement generalization capability.\nFigure 6. Generalization evaluation on extensive datasets. We vi-\nsualize the results of shifting the latent zsem with our proposed\nmethod along the common â€œColorâ€ direction on LSUN Horse\n(upper row), LSUN Cars (middle row), and CelebA (lower row)\ndatasets, respectively.\n5.3. Ablation Study and More Analysis\nEffectiveness of Different Elements. To analyze the ef-\nfectiveness of the proposed parts in our closed-loop disen-\ntanglement framework, we perform an ablation to study the\ninfluence of Semantics Navigation, Knowledge Distillation\nLdt, and Diffusion Feedback Lfd. We take FFHQ as the\ndataset to conduct these ablation studies.\nAs shown in Figure 7, due to the baseline of Diff-AE it-\nself not having a disentanglement ability, when shifting its\nlatent we get generation results with background artifacts.\nIf we directly apply the semantics navigation [79] to the\nbaseline latent, the generation quality has been improved,\nbut the disentanglement is still not guaranteed. For exam-\nple, when shifting along the semantic direction of â€œHairâ€\n(The 1st row of Figure 7), global features such as face\nshape, skin texture, and hairstyle are variously entangled,\nresulting in a younger appearance for the man. The last\ntwo schemes of adding knowledge distillation and diffusion\nfeedback have achieved better performance in disentangle-\nment, particularly for the final closed-loop scheme. We can\nsee that the final scheme together with Ldt & Lfd achieves\na better disentanglement effect on the semantic direction of\nâ€œHairâ€, compared to that only with Ldt which still changes\nface characteristics in the traversal generations.\nAnalysis of Auto-Driven Feedback Mechanism. As in-\ntroduced, the core contribution of this work lies in an auto-\nmatically disentangled learning mechanism, achieved by a\ndynamic Cdyn whose value depends on the increasing infor-\nmation capacity in the reverse diffusion process. To prove\nthat, we visualize the changing curves of Cdyn during the\nCL-Dis training. As illustrated in Figure 8(a), the value of\nCdyn gradually increases with the diffusion feedback, so\nas to highlight the penalty for KL-divergence item in the\nco-pilot Î²-VAE optimization and thus improve disentangle-\nment performance of the entire framework.\n5.4. New Metric & Discrimination Task Validation\nNew Metric Evaluation. As our work belongs to the track\nof unsupervised representation disentanglement, not image\n+Navi\n+Knowledge Distillation      \n+Navi\n+Knowledge Distillation\n+Diffusion Feedback      \nHair\nEyes\nBaseline (Diff-AE)\n+ Navigation (Navi)\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nMouth\nâ€¦\nâ€¦\nâ€¦\nâ€¦\nFigure 7. Ablation study on the FFHQ dataset. We evaluate the effectiveness of different components in our method by shifting different\nattributes, including â€œHairâ€, â€œMouthâ€ and â€œEyesâ€. We can see that as more elements are added, both the disentanglement performance and\nthe generation quality improve: the first column exists artifacts; the second and third columns change other contents except shift attributes.\nTeeth\nEyes\n20 images\n20 images\nIteration\nğ¶à¯—à¯¬à¯¡\n11.2\n11.0\n10.8\n10.6\n10.4\n10.2\n10.0\n50\n100\n150\n200\n250\n(b) Averaged heatmaps of the pixel differences\n(a) Visualization of the ğ¶à¯—à¯¬à¯¡ curve\nFigure 8. (a) Visualization of the curve of changeable Cdyn at\ndifferent iteration steps. Cdyn gradually increases with the dif-\nfusion feedback to improve the latent disentangling. (b) Aver-\naged heatmaps of the pixel differences between the original and\nthe shifted images for the â€œeyes shiftâ€ (left) and the â€œteeth shiftâ€\n(right), revealing a good disentanglement of ours.\ngeneration or editing. The core of this task is to fully un-\nderstand the latent factors of the model to enhance its fine-\ngrained controllability. The qualitative results shown above\nhave illustrated that traversing along the disentangled rep-\nresentations results in an â€œproperty isolatedâ€ generation ef-\nfect. To measure such effect quantitatively and show our\nmethod could progressively change the disentangled prop-\nerties while keeping others frozen, we propose a new metric\n(Sec. 4.3) through the lens of optical flow [76]. As illus-\ntrated in Figure 9, the optical flow visualization between\nthe original and shifted image pairs on FFHQ dataset [40],\nshows that our proposed CL-Dis method demonstrates ef-\nfective disentanglement, where shifting a single dimension\nof the latent space results in isolated property changes in the\ngenerated output. In contrast, the baseline approach (origi-\nnal Diff-AE) changes the entire background content of the\nimage.\nFurthermore, we perform normalization on the optical\nflow map and utilize a dynamic threshold to classify pixels\nas either exceeding or not exceeding it, thereby obtaining\nthe ratio. We make statistics based on the 100 sets, and cal-\nculate the average ratio using a static threshold of 0.5, and\nget the average value. The baseline quantitatively achieves\n0.1606 in this new metric, while our method is only 0.0177\n(the lower the better), indicating that our method has bet-\nter disentanglement performance that only isolated property\nOurs\nBaseline\nOriginal\nImage\nShifted\nImage\nOptical\nFlow\nFigure 9. Evaluation on the proposed new disentanglement metric\nwith the â€˜hairâ€™, â€˜eyesâ€™, and â€˜smileâ€™ attributes. From the optical flow\ntracking visualization, we find traversing along our disentangled\nrepresentations results in an â€œproperty isolatedâ€ generation effect,\nwhile the baseline exhibits a bad performance with a fully changed\nimage content.\nchanges when shifting across certain dimensions.\nLocality of visual effects. To illustrate that the navigation\nalong the semantic directions results in an isolated and â€œdis-\nentangledâ€ effect, for a particular direction, we compute the\nper-pixel differences averaged over the generation results\nwith 20 shift magnitudes from a uniform grid in a range.\nFigure 8(b) shows the averaged heatmap for the â€œEyesâ€ and\nâ€œMouthâ€ directions. We can see that our method achieves\na good disentanglement, where shifting â€œEyesâ€ just affects\nthe eyes-related image content, and shifting â€œMouthâ€ just\naffects the mouth-related image content.\nDiscrimination Task Validation. To validate whether the\ngood disentanglement achieved by our CL-Dis could bene-\nfit the discrimination task, we conducted face recognition\nexperiments on two large well-known datasets, CASIA-\nWebface [95] and VGGFace2 [5]. We use the semantics\nencoder Esem of CL-Dis as an off-the-shelf feature extrac-\ntor and introduce two additional learnable linear layers for\nfine-tuning. Following the same protocol used in [68], we\nperform a fair comparison in Table 4 with mean classifi-\ncation accuracy as a metric. We observe that the seman-\ntic encoder of our CL-Dis performs better than the baseline\nof learning from scratch (LFS) on the widely adopted test\nbenchmark LFW [35], and even our Esem trained with only\n5 epochs still outperforms the LFS model with 50 epochs,\nwhich demonstrates that the well-disentangled representa-\ntions learned by CL-Dis indeed benefit discrimination tasks.\nTable 4. Discrimination task (face recognition) validation. Our\nmethod with good disentangled representations achieves >2.0%\ngains even with fewer (5) training epochs compared to baseline.\nDataset\nCASIA-Webface (%)\nVGGFace2 (%)\nBaseline (50 epochs)\n89.45+-0.015\n81.82+-0.019\nOurs (5 epochs)\n91.72+-0.011\n87.32+-0.014\nOurs (50 epochs)\n94.00+-0.785\n87.97+-0.012\n6. Conclusion\nIn this paper, we present CL-Dis, an unsupervised closed-\nloop disentanglement framework that achieves a strong\nDRL ability without relying on heavy supervision or heuris-\ntic loss constraints. CL-Dis embeds a closed-loop inter-\nconnection, where a pre-trained Î²-VAE empowers a pre-\nliminary disentanglement ability to diffusion model with a\ndistillation loss, while the increasing information capabil-\nity in the diffusion is taken as feedback, in turn, to pro-\ngressively improve Î²-VAE disentangling, forming a core\nmutually-promoting mechanism. Besides, we also design\na self-supervised strategy to navigate semantic directions in\nthe well-disentangled latent for a controllable generation.\nA new metric is further introduced to measure the disentan-\nglement effect. Experimental results indicate that CL-Dis\nachieves a state-of-the-art DRL performance, and its strong\ndisentangled ability benefits both generation and discrimi-\nnation tasks, especially on real-world images.\nSupplementary Material\nA. Understanding the effects of our CL-Dis\nConstrained optimization is important for enabling deep un-\nsupervised models to learn disentangled representations of\nthe independent data generative factors [30]:\n(1) In the Î²-VAE framework, this corresponds to tuning\nthe Î² coefficient in Eq. 1 as follows,\nL(Î¸, Ï•, x, z, Î²) =EqÏ•(z|x)\n\u0002\nlog pÎ¸(x|z)\n\u0003\nâˆ’\nÎ²DKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\n,\n(9)\nÎ² can be taken as a mixing coefficient for balancing the\nmagnitudes of gradients from the reconstruction and the\nprior-matching components of the VAE lower bound for-\nmulation during training. When Î² is too low or too high the\nmodel learns an entangled latent representation due to either\ntoo much or too little capacity in the latent bottleneck [30].\n(2) In the understanding Î²-VAE version with a controlled\ncapacity increase strategy [4], this constrained optimization\ncorresponds to tuning the C coefficient in Eq 2 as follows,\nL(Î¸, Ï•, C; x, z) =EqÏ•(z|x) log pÎ¸(x|z)âˆ’\nÎ²\n\f\fDKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\nâˆ’ C\n\f\f,\n(10)\nwhere the model is trained to reconstruct the images with\nsamples from the factor distribution, but with a range of\ndifferent target encoding capacities by pressuring the KL\ndivergence to be at a controllable value, C. The training ob-\njective combined maximizing the log-likelihood and mini-\nmizing the absolute deviation from C by linearly increasing\nit from a low value to a high value over the course of train-\ning. The intuitive picture behind C is that it gradually adds\nmore latent encoding capacity, enabling progressively more\nfactors of variation to be represented while retaining disen-\ntangling in previously learned factors.\n(3) In the proposed closed-loop unsupervised represen-\ntation disentanglement framework CL-Dis, this constrained\noptimization is totally automatic and dynamic-adaptive.\nThis is achieved under the mutual driving of reverse diffu-\nsion of Diff-AE and VAE latent evolution. We make the\nconstant C in\n[4] become changeable Cdyn, controlling\nits value dynamically according to the information capac-\nity variation in the reverse diffusion process as follows,\nLfd(Î¸, Ï•, Cdyn; x, z) = EqÏ•(z|x) log pÎ¸(x|z)âˆ’\nÎ²\n\f\fDKL\n\u0000qÏ•(z|x)âˆ¥pÎ¸(z)\n\u0001\nâˆ’ Cdyn\n\f\f,\n(11)\nCdyn âˆ I(x0; xt) =\nX X\np(x0, xt) Â· log\n\u0012\np(x0, xt)\np(x0) Â· p(xt)\n\u0013\n(12)\ns.t.\npÎ¸â€²(xtâˆ’1|xt) = N(xt; ÂµÎ¸â€²(xt, t), ÏƒtI),\n(13)\nwhere the Cdyn is proportional to the information capabil-\nity of the reverse process in diffusion (here we use the mu-\ntual information I(x0; xt) to represent its value), which is\nparameterized by Î¸â€² with a U-Net architecture and is a pro-\ngressively increasing process of information capacity (i.e.,\nentropy decreasing). Continuing this intuitive picture, we\ncan imagine that with the capacity of the information capa-\nbility gradually increasing in the reverse diffusion, the dy-\nnamic Cdyn would continue to utilize those extra indicators\nfor an increasingly precise encoding, where a larger im-\nprovement can be obtained by disentangling different fac-\ntors of variations in the latent.\nB. Implementation Details of the New Metric\nTo measure the disentanglement effect quantitatively, espe-\ncially under the unsupervised setting where no factor anno-\ntations are accessible, we propose a new label-free metric\nwith the help of optical flow [76]. The high-level idea is\nthat we leverage optical flow to track the obviously and con-\ntinually changed content to evaluate whether or not these\nchanges are â€œproperty isolatedâ€. First, we calculate the op-\ntical flow between the original image and the shifted image.\nThen, we normalize the values of the optical flow map. Af-\nter that, we apply a threshold to the optical flow. Points\nin the flow exceeding this threshold are considered to have\na large shift magnitude (i.e., those disentangled attribute\npoints), while those below it are considered to have a small\nshift magnitude (i.e., those rest frozen attribute points). We\ncalculate the ratio of the number of flow points exceeding\nthis threshold to those below it and use this ratio value as\nour new metric score. The lower the better, and which ex-\nactly corresponds to the definition of disentangled represen-\ntations where a change in a single unit of the representation\ncorresponds to a change in a single factor of variation of the\ndata while being invariant to others. The specific algorithm\nis shown in Algorithm 1 and Algorithm 2. Empirically and\nexperimentally, we set the threshold as 0.5 for the calcula-\ntion of the proposed new metric.\nAlgorithm 1 Compute New Metric Based on Optical Flow\n1: function\nCOMPUTENEWMETRIC(image,\nshifted image, threshold)\n2:\nflow â† OPTICALFLOW(image, shifted image)\n3:\nnormalized flow â† NORM(flow)\n4:\nlarge shift count L â† 0\n5:\nsmall shift count S â† 0\n6:\nfor all flow point in normalized flow do\n7:\nif MAGNITUDE(flow point)\n>\nthreshold\nthen\n8:\nL â† L + 1\n9:\nelse\n10:\nS â† S + 1\n11:\nend if\n12:\nend for\n13:\nmetric score â† L/S\n14:\nreturn metric score\n15: end function\nAlgorithm 2 Optical Flow Normalization\n1: function NORM(flow)\n2:\nmax norm â† MAX(\npP flow2)\n3:\nnormalized flow â† flow/max norm\n4:\nreturn normalized flow\n5: end function\nTable 5. Influence study with different values of Cbase and Cmax\nin our CL-Dis framework on the Cars3D dataset. We evaluate the\ndisentanglement performance of different schemes using the met-\nrics of FactorVAE score and DCI (mean Â± std, higher is better).\nCbase\nCmax\nFactorVAE score\nDCI\n2\n5\n0.689 Â± 0.038\n0.094 Â± 0.009\n5\n15\n0.745 Â± 0.042\n0.175 Â± 0.012\n10\n11\n0.762 Â± 0.068\n0.179 Â± 0.014\n10\n12\n0.781 Â± 0.035\n0.182 Â± 0.008\n10\n25\n0.794 Â± 0.040\n0.186 Â± 0.007\n20\n50\n0.734 Â± 0.024\n0.163 Â± 0.015\nC. More Experimental Results\nC.1. Influence of Cbase and Cmax in Our CL-Dis\nAs we discussed in the main paper, we propose a novel dif-\nfusion feedback-driven loss Lfd to further facilitate the dis-\nentanglement of the representations. Formally, we make the\noriginal C in Î²-VAE auto-changeable, and control its value\ndynamically according to the information capacity of the\nreverse diffusion process as follows:\n14-0-1\n18-1-1\n21-7-1\n5-0-\nFigure 10. Generalization evaluation on extensive datasets. We vi-\nsualize the results of shifting the disentangled latent zsem with our\nproposed method. From top to bottom, we illustrate the â€œproperty\nisolatedâ€ effects along glasses, mouth, hair, car color, hair color,\nand face illumination, respectively.\nTable 6. Comparisons of disentanglement on the FactorVAE score\nand DCI disentanglement metrics (mean Â± std, higher is better).\nWe can see that our scalar-based CL-Dis even achieves compa-\nrable results with the co-current factor-based vector-wise DisD-\niff [94].\nMethod\nShapes3D\nFactorVAE score\nDCI\nVAE-based:\nFactorVAE [42]\n0.840 Â± 0.066\n0.611 Â± 0.082\nÎ²-TCVAE [7]\n0.873 Â± 0.074\n0.613 Â± 0.114\nGAN-based:\nInfoGAN-CR [54]\n0.587 Â± 0.058\n0.478 Â± 0.055\nPre-trained\nGAN-based:\nGAN Edit v1 [79]\n0.805 Â± 0.064\n0.380 Â± 0.062\nGANspace [27]\n0.788 Â± 0.091\n0.284 Â± 0.034\nDisCo [66]\n0.877 Â± 0.031\n0.708 Â± 0.048\nDiffusion-based:\nDisDiff [94]\n0.902 Â± 0.043\n0.723 Â± 0.013\nCL-Dis (Ours)\n0.952 Â± 0.017\n0.731 Â± 0.045\nCdyn = f(Ext) =\n(\nCbase Â· (\nEx0\nExt ),\nif 0 < f(Ext) < Cmax\nCmax,\nif f(Ext) â‰¥ Cmax\n(14)\nEx0 = âˆ’\nX\npx0 âˆˆR\npx0logpx0,\nExt = âˆ’\nX\npxt âˆˆR\npxtlogpxt,\n(15)\nwhere Cbase, Cmax are the original empirical start-point\nand the maximum upper-bound according to [4], and their\ninfluences are evaluated as follows. The results depicted\nin Table 5 demonstrate that our method is robust enough\nto achieve a stable satisfactory performance with different\nvalues of Cbase, Cmax.\nBesides, these experimental re-\nsults reveal that setting Cbase to 10 consistently yields opti-\nmal disentanglement outcomes. Moreover, our assessment\nalso extends to examining the impact of the upper-bound\nvalue, Cmax. As shown in Table 5, reducing the gap be-\ntween Cmax and Cbase hampers the overall disentangle-\nment performance (0.794 vs. 0.762 in FactorVAE score).\nWe analyze that, the variable scope of the dynamic Cdyn\nwill be limited due to the relatively small range between\nCmax and Cbase. This limitation may negatively impact\nthe control of KL-divergence, consequently hampering the\noverall modelâ€™s ability to effectively disentangle the inner\nlatent space more.\nC.2. More Subjective Results\nIn this section, we further provide more visualization results\nto demonstrate the generalization ability of our CL-Dis on\ndifferent datasets, including FFHQ [40], LSUN Cars [96]\nand CelebA [39].\nAs shown in Figure 10, our CL-Dis\ncould generate fine-grained disentangled results along the\nâ€œglasses, mouth, hair, car color, hair color, and face illu-\nminationâ€ attribute changes across the traversal direction,\nwhich fully illustrates the well-generalized disentanglement\ncapability of our method.\nC.3. Dimension Ablation on Shape3D\nWe further conduct Dimension Ablation on Shapes3D\ndataset in Table 7. We can see below that reducing the di-\nmension of zsem leads to minor changes, which indicates\nour method is robust enough.\nDimension\nFactorVAE score\nDCI\nCL-Dis (512)\n0.840 Â± 0.066\n0.611 Â± 0.082\nCL-Dis (100)\n0.842 Â± 0.042\n0.547 Â± 0.051\nCL-Dis (32)\n0.837 Â± 0.053\n0.582 Â± 0.049\nTable 7.\nAblation study for the dimension of zsem on the\nShapes3D dataset.\nD. Limitation Discussions\nIn Table 6 (corresponds to Table 2 in the main manuscript),\nwe observe our method encounters an unsatisfactory\nperformance on the MPI3D dataset, which indicates that\nour method may be distracted by the real-world natural\nenvironment scenarios. We discuss the potential reason that\nthe collected data in MPI3D [23], focus on bridging the\ngap between simulation and reality, which distinguishes\nit apart from various other synthetic toy datasets [42, 65].\nIn detail,\nreal-world recordings in MPI3D introduce\nmore extra environment interruptions, such as chromatic\naberrations in cameras and complex surface properties\nof objects (e.g., reflectance, radiance, and irradiance).\nThese interruptions pose challenges in learning disen-\ntangled representations due to the mixing of factors and\ndistortions during the real-world recordings, especially\nfor scalar-based methods like ours [23].\nThis unsolved\nlimitation of our method will be left as future research work.\nReferences\n[1] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D\nLawrence, and Zhenwen Dai. Variational information dis-\ntillation for knowledge transfer. In CVPR, pages 9163â€“9171,\n2019. 3\n[2] Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Genera-\ntive modeling for multi-task visual learning. In ICML, pages\n1537â€“1554. PMLR, 2022. 1\n[3] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Repre-\nsentation learning: A review and new perspectives. TPAMI,\n35(8):1798â€“1828, 2013. 1, 2\n[4] Christopher P Burgess, Irina Higgins, Arka Pal, Loic\nMatthey, Nick Watters, Guillaume Desjardins, and Alexan-\nder Lerchner.\nUnderstanding disentangling in Î²-vae.\nNeurIPS Workshop on Learning Disentangled Representa-\ntions, 2017. 1, 2, 3, 4, 5, 6, 10, 11\n[5] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An-\ndrew Zisserman. VGGFace2: A dataset for recognising faces\nacross pose and age. In International Conference on Auto-\nmatic Face and Gesture Recognition, 2018. 6, 8\n[6] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan,\nYuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled\nparameter-efficient tuning for subject-driven text-to-image\ngeneration. arXiv preprint arXiv:2305.03374, 2023. 6\n[7] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K\nDuvenaud.\nIsolating sources of disentanglement in varia-\ntional autoencoders. NeurIPS, 31, 2018. 1, 2, 3, 6, 11\n[8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya\nSutskever, and Pieter Abbeel.\nInfogan: Interpretable rep-\nresentation learning by information maximizing generative\nadversarial nets. NeurIPS, 29, 2016. 1, 2\n[9] Zikun Chen, Ruowei Jiang, Brendan Duke, Han Zhao, and\nParham Aarabi. Exploring gradient-based multi-directional\ncontrols in gans. In ECCV, pages 104â€“119. Springer, 2022.\n3\n[10] Pengyu Cheng, Martin Renqiang Min, Dinghan Shen,\nChristopher Malon, Yizhe Zhang, Yitong Li, and Lawrence\nCarin.\nImproving disentangled text representation learn-\ning with information-theoretic guidance.\narXiv preprint\narXiv:2006.00693, 2020. 2\n[11] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A sur-\nvey of model compression and acceleration for deep neural\nnetworks. arXiv preprint arXiv:1710.09282, 2017. 3\n[12] Anton Cherepkov, Andrey Voynov, and Artem Babenko.\nNavigating the gan parameter space for semantic image edit-\ning. In CVPR, pages 3671â€“3680, 2021. 3, 5, 6, 7\n[13] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu,\nMickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf:\nSafe reinforcement learning from human feedback. arXiv\npreprint arXiv:2310.12773, 2023. 3\n[14] Sanjoy Dasgupta and Sivan Sabato. Robust learning from\ndiscriminative feature feedback.\nIn International Confer-\nence on Artificial Intelligence and Statistics, pages 973â€“982.\nPMLR, 2020. 3\n[15] Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, and Sivan\nSabato. Learning from discriminative feature feedback. Ad-\nvances in Neural Information Processing Systems, 31, 2018.\n3\n[16] Emily L Denton et al. Unsupervised learning of disentangled\nrepresentations from video. NeurIPS, 30, 2017. 6\n[17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch,\nAakanksha\nChowdhery,\nBrian\nIchter,\nAyzaan\nWahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-\ne: An embodied multimodal language model. arXiv preprint\narXiv:2303.03378, 2023. 3\n[18] Emilien Dupont. Learning disentangled joint continuous and\ndiscrete representations. NeurIPS, 31, 2018. 1\n[19] Cian Eastwood and Christopher KI Williams. A framework\nfor the quantitative evaluation of disentangled representa-\ntions. In International conference on learning representa-\ntions, 2018. 6\n[20] Kieran Egan.\nMemory, imagination, and learning: Con-\nnected by the story. Phi Delta Kappan, 70(6):455â€“459, 1989.\n1\n[21] Roland W Fleming. Visual perception of materials and their\nproperties. Vision research, 94:62â€“75, 2014. 1\n[22] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip\nIsola. Ganalyze: Toward visual definitions of cognitive im-\nage properties. In ICCV, pages 5744â€“5753, 2019. 3\n[23] Muhammad Waleed Gondal, Manuel Wuthrich, Djordje\nMiladinovic, Francesco Locatello, Martin Breidt, Valentin\nVolchkov, Joel Akpo, Olivier Bachem, Bernhard SchÂ¨olkopf,\nand Stefan Bauer. On the transfer of inductive bias from sim-\nulation to the real world: a new disentanglement dataset. Ad-\nvances in Neural Information Processing Systems, 32, 2019.\n12\n[24] Abel Gonzalez-Garcia, Joost Van De Weijer, and Yoshua\nBengio. Image-to-image translation for cross-domain dis-\nentanglement. NeurIPS, 31, 2018. 2\n[25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. NeurIPS, 27,\n2014. 1\n[26] Jianping Gou, Baosheng Yu, Stephen J Maybank, and\nDacheng Tao. Knowledge distillation: A survey. IJCV, 129:\n1789â€“1819, 2021. 3\n[27] Erik HÂ¨arkÂ¨onen, Aaron Hertzmann, Jaakko Lehtinen, and\nSylvain Paris. Ganspace: Discovering interpretable gan con-\ntrols. NeurIPS, 33:9841â€“9850, 2020. 6, 11\n[28] Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel\nDahlmeier. An unsupervised neural attention model for as-\npect extraction. In ACL, pages 388â€“397, 2017. 2\n[29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. Advances in neural information processing systems,\n30, 2017. 6\n[30] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\nXavier Glorot, Matthew Botvinick, Shakir Mohamed, and\nAlexander Lerchner. beta-vae: Learning basic visual con-\ncepts with a constrained variational framework.\nIn ICLR,\n2016. 1, 2, 3, 4, 6, 10\n[31] Irina\nHiggins,\nDavid\nAmos,\nDavid\nPfau,\nSebastien\nRacaniere, Loic Matthey, Danilo Rezende, and Alexander\nLerchner. Towards a definition of disentangled representa-\ntions. arXiv preprint arXiv:1812.02230, 2018. 1\n[32] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion\nguidance. arXiv preprint arXiv:2207.12598, 2022. 2\n[33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840â€“6851, 2020. 2,\n3\n[34] Wei-Ning Hsu and James Glass. Disentangling by partition-\ning: A representation learning framework for multimodal\nsensory data. arXiv preprint arXiv:1805.11264, 2018. 2\n[35] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric\nLearned-Miller.\nLabeled faces in the wild: A database\nforstudying face recognition in unconstrained environments.\nIn Workshop on faces inâ€™Real-Lifeâ€™Images: detection, align-\nment, and recognition, 2008. 6, 8\n[36] Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gun-\nhee Kim. Ib-gan: Disentangled representation learning with\ninformation bottleneck generative adversarial networks. In\nAAAI, pages 7926â€“7934, 2021. 1\n[37] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao\nLou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou,\nZhaowei Zhang, et al. Ai alignment: A comprehensive sur-\nvey. arXiv preprint arXiv:2310.19852, 2023. 3\n[38] Mingi Ji, Byeongho Heo, and Sungrae Park. Show, attend\nand distill: Knowledge distillation via attention-based fea-\nture matching. In AAAI, pages 7945â€“7952, 2021. 3\n[39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. arXiv preprint arXiv:1710.10196, 2017. 4, 6,\n12\n[40] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn Proceedings of the IEEE/CVF conference on computer vi-\nsion and pattern recognition, pages 4401â€“4410, 2019. 4, 6,\n8, 12\n[41] Anurag Katakkar, Clay H Yoo, Weiqin Wang, Zachary C\nLipton, and Divyansh Kaushik.\nPractical benefits of fea-\nture feedback under distribution shift.\narXiv preprint\narXiv:2110.07566, 2021. 3\n[42] Hyunjik Kim and Andriy Mnih. Disentangling by factoris-\ning. In ICML, pages 2649â€“2658. PMLR, 2018. 1, 2, 6, 11,\n12\n[43] Minyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir\nPavlovic.\nRelevance factor vae: Learning and identify-\ning disentangled factors. arXiv preprint arXiv:1902.01568,\n2019. 1, 2\n[44] Yeongmin Kim, Dongjun Kim, HyeonMin Lee, and Il-chul\nMoon.\nUnsupervised controllable generation with score-\nbased diffusion models: Disentangled latent code guidance.\nIn NeurIPS 2022 Workshop on Score-Based Methods, 2022.\n2\n[45] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 1, 2, 4,\n6\n[46] W Bradley Knox and Peter Stone. Augmenting reinforce-\nment learning with human feedback. In ICML 2011 Work-\nshop on New Developments in Imitation Learning (July\n2011), page 3, 2011. 3\n[47] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakr-\nishnan. Variational inference of disentangled latent concepts\nfrom unlabeled observations. ICLR, 2018. 1, 2, 3\n[48] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion\nmodels already have a semantic latent space. arXiv preprint\narXiv:2210.10960, 2022. 1, 2, 3, 6, 7\n[49] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum,\nand Samuel J Gershman. Building machines that learn and\nthink like people. Behavioral and brain sciences, 40:e253,\n2017. 1\n[50] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh\nSingh, and Ming-Hsuan Yang.\nDiverse image-to-image\ntranslation via disentangled representations. In ECCV, pages\n35â€“51, 2018. 2\n[51] Ping-Shien Lee and David K Sewell. A revised diffusion\nmodel for conflict tasks. Psychonomic Bulletin & Review,\npages 1â€“31, 2023. 3\n[52] Wonkwang Lee, Donggyun Kim, Seunghoon Hong, and\nHonglak Lee. High-fidelity synthesis with disentangled rep-\nresentation. In ECCV, pages 157â€“174. Springer, 2020. 2\n[53] Minjun Li, Yanghua Jin, and Huachun Zhu. Surrogate gra-\ndient field for latent space manipulation. In CVPR, pages\n6529â€“6538, 2021. 3\n[54] Zinan Lin, Kiran Koshy Thekumparampil, Giulia Fanti, and\nSewoong Oh. Infogan-cr: Disentangling generative adver-\nsarial networks with contrastive regularizers. arXiv preprint\narXiv:1906.06034, page 60, 2019. 1, 2, 6, 11\n[55] Liu Liu, Jiangtong Li, Li Niu, Ruicong Xu, and Liqing\nZhang. Activity image-to-video retrieval by disentangling\nappearance and motion. In AAAI, pages 2145â€“2153, 2021. 6\n[56] Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao,\nHaoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang, and\nMarco De Nadai. Smoothing the disentangled latent style\nspace for unsupervised image-to-image translation.\nIn\nCVPR, pages 10785â€“10794, 2021. 2\n[57] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar\nRaetsch, Sylvain Gelly, Bernhard SchÂ¨olkopf, and Olivier\nBachem. Challenging common assumptions in the unsuper-\nvised learning of disentangled representations. In interna-\ntional conference on machine learning, pages 4114â€“4124.\nPMLR, 2019. 2\n[58] Zhipeng Luo, Yazhou He, Yanbing Xue, Hongjun Wang, Mi-\nlos Hauskrecht, and Tianrui Li. Hierarchical active learning\nwith qualitative feedback on regions. IEEE Transactions on\nHuman-Machine Systems, 2023. 3\n[59] Joanna MaterzyÂ´nska, Antonio Torralba, and David Bau. Dis-\nentangling visual and written concepts in clip.\nIn CVPR,\npages 16410â€“16419, 2022. 2\n[60] Joel Pearson. The human imagination: the cognitive neu-\nroscience of visual mental imagery. Nature reviews neuro-\nscience, 20(10):624â€“634, 2019. 1\n[61] Massimiliano Pierobon and Ian F Akyildiz.\nCapacity of\na diffusion-based molecular communication system with\nchannel memory and molecular noise. IEEE Transactions\non Information Theory, 59(2):942â€“954, 2012. 3\n[62] Antoine Plumerault, HervÂ´e Le Borgne, and CÂ´eline Hude-\nlot. Controlling generative models with continuous factors\nof variations. ICLR, 2020. 3\n[63] Stefanos Poulis and Sanjoy Dasgupta. Learning with feature\nfeedback: from theory to practice. In Artificial Intelligence\nand Statistics, pages 1104â€“1113. PMLR, 2017. 3\n[64] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad-\nwongsa, and Supasorn Suwajanakorn.\nDiffusion autoen-\ncoders: Toward a meaningful and decodable representation.\nIn CVPR, pages 10619â€“10629, 2022. 2, 3, 4, 5, 6\n[65] Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee.\nDeep visual analogy-making. Advances in neural informa-\ntion processing systems, 28, 2015. 12\n[66] Xuanchi Ren, Tao Yang, Yuwang Wang, and Wenjun\nZeng.\nLearning disentangled representation by exploiting\npretrained generative models: A contrastive learning view.\nICLR, 2021. 2, 6, 11\n[67] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,\nYili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia\nLiu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat-\nform for embodied ai research. In ICCV, pages 9339â€“9347,\n2019. 3\n[68] Florian Schroff, Dmitry Kalenichenko, and James Philbin.\nFacenet: A unified embedding for face recognition and clus-\ntering. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 815â€“823, 2015. 8\n[69] Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang,\nShengzhong Liu, Dongxin Liu, Jun Wang, and Tarek Ab-\ndelzaher. Controlvae: Controllable variational autoencoder.\nIn International Conference on Machine Learning, pages\n8655â€“8664. PMLR, 2020. 1, 3\n[70] Yujun Shen and Bolei Zhou. Closed-form factorization of\nlatent semantics in gans. In CVPR, pages 1532â€“1540, 2021.\n3\n[71] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter-\npreting the latent space of gans for semantic face editing. In\nCVPR, pages 9243â€“9252, 2020. 3\n[72] Yichun Shi, Xiao Yang, Yangyue Wan, and Xiaohui Shen.\nSemanticstylegan: Learning compositional generative priors\nfor controllable image synthesis and editing. In CVPR, pages\n11254â€“11264, 2022. 3\n[73] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In ICLR, 2020. 2\n[74] Yue Song, Andy Keller, Nicu Sebe, and Max Welling. Latent\ntraversals in generative models as potential flows.\nICML,\n2023. 3\n[75] Yue Song, Jichao Zhang, Nicu Sebe, and Wei Wang. House-\nholder projector for unsupervised latent semantics discovery.\nIn ICCV, pages 7712â€“7722, 2023. 3\n[76] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field\ntransforms for optical flow.\nIn ECCV, pages 402â€“419.\nSpringer, 2020. 5, 8, 10\n[77] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre-\nsentation learning gan for pose-invariant face recognition. In\nCVPR, pages 1415â€“1424, 2017. 1, 6\n[78] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis-\nPhilippe Morency, and Ruslan Salakhutdinov. Learning fac-\ntorized multimodal representations. ICLR, 2018. 2\n[79] Andrey Voynov and Artem Babenko. Unsupervised discov-\nery of interpretable directions in the gan latent space.\nIn\nICML, pages 9786â€“9796. PMLR, 2020. 3, 5, 6, 7, 11\n[80] Hui Wang, Hanbin Zhao, Xi Li, and Xu Tan. Progressive\nblockwise knowledge distillation for neural network acceler-\nation. In IJCAI, pages 2769â€“2775, 2018. 3\n[81] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and\nQifeng Chen. High-fidelity gan inversion for image attribute\nediting. In CVPR, pages 11379â€“11388, 2022. 1\n[82] Xin Wang, Hong Chen, Siâ€™ao Tang, Zihao Wu, and Wenwu\nZhu. Disentangled representation learning. arXiv preprint\narXiv:2211.11695, 2022. 1\n[83] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion:\nControllable disentangled style transfer via diffusion models.\nIn ICCV, pages 7677â€“7689, 2023. 2\n[84] Jiawei Wu, Xiaoya Li, Xiang Ao, Yuxian Meng, Fei Wu,\nand Jiwei Li. Improving robustness and generality of nlp\nmodels using disentangled representations. arXiv preprint\narXiv:2009.09587, 2020. 2\n[85] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale,\nTrung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu\nChang. Uncovering the disentanglement capability in text-\nto-image diffusion models.\nIn CVPR, pages 1900â€“1910,\n2023. 1, 3\n[86] Yankun Wu, Yuta Nakashima, and Noa Garcia. Not only\ngenerative art: Stable diffusion for content-style disentan-\nglement in art analysis. In Proceedings of the 2023 ACM In-\nternational Conference on Multimedia Retrieval, pages 199â€“\n208, 2023. 1\n[87] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace\nanalysis: Disentangled controls for stylegan image genera-\ntion. In CVPR, pages 12863â€“12872, 2021. 2\n[88] Baao Xie, Bohan Li, Zequn Zhang, Junting Dong, Xin Jin,\nJingyu Yang, and Wenjun Zeng. Navinerf: Nerf-based 3d\nrepresentation disentanglement by latent semantic naviga-\ntion. arXiv preprint arXiv:2304.11342, 2023. 6\n[89] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang,\nand Humphrey Shi. Versatile diffusion: Text, images and\nvariations all in one diffusion model. In ICCV, pages 7754â€“\n7765, 2023. 1\n[90] Zipeng Xu, Tianwei Lin, Hao Tang, Fu Li, Dongliang He,\nNicu Sebe, Radu Timofte, Luc Van Gool, and Errui Ding.\nPredict, prevent, and evaluate: Disentangled text-driven im-\nage manipulation empowered by pre-trained vision-language\nmodel. In CVPR, pages 18229â€“18238, 2022. 2\n[91] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee.\nAttribute2image: Conditional image generation from visual\nattributes. In ECCV, pages 776â€“791. Springer, 2016. 1\n[92] Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and\nNanning Zheng.\nTowards building a group-based unsu-\npervised representation disentanglement framework. ICLR,\n2021. 2\n[93] Tao Yang, Yuwang Wang, Cuiling Lan, Yan Lu, and Nan-\nning Zheng. Vector-based representation is the key: A study\non disentanglement and compositional generalization. arXiv\npreprint arXiv:2305.18063, 2023. 6\n[94] Tao Yang, Yuwang Wang, Yan Lv, and Nanning Zh. Disd-\niff: Unsupervised disentanglement of diffusion probabilistic\nmodels. arXiv preprint arXiv:2301.13721, 2023. 1, 2, 4, 6,\n11\n[95] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn-\ning face representation from scratch.\narXiv preprint\narXiv:1411.7923, 2014. 6, 8\n[96] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas\nFunkhouser, and Jianxiong Xiao. Lsun: Construction of a\nlarge-scale image dataset using deep learning with humans\nin the loop. arXiv preprint arXiv:1506.03365, 2015. 4, 6, 12\n[97] Yuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai, and\nXiaojie Yuan. Learning disentangled representation for mul-\ntimodal cross-domain sentiment analysis. TNNLS, 2022. 2\n[98] Xinqi Zhu, Chang Xu, and Dacheng Tao. Where and what?\nexamining interpretable disentangled representations.\nIn\nCVPR, pages 5861â€“5870, 2021. 1, 2\n"
}