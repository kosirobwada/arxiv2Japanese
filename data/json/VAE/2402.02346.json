{
    "optim": "Closed-Loop Unsupervised Representation Disentanglement with Î²-VAE Distillation and Diffusion Probabilistic Feedback Xin Jin1, *, Bohan Li1, 2, BAAO Xie1, Wenyao Zhang1, 2, Jinming Liu1, 2, Ziqiang Li1, 2, Tao Yang3, Wenjun Zeng1, 1Ningbo Institute of Digital Twin, Eastern Institute of Technology, Ningbo, China 2Shanghai Jiao Tong University, Shanghai, China, 3Xiâ€™an Jiaotong University, Xiâ€™an, China Corresponding: jinxin@eitech.edu.cn Abstract Representation disentanglement may help AI fundamen- tally understand the real world and thus benefit both dis- crimination and generation tasks. It currently has at least three unresolved core issues: (i) heavy reliance on label annotation and synthetic data â€” causing poor generaliza- tion on natural scenarios; (ii) heuristic/hand-craft disen- tangling constraints make it hard to adaptively achieve an optimal training trade-off; (iii) lacking reasonable evalua- tion metric, especially for the real label-free data. To ad- dress these challenges, we propose a Closed-Loop unsu- pervised representation Disentanglement approach dubbed CL-Dis. Specifically, we use diffusion-based autoencoder (Diff-AE) as a backbone while resorting to Î²-VAE as a co- pilot to extract semantically disentangled representations. The strong generation ability of diffusion model and the good disentanglement ability of VAE model are complemen- tary. To strengthen disentangling, VAE-latent distillation and diffusion-wise feedback are interconnected in a closed- loop system for a further mutual promotion. Then, a self- supervised Navigation strategy is introduced to identify in- terpretable semantic directions in the disentangled latent space. Finally, a new metric based on content tracking is designed to evaluate the disentanglement effect. Experi- ments demonstrate the superiority of CL-Dis on applica- tions like real image manipulation and visual analysis. 1. Introduction Disentangled representation learning (DRL) [31] learns the underlying explainable factors behind the observed data, where the different latent factors correspond to different properties, respectively. This matches the way humans per- ceive this world that we understand an object from its var- ious properties (e.g., shape, size, color, etc.) [2, 20, 21, 60, 91]. Thus, DRL is thought to be one of the possible ways for AI to understand the world fundamentally, thus achiev- (a)   VAE-based DRL  (b)   GAN-based DRL (c)    Diffusion-based DRL (d)   our Closed-loop DRL ğ± ğ³ ğ„ğ§ğœ ğ’’ğ“(ğ’›|ğ’™) ğ’‘ğœ½(ğ’™|ğ’›) ğƒğğœ ğ±â€² min ğ‘«(ğ’’ğ“(ğ’›)||ğ’‘(ğ’›)) Infer disentangled latent ğ§ ğ³ ğ†ğğ§ ğ±â€² ğƒğ¢ğ¬ ğ± Noise ğ„ğ§ğœ ğ³â€² Real/ Fake min ğ‘°(ğ’›, ğ’›â€²) Infer disentangled latent ğ±ğŸ ğ±ğ­ Add  noise ğ”ğ§ğğ­ ğ„ğ§ğœ ğƒğğœ ğ“ ğ„ğ§ğœ à·œğ±ğŸ Diffusion process à·œğ³ ğ³â€² Infer disentangled latent by  hand-craft variant-invariant loss ğ³ ğ±ğŸ â€² ğ±ğŸ ğ±ğ­ Add  noise ğ”ğ§ğğ­ ğ„ğ§ğœ ğƒğğœ ğ“ à·œğ±ğŸ ğ³ ğ±ğŸ â€² min ğ‘«(ğ’’ğ“(ğ’›)||ğ’‘(ğ’›)) automatically Infer disentangled latent Distillation Feedback Diffusion  process Gaussian Figure 1. The illustration of disentangled representation learn- ing (DRL) frameworks: (a) VAE-based, (b) GAN-based, (c) Diffusion-based, and (d) our proposed Closed-loop approach. We can intuitively see that previous works all relied on heuristic hand- craft loss constraints to infer disentangled latent, only our method leverages two kinds of generative branches to build a cycle system for mutually-promoting automatically disentangled learning. ing Artificial General Intelligence (AGI) [3, 49, 82]. Most existing DRL methods learn the disentangled rep- resentation based on generative models, such as VAE [4, 7, 18, 30, 42, 43, 45, 47, 69], GAN [8, 25, 36, 54, 77, 98], and even recently popular diffusion [48, 85, 86, 89, 94]. Among them, the VAE-based DRL methods always encourage the disentanglement of the latent variables by forcing the vari- ational posterior to be closer to the factorized Gaussian prior (see Figure 1(a)), but they generally need to balance a hard inherent trade-off between the disentangling ability and generating quality [7, 30, 42]. As shown in Figure 1(b), the GAN-based DRL methods tend to use the mutual in- formation as disentanglement constraints, but they typically suffer from the problem of reconstruction quality due to the gan-inversion difficulty [81, 94]. Besides, VAE- and GAN- arXiv:2402.02346v1  [cs.CV]  4 Feb 2024 based generative models are prone to learn the factorized or conditioned statistics bias [32], which essentially conflicts with the ultimate purpose of a generalizable disentangle- ment that makes AI understand the world like humans. Moreover, given the recently popular diffusion proba- bilistic models (DPMs) show superiority in image gener- ation quality [33, 73] and unsupervised controlling (i.e., learning meaningful representations) of images [64], re- searchers have started to investigate the possibility of DRL in DPMs. Asyrp [48] proposes an asymmetric reverse pro- cess to discover the semantic latent space in frozen pre- trained diffusion models, which reveals that DPMs already have a semantic latent space and thus more suitable for learning disentangled semantics. The co-current work of DisDiff [94] shown in Figure 1(c) uses the gradient field of frozen DPMs to achieve a vector-wise disentangled repre- sentation learning. However, this work applies an external heuristic hand-craft variant-invariant loss [57, 94] to enforce the disentanglement processes, resulting in reduced model flexibility and posing challenges for optimization. In this paper, we propose to leverage two kinds of gen- erative branches of the diffusion model and VAE model, to build a cycle system for mutually-promoting DRL. As shown in Figure 1(d)), we construct an unsupervised Closed-Loop Disentanglement framework dubbed CL-Dis, which is built upon a diffusion-based autoencoder (Diff- AE) backbone while combining with a co-pilot of Î²-VAE, to together learn the meaningful disentangled representa- tions. To collaborate two branches well and facilitate fea- ture disentangling, we encourage Diff-AE and Î²-VAE to complement each other, achieving a win-win effect: we use the pre-trained latent of VAE to guide a semantic-aware reverse diffusion process with a distillation loss. Then, the gradually increasing information capability during the dif- fusion process is taken as feedback, in turn, to progressively enforce the inner latent of VAE more disentangling. Next, a self-supervised Navigation strategy is introduced to clearly identify each factorâ€™s semantic meanings, where shifting along a disentangled semantic direction will result in con- tinuously changed generations. Finally, we further design a label-free metric based on â€œchanges trackingâ€ to quan- titatively measure the disentanglement of learned features, wherein we use optical flow to reflect image variation de- gree objectively. Our main contributions are summarized: â€¢ We build a new unsupervised representation disentan- glement framework CL-Dis, which has a mutually- promoting closed-loop architecture driven by diffusion and VAE models. They complement each other with a distillation loss and a new feedback loss, and thus em- power a more controllable and stronger DRL capability. â€¢ We introduce a self-supervised method in CL-Dis to clearly identify each factorâ€™s semantic meanings by navi- gating directions in the learned disentangled latent space, leading to coherent generated variations. This makes the generation more explainable and fine-controllable. â€¢ We design a new label-free metric to quantitatively mea- sure the interpretability and disentanglement of learned features, and provide an example of immediate practical benefit from our work. Namely, we experimentally show how to exploit our CL-Dis for generative manipulation of real images and visual discrimination tasks. 2. Related Works 2.1. Disentangled Representation Learning (DRL) The concept of DRL was introduced by Bengio et al. [3] in 2013 and was believed helpful in different tasks in practical applications like image generation [7, 8, 42, 43, 45, 47, 54, 87], image editing/translation [24, 50, 56, 83], NLP [10, 28, 84] and multimodal applications [34, 59, 78, 90, 97]. Different from image generation or editing, the core of this task is to fully understand the latent factors of the model to enhance its fine-grained controllability [7, 42, 43, 47, 87]. Most VAE-based DRL approaches like Î²-VAE [30], DIP-VAE [47], FactorVAE [42], etc., achieve unsupervised disentanglement by the direct constraints on probabilistic distributions [7, 30, 42]. While straightforward, Locatello et al. [57] point out that is not enough and emphasize the need for extra inductive bias. Similarly, Burgess [4] proposes to progressively increase the information capacity of the latent code in Î²-VAE. Yang et al. [92] use symmetry properties modeled by group theory as inductive bias. Except for VAE, there are also some DRL works based on GAN, e.g., lever- aging mutual information [8], self-supervised contrastive regularizer [54], spatial constriction [98], and combining with other pre-trained generative models [52, 66]. Fast-developing generative diffusion probabilistic mod- els (DPMs) have accelerated the DRL exploration. LCG- DM [44] uses a vanilla VAE in DPMs to extract semantics codes for controllable generation, but ignores the interac- tion between these two structures. Asyrp [48] discovers the semantic latent space in frozen pre-trained DPMs for image manipulation. Co-current DisDiff [94] proposes a vector-wise method to express more information compared with other scalar-based methods [4, 48, 54]. However, they uses gradient fields as an inductive bias at all time steps to achieve DRL, which relies on an external heuristic hand- craft disentangling loss, lacking a self-driven learning abil- ity and is hard to optimize. In contrast, our CL-Dis builds a closed-loop DRL framework with a mutual-promoting mechanism driven by a two-branch complementary diffu- sion and Î²-VAE architecture, encouraging stronger disen- tangled feature auto-learning. 2.2. Knowledge Distillation and Feature Feedback Our closed-loop CL-Dis is bridged by these two techniques. Knowledge distillation (KD) is originally developed for model compression and acceleration [11, 26, 80], or used in transfer learning [1, 38]. Feature feedback [14, 15, 41, 58, 63] enhances the robustness of learning systems by using explainable information, which is gaining more and more attention in AI alignment [37], RLHF [13, 46], and embodied AI [17, 67]. For instance, ControlVAE [69] achieves much better reconstruction quality with a variant of the proportional-integral-derivative (PID) control, using the output KL-divergence as feedback. Following this idea, we explore taking the strong generation capability of diffu- sion models as information-increasing feedback to automat- ically and adaptively enhance disentangled feature learning. 2.3. Latent Semantics Discovery (LSD) This task emerges to move around the latent code (of GAN [12, 75, 79], Diffusion [48], etc), such that only one factor varies during the traversal generation. They mostly rely on human annotations (i.e., segmentation masks, at- tribute categories, 3D priors, and text descriptions) to define the semantic labels [9, 22, 53, 62, 71, 72]. Unsupervised LSD [70, 74, 79] often learn a set of directions or a classi- fier/matrix (e.g., Hessian, Jacobian regularization, etc.) to identify latent semantics. Wu et al. [85] find semantics in the stable diffusion model by partially changing the text em- beddings. However, these methods are not strictly equiva- lent to DRL, most of them just rely on already disentangled (text) features to get coherent traversal generation results. 3. Background and Motivation 3.1. Understanding disentangling in Î²-VAE Due to the poor disentanglement of vanilla VAE on complex data, Î²-VAE [30] adds explicit inductive bias to strengthen the independence constraint of the variational posterior dis- tribution qÏ•(z|x) with a Î² penalty coefficient in ELBO: L(Î¸, Ï•) =EqÏ•(z|x) \u0002 log pÎ¸(x|z) \u0003 âˆ’ Î²DKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 , (1) when Î²=1, Î²-VAE degenerates to the original VAE, and larger Î² encourages more disentangled representations but harms the performance of reconstruction. Unfortunately, it is practically intractable to obtain the optimal Î² to balance the trade-off between reconstruction quality and the disen- tangling capability [7, 47]. Thus, Burgess et al. [4] under- stand Î²-VAE from the perspective of information bottleneck theory, and propose improving disentangling in Î²-VAE with controlled information capacity increase: L(Î¸, Ï•) =EqÏ•(z|x) log pÎ¸(x|z) âˆ’ Î² \f\fDKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 âˆ’ C \f\f, (2) where C gradually increases from 0 to a value large enough during the training to guarantee the expressiveness of la- tent representations, thus improving disentangling. How- ever, such a hand-craft increasing strategy is hard to con- trol and might hurt disentangling performance, especially for various scenarios on different datasets. Thus, in our work, we learn from this idea but upgrade it by exploiting the essence of the Diffusion Probabilistic Model (DPM) in automatically increasing information capacity [51, 61] for disentangling enhancement. 3.2. DPM and Diffusion Autoencoder (Diff-AE) Here, we first take DDPM [33] as a vanilla DPM example for illustration, which adopts a sequence of fixed variance distributions q(xt|xtâˆ’1) as the noise-adding forward pro- cess to collapse the image distribution p(x0) to N(0, I): q(xt|xtâˆ’1) = N(xt; p 1 âˆ’ Î²txtâˆ’1, Î²tI). (3) The reverse process is fitting by other distributions of Î¸, and it is a progressively increasing process of information ca- pacity (i.e., entropy decreasing) as we mentioned before: pÎ¸(xtâˆ’1|xt) = N(xt; ÂµÎ¸(xt, t), ÏƒtI), (4) where ÂµÎ¸(xt, t) is parameterize by a Unet ÏµÎ¸(xt, t). Its opti- mization is to minimize the variational upper bound of neg- ative log-likelihood LÎ¸ = Ex0,t,Ïµ âˆ¥Ïµ âˆ’ ÏµÎ¸(xt, t)âˆ¥. Furthermore, Diff-AE [64] is proposed for representa- tion learning based on DPM and Autoencoder. It uses a learnable semantic encoder for discovering the high-level meaningful semantics zsem = Esem(x0), and a DPM- based decoder p(xtâˆ’1|xt, zsem) that is conditioned on zsem for reconstruction. However, the latent representa- tion zsem learned by Diff-AE does not explicitly respond to the underlying factors of the data, or say, its disentangling potential has not been fully unleashed. 4. Methodology: Closed-Loop DRL (CL-Dis) Based on the analysis above, we propose to leverage VAE and diffusion to unleash their strengths for a mutually pro- moting DRL. As shown in Figure 2, our CL-Dis is a dual- branch cycle framework, involving multiple iterative phases with (a) diffusion autoencoder (Diff-AE), (b) Î²-VAE, and the core (c) knowledge distillation & feature feedback pro- cess. In Section 4.1, we first present the formulation of closed-loop DRL and the overview of CL-Dis. After that, we introduce different phases of CL-Dis, including Diff-AE pre-training, Î²-VAE pre-training, and knowledge distilla- tion & feature feedback in Section 4.2. Then, we propose a self-supervised Navigation strategy to identify disentangled semantic meanings and a new well-designed DRL metric (Section 4.3) for the final disentanglement measurement. 4.1. Problem Formulation and Overview of CL-Dis Given a practical dataset D derived by N underlying fac- tors C = {1, 2, . . . , N}, the target of disentanglement for Forward  Diffusion ğ’™ğ‘‡ ğ’™ğ‘‡âˆ’1 Real Data ğ‘ğ‘‘ğ‘ğ‘¡ğ‘(ğ±) Probabilistic  Encoder ğ‘ğœ™(ğ³|ğ±) ğœ‡ ğœ Reparameterization ğ³ğğ¢ğ¬ğğ§ Probabilistic  Decoder ğ‘ğœƒ(ğ±|ğ³) Reconstructed Data ğ‘ğ‘”(ğ±) Disentangled  Latent (c) Such increasing information capacity process can be  taken as an indicator to enhance disentangling Semantic  Encoder ğ¸ğ‘ ğ‘’ğ‘š ğ³ğ¬ğğ¦ ğ’™0 Conditioning â€¦ Smile, Azimuth, â€¦, Hair Color (a) Diffusion Autoencoders (b) ğœ· âˆ’ ğ‘½ğ‘¨ğ‘¬ â€¦ ğ’™ğ‘‡âˆ’2 ğ’™0 â€² ğ’™ğ‘‡âˆ’3 ğ’™1 Denoising  U-Net Denoising  U-Net Denoising  U-Net Denoising  U-Net Denoising  U-Net ğ›½ ğ‘(ğ³) ğ’™ğ‘‡âˆ’1 Reverse Denoising  Figure 2. Pipeline of the proposed CL-Dis. We explore using (a) diffusion-based autoencoder (Diff-AE) as a backbone while resorting to (b) Î²-VAE as a co-pilot via knowledge distillation to help Esem extract a semantic disentangled representation zsem of an input image x0 (which is taken as condition for diffusion-based generation). To further facilitate feature disentangling, we further take (c) the reverse diffusion process as an increasing information capacity indicator feedback to, in turn, enhance the disentanglement of feature zdisen in Î²-VAE. As a result, Diff-AE and Î²-VAE are interconnected as a closed-loop cycle system with VAE-latent distillation and diffusion-wise feedback controlling, achieving a win-win effect and well-disentangled fine-controllable representation learning. our CL-Dis is to learn a semantic encoder Esem, for each factor c âˆˆ C, when varying its latent value, its reconstructed generation results will have the corresponding changes. In Figure 2, our CL-Dis takes Diff-AE as the back- bone, which has exactly a semantic encoder Esem to en- code the raw data x0 into the semantic representations zsem for conditioning the diffusion reverse distributions p(xtâˆ’1|xt, zsem). To empower Esem disentangling capa- bility, we additionally construct a Î²-VAE as co-pilot to help zsem become a semantic disentangled representation. Given a dataset X from a distribution pdata(x), a stan- dard Î²-VAE framework [30] is built and Ï•, Î¸ parametrize the VAE encoder qÏ•(z|x) and the decoder pÎ¸(x|z), respec- tively. The prior p(z) is typically set to the isotropic unit Gaussian N(0, 1), using a differentiable â€œreparametrization trick (z = Âµ+Ïƒ)â€ [45]. With the objective of Eq. 1, we get a preliminary disentangled representations zdisen. Then, we use a distillation loss Ldt to transfer such disentangling ca- pability to the semantic latent zsem of Diff-AE, Ldt = DKL \u0000zsem âˆ¥ zdisen \u0001 = X zsem Â· log( zsem zdisen ), (5) As discussed before, gradually increasing the information capacity of the VAE latent with a controlled way could im- prove disentangling [4]. The reverse diffusion process of Diff-AE (Eq. 4) is exactly a procedure of information capa- bility increasing. Thus, we take it as an indicator (see Fig- ure 2(c)) in turn to enhance Î²-VAE disentangling by adap- tively adjusting the objective of Eq. 2, where we change the hand-craft C to a variable that can automatically change in the optimization (Eq. 8). Such a mutually promoting mech- anism forms a closed-loop optimization flow. 4.2. Training of the Whole CL-Dis Framework 4.2.1 Phase-1: Diff-AE and Î²-VAE Pre-training Following [64], the entire Diff-AE model is pre-trained on the various datasets [39, 40, 96] to cover more semantic fac- tors in the latent space of zsem. But at this phase, zsem is still far from well-disentangled, which is just taken as a kind of coarse condition for guiding diffusion generation [94]. Moreover, the co-pilot Î²-VAE is also pre-trained on the corresponding datasets (i.e., if Diff-AE was trained on the facial data, Î²-VAE did the same), following the objective of Eq. 1, its latent zdisen has already enabled a preliminary disentangling capability, i.e., can distinguish facial features of smile, azimuth, hair color, etc (Figure 2) to some degree. 4.2.2 Phase-2: Knowledge Distillation & Diffusion Feedback To transfer the preliminary disentangling capability of zdisen into zsem, we use a distillation loss shown in Eq. 5 to smoothly bridge Diff-AE and Î²-VAE. But this interaction is currently one-way, which is still limited by the intrinsic shortcomings of Î²-VAE discussed after Eq. 1 that it is hard to balance a trade-off and thus hurt the final performance. Therefore, we propose a novel diffusion feedback-driven loss Lfd to further facilitate the disentanglement of the rep- resentations (zdisen, zsem) and break the optimization bot- tleneck of the entire CL-Dis framework. Basically, inspired by the work of understanding Î²-VAE [4] that modified the vanilla objective by adding a hand-craft increasing con- troller C (as shown in Eq. 2), we propose to leverage the reverse diffusion of Diff-AE that adaptively increases infor- mation capacity to build constraints to replace the original format. Formally, we make the original C auto-changeable, and control its value dynamically according to the informa- tion capacity of the reverse diffusion process as follows: Cdyn = f(Ext) = ( Cbase Â· ( Ex0 Ext ), if 0 < f(Ext) < Cmax Cmax, if f(Ext) â‰¥ Cmax (6) Ex0 = âˆ’ X px0 âˆˆR px0logpx0, Ext = âˆ’ X pxt âˆˆR pxtlogpxt, (7) where Ex0 and Ext denote the information entropy of the static target image (i.e., Ground Truth) x0 and the inter- mediate predicted diffusion denoising image result xt, re- spectively. The probability values of px0, pxt are calcu- lated by flattening and probabilizing the values of the im- age matrices across the dimension of RCÃ—HÃ—W . Besides, Cbase, Cmax are the original empirical start-point and the maximum upper-bound according to [4], and their influ- ences are studied in supplementary. We can intuitively see that with the denoising diffusion processing, the entropy Ext of the predicted image result xt will gradually drop (i.e., information capacity increasing), and thus the ratio of Ex0 Ext will increase to raise the influence of Cdyn. So, the ob- jective of Î²-VAE changes from Eq. 2 to the following Lfd, Lfd(Î¸, Ï•, Cdyn; x, z) =EqÏ•(z|x) log pÎ¸(x|z)âˆ’ Î² \f\fDKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 âˆ’ Cdyn \f\f, (8) the above physical meaning is that: when KL-divergence drops to a certain extent (i.e., disentangling ability encoun- ters bottleneck in a naive VAE latent), the dynamic con- troller Cdyn will counteract this change by increasing the value of the second item of Eq. 8. Intuitively, Cdyn that is controlled by diffusion feedback gradually highlights the penalty for KL-divergence, thus improving the latent disen- tangling. Under this closed-loop optimization flow (see Fig- ure 2(c)), the gradually increasing information capacity of diffusion adaptively enhances the disentanglement of latent zdisen, so as to further benefit the semantic code of zsem. Please see supplementary for more theoretical analysis. 4.2.3 Phase-3: Semantics Directions Navigation After getting the well-disentangled representations of zsem, the next step is to identify its semantic meanings. Shifting a Diff-AE Generator Shifter ğ‘€(ğœ…, ğ›¿) Well-disentangled ğ³ğ¬ğğ¦ ğ³ğ¬ğğ¦ â€² = ğ³ğ¬ğğ¦+ ğ‘€(ğœ…, ğ›¿) Shift Predictor ğ‘ƒ ğœ…, ğ›¿ ğœ…â€², ğ›¿â€² Shifting a dimension of ğ’›ğ’”ğ’†ğ’ causes  only one factor (i.e., eyes) changes. Figure 3. A self-supervised Navigation strategy is to discover the interpretable shifts in the well-disentangled space of zsem. A training sample consists of two images, produced by the Diff-AE with original and shifted inputs. The images are given to a predic- tor P that predicts the direction index k and shift magnitude Î´. certain dimension of the well-disentangled condition zsem will result in a generated result that only the corresponding property/factor changes. Thus, we adopt an outer naviga- tion branch to identify the traversal directions in a global view. Specifically, we employ a learnable matrix M (see Figure 3) to manually append a shift on zsem. The shifted code zâ€² sem, together with the original zsem, is fed into our Diff-AE decoder to generate a pair of images, following the reverse conditional diffusion process of p(xtâˆ’1|xt, zsem). A learnable predictor P is then devised to predict the shift (i.e., semantic direction) according to such generated paired images, with a reconstruction loss. In this way, we clearly identify the inner disentangled semantics directions in our CL-Dis for further fine-controllable generations. 4.3. New Disentanglement Metric The conditional diffusion-based generation in our CL-Dis defines a clear mapping between the disentangled represen- tations and the generated image results, so we can quantita- tively measure the performance of disentanglement with the generation difference. Following this idea, we quantify this difference by using an optical flow tracking technique [76]: Given a well-disentangled representation za sem and its shift variant zaâ€² sem, we calculate the optical flow map F between their corresponding generated images. Then, we use a con- tinuously changed hyper-parameter as a dynamic threshold to distinguish these â€œchanged characteristicsâ€ caused by shifted factors from the rest â€œfrozen onesâ€. The average ra- tio of these two parts actually reflects the disentanglement performance, the smaller the better. Due to the space limi- tations, more operation details are in supplementary. 5. Experiment 5.1. Implementation Detail and Experiment Setting For the backbone of Diff-AE, we take pre-trained weights in [64] as the initial parameters with Adam optimizer and the learning rate of 1e-4. For the co-pilot Î²-VAE, we adopt the same Adam optimizer and learning rate. For the navigation branch, we refer to the GAN-based editing methods [12, 79] for a fine-controllable generation. Dif- ferently, our CL-Dis has already in advance got the well- disentangled latent zsem. The whole CL-Dis framework is trained on 2 NVIDIA V100 GPUs with a batch size of 32. In inference, the co-pilot Î²-VAE is discarded for efficiency. The dimension of zsem is set to 512 in our framework. To evaluate disentanglement, we conduct experiments on both generation & perception tasks, on synthetic & real data. Datasets & Baselines. For the image generation task, we conduct experiments on FFHQ [40], CelebA [39], LSUN Horse [96], LSUN Cars [96], Shape3D [42]. We compare our proposed CL-Dis with VAE-based, GAN-based, and Diffusion-based baselines, including Î²-VAE [30], Factor- VAE [42], Beta-TCVAE [7], GAN Edit v1 [79], GAN Edit v2 [12], InfoGAN-CR [54], Asyrp [48], GANspace (GS) [27], NaviNeRF [88], DisCo [66], Diff-AE [64] and vector- based DisDiff [94]. For the perception task, we just choose the classic face recognition for instantiation due to limited space and con- duct experiments on CASIA-Webface [95] and VGGFace2 [5] in comparison with the baseline of LFW [35]. Evaluation Metrics. To alleviate errors, we have multiple runs for each method. We use 4 representative metrics of Factor-VAE score [42], DCI [19], FID [29] and classifica- tion accuracy in face recognition, and our additionally pro- posed new metric for validation. Note that our CL-Dis be- longs to dimension-/scalar-based DRL methods, so we just discuss but do not directly compare with many vector-based ones [6, 16, 55, 77, 93] in the next experiment sections. 5.2. Comparisons with SOTA DRL Methods Table 1. Quantitative comparison re- sults on CelebA dataset. Methods FID VAE [45] 132.80 Î²-VAE [30] 136.23 Î²-TCVAE [7] 139.07 FactorVAE [42] 134.52 CL-Dis (Ours) 6.54 Table 2. Quan- titative comparison results on FFHQ dataset. Methods FID NaviNeRF [88] 13.00 CL-Dis (Ours) 12.15 Quantitative Results. Table 1 and Table 2 report the re- sults of Frechet Inception Distance (FID) [29] to measure the quality of the generated images. We see that our CL- Dis outperforms other typical disentangled methods in gen- eration quality, especially on the real-world face images of CelebA. Besides, the quantitative comparison results of disentan- glement under different metrics are shown in Table 3. As shown in the table, CL-Dis outperforms other competitors, demonstrating the methodâ€™s superior disentanglement abil- ity. Compared with the VAE-based methods, these methods suffer from the the trade-off between generation and dis- entanglement [4, 30], but CL-Dis does not have this prob- lem due to its automatic optimization mechanism. As for the GAN-based methods are typically limited by the latent space of GAN [94]. The diffusion-based DisDiff is a vector- Table 3. Comparisons of disentanglement on the FactorVAE score and DCI disentanglement metrics (mean Â± std, higher is better). We can see that our scalar-based CL-Dis even achieves compa- rable results with the co-current factor-based vector-wise DisD- iff [94]. Method Shapes3D FactorVAE score DCI VAE-based: FactorVAE [42] 0.840 Â± 0.066 0.611 Â± 0.082 Î²-TCVAE [7] 0.873 Â± 0.074 0.613 Â± 0.114 GAN-based: InfoGAN-CR [54] 0.587 Â± 0.058 0.478 Â± 0.055 Pre-trained GAN-based: GAN Edit v1 [79] 0.805 Â± 0.064 0.380 Â± 0.062 GANspace [27] 0.788 Â± 0.091 0.284 Â± 0.034 DisCo [66] 0.877 Â± 0.031 0.708 Â± 0.048 Diffusion-based: DisDiff [94] 0.902 Â± 0.043 0.723 Â± 0.013 CL-Dis (Ours) 0.952 Â± 0.017 0.731 Â± 0.045 Smile Eyes Earrings Teeth Figure 4. Fine-grained Disentanglement Results. The qualitative results present the results of attribute manipulation, which demon- strate the semantic manipulation on the FFHQ dataset. To clearly show the isolated and â€œdisentangledâ€ effects, we use red circles to mark the corresponding changed regions. wise method, in which representations {zc} could express more information than our scalar-based ones zsem, but our CL-Dis still outperforms DisDiff on Shape3D. And, our scalar-based disentangled representations are more light, generic, robust, and easy to apply to different scenarios. Qualitative Results. Unlike most previous papers that only test on the toy synthetic datasets [7, 30, 42], our CL-Dis can be naturally applied to real images. Figure 4 shows the effectiveness of our method on various datasets. To clearly show the isolated and â€œdisentangledâ€ effects, we use red cir- cles to mark the corresponding changed regions. We can ob- serve that CL-Dis can disentangle and change the isolated attributes continuously in an unsupervised manner, such as serious expression â†’ smile, closing eyes â†’ opening eyes, w/o earrings â†’ w/ earrings, etc. Moreover, to validate the superiority of CL-Dis in the disentanglement capability, Figure 5 also provides results for changing human facial attributes to different character- Ours GAN Edit v1  Asyrp GAN Edit v2 Mouth Ours GAN Edit v1 Asyrp GAN Edit v2 Eyes Ours GAN Edit v1  Asyrp GAN Edit v2 Hair Figure 5. Comparison of the â€œmouthâ€, â€œeyesâ€ and â€œhairâ€ attributes with different methods on the FFHQ dataset. Since different meth- ods pre-train on different datasets, we thus compare the disentan- glement with different images according to the previous methods [12, 48, 79]. istics along certain directions like â€œMouthâ€, â€œEyesâ€ and â€œhairâ€, in comparison with three baselines of GAN Edit v1 [79], GAN Edit v2 [12], and Asyrp [48]. We can see that other methods like Asyrp are prone to generate arti- facts (bottom row of Figure 5), and their editing results are not well-disentangled. For example, the whole face content has changed for the schemes of GAN Edit v1 [79] and GAN Edit v2 [12]). In contrast, our method consistently keeps the identity while just changing the disentangled properties, achieving an â€œisolated and disentangledâ€ effect. To demonstrate the generalization ability of our CL- Dis, we also use different datasets and different backbones (supplementary) for validation. As shown in Figure 6, even for cars, horses, and buildings, CL-Dis still obtains a group of fine-grained disentanglement results, where only the â€œcolorâ€ attribute changes across the traversal direction, proving the disentanglement generalization capability. Figure 6. Generalization evaluation on extensive datasets. We vi- sualize the results of shifting the latent zsem with our proposed method along the common â€œColorâ€ direction on LSUN Horse (upper row), LSUN Cars (middle row), and CelebA (lower row) datasets, respectively. 5.3. Ablation Study and More Analysis Effectiveness of Different Elements. To analyze the ef- fectiveness of the proposed parts in our closed-loop disen- tanglement framework, we perform an ablation to study the influence of Semantics Navigation, Knowledge Distillation Ldt, and Diffusion Feedback Lfd. We take FFHQ as the dataset to conduct these ablation studies. As shown in Figure 7, due to the baseline of Diff-AE it- self not having a disentanglement ability, when shifting its latent we get generation results with background artifacts. If we directly apply the semantics navigation [79] to the baseline latent, the generation quality has been improved, but the disentanglement is still not guaranteed. For exam- ple, when shifting along the semantic direction of â€œHairâ€ (The 1st row of Figure 7), global features such as face shape, skin texture, and hairstyle are variously entangled, resulting in a younger appearance for the man. The last two schemes of adding knowledge distillation and diffusion feedback have achieved better performance in disentangle- ment, particularly for the final closed-loop scheme. We can see that the final scheme together with Ldt & Lfd achieves a better disentanglement effect on the semantic direction of â€œHairâ€, compared to that only with Ldt which still changes face characteristics in the traversal generations. Analysis of Auto-Driven Feedback Mechanism. As in- troduced, the core contribution of this work lies in an auto- matically disentangled learning mechanism, achieved by a dynamic Cdyn whose value depends on the increasing infor- mation capacity in the reverse diffusion process. To prove that, we visualize the changing curves of Cdyn during the CL-Dis training. As illustrated in Figure 8(a), the value of Cdyn gradually increases with the diffusion feedback, so as to highlight the penalty for KL-divergence item in the co-pilot Î²-VAE optimization and thus improve disentangle- ment performance of the entire framework. 5.4. New Metric & Discrimination Task Validation New Metric Evaluation. As our work belongs to the track of unsupervised representation disentanglement, not image +Navi +Knowledge Distillation       +Navi +Knowledge Distillation +Diffusion Feedback       Hair Eyes Baseline (Diff-AE) + Navigation (Navi) â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ Mouth â€¦ â€¦ â€¦ â€¦ Figure 7. Ablation study on the FFHQ dataset. We evaluate the effectiveness of different components in our method by shifting different attributes, including â€œHairâ€, â€œMouthâ€ and â€œEyesâ€. We can see that as more elements are added, both the disentanglement performance and the generation quality improve: the first column exists artifacts; the second and third columns change other contents except shift attributes. Teeth Eyes 20 images 20 images Iteration ğ¶à¯—à¯¬à¯¡ 11.2 11.0 10.8 10.6 10.4 10.2 10.0 50 100 150 200 250 (b) Averaged heatmaps of the pixel differences (a) Visualization of the ğ¶à¯—à¯¬à¯¡ curve Figure 8. (a) Visualization of the curve of changeable Cdyn at different iteration steps. Cdyn gradually increases with the dif- fusion feedback to improve the latent disentangling. (b) Aver- aged heatmaps of the pixel differences between the original and the shifted images for the â€œeyes shiftâ€ (left) and the â€œteeth shiftâ€ (right), revealing a good disentanglement of ours. generation or editing. The core of this task is to fully un- derstand the latent factors of the model to enhance its fine- grained controllability. The qualitative results shown above have illustrated that traversing along the disentangled rep- resentations results in an â€œproperty isolatedâ€ generation ef- fect. To measure such effect quantitatively and show our method could progressively change the disentangled prop- erties while keeping others frozen, we propose a new metric (Sec. 4.3) through the lens of optical flow [76]. As illus- trated in Figure 9, the optical flow visualization between the original and shifted image pairs on FFHQ dataset [40], shows that our proposed CL-Dis method demonstrates ef- fective disentanglement, where shifting a single dimension of the latent space results in isolated property changes in the generated output. In contrast, the baseline approach (origi- nal Diff-AE) changes the entire background content of the image. Furthermore, we perform normalization on the optical flow map and utilize a dynamic threshold to classify pixels as either exceeding or not exceeding it, thereby obtaining the ratio. We make statistics based on the 100 sets, and cal- culate the average ratio using a static threshold of 0.5, and get the average value. The baseline quantitatively achieves 0.1606 in this new metric, while our method is only 0.0177 (the lower the better), indicating that our method has bet- ter disentanglement performance that only isolated property Ours Baseline Original Image Shifted Image Optical Flow Figure 9. Evaluation on the proposed new disentanglement metric with the â€˜hairâ€™, â€˜eyesâ€™, and â€˜smileâ€™ attributes. From the optical flow tracking visualization, we find traversing along our disentangled representations results in an â€œproperty isolatedâ€ generation effect, while the baseline exhibits a bad performance with a fully changed image content. changes when shifting across certain dimensions. Locality of visual effects. To illustrate that the navigation along the semantic directions results in an isolated and â€œdis- entangledâ€ effect, for a particular direction, we compute the per-pixel differences averaged over the generation results with 20 shift magnitudes from a uniform grid in a range. Figure 8(b) shows the averaged heatmap for the â€œEyesâ€ and â€œMouthâ€ directions. We can see that our method achieves a good disentanglement, where shifting â€œEyesâ€ just affects the eyes-related image content, and shifting â€œMouthâ€ just affects the mouth-related image content. Discrimination Task Validation. To validate whether the good disentanglement achieved by our CL-Dis could bene- fit the discrimination task, we conducted face recognition experiments on two large well-known datasets, CASIA- Webface [95] and VGGFace2 [5]. We use the semantics encoder Esem of CL-Dis as an off-the-shelf feature extrac- tor and introduce two additional learnable linear layers for fine-tuning. Following the same protocol used in [68], we perform a fair comparison in Table 4 with mean classifi- cation accuracy as a metric. We observe that the seman- tic encoder of our CL-Dis performs better than the baseline of learning from scratch (LFS) on the widely adopted test benchmark LFW [35], and even our Esem trained with only 5 epochs still outperforms the LFS model with 50 epochs, which demonstrates that the well-disentangled representa- tions learned by CL-Dis indeed benefit discrimination tasks. Table 4. Discrimination task (face recognition) validation. Our method with good disentangled representations achieves >2.0% gains even with fewer (5) training epochs compared to baseline. Dataset CASIA-Webface (%) VGGFace2 (%) Baseline (50 epochs) 89.45+-0.015 81.82+-0.019 Ours (5 epochs) 91.72+-0.011 87.32+-0.014 Ours (50 epochs) 94.00+-0.785 87.97+-0.012 6. Conclusion In this paper, we present CL-Dis, an unsupervised closed- loop disentanglement framework that achieves a strong DRL ability without relying on heavy supervision or heuris- tic loss constraints. CL-Dis embeds a closed-loop inter- connection, where a pre-trained Î²-VAE empowers a pre- liminary disentanglement ability to diffusion model with a distillation loss, while the increasing information capabil- ity in the diffusion is taken as feedback, in turn, to pro- gressively improve Î²-VAE disentangling, forming a core mutually-promoting mechanism. Besides, we also design a self-supervised strategy to navigate semantic directions in the well-disentangled latent for a controllable generation. A new metric is further introduced to measure the disentan- glement effect. Experimental results indicate that CL-Dis achieves a state-of-the-art DRL performance, and its strong disentangled ability benefits both generation and discrimi- nation tasks, especially on real-world images. Supplementary Material A. Understanding the effects of our CL-Dis Constrained optimization is important for enabling deep un- supervised models to learn disentangled representations of the independent data generative factors [30]: (1) In the Î²-VAE framework, this corresponds to tuning the Î² coefficient in Eq. 1 as follows, L(Î¸, Ï•, x, z, Î²) =EqÏ•(z|x) \u0002 log pÎ¸(x|z) \u0003 âˆ’ Î²DKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 , (9) Î² can be taken as a mixing coefficient for balancing the magnitudes of gradients from the reconstruction and the prior-matching components of the VAE lower bound for- mulation during training. When Î² is too low or too high the model learns an entangled latent representation due to either too much or too little capacity in the latent bottleneck [30]. (2) In the understanding Î²-VAE version with a controlled capacity increase strategy [4], this constrained optimization corresponds to tuning the C coefficient in Eq 2 as follows, L(Î¸, Ï•, C; x, z) =EqÏ•(z|x) log pÎ¸(x|z)âˆ’ Î² \f\fDKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 âˆ’ C \f\f, (10) where the model is trained to reconstruct the images with samples from the factor distribution, but with a range of different target encoding capacities by pressuring the KL divergence to be at a controllable value, C. The training ob- jective combined maximizing the log-likelihood and mini- mizing the absolute deviation from C by linearly increasing it from a low value to a high value over the course of train- ing. The intuitive picture behind C is that it gradually adds more latent encoding capacity, enabling progressively more factors of variation to be represented while retaining disen- tangling in previously learned factors. (3) In the proposed closed-loop unsupervised represen- tation disentanglement framework CL-Dis, this constrained optimization is totally automatic and dynamic-adaptive. This is achieved under the mutual driving of reverse diffu- sion of Diff-AE and VAE latent evolution. We make the constant C in [4] become changeable Cdyn, controlling its value dynamically according to the information capac- ity variation in the reverse diffusion process as follows, Lfd(Î¸, Ï•, Cdyn; x, z) = EqÏ•(z|x) log pÎ¸(x|z)âˆ’ Î² \f\fDKL \u0000qÏ•(z|x)âˆ¥pÎ¸(z) \u0001 âˆ’ Cdyn \f\f, (11) Cdyn âˆ I(x0; xt) = X X p(x0, xt) Â· log \u0012 p(x0, xt) p(x0) Â· p(xt) \u0013 (12) s.t. pÎ¸â€²(xtâˆ’1|xt) = N(xt; ÂµÎ¸â€²(xt, t), ÏƒtI), (13) where the Cdyn is proportional to the information capabil- ity of the reverse process in diffusion (here we use the mu- tual information I(x0; xt) to represent its value), which is parameterized by Î¸â€² with a U-Net architecture and is a pro- gressively increasing process of information capacity (i.e., entropy decreasing). Continuing this intuitive picture, we can imagine that with the capacity of the information capa- bility gradually increasing in the reverse diffusion, the dy- namic Cdyn would continue to utilize those extra indicators for an increasingly precise encoding, where a larger im- provement can be obtained by disentangling different fac- tors of variations in the latent. B. Implementation Details of the New Metric To measure the disentanglement effect quantitatively, espe- cially under the unsupervised setting where no factor anno- tations are accessible, we propose a new label-free metric with the help of optical flow [76]. The high-level idea is that we leverage optical flow to track the obviously and con- tinually changed content to evaluate whether or not these changes are â€œproperty isolatedâ€. First, we calculate the op- tical flow between the original image and the shifted image. Then, we normalize the values of the optical flow map. Af- ter that, we apply a threshold to the optical flow. Points in the flow exceeding this threshold are considered to have a large shift magnitude (i.e., those disentangled attribute points), while those below it are considered to have a small shift magnitude (i.e., those rest frozen attribute points). We calculate the ratio of the number of flow points exceeding this threshold to those below it and use this ratio value as our new metric score. The lower the better, and which ex- actly corresponds to the definition of disentangled represen- tations where a change in a single unit of the representation corresponds to a change in a single factor of variation of the data while being invariant to others. The specific algorithm is shown in Algorithm 1 and Algorithm 2. Empirically and experimentally, we set the threshold as 0.5 for the calcula- tion of the proposed new metric. Algorithm 1 Compute New Metric Based on Optical Flow 1: function COMPUTENEWMETRIC(image, shifted image, threshold) 2: flow â† OPTICALFLOW(image, shifted image) 3: normalized flow â† NORM(flow) 4: large shift count L â† 0 5: small shift count S â† 0 6: for all flow point in normalized flow do 7: if MAGNITUDE(flow point) > threshold then 8: L â† L + 1 9: else 10: S â† S + 1 11: end if 12: end for 13: metric score â† L/S 14: return metric score 15: end function Algorithm 2 Optical Flow Normalization 1: function NORM(flow) 2: max norm â† MAX( pP flow2) 3: normalized flow â† flow/max norm 4: return normalized flow 5: end function Table 5. Influence study with different values of Cbase and Cmax in our CL-Dis framework on the Cars3D dataset. We evaluate the disentanglement performance of different schemes using the met- rics of FactorVAE score and DCI (mean Â± std, higher is better). Cbase Cmax FactorVAE score DCI 2 5 0.689 Â± 0.038 0.094 Â± 0.009 5 15 0.745 Â± 0.042 0.175 Â± 0.012 10 11 0.762 Â± 0.068 0.179 Â± 0.014 10 12 0.781 Â± 0.035 0.182 Â± 0.008 10 25 0.794 Â± 0.040 0.186 Â± 0.007 20 50 0.734 Â± 0.024 0.163 Â± 0.015 C. More Experimental Results C.1. Influence of Cbase and Cmax in Our CL-Dis As we discussed in the main paper, we propose a novel dif- fusion feedback-driven loss Lfd to further facilitate the dis- entanglement of the representations. Formally, we make the original C in Î²-VAE auto-changeable, and control its value dynamically according to the information capacity of the reverse diffusion process as follows: 14-0-1 18-1-1 21-7-1 5-0- Figure 10. Generalization evaluation on extensive datasets. We vi- sualize the results of shifting the disentangled latent zsem with our proposed method. From top to bottom, we illustrate the â€œproperty isolatedâ€ effects along glasses, mouth, hair, car color, hair color, and face illumination, respectively. Table 6. Comparisons of disentanglement on the FactorVAE score and DCI disentanglement metrics (mean Â± std, higher is better). We can see that our scalar-based CL-Dis even achieves compa- rable results with the co-current factor-based vector-wise DisD- iff [94]. Method Shapes3D FactorVAE score DCI VAE-based: FactorVAE [42] 0.840 Â± 0.066 0.611 Â± 0.082 Î²-TCVAE [7] 0.873 Â± 0.074 0.613 Â± 0.114 GAN-based: InfoGAN-CR [54] 0.587 Â± 0.058 0.478 Â± 0.055 Pre-trained GAN-based: GAN Edit v1 [79] 0.805 Â± 0.064 0.380 Â± 0.062 GANspace [27] 0.788 Â± 0.091 0.284 Â± 0.034 DisCo [66] 0.877 Â± 0.031 0.708 Â± 0.048 Diffusion-based: DisDiff [94] 0.902 Â± 0.043 0.723 Â± 0.013 CL-Dis (Ours) 0.952 Â± 0.017 0.731 Â± 0.045 Cdyn = f(Ext) = ( Cbase Â· ( Ex0 Ext ), if 0 < f(Ext) < Cmax Cmax, if f(Ext) â‰¥ Cmax (14) Ex0 = âˆ’ X px0 âˆˆR px0logpx0, Ext = âˆ’ X pxt âˆˆR pxtlogpxt, (15) where Cbase, Cmax are the original empirical start-point and the maximum upper-bound according to [4], and their influences are evaluated as follows. The results depicted in Table 5 demonstrate that our method is robust enough to achieve a stable satisfactory performance with different values of Cbase, Cmax. Besides, these experimental re- sults reveal that setting Cbase to 10 consistently yields opti- mal disentanglement outcomes. Moreover, our assessment also extends to examining the impact of the upper-bound value, Cmax. As shown in Table 5, reducing the gap be- tween Cmax and Cbase hampers the overall disentangle- ment performance (0.794 vs. 0.762 in FactorVAE score). We analyze that, the variable scope of the dynamic Cdyn will be limited due to the relatively small range between Cmax and Cbase. This limitation may negatively impact the control of KL-divergence, consequently hampering the overall modelâ€™s ability to effectively disentangle the inner latent space more. C.2. More Subjective Results In this section, we further provide more visualization results to demonstrate the generalization ability of our CL-Dis on different datasets, including FFHQ [40], LSUN Cars [96] and CelebA [39]. As shown in Figure 10, our CL-Dis could generate fine-grained disentangled results along the â€œglasses, mouth, hair, car color, hair color, and face illu- minationâ€ attribute changes across the traversal direction, which fully illustrates the well-generalized disentanglement capability of our method. C.3. Dimension Ablation on Shape3D We further conduct Dimension Ablation on Shapes3D dataset in Table 7. We can see below that reducing the di- mension of zsem leads to minor changes, which indicates our method is robust enough. Dimension FactorVAE score DCI CL-Dis (512) 0.840 Â± 0.066 0.611 Â± 0.082 CL-Dis (100) 0.842 Â± 0.042 0.547 Â± 0.051 CL-Dis (32) 0.837 Â± 0.053 0.582 Â± 0.049 Table 7. Ablation study for the dimension of zsem on the Shapes3D dataset. D. Limitation Discussions In Table 6 (corresponds to Table 2 in the main manuscript), we observe our method encounters an unsatisfactory performance on the MPI3D dataset, which indicates that our method may be distracted by the real-world natural environment scenarios. We discuss the potential reason that the collected data in MPI3D [23], focus on bridging the gap between simulation and reality, which distinguishes it apart from various other synthetic toy datasets [42, 65]. In detail, real-world recordings in MPI3D introduce more extra environment interruptions, such as chromatic aberrations in cameras and complex surface properties of objects (e.g., reflectance, radiance, and irradiance). These interruptions pose challenges in learning disen- tangled representations due to the mixing of factors and distortions during the real-world recordings, especially for scalar-based methods like ours [23]. This unsolved limitation of our method will be left as future research work. References [1] Sungsoo Ahn, Shell Xu Hu, Andreas Damianou, Neil D Lawrence, and Zhenwen Dai. Variational information dis- tillation for knowledge transfer. In CVPR, pages 9163â€“9171, 2019. 3 [2] Zhipeng Bao, Martial Hebert, and Yu-Xiong Wang. Genera- tive modeling for multi-task visual learning. In ICML, pages 1537â€“1554. PMLR, 2022. 1 [3] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Repre- sentation learning: A review and new perspectives. TPAMI, 35(8):1798â€“1828, 2013. 1, 2 [4] Christopher P Burgess, Irina Higgins, Arka Pal, Loic Matthey, Nick Watters, Guillaume Desjardins, and Alexan- der Lerchner. Understanding disentangling in Î²-vae. NeurIPS Workshop on Learning Disentangled Representa- tions, 2017. 1, 2, 3, 4, 5, 6, 10, 11 [5] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and An- drew Zisserman. VGGFace2: A dataset for recognising faces across pose and age. In International Conference on Auto- matic Face and Gesture Recognition, 2018. 6, 8 [6] Hong Chen, Yipeng Zhang, Xin Wang, Xuguang Duan, Yuwei Zhou, and Wenwu Zhu. Disenbooth: Disentangled parameter-efficient tuning for subject-driven text-to-image generation. arXiv preprint arXiv:2305.03374, 2023. 6 [7] Ricky TQ Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of disentanglement in varia- tional autoencoders. NeurIPS, 31, 2018. 1, 2, 3, 6, 11 [8] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable rep- resentation learning by information maximizing generative adversarial nets. NeurIPS, 29, 2016. 1, 2 [9] Zikun Chen, Ruowei Jiang, Brendan Duke, Han Zhao, and Parham Aarabi. Exploring gradient-based multi-directional controls in gans. In ECCV, pages 104â€“119. Springer, 2022. 3 [10] Pengyu Cheng, Martin Renqiang Min, Dinghan Shen, Christopher Malon, Yizhe Zhang, Yitong Li, and Lawrence Carin. Improving disentangled text representation learn- ing with information-theoretic guidance. arXiv preprint arXiv:2006.00693, 2020. 2 [11] Yu Cheng, Duo Wang, Pan Zhou, and Tao Zhang. A sur- vey of model compression and acceleration for deep neural networks. arXiv preprint arXiv:1710.09282, 2017. 3 [12] Anton Cherepkov, Andrey Voynov, and Artem Babenko. Navigating the gan parameter space for semantic image edit- ing. In CVPR, pages 3671â€“3680, 2021. 3, 5, 6, 7 [13] Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe rlhf: Safe reinforcement learning from human feedback. arXiv preprint arXiv:2310.12773, 2023. 3 [14] Sanjoy Dasgupta and Sivan Sabato. Robust learning from discriminative feature feedback. In International Confer- ence on Artificial Intelligence and Statistics, pages 973â€“982. PMLR, 2020. 3 [15] Sanjoy Dasgupta, Akansha Dey, Nicholas Roberts, and Sivan Sabato. Learning from discriminative feature feedback. Ad- vances in Neural Information Processing Systems, 31, 2018. 3 [16] Emily L Denton et al. Unsupervised learning of disentangled representations from video. NeurIPS, 30, 2017. 6 [17] Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm- e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. 3 [18] Emilien Dupont. Learning disentangled joint continuous and discrete representations. NeurIPS, 31, 2018. 1 [19] Cian Eastwood and Christopher KI Williams. A framework for the quantitative evaluation of disentangled representa- tions. In International conference on learning representa- tions, 2018. 6 [20] Kieran Egan. Memory, imagination, and learning: Con- nected by the story. Phi Delta Kappan, 70(6):455â€“459, 1989. 1 [21] Roland W Fleming. Visual perception of materials and their properties. Vision research, 94:62â€“75, 2014. 1 [22] Lore Goetschalckx, Alex Andonian, Aude Oliva, and Phillip Isola. Ganalyze: Toward visual definitions of cognitive im- age properties. In ICCV, pages 5744â€“5753, 2019. 3 [23] Muhammad Waleed Gondal, Manuel Wuthrich, Djordje Miladinovic, Francesco Locatello, Martin Breidt, Valentin Volchkov, Joel Akpo, Olivier Bachem, Bernhard SchÂ¨olkopf, and Stefan Bauer. On the transfer of inductive bias from sim- ulation to the real world: a new disentanglement dataset. Ad- vances in Neural Information Processing Systems, 32, 2019. 12 [24] Abel Gonzalez-Garcia, Joost Van De Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain dis- entanglement. NeurIPS, 31, 2018. 2 [25] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. NeurIPS, 27, 2014. 1 [26] Jianping Gou, Baosheng Yu, Stephen J Maybank, and Dacheng Tao. Knowledge distillation: A survey. IJCV, 129: 1789â€“1819, 2021. 3 [27] Erik HÂ¨arkÂ¨onen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. Ganspace: Discovering interpretable gan con- trols. NeurIPS, 33:9841â€“9850, 2020. 6, 11 [28] Ruidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. An unsupervised neural attention model for as- pect extraction. In ACL, pages 388â€“397, 2017. 2 [29] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. Advances in neural information processing systems, 30, 2017. 6 [30] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual con- cepts with a constrained variational framework. In ICLR, 2016. 1, 2, 3, 4, 6, 10 [31] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and Alexander Lerchner. Towards a definition of disentangled representa- tions. arXiv preprint arXiv:1812.02230, 2018. 1 [32] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2 [33] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu- sion probabilistic models. NeurIPS, 33:6840â€“6851, 2020. 2, 3 [34] Wei-Ning Hsu and James Glass. Disentangling by partition- ing: A representation learning framework for multimodal sensory data. arXiv preprint arXiv:1805.11264, 2018. 2 [35] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on faces inâ€™Real-Lifeâ€™Images: detection, align- ment, and recognition, 2008. 6, 8 [36] Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, and Gun- hee Kim. Ib-gan: Disentangled representation learning with information bottleneck generative adversarial networks. In AAAI, pages 7926â€“7934, 2021. 1 [37] Jiaming Ji, Tianyi Qiu, Boyuan Chen, Borong Zhang, Hantao Lou, Kaile Wang, Yawen Duan, Zhonghao He, Jiayi Zhou, Zhaowei Zhang, et al. Ai alignment: A comprehensive sur- vey. arXiv preprint arXiv:2310.19852, 2023. 3 [38] Mingi Ji, Byeongho Heo, and Sungrae Park. Show, attend and distill: Knowledge distillation via attention-based fea- ture matching. In AAAI, pages 7945â€“7952, 2021. 3 [39] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. arXiv preprint arXiv:1710.10196, 2017. 4, 6, 12 [40] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vi- sion and pattern recognition, pages 4401â€“4410, 2019. 4, 6, 8, 12 [41] Anurag Katakkar, Clay H Yoo, Weiqin Wang, Zachary C Lipton, and Divyansh Kaushik. Practical benefits of fea- ture feedback under distribution shift. arXiv preprint arXiv:2110.07566, 2021. 3 [42] Hyunjik Kim and Andriy Mnih. Disentangling by factoris- ing. In ICML, pages 2649â€“2658. PMLR, 2018. 1, 2, 6, 11, 12 [43] Minyoung Kim, Yuting Wang, Pritish Sahu, and Vladimir Pavlovic. Relevance factor vae: Learning and identify- ing disentangled factors. arXiv preprint arXiv:1902.01568, 2019. 1, 2 [44] Yeongmin Kim, Dongjun Kim, HyeonMin Lee, and Il-chul Moon. Unsupervised controllable generation with score- based diffusion models: Disentangled latent code guidance. In NeurIPS 2022 Workshop on Score-Based Methods, 2022. 2 [45] Diederik P Kingma and Max Welling. Auto-encoding varia- tional bayes. arXiv preprint arXiv:1312.6114, 2013. 1, 2, 4, 6 [46] W Bradley Knox and Peter Stone. Augmenting reinforce- ment learning with human feedback. In ICML 2011 Work- shop on New Developments in Imitation Learning (July 2011), page 3, 2011. 3 [47] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakr- ishnan. Variational inference of disentangled latent concepts from unlabeled observations. ICLR, 2018. 1, 2, 3 [48] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. arXiv preprint arXiv:2210.10960, 2022. 1, 2, 3, 6, 7 [49] Brenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253, 2017. 1 [50] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, pages 35â€“51, 2018. 2 [51] Ping-Shien Lee and David K Sewell. A revised diffusion model for conflict tasks. Psychonomic Bulletin & Review, pages 1â€“31, 2023. 3 [52] Wonkwang Lee, Donggyun Kim, Seunghoon Hong, and Honglak Lee. High-fidelity synthesis with disentangled rep- resentation. In ECCV, pages 157â€“174. Springer, 2020. 2 [53] Minjun Li, Yanghua Jin, and Huachun Zhu. Surrogate gra- dient field for latent space manipulation. In CVPR, pages 6529â€“6538, 2021. 3 [54] Zinan Lin, Kiran Koshy Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr: Disentangling generative adver- sarial networks with contrastive regularizers. arXiv preprint arXiv:1906.06034, page 60, 2019. 1, 2, 6, 11 [55] Liu Liu, Jiangtong Li, Li Niu, Ruicong Xu, and Liqing Zhang. Activity image-to-video retrieval by disentangling appearance and motion. In AAAI, pages 2145â€“2153, 2021. 6 [56] Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang, and Marco De Nadai. Smoothing the disentangled latent style space for unsupervised image-to-image translation. In CVPR, pages 10785â€“10794, 2021. 2 [57] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard SchÂ¨olkopf, and Olivier Bachem. Challenging common assumptions in the unsuper- vised learning of disentangled representations. In interna- tional conference on machine learning, pages 4114â€“4124. PMLR, 2019. 2 [58] Zhipeng Luo, Yazhou He, Yanbing Xue, Hongjun Wang, Mi- los Hauskrecht, and Tianrui Li. Hierarchical active learning with qualitative feedback on regions. IEEE Transactions on Human-Machine Systems, 2023. 3 [59] Joanna MaterzyÂ´nska, Antonio Torralba, and David Bau. Dis- entangling visual and written concepts in clip. In CVPR, pages 16410â€“16419, 2022. 2 [60] Joel Pearson. The human imagination: the cognitive neu- roscience of visual mental imagery. Nature reviews neuro- science, 20(10):624â€“634, 2019. 1 [61] Massimiliano Pierobon and Ian F Akyildiz. Capacity of a diffusion-based molecular communication system with channel memory and molecular noise. IEEE Transactions on Information Theory, 59(2):942â€“954, 2012. 3 [62] Antoine Plumerault, HervÂ´e Le Borgne, and CÂ´eline Hude- lot. Controlling generative models with continuous factors of variations. ICLR, 2020. 3 [63] Stefanos Poulis and Sanjoy Dasgupta. Learning with feature feedback: from theory to practice. In Artificial Intelligence and Statistics, pages 1104â€“1113. PMLR, 2017. 3 [64] Konpat Preechakul, Nattanat Chatthee, Suttisak Wizad- wongsa, and Supasorn Suwajanakorn. Diffusion autoen- coders: Toward a meaningful and decodable representation. In CVPR, pages 10619â€“10629, 2022. 2, 3, 4, 5, 6 [65] Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. Advances in neural informa- tion processing systems, 28, 2015. 12 [66] Xuanchi Ren, Tao Yang, Yuwang Wang, and Wenjun Zeng. Learning disentangled representation by exploiting pretrained generative models: A contrastive learning view. ICLR, 2021. 2, 6, 11 [67] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. Habitat: A plat- form for embodied ai research. In ICCV, pages 9339â€“9347, 2019. 3 [68] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clus- tering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815â€“823, 2015. 8 [69] Huajie Shao, Shuochao Yao, Dachun Sun, Aston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang, and Tarek Ab- delzaher. Controlvae: Controllable variational autoencoder. In International Conference on Machine Learning, pages 8655â€“8664. PMLR, 2020. 1, 3 [70] Yujun Shen and Bolei Zhou. Closed-form factorization of latent semantics in gans. In CVPR, pages 1532â€“1540, 2021. 3 [71] Yujun Shen, Jinjin Gu, Xiaoou Tang, and Bolei Zhou. Inter- preting the latent space of gans for semantic face editing. In CVPR, pages 9243â€“9252, 2020. 3 [72] Yichun Shi, Xiao Yang, Yangyue Wan, and Xiaohui Shen. Semanticstylegan: Learning compositional generative priors for controllable image synthesis and editing. In CVPR, pages 11254â€“11264, 2022. 3 [73] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing diffusion implicit models. In ICLR, 2020. 2 [74] Yue Song, Andy Keller, Nicu Sebe, and Max Welling. Latent traversals in generative models as potential flows. ICML, 2023. 3 [75] Yue Song, Jichao Zhang, Nicu Sebe, and Wei Wang. House- holder projector for unsupervised latent semantics discovery. In ICCV, pages 7712â€“7722, 2023. 3 [76] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow. In ECCV, pages 402â€“419. Springer, 2020. 5, 8, 10 [77] Luan Tran, Xi Yin, and Xiaoming Liu. Disentangled repre- sentation learning gan for pose-invariant face recognition. In CVPR, pages 1415â€“1424, 2017. 1, 6 [78] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, Louis- Philippe Morency, and Ruslan Salakhutdinov. Learning fac- torized multimodal representations. ICLR, 2018. 2 [79] Andrey Voynov and Artem Babenko. Unsupervised discov- ery of interpretable directions in the gan latent space. In ICML, pages 9786â€“9796. PMLR, 2020. 3, 5, 6, 7, 11 [80] Hui Wang, Hanbin Zhao, Xi Li, and Xu Tan. Progressive blockwise knowledge distillation for neural network acceler- ation. In IJCAI, pages 2769â€“2775, 2018. 3 [81] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity gan inversion for image attribute editing. In CVPR, pages 11379â€“11388, 2022. 1 [82] Xin Wang, Hong Chen, Siâ€™ao Tang, Zihao Wu, and Wenwu Zhu. Disentangled representation learning. arXiv preprint arXiv:2211.11695, 2022. 1 [83] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In ICCV, pages 7677â€“7689, 2023. 2 [84] Jiawei Wu, Xiaoya Li, Xiang Ao, Yuxian Meng, Fei Wu, and Jiwei Li. Improving robustness and generality of nlp models using disentangled representations. arXiv preprint arXiv:2009.09587, 2020. 2 [85] Qiucheng Wu, Yujian Liu, Handong Zhao, Ajinkya Kale, Trung Bui, Tong Yu, Zhe Lin, Yang Zhang, and Shiyu Chang. Uncovering the disentanglement capability in text- to-image diffusion models. In CVPR, pages 1900â€“1910, 2023. 1, 3 [86] Yankun Wu, Yuta Nakashima, and Noa Garcia. Not only generative art: Stable diffusion for content-style disentan- glement in art analysis. In Proceedings of the 2023 ACM In- ternational Conference on Multimedia Retrieval, pages 199â€“ 208, 2023. 1 [87] Zongze Wu, Dani Lischinski, and Eli Shechtman. Stylespace analysis: Disentangled controls for stylegan image genera- tion. In CVPR, pages 12863â€“12872, 2021. 2 [88] Baao Xie, Bohan Li, Zequn Zhang, Junting Dong, Xin Jin, Jingyu Yang, and Wenjun Zeng. Navinerf: Nerf-based 3d representation disentanglement by latent semantic naviga- tion. arXiv preprint arXiv:2304.11342, 2023. 6 [89] Xingqian Xu, Zhangyang Wang, Gong Zhang, Kai Wang, and Humphrey Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In ICCV, pages 7754â€“ 7765, 2023. 1 [90] Zipeng Xu, Tianwei Lin, Hao Tang, Fu Li, Dongliang He, Nicu Sebe, Radu Timofte, Luc Van Gool, and Errui Ding. Predict, prevent, and evaluate: Disentangled text-driven im- age manipulation empowered by pre-trained vision-language model. In CVPR, pages 18229â€“18238, 2022. 2 [91] Xinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation from visual attributes. In ECCV, pages 776â€“791. Springer, 2016. 1 [92] Tao Yang, Xuanchi Ren, Yuwang Wang, Wenjun Zeng, and Nanning Zheng. Towards building a group-based unsu- pervised representation disentanglement framework. ICLR, 2021. 2 [93] Tao Yang, Yuwang Wang, Cuiling Lan, Yan Lu, and Nan- ning Zheng. Vector-based representation is the key: A study on disentanglement and compositional generalization. arXiv preprint arXiv:2305.18063, 2023. 6 [94] Tao Yang, Yuwang Wang, Yan Lv, and Nanning Zh. Disd- iff: Unsupervised disentanglement of diffusion probabilistic models. arXiv preprint arXiv:2301.13721, 2023. 1, 2, 4, 6, 11 [95] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learn- ing face representation from scratch. arXiv preprint arXiv:1411.7923, 2014. 6, 8 [96] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. 4, 6, 12 [97] Yuhao Zhang, Ying Zhang, Wenya Guo, Xiangrui Cai, and Xiaojie Yuan. Learning disentangled representation for mul- timodal cross-domain sentiment analysis. TNNLS, 2022. 2 [98] Xinqi Zhu, Chang Xu, and Dacheng Tao. Where and what? examining interpretable disentangled representations. In CVPR, pages 5861â€“5870, 2021. 1, 2 "
}