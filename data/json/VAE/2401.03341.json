{
    "optim": "1 Weakly Augmented Variational Autoencoder in Time Series Anomaly Detection Zhangkai Wu, Longbing Cao, Senior Member, IEEE, Qi Zhang, Junxian Zhou, Hui Chen Abstractâ€”Due to their unsupervised training and uncertainty estimation, deep Variational Autoencoders (VAEs) have become powerful tools for reconstruction-based Time Series Anomaly Detection (TSAD). Existing VAE-based TSAD methods, either statistical or deep, tune meta-priors to estimate the likelihood probability for effectively capturing spatiotemporal dependencies in the data. However, these methods confront the challenge of inherent data scarcity, which is often the case in anomaly detec- tion tasks. Such scarcity easily leads to latent holes, discontinuous regions in latent space, resulting in non-robust reconstructions on these discontinuous spaces. We propose a novel generative framework that combines VAEs with self-supervised learning (SSL) to address this issue. Our framework augments latent representation to mitigate the disruptions caused by anomalies in the low-dimensional space, aiming to lead to corresponding more robust reconstructions for detection. This framework marks a significant advancement in VAE design by integrating SSL to refine likelihood enhancement. Our proposed VAE model, specifically tailored for TSAD, augments la- tent representations via enhanced training, increasing robustness to normal data likelihoods and improving sensitivity to anomalies. Additionally, we present a practical implementation of this conceptual framework called the Weakly Augmented Variational Autoencoder (WAVAE), which directly augments input data to enrich the latent representation. This approach synchronizes the training of both augmented and raw models and aligns their convergence in data likelihood optimization space. To achieve this, we maximize mutual information within the Evidence Lower Bound (ELBO), utilizing contrastive learning for shallow learning and a discriminator-based adversarial strategy for deep learning. Extensive empirical experiments on five public synthetic and real datasets validate the efficacy of our framework. These exper- iments provide compelling evidence of the superior performance of our approach in TSAD, as demonstrated by achieving higher ROC-AUC and PR-AUC scores compared to state-of-the-art models. Furthermore, we delve into the nuances of VAE model design and time series preprocessing, offering comprehensive ablation studies to examine the sensitivity of various modules and hyperparameters in deep optimization. Index Termsâ€”Variational Autoencoder, Time Series Anomaly Detection, Self Supervised Learning, Data Augmentation, Con- trast Learning, Adversarial Learning. I. INTRODUCTION The work is partially sponsored by Australian Research Council Discovery and Future Fellowship grants (DP190101079 and FT190100734). Zhangkai Wu, Junxian Zhou with the School of Computer Science, the Uni- versity of Technology Sydney, 15 Broadway, Ultimo 2007, NSW, Australia. (E-mail: berenwu1938@gmail.com, junxian.zhou@student.uts.edu.au) Hui Chen and Longbing Cao are with the DataX Research Centre and School of Computing, Macquarie University, NSW 2109, Australia. (email: hui.chen2@students.mq.edu.au, longbing.cao@mq.edu.au) Qi Zhang is with the Department of Computer Science, Tongji University, Shanghai 201804, China. (E-mail: zhangqi cs@tongji.edu.cn) D EEP probabilistic generative models have revolutionized unsupervised data generation by leveraging the neural networkâ€™s universal approximation theorem. This innovation manifests in various forms, such as encoding-decoding mecha- nisms [1], diffusion-denoise processes [2], and sender-receiver in compression [3]. Notably, VAEs have emerged as a central focus in this domain. Deep VAEs empowered by large-scale neural networks [4] and the organization of semantic repre- sentations [5], [6] have demonstrated exceptional capabilities in reconstructing and generating multimodal data, including images [7], [8], tabular [9], [10], and time series [11], [12]. Owing to their ability to learn generative factors in contin- uous and smooth low-dimensional space while fitting data likelihoods, VAEs exhibit robust representation learning capa- bilities in disentanglement [13], [14], classification [15], and clustering [16]. In particular, VAEs are designed to estimate likelihood distribution learned from most normal samples as a detector, providing an unsupervised and interpretable paradigm for anomaly detection. The underlying assumption is that unknown anomaly patterns typically exhibit statistical charac- teristics that deviate significantly from the normal distribution. Recent research has shown a growing preference for VAEs in Time Series Anomaly Detection (TSAD), particularly those that integrate meta-priors [17] into their design. These meth- ods have been validated to be effective and crucial in capturing the spatiotemporal dependencies within data, thereby enhanc- ing data likelihood modeling. Specifically, these models often assume certain meta-priors, e.g., latent structures, which are crafted using deep learning or probabilistic tools. The goal is to accurately represent the likelihood of most data points, en- abling the models to detect anomalies effectively. For instance, time-varying priors that adapt to dynamic assumptions [18]â€“ [21] have demonstrated effectiveness and powerful capabilities in capturing sequence data likelihoods. Additionally, other studies [22], [23] have proposed the creation of task-specific priors based on a factorized assumption explicitly designed to model contextual dependence structure in latent space. Various strategies have been employed to achieve this, such as using prototype distribution-based representations optimized through meta-learning and decomposing contextual representations. Whether meta-priors are integrated implicitly or explicitly, these methods are fundamentally rooted in model-based de- signs. However, they often cater to specific scenarios and face training challenges stemming from the data scarcity issue in deep learning or statistical estimation techniques. Addition- ally, these methods overlook the effective utilization of data, which becomes particularly problematic when modeling small sequence datasets in real-world time-series TSAD scenarios. arXiv:2401.03341v1  [cs.LG]  7 Jan 2024 2 The issue in VAE-based models easily leads to latent holes [24], i.e., discontinuous regions in latent space, resulting in non-robust reconstruction [21], [25]â€“[28]. The latent holes issue occurs when the encoder in these models maps unknown anomalies into the latent space without being adequately corrected by enough normal data. Since these anomalies lack the spatiotemporal properties of normal time series data, they disrupt the formation of a continuous and smooth latent space for normal samples. Consequently, representations sampled from these latent holes fail to accurately reconstruct the input samples, causing a discrepancy between the representations and the reconstructed data. This mismatch significantly impairs the anomaly detectorâ€™s performance and compromises the modelâ€™s overall robustness. For a more intuitive understanding of this phenomenon, refer to Fig. 1. Raw Input Encode Latent Hole Decode Data Likelihood Samples Augmented Input Robust Latent Space Data Likelihood Fig. 1. Comparison of the latent hole phenomenon induced by anomalies in Nonrobust VAE-based TSAD Models (upper section) with the robust representation learning space fostered by WAVAE (lower section). The upper part of the figure delineates the rise of the latent hole within the Nonrobust TSAD model and its effect on model robustness. Specifically, anomalous sequences xr t (depicted within the blue window in the upper section), when encoded into the representation space, disrupt the structural integrity of the latent space. This disruption results in latent hole primarily because these anomalous sequences xr t lack the spatiotemporal coherence inherent in the normal sequence xr 1. Consequently, sampling from these discontinuous regions leads to a mismatch between the representation (indicated by the blue dot zt) and its generation (also shown by the blue dot in the likelihood func- tion), as illustrated in the upper section, a disproportionately high likelihood function mass characterizes the representation in the latent space. In such scenarios, the TSAD model may erroneously classify an anomaly as normal, compromising its robustness. In contrast, the lower section demonstrates how data augmentation via the WAVAE model can engender a more continuous and smoothly distributed data likelihood (as depicted in the central part of the bottom figure). In this context, representation zt) encoded by anomalous sequences xa t sampled from regions outside the normal latent space are associated with a lower likelihood function mass, thereby enhancing the robustness and efficacy of anomaly detection in TSAD tasks. In light of these challenges, we propose to improve data utilization using self-supervised learning (SSL) to enhance representation learning and induce latent space robustness. SSL [29] enables models to extract more informative repre- sentations from unlabeled data, leading to sufficient training. To achieve this, we employ data augmentation on unlabeled datasets through SSL strategies, facilitating the training of models through contrastive or adversarial methods for the TSAD task. Our contributions can be summarized as: â€¢ Generative self-supervised learning framework for TSAD: We present an enhanced generative framework using self-supervised learning. We define a likelihood function for learning and the derivation of a surrogate error for optimization. This novel approach sets the stage for more effective model design in VAE-based TSAD. â€¢ Deep and shallow learning in augmented models: Building upon this framework, we implement the Weakly Augmented Variational Autoencoder (WAVA) that in- corporates data augmentation, enabling the model to undergo thorough training with support from augmented counterparts. We have also devised two distinct learning approaches, deep and shallow, to integrate these two models effectively. â€¢ State-of-the-art performance: Extensive experiments on five public datasets demonstrate the effectiveness of our approach. We achieved superior performance in ROC- AUC and PR-AUC, surpassing state-of-the-art models. Additionally, we provide comprehensive ablation studies delving into the design of the VAE model, time series preprocessing, and sensitivity analysis on different mod- ules and hyperparameters in deep optimization. ð‘(ð‘¥,ð‘§!) = ð‘\" ð‘¥ ð‘§! ð‘(ð‘§!) ðœƒ ! ð‘§! ð‘¥  ðœ™! N ð‘ž$! (ð‘§!) Augmented View in Plate Diagram Plain View in Plate Diagram ð‘(ð‘¥%,ð‘§%) = ð‘\" ð‘¥% ð‘§% ð‘(ð‘§%) ðœƒ% ð‘ž$\" (ð‘§%) ð‘§% ð‘¥% ðœ™% N ðœ“!&'() Fig. 2. Graphical Model for Augmented Variational Autoencoders. Under the plate notation rules, a white circle denotes a hidden (or latent) variable, while a gray circle signifies an observed variable. The variables contained within the square denote local variables, which are independently repeated N times. Dashed arrow edges imply conditional dependence. Dotted lines represent parameters. Referring to the plate diagram, it is evident that our methodology encompasses the utilization of two generative models. The inference part of models, i.e., qÏ•r and qÏ•a, encode the raw input, denoted as xr, and the augmented input, xa, into their respective low-dimensional representations, zr and za. Subsequently, the generative parts of models pÎ¸r and pÎ¸a, sample the latent space reconstruct the input samples, respectively. We employ a Ïˆ parameterized module to synchronize the learning outcomes of both models. II. BACKGROUD A. Generative model-based Time Series Anomaly Detection 1) Implicit Data Fitting by Non-probabilistic Generative Models: Non-probabilistic generative model-based algorithms for TSAD aim to reconstruct data robustly. Prior works have concentrated on optimizing this reconstructing process to match the characteristics of time-series data via the de- sign of deep network embeddings. Specifically, [30] and [31] implemented an Autoencoder (AE) framework, deploying symmetric encoder-decoder structures and assembling one or multiple CNN-based encoder-decoders for the reconstruction of sequence data. Furthermore, [32] utilized a Generative Adversarial Network (GAN)-based reconstruction for anomaly detection, implicitly fitting a likelihood function based on the normal data through an adversarial mechanism. 3 2) Explicit Data Fitting by Probabilistic generative Models: Unlike AE-based models that learn an encoding-decoding process for datasets, VAE-based models excel in identifying continuous representations within a low-dimensional space. These representations, characterized by their smooth and con- tinuous nature in the hidden space, are essential for preserv- ing probabilistic properties during sampling. Consequently, VAEs can reconstruct samples with increased sharpness and interpretability, outperforming their autoencoder-based coun- terparts. In contrast to GANs, VAEs explicitly model the dataâ€™s likelihood distribution and provide additional constraints on the dataâ€™s posterior distribution based on a preset prior, making them more suitable for modeling data in dynamic areas and designing end-to-end anomaly detection tasks. For instance, [33] utilizes a Gaussian Mixture Model (GMM) assumption for data likelihood distribution, and [34] employs a dynamical prior over time. Issues in VAE based TSAD: VAEs tend to sacrifice representation [35] for data fitting. In that case, the induced latent hole will lead to the lack of robustness Represented by latent hole. At the same time, the modelâ€™s failure to learn the likelihood of the sequence data exacerbates its robustness issues. Specifically, VAE-based anomaly detection algorithms typically employ a Convolutional Neural Network (CNN) architecture for data encoding. While effective for image data, this approach often fails to capture the temporal characteristics of time-series data, such as seasonality, period- icity, and frequency domain features, through CNN encoding filters. The shallow Fully Connected Network (FCN) networks are employed in VAEs as substitutes. As a result, the naive structure cannot capture varying dependence, and compared to the image data, the sequence in training is relatively small. Due to the model and data issue, the generative modal cannot converge to the optimal. Advances in VAE-based TSAD: To remedy this, the traditional variational framework has been revoluted, integrat- ing the meta-prior in generative modeling. For instance, a Variational Recurrent Neural Network (VRNN) has been pro- posed, establishing a model for the variational autoencoderâ€™s inference, prior, and reconstruction processes by capturing the temporal dependencies between intermediate variable h in the deterministic model and input variables x in the recurrent neural network. This approach and its variants [18], [20], [21] effectively utilize the variational autoencoder to learn and model the latent distribution of data while maintaining the temporal dependence of the recurrent neural network. On the other hand, the variational representation can be designed. [23] utilizes prototype-based approaches to define latent representations for Multivariate Time Series (MTS) and learn a robust likelihood distribution of normal data. B. Self-supervised Learning on Time Series Data in Deter- mistic and Generative Models In deterministic models, augmenting time-series data or their representations, combined with specific self-supervised algorithms, can provide sufficient depth for training in down- stream tasks. For instance, in prediction tasks, [36] encodes time-series segments in both time and frequency domains to obtain positive and negative sample pairs, using con- trastive learning to capture the seasonal-trend representation of time-series data. [37] constructs positive pairs with multi- granularity time-series segments and corresponding latent vari- able representations, enhancing fine-grained information for prediction by maximizing mutual information. In classification tasks, [38] forms pairwise representations of global and local input series, obtaining informational gains through adversarial learning. For anomaly detection tasks, [39] aims to acquire spatio-temporal dependent representa- tions suitable for downstream tasks. [40] proposes a multi- layer representation learning framework to obtain consistent, contextual representations of overlapping segments, designing a contrastive loss by decomposing overlapping subsequences in both instance and temporal dimensions to obtain positive and negative sample pairs. Additionally, [41] employs a dual bilinear process at the encoding level to capture positive and negative samples of time sequences, thereby capturing both long and short-term dependencies. In contrast, SSL based on time-series generative models typically focuses on data and representation augmentation as a generative approach. For instance, Autoencoder (AE) based methods, such as those presented in [42]â€“[45], leverage the AE architecture for data augmentation. Similarly, diffusion-based approaches, as seen in [46]â€“[48], employ diffusion processes to augment time-series data and representations. III. AUGUMENTATION GUIDED GENERATIVE ANOMALY DETECTION In this section, we first provide the problem definition for generative model-based TSAD. Subsequently, we depict the structure of the augmented generative model in the form of a plate diagram, illustrating the random variables and their de- pendency structure. The augmented guided generative anomaly detection model operates within the framework of probabilistic generative models, employing self-supervised techniques to augment the latent variables z during the training process of the generative model, thereby enhancing the deep modelâ€™s fit to the data likelihood. Herein, we have implemented a self-supervised variational autoencoder based on input data augmentation, which preprocesses the input data x to generate latent variables za with different views. To align the likeli- hood functions of the raw and augmented models, we have developed two distinct mutual information loss functions, one grounded in depth and the other in statistics. By maximizing the mutual information between them, we draw the models closer to fitting the same distribution. A. Problem Definition Time series data is succinctly represented as X := {(x(i), y(i))}n i=1, encompassing n time-stamped observations x âˆˆ Rc situated within a c-dimensional representation space, each paired with a discrete observation y. The observation y is assigned discrete values across l predefined classes, delineated as y âˆˆ {0, 1, . . . , l âˆ’ 1}. Here, c denotes the feature dimensionality at each time point, categorizing the 4 dataset as a Multivariate Time Series (MTS) when c > 1 and as a Univariate Time Series (TS) for c = 1. In all figures and equations, to enhance notational conciseness, we propose abbreviating raw as r and augmentation as a. In generative model-based TSAD, the focus is on learning a reconstructing model, i.e., Mnormal, that models the mass of loglikelihood of the majority of normal data points within the entire dataset X = {Xnormal, Xabnormal}. Anomalies are then identified in an unsupervised, end-to-end fashion by calculating the anomaly score, denoted as AS(x, Ë†x), which quantifies the difference between a given input x and its modeled copy Ë†x as reconstructed by Mnormal. This approach is feasible, assuming that the log-likelihood learned from normal observations will diverge notably when encountering anomalous data, yielding elevated anomaly scores. B. Data Augmentation Guided Probabilistic Generative Model The plate diagram in Fig. 2 defines an augmented-based probabilistic generative model. The upper part of the diagram specifies the learned joint distribution of the original data xr and its latent variable zr, i.e., p(xr, zr), where the latent variable zr is generated by an inference network qÏ•r(zr|xr) parameterized by Ï•r, and the reconstructed variable Ë†xr is produced by a generative network pÎ¸r(xr|zr) parameterized by Î¸r. The model optimizes an approximate surrogate error Lr ELBO, comprising a reconstruction loss that maximizes the likelihood distribution Lr R and a DKL loss that minimizes the discrepancy between the prior of the latent variables and their variational posterior Lr I, i.e. Lr ELBO := EqÏ•r(zr|xr) h log pÎ¸r(xr | zr) | {z } Lr R i âˆ’ Î² DKL \u0000qÏ•(zr | xr)||p(z) | {z } Lr I \u0001 . (1) The lower part of the plate diagram outlines the probabilistic model of the joint distribution of the augmented view data xa and latent variables za, i.e., p(xa, za) with latent variables za derived from an augmented inference network qÏ•a(za|x), i.e., za âˆ¼ qÏ•a(za|x). Similar to the above, the model optimizes an augmented reconstruction loss La R and inference loss La I , i.e., La ELBO := EqÏ•a(za|xa) h log pÎ¸a(xa | za) | {z } La R i âˆ’ Î² DKL \u0000qÏ•(za | xa)||p(z) | {z } La I \u0001 . (2) On the one hand, both models strive to fit their respective data distribution likelihoods. On the other, we leverage the advantage of data augmentation by maximizing the mutual information I(zr, za) between two latent models, optimizing a mutual information loss parameterized by Ïˆ (in deep learning approximation), to train the models for maximal data likeli- hood synergistically. Given the variety of latent variable augmentations, this pa- per augments the raw data to augment the model. In summary, we propose an augmented probabilistic generative model to learn a joint likelihood function p(xr, zr, xa, za) for anomaly detection while simultaneously optimizing an inference net- work parameterized by Ï•r, Ï•a, a generative network parame- terized by Î¸r, Î¸a, and an alignment network parameterized by Ïˆ. The Ïˆ can be parameterized by neurons in deep learning approximation and pseudo-parameters in shallow learning. The generative process is as follows: p (xr, xa) = Z p (xr, xa, zr, za) dzrdza, (3) where xr represents the raw input datapoint, xa is the aug- mented sample based on the input, zr is the raw latent variable, and za is the augmented latent variable. The joint distribution is often too high-dimensional and sophisticated to solve it directly. To address this, a tractable variational distribution q(zr, za) is employed as an approxima- tion within the framework of Variational Inference (VI). Due to the computational convenience it offers, we typically take the logarithm of the distribution. Consequently, as depicted in Equation 3, the likelihood of data that encompasses latent variables can be decomposed as follows: p (xr, xa) = Z p (xr, xa, zr, za) q(zr, za) q(zr, za) dzrdza, (4) and we can get the log versions as follows: log p (xr, xa) = log Z p (xr, xa, zr, za) q(zr, za) q(zr, za) dzrdza. (5) Given the log is a convex function, we can get a lower bound by Jensenâ€™s inequality: log p (xr, xa) = log Z p (xr, xa, zr, za) q(zr, za) q(zr, za) dzrdza = log Eq(zr,za|xr,xa) \u0014 p (xr, xa, zr, za) q (zr, za | xr, xa) \u0015 â‰¥Eq(zr,za|xr,xa) log \u0014 p (xr, xa, zr, za) q (zr, za | xr, xa) \u0015 =Eq(zr,za|xr,xa) log \u0014p(xr | zr)p (xa | za) p (zr, za) q(zr | xr)q (za | xa) \u0015 = Eq(zr|xr) log[p(xr | zr)] + Eq(za|xa) log [p (xa | za)] | {z } 1 + Eq(zr,za|xr,xa) log \u0014 p (zr, za) q(zr | xr)q (za | xa) \u0015 | {z } 2 . (6) As we can see, the 1 part can be decomposed into two reconstruction losses, i.e., 1 = Lr R + La R and the 2 part in E.q. 6 can be decomposed as: 5 Eq(zr,za|xr,xa) log \u0014 p (zr, za) q(zr | xr)q (za | xa) \u0015 =Eq(zr,za|xr,xa) log \u0014 p (zr, za) p(zr)p (za) q(zr | xr)q (za | xa) p(zr)p (za) \u0015 =Eq(zr,za|xr,xa) log \u0014 p (zr, za) p(zr)p (za) \u0015 + Eq(zr,za|xr,xa) log \u0014 p(zr)p (za) q(zr | xr)q (za | xa) \u0015 = Eq(zr,za|xr,xa) log \u0014 p (zr, za) p(zr)p (za) \u0015 | {z } A âˆ’ DKL[q(zr | xr)âˆ¥p(zr)] | {z } i âˆ’ DKL [q (za | xa) âˆ¥p (za)] | {z } ii , (7) where the 2 part can be the combination of two inference losses and mutual information between latent variables, i.e., 2 = Lr I +La I +I(zr, za), where we denote i = Lr I, ii = La I , and A = I(zr, za). Minimization of the term denoted by 1 leads to an in- creased log-likelihood for both p(xr|zr) and p(xa|za), appli- cable to the raw and augmented data perspectives, respectively. Reducing the inference loss, represented as Lr I, La I within the section labeled 2 , contributes to a more coherent latent space that facilitates the reconstruction process. Moreover, enhancing the mutual information, denoted as I(zr, za), serves to bridge the disparities between the raw and augmented models. This process ensures a cohesive framework for incorporating data augmentation within the generative model. In conclusion, the proposed objective for learning is to approximate the joint distribution p(xr, xa) within an augmentation-informed generative modeling context, denoted as: LAVAE = 1 + A âˆ’ i âˆ’ ii =Lr ELBO + La ELBO + I(zr, za), (8) where LAVAE represents the augmentation based VAE loss. Î¨!! Î¨!\" ð‘§\" ð‘§# z\" $ z# $ 1 0 q% \" q% # Update ðœƒ,ðœ‘ Fix ðœ“ 1 0 Î¨!! Î¨!# ð‘§\" ð‘§# z& $ z' $ 0 1 q% \" q% # Fix ðœƒ,ðœ‘ Update ðœ“ 0 1 Fig. 3. Illusration of adversarial learning in mutation information approxi- mation. In the first stage, the discriminator is frozen to update the parameters of Encoders and decoders. In the second stage, We freeze the parameters of both the generator and the discriminator while simultaneously inverting the pseudo-labels of positive and negative samples to train the discriminator. C. Deep and Shallow Learning in Mutual Information Ap- proximation 1) MI approximation in Shallow Learning: We employ a LinfoNCE loss to approximate the lower bound of MI. Since this method uses a non-parametric variational distribution in variational inference, it can be considered a form of shallow learning. When the variational distribution q(zr|za) is em- ployed to approximate the untractable posterior distribution p(zr|za), as in E.q. (9a), we can derive a lower bound, as in E.q. (9b). Specifically, by using an energy-based varia- tional function q(zr|za) = p(zr) a(za)ef(zr,za), where f(zr, za) is a critic value function, and a(za) = Ep(x) \u0002 ef(x,y)\u0003 for approximation, we use the convexity of the log function to apply Jensenâ€™s inequality to Ep(za)[log a(za)] to further derive a lower bound, as in E.q. (9c). By utilizing the inequality: log(z) â‰¤ z Ï„ +log(Ï„)âˆ’1, we can approximate further to obtain another lower bound, as in E.q. (9d). Using K samples for an unbiased estimate, we obtain E.q. (9e), and through Monte Carlo estimation, we can approximate it to the infoNCE loss, that is, LinfoNCE in (9f): I(zr, za) =Ep(zr,za) \u0014 log q(zr | za)p(zr|za) p(zr)q(zr | za) \u0015 (9a) =Ep(zr,za) \u0014 log q(zr | za) p(zr) \u0015 + Ep(za)[DKL(p(zr | za)âˆ¥q(zr | za))] â‰¥Ep(zr,za)[log q(zr | za)] (9b) â‰¥Ep(zr,za)[f(zr, za)] âˆ’ Ep(za) \" Ep(zr) \u0002 ef(zr,za)\u0003 a(za) + log(a(za)) âˆ’ 1 # (9c) â‰¥1 âˆ’ Ep(z(r,1:K))p(za) \" ef(z(r,1),za) a \u0000za; z(r,1:K) \u0001 # + Ep(z(r,1:K))p(za|z(r,1)) \" log ef(z(r,1),za) a \u0000za, z(r,1:K) \u0001 # (9d) â‰¥E ï£® ï£° 1 K K X i=1 log ef(z(r,i),z(a,i)) 1 K PK j=1 ef(z(r,i),z(a,j)) ï£¹ ï£» (9e) â‰¥E \" 1 K K X i=1 log p \u0000z(a,i) | z(r,i) \u0001 1 K PK j=1 p \u0000z(a,i) | z(r,j) \u0001 # â‰œ LinfoNCE. (9f) Actually, we optimize an infoNCE loss scaled by the temper- ature coefficient Ï„: LInfoNCE = âˆ’ log exp \u0000zr,âŠ¤ u za u/Ï„ \u0001 P v exp \u0010 zr,âŠ¤ u zav/Ï„ \u0011 + P vÌ¸=u exp \u0010 zr,âŠ¤ u zv/Ï„ \u0011, (10) where and the negative pairs are none, indicating that the negative keys for a sample are the positive keys for others. 2) MI approximation in Deep Learning: We can decom- pose the mutation information into two ratios in E.q. (11a) and approximate in density ratio trick, guided by [49]. In 6 that case, the density ratio is approached by a parameterized neural network, and we can approximate the MI implicitly in a deep learning scheme. Specifically, instead of modeling two distributions directly, i.e., q(zr, za) and q(zr), we can learn the ratio r = q(zr,za) q(zr) in an adversival manner, i.e., training a discriminator to classify whether the label comes from the target distribution P or not, as shown in E.q. (11b), where the y is a preset pseudo-label. Since we use the discriminator method to estimate the mutual information, the upper bound is denoted as E.q. (11c). Eq(zr,za) q (zr|za) q (za) =Eq(zr,za) log q (za|za) q(zr) (11a) â‰¤ log P(y = 1 | zr) P(y = 0 | zr) + log P(y = 1 | za) P(y = 0 | za) (11b) â‰¤ log Î¨(zr) 1 âˆ’ Î¨(zr) + log Î¨a(za) 1 âˆ’ Î¨a(za) â‰œ Ladversial (11c) D. End-to-End Anomaly Detection Training This section proposes an end-to-end TSAD model based on a weakly augmented generative model. 1) Weakly Augmentation: In augmentation-based genera- tive models, the likelihood fitting is enhanced by reusing train- ing data. In VAEs, this leads to improved generative models pÎ¸(x | z) parameterized by Î¸ via enriched data representations in the inference network qÏ•(z | x) parameterized by Ï•. The augmented latent variable, za, is derived as za âˆ¼ q(za|x). During data preprocessing, we can augment latent representa- tions directly by manipulating the input data augmentation, represented as za âˆ¼ q(za|xa). Here, the augmented input xa is obtained from the raw input xr using the augmentation operation O, formulated as xa = O(xr). Data augmentation methods for time series data typically require an input array of size (batch, time_steps, channel), with manipulations possible in the batch domain (such as jittering with noise, scaling, and normalization) or in the time domain (including window slicing and warping). Additionally, augmentations can be applied in the frequency domain. These techniques enrich the original dataset through various methods, effectively enhancing data diversity. This diversification is crucial for models to comprehend better and predict time series patterns. Nevertheless, our findings suggest that applying weak aug- mentation to the original input data may yield more favorable outcomes for likelihood fitting in anomaly detection tasks. Specifically, weak augmentation involves subtle modifications to the data, primarily through different normalization tech- niques. These include: â€¢ Standardization: xa = Ostand(xr) = xr âˆ’ Âµ Ïƒ , (12) where, Âµ is the mean and Ïƒ the standard deviation of the data. â€¢ Min-Max Normalization: xa = Omm(xr) = xr âˆ’ min(xr) max(xr) âˆ’ min(xr), (13) this scales the data to a specified range, such as 0 to 1. Such moderate adjustments typically preserve the funda- mental characteristics and trends of the time series. They enable the model to discern the core attributes of the data more effectively, thereby enhancing prediction accuracy. Fur- thermore, weak augmentation maintains the dataâ€™s authentic- ity, mitigating the risk of over-distorting the original data structure. This is vital for preserving both the reliability and interpretability of the model. 2) Training: For the raw input data xr, we first obtain its augmented variable xa during the data preprocessing phase. Then, we train the inference networks for both the raw and augmented perspectives, namely qÏ•r(zr | xr) and qÏ•a(za | xa), as well as the generative networks pÎ¸r(xr | zr) and pÎ¸a(xa | za). At the same time, we optimize the inference and reconstruction losses based on Eq. (1) and E.q. (2). During the training phase, we employ a strategy of shar- ing parameters for joint likelihood consolidation in raw one p(xr, zr) and augmented one p(xa, za) to align the recon- struction effects. This means allowing both the inference and reconstruction networks to share the same structure and parameters during training. Such an approach reduces the number of model parameters and increases the generalization of the model, enabling it to learn normal patterns of different data for reconstructing normal data. In the variational inference of the joint distributionâ€™s poste- rior distribution, we maximize the mutual information of the two latent variables to encourage the original generator and the augmented generator to tend towards producing similar data distributions. In our actual optimization objectives, i.e., the surrogated loss LWAVAE, we implemented two methods to control the divergence of the two likelihood distributions: the LinfoNCE WAVAE loss based on contrastive learning: LinfoNCE WAVAE = 1 + A âˆ’ i âˆ’ ii =Lr ELBO + La ELBO + I(zr, za) =Lr ELBO + La ELBO + LinfoNCE(zr, za), (14) and the Ladversial WAVAE loss based on adversarial learning: Ladversial WAVAE = 1 + A âˆ’ i âˆ’ ii =Lr ELBO + La ELBO + I(zr, za) =Lr ELBO + La ELBO + Ladversial(zr, za), (15) and the discriminatorâ€™s performance is optimized in an adver- sarial manner, with the specific optimization process illustrated in Fig. 3. The first part focuses on maximizing the encoder- decoder capabilities, and the second part involves swapping pseudo-labels to maximize discriminator loss. 3) Anomaly Scores: The training process and the determi- nation of anomalies are illustrated in Fig. 4. Reconstruction- based anomaly detection utilizes the deviation between the original data and the reconstructed data as an anomaly score, denoted as AS(x, Ë†x). We determine whether the input data is anomalous by comparing the anomaly score with a pre-set threshold Î·. The specific process is shown in Algorithm 1. 7 Augmented Input Encoder Encoder Sharing Parameters Decoder Decoder Sharing Parameters Raw Input Raw Output Anomaly Score Threshold Score > Y N Anomaly Normal sequence Anomaly Detector Data Processing Encoding Decoding Close Raw Sequence Augmented Sequence Parameters of decoder Parameters of encoder M utal Information Align Augmented Output Data Outputing Fig. 4. The overall framework of WAVAE, training begins with the raw data xr undergoing an augmentation algorithm AUG, resulting in augmented data xa. Concurrently, we train a shared-parameter VAE separately for both sets of data. However, evaluation, i.e., Anomaly detector, is conducted solely on the original model between raw input xr and itsâ€™ reconstruction Ë†xr, essentially designing an end-to-end anomaly detector. IV. EXPERIMENTS A. Benchmarks To validate the effectiveness of our approach, we selected 16 reconstruction-based models for anomaly detection in time series data as benchmarks, which included 6 generative models (GANs and AEs based), which are specifically: 1) Transformer Autoencoder (TAE) [50]: A transformer autoencoder encodes and decodes time-series data to capture temporal dependencies. 2) The Multi-scale CNN-RNN based Autoencoder (MSCREA): A CNN, RNN-based autoencoder leverages convolutional neural networks for representation extraction across multiple scales and recurrent neural networks for capturing temporal dependencies, tailored for enhancing anomaly detection in time-series data. 3) BeatGAN (BGAN) [32]: A GAN-based model for ECG anomaly detection, learning normal heartbeats to iden- tify irregular patterns in time-series data. 4) RNN-based Autoencoder (RAE) [51]: A Gated Recur- rent Unit (GRU) based autoencoder designed to encode and decode time-series data for anomaly detection by learning sequential patterns and temporal relationships. 5) CNN-based Autoencoder [52]: A CNN-based autoen- coder architecture tailored for time-series analysis, uti- lizing convolutional layers to identify spatial patterns in data, essential for detecting anomalies in sequential datasets. 6) RandNet (RN) [30]: An ensemble of randomly struc- tured autoencoders with adaptive sampling recognize efficiently and robustly detect outliers in data. and 10 anomaly detection methods for time-series data based on probabilistic generative models, namely Variational Au- toencoders, which are specifically: 1) The Gaussian Mixture Model Variational Autoencoder (GMMVAE) [33]: VAE with GMM priors combines the probabilistic framework of Gaussian mixtures with the generative capabilities of VAEs to model complex dis- tributions in time-series data, facilitating robust anomaly detection through learned latent representations. 2) Variational Autoencoder (VAE) [34]: A traditional VAE model to model the likelihood of generative data. 3) Recurrent Neural Network based VAE (RNNVAE) [19]: Merges Recurrent Neural Networks (RNN) with the variational approach to autoencoding, capturing tempo- ral dependencies within sequential data for improved anomaly detection through stochastic latent spaces. 4) The Variational RNN Autoencoder (VRAE) [20]: The Variational RNN Autoencoder combines the sequence modeling strengths of RNNs with the probabilistic latent space of variational autoencoders, aiming to improve anomaly detection in time-series by learning complex temporal structures. 5) Î±-VQRAE, Î²-VQRAE, and Î³-VQRAE [21]: Extensions of VRAE, with RNN substituted by a Quasi-Recurrent Network and Î±, Î², Î³-loglikelihood loss to help the VAE model achieve robust representation. 6) Î±-biVQRAE, Î²-biVQRAE, and Î³-biVQRAE [21]: Vari- ants of VQRAE, with RNN extended to bilevel to achieve time dependence on time-series data, while the Î±, Î², Î³-loglikelihood loss helps the VAE based model achieve robust representation. B. Experiment Setup Datasets: To validate the effectiveness of our proposed methodology, we executed a series of experiments on a quartet of multivariate time series datasets: Genesis Demonstrator 8 Algorithm 1: The training process of WAVAE Input: Dataset D = {Bi tr, Bi e}m i=1, Training batch Btr = {(x(j), y(j)}b j=1 âˆˆ RbÃ—sÃ—f, Evaluation batch Be âˆˆ RbÃ—sÃ—c. // b, s, c is the size of batch, sequence length and features Output: Parameters of encoder fÏ•, decoder gÎ¸, and discriminator Ïˆ, Anomaly threshold Î·. 1 for each Bi in training batch Btr do 2 Bi a = O(Bi); // Here the operation O is defined in E.q. (12) and E.q. (13). 3 Bi a âŠ‚ Ba; 4 end 5 while unconverged do 6 for each Bi, Bi a in D do 7 Compute gradients of Eq. (14) or Eq. (15) w.r.t. Î¸ and Ï•; 8 Update the parameters of f, g; 9 end 10 end 11 for each Bi e in evaluation batch Be do 12 Ë† Bj e = gÎ¸ \u0000fÏ•(Bj e) \u0001 ; // Reconstruct the sequence 13 for each xi r, xi a in Be do 14 Score = AS(xj r, xj a); //Calculate the anomaly score based on the similarity 15 if Score < Î· then 16 xi r is an anomaly; 17 else 18 xi r is not an anomaly; 19 end 20 end 21 end Data for Machine Learning, High Storage System Data for Energy Optimization, Electrocardiogram, and Trajectory Data, along with a single univariate time series dataset, the Yahoo S5. These datasets, encompassing several hundred temporal sequences, are sourced from real-world industrial systems or are synthetically generated, comprehensively evaluating the algorithmâ€™s performance. The Genesis Demonstrator dataset for machine learning (GD)1 comprises 5 distinct sequences, encapsulating contin- uous or discrete signals recorded from portable pick-and- place robots at millisecond intervals. We harness the sequence replete with anomalies to target anomaly detection, specifically the Genesis_AnomalyLabels.csv, which consists of 16,220 records. Within this framework, records marked with class 0 are designated as normal, whereas the remaining classifications, classes 1 and 2, are delineated as anomalies. The High Storage System Data for Energy Optimiza- 1https://www.kaggle.com/datasets/inIT-OWL/ genesis-demonstrator-data-for-machine-learning tion (HSS)2 dataset is composed of 4 sequences docu- menting the readings from induction sensors situated on conveyor belts. Anomaly detection is conducted on two labeled sequences: HRSS_anomalous_standard.csv and HRSS_anomalous_optimized.csv, together en- compassing 23,645 records. Within each sequence, records tagged with class 0 are categorized as normal, whereas those labeled with class 1 are identified as anomalous. The Electrocardiogram dataset (ECG)3 is comprised of a solitary time-series sequence collected from PhysioNet signals attributed to a patient with severe congestive heart failure. For the sake of consistent comparison, we fol- lowed the guidelines proposed by researcher [53], utiliz- ing ECG5000_TRAIN.tsv from the training datasets for anomaly detection. This approach involves classifying three classes (Supraventricular ectopic beats, PVC, and Unclassifi- able events) as anomalies, while the two remaining classes (R-on-T Premature Ventricular Contraction (PVC), Normal) are maintained as the normative data. The Trajectory Data (TD)4 dataset encapsulates a unique time-series sequence, with each data point being two- dimensional. These points represent the detection algorithmâ€™s accuracy in delineating the skeletal structure of a hand, coupled with assessments from three human evaluators on the algorithmâ€™s predictive accuracy. We undertake an unsu- pervised anomaly detection task in this setting using the HandOutlines_TRAIN.tsv file extracted from the train- ing set, comprising 1000 instances. Within this dataset, in- stances classified as normal bear the label of class 1, and those recognized as anomalies carry the label of class 0. The Yahoo S5 (S5)5 dataset encompasses an array of both authentic and synthetic time-series sequences. The synthetic component of this collection is distinguished by sequences that display diverse trends, degrees of noise, and seasonal patterns. Conversely, the real segment of the dataset encap- sulates sequences that chronicle the performance metrics of assorted Yahoo services. This compilation contains a total of 367 labeled anomalies across both real and synthetic time series. The dataset is segmented into four distinct subsets: the A1, A2, A3, and A4 benchmarks. Within this schema, entries marked with class 0 are categorized as normal, whereas those annotated with class 1 are identified as anomalous. The comparison of each dataset concerning data volume, dimensionality, temporal span of sequence acquisition, and proportion of anomalies is illustrated in Table I. Anomaly Scores: The presented code snippet delineates a section of an anomaly detection algorithm employing a Variational Autoencoder (VAE). The VAE is tasked with modeling the probability distribution of the input data. During the anomaly detection phase, the input data is reconstructed, and the reconstruction error is computed as the sum of squared differences between the original data xr and the reconstructed data Ë†x. Anomalies are flagged by setting a threshold at the 2https://www.kaggle.com/datasets/inIT-OWL/ high-storage-system-data-for-energy-optimization 3https://www.cs.ucr.edu/âˆ¼eamonn/time series data 2018/ 4https://www.cs.ucr.edu/âˆ¼eamonn/time series data 2018/ 5https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70 9 TABLE I DATASETS OVERVIEW. Sequences Anomaly Sequences Avg. length Avg. anomalies Avg. Anomaly ratio (%) Features Time Range GD 5 1 16,220 39 0.24% 18 in 760 ms HSS 4 2 19,634 4,517 23.01% 18 in 14 ms ECG 1 1 500 31 6.2% 2 NA TD 1 1 1,000 362 36.2% 2 NA S5 A1 67 67 1,416 25 1.79% 1 NA S5 A2 100 100 1,421 5 0.35% 1 NA S5 A3 100 100 1,680 9 0.54% 1 NA S5 A4 100 100 1,680 8 0.48% 1 NA 99th percentile of the error distribution, isolating the top 1% of instances with the highest errors as anomalies. Performance metrics such as F1 score, precision, and recall are calculated to gauge the accuracy of the test. These metrics are critical in determining the anomaly detection mechanismâ€™s true positive rate (precision) and sensitivity (recall). Further- more, the algorithm utilizes the confusion matrix C, the F1 score, and Cohenâ€™s kappa score Îº to evaluate its performance comprehensively. The utilization of the Area Under the Receiver Operating Characteristic curve (AUROC) and the Precision-Recall curve (PRAUC) is noteworthy: â€¢ AUROC is derived by plotting the False Positive Rate (FPR) against the True Positive Rate (TPR) and calcu- lating the Area Under the Curve (AUC). It is a robust measure of the classifierâ€™s discriminative power, particu- larly in class imbalance. â€¢ PRAUC is obtained by plotting precision against the recall and computing the AUC, providing valuable insight into the performance of the positive class, which is often the minority in anomaly detection tasks. The advantages of AUROC and PRAUC include their ability to provide a holistic measure of model performance insensitive to threshold selection and their effectiveness in conditions of class imbalance. The AUROC reflects the likelihood that the classifier will rank a randomly chosen positive instance more highly than a negative one. On the other hand, the PRAUC focuses on the modelâ€™s performance in the positive class, making it particularly useful for datasets with significant class imbalances. The calculation of these metrics is as follows: â€¢ The function roc_curve computes the FPR and TPR for various threshold values, while the function auc calculates the area under the ROC curve to determine the AUROC. â€¢ The function precision_recall_curve is em- ployed to compute the precision and recall at different thresholds, with the auc function again used to calculate the area under the precision-recall curve, yielding the PRAUC. Using these metrics, the modelâ€™s performance can be eval- uated not solely based on accuracy but also on its robustness against false anomaly classifications (precision) and its capa- bility to identify all true anomalies (recall). Implementation details: Our experimental setup was stan- dardized to ensure a level playing field and control for po- tential performance biases introduced by Pytorch Versions. The versions selected for all implementations were Python 3.7.16, PyTorch 1.1.0, NumPy 1.19.2, CUDA toolkit 10.0.130, and cuDNN 7.6.5. This approach guaranteed that the pro- posed and comparative algorithms were evaluated under equiv- alent computational environments. Our hardware setup in- cluded NVIDIA Quadro RTX 6000 GPUs with driver version 525.105.17 and CUDA version 12.0. Additionally, we incor- porated a randomness control module, employing seed values to govern the stochasticity across computational units such as GPU, Python, and PyTorch. C. Performance The anomaly detection performance on five public datasets can be found in Tables II,III. In terms of PRAUC and ROCAUC metrics, our method outperformed the baseline across all datasets, regardless of whether they are AE and GAN-based generative models or VAE-based ones. Note that our compar- ative data originates from [21]. Additionally, we observed that methods based on adversarial mechanisms generally underper- form compared to those using contrastive loss. D. Sensitivity Analysis To rigorously assess the sensitivity of hyperparameters in our model, we have conducted an extensive series of abla- tion experiments. This comprehensive evaluation encompasses many hyperparameter sets throughout the entire end-to-end training process. â€¢ Specifically, we investigate variations in VAE-related hyperparameters such as the Î² in E.q. (1) and E.q. (2) to balance the inference and reconstruction in VAE training, the dimension of z, and the reconstruction loss LR. â€¢ We also scrutinize SSL-related hyperparameters like the number of discriminator layers, the weight of infoVAE loss, and the augmentation method. â€¢ In addition, we delve into hyperparameters pertinent to time-series processing, including the sequence length and hidden variables in embeddings. â€¢ Lastly, we explore adjustments in deep learning hyper- parameters, including batch size, learning rates, and the number of epochs. 10 TABLE II OVERALL ACCURACY, PR-AUC. FOR EACH DATASET, THE THREE BEST-PERFORMING METHODS ARE DENOTED USING DISTINCT MARKINGS: BOLD FOR THE TOP METHOD, SUPERSCRIPT ASTERISKâˆ— FOR THE SECOND-BEST, AND UNDERLINE FOR THE THIRD-BEST. Models/Datasets GD HSS ECG TD S5 TAE 0.088 0.195 0.138 0.175 0.298 MSCREA 0.075 0.161 0.105 0.148 N/A BGAN 0.109 0.214 0.103 0.151 0.434 RAE 0.128 0.242 0.118 0.163 0.421 CAE 0.116 0.207 0.107 0.177 0.383 RN 0.112 0.146 0.105 0.168 0.232 GMMVAE 0.142 0.216 0.163 0.364 0.458 VAE 0.097 0.203 0.131 0.188 0.272 RNNVAE 0.086 0.204 0.079 0.118 0.211 VRAE 0.131 0.219 0.144 0.165 0.298 Î±-VQRAE 0.235 0.225 0.177 0.428 0.487 Î²-VQRAE 0.242 0.223 0.177 0.427 0.525 Î³-VQRAE 0.245 0.222 0.184 0.423 0.499 Î±-biVQRAE 0.249 0.227âˆ— 0.141 0.429 0.490 Î²-biVQRAE 0.256 0.224 0.189âˆ— 0.430 0.527âˆ— Î³-biVQRAE 0.258âˆ— 0.222 0.186 0.432 0.524 WAVQRAE-Adverisal 0.304 0.286 0.190 0.440 0.805 WAVQRAE-Contrast 0.307 0.358 0.200 0.504 0.838 TABLE III OVERALL ACCURACY, ROC-AUC. Models/Datasets GD HSS ECG TD S5 TAE 0.652 0.563âˆ— 0.542 0.531 0.635 MSCREA 0.582 0.509 0.509 0.519 N/A BGAN 0.673 0.549 0.547 0.622 0.677 RAE 0.608 0.537 0.552 0.593 0.753 CAE 0.641 0.560 0.574 0.583 0.757âˆ— RN 0.731 0.526 0.524 0.533 0.575 GMMVAE 0.763 0.534 0.533 0.531 0.815 VAE 0.664 0.525 0.531 0.643 0.678 RNNVAE 0.595 0.516 0.536 0.574 0.642 VRAE 0.658 0.521 0.551 0.662âˆ— 0.660 Î±-VQRAE 0.970 0.529 0.592 0.539 0.858 Î²-VQRAE 0.968 0.520 0.583 0.535 0.849 Î³-VQRAE 0.969 0.524 0.598 0.547 0.875 Î±-biVQRAE 0.975 0.538 0.597 0.542 0.873 Î²-biVQRAE 0.976 0.527 0.603âˆ— 0.546 0.864 Î³-biVQRAE 0.978âˆ— 0.526 0.601 0.549 0.882âˆ— WAVQRAE-Adverisal 0.991 0.563 0.612 0.579 0.883 WAVQRAE-Contrast 0.996 0.575 0.630 0.646 0.899 In each set of experiments, we systematically vary a selected hyperparameter within its feasible range while maintaining the default settings for all other hyperparameters to isolate and understand the individual impact of each hyperparameter adjustment on the modelâ€™s performance. 1) Effect of VAEs: Ideally, VAE is adept at modeling data distributions, encapsulating the potential to fit the likelihood of diverse data modalities through its sophisticated encoder- decoder architecture rooted in deep neural networks. Concur- rently, it postulates a manifold-based, low-dimensional, contin- uous and smooth space. However, in real-world applications, the efficacy of a VAEâ€™s data likelihood estimation is subject to substantial variability, influenced by the selection of encoder- decoder architectures, the diversity of data modalities, and the specificities of the task at hand. To isolate and assess the effects of these factors on anomaly detection performance, we embark on a systematic sensitivity analysis of hyperparameters spanning three pivotal domains: weight of KL Divergence Î², dimensions of latent variables d, and reconstruction loss function. Through this methodical examination, we aim to elucidate the impact of these variables on the VAEâ€™s recon- struction proficiency, thereby enhancing the modelâ€™s suitability for anomaly detection endeavors. Dimensions of Latent variables: The dimension of the latent variable determines the amount of information the encoder compresses to maximize the log-likelihood under the Information Bottleneck Theory. Simultaneously, it in- fluences the dependency and causality of low-dimensional space representations under the manifold assumption. The fundamental assumptions of generative model-based time- series anomaly detection posit that anomaly data will deviate from the likelihood of normal data. We conducted experi- ments with varying dimensions of the latent variable z to develop a robust likelihood function, specifically exploring {8, 10, 12, 14, 16, 18, 20}. The outcomes and in-depth analysis of these experiments are detailed in Fig. 5 (a), demonstrating the optimal dimension for z. KL Divergence Weight: The KL weight controls the balance between representation learning and reconstruction in the VAE model and the information during the compression process, affecting the modelâ€™s robustness during training. We use the hyperparameter Î² to adjust the VAEâ€™s compression capability. We selected five distinct values for the KL term to assess their impact, specifically {1e âˆ’ 5, 5e âˆ’ 5, 1e âˆ’ 4, 5e âˆ’ 4, 1eâˆ’3}. Detailed results and analysis of this exploration are presented in Fig. 5 (b). Reconstruction Loss Function: In the Eq. (1) and E.q. (2), we fit different likelihood distributions by optimizing the specific reconstruction loss. For discrete data, we optimize the Binary Cross Entropy (BCE) loss, i.e., LBCE R to fit the log- likelihood of a multivariate Bernoulli distribution, denoted as: LBCE R =EqÏ•(z|x) [log pÎ¸ (x | z)] =EqÏ•(z|x) \" D X d=1 xd log Î»Î¸,d (z) + (1 âˆ’ xd) log (1 âˆ’ Î»Î¸,d (z)) # , (16) Where x âˆˆ {0, 1}D and Î» âˆˆ {0, 1}D are the parameters of univariate Bernoulli distributions. For continuous data, we optimize the Mean Square Error (MSE) LMSE R to fit the log- likelihood of a multivariate Gaussian distribution, denoted: LMSE R =Eq(z|x) [log p (x | z)] = 1 D D X d=1 ||xd âˆ’ Ë†xd||2. (17) We also tested two robust variants [54] based on the Bernoulli 11 (b) KL Divergence Weight ð›½ a  Dimensions of z âˆ’ space (c) Reconstruction Loss Function Fig. 5. Sensitivity analysis of VAE related hyperparameters indicates significant findings: (a) reveals that the dimension of z profoundly influences outcomes, with optimal performance when the dimension ranges between 14 and 20. (b) shows that Î² exerts a minimal effect on optimization, peaking in efficacy at 0.001. (c) demonstrates the superior performance of the MSE loss function. (b) MLP Layers a  Latent CLR weight (c) Scaler Fig. 6. Sensitivity analysis of SSL loss related hyperparameters. From (a), it is observed that the weight of the infoNCE Loss has a minimal impact on the overall effectiveness. Conversely, (b) indicates that the number of layers in the discriminator significantly affects the results, with the best performance observed between 2 and 3 layers. (c) illustrates varying augmentation approaches, indicating that using min-max normalization (MinMax) on both original and augmented data is the most effective. b  Dimensions of h âˆ’ space (a) Sequence Length Fig. 7. Sensitivity analysis of sequence-related hyperparameters. (a) indicates that the modelâ€™s anomaly detection performance is not affected by the length of the series. (b) shows that the encoding network achieves the best performance when the hidden state size is 32. (b) Learning Rates a  Batch Sizes (c) Epochs Fig. 8. Sensitivity analysis of deep learning related hyperparameters. (a) A batch size of 64 yields optimal results. (b) The learning rate has minimal impact on the model. (c) The best performance was observed at 50 epochs. 12 likelihood distribution: Lrobust1 R =Î±1 + 1 Î±1  D Y d=1 (xdË†xÎ±1 d + (1 âˆ’ xd) (1 âˆ’ Ë†xd)Î±1) âˆ’ 1 ! , (18) and Gaussian likelihood distribution: Lrobust2 R =Î±2 + 1 Î±2   1 (2Ï€Ïƒ2)Î±2D/2 exp   âˆ’ Î±2 2Ïƒ2 D X d=1 âˆ¥Ë†xd âˆ’ xdâˆ¥2 ! âˆ’ 1 ! , (19) where Î±1, Î±1 are the hyperparameters and Ïƒ is the variance. The analysis and comparison of four types of loss functions are illustrated in Fig. 5 (c). 2) Effect of SSL Loss: The SSL Loss in E.q. (14) and E.q. (15) will be biased by the approximation methods and augmentation types. Mutual Information Approximation: Our study investi- gated two loss functions for mutual information maximization: the infoNCE Loss in contrastive learning, with weight hyper- parameters {0.1, 0.2, 0.3, 0.4, 0.5}, detailed in Fig. 6 (a), and the adversarial learning discriminator, varying layers [3, 4, 5, 6], analyzed in Fig. 6 (b). Augumentation Methods: For self-supervised methods ap- plied to time-series data, augmentation can be employed to mine the intrinsic characteristics of the data, addressing the issue of insufficient data for deep models. To validate the effectiveness of our approach, we experimented with various strong augmentations that enhance the time dependencies and frequency domain representations of time-series data. In parallel, we also explored several weak augmentations, specif- ically normalization techniques applied to time-series data. Our findings indicate that the domains transformed by strong augmentations are ill-suited for generating robust likelihood functions, leading to suboptimal results in anomaly detection. In that case, We conducted sensitivity analysis experiments by testing two combinations of weak augmentations. These com- binations include both raw and augmented data using MinMax (Fig. 6 (c) MM), raw data with MinMax and augmented data with Standard (Fig. 6 (c) MS), raw data with Standard and augmented data with MinMax (Fig. 6 (c) SM), and both raw and augmented data using standardization (Fig. 6 (c) SS). Specific experimental results and analysis are presented in Fig. 6 (c). 3) Effect of Time Series Processing: The time seriesâ€™ inher- ent characteristics, such as the window size in a batch and the memory step length in the encoding model, can impact model performance. Sequence Length In time-series data analysis, the window length is critical as it sets the data truncation extent, which is essential for detecting anomalies with periodicity or spatio- temporal continuity. Furthermore, the length of the time series plays a significant role in identifying contextual anomalies. We chose time series lengths of {8, 16, 32, 64, 96} for our sensitivity analysis. Detailed experimental results and analyses are illustrated in Fig. 7 (a). Hidden Vairbales: We evaluated the impact of different hidden space sizes in the embedding, experimenting with di- mensions of {1, 2, 3, 4, 8, 16, 32, 64, 128, 256}. Detailed anal- ysis and results are presented in Fig. 7 (b). 4) Effect of Deep Learning: In deep models, batch size, learning rates, and epochs cooperate to guide the model convergence to the optimal. Batch Sizes: By modulating the batch size, we gain insights into the stability of gradient updates and their consequent impact on model convergence. To this end, we selected batch sizes {32, 64, 128} to empirically ascertain their influence on the modelâ€™s performance. Detailed experimental results and analyses are illustrated in Fig. 8 (a). Leanring Rates: Step size in gradient descent induced by the learning rates is taken during optimization and can significantly influence the modelâ€™s ability to find minima. We test learning rates of {0.001, 0.01, 0.1} and systematically study their effects and optimize the modelâ€™s performance. Fig. 8 (b) illustrates detailed experimental results and analyses. Number of Epochs: In the context of unsupervised anomaly detection, rather than focusing on model gener- alization, we prioritize the impact of training duration on performance. We fix the randomness and maintain consistent hyperparameters, testing the same modelâ€™s anomaly detection capabilities at epochs {10, 20, 30, 40, 50}. Fig. 8 (c) illustrates detailed experimental results and analyses. V. CONCLUSION The VAE-based anomaly detection effectively captures un- derlying data distributions in time series analysis. As a result, anomalies outside this distribution show notable reconstruction errors. However, the limited amount of data samples can affect the modelâ€™s ability to fit this distribution, especially when considering the rare and hard-to-detect nature of anomalies in real-world situations. To combat data scarcity, we introduce a weakly augmented VAE for time series anomaly detection. The model can achieve a more robust representation in the latent space through joint training on augmented data. Meanwhile, we present two self-supervised strategies, adversarial and contrastive learning, to enhance the performance in data fitting. Quantitative experimental results demonstrate that our ap- proach exhibits commendable performance across five datasets under two distinct foundational model architectures. 13 REFERENCES [1] D. P. Kingma and M. Welling, â€œAuto-encoding variational bayes,â€ arXiv preprint arXiv:1312.6114, 2013. [2] J. Ho, A. Jain, and P. Abbeel, â€œDenoising diffusion probabilistic models,â€ Advances in neural information processing systems, vol. 33, pp. 6840â€“ 6851, 2020. [3] A. Graves, R. K. Srivastava, T. Atkinson, and F. Gomez, â€œBayesian flow networks,â€ arXiv preprint arXiv:2308.07037, 2023. [4] A. Vahdat and J. Kautz, â€œNvae: A deep hierarchical variational autoen- coder,â€ Advances in neural information processing systems, vol. 33, pp. 19 667â€“19 679, 2020. [5] Y. Takida, T. Shibuya, W. Liao, C.-H. Lai, J. Ohmura, T. Uesaka, N. Murata, S. Takahashi, T. Kumakura, and Y. Mitsufuji, â€œSq-vae: Variational bayes on discrete representation with self-annealed stochastic quantization,â€ arXiv preprint arXiv:2205.07547, 2022. [6] L. Manduchi, M. Vandenhirtz, A. Ryser, and J. Vogt, â€œTree variational autoencoders,â€ arXiv preprint arXiv:2306.08984, 2023. [7] A. Razavi, A. Van den Oord, and O. Vinyals, â€œGenerating diverse high-fidelity images with vq-vae-2,â€ Advances in neural information processing systems, vol. 32, 2019. [8] Z. Wu, L. Cao, and L. Qi, â€œevae: Evolutionary variational autoencoder,â€ arXiv preprint arXiv:2301.00011, 2023. [9] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni, â€œModeling tabular data using conditional gan,â€ Advances in neural information processing systems, vol. 32, 2019. [10] A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko, â€œTabddpm: Modelling tabular data with diffusion models,â€ in International Confer- ence on Machine Learning. PMLR, 2023, pp. 17 564â€“17 579. [11] H. Zhu, C. Balsells-Rodas, and Y. Li, â€œMarkovian gaussian process variational autoencoders,â€ in International Conference on Machine Learning. PMLR, 2023, pp. 42 938â€“42 961. [12] X.-B. Jin, W.-T. Gong, J.-L. Kong, Y.-T. Bai, and T.-L. Su, â€œPfvae: a planar flow-based variational auto-encoder prediction model for time series data,â€ Mathematics, vol. 10, no. 4, p. 610, 2022. [13] X. Liu, J. Yuan, B. An, Y. Xu, Y. Yang, and F. Huang, â€œC- disentanglement: Discovering causally-independent generative fac- tors under an inductive bias of confounder,â€ arXiv preprint arXiv:2310.17325, 2023. [14] Z. Wu and L. Cao, â€œC2vae: Gaussian copula-based vae differing disentangled from coupled representations with contrastive posterior,â€ arXiv preprint arXiv:2309.13303, 2023. [15] T. Z. Xiao and R. Bamler, â€œTrading information between latents in hierarchical variational autoencoders,â€ arXiv preprint arXiv:2302.04855, 2023. [16] S. Tonekaboni, C.-L. Li, S. O. Arik, A. Goldenberg, and T. Pfister, â€œDe- coupling local and global representations of time series,â€ in International Conference on Artificial Intelligence and Statistics. PMLR, 2022, pp. 8700â€“8714. [17] M. Tschannen, O. Bachem, and M. Lucic, â€œRecent advances in autoencoder-based representation learning,â€ arXiv preprint arXiv:1812.05069, 2018. [18] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio, â€œA recurrent latent variable model for sequential data,â€ Advances in neural information processing systems, vol. 28, 2015. [19] D. Park, Y. Hoshi, and C. C. Kemp, â€œA multimodal anomaly detector for robot-assisted feeding using an lstm-based variational autoencoder,â€ IEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1544â€“1551, 2018. [20] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, â€œRobust anomaly detection for multivariate time series through stochastic recurrent neural network,â€ in Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, 2019, pp. 2828â€“ 2837. [21] T. Kieu, B. Yang, C. Guo, R.-G. Cirstea, Y. Zhao, Y. Song, and C. S. Jensen, â€œAnomaly detection in time series with robust variational quasi- recurrent autoencoders,â€ in 2022 IEEE 38th International Conference on Data Engineering (ICDE). IEEE, 2022, pp. 1342â€“1354. [22] C.-Y. Lai, F.-K. Sun, Z. Gao, J. H. Lang, and D. S. Boning, â€œNominality score conditioned time series anomaly detection by point/sequential reconstruction,â€ arXiv preprint arXiv:2310.15416, 2023. [23] Y. Li, W. Chen, B. Chen, D. Wang, L. Tian, and M. Zhou, â€œPrototype- oriented unsupervised anomaly detection for multivariate time series,â€ in International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 2023, pp. 19 407â€“19 424. [Online]. Available: https://proceedings.mlr.press/v202/li23d.html [24] A. Khan and A. Storkey, â€œAdversarial robustness of vaes through the lens of local geometry,â€ in International Conference on Artificial Intelligence and Statistics. PMLR, 2023, pp. 8954â€“8967. [25] C. Zhou and R. C. Paffenroth, â€œAnomaly detection with robust deep autoencoders,â€ in Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, 2017, pp. 665â€“674. [26] H. Akrami, A. A. Joshi, J. Li, S. Aydore, and R. M. Leahy, â€œRobust variational autoencoder,â€ arXiv preprint arXiv:1905.09961, 2019. [27] S. Eduardo, A. NazÂ´abal, C. K. Williams, and C. Sutton, â€œRobust variational autoencoders for outlier detection and repair of mixed- type data,â€ in International Conference on Artificial Intelligence and Statistics. PMLR, 2020, pp. 4056â€“4066. [28] S. Cao, J. Li, K. P. Nelson, and M. A. Kon, â€œCoupled vae: Improved accuracy and robustness of a variational autoencoder,â€ Entropy, vol. 24, no. 3, p. 423, 2022. [29] K. Zhang, Q. Wen, C. Zhang, R. Cai, M. Jin, Y. Liu, J. Zhang, Y. Liang, G. Pang, D. Song et al., â€œSelf-supervised learning for time series analysis: Taxonomy, progress, and prospects,â€ arXiv preprint arXiv:2306.10125, 2023. [30] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga, â€œOutlier detection with autoencoder ensembles,â€ in Proceedings of the 2017 SIAM international conference on data mining. SIAM, 2017, pp. 90â€“98. [31] T. Kieu, B. Yang, and C. S. Jensen, â€œOutlier detection for multidi- mensional time series using deep neural networks,â€ in 2018 19th IEEE international conference on mobile data management (MDM). IEEE, 2018, pp. 125â€“134. [32] B. Zhou, S. Liu, B. Hooi, X. Cheng, and J. Ye, â€œBeatgan: Anomalous rhythm detection using adversarially generated time series.â€ in IJCAI, vol. 2019, 2019, pp. 4433â€“4439. [33] W. Liao, Y. Guo, X. Chen, and P. Li, â€œA unified unsupervised gaussian mixture variational autoencoder for high dimensional outlier detection,â€ in 2018 IEEE International Conference on Big Data (Big Data). IEEE, 2018, pp. 1208â€“1217. [34] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei, Y. Feng et al., â€œUnsupervised anomaly detection via variational auto- encoder for seasonal kpis in web applications,â€ in Proceedings of the 2018 world wide web conference, 2018, pp. 187â€“196. [35] S. Zhao, J. Song, and S. Ermon, â€œInfovae: Information maximizing variational autoencoders,â€ arXiv preprint arXiv:1706.02262, 2017. [36] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi, â€œCost: Contrastive learning of disentangled seasonal-trend representations for time series forecasting,â€ arXiv preprint arXiv:2202.01575, 2022. [37] M. Hou, C. Xu, Z. Li, Y. Liu, W. Liu, E. Chen, and J. Bian, â€œMulti- granularity residual learning with confidence estimation for time series prediction,â€ in Proceedings of the ACM Web Conference 2022, 2022, pp. 112â€“121. [38] H. Lee, E. Seong, and D.-K. Chae, â€œSelf-supervised learning with attention-based latent signal augmentation for sleep staging with limited labeled data,â€ in Proceedings of the Thirty-First International Joint Con- ference on Artificial Intelligence, IJCAI-22, LD Raedt, Ed. International Joint Conferences on Artificial Intelligence Organization, vol. 7, 2022, pp. 3868â€“3876. [39] J. Xu, H. Wu, J. Wang, and M. Long, â€œAnomaly transformer: Time series anomaly detection with association discrepancy,â€ arXiv preprint arXiv:2110.02642, 2021. [40] Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu, â€œTs2vec: Towards universal representation of time series,â€ in Proceed- ings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 8, 2022, pp. 8980â€“8987. [41] L. Yang and S. Hong, â€œUnsupervised time-series representation learning with iterative bilinear temporal-spectral fusion,â€ in International Con- ference on Machine Learning. PMLR, 2022, pp. 25 038â€“25 054. [42] Z. Wang, X. Xu, W. Zhang, G. Trajcevski, T. Zhong, and F. Zhou, â€œLearning latent seasonal-trend representations for time series forecast- ing,â€ Advances in Neural Information Processing Systems, vol. 35, pp. 38 775â€“38 787, 2022. [43] W. Chen, L. Tian, B. Chen, L. Dai, Z. Duan, and M. Zhou, â€œDeep variational graph convolutional recurrent network for multivariate time series anomaly detection,â€ in International Conference on Machine Learning. PMLR, 2022, pp. 3621â€“3633. [44] S. N. Shukla and B. M. Marlin, â€œHeteroscedastic temporal varia- tional autoencoder for irregularly sampled time series,â€ arXiv preprint arXiv:2107.11350, 2021. [45] W. Zhang, C. Zhang, and F. Tsung, â€œGrelen: Multivariate time series anomaly detection from the perspective of graph relational learning,â€ 14 in Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, vol. 7, 2022, pp. 2390â€“2397. [46] Y. Li, X. Lu, Y. Wang, and D. Dou, â€œGenerative time series forecasting with diffusion, denoise, and disentanglement,â€ Advances in Neural Information Processing Systems, vol. 35, pp. 23 009â€“23 022, 2022. [47] J. M. L. Alcaraz and N. Strodthoff, â€œDiffusion-based time series imputa- tion and forecasting with structured state space models,â€ arXiv preprint arXiv:2208.09399, 2022. [48] H. Wen, Y. Lin, Y. Xia, H. Wan, R. Zimmermann, and Y. Liang, â€œDiffstg: Probabilistic spatio-temporal graph forecasting with denoising diffusion models,â€ arXiv preprint arXiv:2301.13629, 2023. [49] P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin, â€œClub: A con- trastive log-ratio upper bound of mutual information,â€ in International conference on machine learning. PMLR, 2020, pp. 1779â€“1788. [50] H. Meng, Y. Zhang, Y. Li, and H. Zhao, â€œSpacecraft anomaly detection via transformer reconstruction error,â€ in Proceedings of the Interna- tional Conference on Aerospace System Science and Engineering 2019. Springer, 2020, pp. 351â€“362. [51] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and G. Shroff, â€œLstm-based encoder-decoder for multi-sensor anomaly de- tection,â€ arXiv preprint arXiv:1607.00148, 2016. [52] C. Zhang, D. Song, Y. Chen, X. Feng, C. Lumezanu, W. Cheng, J. Ni, B. Zong, H. Chen, and N. V. Chawla, â€œA deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data,â€ in Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, 2019, pp. 1409â€“1416. [53] Y. Chen, Y. Hao, T. Rakthanmanon, J. Zakaria, B. Hu, and E. n. Keogh, â€œA general framework for never-ending learning from time series streams,â€ Data mining and knowledge discovery, vol. 29, pp. 1622â€“1664, 2015. [54] F. Futami, I. Sato, and M. Sugiyama, â€œVariational inference based on ro- bust divergences,â€ in International Conference on Artificial Intelligence and Statistics. PMLR, 2018, pp. 813â€“822. "
}