{
    "optim": "1\nWeakly Augmented Variational Autoencoder in\nTime Series Anomaly Detection\nZhangkai Wu, Longbing Cao, Senior Member, IEEE, Qi Zhang, Junxian Zhou, Hui Chen\nAbstract—Due to their unsupervised training and uncertainty\nestimation, deep Variational Autoencoders (VAEs) have become\npowerful tools for reconstruction-based Time Series Anomaly\nDetection (TSAD). Existing VAE-based TSAD methods, either\nstatistical or deep, tune meta-priors to estimate the likelihood\nprobability for effectively capturing spatiotemporal dependencies\nin the data. However, these methods confront the challenge of\ninherent data scarcity, which is often the case in anomaly detec-\ntion tasks. Such scarcity easily leads to latent holes, discontinuous\nregions in latent space, resulting in non-robust reconstructions\non these discontinuous spaces. We propose a novel generative\nframework that combines VAEs with self-supervised learning\n(SSL) to address this issue. Our framework augments latent\nrepresentation to mitigate the disruptions caused by anomalies\nin the low-dimensional space, aiming to lead to corresponding\nmore robust reconstructions for detection.\nThis framework marks a significant advancement in VAE\ndesign by integrating SSL to refine likelihood enhancement. Our\nproposed VAE model, specifically tailored for TSAD, augments la-\ntent representations via enhanced training, increasing robustness\nto normal data likelihoods and improving sensitivity to anomalies.\nAdditionally, we present a practical implementation of this\nconceptual framework called the Weakly Augmented Variational\nAutoencoder (WAVAE), which directly augments input data to\nenrich the latent representation. This approach synchronizes the\ntraining of both augmented and raw models and aligns their\nconvergence in data likelihood optimization space. To achieve\nthis, we maximize mutual information within the Evidence Lower\nBound (ELBO), utilizing contrastive learning for shallow learning\nand a discriminator-based adversarial strategy for deep learning.\nExtensive empirical experiments on five public synthetic and\nreal datasets validate the efficacy of our framework. These exper-\niments provide compelling evidence of the superior performance\nof our approach in TSAD, as demonstrated by achieving higher\nROC-AUC and PR-AUC scores compared to state-of-the-art\nmodels. Furthermore, we delve into the nuances of VAE model\ndesign and time series preprocessing, offering comprehensive\nablation studies to examine the sensitivity of various modules\nand hyperparameters in deep optimization.\nIndex Terms—Variational Autoencoder, Time Series Anomaly\nDetection, Self Supervised Learning, Data Augmentation, Con-\ntrast Learning, Adversarial Learning.\nI. INTRODUCTION\nThe work is partially sponsored by Australian Research Council Discovery\nand Future Fellowship grants (DP190101079 and FT190100734).\nZhangkai Wu, Junxian Zhou with the School of Computer Science, the Uni-\nversity of Technology Sydney, 15 Broadway, Ultimo 2007, NSW, Australia.\n(E-mail: berenwu1938@gmail.com, junxian.zhou@student.uts.edu.au)\nHui Chen and Longbing Cao are with the DataX Research Centre and\nSchool of Computing, Macquarie University, NSW 2109, Australia. (email:\nhui.chen2@students.mq.edu.au, longbing.cao@mq.edu.au)\nQi Zhang is with the Department of Computer Science, Tongji University,\nShanghai 201804, China. (E-mail: zhangqi cs@tongji.edu.cn)\nD\nEEP probabilistic generative models have revolutionized\nunsupervised data generation by leveraging the neural\nnetwork’s universal approximation theorem. This innovation\nmanifests in various forms, such as encoding-decoding mecha-\nnisms [1], diffusion-denoise processes [2], and sender-receiver\nin compression [3]. Notably, VAEs have emerged as a central\nfocus in this domain. Deep VAEs empowered by large-scale\nneural networks [4] and the organization of semantic repre-\nsentations [5], [6] have demonstrated exceptional capabilities\nin reconstructing and generating multimodal data, including\nimages [7], [8], tabular [9], [10], and time series [11], [12].\nOwing to their ability to learn generative factors in contin-\nuous and smooth low-dimensional space while fitting data\nlikelihoods, VAEs exhibit robust representation learning capa-\nbilities in disentanglement [13], [14], classification [15], and\nclustering [16]. In particular, VAEs are designed to estimate\nlikelihood distribution learned from most normal samples as a\ndetector, providing an unsupervised and interpretable paradigm\nfor anomaly detection. The underlying assumption is that\nunknown anomaly patterns typically exhibit statistical charac-\nteristics that deviate significantly from the normal distribution.\nRecent research has shown a growing preference for VAEs\nin Time Series Anomaly Detection (TSAD), particularly those\nthat integrate meta-priors [17] into their design. These meth-\nods have been validated to be effective and crucial in capturing\nthe spatiotemporal dependencies within data, thereby enhanc-\ning data likelihood modeling. Specifically, these models often\nassume certain meta-priors, e.g., latent structures, which are\ncrafted using deep learning or probabilistic tools. The goal is\nto accurately represent the likelihood of most data points, en-\nabling the models to detect anomalies effectively. For instance,\ntime-varying priors that adapt to dynamic assumptions [18]–\n[21] have demonstrated effectiveness and powerful capabilities\nin capturing sequence data likelihoods. Additionally, other\nstudies [22], [23] have proposed the creation of task-specific\npriors based on a factorized assumption explicitly designed to\nmodel contextual dependence structure in latent space. Various\nstrategies have been employed to achieve this, such as using\nprototype distribution-based representations optimized through\nmeta-learning and decomposing contextual representations.\nWhether meta-priors are integrated implicitly or explicitly,\nthese methods are fundamentally rooted in model-based de-\nsigns. However, they often cater to specific scenarios and face\ntraining challenges stemming from the data scarcity issue in\ndeep learning or statistical estimation techniques. Addition-\nally, these methods overlook the effective utilization of data,\nwhich becomes particularly problematic when modeling small\nsequence datasets in real-world time-series TSAD scenarios.\narXiv:2401.03341v1  [cs.LG]  7 Jan 2024\n2\nThe issue in VAE-based models easily leads to latent holes\n[24], i.e., discontinuous regions in latent space, resulting in\nnon-robust reconstruction [21], [25]–[28]. The latent holes\nissue occurs when the encoder in these models maps unknown\nanomalies into the latent space without being adequately\ncorrected by enough normal data. Since these anomalies lack\nthe spatiotemporal properties of normal time series data, they\ndisrupt the formation of a continuous and smooth latent space\nfor normal samples. Consequently, representations sampled\nfrom these latent holes fail to accurately reconstruct the input\nsamples, causing a discrepancy between the representations\nand the reconstructed data. This mismatch significantly impairs\nthe anomaly detector’s performance and compromises the\nmodel’s overall robustness. For a more intuitive understanding\nof this phenomenon, refer to Fig. 1.\nRaw Input\nEncode\nLatent Hole\nDecode\nData Likelihood\nSamples\nAugmented Input\nRobust Latent Space\nData Likelihood\nFig. 1.\nComparison of the latent hole phenomenon induced by anomalies\nin Nonrobust VAE-based TSAD Models (upper section) with the robust\nrepresentation learning space fostered by WAVAE (lower section). The upper\npart of the figure delineates the rise of the latent hole within the Nonrobust\nTSAD model and its effect on model robustness. Specifically, anomalous\nsequences xr\nt (depicted within the blue window in the upper section), when\nencoded into the representation space, disrupt the structural integrity of\nthe latent space. This disruption results in latent hole primarily because\nthese anomalous sequences xr\nt lack the spatiotemporal coherence inherent\nin the normal sequence xr\n1. Consequently, sampling from these discontinuous\nregions leads to a mismatch between the representation (indicated by the blue\ndot zt) and its generation (also shown by the blue dot in the likelihood func-\ntion), as illustrated in the upper section, a disproportionately high likelihood\nfunction mass characterizes the representation in the latent space. In such\nscenarios, the TSAD model may erroneously classify an anomaly as normal,\ncompromising its robustness. In contrast, the lower section demonstrates how\ndata augmentation via the WAVAE model can engender a more continuous\nand smoothly distributed data likelihood (as depicted in the central part of\nthe bottom figure). In this context, representation zt) encoded by anomalous\nsequences xa\nt sampled from regions outside the normal latent space are\nassociated with a lower likelihood function mass, thereby enhancing the\nrobustness and efficacy of anomaly detection in TSAD tasks.\nIn light of these challenges, we propose to improve data\nutilization using self-supervised learning (SSL) to enhance\nrepresentation learning and induce latent space robustness.\nSSL [29] enables models to extract more informative repre-\nsentations from unlabeled data, leading to sufficient training.\nTo achieve this, we employ data augmentation on unlabeled\ndatasets through SSL strategies, facilitating the training of\nmodels through contrastive or adversarial methods for the\nTSAD task. Our contributions can be summarized as:\n• Generative self-supervised learning framework for\nTSAD: We present an enhanced generative framework\nusing self-supervised learning. We define a likelihood\nfunction for learning and the derivation of a surrogate\nerror for optimization. This novel approach sets the stage\nfor more effective model design in VAE-based TSAD.\n• Deep and shallow learning in augmented models:\nBuilding upon this framework, we implement the Weakly\nAugmented Variational Autoencoder (WAVA) that in-\ncorporates data augmentation, enabling the model to\nundergo thorough training with support from augmented\ncounterparts. We have also devised two distinct learning\napproaches, deep and shallow, to integrate these two\nmodels effectively.\n• State-of-the-art performance: Extensive experiments on\nfive public datasets demonstrate the effectiveness of our\napproach. We achieved superior performance in ROC-\nAUC and PR-AUC, surpassing state-of-the-art models.\nAdditionally, we provide comprehensive ablation studies\ndelving into the design of the VAE model, time series\npreprocessing, and sensitivity analysis on different mod-\nules and hyperparameters in deep optimization.\n𝑝(𝑥,𝑧!) = 𝑝\" 𝑥 𝑧! 𝑝(𝑧!)\n𝜃 !\n𝑧!\n𝑥 \n𝜙!\nN\n𝑞$! (𝑧!)\nAugmented View in Plate Diagram\nPlain View in Plate Diagram\n𝑝(𝑥%,𝑧%) = 𝑝\" 𝑥% 𝑧% 𝑝(𝑧%)\n𝜃%\n𝑞$\" (𝑧%)\n𝑧%\n𝑥%\n𝜙%\nN\n𝜓!&'()\nFig. 2. Graphical Model for Augmented Variational Autoencoders. Under the\nplate notation rules, a white circle denotes a hidden (or latent) variable, while\na gray circle signifies an observed variable. The variables contained within\nthe square denote local variables, which are independently repeated N times.\nDashed arrow edges imply conditional dependence. Dotted lines represent\nparameters. Referring to the plate diagram, it is evident that our methodology\nencompasses the utilization of two generative models. The inference part of\nmodels, i.e., qϕr and qϕa, encode the raw input, denoted as xr, and the\naugmented input, xa, into their respective low-dimensional representations,\nzr and za. Subsequently, the generative parts of models pθr and pθa, sample\nthe latent space reconstruct the input samples, respectively. We employ a ψ\nparameterized module to synchronize the learning outcomes of both models.\nII. BACKGROUD\nA. Generative model-based Time Series Anomaly Detection\n1) Implicit Data Fitting by Non-probabilistic Generative\nModels: Non-probabilistic generative model-based algorithms\nfor TSAD aim to reconstruct data robustly. Prior works\nhave concentrated on optimizing this reconstructing process\nto match the characteristics of time-series data via the de-\nsign of deep network embeddings. Specifically, [30] and\n[31] implemented an Autoencoder (AE) framework, deploying\nsymmetric encoder-decoder structures and assembling one or\nmultiple CNN-based encoder-decoders for the reconstruction\nof sequence data. Furthermore, [32] utilized a Generative\nAdversarial Network (GAN)-based reconstruction for anomaly\ndetection, implicitly fitting a likelihood function based on the\nnormal data through an adversarial mechanism.\n3\n2) Explicit Data Fitting by Probabilistic generative Models:\nUnlike AE-based models that learn an encoding-decoding\nprocess for datasets, VAE-based models excel in identifying\ncontinuous representations within a low-dimensional space.\nThese representations, characterized by their smooth and con-\ntinuous nature in the hidden space, are essential for preserv-\ning probabilistic properties during sampling. Consequently,\nVAEs can reconstruct samples with increased sharpness and\ninterpretability, outperforming their autoencoder-based coun-\nterparts. In contrast to GANs, VAEs explicitly model the data’s\nlikelihood distribution and provide additional constraints on\nthe data’s posterior distribution based on a preset prior, making\nthem more suitable for modeling data in dynamic areas and\ndesigning end-to-end anomaly detection tasks. For instance,\n[33] utilizes a Gaussian Mixture Model (GMM) assumption\nfor data likelihood distribution, and [34] employs a dynamical\nprior over time.\nIssues in VAE based TSAD: VAEs tend to sacrifice\nrepresentation [35] for data fitting. In that case, the induced\nlatent hole will lead to the lack of robustness Represented\nby latent hole. At the same time, the model’s failure to\nlearn the likelihood of the sequence data exacerbates its\nrobustness issues. Specifically, VAE-based anomaly detection\nalgorithms typically employ a Convolutional Neural Network\n(CNN) architecture for data encoding. While effective for\nimage data, this approach often fails to capture the temporal\ncharacteristics of time-series data, such as seasonality, period-\nicity, and frequency domain features, through CNN encoding\nfilters. The shallow Fully Connected Network (FCN) networks\nare employed in VAEs as substitutes. As a result, the naive\nstructure cannot capture varying dependence, and compared\nto the image data, the sequence in training is relatively small.\nDue to the model and data issue, the generative modal cannot\nconverge to the optimal.\nAdvances in VAE-based TSAD: To remedy this, the\ntraditional variational framework has been revoluted, integrat-\ning the meta-prior in generative modeling. For instance, a\nVariational Recurrent Neural Network (VRNN) has been pro-\nposed, establishing a model for the variational autoencoder’s\ninference, prior, and reconstruction processes by capturing the\ntemporal dependencies between intermediate variable h in the\ndeterministic model and input variables x in the recurrent\nneural network. This approach and its variants [18], [20],\n[21] effectively utilize the variational autoencoder to learn\nand model the latent distribution of data while maintaining\nthe temporal dependence of the recurrent neural network.\nOn the other hand, the variational representation can be\ndesigned. [23] utilizes prototype-based approaches to define\nlatent representations for Multivariate Time Series (MTS) and\nlearn a robust likelihood distribution of normal data.\nB. Self-supervised Learning on Time Series Data in Deter-\nmistic and Generative Models\nIn deterministic models, augmenting time-series data or\ntheir representations, combined with specific self-supervised\nalgorithms, can provide sufficient depth for training in down-\nstream tasks. For instance, in prediction tasks, [36] encodes\ntime-series segments in both time and frequency domains\nto obtain positive and negative sample pairs, using con-\ntrastive learning to capture the seasonal-trend representation\nof time-series data. [37] constructs positive pairs with multi-\ngranularity time-series segments and corresponding latent vari-\nable representations, enhancing fine-grained information for\nprediction by maximizing mutual information.\nIn classification tasks, [38] forms pairwise representations\nof global and local input series, obtaining informational gains\nthrough adversarial learning. For anomaly detection tasks,\n[39] aims to acquire spatio-temporal dependent representa-\ntions suitable for downstream tasks. [40] proposes a multi-\nlayer representation learning framework to obtain consistent,\ncontextual representations of overlapping segments, designing\na contrastive loss by decomposing overlapping subsequences\nin both instance and temporal dimensions to obtain positive\nand negative sample pairs. Additionally, [41] employs a dual\nbilinear process at the encoding level to capture positive and\nnegative samples of time sequences, thereby capturing both\nlong and short-term dependencies.\nIn contrast, SSL based on time-series generative models\ntypically focuses on data and representation augmentation as\na generative approach. For instance, Autoencoder (AE) based\nmethods, such as those presented in [42]–[45], leverage the AE\narchitecture for data augmentation. Similarly, diffusion-based\napproaches, as seen in [46]–[48], employ diffusion processes\nto augment time-series data and representations.\nIII. AUGUMENTATION GUIDED GENERATIVE ANOMALY\nDETECTION\nIn this section, we first provide the problem definition for\ngenerative model-based TSAD. Subsequently, we depict the\nstructure of the augmented generative model in the form of a\nplate diagram, illustrating the random variables and their de-\npendency structure. The augmented guided generative anomaly\ndetection model operates within the framework of probabilistic\ngenerative models, employing self-supervised techniques to\naugment the latent variables z during the training process\nof the generative model, thereby enhancing the deep model’s\nfit to the data likelihood. Herein, we have implemented a\nself-supervised variational autoencoder based on input data\naugmentation, which preprocesses the input data x to generate\nlatent variables za with different views. To align the likeli-\nhood functions of the raw and augmented models, we have\ndeveloped two distinct mutual information loss functions, one\ngrounded in depth and the other in statistics. By maximizing\nthe mutual information between them, we draw the models\ncloser to fitting the same distribution.\nA. Problem Definition\nTime series data is succinctly represented as X\n:=\n{(x(i), y(i))}n\ni=1, encompassing n time-stamped observations\nx ∈ Rc situated within a c-dimensional representation space,\neach paired with a discrete observation y. The observation\ny is assigned discrete values across l predefined classes,\ndelineated as y ∈ {0, 1, . . . , l − 1}. Here, c denotes the\nfeature dimensionality at each time point, categorizing the\n4\ndataset as a Multivariate Time Series (MTS) when c > 1 and\nas a Univariate Time Series (TS) for c = 1. In all figures\nand equations, to enhance notational conciseness, we propose\nabbreviating raw as r and augmentation as a.\nIn generative model-based TSAD, the focus is on learning\na reconstructing model, i.e., Mnormal, that models the mass\nof loglikelihood of the majority of normal data points within\nthe entire dataset X = {Xnormal, Xabnormal}. Anomalies are\nthen identified in an unsupervised, end-to-end fashion by\ncalculating the anomaly score, denoted as AS(x, ˆx), which\nquantifies the difference between a given input x and its\nmodeled copy ˆx as reconstructed by Mnormal. This approach\nis feasible, assuming that the log-likelihood learned from\nnormal observations will diverge notably when encountering\nanomalous data, yielding elevated anomaly scores.\nB. Data\nAugmentation\nGuided\nProbabilistic\nGenerative\nModel\nThe plate diagram in Fig. 2 defines an augmented-based\nprobabilistic generative model. The upper part of the diagram\nspecifies the learned joint distribution of the original data xr\nand its latent variable zr, i.e., p(xr, zr), where the latent\nvariable zr is generated by an inference network qϕr(zr|xr)\nparameterized by ϕr, and the reconstructed variable ˆxr is\nproduced by a generative network pθr(xr|zr) parameterized\nby θr. The model optimizes an approximate surrogate error\nLr\nELBO, comprising a reconstruction loss that maximizes the\nlikelihood distribution Lr\nR and a DKL loss that minimizes the\ndiscrepancy between the prior of the latent variables and their\nvariational posterior Lr\nI, i.e.\nLr\nELBO := Eqϕr(zr|xr)\nh\nlog pθr(xr | zr)\n|\n{z\n}\nLr\nR\ni\n− β DKL\n\u0000qϕ(zr | xr)||p(z)\n|\n{z\n}\nLr\nI\n\u0001\n.\n(1)\nThe lower part of the plate diagram outlines the probabilistic\nmodel of the joint distribution of the augmented view data xa\nand latent variables za, i.e., p(xa, za) with latent variables za\nderived from an augmented inference network qϕa(za|x), i.e.,\nza ∼ qϕa(za|x). Similar to the above, the model optimizes an\naugmented reconstruction loss La\nR and inference loss La\nI , i.e.,\nLa\nELBO := Eqϕa(za|xa)\nh\nlog pθa(xa | za)\n|\n{z\n}\nLa\nR\ni\n− β DKL\n\u0000qϕ(za | xa)||p(z)\n|\n{z\n}\nLa\nI\n\u0001\n.\n(2)\nOn the one hand, both models strive to fit their respective\ndata distribution likelihoods. On the other, we leverage the\nadvantage of data augmentation by maximizing the mutual\ninformation I(zr, za) between two latent models, optimizing\na mutual information loss parameterized by ψ (in deep learning\napproximation), to train the models for maximal data likeli-\nhood synergistically.\nGiven the variety of latent variable augmentations, this pa-\nper augments the raw data to augment the model. In summary,\nwe propose an augmented probabilistic generative model to\nlearn a joint likelihood function p(xr, zr, xa, za) for anomaly\ndetection while simultaneously optimizing an inference net-\nwork parameterized by ϕr, ϕa, a generative network parame-\nterized by θr, θa, and an alignment network parameterized by\nψ. The ψ can be parameterized by neurons in deep learning\napproximation and pseudo-parameters in shallow learning. The\ngenerative process is as follows:\np (xr, xa) =\nZ\np (xr, xa, zr, za) dzrdza,\n(3)\nwhere xr represents the raw input datapoint, xa is the aug-\nmented sample based on the input, zr is the raw latent variable,\nand za is the augmented latent variable.\nThe joint distribution is often too high-dimensional and\nsophisticated to solve it directly. To address this, a tractable\nvariational distribution q(zr, za) is employed as an approxima-\ntion within the framework of Variational Inference (VI). Due\nto the computational convenience it offers, we typically take\nthe logarithm of the distribution. Consequently, as depicted\nin Equation 3, the likelihood of data that encompasses latent\nvariables can be decomposed as follows:\np (xr, xa)\n=\nZ p (xr, xa, zr, za) q(zr, za)\nq(zr, za)\ndzrdza,\n(4)\nand we can get the log versions as follows:\nlog p (xr, xa)\n= log\nZ p (xr, xa, zr, za) q(zr, za)\nq(zr, za)\ndzrdza.\n(5)\nGiven the log is a convex function, we can get a lower bound\nby Jensen’s inequality:\nlog p (xr, xa)\n= log\nZ p (xr, xa, zr, za) q(zr, za)\nq(zr, za)\ndzrdza\n= log Eq(zr,za|xr,xa)\n\u0014 p (xr, xa, zr, za)\nq (zr, za | xr, xa)\n\u0015\n≥Eq(zr,za|xr,xa) log\n\u0014 p (xr, xa, zr, za)\nq (zr, za | xr, xa)\n\u0015\n=Eq(zr,za|xr,xa) log\n\u0014p(xr | zr)p (xa | za) p (zr, za)\nq(zr | xr)q (za | xa)\n\u0015\n= Eq(zr|xr) log[p(xr | zr)] + Eq(za|xa) log [p (xa | za)]\n|\n{z\n}\n1\n+ Eq(zr,za|xr,xa) log\n\u0014\np (zr, za)\nq(zr | xr)q (za | xa)\n\u0015\n|\n{z\n}\n2\n.\n(6)\nAs we can see, the\n1\npart can be decomposed into two\nreconstruction losses, i.e.,\n1\n= Lr\nR + La\nR and the\n2\npart\nin E.q. 6 can be decomposed as:\n5\nEq(zr,za|xr,xa) log\n\u0014\np (zr, za)\nq(zr | xr)q (za | xa)\n\u0015\n=Eq(zr,za|xr,xa) log\n\u0014\np (zr, za) p(zr)p (za)\nq(zr | xr)q (za | xa) p(zr)p (za)\n\u0015\n=Eq(zr,za|xr,xa) log\n\u0014 p (zr, za)\np(zr)p (za)\n\u0015\n+ Eq(zr,za|xr,xa) log\n\u0014\np(zr)p (za)\nq(zr | xr)q (za | xa)\n\u0015\n= Eq(zr,za|xr,xa) log\n\u0014 p (zr, za)\np(zr)p (za)\n\u0015\n|\n{z\n}\nA\n− DKL[q(zr | xr)∥p(zr)]\n|\n{z\n}\ni\n− DKL [q (za | xa) ∥p (za)]\n|\n{z\n}\nii\n,\n(7)\nwhere the\n2\npart can be the combination of two inference\nlosses and mutual information between latent variables, i.e.,\n2 = Lr\nI +La\nI +I(zr, za), where we denote\ni = Lr\nI, ii =\nLa\nI , and A = I(zr, za).\nMinimization of the term denoted by\n1\nleads to an in-\ncreased log-likelihood for both p(xr|zr) and p(xa|za), appli-\ncable to the raw and augmented data perspectives, respectively.\nReducing the inference loss, represented as Lr\nI, La\nI within the\nsection labeled 2 , contributes to a more coherent latent space\nthat facilitates the reconstruction process. Moreover, enhancing\nthe mutual information, denoted as I(zr, za), serves to bridge\nthe disparities between the raw and augmented models. This\nprocess ensures a cohesive framework for incorporating data\naugmentation within the generative model. In conclusion,\nthe proposed objective for learning is to approximate the\njoint distribution p(xr, xa) within an augmentation-informed\ngenerative modeling context, denoted as:\nLAVAE = 1 + A − i − ii\n=Lr\nELBO + La\nELBO + I(zr, za),\n(8)\nwhere LAVAE represents the augmentation based VAE loss.\nΨ!!\nΨ!\"\n𝑧\"\n𝑧#\nz\"\n$\nz#\n$\n1\n0\nq%\n\"\nq%\n#\nUpdate 𝜃,𝜑\nFix 𝜓\n1\n0\nΨ!!\nΨ!#\n𝑧\"\n𝑧#\nz&\n$\nz'\n$\n0\n1\nq%\n\"\nq%\n#\nFix 𝜃,𝜑\nUpdate 𝜓\n0\n1\nFig. 3.\nIllusration of adversarial learning in mutation information approxi-\nmation. In the first stage, the discriminator is frozen to update the parameters\nof Encoders and decoders. In the second stage, We freeze the parameters of\nboth the generator and the discriminator while simultaneously inverting the\npseudo-labels of positive and negative samples to train the discriminator.\nC. Deep and Shallow Learning in Mutual Information Ap-\nproximation\n1) MI approximation in Shallow Learning: We employ a\nLinfoNCE loss to approximate the lower bound of MI. Since\nthis method uses a non-parametric variational distribution in\nvariational inference, it can be considered a form of shallow\nlearning. When the variational distribution q(zr|za) is em-\nployed to approximate the untractable posterior distribution\np(zr|za), as in E.q. (9a), we can derive a lower bound, as\nin E.q. (9b). Specifically, by using an energy-based varia-\ntional function q(zr|za) =\np(zr)\na(za)ef(zr,za), where f(zr, za)\nis a critic value function, and a(za) = Ep(x)\n\u0002\nef(x,y)\u0003\nfor\napproximation, we use the convexity of the log function to\napply Jensen’s inequality to Ep(za)[log a(za)] to further derive\na lower bound, as in E.q. (9c). By utilizing the inequality:\nlog(z) ≤ z\nτ +log(τ)−1, we can approximate further to obtain\nanother lower bound, as in E.q. (9d). Using K samples for an\nunbiased estimate, we obtain E.q. (9e), and through Monte\nCarlo estimation, we can approximate it to the infoNCE loss,\nthat is, LinfoNCE in (9f):\nI(zr, za)\n=Ep(zr,za)\n\u0014\nlog q(zr | za)p(zr|za)\np(zr)q(zr | za)\n\u0015\n(9a)\n=Ep(zr,za)\n\u0014\nlog q(zr | za)\np(zr)\n\u0015\n+ Ep(za)[DKL(p(zr | za)∥q(zr | za))]\n≥Ep(zr,za)[log q(zr | za)]\n(9b)\n≥Ep(zr,za)[f(zr, za)]\n− Ep(za)\n\"\nEp(zr)\n\u0002\nef(zr,za)\u0003\na(za)\n+ log(a(za)) − 1\n#\n(9c)\n≥1 − Ep(z(r,1:K))p(za)\n\"\nef(z(r,1),za)\na\n\u0000za; z(r,1:K)\n\u0001\n#\n+ Ep(z(r,1:K))p(za|z(r,1))\n\"\nlog\nef(z(r,1),za)\na\n\u0000za, z(r,1:K)\n\u0001\n#\n(9d)\n≥E\n\n 1\nK\nK\nX\ni=1\nlog\nef(z(r,i),z(a,i))\n1\nK\nPK\nj=1 ef(z(r,i),z(a,j))\n\n\n(9e)\n≥E\n\"\n1\nK\nK\nX\ni=1\nlog\np\n\u0000z(a,i) | z(r,i)\n\u0001\n1\nK\nPK\nj=1 p\n\u0000z(a,i) | z(r,j)\n\u0001\n#\n≜ LinfoNCE.\n(9f)\nActually, we optimize an infoNCE loss scaled by the temper-\nature coefficient τ:\nLInfoNCE\n= − log\nexp\n\u0000zr,⊤\nu za\nu/τ\n\u0001\nP\nv exp\n\u0010\nzr,⊤\nu zav/τ\n\u0011\n+ P\nv̸=u exp\n\u0010\nzr,⊤\nu zv/τ\n\u0011,\n(10)\nwhere and the negative pairs are none, indicating that the\nnegative keys for a sample are the positive keys for others.\n2) MI approximation in Deep Learning: We can decom-\npose the mutation information into two ratios in E.q. (11a)\nand approximate in density ratio trick, guided by [49]. In\n6\nthat case, the density ratio is approached by a parameterized\nneural network, and we can approximate the MI implicitly in\na deep learning scheme. Specifically, instead of modeling two\ndistributions directly, i.e., q(zr, za) and q(zr), we can learn\nthe ratio r = q(zr,za)\nq(zr)\nin an adversival manner, i.e., training\na discriminator to classify whether the label comes from the\ntarget distribution P or not, as shown in E.q. (11b), where\nthe y is a preset pseudo-label. Since we use the discriminator\nmethod to estimate the mutual information, the upper bound\nis denoted as E.q. (11c).\nEq(zr,za)\nq (zr|za)\nq (za)\n=Eq(zr,za) log q (za|za)\nq(zr)\n(11a)\n≤ log P(y = 1 | zr)\nP(y = 0 | zr) + log P(y = 1 | za)\nP(y = 0 | za)\n(11b)\n≤ log\nΨ(zr)\n1 − Ψ(zr) + log\nΨa(za)\n1 − Ψa(za) ≜ Ladversial\n(11c)\nD. End-to-End Anomaly Detection Training\nThis section proposes an end-to-end TSAD model based on\na weakly augmented generative model.\n1) Weakly Augmentation: In augmentation-based genera-\ntive models, the likelihood fitting is enhanced by reusing train-\ning data. In VAEs, this leads to improved generative models\npθ(x | z) parameterized by θ via enriched data representations\nin the inference network qϕ(z | x) parameterized by ϕ. The\naugmented latent variable, za, is derived as za ∼ q(za|x).\nDuring data preprocessing, we can augment latent representa-\ntions directly by manipulating the input data augmentation,\nrepresented as za ∼ q(za|xa). Here, the augmented input\nxa is obtained from the raw input xr using the augmentation\noperation O, formulated as xa = O(xr).\nData augmentation methods for time series data typically\nrequire an input array of size (batch, time_steps,\nchannel), with manipulations possible in the batch domain\n(such as jittering with noise, scaling, and normalization) or\nin the time domain (including window slicing and warping).\nAdditionally, augmentations can be applied in the frequency\ndomain. These techniques enrich the original dataset through\nvarious methods, effectively enhancing data diversity. This\ndiversification is crucial for models to comprehend better and\npredict time series patterns.\nNevertheless, our findings suggest that applying weak aug-\nmentation to the original input data may yield more favorable\noutcomes for likelihood fitting in anomaly detection tasks.\nSpecifically, weak augmentation involves subtle modifications\nto the data, primarily through different normalization tech-\nniques. These include:\n• Standardization:\nxa = Ostand(xr) = xr − µ\nσ\n,\n(12)\nwhere, µ is the mean and σ the standard deviation of the data.\n• Min-Max Normalization:\nxa = Omm(xr) =\nxr − min(xr)\nmax(xr) − min(xr),\n(13)\nthis scales the data to a specified range, such as 0 to 1.\nSuch moderate adjustments typically preserve the funda-\nmental characteristics and trends of the time series. They\nenable the model to discern the core attributes of the data\nmore effectively, thereby enhancing prediction accuracy. Fur-\nthermore, weak augmentation maintains the data’s authentic-\nity, mitigating the risk of over-distorting the original data\nstructure. This is vital for preserving both the reliability and\ninterpretability of the model.\n2) Training: For the raw input data xr, we first obtain\nits augmented variable xa during the data preprocessing\nphase. Then, we train the inference networks for both the\nraw and augmented perspectives, namely qϕr(zr | xr) and\nqϕa(za | xa), as well as the generative networks pθr(xr | zr)\nand pθa(xa | za). At the same time, we optimize the inference\nand reconstruction losses based on Eq. (1) and E.q. (2).\nDuring the training phase, we employ a strategy of shar-\ning parameters for joint likelihood consolidation in raw one\np(xr, zr) and augmented one p(xa, za) to align the recon-\nstruction effects. This means allowing both the inference\nand reconstruction networks to share the same structure and\nparameters during training. Such an approach reduces the\nnumber of model parameters and increases the generalization\nof the model, enabling it to learn normal patterns of different\ndata for reconstructing normal data.\nIn the variational inference of the joint distribution’s poste-\nrior distribution, we maximize the mutual information of the\ntwo latent variables to encourage the original generator and\nthe augmented generator to tend towards producing similar\ndata distributions. In our actual optimization objectives, i.e.,\nthe surrogated loss LWAVAE, we implemented two methods to\ncontrol the divergence of the two likelihood distributions: the\nLinfoNCE\nWAVAE loss based on contrastive learning:\nLinfoNCE\nWAVAE = 1 + A − i − ii\n=Lr\nELBO + La\nELBO + I(zr, za)\n=Lr\nELBO + La\nELBO + LinfoNCE(zr, za),\n(14)\nand the Ladversial\nWAVAE loss based on adversarial learning:\nLadversial\nWAVAE = 1 + A − i − ii\n=Lr\nELBO + La\nELBO + I(zr, za)\n=Lr\nELBO + La\nELBO + Ladversial(zr, za),\n(15)\nand the discriminator’s performance is optimized in an adver-\nsarial manner, with the specific optimization process illustrated\nin Fig. 3. The first part focuses on maximizing the encoder-\ndecoder capabilities, and the second part involves swapping\npseudo-labels to maximize discriminator loss.\n3) Anomaly Scores: The training process and the determi-\nnation of anomalies are illustrated in Fig. 4. Reconstruction-\nbased anomaly detection utilizes the deviation between the\noriginal data and the reconstructed data as an anomaly score,\ndenoted as AS(x, ˆx). We determine whether the input data\nis anomalous by comparing the anomaly score with a pre-set\nthreshold η. The specific process is shown in Algorithm 1.\n7\nAugmented Input\nEncoder\nEncoder\nSharing Parameters\nDecoder\nDecoder\nSharing Parameters\nRaw Input\nRaw Output\nAnomaly Score\nThreshold\nScore >\nY\nN\nAnomaly\nNormal sequence\nAnomaly Detector\nData Processing\nEncoding\nDecoding\nClose\nRaw Sequence\nAugmented Sequence\nParameters of decoder\nParameters of encoder\nM utal Information\nAlign\nAugmented Output\nData Outputing\nFig. 4. The overall framework of WAVAE, training begins with the raw data xr undergoing an augmentation algorithm AUG, resulting in augmented data\nxa. Concurrently, we train a shared-parameter VAE separately for both sets of data. However, evaluation, i.e., Anomaly detector, is conducted solely on the\noriginal model between raw input xr and its’ reconstruction ˆxr, essentially designing an end-to-end anomaly detector.\nIV. EXPERIMENTS\nA. Benchmarks\nTo validate the effectiveness of our approach, we selected\n16 reconstruction-based models for anomaly detection in time\nseries data as benchmarks, which included 6 generative models\n(GANs and AEs based), which are specifically:\n1) Transformer Autoencoder (TAE) [50]: A transformer\nautoencoder encodes and decodes time-series data to\ncapture temporal dependencies.\n2) The\nMulti-scale\nCNN-RNN\nbased\nAutoencoder\n(MSCREA): A CNN, RNN-based autoencoder leverages\nconvolutional\nneural\nnetworks\nfor\nrepresentation\nextraction across multiple scales and recurrent neural\nnetworks for capturing temporal dependencies, tailored\nfor enhancing anomaly detection in time-series data.\n3) BeatGAN (BGAN) [32]: A GAN-based model for ECG\nanomaly detection, learning normal heartbeats to iden-\ntify irregular patterns in time-series data.\n4) RNN-based Autoencoder (RAE) [51]: A Gated Recur-\nrent Unit (GRU) based autoencoder designed to encode\nand decode time-series data for anomaly detection by\nlearning sequential patterns and temporal relationships.\n5) CNN-based Autoencoder [52]: A CNN-based autoen-\ncoder architecture tailored for time-series analysis, uti-\nlizing convolutional layers to identify spatial patterns\nin data, essential for detecting anomalies in sequential\ndatasets.\n6) RandNet (RN) [30]: An ensemble of randomly struc-\ntured autoencoders with adaptive sampling recognize\nefficiently and robustly detect outliers in data.\nand 10 anomaly detection methods for time-series data based\non probabilistic generative models, namely Variational Au-\ntoencoders, which are specifically:\n1) The Gaussian Mixture Model Variational Autoencoder\n(GMMVAE) [33]: VAE with GMM priors combines the\nprobabilistic framework of Gaussian mixtures with the\ngenerative capabilities of VAEs to model complex dis-\ntributions in time-series data, facilitating robust anomaly\ndetection through learned latent representations.\n2) Variational Autoencoder (VAE) [34]: A traditional VAE\nmodel to model the likelihood of generative data.\n3) Recurrent Neural Network based VAE (RNNVAE) [19]:\nMerges Recurrent Neural Networks (RNN) with the\nvariational approach to autoencoding, capturing tempo-\nral dependencies within sequential data for improved\nanomaly detection through stochastic latent spaces.\n4) The Variational RNN Autoencoder (VRAE) [20]: The\nVariational RNN Autoencoder combines the sequence\nmodeling strengths of RNNs with the probabilistic latent\nspace of variational autoencoders, aiming to improve\nanomaly detection in time-series by learning complex\ntemporal structures.\n5) α-VQRAE, β-VQRAE, and γ-VQRAE [21]: Extensions\nof VRAE, with RNN substituted by a Quasi-Recurrent\nNetwork and α, β, γ-loglikelihood loss to help the VAE\nmodel achieve robust representation.\n6) α-biVQRAE, β-biVQRAE, and γ-biVQRAE [21]: Vari-\nants of VQRAE, with RNN extended to bilevel to\nachieve time dependence on time-series data, while the\nα, β, γ-loglikelihood loss helps the VAE based model\nachieve robust representation.\nB. Experiment Setup\nDatasets: To validate the effectiveness of our proposed\nmethodology, we executed a series of experiments on a quartet\nof multivariate time series datasets: Genesis Demonstrator\n8\nAlgorithm 1: The training process of WAVAE\nInput: Dataset D = {Bi\ntr, Bi\ne}m\ni=1, Training batch\nBtr = {(x(j), y(j)}b\nj=1 ∈ Rb×s×f, Evaluation\nbatch Be ∈ Rb×s×c. // b, s, c is the\nsize of batch, sequence length\nand features\nOutput: Parameters of encoder fϕ, decoder gθ, and\ndiscriminator ψ, Anomaly threshold η.\n1 for each Bi in training batch Btr do\n2\nBi\na = O(Bi); // Here the operation O\nis defined in E.q. (12) and E.q.\n(13).\n3\nBi\na ⊂ Ba;\n4 end\n5 while unconverged do\n6\nfor each Bi, Bi\na in D do\n7\nCompute gradients of Eq. (14) or Eq. (15)\nw.r.t. θ and ϕ;\n8\nUpdate the parameters of f, g;\n9\nend\n10 end\n11 for each Bi\ne in evaluation batch Be do\n12\nˆ\nBj\ne = gθ\n\u0000fϕ(Bj\ne)\n\u0001\n; // Reconstruct the\nsequence\n13\nfor each xi\nr, xi\na in Be do\n14\nScore = AS(xj\nr, xj\na); //Calculate the\nanomaly score based on the\nsimilarity\n15\nif Score < η then\n16\nxi\nr is an anomaly;\n17\nelse\n18\nxi\nr is not an anomaly;\n19\nend\n20\nend\n21 end\nData for Machine Learning, High Storage System Data for\nEnergy Optimization, Electrocardiogram, and Trajectory Data,\nalong with a single univariate time series dataset, the Yahoo\nS5. These datasets, encompassing several hundred temporal\nsequences, are sourced from real-world industrial systems or\nare synthetically generated, comprehensively evaluating the\nalgorithm’s performance.\nThe Genesis Demonstrator dataset for machine learning\n(GD)1 comprises 5 distinct sequences, encapsulating contin-\nuous or discrete signals recorded from portable pick-and-\nplace robots at millisecond intervals. We harness the sequence\nreplete with anomalies to target anomaly detection, specifically\nthe Genesis_AnomalyLabels.csv, which consists of\n16,220 records. Within this framework, records marked with\nclass 0 are designated as normal, whereas the remaining\nclassifications, classes 1 and 2, are delineated as anomalies.\nThe High Storage System Data for Energy Optimiza-\n1https://www.kaggle.com/datasets/inIT-OWL/\ngenesis-demonstrator-data-for-machine-learning\ntion (HSS)2 dataset is composed of 4 sequences docu-\nmenting the readings from induction sensors situated on\nconveyor belts. Anomaly detection is conducted on two\nlabeled\nsequences:\nHRSS_anomalous_standard.csv\nand HRSS_anomalous_optimized.csv, together en-\ncompassing 23,645 records. Within each sequence, records\ntagged with class 0 are categorized as normal, whereas those\nlabeled with class 1 are identified as anomalous.\nThe Electrocardiogram dataset (ECG)3 is comprised of\na solitary time-series sequence collected from PhysioNet\nsignals attributed to a patient with severe congestive heart\nfailure. For the sake of consistent comparison, we fol-\nlowed the guidelines proposed by researcher [53], utiliz-\ning ECG5000_TRAIN.tsv from the training datasets for\nanomaly detection. This approach involves classifying three\nclasses (Supraventricular ectopic beats, PVC, and Unclassifi-\nable events) as anomalies, while the two remaining classes\n(R-on-T Premature Ventricular Contraction (PVC), Normal)\nare maintained as the normative data.\nThe Trajectory Data (TD)4 dataset encapsulates a unique\ntime-series sequence, with each data point being two-\ndimensional. These points represent the detection algorithm’s\naccuracy in delineating the skeletal structure of a hand,\ncoupled with assessments from three human evaluators on\nthe algorithm’s predictive accuracy. We undertake an unsu-\npervised anomaly detection task in this setting using the\nHandOutlines_TRAIN.tsv file extracted from the train-\ning set, comprising 1000 instances. Within this dataset, in-\nstances classified as normal bear the label of class 1, and those\nrecognized as anomalies carry the label of class 0.\nThe Yahoo S5 (S5)5 dataset encompasses an array of both\nauthentic and synthetic time-series sequences. The synthetic\ncomponent of this collection is distinguished by sequences\nthat display diverse trends, degrees of noise, and seasonal\npatterns. Conversely, the real segment of the dataset encap-\nsulates sequences that chronicle the performance metrics of\nassorted Yahoo services. This compilation contains a total of\n367 labeled anomalies across both real and synthetic time\nseries. The dataset is segmented into four distinct subsets: the\nA1, A2, A3, and A4 benchmarks. Within this schema, entries\nmarked with class 0 are categorized as normal, whereas those\nannotated with class 1 are identified as anomalous.\nThe comparison of each dataset concerning data volume,\ndimensionality, temporal span of sequence acquisition, and\nproportion of anomalies is illustrated in Table I.\nAnomaly Scores: The presented code snippet delineates\na section of an anomaly detection algorithm employing a\nVariational Autoencoder (VAE). The VAE is tasked with\nmodeling the probability distribution of the input data. During\nthe anomaly detection phase, the input data is reconstructed,\nand the reconstruction error is computed as the sum of squared\ndifferences between the original data xr and the reconstructed\ndata ˆx. Anomalies are flagged by setting a threshold at the\n2https://www.kaggle.com/datasets/inIT-OWL/\nhigh-storage-system-data-for-energy-optimization\n3https://www.cs.ucr.edu/∼eamonn/time series data 2018/\n4https://www.cs.ucr.edu/∼eamonn/time series data 2018/\n5https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70\n9\nTABLE I\nDATASETS OVERVIEW.\nSequences\nAnomaly Sequences\nAvg. length\nAvg. anomalies\nAvg. Anomaly ratio (%)\nFeatures\nTime Range\nGD\n5\n1\n16,220\n39\n0.24%\n18\nin 760 ms\nHSS\n4\n2\n19,634\n4,517\n23.01%\n18\nin 14 ms\nECG\n1\n1\n500\n31\n6.2%\n2\nNA\nTD\n1\n1\n1,000\n362\n36.2%\n2\nNA\nS5 A1\n67\n67\n1,416\n25\n1.79%\n1\nNA\nS5 A2\n100\n100\n1,421\n5\n0.35%\n1\nNA\nS5 A3\n100\n100\n1,680\n9\n0.54%\n1\nNA\nS5 A4\n100\n100\n1,680\n8\n0.48%\n1\nNA\n99th percentile of the error distribution, isolating the top 1%\nof instances with the highest errors as anomalies.\nPerformance metrics such as F1 score, precision, and recall\nare calculated to gauge the accuracy of the test. These metrics\nare critical in determining the anomaly detection mechanism’s\ntrue positive rate (precision) and sensitivity (recall). Further-\nmore, the algorithm utilizes the confusion matrix C, the F1\nscore, and Cohen’s kappa score κ to evaluate its performance\ncomprehensively.\nThe utilization of the Area Under the Receiver Operating\nCharacteristic curve (AUROC) and the Precision-Recall curve\n(PRAUC) is noteworthy:\n• AUROC is derived by plotting the False Positive Rate\n(FPR) against the True Positive Rate (TPR) and calcu-\nlating the Area Under the Curve (AUC). It is a robust\nmeasure of the classifier’s discriminative power, particu-\nlarly in class imbalance.\n• PRAUC is obtained by plotting precision against the\nrecall and computing the AUC, providing valuable insight\ninto the performance of the positive class, which is often\nthe minority in anomaly detection tasks.\nThe advantages of AUROC and PRAUC include their ability\nto provide a holistic measure of model performance insensitive\nto threshold selection and their effectiveness in conditions of\nclass imbalance. The AUROC reflects the likelihood that the\nclassifier will rank a randomly chosen positive instance more\nhighly than a negative one. On the other hand, the PRAUC\nfocuses on the model’s performance in the positive class,\nmaking it particularly useful for datasets with significant class\nimbalances. The calculation of these metrics is as follows:\n• The function roc_curve computes the FPR and TPR\nfor various threshold values, while the function auc\ncalculates the area under the ROC curve to determine\nthe AUROC.\n• The function precision_recall_curve is em-\nployed to compute the precision and recall at different\nthresholds, with the auc function again used to calculate\nthe area under the precision-recall curve, yielding the\nPRAUC.\nUsing these metrics, the model’s performance can be eval-\nuated not solely based on accuracy but also on its robustness\nagainst false anomaly classifications (precision) and its capa-\nbility to identify all true anomalies (recall).\nImplementation details: Our experimental setup was stan-\ndardized to ensure a level playing field and control for po-\ntential performance biases introduced by Pytorch Versions.\nThe versions selected for all implementations were Python\n3.7.16, PyTorch 1.1.0, NumPy 1.19.2, CUDA toolkit 10.0.130,\nand cuDNN 7.6.5. This approach guaranteed that the pro-\nposed and comparative algorithms were evaluated under equiv-\nalent computational environments. Our hardware setup in-\ncluded NVIDIA Quadro RTX 6000 GPUs with driver version\n525.105.17 and CUDA version 12.0. Additionally, we incor-\nporated a randomness control module, employing seed values\nto govern the stochasticity across computational units such as\nGPU, Python, and PyTorch.\nC. Performance\nThe anomaly detection performance on five public datasets\ncan be found in Tables II,III. In terms of PRAUC and ROCAUC\nmetrics, our method outperformed the baseline across all\ndatasets, regardless of whether they are AE and GAN-based\ngenerative models or VAE-based ones. Note that our compar-\native data originates from [21]. Additionally, we observed that\nmethods based on adversarial mechanisms generally underper-\nform compared to those using contrastive loss.\nD. Sensitivity Analysis\nTo rigorously assess the sensitivity of hyperparameters in\nour model, we have conducted an extensive series of abla-\ntion experiments. This comprehensive evaluation encompasses\nmany hyperparameter sets throughout the entire end-to-end\ntraining process.\n• Specifically, we investigate variations in VAE-related\nhyperparameters such as the β in E.q. (1) and E.q. (2) to\nbalance the inference and reconstruction in VAE training,\nthe dimension of z, and the reconstruction loss LR.\n• We also scrutinize SSL-related hyperparameters like the\nnumber of discriminator layers, the weight of infoVAE\nloss, and the augmentation method.\n• In addition, we delve into hyperparameters pertinent to\ntime-series processing, including the sequence length and\nhidden variables in embeddings.\n• Lastly, we explore adjustments in deep learning hyper-\nparameters, including batch size, learning rates, and the\nnumber of epochs.\n10\nTABLE II\nOVERALL ACCURACY, PR-AUC. FOR EACH DATASET, THE THREE\nBEST-PERFORMING METHODS ARE DENOTED USING DISTINCT MARKINGS:\nBOLD FOR THE TOP METHOD, SUPERSCRIPT ASTERISK∗ FOR THE\nSECOND-BEST, AND UNDERLINE FOR THE THIRD-BEST.\nModels/Datasets\nGD\nHSS\nECG\nTD\nS5\nTAE\n0.088\n0.195\n0.138\n0.175\n0.298\nMSCREA\n0.075\n0.161\n0.105\n0.148\nN/A\nBGAN\n0.109\n0.214\n0.103\n0.151\n0.434\nRAE\n0.128\n0.242\n0.118\n0.163\n0.421\nCAE\n0.116\n0.207\n0.107\n0.177\n0.383\nRN\n0.112\n0.146\n0.105\n0.168\n0.232\nGMMVAE\n0.142\n0.216\n0.163\n0.364\n0.458\nVAE\n0.097\n0.203\n0.131\n0.188\n0.272\nRNNVAE\n0.086\n0.204\n0.079\n0.118\n0.211\nVRAE\n0.131\n0.219\n0.144\n0.165\n0.298\nα-VQRAE\n0.235\n0.225\n0.177\n0.428\n0.487\nβ-VQRAE\n0.242\n0.223\n0.177\n0.427\n0.525\nγ-VQRAE\n0.245\n0.222\n0.184\n0.423\n0.499\nα-biVQRAE\n0.249\n0.227∗\n0.141\n0.429\n0.490\nβ-biVQRAE\n0.256\n0.224\n0.189∗\n0.430\n0.527∗\nγ-biVQRAE\n0.258∗\n0.222\n0.186\n0.432\n0.524\nWAVQRAE-Adverisal\n0.304\n0.286\n0.190\n0.440\n0.805\nWAVQRAE-Contrast\n0.307\n0.358\n0.200\n0.504\n0.838\nTABLE III\nOVERALL ACCURACY, ROC-AUC.\nModels/Datasets\nGD\nHSS\nECG\nTD\nS5\nTAE\n0.652\n0.563∗\n0.542\n0.531\n0.635\nMSCREA\n0.582\n0.509\n0.509\n0.519\nN/A\nBGAN\n0.673\n0.549\n0.547\n0.622\n0.677\nRAE\n0.608\n0.537\n0.552\n0.593\n0.753\nCAE\n0.641\n0.560\n0.574\n0.583\n0.757∗\nRN\n0.731\n0.526\n0.524\n0.533\n0.575\nGMMVAE\n0.763\n0.534\n0.533\n0.531\n0.815\nVAE\n0.664\n0.525\n0.531\n0.643\n0.678\nRNNVAE\n0.595\n0.516\n0.536\n0.574\n0.642\nVRAE\n0.658\n0.521\n0.551\n0.662∗\n0.660\nα-VQRAE\n0.970\n0.529\n0.592\n0.539\n0.858\nβ-VQRAE\n0.968\n0.520\n0.583\n0.535\n0.849\nγ-VQRAE\n0.969\n0.524\n0.598\n0.547\n0.875\nα-biVQRAE\n0.975\n0.538\n0.597\n0.542\n0.873\nβ-biVQRAE\n0.976\n0.527\n0.603∗\n0.546\n0.864\nγ-biVQRAE\n0.978∗\n0.526\n0.601\n0.549\n0.882∗\nWAVQRAE-Adverisal\n0.991\n0.563\n0.612\n0.579\n0.883\nWAVQRAE-Contrast\n0.996\n0.575\n0.630\n0.646\n0.899\nIn each set of experiments, we systematically vary a selected\nhyperparameter within its feasible range while maintaining\nthe default settings for all other hyperparameters to isolate\nand understand the individual impact of each hyperparameter\nadjustment on the model’s performance.\n1) Effect of VAEs: Ideally, VAE is adept at modeling data\ndistributions, encapsulating the potential to fit the likelihood\nof diverse data modalities through its sophisticated encoder-\ndecoder architecture rooted in deep neural networks. Concur-\nrently, it postulates a manifold-based, low-dimensional, contin-\nuous and smooth space. However, in real-world applications,\nthe efficacy of a VAE’s data likelihood estimation is subject to\nsubstantial variability, influenced by the selection of encoder-\ndecoder architectures, the diversity of data modalities, and the\nspecificities of the task at hand. To isolate and assess the\neffects of these factors on anomaly detection performance, we\nembark on a systematic sensitivity analysis of hyperparameters\nspanning three pivotal domains: weight of KL Divergence\nβ, dimensions of latent variables d, and reconstruction loss\nfunction. Through this methodical examination, we aim to\nelucidate the impact of these variables on the VAE’s recon-\nstruction proficiency, thereby enhancing the model’s suitability\nfor anomaly detection endeavors.\nDimensions of Latent variables: The dimension of the\nlatent variable determines the amount of information the\nencoder compresses to maximize the log-likelihood under\nthe Information Bottleneck Theory. Simultaneously, it in-\nfluences the dependency and causality of low-dimensional\nspace representations under the manifold assumption. The\nfundamental assumptions of generative model-based time-\nseries anomaly detection posit that anomaly data will deviate\nfrom the likelihood of normal data. We conducted experi-\nments with varying dimensions of the latent variable z to\ndevelop a robust likelihood function, specifically exploring\n{8, 10, 12, 14, 16, 18, 20}. The outcomes and in-depth analysis\nof these experiments are detailed in Fig. 5 (a), demonstrating\nthe optimal dimension for z.\nKL Divergence Weight: The KL weight controls the\nbalance between representation learning and reconstruction in\nthe VAE model and the information during the compression\nprocess, affecting the model’s robustness during training. We\nuse the hyperparameter β to adjust the VAE’s compression\ncapability. We selected five distinct values for the KL term to\nassess their impact, specifically {1e − 5, 5e − 5, 1e − 4, 5e −\n4, 1e−3}. Detailed results and analysis of this exploration are\npresented in Fig. 5 (b).\nReconstruction Loss Function: In the Eq. (1) and E.q.\n(2), we fit different likelihood distributions by optimizing the\nspecific reconstruction loss. For discrete data, we optimize the\nBinary Cross Entropy (BCE) loss, i.e., LBCE\nR\nto fit the log-\nlikelihood of a multivariate Bernoulli distribution, denoted as:\nLBCE\nR\n=Eqϕ(z|x) [log pθ (x | z)]\n=Eqϕ(z|x)\n\" D\nX\nd=1\nxd log λθ,d (z) + (1 − xd) log (1 − λθ,d (z))\n#\n,\n(16)\nWhere x ∈ {0, 1}D and λ ∈ {0, 1}D are the parameters\nof univariate Bernoulli distributions. For continuous data, we\noptimize the Mean Square Error (MSE) LMSE\nR\nto fit the log-\nlikelihood of a multivariate Gaussian distribution, denoted:\nLMSE\nR\n=Eq(z|x) [log p (x | z)]\n= 1\nD\nD\nX\nd=1\n||xd − ˆxd||2.\n(17)\nWe also tested two robust variants [54] based on the Bernoulli\n11\n(b) KL Divergence Weight 𝛽\na  Dimensions of z − space\n(c) Reconstruction Loss Function\nFig. 5. Sensitivity analysis of VAE related hyperparameters indicates significant findings: (a) reveals that the dimension of z profoundly influences outcomes,\nwith optimal performance when the dimension ranges between 14 and 20. (b) shows that β exerts a minimal effect on optimization, peaking in efficacy at\n0.001. (c) demonstrates the superior performance of the MSE loss function.\n(b) MLP Layers\na  Latent CLR weight\n(c) Scaler\nFig. 6.\nSensitivity analysis of SSL loss related hyperparameters. From (a), it is observed that the weight of the infoNCE Loss has a minimal impact on\nthe overall effectiveness. Conversely, (b) indicates that the number of layers in the discriminator significantly affects the results, with the best performance\nobserved between 2 and 3 layers. (c) illustrates varying augmentation approaches, indicating that using min-max normalization (MinMax) on both original\nand augmented data is the most effective.\nb  Dimensions of h − space\n(a) Sequence Length\nFig. 7. Sensitivity analysis of sequence-related hyperparameters. (a) indicates that the model’s anomaly detection performance is not affected by the length\nof the series. (b) shows that the encoding network achieves the best performance when the hidden state size is 32.\n(b) Learning Rates\na  Batch Sizes\n(c) Epochs\nFig. 8. Sensitivity analysis of deep learning related hyperparameters. (a) A batch size of 64 yields optimal results. (b) The learning rate has minimal impact\non the model. (c) The best performance was observed at 50 epochs.\n12\nlikelihood distribution:\nLrobust1\nR\n=α1 + 1\nα1\n D\nY\nd=1\n(xdˆxα1\nd + (1 − xd) (1 − ˆxd)α1) − 1\n!\n, (18)\nand Gaussian likelihood distribution:\nLrobust2\nR\n=α2 + 1\nα2\n \n1\n(2πσ2)α2D/2 exp\n \n− α2\n2σ2\nD\nX\nd=1\n∥ˆxd − xd∥2\n!\n− 1\n!\n,\n(19)\nwhere α1, α1 are the hyperparameters and σ is the variance.\nThe analysis and comparison of four types of loss functions\nare illustrated in Fig. 5 (c).\n2) Effect of SSL Loss: The SSL Loss in E.q. (14) and\nE.q. (15) will be biased by the approximation methods and\naugmentation types.\nMutual Information Approximation: Our study investi-\ngated two loss functions for mutual information maximization:\nthe infoNCE Loss in contrastive learning, with weight hyper-\nparameters {0.1, 0.2, 0.3, 0.4, 0.5}, detailed in Fig. 6 (a), and\nthe adversarial learning discriminator, varying layers [3, 4, 5,\n6], analyzed in Fig. 6 (b).\nAugumentation Methods: For self-supervised methods ap-\nplied to time-series data, augmentation can be employed to\nmine the intrinsic characteristics of the data, addressing the\nissue of insufficient data for deep models. To validate the\neffectiveness of our approach, we experimented with various\nstrong augmentations that enhance the time dependencies\nand frequency domain representations of time-series data. In\nparallel, we also explored several weak augmentations, specif-\nically normalization techniques applied to time-series data.\nOur findings indicate that the domains transformed by strong\naugmentations are ill-suited for generating robust likelihood\nfunctions, leading to suboptimal results in anomaly detection.\nIn that case, We conducted sensitivity analysis experiments by\ntesting two combinations of weak augmentations. These com-\nbinations include both raw and augmented data using MinMax\n(Fig. 6 (c) MM), raw data with MinMax and augmented data\nwith Standard (Fig. 6 (c) MS), raw data with Standard\nand augmented data with MinMax (Fig. 6 (c) SM), and both\nraw and augmented data using standardization (Fig. 6 (c) SS).\nSpecific experimental results and analysis are presented in Fig.\n6 (c).\n3) Effect of Time Series Processing: The time series’ inher-\nent characteristics, such as the window size in a batch and the\nmemory step length in the encoding model, can impact model\nperformance.\nSequence Length In time-series data analysis, the window\nlength is critical as it sets the data truncation extent, which\nis essential for detecting anomalies with periodicity or spatio-\ntemporal continuity. Furthermore, the length of the time series\nplays a significant role in identifying contextual anomalies.\nWe chose time series lengths of {8, 16, 32, 64, 96} for our\nsensitivity analysis. Detailed experimental results and analyses\nare illustrated in Fig. 7 (a).\nHidden Vairbales: We evaluated the impact of different\nhidden space sizes in the embedding, experimenting with di-\nmensions of {1, 2, 3, 4, 8, 16, 32, 64, 128, 256}. Detailed anal-\nysis and results are presented in Fig. 7 (b).\n4) Effect of Deep Learning: In deep models, batch size,\nlearning rates, and epochs cooperate to guide the model\nconvergence to the optimal.\nBatch Sizes: By modulating the batch size, we gain insights\ninto the stability of gradient updates and their consequent\nimpact on model convergence. To this end, we selected batch\nsizes {32, 64, 128} to empirically ascertain their influence on\nthe model’s performance. Detailed experimental results and\nanalyses are illustrated in Fig. 8 (a).\nLeanring Rates: Step size in gradient descent induced\nby the learning rates is taken during optimization and can\nsignificantly influence the model’s ability to find minima.\nWe test learning rates of {0.001, 0.01, 0.1} and systematically\nstudy their effects and optimize the model’s performance. Fig.\n8 (b) illustrates detailed experimental results and analyses.\nNumber\nof\nEpochs: In the context of unsupervised\nanomaly detection, rather than focusing on model gener-\nalization, we prioritize the impact of training duration on\nperformance. We fix the randomness and maintain consistent\nhyperparameters, testing the same model’s anomaly detection\ncapabilities at epochs {10, 20, 30, 40, 50}. Fig. 8 (c) illustrates\ndetailed experimental results and analyses.\nV. CONCLUSION\nThe VAE-based anomaly detection effectively captures un-\nderlying data distributions in time series analysis. As a result,\nanomalies outside this distribution show notable reconstruction\nerrors. However, the limited amount of data samples can affect\nthe model’s ability to fit this distribution, especially when\nconsidering the rare and hard-to-detect nature of anomalies in\nreal-world situations. To combat data scarcity, we introduce a\nweakly augmented VAE for time series anomaly detection. The\nmodel can achieve a more robust representation in the latent\nspace through joint training on augmented data. Meanwhile,\nwe present two self-supervised strategies, adversarial and\ncontrastive learning, to enhance the performance in data fitting.\nQuantitative experimental results demonstrate that our ap-\nproach exhibits commendable performance across five datasets\nunder two distinct foundational model architectures.\n13\nREFERENCES\n[1] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv\npreprint arXiv:1312.6114, 2013.\n[2] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”\nAdvances in neural information processing systems, vol. 33, pp. 6840–\n6851, 2020.\n[3] A. Graves, R. K. Srivastava, T. Atkinson, and F. Gomez, “Bayesian flow\nnetworks,” arXiv preprint arXiv:2308.07037, 2023.\n[4] A. Vahdat and J. Kautz, “Nvae: A deep hierarchical variational autoen-\ncoder,” Advances in neural information processing systems, vol. 33, pp.\n19 667–19 679, 2020.\n[5] Y. Takida, T. Shibuya, W. Liao, C.-H. Lai, J. Ohmura, T. Uesaka,\nN. Murata, S. Takahashi, T. Kumakura, and Y. Mitsufuji, “Sq-vae:\nVariational bayes on discrete representation with self-annealed stochastic\nquantization,” arXiv preprint arXiv:2205.07547, 2022.\n[6] L. Manduchi, M. Vandenhirtz, A. Ryser, and J. Vogt, “Tree variational\nautoencoders,” arXiv preprint arXiv:2306.08984, 2023.\n[7] A. Razavi, A. Van den Oord, and O. Vinyals, “Generating diverse\nhigh-fidelity images with vq-vae-2,” Advances in neural information\nprocessing systems, vol. 32, 2019.\n[8] Z. Wu, L. Cao, and L. Qi, “evae: Evolutionary variational autoencoder,”\narXiv preprint arXiv:2301.00011, 2023.\n[9] L. Xu, M. Skoularidou, A. Cuesta-Infante, and K. Veeramachaneni,\n“Modeling tabular data using conditional gan,” Advances in neural\ninformation processing systems, vol. 32, 2019.\n[10] A. Kotelnikov, D. Baranchuk, I. Rubachev, and A. Babenko, “Tabddpm:\nModelling tabular data with diffusion models,” in International Confer-\nence on Machine Learning.\nPMLR, 2023, pp. 17 564–17 579.\n[11] H. Zhu, C. Balsells-Rodas, and Y. Li, “Markovian gaussian process\nvariational autoencoders,” in International Conference on Machine\nLearning.\nPMLR, 2023, pp. 42 938–42 961.\n[12] X.-B. Jin, W.-T. Gong, J.-L. Kong, Y.-T. Bai, and T.-L. Su, “Pfvae:\na planar flow-based variational auto-encoder prediction model for time\nseries data,” Mathematics, vol. 10, no. 4, p. 610, 2022.\n[13] X. Liu, J. Yuan, B. An, Y. Xu, Y. Yang, and F. Huang, “C-\ndisentanglement:\nDiscovering\ncausally-independent\ngenerative\nfac-\ntors\nunder\nan\ninductive\nbias\nof\nconfounder,”\narXiv\npreprint\narXiv:2310.17325, 2023.\n[14] Z. Wu and L. Cao, “C2vae: Gaussian copula-based vae differing\ndisentangled from coupled representations with contrastive posterior,”\narXiv preprint arXiv:2309.13303, 2023.\n[15] T. Z. Xiao and R. Bamler, “Trading information between latents in\nhierarchical variational autoencoders,” arXiv preprint arXiv:2302.04855,\n2023.\n[16] S. Tonekaboni, C.-L. Li, S. O. Arik, A. Goldenberg, and T. Pfister, “De-\ncoupling local and global representations of time series,” in International\nConference on Artificial Intelligence and Statistics.\nPMLR, 2022, pp.\n8700–8714.\n[17] M.\nTschannen,\nO.\nBachem,\nand\nM.\nLucic,\n“Recent\nadvances\nin\nautoencoder-based\nrepresentation\nlearning,”\narXiv\npreprint\narXiv:1812.05069, 2018.\n[18] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and Y. Bengio,\n“A recurrent latent variable model for sequential data,” Advances in\nneural information processing systems, vol. 28, 2015.\n[19] D. Park, Y. Hoshi, and C. C. Kemp, “A multimodal anomaly detector\nfor robot-assisted feeding using an lstm-based variational autoencoder,”\nIEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1544–1551,\n2018.\n[20] Y. Su, Y. Zhao, C. Niu, R. Liu, W. Sun, and D. Pei, “Robust anomaly\ndetection for multivariate time series through stochastic recurrent neural\nnetwork,” in Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining, 2019, pp. 2828–\n2837.\n[21] T. Kieu, B. Yang, C. Guo, R.-G. Cirstea, Y. Zhao, Y. Song, and C. S.\nJensen, “Anomaly detection in time series with robust variational quasi-\nrecurrent autoencoders,” in 2022 IEEE 38th International Conference\non Data Engineering (ICDE).\nIEEE, 2022, pp. 1342–1354.\n[22] C.-Y. Lai, F.-K. Sun, Z. Gao, J. H. Lang, and D. S. Boning, “Nominality\nscore conditioned time series anomaly detection by point/sequential\nreconstruction,” arXiv preprint arXiv:2310.15416, 2023.\n[23] Y. Li, W. Chen, B. Chen, D. Wang, L. Tian, and M. Zhou, “Prototype-\noriented unsupervised anomaly detection for multivariate time series,” in\nInternational Conference on Machine Learning, ICML 2023, 23-29 July\n2023, Honolulu, Hawaii, USA, ser. Proceedings of Machine Learning\nResearch, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato,\nand J. Scarlett, Eds., vol. 202.\nPMLR, 2023, pp. 19 407–19 424.\n[Online]. Available: https://proceedings.mlr.press/v202/li23d.html\n[24] A. Khan and A. Storkey, “Adversarial robustness of vaes through the lens\nof local geometry,” in International Conference on Artificial Intelligence\nand Statistics.\nPMLR, 2023, pp. 8954–8967.\n[25] C. Zhou and R. C. Paffenroth, “Anomaly detection with robust deep\nautoencoders,” in Proceedings of the 23rd ACM SIGKDD international\nconference on knowledge discovery and data mining, 2017, pp. 665–674.\n[26] H. Akrami, A. A. Joshi, J. Li, S. Aydore, and R. M. Leahy, “Robust\nvariational autoencoder,” arXiv preprint arXiv:1905.09961, 2019.\n[27] S. Eduardo, A. Naz´abal, C. K. Williams, and C. Sutton, “Robust\nvariational autoencoders for outlier detection and repair of mixed-\ntype data,” in International Conference on Artificial Intelligence and\nStatistics.\nPMLR, 2020, pp. 4056–4066.\n[28] S. Cao, J. Li, K. P. Nelson, and M. A. Kon, “Coupled vae: Improved\naccuracy and robustness of a variational autoencoder,” Entropy, vol. 24,\nno. 3, p. 423, 2022.\n[29] K. Zhang, Q. Wen, C. Zhang, R. Cai, M. Jin, Y. Liu, J. Zhang,\nY. Liang, G. Pang, D. Song et al., “Self-supervised learning for time\nseries analysis: Taxonomy, progress, and prospects,” arXiv preprint\narXiv:2306.10125, 2023.\n[30] J. Chen, S. Sathe, C. Aggarwal, and D. Turaga, “Outlier detection with\nautoencoder ensembles,” in Proceedings of the 2017 SIAM international\nconference on data mining.\nSIAM, 2017, pp. 90–98.\n[31] T. Kieu, B. Yang, and C. S. Jensen, “Outlier detection for multidi-\nmensional time series using deep neural networks,” in 2018 19th IEEE\ninternational conference on mobile data management (MDM).\nIEEE,\n2018, pp. 125–134.\n[32] B. Zhou, S. Liu, B. Hooi, X. Cheng, and J. Ye, “Beatgan: Anomalous\nrhythm detection using adversarially generated time series.” in IJCAI,\nvol. 2019, 2019, pp. 4433–4439.\n[33] W. Liao, Y. Guo, X. Chen, and P. Li, “A unified unsupervised gaussian\nmixture variational autoencoder for high dimensional outlier detection,”\nin 2018 IEEE International Conference on Big Data (Big Data). IEEE,\n2018, pp. 1208–1217.\n[34] H. Xu, W. Chen, N. Zhao, Z. Li, J. Bu, Z. Li, Y. Liu, Y. Zhao, D. Pei,\nY. Feng et al., “Unsupervised anomaly detection via variational auto-\nencoder for seasonal kpis in web applications,” in Proceedings of the\n2018 world wide web conference, 2018, pp. 187–196.\n[35] S. Zhao, J. Song, and S. Ermon, “Infovae: Information maximizing\nvariational autoencoders,” arXiv preprint arXiv:1706.02262, 2017.\n[36] G. Woo, C. Liu, D. Sahoo, A. Kumar, and S. Hoi, “Cost: Contrastive\nlearning of disentangled seasonal-trend representations for time series\nforecasting,” arXiv preprint arXiv:2202.01575, 2022.\n[37] M. Hou, C. Xu, Z. Li, Y. Liu, W. Liu, E. Chen, and J. Bian, “Multi-\ngranularity residual learning with confidence estimation for time series\nprediction,” in Proceedings of the ACM Web Conference 2022, 2022,\npp. 112–121.\n[38] H. Lee, E. Seong, and D.-K. Chae, “Self-supervised learning with\nattention-based latent signal augmentation for sleep staging with limited\nlabeled data,” in Proceedings of the Thirty-First International Joint Con-\nference on Artificial Intelligence, IJCAI-22, LD Raedt, Ed. International\nJoint Conferences on Artificial Intelligence Organization, vol. 7, 2022,\npp. 3868–3876.\n[39] J. Xu, H. Wu, J. Wang, and M. Long, “Anomaly transformer: Time\nseries anomaly detection with association discrepancy,” arXiv preprint\narXiv:2110.02642, 2021.\n[40] Z. Yue, Y. Wang, J. Duan, T. Yang, C. Huang, Y. Tong, and B. Xu,\n“Ts2vec: Towards universal representation of time series,” in Proceed-\nings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 8,\n2022, pp. 8980–8987.\n[41] L. Yang and S. Hong, “Unsupervised time-series representation learning\nwith iterative bilinear temporal-spectral fusion,” in International Con-\nference on Machine Learning.\nPMLR, 2022, pp. 25 038–25 054.\n[42] Z. Wang, X. Xu, W. Zhang, G. Trajcevski, T. Zhong, and F. Zhou,\n“Learning latent seasonal-trend representations for time series forecast-\ning,” Advances in Neural Information Processing Systems, vol. 35, pp.\n38 775–38 787, 2022.\n[43] W. Chen, L. Tian, B. Chen, L. Dai, Z. Duan, and M. Zhou, “Deep\nvariational graph convolutional recurrent network for multivariate time\nseries anomaly detection,” in International Conference on Machine\nLearning.\nPMLR, 2022, pp. 3621–3633.\n[44] S. N. Shukla and B. M. Marlin, “Heteroscedastic temporal varia-\ntional autoencoder for irregularly sampled time series,” arXiv preprint\narXiv:2107.11350, 2021.\n[45] W. Zhang, C. Zhang, and F. Tsung, “Grelen: Multivariate time series\nanomaly detection from the perspective of graph relational learning,”\n14\nin Proceedings of the Thirty-First International Joint Conference on\nArtificial Intelligence, IJCAI-22, vol. 7, 2022, pp. 2390–2397.\n[46] Y. Li, X. Lu, Y. Wang, and D. Dou, “Generative time series forecasting\nwith diffusion, denoise, and disentanglement,” Advances in Neural\nInformation Processing Systems, vol. 35, pp. 23 009–23 022, 2022.\n[47] J. M. L. Alcaraz and N. Strodthoff, “Diffusion-based time series imputa-\ntion and forecasting with structured state space models,” arXiv preprint\narXiv:2208.09399, 2022.\n[48] H. Wen, Y. Lin, Y. Xia, H. Wan, R. Zimmermann, and Y. Liang, “Diffstg:\nProbabilistic spatio-temporal graph forecasting with denoising diffusion\nmodels,” arXiv preprint arXiv:2301.13629, 2023.\n[49] P. Cheng, W. Hao, S. Dai, J. Liu, Z. Gan, and L. Carin, “Club: A con-\ntrastive log-ratio upper bound of mutual information,” in International\nconference on machine learning.\nPMLR, 2020, pp. 1779–1788.\n[50] H. Meng, Y. Zhang, Y. Li, and H. Zhao, “Spacecraft anomaly detection\nvia transformer reconstruction error,” in Proceedings of the Interna-\ntional Conference on Aerospace System Science and Engineering 2019.\nSpringer, 2020, pp. 351–362.\n[51] P. Malhotra, A. Ramakrishnan, G. Anand, L. Vig, P. Agarwal, and\nG. Shroff, “Lstm-based encoder-decoder for multi-sensor anomaly de-\ntection,” arXiv preprint arXiv:1607.00148, 2016.\n[52] C. Zhang, D. Song, Y. Chen, X. Feng, C. Lumezanu, W. Cheng, J. Ni,\nB. Zong, H. Chen, and N. V. Chawla, “A deep neural network for\nunsupervised anomaly detection and diagnosis in multivariate time series\ndata,” in Proceedings of the AAAI conference on artificial intelligence,\nvol. 33, no. 01, 2019, pp. 1409–1416.\n[53] Y. Chen, Y. Hao, T. Rakthanmanon, J. Zakaria, B. Hu, and E. n.\nKeogh, “A general framework for never-ending learning from time series\nstreams,” Data mining and knowledge discovery, vol. 29, pp. 1622–1664,\n2015.\n[54] F. Futami, I. Sato, and M. Sugiyama, “Variational inference based on ro-\nbust divergences,” in International Conference on Artificial Intelligence\nand Statistics.\nPMLR, 2018, pp. 813–822.\n"
}