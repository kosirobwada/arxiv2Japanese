{
    "optim": "SRNDIFF: SHORT-TERM RAINFALL NOWCASTING WITH CONDITION DIFFUSION MODEL ∗ XuDong Ling Faculty of Artificial Intelligence and Big Data Chongqing University of Technology ,Yibin University Yibin 644000,China clearlyzero@stu.cqut.edu.cn ChaoRong Li * Faculty of Artificial Intelligence and Big Data Yibin University Yibin 644000,China lichaorong88@163.com FengQing Qin Faculty of Artificial Intelligence and Big Data Yibin University Yibin 644000,China qinfengqing@163.com Peng Yang Faculty of Artificial Intelligence and Big Data Chongqing University of Technology ,Yibin University Yibin 644000,China 1142065117@QQ.com Yuanyuan Huang Chengdu University of Information Technology Chengdu 610225,China hy@cuit.edu.cn ABSTRACT Diffusion models are widely used in image generation because they can generate high-quality and realistic samples. This is in contrast to generative adversarial networks (GANs) and variational autoencoders (VAEs), which have some limitations in terms of image quality.We introduce the diffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting with condition diffusion model based on historical observational data, which is referred to as SRNDiff. By incorporating an additional conditional decoder module in the denoising process, SRNDiff achieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a denoising network and a conditional Encoder network. The conditional network is composed of multiple independent UNet networks. These networks extract conditional feature maps at different resolutions, providing accurate conditional information that guides the diffusion model for conditional generation.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more computational resources.The SRNDiff model exhibits higher stability and efficiency during training than GANs-based approaches, and generates high-quality precipitation distribution samples that better reflect future actual precipitation conditions. This fully validates the advantages and potential of diffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction. Keywords Diffusion Models · Rainfall prediction · Condition Encoder 1 Introduction In modern society, accurate rainfall information is crucial to various activities. It covers aspects such as flood warning, urban traffic management, and water resource management in agriculture and industry [1]. This critical information is of great value and far-reaching impact to infrastructure managers, emergency services, and the public.Since the 1940s, weather radar, as an active microwave remote sensing device, has been widely used in the field of rainfall ∗corresponding author: ChaoRong Li . email: lichaorong88@163.com arXiv:2402.13737v1  [cs.CV]  21 Feb 2024 observation, which has significantly improved the ability to analyze the spatial characteristics of rainfall. The researchers incrementally used a motion-detection algorithm to derive motion vectors from continuous rainfall measurements from weather radar, and then used these vectors to predict future movement of the rain field. This method is called Lagrangian extrapolation method, which is an effective short-term extrapolation technique, but it only relies on recent observations and does not take into account the non-linearity between rainfalls, and does not consider the entire rainfall process life cycle. In recent years, some studies have improved the extrapolation methods by incorporating background information about the lifecycle of precipitation. For example, they have combined the time variation of radar echo intensity and considered information about the duration of rainfall events to estimate future rainfall conditions [2, 3]. These methods have enhanced the extrapolation techniques to some extent. However, accurately predicting the spatiotemporal evolution of precipitation remains a challenge. several studies [4, 5, 6] have demonstrated the superiority of deep learning approaches over traditional forecasting methods in addressing the non-linear nature of rainfall prediction [7]. However, unresolved issues, such as the lack of accuracy in long-term weather forecasting [8, 9] remain.Using generative models to address the issue of long-term dependencies in rainfall prediction is a highly effective solution.A significant advantage of the generative model is that through sufficient data training, the model can grasp the distribution of rainfall over the entire period of time, so as to achieve accurate prediction of rainfall, not just limited to the prediction of a single point in time. Weather systems often exhibit complex long-term evolution patterns. In rainfall forecasting, the problem of long- term rainfall dependence is particularly important, and generative models can capture these patterns more effectively. Currently, Generative Adversarial Networks(GANs) are a generative model widely used in the field of rainfall pre- diction.GANs consist of two core neural networks: the discriminator and the generator. The discriminator is used to distinguish whether the input is a real sample from the training data set or a fake sample generated by the generator; while the generator is dedicated to generating samples that can \"confuse\" the discriminator, so as to gradually learn to generate an output similar to the real sample . Deep Generative Models of Rainfall (DGMR)[8] is the latest achievement based on deep learning. The model is built on the Conditional Generative Adversarial Network (cGAN)and utilizes various regularization terms to encourage the generation of rainfall predictions that closely resemble real precipita- tion.DGMR can generate realistic rainfall predictions and demonstrate high accuracy in numerical simulations. While the concept of GANs is relatively simple, the adversarial training process often incurs high training costs and may lead to mode collapse issues.GAN and Variational Autoencoders (VAE) have both made significant advancements in generating high-quality samples, but each model has its own limitations. Due to its adversarial training characteristics, the GAN model is not very stable in the training process and is prone to problems such as mode collapse. Moreover, the diversity of samples generated by GAN is low, and it is difficult to capture the whole picture of the training data. The VAE model relies on the loss function of the agent, so the quality of the generated samples is often not as good as that of GAN. At the same time, the hidden space learned by VAE is also relatively vague, which cannot well reflect the internal structure of the data. GANs and VAEs are state-of-the-art in image generation, but their shortcomings make it difficult to extend and apply to new domains.Recently, an attractive generative model—Diffusion Probabilistic Model (DPM) is receiving widespread attention. Some recent large-scale diffusion models, such as DALL·E 2[10], Imagen[11] and Stable Diffusion[12], demonstrate amazing generative capabilities.The key to applying diffusion models to rainfall prediction tasks lies in accurately extracting relevant conditions and effectively utilizing them to guide the prediction process of the diffusion model.Inspired by recent advancements in DDPM , we propose the SRNDiff model.The model achieves end-to-end conditional rainfall prediction by adding an additional conditional decoder module on top of the denoising network.Unlike traditional methods, SRNDiff does not require a separate pre-training of the conditional feature extraction network. Instead, it directly trains the conditional feature extraction network and denoising network together within the DDPM framework, simplifying the model design.The SRNDiff model can directly extract relevant conditions from radar images and use these conditional features to effectively guide the entire diffusion prediction process, thus achieving end-to-end rainfall prediction. In this way, SRNDiff provides a feasible and efficient framework for utilizing diffusion models in rainfall forecasting. 2 Rrelated Work 2.1 UNet-base Encoder-Decoder Encoder-Decoder is the most widely used and useful structure in image processing, by encoding image information into latent representations, and then mapping these latent representations back to image space through the decoder part, thereby generating output with desired properties Image, such as image generation, image segmentation and image reconstruction tasks.The UNet architecture, originally proposed by Ronneberger et al. [13], is an encoder-decoder structure neural network widely used in segmentation tasks due to its simple and efficient feature extraction design. UNet restores the encoder’s feature maps through deconvolution and upsampling operations, generating segmentation results of the same size as the input image. In addition, UNet employs skip connections to concatenate feature maps from 2 Figure 1: Illustration of Encoder-Decoder,UNet and U2Net different encoder and decoder layers to preserve more image detail information and improve segmentation accuracy. Previous studies [10, 12, 14] have highlighted the effectiveness of using the UNet framework for diffusion image generation. UNet is a highly successful foundational network architecture in object detection. It utilizes an encoder-decoder structure to learn semantic information at different levels. Qin et al.[15] further improved detection performance by proposing a nested structure that stacks multiple UNet models on top of each other (as shown in Fig.(1)). The network consists of multiple RSU (Recursive Skip U-Net) modules, which can be seen as miniaturized UNet models. The input to each RSU module comes from different levels of the encoding features. Through this multi-scale feature fusion, the network can learn rich contextual information. Compared to UNet, U2Net has a deeper hierarchy, which expands the receptive field. Additionally, the nested U-connection enables effective feature fusion from different levels. 2.2 Non-generative rainfall prediction Deep learning methods need to combine both temporal and spatial information for rainfall prediction. Researchers [9, 16, 17] have proposed methods to tackle these challenges by extracting and utilizing data from temporal features and image features for precise forecasting. To overcome the limited spatial utilization of LSTM in image processing, Shi et al. [9] introduced the Conv-LSTM model, which integrates multiple Conv-LSTMs for encoding and prediction, providing an end-to-end trainable framework for precipitation forecasting. They later reduced the computational burden of the Conv-LSTM model by proposing the ConvGRU model [17], enabling multiple-frame precipitation prediction using a versatile framework.Chuyao Luo et al. [18] presented an algorithm called RST-LSTM, which builds upon Conv-LSTM and successfully enhances the prediction capability for high radar echo regions by addressing issues related to spatial representation extraction and state connections in traditional convolutions. These approaches have significantly advanced the application of deep learning in precipitation forecasting. Researchers have further improved the accuracy and quality of predictions by incorporating self-attention mechanisms from Transformers with image convolutions [7] and recurrent neural networks [19]. These innovative model structures utilize the shift-invariance and temporal modeling properties of convolutional layers and recurrent neural networks, respectively. Additionally, they utilize the global interaction property of attention mechanisms to enhance the accuracy and robustness of predictions. These methods can be summarized as non-generative prediction methods, the core of which is to establish an Encoder-Decoder network and use a simple loss function to train the network to achieve prediction based on rainfall data. The Encoder-Decoder network is a common architecture used for handling sequential data in rainfall prediction.The network consists of two main parts:Encoder and Decoder.Encoder: The Encoder is responsible for transforming the input data, such as past radar images, into hidden representations or feature vectors. These hidden representations capture essential information from the input data and serve as inputs to the Decoder. Decoder: The decoder takes the output from the encoder, which is the hidden representation or feature vector, and attempts to decode it into the desired output, such as predicting future rainfall images.The network is trained through supervised learning, typically using loss functions like Mean Squared Error (MSE) to measure the difference between the predicted images and the ground truth images, or by quantifying rainfall intensity through pixel-wise classification (as shown in Fig.(2)).However, such methods may result in blurred imaging and suffer from low prediction accuracy, making them difficult to meet the requirements for practical applications[8]. 3 Figure 2: Schematic Diagram of Non-generative Model Rainfall Method 2.3 Generative rainfall prediction A Generative Adversarial Network (GAN) consists of two neural networks, a discriminator and a generator. The main role of the discriminator is to identify the authenticity of the image, which evaluates whether the input image is from real data or fake data generated by the generator. While the generator is trained to generate samples that can fool the discriminator, it gradually learns to generate samples that match the distribution of the training data.When processing time-series data, Generative Adversarial Network (GAN) needs the help of time-series models, such as ConvLSTM, ConvGRU and other recurrent neural networks. For instance, in the work of Liang et al. [20], a generator network architecture is proposed where LSTM modules are employed to model temporal dependencies and long-term patterns effectively.In addition, other works such as MoCoGAN[21] have also adopted variants of LSTM or GRU to handle sequential information. Compared to using only CNN, incorporating recurrent structures can better capture the long-term patterns of sequential data, making it a common technique choice for video generation tasks. The discriminator, when handling spatial and temporal data, often employs a two-discriminator strategy: one for static images and another for sequential data. This results in a dual-stream network structure, as utilized in the Imaginator model proposed by Wang et al.[22], which uses an image discriminator and a video discriminator.The design approach of integrating temporal information into the GAN framework allows GAN models to better generate continuous time-series sequences.With the development of generative models, the researchers found that applying these models to the field of rainfall prediction can generate more realistic rainfall scenarios, beyond the scope of simple loss functions. Generative models tend to predict the distribution of the weather, rather than just predicting the amount or mean of the weather, to paint a more complete picture of future rainfall.In the field of rainfall prediction, there have been numerous research achievements based on GAN models (as shown in Fig.(3)), such as those presented in [23, 24, 25, 26].Among them, the most outstanding method is DGMR (Deep Generative Models of Rainfall) proposed by Ravuri et al.[26]. Compared with other models, this method utilizes the cyclic neural network [17] embedded in the generator, and uses two discriminators to identify the generated samples from the spatial and temporal dimensions, respectively. This strategy ultimately enables the discriminator to generate more realistic and numerically accurate rainfall predictions.The generative methods based 4 on GANs may seem conceptually simple, but training a GAN is still a very challenging process. During training, it is difficult to balance the learning progress between the generator and discriminator, leading to one side learning faster while the other side learning slower, which can cause the model to have difficulty converging or even fail to converge. Additionally, GAN models are prone to mode collapse, where the generator fails to capture the full diversity of the training data and produces limited variations in the generated samples(pattern collapse). Figure 3: (left) Conditional prediction based on GAN, (right) Our proposed diffusion model approach. 2.4 Diffusion models Diffusion model has become one of the most anticipated generative model frameworks [27] with its unique ideas based on the gradual diffusion and reconstruction of noise. The proposal of the diffusion model can be traced back to the work [28] of Sohl et al. in 2015, but due to the limitation of computing power at that time, this research did not attract widespread attention.Until recently, the Denoising diffusion probabilistic models (DDPM)[29] proposed by Jonathan et al. pushed the diffusion model to the forefront in 2020 .In simple terms, the diffusion model is a generative model that transforms Gaussian noise into a learned image distribution through iterative denoising processes. This model can generate corresponding content based on conditions such as labels, text, or image features. The model εθ is trained within a denoising network and can be defined as follows: E[∥ε − εθ( √ αx0 + √ 1 − αε, t, c)∥2] (1) X0 represents the image without added noise, and ε ∼ N(0, 1), where α is a function of time T and c denotes the conditions. In simple terms, the training process involves predicting the added noise given some noise, time, and other information. The reverse process is to iteratively recover the original X0 image from the noise N(0, 1) step by step. Ramesh et al. [10] proposed a two-stage generative model called DALL·E-2, which demonstrated the potential of combining CLIP and diffusion models, successfully achieving conditional generation from text to high-quality images. Specifically, they first pre-trained the CLIP [30] model to extract textual semantic features,at this stage, the CLIP model extracts semantic features from the input text, which are then transformed into image latent space representations by the diffusion model. In the second stage, conditioned on the CLIP text features, the diffusion model or autoregressive model reconstructs and generates the final image from the latent space representation. This two-stage design ensures that the generated images are semantically consistent with the input text. DALL·E-2’s generation results are remarkably realistic, opening up a new direction for text-to-image generation.After DALL·E-2, Saharia et al. proposed a text-to-image generation model called Imagen [11]. Unlike DALL·E-2, Imagen directly utilizes the text features extracted by a text 5 model to guide the diffusion process, without the need to convert text features into image features. Specifically, Imagen uses a pre-trained Transformer text encoder to extract semantic features from the input text. These semantic features are then directly input into the diffusion-based generative model to control the step-by-step reconstruction process from noise to image. This simplifies the text-to-image generation pipeline. To address this issue, Rombach et al. proposed the Latent Diffusion Models (LDM) [12]. The main innovation of LDM is that it first uses a powerful pre-trained autoencoder to compress the images into a lower-dimensional latent space and then performs the diffusion process in this latent space. By encoding the images into the latent space before diffusion, the computational complexity is greatly reduced because the diffusion process’s computational cost is strongly correlated with the data dimension. Compared to the original image space, conducting diffusion in the latent space significantly reduces the computational requirements of LDM. The introduction of LDM alleviates the computational constraints on the application of diffusion models, making it an efficient and feasible generative framework. It provides an effective way to reduce computational costs and improve the practicality of diffusion models.The diffusion models are not only used for text-to-image generation but also widely applied in other fields such as video generation and image segmentation. Its application scope continues to expand, covering a variety of generative tasks. For instance, Jonathan et al. [31] proposed a diffusion model capable of generating videos, which can produce not only short videos but also high-frame-rate slow-motion videos. On the other hand, Wu et al. [32] applied diffusion models to medical image segmentation tasks, exploring its potential in image understanding. These works demonstrate the powerful modeling capabilities of diffusion models.The end-to-end diffusion model image generation refers to the direct generation of images from input data without any intermediate steps, producing the final image result in one step. In contrast, the latent diffusion approach encodes the input data into latent vectors, generates new vectors by diffusing with conditions, and then decodes the latent vectors into images.Although the latent diffusion approach is more resource-efficient compared to the end-to-end approach, it involves two additional steps: encoding and decoding, as well as latent vector transformation. As a result, the generation process is more complex, and there is a possibility of error accumulation. For tasks with low error tolerance, such as rainfall prediction, the end-to-end approach may be more suitable. 3 Method 3.1 SRNDiff In the end-to-end conditional diffusion model, the objective is to predict high-resolution images for future time steps t4 to t8. SRNDiff consists of two core components: the Condition Encoder Net and the Denoise Net. The Condition Encoder takes the image at t0 − t4 as input, and extracts the feature representation of the input image through operations such as convolution. These features contain important prior knowledge related to the target distribution.The conditional features extracted by the encoder contain information such as contours, rainfall intensity, and other relevant details. Next, the conditional features from the encoder are added to the corresponding denoising network at the same level for conditional fusion, guiding the diffusion model to generate the desired content. It is worth noting that the encoder and denoising network are jointly optimized through end-to-end training, allowing the features learned by the encoder to better adapt to the requirements of the denoising network. Compared with multi-stage independent training, the end-to-end mechanism enables each module of the network to cooperate cooperatively, thus effectively avoiding the accumulation of errors. Algorithm 1 Condition DDPM Training 1: while True do 2: x0 ∼ q(x0) 3: t ∼ Uninform({1, . . . , T}) 4: ε ∼ N(0, I) 5: Take gradient descent step on 6: ∇∥ε − εθ(√αtX0 + √1 − atε, t, Condition)∥2 7: end while 3.1.1 Network Structure We adopted the UNet architecture as the denoising network, and the specific model structure is shown in Fig.(4) and Fig.(5)(Fig.(5) shows some of the components used in the model). UNet consists of an encoder and a decoder, with inputs being noisy images of size 256×256×4 and the current time step T.The encoder consists of 5 encoding modules, each utilizing residual blocks to extract shallow semantic information from feature maps and gradually reducing the spatial resolution of the feature maps using pooling layers. In the decoding stage, upsampling is applied to gradually 6 Algorithm 2 DDPM Contidion Sample 1: for (t = T, ..., 1) do 2: if t > 1 then 3: z ∼ N(0, I) 4: else 5: z = 0 6: end if 7: xt−1 = √1 − αt \u0010 xt − √1 − αt 1 1−αt \u0011 εθ(xt, t, Condition) + σtz 8: end for Figure 4: Illustration of Denoise Net restore the spatial resolution, and the feature maps corresponding to the encoder levels are fused to recover richer detail information.The diffusion model typically incorporates self-attention mechanisms in each encoding/decoding module to capture global contextual semantic information. However, considering the high resolution of images, adding too many self-attention modules can lead to a significant increase in computation and parameter volume. Therefore, we only introduce self-attention in the lower-level modules to focus on the global context within the abstract features of the image, striking a balance between computational efficiency and expressive capability.Overall, the denoising network’s encoder captures the details of shallow features through residual learning, while the bottom modules utilize 7 Figure 5: Modules used by the Denoise Net self-attention blocks to learn crucial information. The decoder’s function is to integrate global semantic information from different levels, allowing the network to operate efficiently while maintaining sufficient expressiveness. 3.1.2 Temporal Information Introduction In the introduction of temporal information in the diffusion model, we adopted the mainstream Embedding approach. Specifically, the time step T is mapped to a vector representation, which is then fused with the image features in each encoding/decoding module of the network. This provides the model with contextual information of the time steps. Unlike directly concatenating time steps, Embedding employs a linear layer mapping to learn the semantic representation of the temporal sequence, aiding the model in better perceiving and handling the dynamic changes along the time axis of the image. 3.1.3 Condition Encoder The conditional encoder extracts low level and deep level feature information such as shape, texture, and edge of the image layer by layer from top to bottom. These feature maps are additively fused with the image feature maps of the corresponding layers of the denoising UNet encoder. The fused conditional features are not only passed to the next layer of the encoder, but also passed to the decoder through skip connections. This design enables the conditional information to run through the entire image generation process, and makes full use of the conditional information and image features, so that images that meet the given conditions can be accurately generated. We propose a Triplet Attention UNet (TAU) (show in Fig.(6)) for handling conditional information, based on the RSU module [15] and Triplet Attention [33].Triplet Attention encodes inter-channel and spatial information and participates in the computation of attention. Additionally, through the residual transformation after rotation operations, this mechanism can effectively establish dependencies across dimensions. The structure of Encoder is an excellent balance between computational complexity and performance.TAU consists of multiple Triplet Attention + Conv + BN + ReLU blocks, making it suitable for multi-level feature extraction. The Triplet Attention block is only activated when the height and width of the feature map are less than or equal to 32. By performing downsampling and upsampling operations using the Attention RSU module step by step in the spatial domain, we can capture spatial feature information at different scales.The Condition Encoder is composed of 5 TAU blocks, which undergo multiple downsampling steps to obtain feature maps at different resolutions. In other words, for feature maps at different resolutions, we use different Unet networks, each consisting of TAU.As the conditional information passes through each layer of the Encoder NET, the feature maps are downsampled by a factor of 2 using Downsample layers. Simultaneously, within each TAU block, the Triplet Attention Block, which was initially activated 8 Figure 6: Illustration of Contidion Encoder only in the last 3 layers, gradually transitions to being fully activated throughout the block. This design enables the Condition Net to extract high-quality image information while gradually reducing the complexity of the conditional information. This allows the denoising network to fully utilize the conditional information during the reconstruction stage, improving the accuracy and detail retention capability of image generation. Throughout the entire conditional encoding process, the Condition Net efficiently handles conditional information at different resolutions and combines the flexible activation mechanism of the Triplet Attention Block, ensuring that the generated images accurately represent the input conditional information while maintaining computational speed. 4 Experiment We performed validation and comparative experiments utilizing an open-source nimrod-uk-1km dataset( Fig.(7) shows part of the dataset), encompassing Nimrod rainfall radar information within the United Kingdom from 2016 to 2019. Each sequence within the dataset comprises radar observational data during a two-hour timeframe, with dimensions 1536 × 1280. The highest recorded rainfall intensity in the dataset amounts to 128 mm/hour. Figure 7: This dataset contains 24 images of 256 × 256 pixels size, captured every 5 min for a duration of 120 min. 9 Table 1: Model Training Configuration model (Gen)optimize Discriminator optimize step BatchSize U2Net-GAN Adam|2e − 4 Adam |2e − 5 20M 8 U2Net-GAN* Adam |2e − 4 Adam |2e − 5 20M 8 UNet-GAN Adam |2e − 4 Adam |2e − 5 20M 8 UNet-GAN* Adam |2e − 4 Adam |2e − 5 20M 8 DGMR* Adam |2e − 4 Adam |2e − 5 20M 8 DGMR Adam |2e − 4 Adam |2e − 5 20M 8 SRNDiff Adam |1e − 5 \\ 33M 32 SRNDiffatten Adam |1e − 5 \\ 33M 32 4.1 Evaluating indicator During the stage of evaluating the model’s performance, we applied a binarization process to the generated results and observed images based on the rainfall amount as the threshold. They were divided into the following ranges: 0 ∼ 2mm/h, > 2mm/h, > 4mm/h, > 8mm/h. To We comprehensively evaluate the performance of generator models by selecting critical success index (CSI),FSS fractions skill score (FSS), and Heidke skill score (HSS) evaluation indicators for testing, thereby providing insights into various aspects of the model’s characteristics and performance. CSI is used to evaluate the accuracy of binary predictions when the precipitation amount exceeds a rainfall threshold. CSI is computed using True Positive (TP), False Positive (FP), and False Negative (FN). A higher CSI value indicates better model performance, implying greater accuracy and completeness in rainfall prediction. CSI is defined as Eq.(2),respectively: CSI = TP TP + FP + FN (2) In contrast, FSS offers a significant advantage in predicting rainfall over traditional skill scores. The FSS index provides a comprehensive evaluation by considering the quantity and intensity of predicted heavy rainfall. The FSS values range from 0 to 1, with a higher value representing a higher model prediction accuracy. FSS is defined as Eq.(3),respectively: FSS = 1 − MES(n) MSE(n)ref (3) where Pfi and Poi represent the heavy rainfall amount predicted by the model and observed by radar, respectively. HSS is a statistical method used to measure the accuracy of predictions and evaluate the performance of classifiers in binary classification problems. The HSS ranges from -1 to 1, where 1, 0, and -1 denote accurate, random, and completely incorrect predictions. FSS is defined as Eq.(4),respectively: HSS = TP ∗ TN − FN ∗ FP (TP + TN) ∗ (FN + TN) + (TP + FP)(FP + TN) (4) 4.1.1 SRNDiff Implementation Details We trained two different models: SRNDiffatten (activating the attention block in the conditional network when the feature map size is less than or equal to 32), and the SRNDiff model without the attention block. The diffusion steps were set to 1000, and the Adam Optimizer was used with a learning rate of 1e−5. Each model underwent approximately 33 million training steps and was trained for 72 hours on 10 A6000 GPUs. For additional training configuration details of other models, please refer to Table 1. 4.1.2 GANs Implementation Details We use DGMR as the baseline network, and choose U 2Net and UNet as generators. \"Model*\" means that we train the generator with the following loss function,respectively: LG(θ) = EX1:M+N [ReLU(1 − D(Gθ(Z; X1:M))) +ReLU(1 − T(X1:M; Gθ(Z; X1:M)))] + λLR(θ)]. (5) LR(θ) = 1 HWN ∥EZ[Gθ(Z : X1:M)] − XM+1:M+N ⊙ W(XM+1:M+N)∥ (6) 10 W(i) = max(i, 24) (7) Without the \"*\", the generator is trained using the loss functions mentioned in the reference [8], denoted as equation Eq.(5). We found that the models trained with the loss function in equation Eq.(5) outperformed those trained with the loss function in equation Eq.(8) in terms of predictive performance. As for the discriminator and its loss function, we followed the approach described in the paper [8],respectively: LG(θ) = EX1:M+N[E[D(Gθ(Z; X1:M)) + T(X1:M; Gθ(Z; X1:M))] − λLR(θ)]; (8) LR(θ) = 1 HWN ∥EZ[Gθ(Z : X1:M)] − XM+1:M+N ⊙ W(XM+1:M+N)∥ (9) W(i) = max(i, 24) (10) 4.1.3 Result Table 2: CSI results of precipitation(\"r\" represents precipitation or rainfall.) model r < 2mm/h r > 2mm/h r > 4mm/h r > 8mm/h U2Net-GAN 0.94 0.38 0.29 0.17 U2Net-GAN* 0.94 0.41 0.32 0.19 UNet-GAN 0.95 0.38 0.30 0.16 UNet-GAN* 0.95 0.29 0.18 0.08 DGMR 0.91 0.33 0.25 0.16 DGMR* 0.94 0.36 0.34 0.16 SRNDiff 0.96 0.48 0.329 0.20 SRNDiffatten 0.96 0.49 0.334 0.20 Table 3: HSS results of precipitation (\"r\" represents precipitation or rainfall.) model r > 2mm/h r > 4mm/h r > 8mm/h U2Net-GAN 0.25 0.20 0.13 U2Net-GAN* 0.27 0.23 0.16 UNet-GAN 0.24 0.14 0.07 UNet-GAN* 0.26 0.22 0.13 DGMR 0.23 0.19 0.13 DGMR* 0.24 0.22 0.13 SRNDiff 0.31 0.24 0.16 SRNDiffatten 0.32 0.25 0.17 We use the CSI index to analyze the performance of U2Net-GAN, DGMR, UNet-GAN,SRNDiff and SRNDiff atten in predicting ground rainfall for different rainfall intensities. The Table (2) presents the prediction accuracies of these models under different precipitation intensities. When the precipitation intensity is less than 2h/mm, the prediction accuracies of various methods are close, with the diffusion model slightly outperforming the GANs-based method. However, with increasing rainfall, the advantages of the diffusion model gradually become evident. Specifically, when the rainfall is greater than 2mm/h, the SRNDiff accuracy outperforms U2Net-GAN by up to 8 percentage points in prediction accuracy. Even at precipitation intensities greater than 4 mm/h and 8 mm/h, the diffusion model maintains a certain accuracy advantage. The HSS scores of these models are presented in Table 2. Under rainfall intensity greater than 2 mm/h, SRNDiff outperforms GANs-based prediction methods by 4 percentage points. Moreover, SRNDiff and SRNDiffatten also demonstrate good performance in predicting rainfall exceeding 4 mm/h and 8 mm/h. Among them, SRNDiffatten achieves the highest HSS score, followed by SRNDiff with the second highest. These two evaluation metrics are assessed on a per-pixel basis, thus neither reflects spatial accuracy nor distribution similarity. To comprehensively reflect the accuracy and distribution similarity of model predictions, we calculated the Fractions Skill Score (FSS) and Mean Squared Error (MSE). The FSS metric helps evaluate the consistency between model predictions and actual observations. Across different spatial scales, the closer the FSS value is to 1, the more similar the model predictions are to the actual observations, indicating better performance. Notably, the prediction accuracy of the diffusion model is at least 8 percentage points higher than that of GANs-based methods. This shows that compared to 11 Table 4: The FSS and MES indicator results model FSS ↑ MSE ↓ U2Net-GAN 0.53 2.90 U2Net-GAN* 0.69 1.40 UNet-GAN 0.59 2.54 UNet-GAN* 0.59 1.13 DGMR 0.42 2.32 DGMR* 0.63 1.88 SRNDiff 0.77 0.66 SRNDiffAttention 0.77 0.66 GANs, the diffusion model captures the spatial characteristics of precipitation distribution more accurately, making the predictions closer to actual observations.In addition, we also used the MSE metric. MSE measures the average squared difference between the predicted values and the actual observed values, and a lower MSE indicates that the model’s predictions are closer to the actual observations. From Table (2), we can see that the diffusion model has lower MSE values across different spatial scales and threshold conditions. This further confirms its higher prediction accuracy.We also present the performance improvement brought by the use of attention blocks in the Condition Encoder.In Table (2), Table (3), and Table (4), we compared the performance of the Condition Encoder with and without the attention block using various evaluation metrics. It is evident that, in most metrics (although the improvements in FSS and MSE metrics are relatively small), the Condition Encoder with the Triplet Attention mechanism outperforms the one without it. This confirms the vital role of the attention mechanism in capturing precipitation patterns more accurately in spatial contexts. The introduction of the attention mechanism allows the Condition Encoder to focus more on meaningful information and model complex patterns in precipitation prediction more finely. This ability to locally attend to relevant features enables the model to make better predictions at different scales and spatial ranges, thereby enhancing its generalization capability. The visual analysis shows that the shapes of most generated images predicted by the GANs models deviate from the ground truth. These models produce images that prioritize large rainfall areas while overlooking smaller rainfall areas or isolated pixels, leading to overpredicting high rainfall amounts in the generated images. Although the improved loss function in the DGMR* , UNet-GAN* and U2Net-GAN* model enhances the accuracy of the generated images compared to DGMR and UNet-GAN, inaccuracies in predicting large rainfall areas exist, particularly for UNet- GAN*. Furthermore, these models lose some edge detail information in their predictions.In comparison, SRNDiff and SRNDiffatten demonstrate shapes closer to the ground truth and perform better in predicting heavy rainfall over a large area. The calculated evaluation metrics confirm that SRNDiffatten outperforms all other models. 4.2 Conclusion This study comprehensively analyzes the performance of GAN-based and diffusion-based models in precipitation prediction. The results show significant improvements in CSI and HSS metrics for SRNDiff and SRNDiffatten, especially in scenarios with moderate to heavy rainfall. They outperform GANs-based methods by almost 6 percentage points in FSS and MSE metrics. SRNDiffatten performs better than SRNDiff in terms of CSI and HSS metrics, while the other metrics are relatively close. The exceptional performance of SRNDiffatten can be attributed to two key factors. First, the use of the diffusion model provides stable training and high-quality generation. Second, SRNDiffatten incorporates a UNet network based on nested shapes and possesses a flexible attention activation mechanism, which efficiently extracts deep features for image encoding and decoding. Additionally, the designed end-to-end prediction model enables joint optimization of the encoder and denoising network, allowing the encoder’s learned features to better adapt to the denoising network’s requirements. Compared to independent training, the end-to-end mechanism allows all modules of the network to collaborate and jointly accomplish the task of predicting high-resolution rainfall images. 5 Future Work In this paper, we propose an end-to-end rainfall prediction method based on the diffusion model, named SRNDiff. The model incorporates an additional decoder to guide the diffusion model for conditional generation, enabling the network to better capture key features in rainfall prediction and improve the accuracy and reliability of predictions. However, generating results of size 256×256×4 using this model may require relatively long computation time, approximately 2 minutes. This is due to the complex calculations and inference involved in the generation process, requiring 1000 model 12 Figure 8: Our method’s performance on rainfall prediction, where the top row represents real images. computations. Despite this drawback, we believe that SRNDiff holds significant potential for applications in rainfall prediction and serves as a valuable benchmark model for future research. Going forward, we will further optimize and improve the model to enhance its computational efficiency, allowing for faster and more accurate prediction results. 13 References [1] James W Wilson, Yerong Feng, Min Chen, and Rita D Roberts. Nowcasting challenges during the beijing olympics: Successes, failures, and implications for future nowcasting systems. Weather and Forecasting, 25(6):1691–1714, 2010. [2] Daniel Leuenberger, Alexander Haefele, Nadja Omanovic, Martin Fengler, Giovanni Martucci, Bertrand Calpini, Oliver Fuhrer, and Andrea Rossa. Improving high-impact numerical weather prediction with lidar and drone observations. Bulletin of the American Meteorological Society, 101(7):E1036–E1051, 2020. [3] Amy McGovern, Ryan Lagerquist, David John Gagne, G Eli Jergensen, Kimberly L Elmore, Cameron R Homeyer, and Travis Smith. Making the black box more transparent: Understanding the physical implications of machine learning. Bulletin of the American Meteorological Society, 100(11):2175–2199, 2019. [4] Du Tran, Lubomir Bourdev, Rob Fergus, and et al. Learning spatiotemporal features with 3d convolutional networks. Proceedings of the IEEE international conference on computer vision, 379(2194):4489–4497, 2015. [5] Peng Li, Jie Zhang, and Peter Krebs. Prediction of flow based on a cnn-lstm combined deep learning approach. Water, 14:993, 2022. [6] Daniele Gammelli and Filipe Rodrigues. Recurrent flow networks: A recurrent latent variable model for density estimation of urban mobility. Pattern Recognition, 129:108752, 2022. [7] Katharina Trebing, Tomasz Sta´nczyk, and Siamak Mehrkanoon. Smaat-unet: Precipitation nowcasting using a small attention-unet architecture. Pattern Recognition Letters, 145:178–186, 2021. [8] S. Ravuri, M. Willson, and et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672–677, 2021. [9] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, and Wai-Kin Wong. Convolutional lstm network: A machine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28, 2015. [10] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. [12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684–10695, 2022. [13] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image seg- mentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. [14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780–8794, 2021. [15] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Going deeper with nested u-structure for salient object detection. Pattern recognition, 106:107404, 2020. [16] Jie Liu, Liang Xu, and Ning Chen. A spatiotemporal deep learning model st-lstm-sa for hourly rainfall forecasting using radar echo images. Journal of Hydrology, 609:127748, 2022. [17] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo. Deep learning for precipitation nowcasting: A benchmark and a new model. Advances in neural information processing systems, 30, 2017. [18] Chuyao Luo, Guangning Xu, Xutao Li, and Yunming Ye. The reconstitution predictive network for precipitation nowcasting. Neurocomputing, 507:1–15, 2022. [19] Xueli Zhang, Cankun Zhong, Jianjun Zhang, Ting Wang, and Wing WY Ng. Robust recurrent neural networks for time series forecasting. Neurocomputing, 526:143–157, 2023. [20] Xiaodan Liang, Lisa Lee, Wei Dai, and Eric P Xing. Dual motion gan for future-flow embedded video prediction. In proceedings of the IEEE international conference on computer vision, pages 1744–1752, 2017. [21] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526–1535, 2018. 14 [22] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatio-temporal gan for video generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1160–1169, 2020. [23] Liujia Xu, Dan Niu, Tianbao Zhang, Pengju Chen, Xunlai Chen, and Yinghao Li. Two-stage ua-gan for precipitation nowcasting. Remote Sensing, 14(23):5948, 2022. [24] Ilan Price and Stephan Rasp. Increasing the accuracy and resolution of precipitation forecasts using deep generative models. In International conference on artificial intelligence and statistics, pages 10555–10571. PMLR, 2022. [25] Lucy Harris, Andrew TT McRae, Matthew Chantry, Peter D Dueben, and Tim N Palmer. A generative deep learning approach to stochastic downscaling of precipitation forecasts. Journal of Advances in Modeling Earth Systems, 14(10):e2022MS003120, 2022. [26] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons, Maria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting using deep generative models of radar. Nature, 597(7878):672–677, 2021. [27] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR, 2015. [29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840–6851, 2020. [30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021. [31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models, 2022. [32] Junde Wu, RAO FU, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, and Yanwu Xu. Medsegdiff: Medical image segmentation with diffusion probabilistic model. In Medical Imaging with Deep Learning, 2023. [33] Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, and Qibin Hou. Rotate to attend: Convolutional triplet attention module. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 3139–3148, 2021. 15 "
}