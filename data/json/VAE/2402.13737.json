{
    "optim": "SRNDIFF: SHORT-TERM RAINFALL NOWCASTING WITH\nCONDITION DIFFUSION MODEL ∗\nXuDong Ling\nFaculty of Artificial Intelligence and Big Data\nChongqing University of Technology ,Yibin University\nYibin 644000,China\nclearlyzero@stu.cqut.edu.cn\nChaoRong Li *\nFaculty of Artificial Intelligence and Big Data\nYibin University\nYibin 644000,China\nlichaorong88@163.com\nFengQing Qin\nFaculty of Artificial Intelligence and Big Data\nYibin University\nYibin 644000,China\nqinfengqing@163.com\nPeng Yang\nFaculty of Artificial Intelligence and Big Data\nChongqing University of Technology ,Yibin University\nYibin 644000,China\n1142065117@QQ.com\nYuanyuan Huang\nChengdu University of Information Technology\nChengdu 610225,China\nhy@cuit.edu.cn\nABSTRACT\nDiffusion models are widely used in image generation because they can generate high-quality and\nrealistic samples. This is in contrast to generative adversarial networks (GANs) and variational\nautoencoders (VAEs), which have some limitations in terms of image quality.We introduce the\ndiffusion model to the precipitation forecasting task and propose a short-term precipitation nowcasting\nwith condition diffusion model based on historical observational data, which is referred to as SRNDiff.\nBy incorporating an additional conditional decoder module in the denoising process, SRNDiff\nachieves end-to-end conditional rainfall prediction. SRNDiff is composed of two networks: a\ndenoising network and a conditional Encoder network. The conditional network is composed of\nmultiple independent UNet networks. These networks extract conditional feature maps at different\nresolutions, providing accurate conditional information that guides the diffusion model for conditional\ngeneration.SRNDiff surpasses GANs in terms of prediction accuracy, although it requires more\ncomputational resources.The SRNDiff model exhibits higher stability and efficiency during training\nthan GANs-based approaches, and generates high-quality precipitation distribution samples that better\nreflect future actual precipitation conditions. This fully validates the advantages and potential of\ndiffusion models in precipitation forecasting, providing new insights for enhancing rainfall prediction.\nKeywords Diffusion Models · Rainfall prediction · Condition Encoder\n1\nIntroduction\nIn modern society, accurate rainfall information is crucial to various activities. It covers aspects such as flood warning,\nurban traffic management, and water resource management in agriculture and industry [1]. This critical information\nis of great value and far-reaching impact to infrastructure managers, emergency services, and the public.Since the\n1940s, weather radar, as an active microwave remote sensing device, has been widely used in the field of rainfall\n∗corresponding author: ChaoRong Li . email: lichaorong88@163.com\narXiv:2402.13737v1  [cs.CV]  21 Feb 2024\nobservation, which has significantly improved the ability to analyze the spatial characteristics of rainfall. The researchers\nincrementally used a motion-detection algorithm to derive motion vectors from continuous rainfall measurements from\nweather radar, and then used these vectors to predict future movement of the rain field. This method is called Lagrangian\nextrapolation method, which is an effective short-term extrapolation technique, but it only relies on recent observations\nand does not take into account the non-linearity between rainfalls, and does not consider the entire rainfall process life\ncycle. In recent years, some studies have improved the extrapolation methods by incorporating background information\nabout the lifecycle of precipitation. For example, they have combined the time variation of radar echo intensity and\nconsidered information about the duration of rainfall events to estimate future rainfall conditions [2, 3]. These methods\nhave enhanced the extrapolation techniques to some extent. However, accurately predicting the spatiotemporal evolution\nof precipitation remains a challenge. several studies [4, 5, 6] have demonstrated the superiority of deep learning\napproaches over traditional forecasting methods in addressing the non-linear nature of rainfall prediction [7]. However,\nunresolved issues, such as the lack of accuracy in long-term weather forecasting [8, 9] remain.Using generative models\nto address the issue of long-term dependencies in rainfall prediction is a highly effective solution.A significant advantage\nof the generative model is that through sufficient data training, the model can grasp the distribution of rainfall over the\nentire period of time, so as to achieve accurate prediction of rainfall, not just limited to the prediction of a single point\nin time.\nWeather systems often exhibit complex long-term evolution patterns. In rainfall forecasting, the problem of long-\nterm rainfall dependence is particularly important, and generative models can capture these patterns more effectively.\nCurrently, Generative Adversarial Networks(GANs) are a generative model widely used in the field of rainfall pre-\ndiction.GANs consist of two core neural networks: the discriminator and the generator. The discriminator is used to\ndistinguish whether the input is a real sample from the training data set or a fake sample generated by the generator;\nwhile the generator is dedicated to generating samples that can \"confuse\" the discriminator, so as to gradually learn to\ngenerate an output similar to the real sample . Deep Generative Models of Rainfall (DGMR)[8] is the latest achievement\nbased on deep learning. The model is built on the Conditional Generative Adversarial Network (cGAN)and utilizes\nvarious regularization terms to encourage the generation of rainfall predictions that closely resemble real precipita-\ntion.DGMR can generate realistic rainfall predictions and demonstrate high accuracy in numerical simulations. While\nthe concept of GANs is relatively simple, the adversarial training process often incurs high training costs and may\nlead to mode collapse issues.GAN and Variational Autoencoders (VAE) have both made significant advancements in\ngenerating high-quality samples, but each model has its own limitations. Due to its adversarial training characteristics,\nthe GAN model is not very stable in the training process and is prone to problems such as mode collapse. Moreover, the\ndiversity of samples generated by GAN is low, and it is difficult to capture the whole picture of the training data. The\nVAE model relies on the loss function of the agent, so the quality of the generated samples is often not as good as that\nof GAN. At the same time, the hidden space learned by VAE is also relatively vague, which cannot well reflect the\ninternal structure of the data. GANs and VAEs are state-of-the-art in image generation, but their shortcomings make it\ndifficult to extend and apply to new domains.Recently, an attractive generative model—Diffusion Probabilistic Model\n(DPM) is receiving widespread attention. Some recent large-scale diffusion models, such as DALL·E 2[10], Imagen[11]\nand Stable Diffusion[12], demonstrate amazing generative capabilities.The key to applying diffusion models to rainfall\nprediction tasks lies in accurately extracting relevant conditions and effectively utilizing them to guide the prediction\nprocess of the diffusion model.Inspired by recent advancements in DDPM , we propose the SRNDiff model.The model\nachieves end-to-end conditional rainfall prediction by adding an additional conditional decoder module on top of the\ndenoising network.Unlike traditional methods, SRNDiff does not require a separate pre-training of the conditional\nfeature extraction network. Instead, it directly trains the conditional feature extraction network and denoising network\ntogether within the DDPM framework, simplifying the model design.The SRNDiff model can directly extract relevant\nconditions from radar images and use these conditional features to effectively guide the entire diffusion prediction\nprocess, thus achieving end-to-end rainfall prediction. In this way, SRNDiff provides a feasible and efficient framework\nfor utilizing diffusion models in rainfall forecasting.\n2\nRrelated Work\n2.1\nUNet-base Encoder-Decoder\nEncoder-Decoder is the most widely used and useful structure in image processing, by encoding image information into\nlatent representations, and then mapping these latent representations back to image space through the decoder part,\nthereby generating output with desired properties Image, such as image generation, image segmentation and image\nreconstruction tasks.The UNet architecture, originally proposed by Ronneberger et al. [13], is an encoder-decoder\nstructure neural network widely used in segmentation tasks due to its simple and efficient feature extraction design.\nUNet restores the encoder’s feature maps through deconvolution and upsampling operations, generating segmentation\nresults of the same size as the input image. In addition, UNet employs skip connections to concatenate feature maps from\n2\nFigure 1: Illustration of Encoder-Decoder,UNet and U2Net\ndifferent encoder and decoder layers to preserve more image detail information and improve segmentation accuracy.\nPrevious studies [10, 12, 14] have highlighted the effectiveness of using the UNet framework for diffusion image\ngeneration.\nUNet is a highly successful foundational network architecture in object detection. It utilizes an encoder-decoder\nstructure to learn semantic information at different levels. Qin et al.[15] further improved detection performance by\nproposing a nested structure that stacks multiple UNet models on top of each other (as shown in Fig.(1)). The network\nconsists of multiple RSU (Recursive Skip U-Net) modules, which can be seen as miniaturized UNet models. The input\nto each RSU module comes from different levels of the encoding features. Through this multi-scale feature fusion, the\nnetwork can learn rich contextual information. Compared to UNet, U2Net has a deeper hierarchy, which expands the\nreceptive field. Additionally, the nested U-connection enables effective feature fusion from different levels.\n2.2\nNon-generative rainfall prediction\nDeep learning methods need to combine both temporal and spatial information for rainfall prediction. Researchers\n[9, 16, 17] have proposed methods to tackle these challenges by extracting and utilizing data from temporal features\nand image features for precise forecasting. To overcome the limited spatial utilization of LSTM in image processing,\nShi et al. [9] introduced the Conv-LSTM model, which integrates multiple Conv-LSTMs for encoding and prediction,\nproviding an end-to-end trainable framework for precipitation forecasting. They later reduced the computational burden\nof the Conv-LSTM model by proposing the ConvGRU model [17], enabling multiple-frame precipitation prediction\nusing a versatile framework.Chuyao Luo et al. [18] presented an algorithm called RST-LSTM, which builds upon\nConv-LSTM and successfully enhances the prediction capability for high radar echo regions by addressing issues related\nto spatial representation extraction and state connections in traditional convolutions. These approaches have significantly\nadvanced the application of deep learning in precipitation forecasting. Researchers have further improved the accuracy\nand quality of predictions by incorporating self-attention mechanisms from Transformers with image convolutions [7]\nand recurrent neural networks [19]. These innovative model structures utilize the shift-invariance and temporal modeling\nproperties of convolutional layers and recurrent neural networks, respectively. Additionally, they utilize the global\ninteraction property of attention mechanisms to enhance the accuracy and robustness of predictions. These methods can\nbe summarized as non-generative prediction methods, the core of which is to establish an Encoder-Decoder network\nand use a simple loss function to train the network to achieve prediction based on rainfall data. The Encoder-Decoder\nnetwork is a common architecture used for handling sequential data in rainfall prediction.The network consists of two\nmain parts:Encoder and Decoder.Encoder: The Encoder is responsible for transforming the input data, such as past\nradar images, into hidden representations or feature vectors. These hidden representations capture essential information\nfrom the input data and serve as inputs to the Decoder. Decoder: The decoder takes the output from the encoder, which\nis the hidden representation or feature vector, and attempts to decode it into the desired output, such as predicting\nfuture rainfall images.The network is trained through supervised learning, typically using loss functions like Mean\nSquared Error (MSE) to measure the difference between the predicted images and the ground truth images, or by\nquantifying rainfall intensity through pixel-wise classification (as shown in Fig.(2)).However, such methods may result\nin blurred imaging and suffer from low prediction accuracy, making them difficult to meet the requirements for practical\napplications[8].\n3\nFigure 2: Schematic Diagram of Non-generative Model Rainfall Method\n2.3\nGenerative rainfall prediction\nA Generative Adversarial Network (GAN) consists of two neural networks, a discriminator and a generator. The main\nrole of the discriminator is to identify the authenticity of the image, which evaluates whether the input image is from\nreal data or fake data generated by the generator. While the generator is trained to generate samples that can fool the\ndiscriminator, it gradually learns to generate samples that match the distribution of the training data.When processing\ntime-series data, Generative Adversarial Network (GAN) needs the help of time-series models, such as ConvLSTM,\nConvGRU and other recurrent neural networks.\nFor instance, in the work of Liang et al. [20], a generator network architecture is proposed where LSTM modules\nare employed to model temporal dependencies and long-term patterns effectively.In addition, other works such as\nMoCoGAN[21] have also adopted variants of LSTM or GRU to handle sequential information. Compared to using\nonly CNN, incorporating recurrent structures can better capture the long-term patterns of sequential data, making\nit a common technique choice for video generation tasks. The discriminator, when handling spatial and temporal\ndata, often employs a two-discriminator strategy: one for static images and another for sequential data. This results\nin a dual-stream network structure, as utilized in the Imaginator model proposed by Wang et al.[22], which uses\nan image discriminator and a video discriminator.The design approach of integrating temporal information into the\nGAN framework allows GAN models to better generate continuous time-series sequences.With the development of\ngenerative models, the researchers found that applying these models to the field of rainfall prediction can generate\nmore realistic rainfall scenarios, beyond the scope of simple loss functions. Generative models tend to predict the\ndistribution of the weather, rather than just predicting the amount or mean of the weather, to paint a more complete\npicture of future rainfall.In the field of rainfall prediction, there have been numerous research achievements based on\nGAN models (as shown in Fig.(3)), such as those presented in [23, 24, 25, 26].Among them, the most outstanding\nmethod is DGMR (Deep Generative Models of Rainfall) proposed by Ravuri et al.[26]. Compared with other models,\nthis method utilizes the cyclic neural network [17] embedded in the generator, and uses two discriminators to identify\nthe generated samples from the spatial and temporal dimensions, respectively. This strategy ultimately enables the\ndiscriminator to generate more realistic and numerically accurate rainfall predictions.The generative methods based\n4\non GANs may seem conceptually simple, but training a GAN is still a very challenging process. During training, it is\ndifficult to balance the learning progress between the generator and discriminator, leading to one side learning faster\nwhile the other side learning slower, which can cause the model to have difficulty converging or even fail to converge.\nAdditionally, GAN models are prone to mode collapse, where the generator fails to capture the full diversity of the\ntraining data and produces limited variations in the generated samples(pattern collapse).\nFigure 3: (left) Conditional prediction based on GAN, (right) Our proposed diffusion model approach.\n2.4\nDiffusion models\nDiffusion model has become one of the most anticipated generative model frameworks [27] with its unique ideas based\non the gradual diffusion and reconstruction of noise. The proposal of the diffusion model can be traced back to the\nwork [28] of Sohl et al. in 2015, but due to the limitation of computing power at that time, this research did not attract\nwidespread attention.Until recently, the Denoising diffusion probabilistic models (DDPM)[29] proposed by Jonathan\net al. pushed the diffusion model to the forefront in 2020 .In simple terms, the diffusion model is a generative model\nthat transforms Gaussian noise into a learned image distribution through iterative denoising processes. This model can\ngenerate corresponding content based on conditions such as labels, text, or image features. The model εθ is trained\nwithin a denoising network and can be defined as follows:\nE[∥ε − εθ(\n√\nαx0 +\n√\n1 − αε, t, c)∥2]\n(1)\nX0 represents the image without added noise, and ε ∼ N(0, 1), where α is a function of time T and c denotes the\nconditions. In simple terms, the training process involves predicting the added noise given some noise, time, and other\ninformation. The reverse process is to iteratively recover the original X0 image from the noise N(0, 1) step by step.\nRamesh et al. [10] proposed a two-stage generative model called DALL·E-2, which demonstrated the potential of\ncombining CLIP and diffusion models, successfully achieving conditional generation from text to high-quality images.\nSpecifically, they first pre-trained the CLIP [30] model to extract textual semantic features,at this stage, the CLIP model\nextracts semantic features from the input text, which are then transformed into image latent space representations by the\ndiffusion model. In the second stage, conditioned on the CLIP text features, the diffusion model or autoregressive model\nreconstructs and generates the final image from the latent space representation. This two-stage design ensures that the\ngenerated images are semantically consistent with the input text. DALL·E-2’s generation results are remarkably realistic,\nopening up a new direction for text-to-image generation.After DALL·E-2, Saharia et al. proposed a text-to-image\ngeneration model called Imagen [11]. Unlike DALL·E-2, Imagen directly utilizes the text features extracted by a text\n5\nmodel to guide the diffusion process, without the need to convert text features into image features. Specifically, Imagen\nuses a pre-trained Transformer text encoder to extract semantic features from the input text. These semantic features are\nthen directly input into the diffusion-based generative model to control the step-by-step reconstruction process from\nnoise to image. This simplifies the text-to-image generation pipeline. To address this issue, Rombach et al. proposed\nthe Latent Diffusion Models (LDM) [12]. The main innovation of LDM is that it first uses a powerful pre-trained\nautoencoder to compress the images into a lower-dimensional latent space and then performs the diffusion process\nin this latent space. By encoding the images into the latent space before diffusion, the computational complexity is\ngreatly reduced because the diffusion process’s computational cost is strongly correlated with the data dimension.\nCompared to the original image space, conducting diffusion in the latent space significantly reduces the computational\nrequirements of LDM. The introduction of LDM alleviates the computational constraints on the application of diffusion\nmodels, making it an efficient and feasible generative framework. It provides an effective way to reduce computational\ncosts and improve the practicality of diffusion models.The diffusion models are not only used for text-to-image\ngeneration but also widely applied in other fields such as video generation and image segmentation. Its application\nscope continues to expand, covering a variety of generative tasks. For instance, Jonathan et al. [31] proposed a diffusion\nmodel capable of generating videos, which can produce not only short videos but also high-frame-rate slow-motion\nvideos. On the other hand, Wu et al. [32] applied diffusion models to medical image segmentation tasks, exploring its\npotential in image understanding. These works demonstrate the powerful modeling capabilities of diffusion models.The\nend-to-end diffusion model image generation refers to the direct generation of images from input data without any\nintermediate steps, producing the final image result in one step. In contrast, the latent diffusion approach encodes the\ninput data into latent vectors, generates new vectors by diffusing with conditions, and then decodes the latent vectors\ninto images.Although the latent diffusion approach is more resource-efficient compared to the end-to-end approach, it\ninvolves two additional steps: encoding and decoding, as well as latent vector transformation. As a result, the generation\nprocess is more complex, and there is a possibility of error accumulation. For tasks with low error tolerance, such as\nrainfall prediction, the end-to-end approach may be more suitable.\n3\nMethod\n3.1\nSRNDiff\nIn the end-to-end conditional diffusion model, the objective is to predict high-resolution images for future time steps\nt4 to t8. SRNDiff consists of two core components: the Condition Encoder Net and the Denoise Net. The Condition\nEncoder takes the image at t0 − t4 as input, and extracts the feature representation of the input image through operations\nsuch as convolution. These features contain important prior knowledge related to the target distribution.The conditional\nfeatures extracted by the encoder contain information such as contours, rainfall intensity, and other relevant details.\nNext, the conditional features from the encoder are added to the corresponding denoising network at the same level\nfor conditional fusion, guiding the diffusion model to generate the desired content. It is worth noting that the encoder\nand denoising network are jointly optimized through end-to-end training, allowing the features learned by the encoder\nto better adapt to the requirements of the denoising network. Compared with multi-stage independent training, the\nend-to-end mechanism enables each module of the network to cooperate cooperatively, thus effectively avoiding the\naccumulation of errors.\nAlgorithm 1 Condition DDPM Training\n1: while True do\n2:\nx0 ∼ q(x0)\n3:\nt ∼ Uninform({1, . . . , T})\n4:\nε ∼ N(0, I)\n5:\nTake gradient descent step on\n6:\n∇∥ε − εθ(√αtX0 + √1 − atε, t, Condition)∥2\n7: end while\n3.1.1\nNetwork Structure\nWe adopted the UNet architecture as the denoising network, and the specific model structure is shown in Fig.(4) and\nFig.(5)(Fig.(5) shows some of the components used in the model). UNet consists of an encoder and a decoder, with\ninputs being noisy images of size 256×256×4 and the current time step T.The encoder consists of 5 encoding modules,\neach utilizing residual blocks to extract shallow semantic information from feature maps and gradually reducing the\nspatial resolution of the feature maps using pooling layers. In the decoding stage, upsampling is applied to gradually\n6\nAlgorithm 2 DDPM Contidion Sample\n1: for (t = T, ..., 1) do\n2:\nif t > 1 then\n3:\nz ∼ N(0, I)\n4:\nelse\n5:\nz = 0\n6:\nend if\n7:\nxt−1 = √1 − αt\n\u0010\nxt − √1 − αt\n1\n1−αt\n\u0011\nεθ(xt, t, Condition) + σtz\n8: end for\nFigure 4: Illustration of Denoise Net\nrestore the spatial resolution, and the feature maps corresponding to the encoder levels are fused to recover richer detail\ninformation.The diffusion model typically incorporates self-attention mechanisms in each encoding/decoding module\nto capture global contextual semantic information. However, considering the high resolution of images, adding too\nmany self-attention modules can lead to a significant increase in computation and parameter volume. Therefore, we\nonly introduce self-attention in the lower-level modules to focus on the global context within the abstract features\nof the image, striking a balance between computational efficiency and expressive capability.Overall, the denoising\nnetwork’s encoder captures the details of shallow features through residual learning, while the bottom modules utilize\n7\nFigure 5: Modules used by the Denoise Net\nself-attention blocks to learn crucial information. The decoder’s function is to integrate global semantic information\nfrom different levels, allowing the network to operate efficiently while maintaining sufficient expressiveness.\n3.1.2\nTemporal Information Introduction\nIn the introduction of temporal information in the diffusion model, we adopted the mainstream Embedding approach.\nSpecifically, the time step T is mapped to a vector representation, which is then fused with the image features in\neach encoding/decoding module of the network. This provides the model with contextual information of the time\nsteps. Unlike directly concatenating time steps, Embedding employs a linear layer mapping to learn the semantic\nrepresentation of the temporal sequence, aiding the model in better perceiving and handling the dynamic changes along\nthe time axis of the image.\n3.1.3\nCondition Encoder\nThe conditional encoder extracts low level and deep level feature information such as shape, texture, and edge of the\nimage layer by layer from top to bottom. These feature maps are additively fused with the image feature maps of the\ncorresponding layers of the denoising UNet encoder. The fused conditional features are not only passed to the next layer\nof the encoder, but also passed to the decoder through skip connections. This design enables the conditional information\nto run through the entire image generation process, and makes full use of the conditional information and image features,\nso that images that meet the given conditions can be accurately generated. We propose a Triplet Attention UNet (TAU)\n(show in Fig.(6)) for handling conditional information, based on the RSU module [15] and Triplet Attention [33].Triplet\nAttention encodes inter-channel and spatial information and participates in the computation of attention. Additionally,\nthrough the residual transformation after rotation operations, this mechanism can effectively establish dependencies\nacross dimensions.\nThe structure of Encoder is an excellent balance between computational complexity and performance.TAU consists\nof multiple Triplet Attention + Conv + BN + ReLU blocks, making it suitable for multi-level feature extraction. The\nTriplet Attention block is only activated when the height and width of the feature map are less than or equal to 32.\nBy performing downsampling and upsampling operations using the Attention RSU module step by step in the spatial\ndomain, we can capture spatial feature information at different scales.The Condition Encoder is composed of 5 TAU\nblocks, which undergo multiple downsampling steps to obtain feature maps at different resolutions. In other words,\nfor feature maps at different resolutions, we use different Unet networks, each consisting of TAU.As the conditional\ninformation passes through each layer of the Encoder NET, the feature maps are downsampled by a factor of 2 using\nDownsample layers. Simultaneously, within each TAU block, the Triplet Attention Block, which was initially activated\n8\nFigure 6: Illustration of Contidion Encoder\nonly in the last 3 layers, gradually transitions to being fully activated throughout the block. This design enables the\nCondition Net to extract high-quality image information while gradually reducing the complexity of the conditional\ninformation. This allows the denoising network to fully utilize the conditional information during the reconstruction\nstage, improving the accuracy and detail retention capability of image generation. Throughout the entire conditional\nencoding process, the Condition Net efficiently handles conditional information at different resolutions and combines\nthe flexible activation mechanism of the Triplet Attention Block, ensuring that the generated images accurately represent\nthe input conditional information while maintaining computational speed.\n4\nExperiment\nWe performed validation and comparative experiments utilizing an open-source nimrod-uk-1km dataset( Fig.(7) shows\npart of the dataset), encompassing Nimrod rainfall radar information within the United Kingdom from 2016 to 2019.\nEach sequence within the dataset comprises radar observational data during a two-hour timeframe, with dimensions\n1536 × 1280. The highest recorded rainfall intensity in the dataset amounts to 128 mm/hour.\nFigure 7: This dataset contains 24 images of 256 × 256 pixels size, captured every 5 min for a duration of 120 min.\n9\nTable 1: Model Training Configuration\nmodel\n(Gen)optimize\nDiscriminator optimize\nstep\nBatchSize\nU2Net-GAN\nAdam|2e − 4\nAdam |2e − 5\n20M\n8\nU2Net-GAN*\nAdam |2e − 4\nAdam |2e − 5\n20M\n8\nUNet-GAN\nAdam |2e − 4\nAdam |2e − 5\n20M\n8\nUNet-GAN*\nAdam |2e − 4\nAdam |2e − 5\n20M\n8\nDGMR*\nAdam |2e − 4\nAdam |2e − 5\n20M\n8\nDGMR\nAdam |2e − 4\nAdam |2e − 5\n20M\n8\nSRNDiff\nAdam |1e − 5\n\\\n33M\n32\nSRNDiffatten\nAdam |1e − 5\n\\\n33M\n32\n4.1\nEvaluating indicator\nDuring the stage of evaluating the model’s performance, we applied a binarization process to the generated results\nand observed images based on the rainfall amount as the threshold. They were divided into the following ranges:\n0 ∼ 2mm/h, > 2mm/h, > 4mm/h, > 8mm/h. To We comprehensively evaluate the performance of generator\nmodels by selecting critical success index (CSI),FSS fractions skill score (FSS), and Heidke skill score (HSS) evaluation\nindicators for testing, thereby providing insights into various aspects of the model’s characteristics and performance.\nCSI is used to evaluate the accuracy of binary predictions when the precipitation amount exceeds a rainfall threshold.\nCSI is computed using True Positive (TP), False Positive (FP), and False Negative (FN). A higher CSI value indicates\nbetter model performance, implying greater accuracy and completeness in rainfall prediction. CSI is defined as\nEq.(2),respectively:\nCSI =\nTP\nTP + FP + FN\n(2)\nIn contrast, FSS offers a significant advantage in predicting rainfall over traditional skill scores. The FSS index provides\na comprehensive evaluation by considering the quantity and intensity of predicted heavy rainfall. The FSS values range\nfrom 0 to 1, with a higher value representing a higher model prediction accuracy. FSS is defined as Eq.(3),respectively:\nFSS = 1 −\nMES(n)\nMSE(n)ref\n(3)\nwhere Pfi and Poi represent the heavy rainfall amount predicted by the model and observed by radar, respectively. HSS\nis a statistical method used to measure the accuracy of predictions and evaluate the performance of classifiers in binary\nclassification problems. The HSS ranges from -1 to 1, where 1, 0, and -1 denote accurate, random, and completely\nincorrect predictions. FSS is defined as Eq.(4),respectively:\nHSS =\nTP ∗ TN − FN ∗ FP\n(TP + TN) ∗ (FN + TN) + (TP + FP)(FP + TN)\n(4)\n4.1.1\nSRNDiff Implementation Details\nWe trained two different models: SRNDiffatten (activating the attention block in the conditional network when the\nfeature map size is less than or equal to 32), and the SRNDiff model without the attention block. The diffusion steps\nwere set to 1000, and the Adam Optimizer was used with a learning rate of 1e−5. Each model underwent approximately\n33 million training steps and was trained for 72 hours on 10 A6000 GPUs. For additional training configuration details\nof other models, please refer to Table 1.\n4.1.2\nGANs Implementation Details\nWe use DGMR as the baseline network, and choose U 2Net and UNet as generators. \"Model*\" means that we train the\ngenerator with the following loss function,respectively:\nLG(θ) =\nEX1:M+N [ReLU(1 − D(Gθ(Z; X1:M)))\n+ReLU(1 − T(X1:M; Gθ(Z; X1:M)))] + λLR(θ)].\n(5)\nLR(θ) =\n1\nHWN ∥EZ[Gθ(Z : X1:M)] − XM+1:M+N ⊙ W(XM+1:M+N)∥\n(6)\n10\nW(i) = max(i, 24)\n(7)\nWithout the \"*\", the generator is trained using the loss functions mentioned in the reference [8], denoted as equation\nEq.(5). We found that the models trained with the loss function in equation Eq.(5) outperformed those trained with the\nloss function in equation Eq.(8) in terms of predictive performance. As for the discriminator and its loss function, we\nfollowed the approach described in the paper [8],respectively:\nLG(θ) = EX1:M+N[E[D(Gθ(Z; X1:M)) + T(X1:M; Gθ(Z; X1:M))] − λLR(θ)];\n(8)\nLR(θ) =\n1\nHWN ∥EZ[Gθ(Z : X1:M)] − XM+1:M+N ⊙ W(XM+1:M+N)∥\n(9)\nW(i) = max(i, 24)\n(10)\n4.1.3\nResult\nTable 2: CSI results of precipitation(\"r\" represents precipitation or rainfall.)\nmodel\nr < 2mm/h\nr > 2mm/h\nr > 4mm/h\nr > 8mm/h\nU2Net-GAN\n0.94\n0.38\n0.29\n0.17\nU2Net-GAN*\n0.94\n0.41\n0.32\n0.19\nUNet-GAN\n0.95\n0.38\n0.30\n0.16\nUNet-GAN*\n0.95\n0.29\n0.18\n0.08\nDGMR\n0.91\n0.33\n0.25\n0.16\nDGMR*\n0.94\n0.36\n0.34\n0.16\nSRNDiff\n0.96\n0.48\n0.329\n0.20\nSRNDiffatten\n0.96\n0.49\n0.334\n0.20\nTable 3: HSS results of precipitation (\"r\" represents precipitation or rainfall.)\nmodel\nr > 2mm/h\nr > 4mm/h\nr > 8mm/h\nU2Net-GAN\n0.25\n0.20\n0.13\nU2Net-GAN*\n0.27\n0.23\n0.16\nUNet-GAN\n0.24\n0.14\n0.07\nUNet-GAN*\n0.26\n0.22\n0.13\nDGMR\n0.23\n0.19\n0.13\nDGMR*\n0.24\n0.22\n0.13\nSRNDiff\n0.31\n0.24\n0.16\nSRNDiffatten\n0.32\n0.25\n0.17\nWe use the CSI index to analyze the performance of U2Net-GAN, DGMR, UNet-GAN,SRNDiff and SRNDiff atten in\npredicting ground rainfall for different rainfall intensities. The Table (2) presents the prediction accuracies of these\nmodels under different precipitation intensities. When the precipitation intensity is less than 2h/mm, the prediction\naccuracies of various methods are close, with the diffusion model slightly outperforming the GANs-based method.\nHowever, with increasing rainfall, the advantages of the diffusion model gradually become evident. Specifically, when\nthe rainfall is greater than 2mm/h, the SRNDiff accuracy outperforms U2Net-GAN by up to 8 percentage points in\nprediction accuracy. Even at precipitation intensities greater than 4 mm/h and 8 mm/h, the diffusion model maintains a\ncertain accuracy advantage. The HSS scores of these models are presented in Table 2. Under rainfall intensity greater\nthan 2 mm/h, SRNDiff outperforms GANs-based prediction methods by 4 percentage points. Moreover, SRNDiff\nand SRNDiffatten also demonstrate good performance in predicting rainfall exceeding 4 mm/h and 8 mm/h. Among\nthem, SRNDiffatten achieves the highest HSS score, followed by SRNDiff with the second highest. These two\nevaluation metrics are assessed on a per-pixel basis, thus neither reflects spatial accuracy nor distribution similarity.\nTo comprehensively reflect the accuracy and distribution similarity of model predictions, we calculated the Fractions\nSkill Score (FSS) and Mean Squared Error (MSE). The FSS metric helps evaluate the consistency between model\npredictions and actual observations. Across different spatial scales, the closer the FSS value is to 1, the more similar the\nmodel predictions are to the actual observations, indicating better performance. Notably, the prediction accuracy of the\ndiffusion model is at least 8 percentage points higher than that of GANs-based methods. This shows that compared to\n11\nTable 4: The FSS and MES indicator results\nmodel\nFSS ↑\nMSE ↓\nU2Net-GAN\n0.53\n2.90\nU2Net-GAN*\n0.69\n1.40\nUNet-GAN\n0.59\n2.54\nUNet-GAN*\n0.59\n1.13\nDGMR\n0.42\n2.32\nDGMR*\n0.63\n1.88\nSRNDiff\n0.77\n0.66\nSRNDiffAttention\n0.77\n0.66\nGANs, the diffusion model captures the spatial characteristics of precipitation distribution more accurately, making the\npredictions closer to actual observations.In addition, we also used the MSE metric. MSE measures the average squared\ndifference between the predicted values and the actual observed values, and a lower MSE indicates that the model’s\npredictions are closer to the actual observations. From Table (2), we can see that the diffusion model has lower MSE\nvalues across different spatial scales and threshold conditions. This further confirms its higher prediction accuracy.We\nalso present the performance improvement brought by the use of attention blocks in the Condition Encoder.In Table (2),\nTable (3), and Table (4), we compared the performance of the Condition Encoder with and without the attention block\nusing various evaluation metrics. It is evident that, in most metrics (although the improvements in FSS and MSE metrics\nare relatively small), the Condition Encoder with the Triplet Attention mechanism outperforms the one without it. This\nconfirms the vital role of the attention mechanism in capturing precipitation patterns more accurately in spatial contexts.\nThe introduction of the attention mechanism allows the Condition Encoder to focus more on meaningful information\nand model complex patterns in precipitation prediction more finely. This ability to locally attend to relevant features\nenables the model to make better predictions at different scales and spatial ranges, thereby enhancing its generalization\ncapability.\nThe visual analysis shows that the shapes of most generated images predicted by the GANs models deviate from the\nground truth. These models produce images that prioritize large rainfall areas while overlooking smaller rainfall areas\nor isolated pixels, leading to overpredicting high rainfall amounts in the generated images. Although the improved\nloss function in the DGMR* , UNet-GAN* and U2Net-GAN* model enhances the accuracy of the generated images\ncompared to DGMR and UNet-GAN, inaccuracies in predicting large rainfall areas exist, particularly for UNet-\nGAN*. Furthermore, these models lose some edge detail information in their predictions.In comparison, SRNDiff and\nSRNDiffatten demonstrate shapes closer to the ground truth and perform better in predicting heavy rainfall over a large\narea. The calculated evaluation metrics confirm that SRNDiffatten outperforms all other models.\n4.2\nConclusion\nThis study comprehensively analyzes the performance of GAN-based and diffusion-based models in precipitation\nprediction. The results show significant improvements in CSI and HSS metrics for SRNDiff and SRNDiffatten,\nespecially in scenarios with moderate to heavy rainfall. They outperform GANs-based methods by almost 6 percentage\npoints in FSS and MSE metrics. SRNDiffatten performs better than SRNDiff in terms of CSI and HSS metrics, while\nthe other metrics are relatively close.\nThe exceptional performance of SRNDiffatten can be attributed to two key factors. First, the use of the diffusion model\nprovides stable training and high-quality generation. Second, SRNDiffatten incorporates a UNet network based on\nnested shapes and possesses a flexible attention activation mechanism, which efficiently extracts deep features for\nimage encoding and decoding. Additionally, the designed end-to-end prediction model enables joint optimization of\nthe encoder and denoising network, allowing the encoder’s learned features to better adapt to the denoising network’s\nrequirements. Compared to independent training, the end-to-end mechanism allows all modules of the network to\ncollaborate and jointly accomplish the task of predicting high-resolution rainfall images.\n5\nFuture Work\nIn this paper, we propose an end-to-end rainfall prediction method based on the diffusion model, named SRNDiff. The\nmodel incorporates an additional decoder to guide the diffusion model for conditional generation, enabling the network\nto better capture key features in rainfall prediction and improve the accuracy and reliability of predictions. However,\ngenerating results of size 256×256×4 using this model may require relatively long computation time, approximately 2\nminutes. This is due to the complex calculations and inference involved in the generation process, requiring 1000 model\n12\nFigure 8: Our method’s performance on rainfall prediction, where the top row represents real images.\ncomputations. Despite this drawback, we believe that SRNDiff holds significant potential for applications in rainfall\nprediction and serves as a valuable benchmark model for future research. Going forward, we will further optimize and\nimprove the model to enhance its computational efficiency, allowing for faster and more accurate prediction results.\n13\nReferences\n[1] James W Wilson, Yerong Feng, Min Chen, and Rita D Roberts. Nowcasting challenges during the beijing olympics:\nSuccesses, failures, and implications for future nowcasting systems. Weather and Forecasting, 25(6):1691–1714,\n2010.\n[2] Daniel Leuenberger, Alexander Haefele, Nadja Omanovic, Martin Fengler, Giovanni Martucci, Bertrand Calpini,\nOliver Fuhrer, and Andrea Rossa. Improving high-impact numerical weather prediction with lidar and drone\nobservations. Bulletin of the American Meteorological Society, 101(7):E1036–E1051, 2020.\n[3] Amy McGovern, Ryan Lagerquist, David John Gagne, G Eli Jergensen, Kimberly L Elmore, Cameron R Homeyer,\nand Travis Smith. Making the black box more transparent: Understanding the physical implications of machine\nlearning. Bulletin of the American Meteorological Society, 100(11):2175–2199, 2019.\n[4] Du Tran, Lubomir Bourdev, Rob Fergus, and et al. Learning spatiotemporal features with 3d convolutional\nnetworks. Proceedings of the IEEE international conference on computer vision, 379(2194):4489–4497, 2015.\n[5] Peng Li, Jie Zhang, and Peter Krebs. Prediction of flow based on a cnn-lstm combined deep learning approach.\nWater, 14:993, 2022.\n[6] Daniele Gammelli and Filipe Rodrigues. Recurrent flow networks: A recurrent latent variable model for density\nestimation of urban mobility. Pattern Recognition, 129:108752, 2022.\n[7] Katharina Trebing, Tomasz Sta´nczyk, and Siamak Mehrkanoon. Smaat-unet: Precipitation nowcasting using a\nsmall attention-unet architecture. Pattern Recognition Letters, 145:178–186, 2021.\n[8] S. Ravuri, M. Willson, and et al. Skilful precipitation nowcasting using deep generative models of radar. Nature,\n597(7878):672–677, 2021.\n[9] Xingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, and Wai-Kin Wong. Convolutional lstm network: A\nmachine learning approach for precipitation nowcasting. Advances in neural information processing systems, 28,\n2015.\n[10] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image\ngeneration with clip latents. arXiv preprint arXiv:2204.06125, 2022.\n[11] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models\nwith deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022.\n[12] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image\nsynthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 10684–10695, 2022.\n[13] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image seg-\nmentation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International\nConference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015.\n[14] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural\ninformation processing systems, 34:8780–8794, 2021.\n[15] Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. U2-net:\nGoing deeper with nested u-structure for salient object detection. Pattern recognition, 106:107404, 2020.\n[16] Jie Liu, Liang Xu, and Ning Chen. A spatiotemporal deep learning model st-lstm-sa for hourly rainfall forecasting\nusing radar echo images. Journal of Hydrology, 609:127748, 2022.\n[17] Xingjian Shi, Zhihan Gao, Leonard Lausen, Hao Wang, Dit-Yan Yeung, Wai-kin Wong, and Wang-chun Woo.\nDeep learning for precipitation nowcasting: A benchmark and a new model. Advances in neural information\nprocessing systems, 30, 2017.\n[18] Chuyao Luo, Guangning Xu, Xutao Li, and Yunming Ye. The reconstitution predictive network for precipitation\nnowcasting. Neurocomputing, 507:1–15, 2022.\n[19] Xueli Zhang, Cankun Zhong, Jianjun Zhang, Ting Wang, and Wing WY Ng. Robust recurrent neural networks for\ntime series forecasting. Neurocomputing, 526:143–157, 2023.\n[20] Xiaodan Liang, Lisa Lee, Wei Dai, and Eric P Xing. Dual motion gan for future-flow embedded video prediction.\nIn proceedings of the IEEE international conference on computer vision, pages 1744–1752, 2017.\n[21] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content\nfor video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages\n1526–1535, 2018.\n14\n[22] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. Imaginator: Conditional spatio-temporal\ngan for video generation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision,\npages 1160–1169, 2020.\n[23] Liujia Xu, Dan Niu, Tianbao Zhang, Pengju Chen, Xunlai Chen, and Yinghao Li.\nTwo-stage ua-gan for\nprecipitation nowcasting. Remote Sensing, 14(23):5948, 2022.\n[24] Ilan Price and Stephan Rasp. Increasing the accuracy and resolution of precipitation forecasts using deep generative\nmodels. In International conference on artificial intelligence and statistics, pages 10555–10571. PMLR, 2022.\n[25] Lucy Harris, Andrew TT McRae, Matthew Chantry, Peter D Dueben, and Tim N Palmer. A generative deep\nlearning approach to stochastic downscaling of precipitation forecasts. Journal of Advances in Modeling Earth\nSystems, 14(10):e2022MS003120, 2022.\n[26] Suman Ravuri, Karel Lenc, Matthew Willson, Dmitry Kangin, Remi Lam, Piotr Mirowski, Megan Fitzsimons,\nMaria Athanassiadou, Sheleem Kashem, Sam Madge, et al. Skilful precipitation nowcasting using deep generative\nmodels of radar. Nature, 597(7878):672–677, 2021.\n[27] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A\nsurvey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\n[28] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256–2265. PMLR,\n2015.\n[29] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840–6851, 2020.\n[30] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning, pages 8748–8763. PMLR, 2021.\n[31] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video\ndiffusion models, 2022.\n[32] Junde Wu, RAO FU, Huihui Fang, Yu Zhang, Yehui Yang, Haoyi Xiong, Huiying Liu, and Yanwu Xu. Medsegdiff:\nMedical image segmentation with diffusion probabilistic model. In Medical Imaging with Deep Learning, 2023.\n[33] Diganta Misra, Trikay Nalamada, Ajay Uppili Arasanipalai, and Qibin Hou. Rotate to attend: Convolutional\ntriplet attention module. In Proceedings of the IEEE/CVF winter conference on applications of computer vision,\npages 3139–3148, 2021.\n15\n"
}