{
    "optim": "InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Bowen Lei 1 Rajarshi Guhaniyogi 1 Krishnendu Chandra 1 Aaron Scheffler 2 Bani Mallick 1 for the Alzheimer’s Disease Neuroimaging Initiative* 3 Abstract There is a significant interest in exploring non- linear associations among multiple images de- rived from diverse imaging modalities. While there is a growing literature on image-on-image regression to delineate predictive inference of an image based on multiple images, existing ap- proaches have limitations in efficiently borrowing information between multiple imaging modalities in the prediction of an image. Building on the literature of Variational Auto Encoders (VAEs), this article proposes a novel approach, referred to as Integrative Variational Autoencoder (InVA) method, which borrows information from mul- tiple images obtained from different sources to draw predictive inference of an image. The pro- posed approach captures complex non-linear asso- ciation between the outcome image and input im- ages, while allowing rapid computation. Numer- ical results demonstrate substantial advantages of InVA over VAEs, which typically do not al- low borrowing information between input images. The proposed framework offers highly accurate predictive inferences for costly positron emission topography (PET) from multiple measures of cor- tical structure in human brain scans readily avail- able from magnetic resonance imaging (MRI). 1Department of Statistics, Texas A&M University, College Station, U.S. 2University of California, San Francisco, San Francisco, U.S. 3*Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative(ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp- content/uploads/how to apply/ADNI Acknowledgement List.pdf. Correspondence to: Bani Mallick <bmallick@stat.tamu.edu>. 1. Introduction This article is motivated by a clinical application on patients suffering from Alzheimer’s disease (AD), a neurodegener- ative disorder characterized by progressive brain atrophy and cognitive decline. Central to the pathophysiological cascade that leads to AD is amyloid-β (Aβ), a protein that accumulates into plaques in the brain of AD patients, and is thus a target for clinical therapeutics and molecular imaging (Hampel et al., 2021). While PET with 18F-AV-45 (florbe- tapir) radiotracer can characterize deposition of Aβ in vivo to monitor disease progression and response to treatment, PET is a specialty imaging technique that is difficult to ob- tain and costly and it is of great interest to use more readily available MRI scans to reconstitute information from spe- cialized and expensive Aβ PET scans (Camus et al., 2012; Zhang et al., 2022). To this end, a natural approach would be to model Aβ PET images from MRI derived metrics of cortical structure which have been shown to be associated with Aβ deposition in patients with AD (Spotorno et al., 2023). Rather than considering a single measure of cor- tical structure, neuroscientists posit that multiple metrics (e.g. cortical thickness and volume) can be used as inputs to form a multi-modal imaging inputs which utilizes the cross-information among different images to improve pre- diction of Aβ molecular images (Zhang et al., 2022; 2023). To this end, Section 1.1 offers a brief review of the existing literature on image-on-image regression in the context of predicting an output image from input images. 1.1. Image-on-image Regression Image-on-image regression involves predicting one imaging modality based on other imaging modalities. This approach is commonly used when the imaging modality to be pre- dicted is either too costly to acquire or when a clear version of the image is not available (Jeong et al., 2021; Subrama- nian et al., 2023; Onishi et al., 2023). In the domain of image-on-image regression, the prevailing method involves conducting region-by-region regression analyses between images. For example, in a study related to Multiple Sclero- sis, (Sweeney et al., 2013) applied region-wise logistic re- gression models, incorporating T1-weighted, T2-weighted, 1 arXiv:2402.02734v1  [eess.IV]  5 Feb 2024 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data FLAIR, and PD volumes to predict lesion incidence. How- ever, a notable limitation of these region-wise approaches is their inability to capture associations between different re- gions, resulting in reduced accuracy of predicting the output image from input images. To overcome this limitation, some methods employ adaptive smoothing techniques to integrate information from neighboring regions (Hazra et al., 2019). A more general and effective approach involves smoothing coefficients connecting outcome and input images using spa- tially varying coefficient models, well-suited for exploring regression relationships between spatially structured images (Niyogi et al., 2023; Mu et al., 2018; Zhu et al., 2014; Mu, 2019). Extending this direction of research, spatial latent factor models have been introduced to capture more com- plex non-linear spatial dependencies between outcome and input images (Guo et al., 2022). The utilization of spatially varying coefficients in image- on-image regression is effective for leveraging information between regions. However, these methods tend to be com- putationally expensive, even when either the sample size or number of regions is moderately large. Another research direction treats both response and input images as tensors, leading to the emergence of tensor-on-tensor regression ap- proaches (Lock, 2018; Gahrooei et al., 2021; Miranda et al., 2018; Guhaniyogi & Rodriguez, 2020; Guha & Guhaniyogi, 2021; Guhaniyogi & Spencer, 2021). While these methods implicitly consider smoothing among neighboring regions, they often necessitate scaling down images due to signifi- cant computational demands and low signal-to-noise ratios. Furthermore, these approaches have not yet addressed the modeling of non-linear associations between images. A third avenue of research focuses on developing multivari- ate support vector machines for predicting missing spatial information in EEG from fMRI (De Martino et al., 2011) or missing temporal information in fMRI from EEG data (Jansen et al., 2012). Compared with the original high-dimensional images, the low-dimensional features of images can facilitate estimating relationships between an outcome image and input images. With this motivation, we are particularly intrigued by deep neural networks (DNNs) for non-linear dimension reduction. Generative algorithms, such as Variational Auto-Encoders (VAEs) (Kingma & Welling, 2013; Doersch, 2016; Girin et al., 2020; Zhao & Linderman, 2023), have proven suc- cessful in representing images via low-dimensional latent variables. VAEs model the population distribution of image data through a simple distribution, often Gaussian distri- bution, for the latent variables combined with a complex non-linear mapping function. A key to the success of such methods is the use of flexible probability fields to represent the important information of images, as well as rapid com- putation with high-dimensional images and large sample. Despite the success of VAEs in imaging analysis, they do not fully explore shared information between input images to enhance performance in predicting the outcome image. More precisely, the existing approaches on VAEs synthe- size multi-modal imaging data at two levels, input-level and decision-level. Input-level fusion usually involves merging multiple inputs together before modeling, which can result in a large feature vector. Deciding how to merge the images is not an easy task, and naive merging may lead to poor performance. In contrast, in decision-level fusion, each type of image is used to train a model, and the output of the model is fused to make a final decision. This type of fusion separates each type of image information during training and ignores the contribution of complementary information between different types of images to the training. Ignoring interconnection between input images not only limits the biological plausibility and interpretation of predictive infer- ence for the outcome image, but broadly has been shown to reduce statistical efficiency (Dai & Li, 2021), and increase sensitivity to noise in the images (Calhoun & Sui, 2016). 1.2. Our Contributions Hierarchical Bayesian methods allow structured information to be borrowed explicitly among image inputs via joint prior structure on coefficients at different layers of hierarchy and offer inference via the joint posterior distribution (Jin et al., 2020; Lee et al., 2020; Lei et al., 2021; Su et al., 2022; Kaplan et al., 2023). However, this perspective has been under-utilized due to computational bottlenecks and lack of appropriate modeling architecture. Motivated by the hierarchical Bayesian principle of leverag- ing shared information, this paper introduces an integrative variational autoencoder (InVA) designed for the harmoniza- tion of multi-modal neuroimaging data in a computationally efficient manner. Our primary contributions are outlined as follows. First, we construct DNN-based encoder ek and de- coder dk corresponding to the kth input image, k = 1, ..., K. These encoders and decoders are utilized to map the kth input image to low-dimensional multivariate Gaussian distri- butions, independently for each k = 1, ..., K, all of the same dimension. The construction preserves sufficient flexibility of modeling each input image while providing a shallow- level representation of the images in the context of hierar- chical Bayesian modeling. Second, we sample hk from each learned multivariate Gaus- sian distribution, representing the features of the kth input image. Subsequently, we construct a DNN-based encoder ˜e and decoder ˜d shared over input images to map these features hk to another multivariate Gaussian distribution N(mk, bk). The shared DNN architecture ˜e and ˜d across input images, combined with the independent distributions N(mk, bk) for each k, strike a favorable balance between individual and shared information. This configuration repre- 2 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data sents a deeper-level of hierarchical Bayesian modeling. Third, the parameters from the encoders and decoders spe- cific to each input image, along with those from the shared encoder and decoder, are collectively fed into a Deep Neural Network (DNN)-based predictor designed for prediction of the output image. The joint learning of input parameters from all imaging inputs facilitates the sharing of informa- tion across different inputs, leading to more accurate pre- dictions of the output image. The performance of InVA surpasses that of ordinary Variational Autoencoders (VAEs) constructed independently using different input images, as well as other popular image-on-image regression competi- tors, in predicting the output image across various simu- lation studies. The exceptional performance of InVA in predicting a costly image from inexpensive imaging inputs in multi-modal neuroimaging data underscores the impor- tance of borrowing information from input images. 1.3. Connection to Hierarchical VAE Our InVA approach has incorporated novel modeling archi- tecture over the existing literature on hierarchical VAEs. In the hierarchical VAE literature, DRAW (Gregor et al., 2015) introduces a deep and recurrent approach that combines a sequence of VAEs to obtain more realistic image generation. Ladder Variational Autoencoders (Sønderby et al., 2016) also designs a new version of layered VAE that recursively corrects the generative distribution and produces highly expressive models. This is further generalized to other hi- erarchical variational models to get expressive variational distribution as well as efficient computation (Ranganath et al., 2016). Hierarchical priors (Klushyn et al., 2019) are then proposed in VAE to avoid the overregularization that results from the standard normal prior distribution and to en- courage the properties desirable for model learning, such as smoothness and simple explanatory factors. More recently, NVAE (Vahdat & Kautz, 2020) has also designed a hierar- chical VAE, which utilizes a deep hierarchical structure to achieve more stable and accurate image generation. Despite their good performance, these hierarchical VAEs are designed in pursuit of better performance when mod- eling a single source imaging data and are not designed to adequately extract shared information from multiple imag- ing inputs in the prediction of an output image. In contrast to these hierarchical VAEs, our InVA has a more flexi- ble structure to handle both single-source and multi-source imaging data. On the one hand, InVA has a hierarchical structure to capture complex patterns. On the other hand, InVA introduces both input-specific and shared model com- ponents. This new architecture allows better integration and information borrowing when facing multiple imaging inputs, leading to more accurate output image prediction. At the same time, even when dealing with a single imaging input, this new architecture enables better generalization of hierarchical VAE literature by allowing the same im- ages from multiple views (Yu et al., 2023; Yan et al., 2021). Therefore, our InVA fills an important gap in bridging the literature between hierarchical VAEs and image-on-image regression, broadening applications of VAEs in the analysis of multi-modal neuroimaging data. 2. Methods We propose an Integrative Variational Autoencoder (InVA) to better integrate multiple imaging inputs for more accurate prediction of an imaging output. We first begin by defining notations and offering a brief overview on VAEs. For i = 1, ..., n, we observe K different imaging inputs X1,i, ..., XK,i from the ith subject. Given the imaging in- puts Xk = {Xk,i : i = 1, ..., n}, k = 1, ..., K, we aim to predict an output image Y = {Yi : i = 1, ..., n}. Each imaging input and output can either be a vector, or a matrix, or a higher-order tensor. The output and input images are assumed to be of the same dimension. We denote the input data for the ith subject to be X(i) = {X1,i, ..., XK,i}. 2.1. Preliminary: Variational Autoencoder Autoencoder (AE) is a widely-used unsupervised learning method that utilizes an encoder to compress data and re- construct the data from the encoded features through a de- coder (Geng et al., 2015; Tschannen et al., 2018; Chorowski et al., 2019; Nazari et al., 2023; Hao & Shafto, 2023). To cope with different scenarios, variants of autoencoders have also been inspired (Ng & Autoencoder, 2011; Rifai et al., 2011a;b; Chen et al., 2012; 2014; Ranjan et al., 2017; Kingma & Welling, 2013; Tolstikhin et al., 2017; Pei et al., 2018; Vahdat & Kautz, 2020). Based on AE, variational autoencoders (VAEs) are designed to model the data distribu- tion (Doersch, 2016; Girin et al., 2020; Zhao & Linderman, 2023), which maps the input data into latent Gaussian dis- tribution through the encoder (Kviman et al., 2023; Hao & Shafto, 2023; Janjos et al., 2023). In VAE, the learning of encoder and decoder weights is usu- ally based on variational inference (Blei et al., 2017; Naka- mura et al., 2023), where the loss is defined as the negative variational lower bound on the marginal likelihood (Kingma & Welling, 2013). To be more specific, let the input im- age Xk,i be mapped to the latent variables zk,i ∈ Rp which follows a prior distribution p(zk,i) = N(0, Ip). As- sume that q(zk,i|Xk,i) denotes the variational distribution for zk,i, pϕ(Xk,i|zk,i) denotes the likelihood of Xk,i, and KL stands for the Kullback–Leibler divergence. Assuming q(zk,i|Xk,i) = Qp j=1 N(µi,j, σi,j), the training objective is to minimize the negative of the evidence lower bound 3 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Figure 1. Achitecture of Integrative Variational Autoencoder (InVA), which includes modality-specific encoders ek, k ∈ {1, · · · , K} (in green), shared encoder ˜e (in green), shared decoder ˜d (in orange), and modality-specific decoders dk, k ∈ {1, · · · , K} (in orange). Figure 2. Achitecture of multi-level conditional structure in Integrative Variational Autoencoder (InVA), which combine modality-specific feature at shallow level (i.e, µk and σk, k ∈ {1, · · · , K}) and deep-level features (i.e., mk and bk, k ∈ {1, · · · , K}) to more accurately predict the response. (ELBO), given by, L(Xk, ˆXk) = 1 n n X i=1 \u001a KL \u0000q(zk,i|Xk,i)||p(zk,i) \u0001 − Ezk,i∼q(zk,i|Xk,i) \u0002 log pϕ(Xk,i|zk,i) \u0003\u001b , (1) = 1 n n X i=1 \u001a ||Xk,i − ˆXk,i||2 2 + 1 2 p X j=1 (− log σ2 i,j + µ2 i,j + σ2 i,j − 1) \u001b , (2) where ˆXk is the reconstruction of Xk through the decoder. Importantly, the standard VAE architecture does not allow borrowing of information between multiple imaging inputs. 2.2. Integrative Variational Autoencoder We adopt an architecture inspired by hierarchical Bayesian modeling to enhance the learning of the latent variable distri- bution from multiple imaging inputs. Our integrative varia- tional autoencoder incorporates image-specific encoders and decoders for each input image at a shallow level to capture image-specific features. Additionally, it includes encoders and decoders shared by all input images at a deeper level to facilitate information borrowing and capture shared features of the input images. Image-specific encoder: For every input image Xk, we employ a DNN-based image-specific encoder ek to project it onto a hidden p-variate Gaussian distribution N(µk, σk). Subsequently, we sample hk ∈ Rp from this distribution to depict the shallow-level imaging features. This process effectively maps various input images to a common latent feature space, facilitating finer feature extraction. Shared encoder: To leverage on the shared information provided by the input images and enhance feature extrac- tion, we construct a shared DNN-based encoder ˜e. This encoder maps the hidden features hk from each image to a deeper hidden p-variate Gaussian distribution N(mk, bk). Subsequently, we sample zk ∈ Rp from this distribution to represent deep-level features corresponding to each input image. These features are analogous to hyperparameters in Bayesian hierarchical models, facilitating information borrowing. Shared decoder: Next, we incorporate a shared DNN-based decoder ˜d to predict image-specific features hk from finer features zk. Through minimizing the difference between 4 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data hk and the fitted ˆhk, we enable information borrowing and optimize the weights of the shared encoder ˜e and decoder ˜d to achieve a well-fitted deep hidden distribution N(mk, bk). Image-specific decoder: After the shared decoder ˜d, we also use a image-specific decoder dk for each imaging input, which maps the predicted ˆhk to the input image space and predicts ˆXk. By minimizing the difference between the input image Xk and the predicted ˆXk, the modality-specific information can be used to learn the weights in ek and dk and thus better estimation of the shallow-level hidden distribution. 2.3. Multi-level Conditional Structure To further predict the output image Y based on the extracted hidden data distributions, we utilize a multi-level condi- tional structure in our InVA. Specifically, to predict the shared response Y with both modality-specific information and shared knowledge, we concatenate the extracted distri- bution parameters at different levels, i.e., {µk, σk, mk, bk} where k ∈ {1, · · · , K}. The obtained vector is then sent to a DNN-based predictor, which maps it to the response space and predicts ˆY . In order to learn the data distribution and predict the response Y at the same time, we add the difference between the response Y and the prediction ˆY to the loss function and then minimize the loss. Without loss of generality, we take K = 2 as an example and illustrate the conditional structure in Figure 2. 2.4. Variational Inference Calculation The learning of the weights of our InVA is based on varia- tional inference. The marginal likelihood of imaging inputs consists of the sum of the marginal likelihoods for indi- vidual data from different data sources and can be written as Eq. (3), where H(i) = {h1,i, · · · , hK,i} and Z(i) = {z1,i, · · · , zK,i} are hidden features of {X1,i, ..., XK,i} at shallow and deeper levels, respectively, for i = 1, ..., n. log pθ(X1, · · · , XK) = n X i=1 log pθ(X1,i, · · · , XK,i), = KL \u0000qϕ(Z(i)|X(i))||pθ(Z(i)|X(i)) \u0001 + L(θ, ϕ, X(i)). (3) The first term in Eq. (3) is KL divergence between the pos- terior distribution of Z(i) and its variational approximation which is non-negative. Thus, to maximize the first term in Eq. (3), we minimize the second term L(θ, ϕ, X(i)) which is called the variational lower bound on the marginal likeli- hood of i-th subject and can be written as Eq. (4): L(θ, ϕ, X(i)) = K X k=1 \u001a Eqϕ(zk,i|X(i))[log pθ(Xk,i|zk,i)]− KL \u0000qϕ(hk,i|Xk,i)|pθ(hk,i) \u0001 − KL \u0000qϕ(zk,i|hk,i)|pθ(zk,i) \u0001\u001b , (4) where pθ(Xk,i|zk,i) is the data likelihood, qϕ(hk,i|Xk,i) and qϕ(zk,i|hk,i) represent the variational distribution of hk,i and zk,i, respectively. We assume these variational distributions assume the form of p-variate Gaussian dis- tributions, i.e., qϕ(hk,i|Xk,i) = Qp j=1 N(µk,i,j, σk,i,j) and qϕ(zk,i|hk,i) = Qp j=1 N(mk,i,j, bk,i,j). The priors for z and h are set as standard normal distribution, i.e., pθ(hk,i) = N(0, Ip) and pθ(zk,i) = N(0, Ip). During the minimization of Eq. (4), the first term represents the differ- ence between input data X and the reconstructed ˆX, which can be rewritten as Eqϕ(zk,i|X(i))[log pθ(Xk,i|zk,i)] = ||Xk,i − ˆXk,i||2 2. (5) For the second term, it is a penalty for the data-specific feature extraction and takes the form of a KL divergence between the variational distribution qϕ(hk,i|Xk,i) and the prior distribution on hk,i, which assumes a closed form, KL \u0000qϕ(hk,i|Xk,i)|pθ(hk,i) \u0001 = 1 2 p X j=1 (− log σ2 k,i,j + µ2 k,i,j + σ2 k,i,j − 1). (6) In addition, the third term is for the extracted deep features. This term also assumes a closed form given by, KL \u0000qϕ(zk,i|hk,i)|pθ(zk,i) \u0001 = 1 2 p X j=1 (− log b2 k,i,j + m2 k,i,j + b2 k,i,j − 1). (7) 3. Simulation Studies We generate simulated 3D input and output images to assess the image prediction accuracy of our InVA in comparison to other baseline methods. To evaluate the models, we employ the out-of-sample mean square error (MSE) between the output images and the predicted images as our comparison metric, with a smaller MSE indicating better prediction performance. The specifics of the simulation settings are provided in Section 3.1. 3.1. Simulation Settings Data Simulation: For the i-th subject, where i = 1, ..., n, we generate two input images, X1,i and X2,i, with each be- ing a 3-way tensor having dimensions d×d×d, comprising 5 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Table 1. Mean squared error comparison between our InVA and the variational autoencoder model (VAE), Bayesian varying coefficient model (Var-Coef), Bayesian additive regression trees (BART), and tensor regression (TensorReg) at n = 100 and d = 2. Across different signal-to-noise ratios, our InVA outperforms baseline methods when the true relationship between images is complex and unknown (d = 2, 3), and is one of the best methods when d = 1. Method Data order = 1 order = 2 order = 3 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 VAE X1 2.80 2.88 3.01 15.12 15.20 15.35 134.31 134.56 135.32 VAE X2 2.78 2.89 2.98 15.14 15.18 15.32 134.29 134.59 135.29 Var-Coef X1 & X2 0.01 0.01 0.25 22.86 22.95 23.16 61.27 61.17 61.15 BART X1 & X2 0.36 0.43 0.51 5.18 5.30 5.39 130.63 130.67 130.84 TensorReg X1 & X2 3.98 4.08 4.21 15.89 15.95 16.05 192.74 192.82 192.96 InVA X1 & X2 0.27 0.41 0.69 2.75 2.86 3.10 54.92 55.21 55.39 Table 2. Mean squared error comparison between our InVA and the variational autoencoder model (VAE), Bayesian additive regression trees (BART), and tensor regression (TensorReg) at n = 800 and d = 3. Across different signal-to-noise ratios and polynomial orders, our InVA outperforms baseline methods. Method Data order = 1 order = 2 order = 3 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 VAE X1 4.10 4.19 4.34 11.31 11.44 11.52 60.23 60.41 60.75 VAE X2 4.12 4.21 4.32 11.35 11.43 11.56 60.26 60.37 60.72 BART X1 & X2 2.13 2.24 2.31 14.77 14.85 14.94 93.61 93.85 94.12 TensorReg X1 & X2 5.58 5.65 5.77 21.82 21.93 22.01 78.20 78.45 78.71 InVA X1 & X2 0.49 0.62 0.82 5.72 5.78 6.17 36.52 36.75 36.82 d3 cells. Each cell entry of X1,i and X2,i is independently and identically simulated from the normal distribution N(0,1). The j = (j1, j2, j3)-th cell of the outcome image is generated from a polynomial of order O with varying coef- ficients as follows: yi(j) = PO o=1 P2 k=1 βo,k(j)xk,i(j)o + ϵi(j), where ϵi(j) i.i.d. ∼ N(0, σ2) and xk,i(j) represents the jth cell of the kth input image for the ith sample. The co- efficient βo,k(j) is generated from a Gaussian process with an exponential correlation kernel, allowing for a nonlinear relationship between the outcome and input images, as well as imposing correlation between cells of the outcome image. Further, the scale parameter for the exponential correlation kernel is set at 0.25 for all these Gaussian processes to bor- row information across input image coefficients. We explore a wide range of simulation scenarios by varying the order of the polynomial O = 1, 2, 3 and the signal-to-noise ra- tio, represented by varying σ = 0.1, 0.3, 0.5. We consider two different combinations of (n, d) = (100, 2), (800, 3). The setting n = 800, d = 3 closely matches the scenario in multi-modal neuroimaging data in Section 4. For the test data, we further simulate the equivalent of 20% of the training data using the same simulation settings. Baselines: We conduct a comparison between our InVA and the Variational Autoencoder model (VAE), using either X1 = {X1,i : i = 1, .., n} or X2 = {X2,i : i = 1, .., n} as input, to evaluate the advantages of integrating infor- mation from multiple imaging inputs. They are denoted by VAE (X1) and VAE (X2), respectively. Furthermore, the proposed model is contrasted with popular image-on- image regression approaches, namely (i) Bayesian varying coefficient model (Var-Coef) (Guhaniyogi et al., 2022), (ii) Bayesian additive regression trees (BART) (Chipman et al., 1998), and (iii) tensor regression (TensorReg) (Gahrooei et al., 2021). Both Var-Coef and BART are capable of capturing nonlinear associations between input and output images. In contrast, TensorReg conceptualizes each image as a tensor and establishes a regression framework between input and output images, accounting for the tensor structure of images. 3.2. Output Image Prediction Performance In the scenario where n = 100 and d = 2, the performance of all competitors deteriorates under increased noise vari- ance or when the relationship between the outcome and input images becomes more complex, as evidenced by a higher order of the polynomial governing the outcome im- age. As indicated in Table 1, InVA consistently achieves a smaller MSE across various orders and noise levels σ com- pared to TensorReg. This can be attributed to its effective capture of the non-linear association between the outcome and input images. Although VAE (X1), VAE (X2) and BART also capture the non-linear association between the outcome and input images, InVA significantly outperforms all of them. Notably, the smaller MSE of InVA compared 6 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data to VAE (X1) and VAE (X2) highlights the advantage of borrowing information from multiple imaging inputs. Var- Coef performs exceptionally well when the order of the polynomial used to simulate the outcome image is O = 1. In this case, the fitted Var-Coef model is identical to the data-generating model. However, InVA outperforms Var- Coef when the order O is set to 2 or 3. This suggests that our approach is advantageous when the true relationship between images is complex and unknown. In the case of n = 800 and d = 3, InVA continues to outper- form the baseline competitors (refer to Table 2). Var-Coef is not included as a baseline due to computational challenges with n = 800. Similar to Table 1, Table 2 demonstrates a decline in performance with increasing noise variance and the order of the true data-generating polynomial. Impor- tantly, both tables establish significantly superior perfor- mance when information is suitably borrowed from the two input images in predicting the output image. 3.3. Ablation Studies We do ablation studies to demonstrate the importance of each component in our InVA, where we train our InVA without shared components (InVA w/o Shd) and without in- put image-specific components (InVA w/o IS), respectively. In InVA w/o Shd, we train input image-specific encoders and decoders for each modality and average their predictions to obtain a final prediction without using a shared encoder and decoder. In InVA w/o IS, we combine different modali- ties and train a shared encoder and decoder without adding input image-specific encoders and decoders. We continue using mean squared error as the comparison metric and summarize the results in Table 3. As we can see, across different signal-to-noise ratios and polynomial orders, our InVA always has a smaller mean squared error com- pared to InVA w/o Shd and InVA w/o IS, which indicates that both the input image-specific and shared components in our InVA are crucial for the harmonization of multi-modal neuroimaging data analysis. 4. Multi-modal Neuroimaging Data Analysis We further apply our InVA approach in the study of multi- modal neuroimaging data. Data used in the preparation of this article were obtained from the Alzheimer’s Disease Neu- roimaging Initiative (ADNI) database (adni.loni.usc.edu)*. *Data used in preparation of this article were obtained from the Alzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neu- ropsychological assessment can be combined to measure the progression of AD. Specifically, we consider the base- line visit for participants in the ADNI 1, GO, and 2 cohorts. The goal of this analysis is to model molecular Aβ PET images as a function of MRI images of cortical thickness and volume. To do so, PET and MRI images were regis- tered to a common template space and segmented into 40 regions of interest (ROI) via the Desikan-Killiany cortical atlas (Desikan et al., 2006) using standard ADNI pipelines as described in Marinescu et al. (2019). Measurements of Aβ deposition were characterized by standardized uptake value ratio (SUVR) images which detect Aβ via binding of the florbetapir radiotracer. Cortical thickness and vol- ume were extracted and measured in millimeters (mm) and mm3 using FreeSurfer (Fischl, 2012). Complete imaging data was available for 711 subjects whose clinical status ranged from some cognitive impairment to a diagnosis of AD. We randomly divided the data into two parts, one part 80% as the training set, and one part 20% as the test set. The goal in this data is to predict the PET image using cortical thickness and cortical volume obtained from MRI. In our comparisons, all baseline competitors mentioned in Section 3.1 are compared with InVA , excluding TensorReg and Var-Coef. Var-Coef is computationally demanding for the size of the dataset, and TensorReg is not applicable to the dataset since the input and output images are not tensors in the real data, unlike in our simulation settings. In Figure 3, the displayed PET response is observed along- side the predicted PET response for a randomly selected sub- ject, illustrating the accurate reconstruction of the observed PET response by the estimated PET response. Table 4 presents the performance metrics, indicating that InVA out- performs VAEs when utilizing either cortical thickness or cortical volume as input. This highlights the advantage of in- tegrating information from multiple input images. Addition- ally, InVA demonstrates superior performance compared to BART, showcasing its ability to capture the complex non- linear regression relationship between input images and the output image. Overall, the results underscore the superior predictive performance of InVA . 5. Conclusion and Discussions We introduce a novel integrative variational autoencoder approach designed to leverage information from multiple imaging inputs, allowing for the development of a nonlin- ear relationship between input images and an image output. Empirical results from simulation studies demonstrate the superior performance of our proposed approach compared to existing image-on-image regression methods, particu- http://adni.loni.usc.edu/-/. 7 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Table 3. Ablation studies: Mean squared error comparison between our InVA and our InVA without shared components (InVA w/o Shd) and our InVA without input image-specific components (InVA w/o IS) at n = 100 and d = 2. Across different signal-to-noise ratios and polynomial orders, our InVA outperforms InVA w/o Shd and InVA w/o IS, demonstrating the importance of both the input image-specific and shared components in our InVA. Method Data order = 1 order = 2 order = 3 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 σ = 0.1 σ = 0.3 σ = 0.5 InVA w/o Shd X1 & X2 3.42 3.49 3.58 11.48 11.54 11.62 152.10 152.26 152.45 InVA w/o IS X1 & X2 1.48 1.55 1.61 5.78 5.85 5.95 101.63 101.84 102.08 InVA X1 & X2 0.27 0.41 0.69 2.75 2.86 3.10 54.92 55.21 55.39 (a) Observed PET Image (b) Predicted PET Image Figure 3. Observed and predicted PET image for a randomly selected subject. The observed and predicted PET image show strong similarity, suggesting that the observed PET response is accurately reconstructed using the estimated PET response. Table 4. Mean squared error (MSE) comparison between the InVA and variational autoencoder model (VAE) and Bayesian additive regression trees (BART) for the real multi-modal neuroimaging data. Our InVA produces smaller mean squared error, indicating more accurate multi-modal neuroimaging data analysis. Method Data MSE VAE Cortical Thickness 0.0674 VAE Cortical Volume 0.0659 BART Cortical Thickness & Volume 0.0681 InVA Cortical Thickness & Volume 0.0602 larly in drawing predictive inferences on the outcome image. This approach holds transformative potential in the field of multi-modal neuroimaging, especially in accurately predict- ing costly tau-PET images using more affordable imaging modalities for the study of neurodegenerative diseases, such as Alzheimer’s. Despite the harmonization of multi-modal neuroimaging data modeling, this article does not comprehensively explore our approach for a gamut of other multi-modal perspective, such as text data, video data, and audio data (Jabeen et al., 2023; Xu et al., 2023). It is also important to provide appro- priate analysis techniques for these more diverse modalities, which can further improve the accuracy of image regression as corresponding new modality data become available. We plan to explore this issue in a future article. Additionally, it is intuitive that our integrative variational autoencoder can be combined with existing uni-modal VAEs to equip each encoder and decoder component with a more expressive architecture. Finding the optimal combination and design remains to be explored, and this may be a future research direction. Broader Impact The research presented in this paper holds the potential to transform critical scientific domains, particularly at the inter- section of machine learning and computational neuroscience. A notable gap in the utilization of hierarchical Bayesian modeling in multi-modal neuroimaging data arises from the limited exploration of effectively incorporating shared infor- mation across multiple imaging modalities, primarily due to a lack of adequately expressive modeling architectures and computational hurdles. In addressing this challenge, this 8 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data article introduces a novel methodology for efficiently mod- eling the harmonization of multi-modal neuroimaging data. This contribution is expected to serve as a catalyst for future research in effectively modeling shared information between different imaging modalities in multi-modal neuroimaging analysis. Furthermore, the work pioneers the development of a hierarchical VAE architecture for integrating multiple images, setting the stage for potential advancements in en- coder and decoder architectures through the integration of our approach with existing uni-modal VAEs. Acknowledgements Data collection and sharing for this project was funded by the Alzheimer’s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National In- stitute on Aging, the National Institute of Biomedical Imag- ing and Bioengineering, and through generous contribu- tions from the following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery Foundation; Araclon Biotech; BioClinica, Inc.; Biogen; Bristol-Myers Squibb Company; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli Lilly and Company; EuroImmun; F. Hoffmann- La Roche Ltd and its affiliated company Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer Immunotherapy Research & Development, LLC.; Johnson & Johnson Pharmaceutical Research & Development LLC.; Lumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Di- agnostics, LLC.; NeuroRx Research; Neurotrack Technolo- gies; Novartis Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda Pharmaceutical Com- pany; and Transition Therapeutics. The Canadian Institutes of Health Research is providing funds to support ADNI clinical sites in Canada. Private sector contributions are facilitated by the Foundation for the National Institutes of Health (www.fnih.org). The grantee organization is the Northern California Institute for Research and Education, and the study is coordinated by the Alzheimer’s Therapeutic Research Institute at the University of Southern California. ADNI data are disseminated by the Laboratory for Neuro Imaging at the University of Southern California. 9 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data References Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- tional inference: A review for statisticians. Journal of the American statistical Association, 112(518):859–877, 2017. Calhoun, V. D. and Sui, J. Multimodal fusion of brain imaging data: A key to finding the missing link (s) in complex mental illness. Biological psychiatry: cognitive neuroscience and neuroimaging, 1(3):230–244, 2016. Camus, V., Payoux, P., Barr´e, L., Desgranges, B., Voisin, T., Tauber, C., La Joie, R., Tafani, M., Hommet, C., Ch´etelat, G., Mondon, K., de La Sayette, V., Cottier, J. P., Beaufils, E., Ribeiro, M. J., Gissot, V., Vierron, E., Vercouillie, J., Vellas, B., Eustache, F., and Guilloteau, D. Using PET with 18F-AV-45 (florbetapir) to quantify brain amyloid load in a clinical environment. Eur. J. Nucl. Med. Mol. Imaging, 39(4):621–631, April 2012. Chen, M., Xu, Z., Weinberger, K., and Sha, F. Marginalized denoising autoencoders for domain adaptation. arXiv preprint arXiv:1206.4683, 2012. Chen, M., Weinberger, K., Sha, F., and Bengio, Y. Marginal- ized denoising auto-encoders for nonlinear representa- tions. In International conference on machine learning, pp. 1476–1484. PMLR, 2014. Chipman, H. A., George, E. I., and McCulloch, R. E. Bayesian cart model search. Journal of the American Statistical Association, 93(443):935–948, 1998. Chorowski, J., Weiss, R. J., Bengio, S., and van den Oord, A. Unsupervised speech representation learning using wavenet autoencoders. IEEE/ACM transactions on au- dio, speech, and language processing, 27(12):2041–2053, 2019. Dai, X. and Li, L. Orthogonal statistical inference for multi- modal data analysis. arXiv preprint arXiv:2103.07088, 2021. De Martino, F., De Borst, A. W., Valente, G., Goebel, R., and Formisano, E. Predicting eeg single trial responses with simultaneous fMRI and relevance vector machine regression. Neuroimage, 56(2):826–836, 2011. Desikan, R. S., S´egonne, F., Fischl, B., Quinn, B. T., Dick- erson, B. C., Blacker, D., Buckner, R. L., Dale, A. M., Maguire, R. P., Hyman, B. T., Albert, M. S., and Killiany, R. J. An automated labeling system for subdividing the human cerebral cortex on MRI scans into gyral based regions of interest. Neuroimage, 31(3):968–980, July 2006. Doersch, C. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908, 2016. Fischl, B. FreeSurfer. Neuroimage, 62(2):774–781, August 2012. Gahrooei, M. R., Yan, H., Paynabar, K., and Shi, J. Multiple tensor-on-tensor regression: An approach for modeling processes with heterogeneous sources of data. Techno- metrics, 63(2):147–159, 2021. Geng, J., Fan, J., Wang, H., Ma, X., Li, B., and Chen, F. High-resolution sar image classification via deep con- volutional autoencoders. IEEE Geoscience and Remote Sensing Letters, 12(11):2351–2355, 2015. Girin, L., Leglaive, S., Bie, X., Diard, J., Hueber, T., and Alameda-Pineda, X. Dynamical variational au- toencoders: A comprehensive review. arXiv preprint arXiv:2008.12595, 2020. Gregor, K., Danihelka, I., Graves, A., Rezende, D., and Wierstra, D. Draw: A recurrent neural network for im- age generation. In International conference on machine learning, pp. 1462–1471. PMLR, 2015. Guha, S. and Guhaniyogi, R. Bayesian generalized sparse symmetric tensor-on-vector regression. Technometrics, 63(2):160–170, 2021. Guhaniyogi, R. and Rodriguez, A. Joint modeling of longi- tudinal relational data and exogenous variables. 2020. Guhaniyogi, R. and Spencer, D. Bayesian tensor response regression with an application to brain activation studies. Bayesian Analysis, 16(4):1221–1249, 2021. Guhaniyogi, R., Li, C., Savitsky, T. D., and Srivastava, S. Distributed bayesian varying coefficient modeling using a gaussian process prior. The Journal of Machine Learning Research, 23(1):3642–3700, 2022. Guo, C., Kang, J., and Johnson, T. D. A spatial bayesian latent factor model for image-on-image regression. Bio- metrics, 78(1):72–84, 2022. Hampel, H., Hardy, J., Blennow, K., Chen, C., Perry, G., Kim, S. H., Villemagne, V. L., Aisen, P., Vendruscolo, M., Iwatsubo, T., Masters, C. L., Cho, M., Lannfelt, L., Cum- mings, J. L., and Vergallo, A. The Amyloid-β pathway in alzheimer’s disease. Mol. Psychiatry, 26(10):5481–5503, October 2021. Hao, X. and Shafto, P. Coupled variational autoencoder. arXiv preprint arXiv:2306.02565, 2023. Hazra, A., Reich, B. J., Reich, D. S., Shinohara, R. T., and Staicu, A.-M. A spatio-temporal model for longitudinal image-on-image regression. Statistics in biosciences, 11: 22–46, 2019. 10 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Jabeen, S., Li, X., Amin, M. S., Bourahla, O., Li, S., and Jabbar, A. A review on methods and applications in mul- timodal deep learning. ACM Transactions on Multimedia Computing, Communications and Applications, 19(2s): 1–41, 2023. Janjos, F., Rosenbaum, L., Dolgov, M., and Z¨ollner, J. M. Unscented autoencoder. In International Conference on Machine Learning, pp. 14758–14779. PMLR, 2023. Jansen, M., White, T. P., Mullinger, K. J., Liddle, E. B., Gowland, P. A., Francis, S. T., Bowtell, R., and Liddle, P. F. Motion-related artefacts in EEG predict neuronally plausible patterns of activation in fMRI data. Neuroimage, 59(1):261–270, 2012. Jeong, Y. J., Park, H. S., Jeong, J. E., Yoon, H. J., Jeon, K., Cho, K., and Kang, D.-Y. Restoration of amyloid pet images obtained with short-time data using a generative adversarial networks framework. Scientific reports, 11 (1):4825, 2021. Jin, J., Riviere, M.-K., Luo, X., and Dong, Y. Bayesian methods for the analysis of early-phase oncology basket trials with information borrowing across cancer types. Statistics in Medicine, 39(25):3459–3475, 2020. Kaplan, D., Chen, J., Yavuz, S., and Lyu, W. Bayesian dynamic borrowing of historical information with appli- cations to the analysis of large-scale assessments. Psy- chometrika, 88(1):1–30, 2023. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Klushyn, A., Chen, N., Kurle, R., Cseke, B., and van der Smagt, P. Learning hierarchical priors in vaes. Advances in neural information processing systems, 32, 2019. Kviman, O., Mol´en, R., Hotti, A., Kurt, S., Elvira, V., and Lagergren, J. Cooperation in the latent space: The bene- fits of adding mixture components in variational autoen- coders. In International Conference on Machine Learn- ing, pp. 18008–18022. PMLR, 2023. Lee, S. Y., Lei, B., and Mallick, B. Estimation of covid- 19 spread curves integrating global data and borrowing information. PloS one, 15(7):e0236860, 2020. Lei, B., Kirk, T. Q., Bhattacharya, A., Pati, D., Qian, X., Arroyave, R., and Mallick, B. K. Bayesian optimization with adaptive surrogate models for automated experimen- tal design. Npj Computational Materials, 7(1):194, 2021. Lock, E. F. Tensor-on-tensor regression. Journal of Compu- tational and Graphical Statistics, 27(3):638–647, 2018. Marinescu, R. V., Oxtoby, N. P., Young, A. L., Bron, E. E., Toga, A. W., Weiner, M. W., Barkhof, F., Fox, N. C., Gol- land, P., Klein, S., and Alexander, D. C. TADPOLE chal- lenge: Accurate alzheimer’s disease prediction through crowdsourced forecasting of future data. Predict Intell Med, 11843:1–10, October 2019. Miranda, M. F., Zhu, H., and Ibrahim, J. G. TPRM: Tensor partition regression models with applications in imaging biomarker detection. The annals of applied statistics, 12 (3):1422, 2018. Mu, J. Spatially varying coefficient models: Theory and methods. PhD thesis, Iowa State University, 2019. Mu, J., Wang, G., and Wang, L. Estimation and inference in spatially varying coefficient models. Environmetrics, 29(1):e2485, 2018. Nakamura, H., Okada, M., and Taniguchi, T. Representa- tion uncertainty in self-supervised learning as variational inference. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 16484–16493, 2023. Nazari, P., Damrich, S., and Hamprecht, F. A. Geometric autoencoders–what you see is what you decode. arXiv preprint arXiv:2306.17638, 2023. Ng, A. and Autoencoder, S. Cs294a lecture notes. Dosegljivo: https://web. stanford. edu/class/cs294a/sparseAutoencoder 2011new. pdf.[Dostopano 20. 7. 2016], 2011. Niyogi, P. G., Lindquist, M. A., and Maiti, T. A tensor based varying-coefficient model for multi-modal neuroimaging data analysis. arXiv preprint arXiv:2303.16443, 2023. Onishi, Y., Hashimoto, F., Ote, K., Matsubara, K., and Ibaraki, M. Self-supervised pre-training for deep image prior-based robust pet image denoising. IEEE Transac- tions on Radiation and Plasma Medical Sciences, 2023. Pei, Y. et al. A study on feature extraction of handwriting data using kernel method-based autoencoder. In 2018 9th International Conference on Awareness Science and Technology (iCAST), pp. 1–6. IEEE, 2018. Ranganath, R., Tran, D., and Blei, D. Hierarchical varia- tional models. In International conference on machine learning, pp. 324–333. PMLR, 2016. Ranjan, R., Patel, V. M., and Chellappa, R. Hyperface: A deep multi-task learning framework for face detec- tion, landmark localization, pose estimation, and gender recognition. IEEE transactions on pattern analysis and machine intelligence, 41(1):121–135, 2017. 11 InVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data Rifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y., Dauphin, Y., and Glorot, X. Higher order contractive auto- encoder. In Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2011, Athens, Greece, September 5-9, 2011, Proceedings, Part II 22, pp. 645–660. Springer, 2011a. Rifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio, Y. Contractive auto-encoders: Explicit invariance during feature extraction. In Proceedings of the 28th interna- tional conference on international conference on machine learning, pp. 833–840, 2011b. Sønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and Winther, O. Ladder variational autoencoders. Advances in neural information processing systems, 29, 2016. Spotorno, N., Strandberg, O., Vis, G., Stomrud, E., Nilsson, M., and Hansson, O. Measures of cortical microstructure are linked to amyloid pathology in alzheimer’s disease. Brain, 146(4):1602–1614, April 2023. Su, L., Chen, X., Zhang, J., and Yan, F. Comparative study of bayesian information borrowing methods in oncology clinical trials. JCO Precision Oncology, 6:e2100394, 2022. Subramanian, K., Martinez, J., Huicochea Castellanos, S., Ivanidze, J., Nagar, H., Nicholson, S., Youn, T., Nau- seef, J. T., Tagawa, S., and Osborne, J. R. Complex implementation factors demonstrated when evaluating cost-effectiveness and monitoring racial disparities as- sociated with [18f] dcfpyl pet/ct in prostate cancer men. Scientific Reports, 13(1):8321, 2023. Sweeney, E., Shinohara, R., Shea, C., Reich, D., and Crainiceanu, C. M. Automatic lesion incidence estima- tion and detection in multiple sclerosis using multise- quence longitudinal mri. American Journal of Neuroradi- ology, 34(1):68–73, 2013. Tolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf, B. Wasserstein auto-encoders. arXiv preprint arXiv:1711.01558, 2017. Tschannen, M., Bachem, O., and Lucic, M. Recent advances in autoencoder-based representation learning. arXiv preprint arXiv:1812.05069, 2018. Vahdat, A. and Kautz, J. Nvae: A deep hierarchical vari- ational autoencoder. Advances in neural information processing systems, 33:19667–19679, 2020. Xu, P., Zhu, X., and Clifton, D. A. Multimodal learning with transformers: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. Yan, X., Hu, S., Mao, Y., Ye, Y., and Yu, H. Deep multi- view learning methods: A review. Neurocomputing, 448: 106–129, 2021. Yu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan, Z., Zhu, C., Xiong, Z., Liang, T., et al. Mvimgnet: A large-scale dataset of multi-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 9150–9161, 2023. Zhang, J., He, X., Qing, L., Gao, F., and Wang, B. BP- GAN: Brain PET synthesis from MRI using generative adversarial network for multi-modal alzheimer’s disease diagnosis. Comput. Methods Programs Biomed., 217(C), April 2022. Zhang, Y., Li, X., Ji, Y., Ding, H., Suo, X., He, X., Xie, Y., Liang, M., Zhang, S., Yu, C., and Qin, W. MRAβ: A multimodal MRI-derived amyloid-β biomarker for alzheimer’s disease. Hum. Brain Mapp., 44(15):5139– 5152, October 2023. Zhao, Y. and Linderman, S. Revisiting structured variational autoencoders. In International Conference on Machine Learning, pp. 42046–42057. PMLR, 2023. Zhu, H., Fan, J., and Kong, L. Spatially varying coefficient model for neuroimaging data with jump discontinuities. Journal of the American Statistical Association, 109(507): 1084–1098, 2014. 12 "
}