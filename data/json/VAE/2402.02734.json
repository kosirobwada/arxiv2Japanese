{
    "optim": "InVA: Integrative Variational Autoencoder\nfor Harmonization of Multi-modal Neuroimaging Data\nBowen Lei 1 Rajarshi Guhaniyogi 1 Krishnendu Chandra 1 Aaron Scheffler 2 Bani Mallick 1\nfor the Alzheimer’s Disease Neuroimaging Initiative* 3\nAbstract\nThere is a significant interest in exploring non-\nlinear associations among multiple images de-\nrived from diverse imaging modalities. While\nthere is a growing literature on image-on-image\nregression to delineate predictive inference of\nan image based on multiple images, existing ap-\nproaches have limitations in efficiently borrowing\ninformation between multiple imaging modalities\nin the prediction of an image. Building on the\nliterature of Variational Auto Encoders (VAEs),\nthis article proposes a novel approach, referred\nto as Integrative Variational Autoencoder (InVA)\nmethod, which borrows information from mul-\ntiple images obtained from different sources to\ndraw predictive inference of an image. The pro-\nposed approach captures complex non-linear asso-\nciation between the outcome image and input im-\nages, while allowing rapid computation. Numer-\nical results demonstrate substantial advantages\nof InVA over VAEs, which typically do not al-\nlow borrowing information between input images.\nThe proposed framework offers highly accurate\npredictive inferences for costly positron emission\ntopography (PET) from multiple measures of cor-\ntical structure in human brain scans readily avail-\nable from magnetic resonance imaging (MRI).\n1Department of Statistics, Texas A&M University, College\nStation, U.S.\n2University of California, San Francisco, San\nFrancisco, U.S.\n3*Data used in preparation of this article\nwere obtained from the Alzheimer’s Disease Neuroimaging\nInitiative(ADNI) database (adni.loni.usc.edu).\nAs such, the\ninvestigators within the ADNI contributed to the design and\nimplementation of ADNI and/or provided data but did not\nparticipate in analysis or writing of this report. A complete listing\nof ADNI investigators can be found at: http://adni.loni.usc.edu/wp-\ncontent/uploads/how to apply/ADNI Acknowledgement List.pdf.\nCorrespondence to: Bani Mallick <bmallick@stat.tamu.edu>.\n1. Introduction\nThis article is motivated by a clinical application on patients\nsuffering from Alzheimer’s disease (AD), a neurodegener-\native disorder characterized by progressive brain atrophy\nand cognitive decline. Central to the pathophysiological\ncascade that leads to AD is amyloid-β (Aβ), a protein that\naccumulates into plaques in the brain of AD patients, and is\nthus a target for clinical therapeutics and molecular imaging\n(Hampel et al., 2021). While PET with 18F-AV-45 (florbe-\ntapir) radiotracer can characterize deposition of Aβ in vivo\nto monitor disease progression and response to treatment,\nPET is a specialty imaging technique that is difficult to ob-\ntain and costly and it is of great interest to use more readily\navailable MRI scans to reconstitute information from spe-\ncialized and expensive Aβ PET scans (Camus et al., 2012;\nZhang et al., 2022). To this end, a natural approach would\nbe to model Aβ PET images from MRI derived metrics of\ncortical structure which have been shown to be associated\nwith Aβ deposition in patients with AD (Spotorno et al.,\n2023). Rather than considering a single measure of cor-\ntical structure, neuroscientists posit that multiple metrics\n(e.g. cortical thickness and volume) can be used as inputs\nto form a multi-modal imaging inputs which utilizes the\ncross-information among different images to improve pre-\ndiction of Aβ molecular images (Zhang et al., 2022; 2023).\nTo this end, Section 1.1 offers a brief review of the existing\nliterature on image-on-image regression in the context of\npredicting an output image from input images.\n1.1. Image-on-image Regression\nImage-on-image regression involves predicting one imaging\nmodality based on other imaging modalities. This approach\nis commonly used when the imaging modality to be pre-\ndicted is either too costly to acquire or when a clear version\nof the image is not available (Jeong et al., 2021; Subrama-\nnian et al., 2023; Onishi et al., 2023). In the domain of\nimage-on-image regression, the prevailing method involves\nconducting region-by-region regression analyses between\nimages. For example, in a study related to Multiple Sclero-\nsis, (Sweeney et al., 2013) applied region-wise logistic re-\ngression models, incorporating T1-weighted, T2-weighted,\n1\narXiv:2402.02734v1  [eess.IV]  5 Feb 2024\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nFLAIR, and PD volumes to predict lesion incidence. How-\never, a notable limitation of these region-wise approaches is\ntheir inability to capture associations between different re-\ngions, resulting in reduced accuracy of predicting the output\nimage from input images. To overcome this limitation, some\nmethods employ adaptive smoothing techniques to integrate\ninformation from neighboring regions (Hazra et al., 2019).\nA more general and effective approach involves smoothing\ncoefficients connecting outcome and input images using spa-\ntially varying coefficient models, well-suited for exploring\nregression relationships between spatially structured images\n(Niyogi et al., 2023; Mu et al., 2018; Zhu et al., 2014; Mu,\n2019). Extending this direction of research, spatial latent\nfactor models have been introduced to capture more com-\nplex non-linear spatial dependencies between outcome and\ninput images (Guo et al., 2022).\nThe utilization of spatially varying coefficients in image-\non-image regression is effective for leveraging information\nbetween regions. However, these methods tend to be com-\nputationally expensive, even when either the sample size or\nnumber of regions is moderately large. Another research\ndirection treats both response and input images as tensors,\nleading to the emergence of tensor-on-tensor regression ap-\nproaches (Lock, 2018; Gahrooei et al., 2021; Miranda et al.,\n2018; Guhaniyogi & Rodriguez, 2020; Guha & Guhaniyogi,\n2021; Guhaniyogi & Spencer, 2021). While these methods\nimplicitly consider smoothing among neighboring regions,\nthey often necessitate scaling down images due to signifi-\ncant computational demands and low signal-to-noise ratios.\nFurthermore, these approaches have not yet addressed the\nmodeling of non-linear associations between images. A\nthird avenue of research focuses on developing multivari-\nate support vector machines for predicting missing spatial\ninformation in EEG from fMRI (De Martino et al., 2011)\nor missing temporal information in fMRI from EEG data\n(Jansen et al., 2012).\nCompared with the original high-dimensional images, the\nlow-dimensional features of images can facilitate estimating\nrelationships between an outcome image and input images.\nWith this motivation, we are particularly intrigued by deep\nneural networks (DNNs) for non-linear dimension reduction.\nGenerative algorithms, such as Variational Auto-Encoders\n(VAEs) (Kingma & Welling, 2013; Doersch, 2016; Girin\net al., 2020; Zhao & Linderman, 2023), have proven suc-\ncessful in representing images via low-dimensional latent\nvariables. VAEs model the population distribution of image\ndata through a simple distribution, often Gaussian distri-\nbution, for the latent variables combined with a complex\nnon-linear mapping function. A key to the success of such\nmethods is the use of flexible probability fields to represent\nthe important information of images, as well as rapid com-\nputation with high-dimensional images and large sample.\nDespite the success of VAEs in imaging analysis, they do\nnot fully explore shared information between input images\nto enhance performance in predicting the outcome image.\nMore precisely, the existing approaches on VAEs synthe-\nsize multi-modal imaging data at two levels, input-level and\ndecision-level. Input-level fusion usually involves merging\nmultiple inputs together before modeling, which can result\nin a large feature vector. Deciding how to merge the images\nis not an easy task, and naive merging may lead to poor\nperformance. In contrast, in decision-level fusion, each type\nof image is used to train a model, and the output of the\nmodel is fused to make a final decision. This type of fusion\nseparates each type of image information during training\nand ignores the contribution of complementary information\nbetween different types of images to the training. Ignoring\ninterconnection between input images not only limits the\nbiological plausibility and interpretation of predictive infer-\nence for the outcome image, but broadly has been shown to\nreduce statistical efficiency (Dai & Li, 2021), and increase\nsensitivity to noise in the images (Calhoun & Sui, 2016).\n1.2. Our Contributions\nHierarchical Bayesian methods allow structured information\nto be borrowed explicitly among image inputs via joint prior\nstructure on coefficients at different layers of hierarchy and\noffer inference via the joint posterior distribution (Jin et al.,\n2020; Lee et al., 2020; Lei et al., 2021; Su et al., 2022;\nKaplan et al., 2023). However, this perspective has been\nunder-utilized due to computational bottlenecks and lack of\nappropriate modeling architecture.\nMotivated by the hierarchical Bayesian principle of leverag-\ning shared information, this paper introduces an integrative\nvariational autoencoder (InVA) designed for the harmoniza-\ntion of multi-modal neuroimaging data in a computationally\nefficient manner. Our primary contributions are outlined as\nfollows. First, we construct DNN-based encoder ek and de-\ncoder dk corresponding to the kth input image, k = 1, ..., K.\nThese encoders and decoders are utilized to map the kth\ninput image to low-dimensional multivariate Gaussian distri-\nbutions, independently for each k = 1, ..., K, all of the same\ndimension. The construction preserves sufficient flexibility\nof modeling each input image while providing a shallow-\nlevel representation of the images in the context of hierar-\nchical Bayesian modeling.\nSecond, we sample hk from each learned multivariate Gaus-\nsian distribution, representing the features of the kth input\nimage. Subsequently, we construct a DNN-based encoder\n˜e and decoder ˜d shared over input images to map these\nfeatures hk to another multivariate Gaussian distribution\nN(mk, bk). The shared DNN architecture ˜e and ˜d across\ninput images, combined with the independent distributions\nN(mk, bk) for each k, strike a favorable balance between\nindividual and shared information. This configuration repre-\n2\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nsents a deeper-level of hierarchical Bayesian modeling.\nThird, the parameters from the encoders and decoders spe-\ncific to each input image, along with those from the shared\nencoder and decoder, are collectively fed into a Deep Neural\nNetwork (DNN)-based predictor designed for prediction of\nthe output image. The joint learning of input parameters\nfrom all imaging inputs facilitates the sharing of informa-\ntion across different inputs, leading to more accurate pre-\ndictions of the output image. The performance of InVA\nsurpasses that of ordinary Variational Autoencoders (VAEs)\nconstructed independently using different input images, as\nwell as other popular image-on-image regression competi-\ntors, in predicting the output image across various simu-\nlation studies. The exceptional performance of InVA in\npredicting a costly image from inexpensive imaging inputs\nin multi-modal neuroimaging data underscores the impor-\ntance of borrowing information from input images.\n1.3. Connection to Hierarchical VAE\nOur InVA approach has incorporated novel modeling archi-\ntecture over the existing literature on hierarchical VAEs. In\nthe hierarchical VAE literature, DRAW (Gregor et al., 2015)\nintroduces a deep and recurrent approach that combines a\nsequence of VAEs to obtain more realistic image generation.\nLadder Variational Autoencoders (Sønderby et al., 2016)\nalso designs a new version of layered VAE that recursively\ncorrects the generative distribution and produces highly\nexpressive models. This is further generalized to other hi-\nerarchical variational models to get expressive variational\ndistribution as well as efficient computation (Ranganath\net al., 2016). Hierarchical priors (Klushyn et al., 2019) are\nthen proposed in VAE to avoid the overregularization that\nresults from the standard normal prior distribution and to en-\ncourage the properties desirable for model learning, such as\nsmoothness and simple explanatory factors. More recently,\nNVAE (Vahdat & Kautz, 2020) has also designed a hierar-\nchical VAE, which utilizes a deep hierarchical structure to\nachieve more stable and accurate image generation.\nDespite their good performance, these hierarchical VAEs\nare designed in pursuit of better performance when mod-\neling a single source imaging data and are not designed to\nadequately extract shared information from multiple imag-\ning inputs in the prediction of an output image. In contrast\nto these hierarchical VAEs, our InVA has a more flexi-\nble structure to handle both single-source and multi-source\nimaging data. On the one hand, InVA has a hierarchical\nstructure to capture complex patterns. On the other hand,\nInVA introduces both input-specific and shared model com-\nponents. This new architecture allows better integration\nand information borrowing when facing multiple imaging\ninputs, leading to more accurate output image prediction.\nAt the same time, even when dealing with a single imaging\ninput, this new architecture enables better generalization\nof hierarchical VAE literature by allowing the same im-\nages from multiple views (Yu et al., 2023; Yan et al., 2021).\nTherefore, our InVA fills an important gap in bridging the\nliterature between hierarchical VAEs and image-on-image\nregression, broadening applications of VAEs in the analysis\nof multi-modal neuroimaging data.\n2. Methods\nWe propose an Integrative Variational Autoencoder (InVA)\nto better integrate multiple imaging inputs for more accurate\nprediction of an imaging output. We first begin by defining\nnotations and offering a brief overview on VAEs.\nFor i = 1, ..., n, we observe K different imaging inputs\nX1,i, ..., XK,i from the ith subject. Given the imaging in-\nputs Xk = {Xk,i : i = 1, ..., n}, k = 1, ..., K, we aim\nto predict an output image Y = {Yi : i = 1, ..., n}. Each\nimaging input and output can either be a vector, or a matrix,\nor a higher-order tensor. The output and input images are\nassumed to be of the same dimension. We denote the input\ndata for the ith subject to be X(i) = {X1,i, ..., XK,i}.\n2.1. Preliminary: Variational Autoencoder\nAutoencoder (AE) is a widely-used unsupervised learning\nmethod that utilizes an encoder to compress data and re-\nconstruct the data from the encoded features through a de-\ncoder (Geng et al., 2015; Tschannen et al., 2018; Chorowski\net al., 2019; Nazari et al., 2023; Hao & Shafto, 2023).\nTo cope with different scenarios, variants of autoencoders\nhave also been inspired (Ng & Autoencoder, 2011; Rifai\net al., 2011a;b; Chen et al., 2012; 2014; Ranjan et al., 2017;\nKingma & Welling, 2013; Tolstikhin et al., 2017; Pei et al.,\n2018; Vahdat & Kautz, 2020). Based on AE, variational\nautoencoders (VAEs) are designed to model the data distribu-\ntion (Doersch, 2016; Girin et al., 2020; Zhao & Linderman,\n2023), which maps the input data into latent Gaussian dis-\ntribution through the encoder (Kviman et al., 2023; Hao &\nShafto, 2023; Janjos et al., 2023).\nIn VAE, the learning of encoder and decoder weights is usu-\nally based on variational inference (Blei et al., 2017; Naka-\nmura et al., 2023), where the loss is defined as the negative\nvariational lower bound on the marginal likelihood (Kingma\n& Welling, 2013). To be more specific, let the input im-\nage Xk,i be mapped to the latent variables zk,i ∈ Rp\nwhich follows a prior distribution p(zk,i) = N(0, Ip). As-\nsume that q(zk,i|Xk,i) denotes the variational distribution\nfor zk,i, pϕ(Xk,i|zk,i) denotes the likelihood of Xk,i, and\nKL stands for the Kullback–Leibler divergence. Assuming\nq(zk,i|Xk,i) = Qp\nj=1 N(µi,j, σi,j), the training objective\nis to minimize the negative of the evidence lower bound\n3\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nFigure 1. Achitecture of Integrative Variational Autoencoder (InVA), which includes modality-specific encoders ek, k ∈ {1, · · · , K} (in\ngreen), shared encoder ˜e (in green), shared decoder ˜d (in orange), and modality-specific decoders dk, k ∈ {1, · · · , K} (in orange).\nFigure 2. Achitecture of multi-level conditional structure in Integrative Variational Autoencoder (InVA), which combine modality-specific\nfeature at shallow level (i.e, µk and σk, k ∈ {1, · · · , K}) and deep-level features (i.e., mk and bk, k ∈ {1, · · · , K}) to more accurately\npredict the response.\n(ELBO), given by,\nL(Xk, ˆXk) = 1\nn\nn\nX\ni=1\n\u001a\nKL\n\u0000q(zk,i|Xk,i)||p(zk,i)\n\u0001\n− Ezk,i∼q(zk,i|Xk,i)\n\u0002\nlog pϕ(Xk,i|zk,i)\n\u0003\u001b\n,\n(1)\n= 1\nn\nn\nX\ni=1\n\u001a\n||Xk,i − ˆXk,i||2\n2\n+ 1\n2\np\nX\nj=1\n(− log σ2\ni,j + µ2\ni,j + σ2\ni,j − 1)\n\u001b\n,\n(2)\nwhere ˆXk is the reconstruction of Xk through the decoder.\nImportantly, the standard VAE architecture does not allow\nborrowing of information between multiple imaging inputs.\n2.2. Integrative Variational Autoencoder\nWe adopt an architecture inspired by hierarchical Bayesian\nmodeling to enhance the learning of the latent variable distri-\nbution from multiple imaging inputs. Our integrative varia-\ntional autoencoder incorporates image-specific encoders and\ndecoders for each input image at a shallow level to capture\nimage-specific features. Additionally, it includes encoders\nand decoders shared by all input images at a deeper level to\nfacilitate information borrowing and capture shared features\nof the input images.\nImage-specific encoder: For every input image Xk, we\nemploy a DNN-based image-specific encoder ek to project\nit onto a hidden p-variate Gaussian distribution N(µk, σk).\nSubsequently, we sample hk ∈ Rp from this distribution\nto depict the shallow-level imaging features. This process\neffectively maps various input images to a common latent\nfeature space, facilitating finer feature extraction.\nShared encoder: To leverage on the shared information\nprovided by the input images and enhance feature extrac-\ntion, we construct a shared DNN-based encoder ˜e. This\nencoder maps the hidden features hk from each image to a\ndeeper hidden p-variate Gaussian distribution N(mk, bk).\nSubsequently, we sample zk ∈ Rp from this distribution to\nrepresent deep-level features corresponding to each input\nimage. These features are analogous to hyperparameters\nin Bayesian hierarchical models, facilitating information\nborrowing.\nShared decoder: Next, we incorporate a shared DNN-based\ndecoder ˜d to predict image-specific features hk from finer\nfeatures zk. Through minimizing the difference between\n4\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nhk and the fitted ˆhk, we enable information borrowing and\noptimize the weights of the shared encoder ˜e and decoder ˜d\nto achieve a well-fitted deep hidden distribution N(mk, bk).\nImage-specific decoder: After the shared decoder ˜d, we\nalso use a image-specific decoder dk for each imaging input,\nwhich maps the predicted ˆhk to the input image space and\npredicts ˆXk. By minimizing the difference between the\ninput image Xk and the predicted ˆXk, the modality-specific\ninformation can be used to learn the weights in ek and\ndk and thus better estimation of the shallow-level hidden\ndistribution.\n2.3. Multi-level Conditional Structure\nTo further predict the output image Y based on the extracted\nhidden data distributions, we utilize a multi-level condi-\ntional structure in our InVA. Specifically, to predict the\nshared response Y with both modality-specific information\nand shared knowledge, we concatenate the extracted distri-\nbution parameters at different levels, i.e., {µk, σk, mk, bk}\nwhere k ∈ {1, · · · , K}. The obtained vector is then sent\nto a DNN-based predictor, which maps it to the response\nspace and predicts ˆY . In order to learn the data distribution\nand predict the response Y at the same time, we add the\ndifference between the response Y and the prediction ˆY to\nthe loss function and then minimize the loss. Without loss\nof generality, we take K = 2 as an example and illustrate\nthe conditional structure in Figure 2.\n2.4. Variational Inference Calculation\nThe learning of the weights of our InVA is based on varia-\ntional inference. The marginal likelihood of imaging inputs\nconsists of the sum of the marginal likelihoods for indi-\nvidual data from different data sources and can be written\nas Eq. (3), where H(i) = {h1,i, · · · , hK,i} and Z(i) =\n{z1,i, · · · , zK,i} are hidden features of {X1,i, ..., XK,i} at\nshallow and deeper levels, respectively, for i = 1, ..., n.\nlog pθ(X1, · · · , XK) =\nn\nX\ni=1\nlog pθ(X1,i, · · · , XK,i),\n= KL\n\u0000qϕ(Z(i)|X(i))||pθ(Z(i)|X(i))\n\u0001\n+ L(θ, ϕ, X(i)).\n(3)\nThe first term in Eq. (3) is KL divergence between the pos-\nterior distribution of Z(i) and its variational approximation\nwhich is non-negative. Thus, to maximize the first term in\nEq. (3), we minimize the second term L(θ, ϕ, X(i)) which\nis called the variational lower bound on the marginal likeli-\nhood of i-th subject and can be written as Eq. (4):\nL(θ, ϕ, X(i)) =\nK\nX\nk=1\n\u001a\nEqϕ(zk,i|X(i))[log pθ(Xk,i|zk,i)]−\nKL\n\u0000qϕ(hk,i|Xk,i)|pθ(hk,i)\n\u0001\n− KL\n\u0000qϕ(zk,i|hk,i)|pθ(zk,i)\n\u0001\u001b\n,\n(4)\nwhere pθ(Xk,i|zk,i) is the data likelihood, qϕ(hk,i|Xk,i)\nand qϕ(zk,i|hk,i) represent the variational distribution of\nhk,i and zk,i, respectively. We assume these variational\ndistributions assume the form of p-variate Gaussian dis-\ntributions, i.e., qϕ(hk,i|Xk,i) = Qp\nj=1 N(µk,i,j, σk,i,j)\nand qϕ(zk,i|hk,i) = Qp\nj=1 N(mk,i,j, bk,i,j). The priors\nfor z and h are set as standard normal distribution, i.e.,\npθ(hk,i) = N(0, Ip) and pθ(zk,i) = N(0, Ip). During the\nminimization of Eq. (4), the first term represents the differ-\nence between input data X and the reconstructed ˆX, which\ncan be rewritten as\nEqϕ(zk,i|X(i))[log pθ(Xk,i|zk,i)] = ||Xk,i − ˆXk,i||2\n2.\n(5)\nFor the second term, it is a penalty for the data-specific\nfeature extraction and takes the form of a KL divergence\nbetween the variational distribution qϕ(hk,i|Xk,i) and the\nprior distribution on hk,i, which assumes a closed form,\nKL\n\u0000qϕ(hk,i|Xk,i)|pθ(hk,i)\n\u0001\n=\n1\n2\np\nX\nj=1\n(− log σ2\nk,i,j + µ2\nk,i,j + σ2\nk,i,j − 1).\n(6)\nIn addition, the third term is for the extracted deep features.\nThis term also assumes a closed form given by,\nKL\n\u0000qϕ(zk,i|hk,i)|pθ(zk,i)\n\u0001\n=\n1\n2\np\nX\nj=1\n(− log b2\nk,i,j + m2\nk,i,j + b2\nk,i,j − 1).\n(7)\n3. Simulation Studies\nWe generate simulated 3D input and output images to assess\nthe image prediction accuracy of our InVA in comparison to\nother baseline methods. To evaluate the models, we employ\nthe out-of-sample mean square error (MSE) between the\noutput images and the predicted images as our comparison\nmetric, with a smaller MSE indicating better prediction\nperformance. The specifics of the simulation settings are\nprovided in Section 3.1.\n3.1. Simulation Settings\nData Simulation: For the i-th subject, where i = 1, ..., n,\nwe generate two input images, X1,i and X2,i, with each be-\ning a 3-way tensor having dimensions d×d×d, comprising\n5\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nTable 1. Mean squared error comparison between our InVA and the variational autoencoder model (VAE), Bayesian varying coefficient\nmodel (Var-Coef), Bayesian additive regression trees (BART), and tensor regression (TensorReg) at n = 100 and d = 2. Across different\nsignal-to-noise ratios, our InVA outperforms baseline methods when the true relationship between images is complex and unknown\n(d = 2, 3), and is one of the best methods when d = 1.\nMethod\nData\norder = 1\norder = 2\norder = 3\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nVAE\nX1\n2.80\n2.88\n3.01\n15.12\n15.20\n15.35\n134.31\n134.56\n135.32\nVAE\nX2\n2.78\n2.89\n2.98\n15.14\n15.18\n15.32\n134.29\n134.59\n135.29\nVar-Coef\nX1 & X2\n0.01\n0.01\n0.25\n22.86\n22.95\n23.16\n61.27\n61.17\n61.15\nBART\nX1 & X2\n0.36\n0.43\n0.51\n5.18\n5.30\n5.39\n130.63\n130.67\n130.84\nTensorReg\nX1 & X2\n3.98\n4.08\n4.21\n15.89\n15.95\n16.05\n192.74\n192.82\n192.96\nInVA\nX1 & X2\n0.27\n0.41\n0.69\n2.75\n2.86\n3.10\n54.92\n55.21\n55.39\nTable 2. Mean squared error comparison between our InVA and the variational autoencoder model (VAE), Bayesian additive regression\ntrees (BART), and tensor regression (TensorReg) at n = 800 and d = 3. Across different signal-to-noise ratios and polynomial orders,\nour InVA outperforms baseline methods.\nMethod\nData\norder = 1\norder = 2\norder = 3\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nVAE\nX1\n4.10\n4.19\n4.34\n11.31\n11.44\n11.52\n60.23\n60.41\n60.75\nVAE\nX2\n4.12\n4.21\n4.32\n11.35\n11.43\n11.56\n60.26\n60.37\n60.72\nBART\nX1 & X2\n2.13\n2.24\n2.31\n14.77\n14.85\n14.94\n93.61\n93.85\n94.12\nTensorReg\nX1 & X2\n5.58\n5.65\n5.77\n21.82\n21.93\n22.01\n78.20\n78.45\n78.71\nInVA\nX1 & X2\n0.49\n0.62\n0.82\n5.72\n5.78\n6.17\n36.52\n36.75\n36.82\nd3 cells. Each cell entry of X1,i and X2,i is independently\nand identically simulated from the normal distribution\nN(0,1). The j = (j1, j2, j3)-th cell of the outcome image is\ngenerated from a polynomial of order O with varying coef-\nficients as follows: yi(j) = PO\no=1\nP2\nk=1 βo,k(j)xk,i(j)o +\nϵi(j), where ϵi(j)\ni.i.d.\n∼ N(0, σ2) and xk,i(j) represents the\njth cell of the kth input image for the ith sample. The co-\nefficient βo,k(j) is generated from a Gaussian process with\nan exponential correlation kernel, allowing for a nonlinear\nrelationship between the outcome and input images, as well\nas imposing correlation between cells of the outcome image.\nFurther, the scale parameter for the exponential correlation\nkernel is set at 0.25 for all these Gaussian processes to bor-\nrow information across input image coefficients. We explore\na wide range of simulation scenarios by varying the order\nof the polynomial O = 1, 2, 3 and the signal-to-noise ra-\ntio, represented by varying σ = 0.1, 0.3, 0.5. We consider\ntwo different combinations of (n, d) = (100, 2), (800, 3).\nThe setting n = 800, d = 3 closely matches the scenario\nin multi-modal neuroimaging data in Section 4. For the\ntest data, we further simulate the equivalent of 20% of the\ntraining data using the same simulation settings.\nBaselines: We conduct a comparison between our InVA\nand the Variational Autoencoder model (VAE), using either\nX1 = {X1,i : i = 1, .., n} or X2 = {X2,i : i = 1, .., n}\nas input, to evaluate the advantages of integrating infor-\nmation from multiple imaging inputs. They are denoted\nby VAE (X1) and VAE (X2), respectively. Furthermore,\nthe proposed model is contrasted with popular image-on-\nimage regression approaches, namely (i) Bayesian varying\ncoefficient model (Var-Coef) (Guhaniyogi et al., 2022), (ii)\nBayesian additive regression trees (BART) (Chipman et al.,\n1998), and (iii) tensor regression (TensorReg) (Gahrooei\net al., 2021). Both Var-Coef and BART are capable of\ncapturing nonlinear associations between input and output\nimages. In contrast, TensorReg conceptualizes each image\nas a tensor and establishes a regression framework between\ninput and output images, accounting for the tensor structure\nof images.\n3.2. Output Image Prediction Performance\nIn the scenario where n = 100 and d = 2, the performance\nof all competitors deteriorates under increased noise vari-\nance or when the relationship between the outcome and\ninput images becomes more complex, as evidenced by a\nhigher order of the polynomial governing the outcome im-\nage. As indicated in Table 1, InVA consistently achieves a\nsmaller MSE across various orders and noise levels σ com-\npared to TensorReg. This can be attributed to its effective\ncapture of the non-linear association between the outcome\nand input images. Although VAE (X1), VAE (X2) and\nBART also capture the non-linear association between the\noutcome and input images, InVA significantly outperforms\nall of them. Notably, the smaller MSE of InVA compared\n6\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nto VAE (X1) and VAE (X2) highlights the advantage of\nborrowing information from multiple imaging inputs. Var-\nCoef performs exceptionally well when the order of the\npolynomial used to simulate the outcome image is O = 1.\nIn this case, the fitted Var-Coef model is identical to the\ndata-generating model. However, InVA outperforms Var-\nCoef when the order O is set to 2 or 3. This suggests that\nour approach is advantageous when the true relationship\nbetween images is complex and unknown.\nIn the case of n = 800 and d = 3, InVA continues to outper-\nform the baseline competitors (refer to Table 2). Var-Coef is\nnot included as a baseline due to computational challenges\nwith n = 800. Similar to Table 1, Table 2 demonstrates a\ndecline in performance with increasing noise variance and\nthe order of the true data-generating polynomial. Impor-\ntantly, both tables establish significantly superior perfor-\nmance when information is suitably borrowed from the two\ninput images in predicting the output image.\n3.3. Ablation Studies\nWe do ablation studies to demonstrate the importance of\neach component in our InVA, where we train our InVA\nwithout shared components (InVA w/o Shd) and without in-\nput image-specific components (InVA w/o IS), respectively.\nIn InVA w/o Shd, we train input image-specific encoders\nand decoders for each modality and average their predictions\nto obtain a final prediction without using a shared encoder\nand decoder. In InVA w/o IS, we combine different modali-\nties and train a shared encoder and decoder without adding\ninput image-specific encoders and decoders.\nWe continue using mean squared error as the comparison\nmetric and summarize the results in Table 3. As we can see,\nacross different signal-to-noise ratios and polynomial orders,\nour InVA always has a smaller mean squared error com-\npared to InVA w/o Shd and InVA w/o IS, which indicates\nthat both the input image-specific and shared components in\nour InVA are crucial for the harmonization of multi-modal\nneuroimaging data analysis.\n4. Multi-modal Neuroimaging Data Analysis\nWe further apply our InVA approach in the study of multi-\nmodal neuroimaging data. Data used in the preparation of\nthis article were obtained from the Alzheimer’s Disease Neu-\nroimaging Initiative (ADNI) database (adni.loni.usc.edu)*.\n*Data used in preparation of this article were obtained from\nthe Alzheimer’s Disease Neuroimaging Initiative (ADNI) database\n(adni.loni.usc.edu). As such, the investigators within the ADNI\ncontributed to the design and implementation of ADNI and/or\nprovided data but did not participate in analysis or writing of this\nreport. A complete listing of ADNI investigators can be found at:\nThe primary goal of ADNI has been to test whether serial\nMRI, PET, other biological markers, and clinical and neu-\nropsychological assessment can be combined to measure\nthe progression of AD. Specifically, we consider the base-\nline visit for participants in the ADNI 1, GO, and 2 cohorts.\nThe goal of this analysis is to model molecular Aβ PET\nimages as a function of MRI images of cortical thickness\nand volume. To do so, PET and MRI images were regis-\ntered to a common template space and segmented into 40\nregions of interest (ROI) via the Desikan-Killiany cortical\natlas (Desikan et al., 2006) using standard ADNI pipelines\nas described in Marinescu et al. (2019). Measurements of\nAβ deposition were characterized by standardized uptake\nvalue ratio (SUVR) images which detect Aβ via binding\nof the florbetapir radiotracer. Cortical thickness and vol-\nume were extracted and measured in millimeters (mm) and\nmm3 using FreeSurfer (Fischl, 2012). Complete imaging\ndata was available for 711 subjects whose clinical status\nranged from some cognitive impairment to a diagnosis of\nAD. We randomly divided the data into two parts, one part\n80% as the training set, and one part 20% as the test set. The\ngoal in this data is to predict the PET image using cortical\nthickness and cortical volume obtained from MRI. In our\ncomparisons, all baseline competitors mentioned in Section\n3.1 are compared with InVA , excluding TensorReg and\nVar-Coef. Var-Coef is computationally demanding for the\nsize of the dataset, and TensorReg is not applicable to the\ndataset since the input and output images are not tensors in\nthe real data, unlike in our simulation settings.\nIn Figure 3, the displayed PET response is observed along-\nside the predicted PET response for a randomly selected sub-\nject, illustrating the accurate reconstruction of the observed\nPET response by the estimated PET response. Table 4\npresents the performance metrics, indicating that InVA out-\nperforms VAEs when utilizing either cortical thickness or\ncortical volume as input. This highlights the advantage of in-\ntegrating information from multiple input images. Addition-\nally, InVA demonstrates superior performance compared to\nBART, showcasing its ability to capture the complex non-\nlinear regression relationship between input images and the\noutput image. Overall, the results underscore the superior\npredictive performance of InVA .\n5. Conclusion and Discussions\nWe introduce a novel integrative variational autoencoder\napproach designed to leverage information from multiple\nimaging inputs, allowing for the development of a nonlin-\near relationship between input images and an image output.\nEmpirical results from simulation studies demonstrate the\nsuperior performance of our proposed approach compared\nto existing image-on-image regression methods, particu-\nhttp://adni.loni.usc.edu/-/.\n7\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nTable 3. Ablation studies: Mean squared error comparison between our InVA and our InVA without shared components (InVA w/o\nShd) and our InVA without input image-specific components (InVA w/o IS) at n = 100 and d = 2. Across different signal-to-noise\nratios and polynomial orders, our InVA outperforms InVA w/o Shd and InVA w/o IS, demonstrating the importance of both the input\nimage-specific and shared components in our InVA.\nMethod\nData\norder = 1\norder = 2\norder = 3\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nσ = 0.1\nσ = 0.3\nσ = 0.5\nInVA w/o Shd\nX1 & X2\n3.42\n3.49\n3.58\n11.48\n11.54\n11.62\n152.10\n152.26\n152.45\nInVA w/o IS\nX1 & X2\n1.48\n1.55\n1.61\n5.78\n5.85\n5.95\n101.63\n101.84\n102.08\nInVA\nX1 & X2\n0.27\n0.41\n0.69\n2.75\n2.86\n3.10\n54.92\n55.21\n55.39\n(a) Observed PET Image\n(b) Predicted PET Image\nFigure 3. Observed and predicted PET image for a randomly selected subject. The observed and predicted PET image show strong\nsimilarity, suggesting that the observed PET response is accurately reconstructed using the estimated PET response.\nTable 4. Mean squared error (MSE) comparison between the InVA\nand variational autoencoder model (VAE) and Bayesian additive\nregression trees (BART) for the real multi-modal neuroimaging\ndata. Our InVA produces smaller mean squared error, indicating\nmore accurate multi-modal neuroimaging data analysis.\nMethod\nData\nMSE\nVAE\nCortical Thickness\n0.0674\nVAE\nCortical Volume\n0.0659\nBART\nCortical Thickness & Volume\n0.0681\nInVA\nCortical Thickness & Volume\n0.0602\nlarly in drawing predictive inferences on the outcome image.\nThis approach holds transformative potential in the field of\nmulti-modal neuroimaging, especially in accurately predict-\ning costly tau-PET images using more affordable imaging\nmodalities for the study of neurodegenerative diseases, such\nas Alzheimer’s.\nDespite the harmonization of multi-modal neuroimaging\ndata modeling, this article does not comprehensively explore\nour approach for a gamut of other multi-modal perspective,\nsuch as text data, video data, and audio data (Jabeen et al.,\n2023; Xu et al., 2023). It is also important to provide appro-\npriate analysis techniques for these more diverse modalities,\nwhich can further improve the accuracy of image regression\nas corresponding new modality data become available. We\nplan to explore this issue in a future article. Additionally, it\nis intuitive that our integrative variational autoencoder can\nbe combined with existing uni-modal VAEs to equip each\nencoder and decoder component with a more expressive\narchitecture. Finding the optimal combination and design\nremains to be explored, and this may be a future research\ndirection.\nBroader Impact\nThe research presented in this paper holds the potential to\ntransform critical scientific domains, particularly at the inter-\nsection of machine learning and computational neuroscience.\nA notable gap in the utilization of hierarchical Bayesian\nmodeling in multi-modal neuroimaging data arises from the\nlimited exploration of effectively incorporating shared infor-\nmation across multiple imaging modalities, primarily due to\na lack of adequately expressive modeling architectures and\ncomputational hurdles. In addressing this challenge, this\n8\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\narticle introduces a novel methodology for efficiently mod-\neling the harmonization of multi-modal neuroimaging data.\nThis contribution is expected to serve as a catalyst for future\nresearch in effectively modeling shared information between\ndifferent imaging modalities in multi-modal neuroimaging\nanalysis. Furthermore, the work pioneers the development\nof a hierarchical VAE architecture for integrating multiple\nimages, setting the stage for potential advancements in en-\ncoder and decoder architectures through the integration of\nour approach with existing uni-modal VAEs.\nAcknowledgements\nData collection and sharing for this project was funded by\nthe Alzheimer’s Disease Neuroimaging Initiative (ADNI)\n(National Institutes of Health Grant U01 AG024904)\nand DOD ADNI (Department of Defense award number\nW81XWH-12-2-0012). ADNI is funded by the National In-\nstitute on Aging, the National Institute of Biomedical Imag-\ning and Bioengineering, and through generous contribu-\ntions from the following: AbbVie, Alzheimer’s Association;\nAlzheimer’s Drug Discovery Foundation; Araclon Biotech;\nBioClinica, Inc.; Biogen; Bristol-Myers Squibb Company;\nCereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals,\nInc.; Eli Lilly and Company; EuroImmun; F. Hoffmann-\nLa Roche Ltd and its affiliated company Genentech, Inc.;\nFujirebio; GE Healthcare; IXICO Ltd.; Janssen Alzheimer\nImmunotherapy Research & Development, LLC.; Johnson\n& Johnson Pharmaceutical Research & Development LLC.;\nLumosity; Lundbeck; Merck & Co., Inc.; Meso Scale Di-\nagnostics, LLC.; NeuroRx Research; Neurotrack Technolo-\ngies; Novartis Pharmaceuticals Corporation; Pfizer Inc.;\nPiramal Imaging; Servier; Takeda Pharmaceutical Com-\npany; and Transition Therapeutics. The Canadian Institutes\nof Health Research is providing funds to support ADNI\nclinical sites in Canada. Private sector contributions are\nfacilitated by the Foundation for the National Institutes of\nHealth (www.fnih.org). The grantee organization is the\nNorthern California Institute for Research and Education,\nand the study is coordinated by the Alzheimer’s Therapeutic\nResearch Institute at the University of Southern California.\nADNI data are disseminated by the Laboratory for Neuro\nImaging at the University of Southern California.\n9\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nReferences\nBlei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia-\ntional inference: A review for statisticians. Journal of\nthe American statistical Association, 112(518):859–877,\n2017.\nCalhoun, V. D. and Sui, J. Multimodal fusion of brain\nimaging data: A key to finding the missing link (s) in\ncomplex mental illness. Biological psychiatry: cognitive\nneuroscience and neuroimaging, 1(3):230–244, 2016.\nCamus, V., Payoux, P., Barr´e, L., Desgranges, B., Voisin, T.,\nTauber, C., La Joie, R., Tafani, M., Hommet, C., Ch´etelat,\nG., Mondon, K., de La Sayette, V., Cottier, J. P., Beaufils,\nE., Ribeiro, M. J., Gissot, V., Vierron, E., Vercouillie, J.,\nVellas, B., Eustache, F., and Guilloteau, D. Using PET\nwith 18F-AV-45 (florbetapir) to quantify brain amyloid\nload in a clinical environment. Eur. J. Nucl. Med. Mol.\nImaging, 39(4):621–631, April 2012.\nChen, M., Xu, Z., Weinberger, K., and Sha, F. Marginalized\ndenoising autoencoders for domain adaptation. arXiv\npreprint arXiv:1206.4683, 2012.\nChen, M., Weinberger, K., Sha, F., and Bengio, Y. Marginal-\nized denoising auto-encoders for nonlinear representa-\ntions. In International conference on machine learning,\npp. 1476–1484. PMLR, 2014.\nChipman, H. A., George, E. I., and McCulloch, R. E.\nBayesian cart model search. Journal of the American\nStatistical Association, 93(443):935–948, 1998.\nChorowski, J., Weiss, R. J., Bengio, S., and van den Oord,\nA. Unsupervised speech representation learning using\nwavenet autoencoders. IEEE/ACM transactions on au-\ndio, speech, and language processing, 27(12):2041–2053,\n2019.\nDai, X. and Li, L. Orthogonal statistical inference for multi-\nmodal data analysis. arXiv preprint arXiv:2103.07088,\n2021.\nDe Martino, F., De Borst, A. W., Valente, G., Goebel, R.,\nand Formisano, E. Predicting eeg single trial responses\nwith simultaneous fMRI and relevance vector machine\nregression. Neuroimage, 56(2):826–836, 2011.\nDesikan, R. S., S´egonne, F., Fischl, B., Quinn, B. T., Dick-\nerson, B. C., Blacker, D., Buckner, R. L., Dale, A. M.,\nMaguire, R. P., Hyman, B. T., Albert, M. S., and Killiany,\nR. J. An automated labeling system for subdividing the\nhuman cerebral cortex on MRI scans into gyral based\nregions of interest. Neuroimage, 31(3):968–980, July\n2006.\nDoersch, C. Tutorial on variational autoencoders. arXiv\npreprint arXiv:1606.05908, 2016.\nFischl, B. FreeSurfer. Neuroimage, 62(2):774–781, August\n2012.\nGahrooei, M. R., Yan, H., Paynabar, K., and Shi, J. Multiple\ntensor-on-tensor regression: An approach for modeling\nprocesses with heterogeneous sources of data. Techno-\nmetrics, 63(2):147–159, 2021.\nGeng, J., Fan, J., Wang, H., Ma, X., Li, B., and Chen, F.\nHigh-resolution sar image classification via deep con-\nvolutional autoencoders. IEEE Geoscience and Remote\nSensing Letters, 12(11):2351–2355, 2015.\nGirin, L., Leglaive, S., Bie, X., Diard, J., Hueber, T.,\nand Alameda-Pineda, X.\nDynamical variational au-\ntoencoders: A comprehensive review. arXiv preprint\narXiv:2008.12595, 2020.\nGregor, K., Danihelka, I., Graves, A., Rezende, D., and\nWierstra, D. Draw: A recurrent neural network for im-\nage generation. In International conference on machine\nlearning, pp. 1462–1471. PMLR, 2015.\nGuha, S. and Guhaniyogi, R. Bayesian generalized sparse\nsymmetric tensor-on-vector regression. Technometrics,\n63(2):160–170, 2021.\nGuhaniyogi, R. and Rodriguez, A. Joint modeling of longi-\ntudinal relational data and exogenous variables. 2020.\nGuhaniyogi, R. and Spencer, D. Bayesian tensor response\nregression with an application to brain activation studies.\nBayesian Analysis, 16(4):1221–1249, 2021.\nGuhaniyogi, R., Li, C., Savitsky, T. D., and Srivastava, S.\nDistributed bayesian varying coefficient modeling using a\ngaussian process prior. The Journal of Machine Learning\nResearch, 23(1):3642–3700, 2022.\nGuo, C., Kang, J., and Johnson, T. D. A spatial bayesian\nlatent factor model for image-on-image regression. Bio-\nmetrics, 78(1):72–84, 2022.\nHampel, H., Hardy, J., Blennow, K., Chen, C., Perry, G.,\nKim, S. H., Villemagne, V. L., Aisen, P., Vendruscolo, M.,\nIwatsubo, T., Masters, C. L., Cho, M., Lannfelt, L., Cum-\nmings, J. L., and Vergallo, A. The Amyloid-β pathway in\nalzheimer’s disease. Mol. Psychiatry, 26(10):5481–5503,\nOctober 2021.\nHao, X. and Shafto, P. Coupled variational autoencoder.\narXiv preprint arXiv:2306.02565, 2023.\nHazra, A., Reich, B. J., Reich, D. S., Shinohara, R. T., and\nStaicu, A.-M. A spatio-temporal model for longitudinal\nimage-on-image regression. Statistics in biosciences, 11:\n22–46, 2019.\n10\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nJabeen, S., Li, X., Amin, M. S., Bourahla, O., Li, S., and\nJabbar, A. A review on methods and applications in mul-\ntimodal deep learning. ACM Transactions on Multimedia\nComputing, Communications and Applications, 19(2s):\n1–41, 2023.\nJanjos, F., Rosenbaum, L., Dolgov, M., and Z¨ollner, J. M.\nUnscented autoencoder. In International Conference on\nMachine Learning, pp. 14758–14779. PMLR, 2023.\nJansen, M., White, T. P., Mullinger, K. J., Liddle, E. B.,\nGowland, P. A., Francis, S. T., Bowtell, R., and Liddle,\nP. F. Motion-related artefacts in EEG predict neuronally\nplausible patterns of activation in fMRI data. Neuroimage,\n59(1):261–270, 2012.\nJeong, Y. J., Park, H. S., Jeong, J. E., Yoon, H. J., Jeon, K.,\nCho, K., and Kang, D.-Y. Restoration of amyloid pet\nimages obtained with short-time data using a generative\nadversarial networks framework. Scientific reports, 11\n(1):4825, 2021.\nJin, J., Riviere, M.-K., Luo, X., and Dong, Y. Bayesian\nmethods for the analysis of early-phase oncology basket\ntrials with information borrowing across cancer types.\nStatistics in Medicine, 39(25):3459–3475, 2020.\nKaplan, D., Chen, J., Yavuz, S., and Lyu, W. Bayesian\ndynamic borrowing of historical information with appli-\ncations to the analysis of large-scale assessments. Psy-\nchometrika, 88(1):1–30, 2023.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\nKlushyn, A., Chen, N., Kurle, R., Cseke, B., and van der\nSmagt, P. Learning hierarchical priors in vaes. Advances\nin neural information processing systems, 32, 2019.\nKviman, O., Mol´en, R., Hotti, A., Kurt, S., Elvira, V., and\nLagergren, J. Cooperation in the latent space: The bene-\nfits of adding mixture components in variational autoen-\ncoders. In International Conference on Machine Learn-\ning, pp. 18008–18022. PMLR, 2023.\nLee, S. Y., Lei, B., and Mallick, B. Estimation of covid-\n19 spread curves integrating global data and borrowing\ninformation. PloS one, 15(7):e0236860, 2020.\nLei, B., Kirk, T. Q., Bhattacharya, A., Pati, D., Qian, X.,\nArroyave, R., and Mallick, B. K. Bayesian optimization\nwith adaptive surrogate models for automated experimen-\ntal design. Npj Computational Materials, 7(1):194, 2021.\nLock, E. F. Tensor-on-tensor regression. Journal of Compu-\ntational and Graphical Statistics, 27(3):638–647, 2018.\nMarinescu, R. V., Oxtoby, N. P., Young, A. L., Bron, E. E.,\nToga, A. W., Weiner, M. W., Barkhof, F., Fox, N. C., Gol-\nland, P., Klein, S., and Alexander, D. C. TADPOLE chal-\nlenge: Accurate alzheimer’s disease prediction through\ncrowdsourced forecasting of future data. Predict Intell\nMed, 11843:1–10, October 2019.\nMiranda, M. F., Zhu, H., and Ibrahim, J. G. TPRM: Tensor\npartition regression models with applications in imaging\nbiomarker detection. The annals of applied statistics, 12\n(3):1422, 2018.\nMu, J. Spatially varying coefficient models: Theory and\nmethods. PhD thesis, Iowa State University, 2019.\nMu, J., Wang, G., and Wang, L. Estimation and inference\nin spatially varying coefficient models. Environmetrics,\n29(1):e2485, 2018.\nNakamura, H., Okada, M., and Taniguchi, T. Representa-\ntion uncertainty in self-supervised learning as variational\ninference. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 16484–16493, 2023.\nNazari, P., Damrich, S., and Hamprecht, F. A. Geometric\nautoencoders–what you see is what you decode. arXiv\npreprint arXiv:2306.17638, 2023.\nNg,\nA.\nand\nAutoencoder,\nS.\nCs294a\nlecture\nnotes.\nDosegljivo:\nhttps://web.\nstanford.\nedu/class/cs294a/sparseAutoencoder 2011new.\npdf.[Dostopano 20. 7. 2016], 2011.\nNiyogi, P. G., Lindquist, M. A., and Maiti, T. A tensor based\nvarying-coefficient model for multi-modal neuroimaging\ndata analysis. arXiv preprint arXiv:2303.16443, 2023.\nOnishi, Y., Hashimoto, F., Ote, K., Matsubara, K., and\nIbaraki, M. Self-supervised pre-training for deep image\nprior-based robust pet image denoising. IEEE Transac-\ntions on Radiation and Plasma Medical Sciences, 2023.\nPei, Y. et al. A study on feature extraction of handwriting\ndata using kernel method-based autoencoder. In 2018\n9th International Conference on Awareness Science and\nTechnology (iCAST), pp. 1–6. IEEE, 2018.\nRanganath, R., Tran, D., and Blei, D. Hierarchical varia-\ntional models. In International conference on machine\nlearning, pp. 324–333. PMLR, 2016.\nRanjan, R., Patel, V. M., and Chellappa, R. Hyperface:\nA deep multi-task learning framework for face detec-\ntion, landmark localization, pose estimation, and gender\nrecognition. IEEE transactions on pattern analysis and\nmachine intelligence, 41(1):121–135, 2017.\n11\nInVA: Integrative Variational Autoencoder for Harmonization of Multi-modal Neuroimaging Data\nRifai, S., Mesnil, G., Vincent, P., Muller, X., Bengio, Y.,\nDauphin, Y., and Glorot, X. Higher order contractive auto-\nencoder. In Machine Learning and Knowledge Discovery\nin Databases: European Conference, ECML PKDD 2011,\nAthens, Greece, September 5-9, 2011, Proceedings, Part\nII 22, pp. 645–660. Springer, 2011a.\nRifai, S., Vincent, P., Muller, X., Glorot, X., and Bengio,\nY. Contractive auto-encoders: Explicit invariance during\nfeature extraction. In Proceedings of the 28th interna-\ntional conference on international conference on machine\nlearning, pp. 833–840, 2011b.\nSønderby, C. K., Raiko, T., Maaløe, L., Sønderby, S. K., and\nWinther, O. Ladder variational autoencoders. Advances\nin neural information processing systems, 29, 2016.\nSpotorno, N., Strandberg, O., Vis, G., Stomrud, E., Nilsson,\nM., and Hansson, O. Measures of cortical microstructure\nare linked to amyloid pathology in alzheimer’s disease.\nBrain, 146(4):1602–1614, April 2023.\nSu, L., Chen, X., Zhang, J., and Yan, F. Comparative study\nof bayesian information borrowing methods in oncology\nclinical trials. JCO Precision Oncology, 6:e2100394,\n2022.\nSubramanian, K., Martinez, J., Huicochea Castellanos, S.,\nIvanidze, J., Nagar, H., Nicholson, S., Youn, T., Nau-\nseef, J. T., Tagawa, S., and Osborne, J. R. Complex\nimplementation factors demonstrated when evaluating\ncost-effectiveness and monitoring racial disparities as-\nsociated with [18f] dcfpyl pet/ct in prostate cancer men.\nScientific Reports, 13(1):8321, 2023.\nSweeney, E., Shinohara, R., Shea, C., Reich, D., and\nCrainiceanu, C. M. Automatic lesion incidence estima-\ntion and detection in multiple sclerosis using multise-\nquence longitudinal mri. American Journal of Neuroradi-\nology, 34(1):68–73, 2013.\nTolstikhin, I., Bousquet, O., Gelly, S., and Schoelkopf,\nB.\nWasserstein auto-encoders.\narXiv preprint\narXiv:1711.01558, 2017.\nTschannen, M., Bachem, O., and Lucic, M. Recent advances\nin autoencoder-based representation learning.\narXiv\npreprint arXiv:1812.05069, 2018.\nVahdat, A. and Kautz, J. Nvae: A deep hierarchical vari-\national autoencoder. Advances in neural information\nprocessing systems, 33:19667–19679, 2020.\nXu, P., Zhu, X., and Clifton, D. A. Multimodal learning with\ntransformers: A survey. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2023.\nYan, X., Hu, S., Mao, Y., Ye, Y., and Yu, H. Deep multi-\nview learning methods: A review. Neurocomputing, 448:\n106–129, 2021.\nYu, X., Xu, M., Zhang, Y., Liu, H., Ye, C., Wu, Y., Yan,\nZ., Zhu, C., Xiong, Z., Liang, T., et al. Mvimgnet: A\nlarge-scale dataset of multi-view images. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 9150–9161, 2023.\nZhang, J., He, X., Qing, L., Gao, F., and Wang, B. BP-\nGAN: Brain PET synthesis from MRI using generative\nadversarial network for multi-modal alzheimer’s disease\ndiagnosis. Comput. Methods Programs Biomed., 217(C),\nApril 2022.\nZhang, Y., Li, X., Ji, Y., Ding, H., Suo, X., He, X., Xie,\nY., Liang, M., Zhang, S., Yu, C., and Qin, W. MRAβ:\nA multimodal MRI-derived amyloid-β biomarker for\nalzheimer’s disease. Hum. Brain Mapp., 44(15):5139–\n5152, October 2023.\nZhao, Y. and Linderman, S. Revisiting structured variational\nautoencoders. In International Conference on Machine\nLearning, pp. 42046–42057. PMLR, 2023.\nZhu, H., Fan, J., and Kong, L. Spatially varying coefficient\nmodel for neuroimaging data with jump discontinuities.\nJournal of the American Statistical Association, 109(507):\n1084–1098, 2014.\n12\n"
}