{
    "optim": "Can We Generate Realistic Hands Only Using Convolution?\nMehran Hosseini*\nDepartment of Informatics\nKing’s College London\nmehran.hosseini@kcl.ac.uk\nPeyman Hosseini∗\nSchool of Electronic Engineering & Computer Science\nQueen Mary University of London\ns.hosseini@qmul.ac.uk\nAbstract\nThe enduring inability of image generative models to\nrecreate intricate geometric features, such as those present\nin human hands and fingers has been an ongoing problem\nin image generation for nearly a decade. While strides have\nbeen made by increasing model sizes and diversifying train-\ning datasets, this issue remains prevalent across all models,\nfrom denoising diffusion models to Generative Adversarial\nNetworks (GAN), pointing to a fundamental shortcoming in\nthe underlying architectures. In this paper, we demonstrate\nhow this problem can be mitigated by augmenting convo-\nlution layers geometric capabilities through providing them\nwith a single input channel incorporating the relative n-\ndimensional Cartesian coordinate system.\nWe show that\nthis drastically improves quality of hand and face images\ngenerated by GANs and Variational AutoEncoders (VAE).\n1. Introduction\nGenerative models have gained immense popularity and\ngenerated unprecedented hype in the last few years, revo-\nlutionising the way we approach tasks that involve gener-\nating new content. State-of-the-art image generative mod-\nels, such as OpenAI’s DALL·E 3 [44–46], Stable Diffusion\n[47], Midjourney [37], and Nvidia’s StyleGAN [23–25] are\nused to create mesmerising high-resolution images.\nHowever, all of these models have a peculiar shortcom-\ning when it comes to learning and reproducing certain geo-\nmetric patterns, such as those present in human hands and\nfingers. For example, Fig. 1b shows the images generated\nby DALL·E 3, when prompted “a realistic human hand\nshowing number n”, for n = 2, 4. This phenomenon is uni-\nversally present in all families of generative models, from\nGANs [12] to denoising diffusion models [17, 52], whether\nthey are based on convolution [11, 29], Vision Transformers\n(ViT) [10, 43], or a combination of both [58].\nHuman painters, on the other hand, are able to draw flaw-\n∗Equal contribution.\n(a) Hand drawn\n(b) DALL·E 3\n(c) ConvGAN\n(d) GeoGAN\nFigure 1. Human hands showing numbers 2 and 4 as drawn by\nhand (Fig. 1a) and as generated by DALL·E 3 (Fig. 1b), a stan-\ndard convolutional GAN (Fig. 1c), and GeoGAN (ours) (Fig. 1d).\nThe comparison is between ConvGAN and GeoGAN only. Images\ngenerated by DALL·E 3 are included to illustrate the struggles of\nSoA models in generating human hands.\nless pictures of hands. It is, in part, because, unlike the\ngenerative models, painters know how hands work, provid-\ning them with a knowledge of what hands can and cannot\ndo. Another contributing factor is that human painters learn\nhow to draw hands by breaking down and simplifying them\ninto simple geometric shapes, as shown in Fig. 1a.\nGenerative models’ shortcomings are caused by two con-\ntributing factors, models’ design and architecture, as well as\nthe training dataset and methodology. Taking into consider-\nation that the SoA image generative models are trained on a\nvast collection of images on the Internet and are further en-\nhanced by methods, such as Reinforcement Learning with\nHuman Feedback (RLHF) [8], the latter is not the root of\nthe problem. In the last few years, it has become evident\nthat the model size directly corresponds to the quality of\ngenerated images, resulting in models that produce hyper-\nrealistic images with incredible texture and lighting, which\nyet fall short in generating intricate patterns, indicating the\nfundamental inability of these models in learning the geo-\nmetric representation of human hands and fingers. To better\nunderstand the ability, or lack thereof, of convolution op-\narXiv:2401.01951v1  [cs.CV]  3 Jan 2024\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n0\n1\n2\n3\n4\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n2\n2\n2\n2\n2\n3\n3\n3\n3\n3\n4\n4\n4\n4\n4\n+\n+\n-1 -1 -1 -1 -1\n-1 -1 -1 -1 -1\n-1 -1 -1 -1 -1\n-1 -1 -1 -1 -1\n-1 -1 -1 -1 -1\n-3 -3 -3 -3 -3\n-3 -3 -3 -3 -3\n-3 -3 -3 -3 -3\n-3 -3 -3 -3 -3\n-3 -3 -3 -3 -3\n=\n=\n-1 0\n1\n2\n3\n-1 0\n1\n2\n3\n-1 0\n1\n2\n3\n-1 0\n1\n2\n3\n-1 0\n1\n2\n3\n-3 -3 -3 -3 -3\n-2 -2 -2 -2 -2\n-1 -1 -1 -1 -1\n0\n0\n0\n0\n0\n1\n1\n1\n1\n1\n+\n=\n-4 -3 -2 -1 0\n-3 -2 -1 0\n1\n-2 -1 0\n1\n2\n-1 0\n1\n2\n3\n0\n1\n2\n3\n4\nFigure 2. A 5 × 5 geometry channel of rank 2 is illustrated in the\nrightmost tensor. The top and bottom rows correspond to horizon-\ntal and vertical coordinates, respectively. The standard horizontal\nand vertical coordinates are shown in the leftmost column. Tensors\nin the second column show random horizontal and vertical shifts.\nIn the implementation, coordinate channels are divided by their\nsizes (in this case 4), and for optimisation, we sample horizontal\nand vertical shifts at once as a single random number representing\ntheir sum; thus, reducing the number of additions and samplings.\neration in learning geometric information, we evaluate its\nperformance on a geometric task, computing the centre of\nmass of finitely many points in a 2-dimensional plane.\nIn this paper, we show that by providing convolution lay-\ners with a single Geometry Channel (GeoChannel), encod-\ning the Cartesian coordinates, as presented in Fig. 2, we sig-\nnificantly improve convolution’s capabilities. As illustrated\nin Fig. 3, GeoChannel is appended to the convolution’s in-\nput. We refer to the consecutive concatenation of GeoChan-\nnel and application of convolution as GeoConv. Compared\nto the existing approaches, like CoordConv [32], GeoConv\n1. as we prove in Theorem 2, is computationally optimal\nand only concatenates a single channel to the convolu-\ntion’s input, compared to the n channels of CoordConv\nfor n-dimensional convolution,\n2. allows for random translations (shifts) of the Cartesian\ncoordinate system to avoid learning incorrect absolute\npositional correlations (cf. Sec. 3.2), and\n3. is more robust due to the smoothing effect of random\nshifts, making it the ideal candidate in generative appli-\ncations, such as in GANs and VAEs.\nNote that the random shifts in the GeoChannel are dif-\nferent from those of the input. In particular, we found out\ncontrary to the claim of [32], the mere addition of coor-\ndinate channels does not prevent mode collapse in GANs,\neven when we augment its inputs with random transforma-\ntions. In fact, we found out that CoordConv, which does not\nincorporate random shifts, is more prone to mode collapse\nin GANs than even the vanilla convolution.\nIn the rest of Sec. 3, we show that a GeoConv-based\nGAN (GeoGAN) allows us to generate realistic hand ges-\ntures in the American Sign Language (ASL), while a stan-\nµ\nσ\nFigure 3. GeoConv in a VAE. Purple blocks indicate the input\nand output tensors, yellow blocks represent the output tensors re-\nsulting from previous layers’ convolution operation, and orange\nblocks indicate the geometry channels appended to them during\nthe GeoConv’s operation before applying the next convolution.\ndard Convolutional GAN (ConvGAN) with the same design,\nbut based on standard convolution, as well as SoA mod-\nels, such as DALL·E 3 fall short of achieving the same as\nshown in Fig. 1. Before presenting our results on hand ges-\nture synthesis, we evaluate GeoGANs on the widely used\nCelebA-HQ. In more details, the experiments in this paper\nare organised in the following order.\n• We first demonstrate Theorems 1 and 2 in practice on two\nsmall geometric experiments in Sections 3.2 and 3.1.\n• In Sec. 3.3.2, we show that a GeoGAN trained on the\nCelebA-HQ dataset [22] generates more realistic human\nfaces than a similar ConvGAN. Moreover, while Con-\nvGAN collapses within 250 epochs, GeoGAN remains\nstable throughout the training and produces more diverse\nimages that match the dataset’s distribution.\n• In Sec. 3.3.2, we train the GANs using the Wasserstein\ndistance [1, 21, 53] and gradient penalty [13] to prevent\nmode collapse in the ConvGAN. The resulting models are\ncommonly referred to as WGAN-GP. Nevertheless, Ge-\noGAN retains its edge.\n• In Sec. 3.3.3, we show that the same GeoGAN trained on\nthe Hand Gesture dataset [4], generates realistic hand ges-\ntures in the American sign language, while the ConvGAN\nstruggles to properly generate many of the hand gestures\n(cf. Figures 1c, 1d and 6.)\n• In Sec. 3.4, we evaluate GeoConv for using in VAEs,\nsince VAEs offer numerical metrics that allow comparing\nGeoConv to CoordConv and standard convolution quanti-\ntatively. We repeat the experiments with VAEs, and train\nthem on CelebA dataset [33]. The VAE based on Geo-\nConv outperforms other VAEs in both image quality and\ndiversity, as well as in achieving smaller losses.\nRelated work\nCNNs have been ubiquitously deployed to achieve superhu-\nman performance in image classification and object detec-\ntion [14, 49]. More recently, they have been used for image\ngeneration using GANs [12, 22, 42], VAEs [26, 44, 45], and\ndenoising diffusion models [17, 52].\nIn recent years, there has been a surge in the adoption\nof ViT [10, 43], inspired by the successful adoption of the\nattention mechanism [2] and transformers [54] in natural\nlanguage processing. Despite their tremendous success in\nvision tasks, recent studies indicate that CNNs are on par\nwith ViT in both accuracy [34, 51] and robustness [3, 41].\nCNNs differ from human vision in many ways [27]. For\nexample, they are often criticised for their limited recep-\ntive field, preventing them from learning wide-apart fea-\ntures within images [9, 27, 36]. Some of the attempts to ad-\ndress this issue include augmenting CNNs with transform-\ners [15], using deformable CNNs [9], and augmenting con-\nvolutions with coordinate information [32]. It is worth men-\ntioning that similar ideas have been used to improve ViT as\nwell [18, 59]; however, these approaches are fundamentally\ndifferent from the approach taken here, not because of their\nfocus on transformers, but mainly because they try to ad-\ndress a different problem in ViTs.\nLiu et al. [32] demonstrated that CNNs also fail in trans-\nforming the spatial representation between input and output.\nThey introduced CoordConv as a solution to this problem of\nCNNs. CoordConv adds one channel per input dimension\nto the convolution’s input, called coordinate channel. This\nhas proven to improve CNNs’ performance in an array of\ntasks [32]. CoordConv has since been adopted in an array\nof applications [7, 31, 35, 55]. Nonetheless, CoordConv has\nseveral drawbacks as we discuss in more details in Problems\n1 and 2 as well as in Sec. 3.\n2. Geometry-aware convolution\nAs we discussed in the related work, CoordConv mitigates\nthe limited receptive field of convolutional layers as well as\ntheir inability to learn positional information in images by\nadding two coordinate channels, one for each dimension,\nbefore applying the convolution operation. These channels\nare shown in the two leftmost columns in Fig. 2. Coord-\nConv has shown considerable improvements compared to\nconvolution in an array of tasks [7, 31, 35, 55]. However,\nas we show in this paper, CoordConv has several drawbacks\nboth in theory and practice. In theory,\n1. CoordConv learns absolute positional correlations from\nthe dataset, thus, resulting in biased models with poor\nperformance in various tasks, while GeoConv learns the\nrelative positional correlations when using the random\nshift (cf. Theorem 1), and\n2. CoordConv is suboptimal (cf. Theorem 2), i.e., it in-\ntroduces nℓs1 · · · sn learnable parameters for a single\nn-dimensional convolution operation with kernel size\ns1 × · · · × sn and ℓ output channels, instead of Geo-\nConv’s ℓs1 · · · sn extra parameters.\nAs we demonstrate in Sec. 3, these problems result in sub-\npar performance in practice.\nIn this section, we introduce the Geometry-aware Convo-\nlution, or GeoConv for short, which not only resolves con-\nvolution’s limited receptive field and inability to learn po-\nsitional information, but also addresses the aforementioned\nproblems of CoordConv. In summary, GeoConv works as\nfollows. Given an input tensor of size r1 × · · · × rn with\nk channels x ∈ Rr1×···×rn×k, we first create a GeoChan-\nnel g ∈ Rr1×···×rn, encoding the coordinates as well as a\nrandom coordinate shift, similar to the one in the right most\ncolumn of Fig. 2. Tensor g is then appended to x resulting\nin tensor (x, g) ∈ Rr1×···×rn×(k+1), which is then fed into\nan n-dimensional convolution f. To better understand how\nGeoConv works, let us begin by describing how it resolves\nproblems 1 and 2.\nSolution to Problem 1.\nThe problem with adding the raw\ncoordinate channels to the images is that, in addition to\nlearning the spatial information about the image content,\nthe model develops correlations between features and where\nthey appear in images rather than their relative position with\nrespect to one another. This is a fundamental flaw in most\napplications. For instance, if due to the bias in the training\ndataset a feature mostly appears in a certain part of the im-\nages, the model begins to develop bias for the position of\nthat feature. Such correlations are undesirable in most real-\nworld scenarios. For example, when training face recogni-\ntion models, the input images or videos are nicely cropped\nand the faces are centred in the training set; however, in the\nreal world, where the model is deployed, this is rarely the\ncase. Thus, it is more essential for a face recognition model\nto learn where a person’s facial features are located with re-\nspect to each other than where they are exactly located in the\ninput image or video. In Sec. 3.2, we explore this problem\nof CoordConv and GeoConv’s solution in more detail.\nTherefore, in GeoConv, we introduce random shifts to\ncoordinate channels to prevent the model from learning un-\nwanted positional bias, as formally stated and proven in\nTheorem 1. Random shifts are shown in the second col-\numn of Fig. 2. Note that these random shifts are different\nfrom random shifts applied to the input in data augmenta-\ntion, e.g., values on the edge of the GeoChannel are defined\nin the same way as the ones in the centre, unlike the input’s\nrandom shift, where the values on the edge are defined via\nsome padding. Most notably, applying random shifts to the\ninput does not prevent mode collapse in GANs that utilise\nCoordConv architecture.\nTheorem 1. When using random shift, GeoConv learns the\nrelative positional information rather than the absolute po-\nsitional information, as in CooordConv.\nProof. Let us denote the convolution operator with ∗. As\nwe prove in Theorem 2, we can combine the n coordi-\nnate channels of CoordConv to a single channel, similar to\nGeoConv (but with no random shift), without affecting its\nperformance. We denote this channel by c and GeoConv’s\nGeoChannel by g. Now, given an input tensor x of rank n\nwith k channels, an s1 ×· · ·×sn convolution operator f on\nthe k input channels, and a single GeoChannel g (amount-\ning to a total of k + 1 channels), we have that\nf ∗ (x, g) = f (1,...,k)∗ x + f (k+1)∗ g,\n(1)\nwhere f (1,...,k) and f (k+1) denote the first k filters of f and\nthe last filter of f corresponding to the input and GeoChan-\nnel, respectively. Let g′ = f (k+1)∗ g. We observe that\ng′\nj1,...,jn =\nX\ni1,...,in\nf (k+1)\ni\ngj1+i1,...,jn+in\n=\nX\ni1,...,in\nf (k+1)\ni\n(cj1+i1,...,jn+in + r)\n=\nX\ni1,...,in\nf (k+1)\ni\ncj1+i1,...,jn+in + s1 · · · snr\n= f (k+1)∗ c + s1 · · · snr,\n(2)\nwhere r is a random shift sampled from a uniform distri-\nbution in GeoConv and 1 ≤ jℓ ≤ tℓ for 1 ≤ ℓ ≤ n, with\nt1 × · · · × tn being the input shape. It follows from Equa-\ntions (1) and (2) that\nf ∗ (x, g) = f ∗ (x, c) + s1 . . . snr.\n(3)\nHence, f ∗ (x, g) is equal to f ∗ (x, c) modulo a random\nnumber s1 . . . snr. This prevents GeoConv from develop-\ning unwanted correlations between f ∗ (x, c) and locations\nresulting in this value, while still allowing it to learn the\npatterns present in x.\nSolution to Problem 2.\nCoordConv adds one coordinate\nchannel per dimension to the input. Nevertheless, as we\nformally state and prove in Theorem 2, this is unnecessary\nand inefficient, and in fact, a single filter is sufficient.\nTheorem 2. An s1 × · · · × sn convolution filter on the ℓ-th\ncoordinate channel c(ℓ) in CoordConv does not extract any\nmore information than a 1 × · · · × 1 × sℓ × 1 × · · · × 1\nconvolution filter.\nProof. Let us use the same notation as in Theorem 1. Since\nthe proof is similar for all coordinate channels, we only\nprove this for the first channel. Let f = (fi1,...,in) be the\nconvolution filter corresponding the first coordinate channel\nc in CoordConv. Let\n¯fi1 =\nX\ni2,...,in\nfi1,...,in,\n(4)\nwhere 1 ≤ ik ≤ sk for 1 ≤ k ≤ n. At each step of the\nconvolution operation, we have that\nX\ni1,...,in\nfi1,...,inci1+j1,...,in+jn\n=\nX\ni1\n(\nX\ni2,...,in\nfi1,...,in)ci1+j1,j2,...,jn\n=\nX\ni1\n¯fiℓci1+j1,j2,...,jn.\n(5)\nHence, the s1 × · · · × sn filter f does not extract any more\ninformation from the first coordinate channel c than the s1×\n1 × · · · × 1 filter ¯f = ( ¯fi1).\nTherefore, in GeoConv, we combine all coordinate\nchannels into one by adding them together, resulting in\nGeoChannel, illustrated in the rightmost column of Fig. 2.\nThe GeoChannel is then concatenated to the input chan-\nnels as demonstrated in Fig. 3. By using a single geom-\netry channel instead of the n coordinate channels in Co-\nordConv, alongside the random shift, we achieve superior\nperformance compared to CoordConv while using (n − 1)ℓ\nless filter per convolution, where ℓ is the number of out-\nput channels of the convolution.\nConsequently, we use\n(n−1)ℓs1s2 · · · sn less learnable parameters. This provides\nus with a model that is easier to train, faster, smaller, and\nthus, deployable in a wider range of edge devices.\n3. Evaluation\nIn this section, we evaluate GeoConv on a comprehensive\nrange of tasks. In Sec. 3.1, we evaluate GeoConv capa-\nbility in geometric tasks by introducing the centre of mass\nbenchmark, where GeoConv outperforms convolution and\nCoordConv by up to 50% and 35%, respectively (cf. Fig. 4).\nIn Sec. 3.2, we compare all three architectures on a task\nfor their absolute positional bias on a simple task consist-\ning of classifying images containing the Greek numbers I,\nII, and III. GeoConv and convolution demonstrate the least\nbias, while CoordConv has the most. In Sec. 3.3, we com-\npare all these architectures for use in GANs. We consider\nstandard GAN [12] as well as WGAN-GP [13] for gener-\nating human faces by training on the CelebA-HQ [22] and\nhands by training on the Hand Gesture dataset [4]. Geo-\nConv generates the most realistic and diverse faces and\nhands, while CoordConv collapses in early epochs, per-\nforming even worse than standard convolution.\nWe also compute the Fr´echet Inception Distance (FID)\n[16] and Inception Score (IS) [48] of the images generated\nby GANs using different architectures. Despite Generating\nmore realistic and diverse images, GeoConv does not al-\nways score favourably in terms of FID and IS, casting doubt\non the applicability and validity of these metrics. We pre-\ndict this is due to the Inception network’s different training\nset and architectural flaws due to using vanilla convolution.\nFinally, we compare all three layers for use in VAEs in\nSec. 3.4. GeoConv again outperforms others in terms of im-\nage quality and diversity as well as achieving smaller losses\non both train and validation data. We have included the in-\ndepth details of all experiments in Appendix A.\nThe code for all the experiments is provided in the sup-\nplementary materials. All of the experiments have been per-\nformed in a GPU-poor setting on a computer with 128 GB\nof RAM and a single GeForce RTX 4090 GPU.\n3.1. Calculating centre of mass\nIn this benchmark, the goal is to compute the centre of mass\nof finitely many points in an image. The benchmark consists\nof datasets with different densities d. Each dataset consists\nof images containing white points on a black canvas, and\nd denotes the percentage of white points in the images in a\ndataset. Details of the dataset are included in Appendix A.1.\nFor the ablation study, we consider four different designs\nwith i layers and j filters for i, j ∈ {1, 2}, denoted by\nixj. Models are trained on datasets with different densities,\nd ∈\n\b\n0.001 × 3k : 0 ≤ k ≤ 6\n\t\n, using the Euclidean norm.\nTherefore, for each training density d, a total of 3 × 4 = 12\nmodels are trained. All models are then evaluated on the\ntest sets with the same density as well as all other densities.\nTo make the comparison comprehensive, we also com-\nputed the normalised losses (by dividing by the summa-\ntion over all losses across different architectures and test\nand train ratios). We have explained this in detail in Ap-\npendix A.1. As outlined in Tab. 1, GeoConv shows consid-\nerable advantage compared to convolution and CoordConv,\noutperforming them by 46% and 57%, respectively. More-\nover, Fig. 4 shows the normalised losses averaged over all\ntrain and test densities for each architecture and for different\nnumber of layers and filters. Again, GeoConv outperforms\nconvolution and CoordConv in all combinations.\n3.2. Positional dependencies\nThis experiment is designed to evaluate the (absolute) posi-\ntional bias learnt by different architectures. The models are\ntrained on the train dataset consisting of images of Greek\nnumbers I, II, and III; However, the distribution of where\nthe numbers are located in the images is different among\nthe train and test datasets, as described in Appendix A.2.\nTable 1. Comparison of average, normalised average, and best\nperformances of convolution, CoordConv, and GeoConv on the\nmass centre experiment.\nConv\nCoordConv\nGeoConv\nAvg. loss\n274.1\n218.0\n117.1\nNorm. avg. loss\n0.416\n0.337\n0.246\n# of best perf.\n8\n15\n26\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n1x1\n1x2\n2x1\n2x2\nConv\nCoordConv\nGeoConv\nGPT-4V\nFigure 4.\nAblation study on the performance of models using\neach architecture with different number of layers and filters. A\nside observation is GPT-4V’s [39, 40] intriguing failure in this\ntask. We evaluated GPT-4V’s performance on 140 (20 per den-\nsity) image from the dataset, without fine-tuning, but with prompt-\nengineering, and scaled it by the same scaling factors as other bars.\nAs shown in Tab. 2, CoordConv has the worst performance\namong all three architectures, and GeoConv and convolu-\ntion are on par with each other. Details of models and train-\ning details are included in Appendix A.2\n3.3. Generative adversarial networks\nIn this section, we use GeoConv for generating human faces\nand hand images using GANs [12]. GANs are widely used\nin an array of tasks besides the applications considered here,\nsuch as super-resolution [30], photo blending [57], etc. and\nour contribution can open new doors in those applications\nas well. For all experiments in this section, we have used the\nsame design for the models, as described in details in Ap-\npendix A.3. For simplicity, we prepend “Conv”, “Coord”,\nand “Geo” prefixes for the name of the models. For exam-\nple, a GAN which uses GeoConv is referred to as GeoGAN.\nWe have organised this section as follows.\nStandard GAN for face generation.\nIn Sec. 3.3.1, we\nevaluate the performances of the convolution, GeoConv,\nand CoordConv in standard GANs [12, 42] for generating\nhuman faces, by training on the CelebA-HQ dataset [22]\nfor 450 epochs. CoordGAN collapses in the first 30 epochs\nand does not yield meaningful images. ConvGAN collapses\nwithin 250-300 epochs, while GeoConv did not collapse\nwithin 450 epochs. We have provided qualitative and quan-\ntitative summaries of performances in Fig. 5 and Tab. 3.\nTable 2. The average loss and accuracy of the models when the\nnumbers are moved to all of the possible positions in a 64 × 64\ncanvas\nConv\nCoordConv\nGeoConv\nAvg. loss\n1.84\n2.31\n1.59\nAvg. acc. (%)\n34.9\n34.1\n34.8\n# of best perf.\n2\n0\n3\n(a) ConvGAN\n(b) GeoGAN\n(c) ConvWGAN-GP\n(d) GeoWGAN-GP\nFigure 5. Human faces generated by ConvGAN (Fig. 5a), GeoGAN (Fig. 5b), ConvWGAN-GP (Fig. 5c), and GeoWGAN-GP (Fig. 5d),\ntrained on CelebA-HQ. Each image is generated as follows. For each of the models, we generated 10 images from randomly sampled latent\npoints. The image with the highest score from the discriminator is added to the canvas. This is repeated 16 times for a 4×4 canvas.\nWGAN-GP for face generation.\nTo prevent mode col-\nlapse in CoordGAN and ConvGAN, we used WGAN-GP\n[13] with the same design. This prevented mode collapse in\nConvGAN; nevertheless, CoordGAN collapsed within the\nfirst 20 epochs.\nWe also reduced the number of epochs\nto 150 since training with gradient, requires computing\nsecond-order derivatives and is computationally expensive;\nmoreover, we observed that the generated images do not\nimprove after 100 epochs. We have provided a qualitative\nsummary of performances in Fig. 5.\nWGAN-GP for hand generation.\nWe trained conditional\nWGAN-GP, with the same design, on the ASL Hand Ges-\nture dataset [4] for generating human hand gestures show-\ning numbers “0” to “9” and letters “a” to “z” in the Amer-\nican sign language for 1,000 epochs. The dataset consists\nof 2,524 images, with around 70 images per each of the 36\nlabels. As expected, CoordWGAN-GP collapses on such a\nsmall dataset. Even though ConvGAN succeeds in generat-\ning meaningful hand gestures, it, sometimes, falls short of\nreproducing the correct gesture and suffers in terms of im-\nage quality, while GeoConv manages to generate the best\nimages with correct gesture, as evident in Fig. 6.\n3.3.1\nGAN for face generation.\nFigures 5a and 5b show the images generated by the Con-\nvGAN and GeoGAN. We have included the training and\nmodels’ details, sampling process, as well as more images\ngenerated by each model in Appendix A.3. From Figures\n5a and 5b, we observe that GeoGAN produces images\n1. that are better in terms of the overall face layout,\n2. have more detail, e.g., teeth, makeup, skin tone, etc., and\n3. are more diverse, including 68% female and 31% male\nimages closely replicating the training set’s distribution\nwith 63% female and 37% male images.\nAdditionally, we compared the generator and discrimina-\ntor of each of the models against one another and the dataset\nin Tab. 3. GeoGAN’s generator deceives ConvGAN’s dis-\ncriminator more by a factor of 10, and GeoGAN’s discrim-\ninator is 50% less likely to misclassify real images.\n3.3.2\nWGAN-GP for face generation.\nWasserstein GANs [1] with Gradient Penalty (WGAN-GP)\n[13] emerged as a solution to the mode collapse prob-\nlem in standard GANs. In hopes of addressing mode col-\nlapse in ConvGAN and CoordGAN, we trained the mod-\nels with the same designs as those in Sec. 3.3.1 on the\nCelebA-HQ dataset. We have included the training detail\nin Appendix A.3. CoordWGAN-GP again failed to produce\nmeaningful results due to early mode collapse. However,\nConvWGAN-GP succeeded in generating more diverse im-\nages, despite falling short in comparison to GeoWGAN-GP\nas evident in Fig. 5c and Fig. 5d. The qualities of images\ngenerated by both models slightly decreased compared to\nthe standard GANs. Nonetheless, GeoWGAN-GP still pro-\nduced better images compared to ConvWGAN-GP in terms\nof Items 1-3 above.\nTable 3. Duels between ConvGAN and GeoGAN discriminators\nand generators on 10,000 images generated by each of the genera-\ntors and real images from CelebA-HQ dataset. Numbers show the\npercentage of images misclassified by each of the discriminators\nagainst its generator (Self) and opponent’s generator (Opp). Co-\nordconv is not included due to early mode collapse.\nMisclassification Rate (%)\nArchitecture\nSelf\nOpp.\nReal\nConvGAN\n75.02\n7.88\n0.50\nGeoGAN\n42.94\n0.84\n0.26\nFigure 6. Hand gestures generated by ConvWGAN-GP (top), and GeoWGAN-GP (bottom), trained on the ASL Hand dataset. Each image\nis generated as follows. For a given model and label, we generated 10 images from randomly sampled latent points. The image with highest\nscore from the discriminator is added to the canvas. We repeat this for each of the 36 labels. Hand gestures generated by GeoWGAN-GP,\nin addition to being clearer, have the correct formation and correspond to the correct label, while some of the gestures by ConvWGAN-GP,\nlike ‘4’, ‘6, ‘h’, ‘r’, and ‘s’, show incorrect gestures and some other, like ‘3’, ‘7’, ‘c’, ‘f’, ‘i’, and ‘o’, are deformed.\n3.3.3\nWGAN-GP for hand generation.\nFig. 6 shows hand gestures generated by ConvWGAN-GP\nand GeoWGAN-GP. Training details and more images are\nincluded in Appendix A.3. ConvWGAN-GP fails to learn\nthe correct representations for some of the gestures that\nrequire intricate geometric understanding, such as in ‘r’,\nwhere the middle and index fingers are crossed.\nIt also\ngenerates mutated and contorted fingers for some other ges-\ntures, such as when a finger is hidden behind another as\nin ‘o’ or ‘c’. This shows standard convolution’s inherent\ninability to learn complex details. GeoWGAN-GP, on the\nother hand, learns more accurate representations for differ-\nent hand gestures and generates images of higher quality\nclear of mutations and contortions.\n3.3.4\nFID and IS metrics\nConventional GAN assessment metrics like IS and FID are\noften used for evaluating GANs; however, they often fail to\nreflect the true performance of a GAN [5, 6, 19, 23]. This\nTable 4. The FID and Inception Score of the GAN and WGAN-\nGP models, trained on CelebA-HQdataset, calculated over 25,000\nimages. Coordconv is not included due to mode collapse.\nArchitecture\nIS\nFID\nConvGAN\n1.122\n5.028\nGeoGAN\n1.113\n7.151\nConvWGAN-GP\n1.317\n9.930\nGeoWGAN-GP\n1.564\n18.870\nis because the Inception model’s training dataset as well as\nits architectural flaws affect these scores. In particular, this\ncould be due to the use of vanilla convolution, which has\nlimited receptive field and cannot learn positional informa-\ntion [32]. Nonetheless, we evaluated ConvGANs and Ge-\noGANs using these metrics, as summarised in Tab. 4. Com-\nputing IS and FID reliably require at least 5,000 images;\nhence, we only evaluate on CelebA-HQ dataset. Contrary\nto the quality and diversity of generated images, ConvGANs\nscores favourably compared to GeoGANs.\n3.4. Variational autoencoders\nDue to challenges in the quantitative comparison of GANs,\nwe also evaluate GeoConv for use in VAEs [26], especially\nsince the effectiveness of convolutions in VAE applications\nrelies on learning both local and global features from im-\nages [27, 32]. VAEs are used in a range of applications\n[20, 28, 50]; however, in this section, we only focus on gen-\nerating human faces by training on the CelebA dataset [33].\nWe have included a similar experiment for generating hand\ngestures for ASL numbers and letters, similar to Sec. 3.3.3,\nas well as model and training details in Appendix A.4.1.\nFor each latent dimension d = 256, 384, 512, we trained\nGeoVAE, CoordVAE, and ConvVAE five times to obtain the\nmeans and 95% Confidence Intervals (CI) in Fig. 7. Across\ndifferent latent sizes, GeoVAE obtains 10-25% smaller loss\nand validation loss. Another notable observation is that,\nunlike ConvVAE and CoordVAE, GeoVAE’s loss does not\nfluctuate, and the 95% CI is quite small, especially com-\npared to ConvVAE. We predict that this may be due to the\nsmoothing effect of the random shift in GeoConv.\nImages generated by each of the VAEs for different la-\n0\n5\n10\n15\n20\n25\n30\nEpochs\n8000\n9000\n10000\n11000\n12000\n13000\n14000\nLoss\nConv loss \nConv val. loss \nGeoConv loss \nGeoConv val. loss \nCoordConv loss \nCoordConv val. loss\nd = 256\n0\n5\n10\n15\n20\n25\n30\n8000\n9000\n10000\n11000\n12000\n13000\n14000\nd = 384\nEpochs\n0\n5\n10\n15\n20\n25\n30\n8000\n9000\n10000\n11000\n12000\n13000\n14000\nd = 512\nEpochs\nFigure 7. Mean and 95% CI of train and validation losses of GeoVAE (red lines), CoordVAE (dotted brown lines), and ConvVAE (dashed\nblue lines), trained on CelebA dataset for latent dimensions d ∈ {256, 384, 512} over five runs with seeds 0, . . . , 4. GeoVAE is more\nconsistent across all runs and latent dimensions and obtains smaller mean loss and validation loss than both ConvVAE and CoordVAE.\nbels and latent values after 30 epochs are shown in Fig-\nures 8 and 9, respectively.\nGeoVAE demonstrates a no-\ntable capacity to produce diverse images given different la-\ntent points.\nIn stark contrast, ConvVAE and CoordVAE\nfail to capture the dataset’s diversity, consistently generat-\ning similar outputs for all latent points. GeoVAE also ex-\nhibits adaptability in attributes like hairstyle, eye and eye-\nbrow styles, and even skin tones. Conversely, other models\nexhibit limited flexibility, yielding less diverse images. Fur-\nthermore, GeoVAE consistently produces higher-resolution\nimages for all labels and latent points imbued with more\npronounced and distinctive features compared to images\ngenerated by ConvVAE and CoordVAE.\n4. Discussion\nIn Sec. 3.3, we observed that GeoGANs generate more di-\nverse images that match training data’s distribution com-\npared to ConvGANs.\nIn the same way, we observe in\nSec. 3.4, that GeoVAEs show more variation in generating\nhuman faces for different labels and latent points compared\nto CoordVAEs and ConvVAEs. Even though the better per-\nformance of GeoConv models is expected, it remains un-\nConv\nCoordConv\nGeoConv\nFigure 8. Images generated by each of the VAEs for different\nlabels. Images generated by GeoVAE (bottom) are clearer, have\nsharper edges, and contain more details than those generated by\nConvVAE (top) and CoordVAE (middle).\nclear and requires further investigation why and how Geo-\nConv models create more diverse images.\nAnother significant observation from Fig. 7, is the re-\nmarkable consistency of GeoVAEs’ loss curves across dif-\nferent runs and latent dimensions. Intuitively, we expected\nGeoVAEs to outperform their counterparts, but how this led\nto 5 and 11 times smaller 95% CI, in comparison to Coord-\nVAEs and ConvVAEs, requires additional exploration.\n5. Conclusions and future directions\nIn this paper, we demonstrated GeoConv’s capabilities in\nconsistently producing better images with more details and\ndiversity compared to existing convolutional architectures.\nWe showed this for GANs and VAEs in generating hand\ngestures and human faces. Given that diffusion models suf-\nfer from some of the same problems, in particular in gener-\nating human hands, GeoConv provides a promising research\navenue to pursue in the future.\nGiven the promising performance of GeoConv in the\nmodels considered here, we foresee it can improve large-\nscale SoA models, which we could not investigate due to\ncomputational constraints. We plan to investigate this fur-\nConv\nCoordConv\nGeoConv\nFigure 9. Images generated by each of the VAEs for different\nrandom latent points. Images generated by GeoVAE (bottom) are\nmore diverse and vary with the latent, while images generated by\nConvVAE (top) and CoordVAE (middle) remain untouched.\nther in our future work. Other avenues of research that we\nforesee GeoConv will contribute to include geometric tasks,\nsuch as depth estimation, object segmentation, 3D recon-\nstruction, video generation, and several other applications.\nAcknowledgements\nThis work is partially supported by the UK EPSRC via\nthe Centre for Doctoral Training in Intelligent Games and\nGame Intelligence (IGGI; EP/S022325/1) and REXASI-\nPRO Horizon Euorope project (10.3030/10107002).\nWe\nalso thank Zahraa Al Sahili for providing feedback on pre-\nvious versions of this work.\nReferences\n[1] Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou.\nWasserstein generative adversarial networks.\nIn 34th In-\nternational Conference on Machine Learning, ICML, pages\n214–223. PMLR, 2017. 2, 6\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In 3rd International Conference on Learning Rep-\nresentations, ICLR. OpenReview.net, 2015. 3\n[3] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are\ntransformers more robust than cnns?\nIn Advances in Neu-\nral Information Processing Systems, NeurIPS, pages 26831–\n26843. Curran Associates, Inc., 2021. 3\n[4] Andre L. C. Barczak, Napoleon H. Reyes, Maria Abastillas,\nAna Piccio, and Teo Susnjak. A new 2d static hand gesture\ncolour image dataset for asl gestures. Research Letters in the\nInformation and Mathematical Sciences, 15:12–20, 2011. 2,\n4, 6, 12\n[5] Shane Barratt and Rishi Sharma. A note on the inception\nscore, 2018. arXiv preprint arXiv:1801.01973. 7\n[6] Ali Borji. Pros and cons of gan evaluation measures: New\ndevelopments. Computer Vision and Image Understanding,\n215:103329–103344, 2022. 7\n[7] Sungha Choi, Joanne T Kim, and Jaegul Choo. Cars can’t\nfly up in the sky: Improving urban-scene segmentation via\nheight-driven attention networks. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR, pages\n9370–9380. Computer Vision Foundation / IEEE, 2020. 3\n[8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic,\nShane Legg, and Dario Amodei. Deep reinforcement learn-\ning from human preferences. In Advances in Neural Informa-\ntion Processing Systems, NeurIPS, pages 4299–4307. Curran\nAssociates, Inc., 2017. 1\n[9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In IEEE/CVF International Conference on Com-\nputer Vision, ICCV, pages 764–773. IEEE, 2017. 3\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is\nworth 16x16 words: Transformers for image recognition at\nscale. In 9th International Conference on Learning Repre-\nsentations, ICLR. OpenReview.net, 2021. 1, 3\n[11] Kunihiko Fukushima and Sei Miyake. Neocognitron: A new\nalgorithm for pattern recognition tolerant of deformations\nand shifts in position. Pattern recognition, 15(6):455–469,\n1982. 1\n[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing\nXu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and\nYoshua Bengio. Generative adversarial nets. In Advances\nin neural information processing systems NeurIPS, pages\n2672–2680. Curran Associates, Inc., 2014. 1, 2, 4, 5\n[13] Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent\nDumoulin, and Aaron C. Courville. Improved training of\nwasserstein gans. In Advances in Neural Information Pro-\ncessing Systems, NeurIPS, 2017. 2, 4, 6\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\nCVPR, pages 770–778. Computer Vision Foundation / IEEE,\n2016. 2\n[15] Willy Fitra Hendria,\nQuang Thinh Phan,\nFikriansyah\nAdzaka, and Cheol Jeong. Combining transformer and cnn\nfor object detection in uav imagery. ICT Express, 9(2):258–\n263, 2023. 3\n[16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. In Advances in Neural Information Processing Sys-\ntems, NeurIPS, pages 6626–6637. Curran Associates, Inc.,\n2017. 4\n[17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. In Advances in Neural Informa-\ntion Processing Systems, NeurIPS, pages 6840–6851. Curran\nAssociates, Inc., 2020. 1, 2\n[18] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate atten-\ntion for efficient mobile network design. In IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, CVPR,\npages 13713–13722. Computer Vision Foundation / IEEE,\n2021. 3\n[19] Min Jin Chong and David A. Forsyth.\nEffectively un-\nbiased FID and inception score and where to find them.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR, pages 6069–6078. Computer Vision\nFoundation / IEEE, 2020. 7\n[20] Hiroshi Kajino. Molecular hypergraph grammar with its ap-\nplication to molecular optimization. In 36th International\nConference on Machine Learning ICML, pages 3183–3191.\nPMLR, 2019. 7\n[21] Leonid V. Kantorovich. Mathematical methods of organizing\nand planning production. Management Science, 6(4):366–\n422, 1960. 2\n[22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen.\nProgressive growing of gans for improved quality, stability,\nand variation. In 6th International Conference on Learning\nRepresentations, ICLR. OpenReview.net, 2018. 2, 4, 5, 12\n[23] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks.\nIn IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, CVPR, pages 4401–4410. Computer Vision\nFoundation / IEEE, 2019. 1, 7\n[24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten,\nJaakko Lehtinen, and Timo Aila.\nAnalyzing and improv-\ning the image quality of stylegan. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR, pages\n8107–8116. Computer Vision Foundation / IEEE, 2020.\n[25] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen,\nJanne Hellsten, Jaakko Lehtinen, and Timo Aila.\nAlias-\nfree generative adversarial networks. In Advances in Neural\nInformation Processing Systems, NeurIPS, pages 852–863.\nCurran Associates, Inc., 2021. 1\n[26] Diederik P Kingma and Max Welling. Auto-encoding vari-\national bayes. In 2nd International Conference on Learning\nRepresentations, ICLR. OpenReview.net, 2014. 2, 7\n[27] Adam Kosiorek, Sara Sabour, Yee Whye Teh, and Geof-\nfrey E. Hinton. Stacked capsule autoencoders. In Advances\nin neural information processing systems, NeurIPS, pages\n15486–15496. Curran Associates, Inc., 2019. 3, 7\n[28] Volodymyr Kovenko and Ilona Bogach.\nA comprehen-\nsive study of autoencoders’ applications related to images.\nIn Proceedings of the 7th International Conference ”Infor-\nmation Technology and Interactions”, IT&I, pages 43–54.\nCEUR-WS.org, 2020. 7\n[29] Yann LeCun, Bernhard Boser, John Denker, Donnie Hen-\nderson, R. Howard, Wayne Hubbard, and Lawrence Jackel.\nHandwritten digit recognition with a back-propagation net-\nwork. In Advances in Neural Information Processing Sys-\ntems, NeurIPS, pages 396–404. Morgan-Kaufmann, 1989. 1\n[30] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero,\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\nAlykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe\nShi.\nPhoto-realistic single image super-resolution using a\ngenerative adversarial network.\nIn IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR, pages\n4681–4690. Computer Vision Foundation / IEEE, 2017. 5\n[31] Younggun Lee and Taesu Kim.\nRobust and fine-grained\nprosody control of end-to-end speech synthesis.\nIn IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing, ICASSP, pages 5911–5915, 2019. 3\n[32] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Pet-\nroski Such, Eric Frank, Alex Sergeev, and Jason Yosinski.\nAn intriguing failing of convolutional neural networks and\nthe coordconv solution. In Advances in Neural Information\nProcessing Systems, NeurIPS, pages 9628–9639. Curran As-\nsociates, Inc., 2018. 2, 3, 7\n[33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.\nDeep learning face attributes in the wild. In IEEE/CVF In-\nternational Conference on Computer Vision, ICCV, pages\n3730–3738. IEEE, 2015. 2, 7\n[34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht-\nenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. In IEEE/CVF Conference on Computer Vision and\nPattern Recognition, CVPR, pages 11966–11976. Computer\nVision Foundation / IEEE, 2022. 3\n[35] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang,\nQingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin\nHan, Errui Ding, and Shilei Wen. PP-YOLO: an effective\nand efficient implementation of object detector, 2020. arXiv\npreprint arXiv:2007.12099. 3\n[36] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel.\nUnderstanding the effective receptive field in deep convolu-\ntional neural networks. In Advances in Neural Information\nProcessing Systems NeurIPS, pages 4898–4906. Curran As-\nsociates, Inc., 2016. 3\n[37] Midjourney. Midjourney.com, 2022. midjourney.com.\n1\n[38] Dongbin Na. CelebA-HQ face identity and attributes recog-\nnition using PyTorch, 2021. Accessed: 2023-09-22. 12\n[39] OpenAI.\nGPT-4 technical report, 2023.\narXiv preprint\narXiv:2303.08774. 5\n[40] OpenAI. ChatGPT can now see, hear, and speak, 2023. Blog\nPost. 5\n[41] Francesco Pinto, Philip H. S. Torr, and Puneet K. Dokania.\nAn impartial take to the CNN vs transformer robustness con-\ntest. In European Conference on Computer Vision, ECCV,\npages 466–480. Springer, 2022. 3\n[42] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper-\nvised representation learning with deep convolutional gener-\native adversarial networks. In 4th International Conference\non Learning Representations, ICLR. OpenReview.net, 2016.\n2, 5\n[43] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jon Shlens. Stand-alone self-\nattention in vision models. In Advances in neural informa-\ntion processing systems, NeurIPS, 2019. 1, 3\n[44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation.\nIn 38th International\nConference on Machine Learning, ICML, pages 8821–8831.\nPMLR, 2021. 1, 2\n[45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey\nChu, and Mark Chen.\nHierarchical text-conditional im-\nage generation with CLIP latents, 2022.\narXiv preprint\narXiv:2204.06125. 2\n[46] Aditya Ramesh, James Betker, Gabriel Goh, Li Jing,\nTim Brooks Jianfeng Wang, Linjie Li, Long Ouyang,\nJuntang Zhuang, Joyce Lee, Yufei Guo, Wesam Man-\nassra, Prafulla Dhariwal, Casey Chu, and Yunxin Jiao.\nImproving image generation with better captions, 2023.\ncdn.openai.com/papers/dall-e-3.pdf. 1\n[47] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In IEEE/CVF Conference\non Computer Vision and Pattern Recognition, CVPR, pages\n10684–10695. Computer Vision Foundation / IEEE, 2022. 1\n[48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, Xi Chen, and Xi Chen. Improved\ntechniques for training gans. In Advances in Neural Informa-\ntion Processing Systems, NeurIPS, pages 2226–2234. Curran\nAssociates, Inc., 2016. 4\n[49] Karen Simonyan and Andrew Zisserman. Very deep con-\nvolutional networks for large-scale image recognition.\nIn\n3rd International Conference on Learning Representations,\nICLR. OpenReview.net, 2015. 2\n[50] Aman Singh and Tokunbo Ogunfunmi. An overview of vari-\national autoencoders for source separation, finance, and bio-\nsignal applications. Entropy, 24(1):55, 2022. 7\n[51] Samuel L. Smith, Andrew Brock, Leonard Berrada, and So-\nham De. Convnets match vision transformers at scale, 2023.\narXiv preprint arXiv:2303.08774. 3\n[52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-\ning diffusion implicit models. In 9th International Confer-\nence on Learning Representations, ICLR. OpenReview.net,\n2021. 1, 2\n[53] Leonid N. Vaserstein. Markov processes over denumerable\nproducts of spaces, describing large systems of automata.\nProblems of Information Transmission, 5(3):64–72, 1969. 2\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, NeurIPS, pages 5998–6008.\nCurran Associates, Inc., 2017. 3\n[55] Xinyao Wang, Liefeng Bo, and Fuxin Li. Adaptive wing\nloss for robust face alignment via heatmap regression. In\nIEEE/CVF International Conference on Computer Vision,\nICCV, pages 6970–6980. IEEE, 2019. 3\n[56] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Mul-\ntiscale structural similarity for image quality assessment. In\nThe Thrity-Seventh Asilomar Conference on Signals, Systems\n& Computers, 2003, pages 1398–1402. IEEE, 2003. 14\n[57] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang.\nGP-GAN: towards realistic high-resolution image blending.\nIn 27th ACM International Conference on Multimedia, MM,\npages 2487–2495. ACM, 2019. 5\n[58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu,\nXiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con-\nvolutions to vision transformers. In IEEE/CVF International\nConference on Computer Vision, ICCV, pages 22–31. IEEE,\n2021. 1\n[59] Chao Xie, Hongyu Zhu, and Yeqi Fei. Deep coordinate at-\ntention network for single image super-resolution. IET Image\nProcessing, 16(1):273–284, 2022. 3\nA. Experimental setup\nIn this appendix, we explain the experimental setup in this\npaper and provide more images, figures, and tables.\nA.1. Centre of mass\nOur motivation for choosing this task and configuration is\nthat it requires the models to have a good understanding of\nthe locations of a varying number of points spread out in\na 2-dimensional plane with a few convolutional layers and\nfilters. Therefore, the models need to obtain a geometric\nand global knowledge of where the points are, rather than a\nlocal knowledge provided by standard convolutions.\nDataset details\nTo cover different scenarios and have a\ncomprehensive comparison between the architectures, we\ntrained the networks on 7 synthesised datasets, each con-\ntaining 100, 000 images, of size 32 × 32 with point density\nd, where d ∈ D = {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.9}.\nThe set D is roughly defined by the geometric progression\n0.001 × 3k for 0 ≤ k ≤ 6, and covers a varying range\nof points starting from 0.001 and increasing geometrically\nwith a factor of roughly 3, up to 0.9 density.\nWe then evaluated the performances of each of the net-\nworks on 7 test datasets, each containing 20, 000 images\nwith density d ∈ D. All of the networks were trained using\nthe Euclidean distance, between the predicted mass centre\nand the true mass centre, as the loss function. We have in-\ncluded the detailed results for the base models, i.e., models\nwith 1 convolution layer and 1 filter (this is shown as 1x1 in\nFig. 4) in Tab. 5. Detailed results for other models can be\neasily obtained by running the provided code.\nModel design\nAll networks use convolution layers with a\nkernel size of 3 and a stride of 2 with ReLU activation, com-\nbined with a dense output layer with 2 nodes, corresponding\nto the x and y coordinates of the mass centre. As an ablation\nstudy, we consider 4 networks ixj, where 0 ≤ i, j ≤ 2.\nA.2. Positional dependencies\nDetaset details\nThis dataset is designed to evaluate posi-\ntional bias, described in Sec. 3.2, in vision models. This\ndataset includes 64 × 64 images containing Greek numbers\nI, II, and III, corresponding to labels 1, 2, and 3. In the train-\ning set, the Greek numbers are almost centred in the image,\nwith little horizontal and vertical shifts, while in the test sets\nthe Greek numbers move farther from the centre the images.\nModel Design\nWe consider convolutional models with\nvarying number of layers ranging over 1, . . . , 5. The n-th\nlayer in each model has 2n−1 filters. All convolutions lay-\ners have kernel size of 3 and use a stride of 2, with ReLU\nactivation. The only other layer, is the output layer, which\nis a dense layer of size 3. The models are trained on the\ntraining set using the categorical cross entropy loss, which\nis the standard choice for multi-class classification tasks.\nAs you can see in Tab. 2, despite having the highest num-\nber of learnable parameters, CoordConv has the worst per-\nformance amongst all the architectures due to the positional\nbias learnt during the training. The complete results that\nTab. 2 is derived from is available in Tab. 6.\nA.3. GAN\nIn this section of the appendix, we discuss the details of the\nexperiments in Sec. 3.3.\nA.3.1\nDataset details\nCelebA-HQ dataset\nCelebA-HQ [22], introduced in\n2018, is a dataset consisting of 30,000 human face im-\nages with 1024×1024 resolution. Since its introduction,\nit has been widely used in various applications for generat-\ning realistic human faces. Unlike CelebA dataset, CelebA-\nHQ does not include annotations on facial features. This\ndataset includes 18,943 (63.15%) female images and 11,057\n(36.85%) male images [38]. We use this dataset in our GAN\nexperiments to gain insights on the capabilities and limita-\ntions of models using different convolution architectures.\nASL Hand Gesture dataset\nASL Hand Gesture [4] is a\nsmall dataset consisting of 2,524 annotated hand gesture im-\nages representing numbers ‘0’ to ‘9’ and English alphabets\n‘a’ to ‘z’ in the American sign language. The dataset im-\nages are almost equally distributed between all the 36 la-\nbels; there are approximately 70 images per each labels.\nAll images are on a black background and of different sizes,\nwhich are resized to 256×256 resolution at preprocessing.\nA.3.2\nGANs for for generating face images\nModel design\nThe generator and discriminator are de-\nsigned according to common practices in training GANs.\nThe discriminator’s architecture is similar to VGG-13.\nHere, we discuss the generator architecture. In the gener-\nator, after one dense layer, and a reshape layer that takes the\n1-dimensional latent to a 3-dimensional tensor, we have 5\nblocks of layers, each consisting of the following 3 layers:\n• A transposed convolution/GeoConv/CoordConv layer\nwith a stride of 2 and kernel size of 3 with no padding\nand leaky ReLU activation.\n• A convolution/GeoConv/CoordConv layer with a stride of\n1 and kernel size of 3 with leaky ReLU activation.\n• A batch normalization layer.\nIn the end, the output layer of the generator is a convolu-\ntion/GeoConv/CoordConv layer with the same specification\nas before except for the activation which is sigmoid.\nTable 5. The detailed loss table for 1x1 models in Fig. 4.\nTest ratio\nTrain ratio\nArchitecture\n0.001\n0.003\n0.01\n0.03\n0.1\n0.3\n0.9\nGeoConv\n2.581\n3.598\n18.87\n79.65\n296.7\n916\n2777\n0.001\nCoordConv\n2.267\n4.630\n27.02\n107.5\n392.9\n1208\n3654\nConv\n2.438\n4.622\n24.88\n100.7\n370.3\n1140\n3449\nGeoConv\n5.435\n2.640\n2.871\n6.01\n20.12\n66.90\n211.4\n0.003\nCoordConv\n4.530\n2.112\n3.553\n18.33\n76.43\n242.8\n742.3\nConv\n4.356\n2.104\n4.025\n21.25\n87.28\n276.4\n844.0\nGeoConv\n6.381\n3.180\n1.291\n4.558\n24.15\n80.95\n251.6\n0.01\nCoordConv\n9.380\n4.875\n1.971\n2.978\n8.45\n14.17\n15.1\nConv\n6.329\n3.145\n1.261\n4.608\n24.48\n82.07\n255.1\nGeoConv\n9.36\n5.008\n2.142\n1.095\n1.495\n4.72\n13.14\n0.03\nCoordConv\n11.15\n6.370\n2.803\n1.145\n4.580\n11.74\n14.90\nConv\n6.84\n3.668\n2.133\n0.890\n7.321\n29.40\n95.90\nGeoConv\n7.371\n3.948\n2.405\n1.925\n0.610\n5.696\n23.21\n0.1\nCoordConv\n7.548\n4.042\n2.377\n1.837\n0.601\n5.216\n21.29\nConv\n8.369\n4.426\n2.164\n1.398\n0.667\n2.916\n12.06\nGeoConv\n6.942\n3.859\n2.875\n2.988\n2.440\n0.350\n7.430\n0.3\nCoordConv\n7.874\n4.163\n2.279\n1.895\n1.506\n0.342\n4.474\nConv\n9.035\n4.841\n2.231\n1.300\n0.789\n0.348\n1.467\nGeoConv\n9.228\n5.114\n2.61\n1.75\n1.26\n0.888\n0.349\n0.9\nCoordConv\n5.176\n5.655\n7.66\n8.44\n8.07\n6.095\n0.147\nConv\n5.221\n7.904\n10.67\n11.39\n10.77\n8.085\n0.156\nTable 6. The average loss and accuracy of the models when the numbers are moved to all of the possible positions in a 64 × 64 canvas\nMetric\nArchitecture\n1 Layer\n2 Layers\n3 Layers\n4 Layers\n5 Layers\nConv2D\n1.16\n1.26\n1.60\n2.20\n2.95\nLoss\nCoordConv\n1.78\n2.35\n2.45\n2.06\n2.91\nGeoConv\n1.23\n1.63\n1.50\n1.59\n2.04\nConv2D\n36.82\n35.25\n34.06\n34.00\n34.40\nAcc. (%)\nCoordConv\n34.08\n34.03\n34.35\n34.17\n34.10\nGeoConv\n34.93\n34.29\n36.20\n34.35\n33.76\nTraining detail\nFor training the models in this experi-\nment, we use the binary cross-entropy loss which is the\ncommon method for training GANs. We trained each model\nfor 500 epochs. After reaching 400 epochs, none of the\nmodels showed any improvements.\nA closer look at generated images\nFig. 10 portrays a 6×6\ncanvas with more images of ConvGAN and GeoGAN. No-\ntice the quality, colour, and diversity of the images by each\nof the models.\nA.3.3\nWGAN-GPs for generating face images\nModel design\nThe design of the generator and discrimi-\nnator in this section is similar to the generator and discrim-\ninator explained in Appendix A.3.2.\nTraining detail\nWGAN-GPs use Wasserstein distance for\ntheir loss alongside gradient penalty.\nSince none of the\nmodels showed any improvements after around 100 epochs,\nwe set the number of epochs to 150.\nA closer look at generated images\nFig. 11 portrays a\n6×6 canvas with more images generated by the WGAN-\n(a) ConvGAN\n(b) GeoGAN\nFigure 10. Human faces generated by ConvGAN (Fig. 10a) and GeoGAN (Fig. 10b) trained on CelebA-HQ dataset. Each image is\ngenerated as follows. For each of the models, we generated 10 images from randomly sampled latent points. The image with the highest\nscore from the discriminator is added to the canvas. This is repeated 36 times for a 6×6 canvas.\nGPs. Notice the quality, colour, and diversity of the images\ngenerated by each of the models.\nA.3.4\nWGAN-GPs for generating hand gestures\nModel design\nThe design of the generator and discrimi-\nnator in this section is similar to the generator and discrim-\ninator explained in Appendix A.3.2.\nTraining detail\nTraining details are similar to Ap-\npendix A.3.3, except that we run the experiments for 1,000\nepochs to make sure all the models reach peak performance.\nA closer look at the generated images\nFig. 12 shows\nthe hand gestures generated by both ConvWGAN-GP and\nGeoWGAN-GP for each label of the ASL language. These\nare the same images as in Fig. 6; however, they have been\nscaled up for visualising more details and easier comparison\nbetween the images generated by each of the models.\nA.4. VAE\nIn this section of the appendix, we discuss the details of the\nexperiments in Sec. 3.4.\nA.4.1\nLoss function\nS In VAEs, since the quality of generated images is closely\nassociate to the loss function, we chose a loss function that\nhelps training a model that not only generates images from\nthe same distribution as the train images, but also helps gen-\nerating images that are sharper and have similar structural\nsimilarity. Therefore, we chose the loss the function to be a\ncombination of\n• Binary Cross Entropy (BCE). BCE loss is used as a\npixel-wise reconstruction loss in VAEs. It encourages the\nVAE to produce reconstructions that are statistically sim-\nilar to the input data in a pixel-wise manner.\n• Mean Squared Error (MSE). MSE penalises large\npixel-wise differences more heavily and is more sensitive\nto outliers than BCE.\n• Mean Absolute Error (MAE). MAE is less sensitive to\noutliers than MSE. Like MSE, it helps reduce pixel-wise\ndifferences between input and reconstruction, though the\nmagnitude of errors is emphasised differently.\n• Multi-scale Structural Similarity (SSIM): SSIM [56]\nassesses structural similarity between images, consider-\ning luminance, contrast, and structure. It helps capture\nhigh-level features and generate images that are struc-\nturally more similar to the training images.\n• Absolute difference of Sobel edge maps: Sobel edge\nmaps highlight edges and gradients in images. Penalising\n(a) ConvWGAN-GP\n(b) GeoWGAN-GP\nFigure 11. Human faces generated by ConvWGAN-GP (Fig. 11a) and GeoWGAN-GP (Fig. 11b) trained on CelebA-HQ dataset. Each\nimage is generated as follows. For each of the models, we generated 10 images from randomly sampled latent points. The image with the\nhighest score from the discriminator is added to the canvas. This is repeated 36 times for a 6×6 canvas.\nthe absolute difference between these maps encourages\nthe VAE to reproduce edges accurately. It helps improve\nthe sharpness and structural details in generated images.\nA.4.2\nDataset details\nCelebA dataset\nCelebA dataset is one of the most com-\nmonly used datasets in both generative and discriminative\napplications in computer vision. This dataset includes 200k\nhuman face images.\nEach image comes with 40 binary\nattribute annotations about different features such as eye-\nbrows, cheeks, nose, hair, eyeglasses, neckties, etc.\nASL Hand Gesture dataset\nPlease see Appendix A.3.1.\nA.4.3\nConditional VAEs for generating face images\nModel design\nBoth encoder and decoder are designed ac-\ncording to standard practices. The encoder first feeds the in-\nput image through three convolution/GeoConv/CoordConv\nlayers consecutively. All these layers have a kernel size of\n3 and a stride of 2 and use ReLU activation. The result is\nthen flattened and concatenated with the label. Then, we\nuse two dense layers to learn the mean and standard devia-\ntion of the latent space. Then, a latent is sampled using the\nnormal distribution with this mean and standard deviation.\nThis latent and the label are then fed into the de-\ncoder which will generate an image reconstructing the\noriginal image.\nAfter that,\n5 transposed convolu-\ntion/GeoConv/CoordConv layers consecutively expand the\nfeature map. Each of those layers has a kernel size of 3 and\na stride of 2 and uses ReLU activation. Finally, a convo-\nlution/GeoConv/CoordConv layer with 3 channels, kernel\nsize of 3 and a stride of 1 with sigmoid activation, synthe-\nsises the final image.\nTraining detail\nWe explained the loss function we use for\ntraining the VAEs in Appendix A.4.1. During the training,\nthe loss curves start to flatten out after 20 epochs. Nonethe-\nless, we continued training the VAEs until 30 epochs.\nA.4.4\nConditional VAE for generating hand gestures\nThe findings from the experiment on VAEs presented in the\nmain body show the significant enhancements achieved by\nincorporating GeoConv into a VAE. These enhancements\nare observed both in qualitative and quantitative perfor-\nmance, as well as in a heightened capacity to capture the\ndataset’s diversity. In alignment with our experiments in the\nGAN section, in this section, we use VAEs for generating\nimages of ASL hand gestures.\nWhile CelebA is a vast and diverse collection, compris-\ning approximately 200k human face images, the hand ges-\n0\n1\n2\n3\n4\n5\n6\n7\n8\n GeoWGAN-GP    ConvWGAN-GP\n9\na\nb\nc\nd\ne\nf\ng\nh\n GeoWGAN-GP    ConvWGAN-GP\ni\nj\nk\nl \nn\no \nq\nm \np\n GeoWGAN-GP    ConvWGAN-GP\nr\nt\nu  \nw\nx\ny\nz\n GeoWGAN-GP    ConvWGAN-GP\nv\ns\nFigure 12. Hand gestures generated by ConvWGAN-GP (first rows), and GeoWGAN-GP (second rows), trained on the ASL Hand Gesture\ndataset. These are copies of images included in Fig. 6 of the main body, scaled up for better comparison.\nture dataset only contains just over 2,500 images, each shar-\ning a similar appearance, primarily differing based on the\nrepresented alphabet or number. Consequently, this dataset\nintroduces a distinct set of challenges for the VAEs. We\ntrain two conditional VAEs on the the gesture dataset for\n100 epochs. We use the same architecture for the VAEs as\nin Appendix A.4.3, only differing in some hyperparameters.\nWe run experiments using latent dimensions 64, 128, and\n192.\nAdditionally, each VAE is trained five times, with\nseeds 0, 1, . . . , 4. The training and validation loss during\n0\n20\n40\n60\n80\n100\nEpochs\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\nLoss\nConv2D loss \nConv2D val_loss \nGeoConv2D loss \nGeoConv2D val_loss\n(a) d = 64\n0\n20\n40\n60\n80\n100\nEpochs\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n(b) d = 128\n0\n20\n40\n60\n80\n100\nEpochs\n4000\n4500\n5000\n5500\n6000\n6500\n7000\n7500\n8000\n(c) d = 192\nFigure 13. Mean and 95% CI of train and validation losses of GeoVAE (red lines), and ConvVAE (dashed blue lines), trained on Hand\nGesture dataset for latent dimensions d ∈ {64, 128, 192} over five runs with seeds 0, . . . , 4 during 100 training epochs.\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\nConvVAE\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\na\nb\nc\nd\ne\nf\ng\nh\ni\nj\nk\nl\nm\nn\no\np\nq\nr\ns\nt\nu\nv\nw\nx\ny\nz\nGeoVAE\nFigure 14. Hand gestures generated by ConvVAE (top row) and GeoVAE (bottom row) with 192-dimensional latent spaces. Images\ngenerated by GeoVAE have more realistic colours and are slightly sharper.\n100 epochs of training are visualised in Fig. 13. As antic-\nipated, training and validation losses are similar for both\narchitectures across various latent dimensions. As we dis-\ncussed before, this is because of the Hand Gesture dataset’s\nsmall size and limited diversity.\nFig. 14 presents the generated images produced by each\nof the conditional VAEs. Both models perform reasonably\nwell in representing the correct gestures even though they\ndo not produce high-resolution images compared to GANs.\nDigging deeper into the details, images generated by Geo-\nVAE have more realistic colours and sharper details such as\nmore distinct fingers in comparison to the ConvVAE.\n"
}