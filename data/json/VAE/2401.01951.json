{
    "optim": "Can We Generate Realistic Hands Only Using Convolution? Mehran Hosseini* Department of Informatics King’s College London mehran.hosseini@kcl.ac.uk Peyman Hosseini∗ School of Electronic Engineering & Computer Science Queen Mary University of London s.hosseini@qmul.ac.uk Abstract The enduring inability of image generative models to recreate intricate geometric features, such as those present in human hands and fingers has been an ongoing problem in image generation for nearly a decade. While strides have been made by increasing model sizes and diversifying train- ing datasets, this issue remains prevalent across all models, from denoising diffusion models to Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in the underlying architectures. In this paper, we demonstrate how this problem can be mitigated by augmenting convo- lution layers geometric capabilities through providing them with a single input channel incorporating the relative n- dimensional Cartesian coordinate system. We show that this drastically improves quality of hand and face images generated by GANs and Variational AutoEncoders (VAE). 1. Introduction Generative models have gained immense popularity and generated unprecedented hype in the last few years, revo- lutionising the way we approach tasks that involve gener- ating new content. State-of-the-art image generative mod- els, such as OpenAI’s DALL·E 3 [44–46], Stable Diffusion [47], Midjourney [37], and Nvidia’s StyleGAN [23–25] are used to create mesmerising high-resolution images. However, all of these models have a peculiar shortcom- ing when it comes to learning and reproducing certain geo- metric patterns, such as those present in human hands and fingers. For example, Fig. 1b shows the images generated by DALL·E 3, when prompted “a realistic human hand showing number n”, for n = 2, 4. This phenomenon is uni- versally present in all families of generative models, from GANs [12] to denoising diffusion models [17, 52], whether they are based on convolution [11, 29], Vision Transformers (ViT) [10, 43], or a combination of both [58]. Human painters, on the other hand, are able to draw flaw- ∗Equal contribution. (a) Hand drawn (b) DALL·E 3 (c) ConvGAN (d) GeoGAN Figure 1. Human hands showing numbers 2 and 4 as drawn by hand (Fig. 1a) and as generated by DALL·E 3 (Fig. 1b), a stan- dard convolutional GAN (Fig. 1c), and GeoGAN (ours) (Fig. 1d). The comparison is between ConvGAN and GeoGAN only. Images generated by DALL·E 3 are included to illustrate the struggles of SoA models in generating human hands. less pictures of hands. It is, in part, because, unlike the generative models, painters know how hands work, provid- ing them with a knowledge of what hands can and cannot do. Another contributing factor is that human painters learn how to draw hands by breaking down and simplifying them into simple geometric shapes, as shown in Fig. 1a. Generative models’ shortcomings are caused by two con- tributing factors, models’ design and architecture, as well as the training dataset and methodology. Taking into consider- ation that the SoA image generative models are trained on a vast collection of images on the Internet and are further en- hanced by methods, such as Reinforcement Learning with Human Feedback (RLHF) [8], the latter is not the root of the problem. In the last few years, it has become evident that the model size directly corresponds to the quality of generated images, resulting in models that produce hyper- realistic images with incredible texture and lighting, which yet fall short in generating intricate patterns, indicating the fundamental inability of these models in learning the geo- metric representation of human hands and fingers. To better understand the ability, or lack thereof, of convolution op- arXiv:2401.01951v1  [cs.CV]  3 Jan 2024 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 1 2 3 4 0 0 0 0 0 1 1 1 1 1 2 2 2 2 2 3 3 3 3 3 4 4 4 4 4 + + -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 -3 = = -1 0 1 2 3 -1 0 1 2 3 -1 0 1 2 3 -1 0 1 2 3 -1 0 1 2 3 -3 -3 -3 -3 -3 -2 -2 -2 -2 -2 -1 -1 -1 -1 -1 0 0 0 0 0 1 1 1 1 1 + = -4 -3 -2 -1 0 -3 -2 -1 0 1 -2 -1 0 1 2 -1 0 1 2 3 0 1 2 3 4 Figure 2. A 5 × 5 geometry channel of rank 2 is illustrated in the rightmost tensor. The top and bottom rows correspond to horizon- tal and vertical coordinates, respectively. The standard horizontal and vertical coordinates are shown in the leftmost column. Tensors in the second column show random horizontal and vertical shifts. In the implementation, coordinate channels are divided by their sizes (in this case 4), and for optimisation, we sample horizontal and vertical shifts at once as a single random number representing their sum; thus, reducing the number of additions and samplings. eration in learning geometric information, we evaluate its performance on a geometric task, computing the centre of mass of finitely many points in a 2-dimensional plane. In this paper, we show that by providing convolution lay- ers with a single Geometry Channel (GeoChannel), encod- ing the Cartesian coordinates, as presented in Fig. 2, we sig- nificantly improve convolution’s capabilities. As illustrated in Fig. 3, GeoChannel is appended to the convolution’s in- put. We refer to the consecutive concatenation of GeoChan- nel and application of convolution as GeoConv. Compared to the existing approaches, like CoordConv [32], GeoConv 1. as we prove in Theorem 2, is computationally optimal and only concatenates a single channel to the convolu- tion’s input, compared to the n channels of CoordConv for n-dimensional convolution, 2. allows for random translations (shifts) of the Cartesian coordinate system to avoid learning incorrect absolute positional correlations (cf. Sec. 3.2), and 3. is more robust due to the smoothing effect of random shifts, making it the ideal candidate in generative appli- cations, such as in GANs and VAEs. Note that the random shifts in the GeoChannel are dif- ferent from those of the input. In particular, we found out contrary to the claim of [32], the mere addition of coor- dinate channels does not prevent mode collapse in GANs, even when we augment its inputs with random transforma- tions. In fact, we found out that CoordConv, which does not incorporate random shifts, is more prone to mode collapse in GANs than even the vanilla convolution. In the rest of Sec. 3, we show that a GeoConv-based GAN (GeoGAN) allows us to generate realistic hand ges- tures in the American Sign Language (ASL), while a stan- µ σ Figure 3. GeoConv in a VAE. Purple blocks indicate the input and output tensors, yellow blocks represent the output tensors re- sulting from previous layers’ convolution operation, and orange blocks indicate the geometry channels appended to them during the GeoConv’s operation before applying the next convolution. dard Convolutional GAN (ConvGAN) with the same design, but based on standard convolution, as well as SoA mod- els, such as DALL·E 3 fall short of achieving the same as shown in Fig. 1. Before presenting our results on hand ges- ture synthesis, we evaluate GeoGANs on the widely used CelebA-HQ. In more details, the experiments in this paper are organised in the following order. • We first demonstrate Theorems 1 and 2 in practice on two small geometric experiments in Sections 3.2 and 3.1. • In Sec. 3.3.2, we show that a GeoGAN trained on the CelebA-HQ dataset [22] generates more realistic human faces than a similar ConvGAN. Moreover, while Con- vGAN collapses within 250 epochs, GeoGAN remains stable throughout the training and produces more diverse images that match the dataset’s distribution. • In Sec. 3.3.2, we train the GANs using the Wasserstein distance [1, 21, 53] and gradient penalty [13] to prevent mode collapse in the ConvGAN. The resulting models are commonly referred to as WGAN-GP. Nevertheless, Ge- oGAN retains its edge. • In Sec. 3.3.3, we show that the same GeoGAN trained on the Hand Gesture dataset [4], generates realistic hand ges- tures in the American sign language, while the ConvGAN struggles to properly generate many of the hand gestures (cf. Figures 1c, 1d and 6.) • In Sec. 3.4, we evaluate GeoConv for using in VAEs, since VAEs offer numerical metrics that allow comparing GeoConv to CoordConv and standard convolution quanti- tatively. We repeat the experiments with VAEs, and train them on CelebA dataset [33]. The VAE based on Geo- Conv outperforms other VAEs in both image quality and diversity, as well as in achieving smaller losses. Related work CNNs have been ubiquitously deployed to achieve superhu- man performance in image classification and object detec- tion [14, 49]. More recently, they have been used for image generation using GANs [12, 22, 42], VAEs [26, 44, 45], and denoising diffusion models [17, 52]. In recent years, there has been a surge in the adoption of ViT [10, 43], inspired by the successful adoption of the attention mechanism [2] and transformers [54] in natural language processing. Despite their tremendous success in vision tasks, recent studies indicate that CNNs are on par with ViT in both accuracy [34, 51] and robustness [3, 41]. CNNs differ from human vision in many ways [27]. For example, they are often criticised for their limited recep- tive field, preventing them from learning wide-apart fea- tures within images [9, 27, 36]. Some of the attempts to ad- dress this issue include augmenting CNNs with transform- ers [15], using deformable CNNs [9], and augmenting con- volutions with coordinate information [32]. It is worth men- tioning that similar ideas have been used to improve ViT as well [18, 59]; however, these approaches are fundamentally different from the approach taken here, not because of their focus on transformers, but mainly because they try to ad- dress a different problem in ViTs. Liu et al. [32] demonstrated that CNNs also fail in trans- forming the spatial representation between input and output. They introduced CoordConv as a solution to this problem of CNNs. CoordConv adds one channel per input dimension to the convolution’s input, called coordinate channel. This has proven to improve CNNs’ performance in an array of tasks [32]. CoordConv has since been adopted in an array of applications [7, 31, 35, 55]. Nonetheless, CoordConv has several drawbacks as we discuss in more details in Problems 1 and 2 as well as in Sec. 3. 2. Geometry-aware convolution As we discussed in the related work, CoordConv mitigates the limited receptive field of convolutional layers as well as their inability to learn positional information in images by adding two coordinate channels, one for each dimension, before applying the convolution operation. These channels are shown in the two leftmost columns in Fig. 2. Coord- Conv has shown considerable improvements compared to convolution in an array of tasks [7, 31, 35, 55]. However, as we show in this paper, CoordConv has several drawbacks both in theory and practice. In theory, 1. CoordConv learns absolute positional correlations from the dataset, thus, resulting in biased models with poor performance in various tasks, while GeoConv learns the relative positional correlations when using the random shift (cf. Theorem 1), and 2. CoordConv is suboptimal (cf. Theorem 2), i.e., it in- troduces nℓs1 · · · sn learnable parameters for a single n-dimensional convolution operation with kernel size s1 × · · · × sn and ℓ output channels, instead of Geo- Conv’s ℓs1 · · · sn extra parameters. As we demonstrate in Sec. 3, these problems result in sub- par performance in practice. In this section, we introduce the Geometry-aware Convo- lution, or GeoConv for short, which not only resolves con- volution’s limited receptive field and inability to learn po- sitional information, but also addresses the aforementioned problems of CoordConv. In summary, GeoConv works as follows. Given an input tensor of size r1 × · · · × rn with k channels x ∈ Rr1×···×rn×k, we first create a GeoChan- nel g ∈ Rr1×···×rn, encoding the coordinates as well as a random coordinate shift, similar to the one in the right most column of Fig. 2. Tensor g is then appended to x resulting in tensor (x, g) ∈ Rr1×···×rn×(k+1), which is then fed into an n-dimensional convolution f. To better understand how GeoConv works, let us begin by describing how it resolves problems 1 and 2. Solution to Problem 1. The problem with adding the raw coordinate channels to the images is that, in addition to learning the spatial information about the image content, the model develops correlations between features and where they appear in images rather than their relative position with respect to one another. This is a fundamental flaw in most applications. For instance, if due to the bias in the training dataset a feature mostly appears in a certain part of the im- ages, the model begins to develop bias for the position of that feature. Such correlations are undesirable in most real- world scenarios. For example, when training face recogni- tion models, the input images or videos are nicely cropped and the faces are centred in the training set; however, in the real world, where the model is deployed, this is rarely the case. Thus, it is more essential for a face recognition model to learn where a person’s facial features are located with re- spect to each other than where they are exactly located in the input image or video. In Sec. 3.2, we explore this problem of CoordConv and GeoConv’s solution in more detail. Therefore, in GeoConv, we introduce random shifts to coordinate channels to prevent the model from learning un- wanted positional bias, as formally stated and proven in Theorem 1. Random shifts are shown in the second col- umn of Fig. 2. Note that these random shifts are different from random shifts applied to the input in data augmenta- tion, e.g., values on the edge of the GeoChannel are defined in the same way as the ones in the centre, unlike the input’s random shift, where the values on the edge are defined via some padding. Most notably, applying random shifts to the input does not prevent mode collapse in GANs that utilise CoordConv architecture. Theorem 1. When using random shift, GeoConv learns the relative positional information rather than the absolute po- sitional information, as in CooordConv. Proof. Let us denote the convolution operator with ∗. As we prove in Theorem 2, we can combine the n coordi- nate channels of CoordConv to a single channel, similar to GeoConv (but with no random shift), without affecting its performance. We denote this channel by c and GeoConv’s GeoChannel by g. Now, given an input tensor x of rank n with k channels, an s1 ×· · ·×sn convolution operator f on the k input channels, and a single GeoChannel g (amount- ing to a total of k + 1 channels), we have that f ∗ (x, g) = f (1,...,k)∗ x + f (k+1)∗ g, (1) where f (1,...,k) and f (k+1) denote the first k filters of f and the last filter of f corresponding to the input and GeoChan- nel, respectively. Let g′ = f (k+1)∗ g. We observe that g′ j1,...,jn = X i1,...,in f (k+1) i gj1+i1,...,jn+in = X i1,...,in f (k+1) i (cj1+i1,...,jn+in + r) = X i1,...,in f (k+1) i cj1+i1,...,jn+in + s1 · · · snr = f (k+1)∗ c + s1 · · · snr, (2) where r is a random shift sampled from a uniform distri- bution in GeoConv and 1 ≤ jℓ ≤ tℓ for 1 ≤ ℓ ≤ n, with t1 × · · · × tn being the input shape. It follows from Equa- tions (1) and (2) that f ∗ (x, g) = f ∗ (x, c) + s1 . . . snr. (3) Hence, f ∗ (x, g) is equal to f ∗ (x, c) modulo a random number s1 . . . snr. This prevents GeoConv from develop- ing unwanted correlations between f ∗ (x, c) and locations resulting in this value, while still allowing it to learn the patterns present in x. Solution to Problem 2. CoordConv adds one coordinate channel per dimension to the input. Nevertheless, as we formally state and prove in Theorem 2, this is unnecessary and inefficient, and in fact, a single filter is sufficient. Theorem 2. An s1 × · · · × sn convolution filter on the ℓ-th coordinate channel c(ℓ) in CoordConv does not extract any more information than a 1 × · · · × 1 × sℓ × 1 × · · · × 1 convolution filter. Proof. Let us use the same notation as in Theorem 1. Since the proof is similar for all coordinate channels, we only prove this for the first channel. Let f = (fi1,...,in) be the convolution filter corresponding the first coordinate channel c in CoordConv. Let ¯fi1 = X i2,...,in fi1,...,in, (4) where 1 ≤ ik ≤ sk for 1 ≤ k ≤ n. At each step of the convolution operation, we have that X i1,...,in fi1,...,inci1+j1,...,in+jn = X i1 ( X i2,...,in fi1,...,in)ci1+j1,j2,...,jn = X i1 ¯fiℓci1+j1,j2,...,jn. (5) Hence, the s1 × · · · × sn filter f does not extract any more information from the first coordinate channel c than the s1× 1 × · · · × 1 filter ¯f = ( ¯fi1). Therefore, in GeoConv, we combine all coordinate channels into one by adding them together, resulting in GeoChannel, illustrated in the rightmost column of Fig. 2. The GeoChannel is then concatenated to the input chan- nels as demonstrated in Fig. 3. By using a single geom- etry channel instead of the n coordinate channels in Co- ordConv, alongside the random shift, we achieve superior performance compared to CoordConv while using (n − 1)ℓ less filter per convolution, where ℓ is the number of out- put channels of the convolution. Consequently, we use (n−1)ℓs1s2 · · · sn less learnable parameters. This provides us with a model that is easier to train, faster, smaller, and thus, deployable in a wider range of edge devices. 3. Evaluation In this section, we evaluate GeoConv on a comprehensive range of tasks. In Sec. 3.1, we evaluate GeoConv capa- bility in geometric tasks by introducing the centre of mass benchmark, where GeoConv outperforms convolution and CoordConv by up to 50% and 35%, respectively (cf. Fig. 4). In Sec. 3.2, we compare all three architectures on a task for their absolute positional bias on a simple task consist- ing of classifying images containing the Greek numbers I, II, and III. GeoConv and convolution demonstrate the least bias, while CoordConv has the most. In Sec. 3.3, we com- pare all these architectures for use in GANs. We consider standard GAN [12] as well as WGAN-GP [13] for gener- ating human faces by training on the CelebA-HQ [22] and hands by training on the Hand Gesture dataset [4]. Geo- Conv generates the most realistic and diverse faces and hands, while CoordConv collapses in early epochs, per- forming even worse than standard convolution. We also compute the Fr´echet Inception Distance (FID) [16] and Inception Score (IS) [48] of the images generated by GANs using different architectures. Despite Generating more realistic and diverse images, GeoConv does not al- ways score favourably in terms of FID and IS, casting doubt on the applicability and validity of these metrics. We pre- dict this is due to the Inception network’s different training set and architectural flaws due to using vanilla convolution. Finally, we compare all three layers for use in VAEs in Sec. 3.4. GeoConv again outperforms others in terms of im- age quality and diversity as well as achieving smaller losses on both train and validation data. We have included the in- depth details of all experiments in Appendix A. The code for all the experiments is provided in the sup- plementary materials. All of the experiments have been per- formed in a GPU-poor setting on a computer with 128 GB of RAM and a single GeForce RTX 4090 GPU. 3.1. Calculating centre of mass In this benchmark, the goal is to compute the centre of mass of finitely many points in an image. The benchmark consists of datasets with different densities d. Each dataset consists of images containing white points on a black canvas, and d denotes the percentage of white points in the images in a dataset. Details of the dataset are included in Appendix A.1. For the ablation study, we consider four different designs with i layers and j filters for i, j ∈ {1, 2}, denoted by ixj. Models are trained on datasets with different densities, d ∈ \b 0.001 × 3k : 0 ≤ k ≤ 6 \t , using the Euclidean norm. Therefore, for each training density d, a total of 3 × 4 = 12 models are trained. All models are then evaluated on the test sets with the same density as well as all other densities. To make the comparison comprehensive, we also com- puted the normalised losses (by dividing by the summa- tion over all losses across different architectures and test and train ratios). We have explained this in detail in Ap- pendix A.1. As outlined in Tab. 1, GeoConv shows consid- erable advantage compared to convolution and CoordConv, outperforming them by 46% and 57%, respectively. More- over, Fig. 4 shows the normalised losses averaged over all train and test densities for each architecture and for different number of layers and filters. Again, GeoConv outperforms convolution and CoordConv in all combinations. 3.2. Positional dependencies This experiment is designed to evaluate the (absolute) posi- tional bias learnt by different architectures. The models are trained on the train dataset consisting of images of Greek numbers I, II, and III; However, the distribution of where the numbers are located in the images is different among the train and test datasets, as described in Appendix A.2. Table 1. Comparison of average, normalised average, and best performances of convolution, CoordConv, and GeoConv on the mass centre experiment. Conv CoordConv GeoConv Avg. loss 274.1 218.0 117.1 Norm. avg. loss 0.416 0.337 0.246 # of best perf. 8 15 26 0.000 0.025 0.050 0.075 0.100 0.125 1x1 1x2 2x1 2x2 Conv CoordConv GeoConv GPT-4V Figure 4. Ablation study on the performance of models using each architecture with different number of layers and filters. A side observation is GPT-4V’s [39, 40] intriguing failure in this task. We evaluated GPT-4V’s performance on 140 (20 per den- sity) image from the dataset, without fine-tuning, but with prompt- engineering, and scaled it by the same scaling factors as other bars. As shown in Tab. 2, CoordConv has the worst performance among all three architectures, and GeoConv and convolu- tion are on par with each other. Details of models and train- ing details are included in Appendix A.2 3.3. Generative adversarial networks In this section, we use GeoConv for generating human faces and hand images using GANs [12]. GANs are widely used in an array of tasks besides the applications considered here, such as super-resolution [30], photo blending [57], etc. and our contribution can open new doors in those applications as well. For all experiments in this section, we have used the same design for the models, as described in details in Ap- pendix A.3. For simplicity, we prepend “Conv”, “Coord”, and “Geo” prefixes for the name of the models. For exam- ple, a GAN which uses GeoConv is referred to as GeoGAN. We have organised this section as follows. Standard GAN for face generation. In Sec. 3.3.1, we evaluate the performances of the convolution, GeoConv, and CoordConv in standard GANs [12, 42] for generating human faces, by training on the CelebA-HQ dataset [22] for 450 epochs. CoordGAN collapses in the first 30 epochs and does not yield meaningful images. ConvGAN collapses within 250-300 epochs, while GeoConv did not collapse within 450 epochs. We have provided qualitative and quan- titative summaries of performances in Fig. 5 and Tab. 3. Table 2. The average loss and accuracy of the models when the numbers are moved to all of the possible positions in a 64 × 64 canvas Conv CoordConv GeoConv Avg. loss 1.84 2.31 1.59 Avg. acc. (%) 34.9 34.1 34.8 # of best perf. 2 0 3 (a) ConvGAN (b) GeoGAN (c) ConvWGAN-GP (d) GeoWGAN-GP Figure 5. Human faces generated by ConvGAN (Fig. 5a), GeoGAN (Fig. 5b), ConvWGAN-GP (Fig. 5c), and GeoWGAN-GP (Fig. 5d), trained on CelebA-HQ. Each image is generated as follows. For each of the models, we generated 10 images from randomly sampled latent points. The image with the highest score from the discriminator is added to the canvas. This is repeated 16 times for a 4×4 canvas. WGAN-GP for face generation. To prevent mode col- lapse in CoordGAN and ConvGAN, we used WGAN-GP [13] with the same design. This prevented mode collapse in ConvGAN; nevertheless, CoordGAN collapsed within the first 20 epochs. We also reduced the number of epochs to 150 since training with gradient, requires computing second-order derivatives and is computationally expensive; moreover, we observed that the generated images do not improve after 100 epochs. We have provided a qualitative summary of performances in Fig. 5. WGAN-GP for hand generation. We trained conditional WGAN-GP, with the same design, on the ASL Hand Ges- ture dataset [4] for generating human hand gestures show- ing numbers “0” to “9” and letters “a” to “z” in the Amer- ican sign language for 1,000 epochs. The dataset consists of 2,524 images, with around 70 images per each of the 36 labels. As expected, CoordWGAN-GP collapses on such a small dataset. Even though ConvGAN succeeds in generat- ing meaningful hand gestures, it, sometimes, falls short of reproducing the correct gesture and suffers in terms of im- age quality, while GeoConv manages to generate the best images with correct gesture, as evident in Fig. 6. 3.3.1 GAN for face generation. Figures 5a and 5b show the images generated by the Con- vGAN and GeoGAN. We have included the training and models’ details, sampling process, as well as more images generated by each model in Appendix A.3. From Figures 5a and 5b, we observe that GeoGAN produces images 1. that are better in terms of the overall face layout, 2. have more detail, e.g., teeth, makeup, skin tone, etc., and 3. are more diverse, including 68% female and 31% male images closely replicating the training set’s distribution with 63% female and 37% male images. Additionally, we compared the generator and discrimina- tor of each of the models against one another and the dataset in Tab. 3. GeoGAN’s generator deceives ConvGAN’s dis- criminator more by a factor of 10, and GeoGAN’s discrim- inator is 50% less likely to misclassify real images. 3.3.2 WGAN-GP for face generation. Wasserstein GANs [1] with Gradient Penalty (WGAN-GP) [13] emerged as a solution to the mode collapse prob- lem in standard GANs. In hopes of addressing mode col- lapse in ConvGAN and CoordGAN, we trained the mod- els with the same designs as those in Sec. 3.3.1 on the CelebA-HQ dataset. We have included the training detail in Appendix A.3. CoordWGAN-GP again failed to produce meaningful results due to early mode collapse. However, ConvWGAN-GP succeeded in generating more diverse im- ages, despite falling short in comparison to GeoWGAN-GP as evident in Fig. 5c and Fig. 5d. The qualities of images generated by both models slightly decreased compared to the standard GANs. Nonetheless, GeoWGAN-GP still pro- duced better images compared to ConvWGAN-GP in terms of Items 1-3 above. Table 3. Duels between ConvGAN and GeoGAN discriminators and generators on 10,000 images generated by each of the genera- tors and real images from CelebA-HQ dataset. Numbers show the percentage of images misclassified by each of the discriminators against its generator (Self) and opponent’s generator (Opp). Co- ordconv is not included due to early mode collapse. Misclassification Rate (%) Architecture Self Opp. Real ConvGAN 75.02 7.88 0.50 GeoGAN 42.94 0.84 0.26 Figure 6. Hand gestures generated by ConvWGAN-GP (top), and GeoWGAN-GP (bottom), trained on the ASL Hand dataset. Each image is generated as follows. For a given model and label, we generated 10 images from randomly sampled latent points. The image with highest score from the discriminator is added to the canvas. We repeat this for each of the 36 labels. Hand gestures generated by GeoWGAN-GP, in addition to being clearer, have the correct formation and correspond to the correct label, while some of the gestures by ConvWGAN-GP, like ‘4’, ‘6, ‘h’, ‘r’, and ‘s’, show incorrect gestures and some other, like ‘3’, ‘7’, ‘c’, ‘f’, ‘i’, and ‘o’, are deformed. 3.3.3 WGAN-GP for hand generation. Fig. 6 shows hand gestures generated by ConvWGAN-GP and GeoWGAN-GP. Training details and more images are included in Appendix A.3. ConvWGAN-GP fails to learn the correct representations for some of the gestures that require intricate geometric understanding, such as in ‘r’, where the middle and index fingers are crossed. It also generates mutated and contorted fingers for some other ges- tures, such as when a finger is hidden behind another as in ‘o’ or ‘c’. This shows standard convolution’s inherent inability to learn complex details. GeoWGAN-GP, on the other hand, learns more accurate representations for differ- ent hand gestures and generates images of higher quality clear of mutations and contortions. 3.3.4 FID and IS metrics Conventional GAN assessment metrics like IS and FID are often used for evaluating GANs; however, they often fail to reflect the true performance of a GAN [5, 6, 19, 23]. This Table 4. The FID and Inception Score of the GAN and WGAN- GP models, trained on CelebA-HQdataset, calculated over 25,000 images. Coordconv is not included due to mode collapse. Architecture IS FID ConvGAN 1.122 5.028 GeoGAN 1.113 7.151 ConvWGAN-GP 1.317 9.930 GeoWGAN-GP 1.564 18.870 is because the Inception model’s training dataset as well as its architectural flaws affect these scores. In particular, this could be due to the use of vanilla convolution, which has limited receptive field and cannot learn positional informa- tion [32]. Nonetheless, we evaluated ConvGANs and Ge- oGANs using these metrics, as summarised in Tab. 4. Com- puting IS and FID reliably require at least 5,000 images; hence, we only evaluate on CelebA-HQ dataset. Contrary to the quality and diversity of generated images, ConvGANs scores favourably compared to GeoGANs. 3.4. Variational autoencoders Due to challenges in the quantitative comparison of GANs, we also evaluate GeoConv for use in VAEs [26], especially since the effectiveness of convolutions in VAE applications relies on learning both local and global features from im- ages [27, 32]. VAEs are used in a range of applications [20, 28, 50]; however, in this section, we only focus on gen- erating human faces by training on the CelebA dataset [33]. We have included a similar experiment for generating hand gestures for ASL numbers and letters, similar to Sec. 3.3.3, as well as model and training details in Appendix A.4.1. For each latent dimension d = 256, 384, 512, we trained GeoVAE, CoordVAE, and ConvVAE five times to obtain the means and 95% Confidence Intervals (CI) in Fig. 7. Across different latent sizes, GeoVAE obtains 10-25% smaller loss and validation loss. Another notable observation is that, unlike ConvVAE and CoordVAE, GeoVAE’s loss does not fluctuate, and the 95% CI is quite small, especially com- pared to ConvVAE. We predict that this may be due to the smoothing effect of the random shift in GeoConv. Images generated by each of the VAEs for different la- 0 5 10 15 20 25 30 Epochs 8000 9000 10000 11000 12000 13000 14000 Loss Conv loss  Conv val. loss  GeoConv loss  GeoConv val. loss  CoordConv loss  CoordConv val. loss d = 256 0 5 10 15 20 25 30 8000 9000 10000 11000 12000 13000 14000 d = 384 Epochs 0 5 10 15 20 25 30 8000 9000 10000 11000 12000 13000 14000 d = 512 Epochs Figure 7. Mean and 95% CI of train and validation losses of GeoVAE (red lines), CoordVAE (dotted brown lines), and ConvVAE (dashed blue lines), trained on CelebA dataset for latent dimensions d ∈ {256, 384, 512} over five runs with seeds 0, . . . , 4. GeoVAE is more consistent across all runs and latent dimensions and obtains smaller mean loss and validation loss than both ConvVAE and CoordVAE. bels and latent values after 30 epochs are shown in Fig- ures 8 and 9, respectively. GeoVAE demonstrates a no- table capacity to produce diverse images given different la- tent points. In stark contrast, ConvVAE and CoordVAE fail to capture the dataset’s diversity, consistently generat- ing similar outputs for all latent points. GeoVAE also ex- hibits adaptability in attributes like hairstyle, eye and eye- brow styles, and even skin tones. Conversely, other models exhibit limited flexibility, yielding less diverse images. Fur- thermore, GeoVAE consistently produces higher-resolution images for all labels and latent points imbued with more pronounced and distinctive features compared to images generated by ConvVAE and CoordVAE. 4. Discussion In Sec. 3.3, we observed that GeoGANs generate more di- verse images that match training data’s distribution com- pared to ConvGANs. In the same way, we observe in Sec. 3.4, that GeoVAEs show more variation in generating human faces for different labels and latent points compared to CoordVAEs and ConvVAEs. Even though the better per- formance of GeoConv models is expected, it remains un- Conv CoordConv GeoConv Figure 8. Images generated by each of the VAEs for different labels. Images generated by GeoVAE (bottom) are clearer, have sharper edges, and contain more details than those generated by ConvVAE (top) and CoordVAE (middle). clear and requires further investigation why and how Geo- Conv models create more diverse images. Another significant observation from Fig. 7, is the re- markable consistency of GeoVAEs’ loss curves across dif- ferent runs and latent dimensions. Intuitively, we expected GeoVAEs to outperform their counterparts, but how this led to 5 and 11 times smaller 95% CI, in comparison to Coord- VAEs and ConvVAEs, requires additional exploration. 5. Conclusions and future directions In this paper, we demonstrated GeoConv’s capabilities in consistently producing better images with more details and diversity compared to existing convolutional architectures. We showed this for GANs and VAEs in generating hand gestures and human faces. Given that diffusion models suf- fer from some of the same problems, in particular in gener- ating human hands, GeoConv provides a promising research avenue to pursue in the future. Given the promising performance of GeoConv in the models considered here, we foresee it can improve large- scale SoA models, which we could not investigate due to computational constraints. We plan to investigate this fur- Conv CoordConv GeoConv Figure 9. Images generated by each of the VAEs for different random latent points. Images generated by GeoVAE (bottom) are more diverse and vary with the latent, while images generated by ConvVAE (top) and CoordVAE (middle) remain untouched. ther in our future work. Other avenues of research that we foresee GeoConv will contribute to include geometric tasks, such as depth estimation, object segmentation, 3D recon- struction, video generation, and several other applications. Acknowledgements This work is partially supported by the UK EPSRC via the Centre for Doctoral Training in Intelligent Games and Game Intelligence (IGGI; EP/S022325/1) and REXASI- PRO Horizon Euorope project (10.3030/10107002). We also thank Zahraa Al Sahili for providing feedback on pre- vious versions of this work. References [1] Mart´ın Arjovsky, Soumith Chintala, and L´eon Bottou. Wasserstein generative adversarial networks. In 34th In- ternational Conference on Machine Learning, ICML, pages 214–223. PMLR, 2017. 2, 6 [2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Rep- resentations, ICLR. OpenReview.net, 2015. 3 [3] Yutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are transformers more robust than cnns? In Advances in Neu- ral Information Processing Systems, NeurIPS, pages 26831– 26843. Curran Associates, Inc., 2021. 3 [4] Andre L. C. Barczak, Napoleon H. Reyes, Maria Abastillas, Ana Piccio, and Teo Susnjak. A new 2d static hand gesture colour image dataset for asl gestures. Research Letters in the Information and Mathematical Sciences, 15:12–20, 2011. 2, 4, 6, 12 [5] Shane Barratt and Rishi Sharma. A note on the inception score, 2018. arXiv preprint arXiv:1801.01973. 7 [6] Ali Borji. Pros and cons of gan evaluation measures: New developments. Computer Vision and Image Understanding, 215:103329–103344, 2022. 7 [7] Sungha Choi, Joanne T Kim, and Jaegul Choo. Cars can’t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 9370–9380. Computer Vision Foundation / IEEE, 2020. 3 [8] Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learn- ing from human preferences. In Advances in Neural Informa- tion Processing Systems, NeurIPS, pages 4299–4307. Curran Associates, Inc., 2017. 1 [9] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In IEEE/CVF International Conference on Com- puter Vision, ICCV, pages 764–773. IEEE, 2017. 3 [10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl- vain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Repre- sentations, ICLR. OpenReview.net, 2021. 1, 3 [11] Kunihiko Fukushima and Sei Miyake. Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position. Pattern recognition, 15(6):455–469, 1982. 1 [12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems NeurIPS, pages 2672–2680. Curran Associates, Inc., 2014. 1, 2, 4, 5 [13] Ishaan Gulrajani, Faruk Ahmed, Mart´ın Arjovsky, Vincent Dumoulin, and Aaron C. Courville. Improved training of wasserstein gans. In Advances in Neural Information Pro- cessing Systems, NeurIPS, 2017. 2, 4, 6 [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 770–778. Computer Vision Foundation / IEEE, 2016. 2 [15] Willy Fitra Hendria, Quang Thinh Phan, Fikriansyah Adzaka, and Cheol Jeong. Combining transformer and cnn for object detection in uav imagery. ICT Express, 9(2):258– 263, 2023. 3 [16] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilib- rium. In Advances in Neural Information Processing Sys- tems, NeurIPS, pages 6626–6637. Curran Associates, Inc., 2017. 4 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. In Advances in Neural Informa- tion Processing Systems, NeurIPS, pages 6840–6851. Curran Associates, Inc., 2020. 1, 2 [18] Qibin Hou, Daquan Zhou, and Jiashi Feng. Coordinate atten- tion for efficient mobile network design. In IEEE/CVF Con- ference on Computer Vision and Pattern Recognition, CVPR, pages 13713–13722. Computer Vision Foundation / IEEE, 2021. 3 [19] Min Jin Chong and David A. Forsyth. Effectively un- biased FID and inception score and where to find them. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 6069–6078. Computer Vision Foundation / IEEE, 2020. 7 [20] Hiroshi Kajino. Molecular hypergraph grammar with its ap- plication to molecular optimization. In 36th International Conference on Machine Learning ICML, pages 3183–3191. PMLR, 2019. 7 [21] Leonid V. Kantorovich. Mathematical methods of organizing and planning production. Management Science, 6(4):366– 422, 1960. 2 [22] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In 6th International Conference on Learning Representations, ICLR. OpenReview.net, 2018. 2, 4, 5, 12 [23] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 4401–4410. Computer Vision Foundation / IEEE, 2019. 1, 7 [24] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improv- ing the image quality of stylegan. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 8107–8116. Computer Vision Foundation / IEEE, 2020. [25] Tero Karras, Miika Aittala, Samuli Laine, Erik H¨ark¨onen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias- free generative adversarial networks. In Advances in Neural Information Processing Systems, NeurIPS, pages 852–863. Curran Associates, Inc., 2021. 1 [26] Diederik P Kingma and Max Welling. Auto-encoding vari- ational bayes. In 2nd International Conference on Learning Representations, ICLR. OpenReview.net, 2014. 2, 7 [27] Adam Kosiorek, Sara Sabour, Yee Whye Teh, and Geof- frey E. Hinton. Stacked capsule autoencoders. In Advances in neural information processing systems, NeurIPS, pages 15486–15496. Curran Associates, Inc., 2019. 3, 7 [28] Volodymyr Kovenko and Ilona Bogach. A comprehen- sive study of autoencoders’ applications related to images. In Proceedings of the 7th International Conference ”Infor- mation Technology and Interactions”, IT&I, pages 43–54. CEUR-WS.org, 2020. 7 [29] Yann LeCun, Bernhard Boser, John Denker, Donnie Hen- derson, R. Howard, Wayne Hubbard, and Lawrence Jackel. Handwritten digit recognition with a back-propagation net- work. In Advances in Neural Information Processing Sys- tems, NeurIPS, pages 396–404. Morgan-Kaufmann, 1989. 1 [30] Christian Ledig, Lucas Theis, Ferenc Husz´ar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, and Wenzhe Shi. Photo-realistic single image super-resolution using a generative adversarial network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 4681–4690. Computer Vision Foundation / IEEE, 2017. 5 [31] Younggun Lee and Taesu Kim. Robust and fine-grained prosody control of end-to-end speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP, pages 5911–5915, 2019. 3 [32] Rosanne Liu, Joel Lehman, Piero Molino, Felipe Pet- roski Such, Eric Frank, Alex Sergeev, and Jason Yosinski. An intriguing failing of convolutional neural networks and the coordconv solution. In Advances in Neural Information Processing Systems, NeurIPS, pages 9628–9639. Curran As- sociates, Inc., 2018. 2, 3, 7 [33] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In IEEE/CVF In- ternational Conference on Computer Vision, ICCV, pages 3730–3738. IEEE, 2015. 2, 7 [34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feicht- enhofer, Trevor Darrell, and Saining Xie. A convnet for the 2020s. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 11966–11976. Computer Vision Foundation / IEEE, 2022. 3 [35] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, and Shilei Wen. PP-YOLO: an effective and efficient implementation of object detector, 2020. arXiv preprint arXiv:2007.12099. 3 [36] Wenjie Luo, Yujia Li, Raquel Urtasun, and Richard Zemel. Understanding the effective receptive field in deep convolu- tional neural networks. In Advances in Neural Information Processing Systems NeurIPS, pages 4898–4906. Curran As- sociates, Inc., 2016. 3 [37] Midjourney. Midjourney.com, 2022. midjourney.com. 1 [38] Dongbin Na. CelebA-HQ face identity and attributes recog- nition using PyTorch, 2021. Accessed: 2023-09-22. 12 [39] OpenAI. GPT-4 technical report, 2023. arXiv preprint arXiv:2303.08774. 5 [40] OpenAI. ChatGPT can now see, hear, and speak, 2023. Blog Post. 5 [41] Francesco Pinto, Philip H. S. Torr, and Puneet K. Dokania. An impartial take to the CNN vs transformer robustness con- test. In European Conference on Computer Vision, ECCV, pages 466–480. Springer, 2022. 3 [42] Alec Radford, Luke Metz, and Soumith Chintala. Unsuper- vised representation learning with deep convolutional gener- ative adversarial networks. In 4th International Conference on Learning Representations, ICLR. OpenReview.net, 2016. 2, 5 [43] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jon Shlens. Stand-alone self- attention in vision models. In Advances in neural informa- tion processing systems, NeurIPS, 2019. 1, 3 [44] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In 38th International Conference on Machine Learning, ICML, pages 8821–8831. PMLR, 2021. 1, 2 [45] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional im- age generation with CLIP latents, 2022. arXiv preprint arXiv:2204.06125. 2 [46] Aditya Ramesh, James Betker, Gabriel Goh, Li Jing, Tim Brooks Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Man- assra, Prafulla Dhariwal, Casey Chu, and Yunxin Jiao. Improving image generation with better captions, 2023. cdn.openai.com/papers/dall-e-3.pdf. 1 [47] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image syn- thesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR, pages 10684–10695. Computer Vision Foundation / IEEE, 2022. 1 [48] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. In Advances in Neural Informa- tion Processing Systems, NeurIPS, pages 2226–2234. Curran Associates, Inc., 2016. 4 [49] Karen Simonyan and Andrew Zisserman. Very deep con- volutional networks for large-scale image recognition. In 3rd International Conference on Learning Representations, ICLR. OpenReview.net, 2015. 2 [50] Aman Singh and Tokunbo Ogunfunmi. An overview of vari- ational autoencoders for source separation, finance, and bio- signal applications. Entropy, 24(1):55, 2022. 7 [51] Samuel L. Smith, Andrew Brock, Leonard Berrada, and So- ham De. Convnets match vision transformers at scale, 2023. arXiv preprint arXiv:2303.08774. 3 [52] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois- ing diffusion implicit models. In 9th International Confer- ence on Learning Representations, ICLR. OpenReview.net, 2021. 1, 2 [53] Leonid N. Vaserstein. Markov processes over denumerable products of spaces, describing large systems of automata. Problems of Information Transmission, 5(3):64–72, 1969. 2 [54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, NeurIPS, pages 5998–6008. Curran Associates, Inc., 2017. 3 [55] Xinyao Wang, Liefeng Bo, and Fuxin Li. Adaptive wing loss for robust face alignment via heatmap regression. In IEEE/CVF International Conference on Computer Vision, ICCV, pages 6970–6980. IEEE, 2019. 3 [56] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Mul- tiscale structural similarity for image quality assessment. In The Thrity-Seventh Asilomar Conference on Signals, Systems & Computers, 2003, pages 1398–1402. IEEE, 2003. 14 [57] Huikai Wu, Shuai Zheng, Junge Zhang, and Kaiqi Huang. GP-GAN: towards realistic high-resolution image blending. In 27th ACM International Conference on Multimedia, MM, pages 2487–2495. ACM, 2019. 5 [58] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvt: Introducing con- volutions to vision transformers. In IEEE/CVF International Conference on Computer Vision, ICCV, pages 22–31. IEEE, 2021. 1 [59] Chao Xie, Hongyu Zhu, and Yeqi Fei. Deep coordinate at- tention network for single image super-resolution. IET Image Processing, 16(1):273–284, 2022. 3 A. Experimental setup In this appendix, we explain the experimental setup in this paper and provide more images, figures, and tables. A.1. Centre of mass Our motivation for choosing this task and configuration is that it requires the models to have a good understanding of the locations of a varying number of points spread out in a 2-dimensional plane with a few convolutional layers and filters. Therefore, the models need to obtain a geometric and global knowledge of where the points are, rather than a local knowledge provided by standard convolutions. Dataset details To cover different scenarios and have a comprehensive comparison between the architectures, we trained the networks on 7 synthesised datasets, each con- taining 100, 000 images, of size 32 × 32 with point density d, where d ∈ D = {0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 0.9}. The set D is roughly defined by the geometric progression 0.001 × 3k for 0 ≤ k ≤ 6, and covers a varying range of points starting from 0.001 and increasing geometrically with a factor of roughly 3, up to 0.9 density. We then evaluated the performances of each of the net- works on 7 test datasets, each containing 20, 000 images with density d ∈ D. All of the networks were trained using the Euclidean distance, between the predicted mass centre and the true mass centre, as the loss function. We have in- cluded the detailed results for the base models, i.e., models with 1 convolution layer and 1 filter (this is shown as 1x1 in Fig. 4) in Tab. 5. Detailed results for other models can be easily obtained by running the provided code. Model design All networks use convolution layers with a kernel size of 3 and a stride of 2 with ReLU activation, com- bined with a dense output layer with 2 nodes, corresponding to the x and y coordinates of the mass centre. As an ablation study, we consider 4 networks ixj, where 0 ≤ i, j ≤ 2. A.2. Positional dependencies Detaset details This dataset is designed to evaluate posi- tional bias, described in Sec. 3.2, in vision models. This dataset includes 64 × 64 images containing Greek numbers I, II, and III, corresponding to labels 1, 2, and 3. In the train- ing set, the Greek numbers are almost centred in the image, with little horizontal and vertical shifts, while in the test sets the Greek numbers move farther from the centre the images. Model Design We consider convolutional models with varying number of layers ranging over 1, . . . , 5. The n-th layer in each model has 2n−1 filters. All convolutions lay- ers have kernel size of 3 and use a stride of 2, with ReLU activation. The only other layer, is the output layer, which is a dense layer of size 3. The models are trained on the training set using the categorical cross entropy loss, which is the standard choice for multi-class classification tasks. As you can see in Tab. 2, despite having the highest num- ber of learnable parameters, CoordConv has the worst per- formance amongst all the architectures due to the positional bias learnt during the training. The complete results that Tab. 2 is derived from is available in Tab. 6. A.3. GAN In this section of the appendix, we discuss the details of the experiments in Sec. 3.3. A.3.1 Dataset details CelebA-HQ dataset CelebA-HQ [22], introduced in 2018, is a dataset consisting of 30,000 human face im- ages with 1024×1024 resolution. Since its introduction, it has been widely used in various applications for generat- ing realistic human faces. Unlike CelebA dataset, CelebA- HQ does not include annotations on facial features. This dataset includes 18,943 (63.15%) female images and 11,057 (36.85%) male images [38]. We use this dataset in our GAN experiments to gain insights on the capabilities and limita- tions of models using different convolution architectures. ASL Hand Gesture dataset ASL Hand Gesture [4] is a small dataset consisting of 2,524 annotated hand gesture im- ages representing numbers ‘0’ to ‘9’ and English alphabets ‘a’ to ‘z’ in the American sign language. The dataset im- ages are almost equally distributed between all the 36 la- bels; there are approximately 70 images per each labels. All images are on a black background and of different sizes, which are resized to 256×256 resolution at preprocessing. A.3.2 GANs for for generating face images Model design The generator and discriminator are de- signed according to common practices in training GANs. The discriminator’s architecture is similar to VGG-13. Here, we discuss the generator architecture. In the gener- ator, after one dense layer, and a reshape layer that takes the 1-dimensional latent to a 3-dimensional tensor, we have 5 blocks of layers, each consisting of the following 3 layers: • A transposed convolution/GeoConv/CoordConv layer with a stride of 2 and kernel size of 3 with no padding and leaky ReLU activation. • A convolution/GeoConv/CoordConv layer with a stride of 1 and kernel size of 3 with leaky ReLU activation. • A batch normalization layer. In the end, the output layer of the generator is a convolu- tion/GeoConv/CoordConv layer with the same specification as before except for the activation which is sigmoid. Table 5. The detailed loss table for 1x1 models in Fig. 4. Test ratio Train ratio Architecture 0.001 0.003 0.01 0.03 0.1 0.3 0.9 GeoConv 2.581 3.598 18.87 79.65 296.7 916 2777 0.001 CoordConv 2.267 4.630 27.02 107.5 392.9 1208 3654 Conv 2.438 4.622 24.88 100.7 370.3 1140 3449 GeoConv 5.435 2.640 2.871 6.01 20.12 66.90 211.4 0.003 CoordConv 4.530 2.112 3.553 18.33 76.43 242.8 742.3 Conv 4.356 2.104 4.025 21.25 87.28 276.4 844.0 GeoConv 6.381 3.180 1.291 4.558 24.15 80.95 251.6 0.01 CoordConv 9.380 4.875 1.971 2.978 8.45 14.17 15.1 Conv 6.329 3.145 1.261 4.608 24.48 82.07 255.1 GeoConv 9.36 5.008 2.142 1.095 1.495 4.72 13.14 0.03 CoordConv 11.15 6.370 2.803 1.145 4.580 11.74 14.90 Conv 6.84 3.668 2.133 0.890 7.321 29.40 95.90 GeoConv 7.371 3.948 2.405 1.925 0.610 5.696 23.21 0.1 CoordConv 7.548 4.042 2.377 1.837 0.601 5.216 21.29 Conv 8.369 4.426 2.164 1.398 0.667 2.916 12.06 GeoConv 6.942 3.859 2.875 2.988 2.440 0.350 7.430 0.3 CoordConv 7.874 4.163 2.279 1.895 1.506 0.342 4.474 Conv 9.035 4.841 2.231 1.300 0.789 0.348 1.467 GeoConv 9.228 5.114 2.61 1.75 1.26 0.888 0.349 0.9 CoordConv 5.176 5.655 7.66 8.44 8.07 6.095 0.147 Conv 5.221 7.904 10.67 11.39 10.77 8.085 0.156 Table 6. The average loss and accuracy of the models when the numbers are moved to all of the possible positions in a 64 × 64 canvas Metric Architecture 1 Layer 2 Layers 3 Layers 4 Layers 5 Layers Conv2D 1.16 1.26 1.60 2.20 2.95 Loss CoordConv 1.78 2.35 2.45 2.06 2.91 GeoConv 1.23 1.63 1.50 1.59 2.04 Conv2D 36.82 35.25 34.06 34.00 34.40 Acc. (%) CoordConv 34.08 34.03 34.35 34.17 34.10 GeoConv 34.93 34.29 36.20 34.35 33.76 Training detail For training the models in this experi- ment, we use the binary cross-entropy loss which is the common method for training GANs. We trained each model for 500 epochs. After reaching 400 epochs, none of the models showed any improvements. A closer look at generated images Fig. 10 portrays a 6×6 canvas with more images of ConvGAN and GeoGAN. No- tice the quality, colour, and diversity of the images by each of the models. A.3.3 WGAN-GPs for generating face images Model design The design of the generator and discrimi- nator in this section is similar to the generator and discrim- inator explained in Appendix A.3.2. Training detail WGAN-GPs use Wasserstein distance for their loss alongside gradient penalty. Since none of the models showed any improvements after around 100 epochs, we set the number of epochs to 150. A closer look at generated images Fig. 11 portrays a 6×6 canvas with more images generated by the WGAN- (a) ConvGAN (b) GeoGAN Figure 10. Human faces generated by ConvGAN (Fig. 10a) and GeoGAN (Fig. 10b) trained on CelebA-HQ dataset. Each image is generated as follows. For each of the models, we generated 10 images from randomly sampled latent points. The image with the highest score from the discriminator is added to the canvas. This is repeated 36 times for a 6×6 canvas. GPs. Notice the quality, colour, and diversity of the images generated by each of the models. A.3.4 WGAN-GPs for generating hand gestures Model design The design of the generator and discrimi- nator in this section is similar to the generator and discrim- inator explained in Appendix A.3.2. Training detail Training details are similar to Ap- pendix A.3.3, except that we run the experiments for 1,000 epochs to make sure all the models reach peak performance. A closer look at the generated images Fig. 12 shows the hand gestures generated by both ConvWGAN-GP and GeoWGAN-GP for each label of the ASL language. These are the same images as in Fig. 6; however, they have been scaled up for visualising more details and easier comparison between the images generated by each of the models. A.4. VAE In this section of the appendix, we discuss the details of the experiments in Sec. 3.4. A.4.1 Loss function S In VAEs, since the quality of generated images is closely associate to the loss function, we chose a loss function that helps training a model that not only generates images from the same distribution as the train images, but also helps gen- erating images that are sharper and have similar structural similarity. Therefore, we chose the loss the function to be a combination of • Binary Cross Entropy (BCE). BCE loss is used as a pixel-wise reconstruction loss in VAEs. It encourages the VAE to produce reconstructions that are statistically sim- ilar to the input data in a pixel-wise manner. • Mean Squared Error (MSE). MSE penalises large pixel-wise differences more heavily and is more sensitive to outliers than BCE. • Mean Absolute Error (MAE). MAE is less sensitive to outliers than MSE. Like MSE, it helps reduce pixel-wise differences between input and reconstruction, though the magnitude of errors is emphasised differently. • Multi-scale Structural Similarity (SSIM): SSIM [56] assesses structural similarity between images, consider- ing luminance, contrast, and structure. It helps capture high-level features and generate images that are struc- turally more similar to the training images. • Absolute difference of Sobel edge maps: Sobel edge maps highlight edges and gradients in images. Penalising (a) ConvWGAN-GP (b) GeoWGAN-GP Figure 11. Human faces generated by ConvWGAN-GP (Fig. 11a) and GeoWGAN-GP (Fig. 11b) trained on CelebA-HQ dataset. Each image is generated as follows. For each of the models, we generated 10 images from randomly sampled latent points. The image with the highest score from the discriminator is added to the canvas. This is repeated 36 times for a 6×6 canvas. the absolute difference between these maps encourages the VAE to reproduce edges accurately. It helps improve the sharpness and structural details in generated images. A.4.2 Dataset details CelebA dataset CelebA dataset is one of the most com- monly used datasets in both generative and discriminative applications in computer vision. This dataset includes 200k human face images. Each image comes with 40 binary attribute annotations about different features such as eye- brows, cheeks, nose, hair, eyeglasses, neckties, etc. ASL Hand Gesture dataset Please see Appendix A.3.1. A.4.3 Conditional VAEs for generating face images Model design Both encoder and decoder are designed ac- cording to standard practices. The encoder first feeds the in- put image through three convolution/GeoConv/CoordConv layers consecutively. All these layers have a kernel size of 3 and a stride of 2 and use ReLU activation. The result is then flattened and concatenated with the label. Then, we use two dense layers to learn the mean and standard devia- tion of the latent space. Then, a latent is sampled using the normal distribution with this mean and standard deviation. This latent and the label are then fed into the de- coder which will generate an image reconstructing the original image. After that, 5 transposed convolu- tion/GeoConv/CoordConv layers consecutively expand the feature map. Each of those layers has a kernel size of 3 and a stride of 2 and uses ReLU activation. Finally, a convo- lution/GeoConv/CoordConv layer with 3 channels, kernel size of 3 and a stride of 1 with sigmoid activation, synthe- sises the final image. Training detail We explained the loss function we use for training the VAEs in Appendix A.4.1. During the training, the loss curves start to flatten out after 20 epochs. Nonethe- less, we continued training the VAEs until 30 epochs. A.4.4 Conditional VAE for generating hand gestures The findings from the experiment on VAEs presented in the main body show the significant enhancements achieved by incorporating GeoConv into a VAE. These enhancements are observed both in qualitative and quantitative perfor- mance, as well as in a heightened capacity to capture the dataset’s diversity. In alignment with our experiments in the GAN section, in this section, we use VAEs for generating images of ASL hand gestures. While CelebA is a vast and diverse collection, compris- ing approximately 200k human face images, the hand ges- 0 1 2 3 4 5 6 7 8  GeoWGAN-GP    ConvWGAN-GP 9 a b c d e f g h  GeoWGAN-GP    ConvWGAN-GP i j k l  n o  q m  p  GeoWGAN-GP    ConvWGAN-GP r t u   w x y z  GeoWGAN-GP    ConvWGAN-GP v s Figure 12. Hand gestures generated by ConvWGAN-GP (first rows), and GeoWGAN-GP (second rows), trained on the ASL Hand Gesture dataset. These are copies of images included in Fig. 6 of the main body, scaled up for better comparison. ture dataset only contains just over 2,500 images, each shar- ing a similar appearance, primarily differing based on the represented alphabet or number. Consequently, this dataset introduces a distinct set of challenges for the VAEs. We train two conditional VAEs on the the gesture dataset for 100 epochs. We use the same architecture for the VAEs as in Appendix A.4.3, only differing in some hyperparameters. We run experiments using latent dimensions 64, 128, and 192. Additionally, each VAE is trained five times, with seeds 0, 1, . . . , 4. The training and validation loss during 0 20 40 60 80 100 Epochs 4000 4500 5000 5500 6000 6500 7000 7500 8000 Loss Conv2D loss  Conv2D val_loss  GeoConv2D loss  GeoConv2D val_loss (a) d = 64 0 20 40 60 80 100 Epochs 4000 4500 5000 5500 6000 6500 7000 7500 8000 (b) d = 128 0 20 40 60 80 100 Epochs 4000 4500 5000 5500 6000 6500 7000 7500 8000 (c) d = 192 Figure 13. Mean and 95% CI of train and validation losses of GeoVAE (red lines), and ConvVAE (dashed blue lines), trained on Hand Gesture dataset for latent dimensions d ∈ {64, 128, 192} over five runs with seeds 0, . . . , 4 during 100 training epochs. 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z ConvVAE 0 1 2 3 4 5 6 7 8 9 a b c d e f g h i j k l m n o p q r s t u v w x y z GeoVAE Figure 14. Hand gestures generated by ConvVAE (top row) and GeoVAE (bottom row) with 192-dimensional latent spaces. Images generated by GeoVAE have more realistic colours and are slightly sharper. 100 epochs of training are visualised in Fig. 13. As antic- ipated, training and validation losses are similar for both architectures across various latent dimensions. As we dis- cussed before, this is because of the Hand Gesture dataset’s small size and limited diversity. Fig. 14 presents the generated images produced by each of the conditional VAEs. Both models perform reasonably well in representing the correct gestures even though they do not produce high-resolution images compared to GANs. Digging deeper into the details, images generated by Geo- VAE have more realistic colours and sharper details such as more distinct fingers in comparison to the ConvVAE. "
}