{
    "optim": "1\nDynamic Texture Transfer using\nPatchMatch and Transformers\nGuo Pu, Shiyao Xu, Xixin Cao, and Zhouhui Lian Member, IEEE,\nAbstract—How to automatically transfer the dynamic texture\nof a given video to the target still image is a challenging and\nongoing problem. In this paper, we propose to handle this task\nvia a simple yet effective model that utilizes both PatchMatch and\nTransformers. The key idea is to decompose the task of dynamic\ntexture transfer into two stages, where the start frame of the\ntarget video with the desired dynamic texture is synthesized in\nthe first stage via a distance map guided texture transfer module\nbased on the PatchMatch algorithm. Then, in the second stage,\nthe synthesized image is decomposed into structure-agnostic\npatches, according to which their corresponding subsequent\npatches can be predicted by exploiting the powerful capability of\nTransformers equipped with VQ-VAE for processing long discrete\nsequences. After getting all those patches, we apply a Gaussian\nweighted average merging strategy to smoothly assemble them\ninto each frame of the target stylized video. Experimental results\ndemonstrate the effectiveness and superiority of the proposed\nmethod in dynamic texture transfer compared to the state of the\nart.\nIndex Terms—Texture Transfer, video synthesis, image gener-\nation.\nI. INTRODUCTION\nN\nOWADAYS, with the rapid development of computer and\nnetwork technologies, dynamic textures have become\npopular in a variety of media, such as films, digital posters,\nadvertisements, and online chatting, making contents more\nappealing and varied. However, designing a set of dynamic\nspecial effects takes numerous times and efforts even for\nskilled designers. Taking artistic dynamic texts as an example,\nfont designers need to design the special text effect frame by\nframe, and apply this dynamic effect to different characters\nin the entire font library, which is extremely labor-intensive\nand time-consuming. Hence, the aim of this paper is to\nautomatically transfer the dynamic effect of a given video\nsample to a target shape or image, significantly improving the\ndesigning efficiency of dynamic textures.\nThe above-mentioned process is called dynamic texture\ntransfer. Compared with static texture transfer, both the in-\nternal spatial information of the image and the temporal\ninformation across all frames need to be considered. What is\nmore, the flame flickering and shaking problems that appear\nin the video synthesis process also need to be avoided.\nMost current texture transfer methods [1], [2], [3], [4]\naim at handling static images which do not apply temporal\nGuo Pu, Shiyao Xu and Zhouhui Lian are with Wangxuan Institute of\nComputer Technology, Peking University, Beijing 100871, China. Xixin Cao\nis with School of Software and Microelectronics, Peking University, Beijing\n100871, China\nCorresponding Author: Zhouhui Lian (email: lianzhouhui@pku.edu.cn)\nTransfer\n!!!\nFig. 1. An example of dynamic texture transfer. Given a sample video and a\ntarget still image, the proposed method is able to synthesize the target video\nby transferring the dynamic texture of the sample video into the target image.\nconstraints thus fail to satisfactorily address the dynamic tex-\nture transfer problem. Moreover, existing neural-based meth-\nods [5], [6], [7], [8] typically require huge amounts of data\nto train their models which are incapable of handling the\none-shot effects transfer task. Furthermore, these neural-based\nmodels are required to resize all images to a certain aligned\nscale during training which causes detail losing of delicate\ntextures, while our method works in a full-resolution fashion\nand thus is capable of transferring complex textures on their\noriginal resolution. The most relevant work to our paper is [9],\nwhere Men et al. [9] proposed to stylize a still semantic\nmap to the target video. However, since a common Nearest-\nneighbor Field (NNF) is adopted to guide the texture synthesis\nprocedure across all keyframes simultaneously, their method\nfails to apply to complex dynamic effects (e.g. with motions).\nIn addition, the correlation of video frames is not properly\nexploited and thus there still exist some incoherent regions in\ntheir synthesis results.\nTo solve the above issues, we propose DynTexture, a neural-\nbased approach to automatically transfer various dynamic\neffects. As shown in Fig. 1, our method requires only a source\ntexture video and a target semantic map as input, then it can\nautomatically generate the target video with the same texture\neffect as the source video.\nTo the best of our knowledge, the proposed method is the\nfirst neural-based approach to tackle this one-shot dynamic\ntexture transfer problem. To animate the binary semantic\nimage, we first introduce an attention term called distance\nmap, which can help to better preserve the structural informa-\ntion. With the guidance of distance maps, the offset and the\ncorrespondence between inputs are calculated and the initial\ntarget frame can then be synthesized via the PatchMatch [10]\nalgorithm. Afterwards, we cut the start frame into structure-\nagnostic patches and calculate their embeddings to make better\nuse of the texture continuity and the patch distributions inside\nthe image. In the second stage, we use the Transformer [11]\narXiv:2402.00606v1  [cs.CV]  1 Feb 2024\n2\nmodel to predict the discretized code sequences, which is\ncapable of capturing the long-distance dependencies between\nframes.\nIn summary, our major contributions are threefold:\n• We obtain impressive results in the dynamic text effects\ntransfer task with a new texture transfer and animation\nscheme that consists of a distance map guided texture\ntransfer module and a novel deep sequence forecasting\nmodule. Extensive experiments demonstrate our method’s\nsuperiority over state-of-the-art methods and versatility in\nvarious texture transfer and animation tasks.\n• We tackle the challenge of insufficient data when training\nthe deep sequence forecasting module through over-\nlapping patch splitting and merging data augmentation,\nenabling the model to effectively resolve the one-shot\ndynamic texture transfer problem.\n• The independence of the texture rendering module and\nthe sequence forecasting module makes our method capa-\nble of synthesizing more complex dynamic effects (e.g.,\nwith motions) that cannot be properly handled by existing\nstate-of-the-art approaches (e.g., [9]).\nII. RELATED WORK\nDynamic texture transfer has been a challenging task. At\npresent, there exist very few works that try to solve this\nproblem. In this section, we briefly survey the texture transfer\nand animating literature and summarize the current state of\nthe art. The following highlights the techniques most closely\nrelated to ours.\nThe still texture effects transfer task has been well resolved\nby existing approaches (e.g., [1], [2], [3], [12]) that take\nthe input semantic map as structure guidance to learn the\ncorrespondence between the source and target images. Yang\net al. [1] proposed to transfer the text effects by analyzing the\nhigh regularity of the spatial distribution for target effects.\nActually their work can be seen as the extension of [10],\n[13]. They performed transferring by calculating the similarity\nattractively between patches to find the best matching. As\nfor deep-learning-based methods, Yang et al. [14] designed a\nGAN-based network to accomplish both the objective of style\ntransfer and style removal so that it can learn to disentangle\nand recombine the texture of content. However, like most deep\nlearning models, their work requires a lot of paired data and\nis unsuited to handle the one-shot texture transfer task in our\ncase. Shaham et al. [4] trained the model only on one single\nimage to capture the internal distribution of patches, using a\npyramid of fully convolutional GANs, to capture details at\ndifferent scales. However, when applying the method [4] on\ncomplex cases, for example, the burning flame with dynamic\ntexture effects, the generated results are blurry and can hardly\npreserve the desired structure. Since temporal consistency is\nnot considered, the above-mentioned methods fail to handle\nthe task of dynamic effects transfer.\nTo take temporal constraints into consideration, dynamic\neffects transfer methods like [15] perform dynamic effects\ntransfer by analyzing the motion of textured particles in the\nvideo while the method is not a fully automatic method\nPatchMatch\nInitial Source \nFrame\nGenerated Initial \nTarget Frame\nCalculate Distance\nFig. 2.\nUtilizing distance information to guide the PatchMatch algorithm,\nletting the flow of information outward from the boundary.\ndue to the requirement of user-specified flows. Tesfaldet et\nal. [6] utilized a pre-trained convolutional network to model\nthe dynamics of per-frame appearances of textures to generate\ntarget textures. Xie et al. [5] defined a probability distribution\nover video sequences and introduced the temporal constraint\nthrough multi-scale model layers to capture spatial-temporal\npatterns. However, these methods require plenty of training\ndata and struggle to capture the details of textures, thus failing\nto deal with the one-shot texture transfer task.\nMost image animation approaches [16], [17], [9], [8], [7]\neither require large amounts of videos for training, failing\nto work in the one-shot learning fashion, or have strong\nrestrictions for the input objects such as human poses, thus\ncannot be applied to synthesize dynamic textures.\nRecently, Men et al. [9] extended the NNF search of\nPatchMatch [10] to the spatial-temporal domain and obtained\nimpressive results in dynamic text effects transfer. They re-\nstricted the procedure of patch searching within the extracted\nkeyframes to maintain both spatial and temporal consistencies,\nin the meantime losing inter-frame information which causes\nincoherent structures in their synthesis results. Moreover, their\nmethod only searches for a single global NNF, and thus fails\nto process dynamic textures with moving samples which shift\nacross source videos.\nTransformer-based methods are emerging in Computer Vi-\nsion tasks, demonstrating their inherent advantages in captur-\ning relationships and dependencies between sequences. Esser\net al. [18] introduced the convolutional VQ-VAE [19] to\nlearn compressed discrete representations for images and then\nmodeled learnt embeddings with Transformers to generate\ncontrollable realistic images. Yan et al. [20] used a simple\nGPT-like architecture to model the discrete latent codes learnt\nfrom videos by VQ-VAE [19] to generate new videos. Con-\nstrained by the compressing ability of VQ-VAE, their results\nare limited to a very low resolution.\nIII. METHOD DESCRIPTION\nIn this section, we first formulate the task of dynamic texture\ntransfer (Section 3.1) and then present the detailed method\ndescription of our approach (Section 3.2 and 3.3), which is\nnamed as DynTexture (Dynamic Texture transfer) for short.\nFig. 3 demonstrates an overview of our proposed DynTex-\nture. The key idea is to decompose the one-shot dynamic\n3\nTransformer\nDecoder\nPatchMatch\nInitial Source Frame\nGenerated Initial Frame\nSource Frames\nPredicted Frames\nSource Text and \nDistance Map\nTarget Text and \nDistance Map\nCodebook\nPatch \nCutting\nGaussian\nPatch \nMerging\nEncoder\nPredicted Latents\nDiscrete Latents\nFig. 3. Overview of the proposed DynTexture, which is designed as a two-stage architecture, where the distance map guided texture rendering module generates\nthe initial frame, and the novel deep sequence forecasting module predicts and synthesizes the subsequent frames based on the previously-synthesized initial\nframe.\ntexture transfer task into two stages where the start frame of\nthe target video with the desired dynamic texture is synthesized\nin the first stage (Section 3.2) via a distance map guided tex-\nture transfer module based on the PatchMatch algorithm [10].\nThen, in the second stage (Section 3.3), the synthesized image\nis decomposed into structure-agnostic patches, according to\nwhich their corresponding subsequent patches can be predicted\nby exploiting the powerful capability of Transformers [11]\nequipped with VQ-VAE [19] for processing long discrete\nsequences. After getting all those patches, we apply a Gaussian\nweighted average merging strategy to smoothly assemble them\ninto each frame of the target stylized video.\nA. Problem formulation and analysis\nThe proposed DynTexture can be used to handle various\ntypes of dynamic texture transferring tasks. For the sake\nof convenience, we formulate the task of dynamic texture\ntransfer under the scenario of dynamic text effects transfer.\nMore specifically, given a source text image Stext, its styl-\nized animation Sstyle and a target image Ttext, the goal\nof dynamic text effects transfer is to synthesize the target\nstylized animation Tstyle such that Stext:Sstyle::Ttext:Tstyle,\nas shown in Fig. 1. We decompose the task of dynamic text\neffects transfer into two sub-problems: still image texture\ntransfer and dynamic effects animation. For the first sub-\nproblem, PatchMatch-based methods like [1] have achieved\nimpressive results for texture transfer in still images. We tackle\nthe still image texture transfer based on PatchMatch as well.\nAs for how to transfer the dynamic effects to the still target\nimage, we take advantage of the Transformers’ long sequence\nprediction ability, decomposing the image into large amounts\nof structure-agnostic patches and training Transformers to\nlearn the correlation between frames.\nB. Initial frame synthesis\nWe use the distance map guided PatchMatch [10] algorithm\nto search an NNF for the initial frame and generate the initial\nframe with the desired texture effect.\nFor still text effects transfer, the target stylized image\nis synthesized with the semantic guidance Ttext. Obviously,\nthe patches in the text contour contain more features and\ncould easily find proper correspondences from the source.\nBy exploiting the preserving and propagating features of the\nPatchMatch algorithm, we can find the optimal matches by\nenforcing texture continuity within and near the contour. This\nguarantees the spatial consistency of the initial frame.\nUnfortunately, the naive utilization of PatchMatch [10] in\nthe application of text effects transfer fails to transfer com-\nplicated textures without semantic guidance outside the text\n(see the second row of Fig. 11). Inspired by [9], we calculate\na distance map as the propagation guidance to enable the\ntexture effect to spread around the text contour. Following [9],\nthe distance is calculated as the normalized distance between\neach pixel of Ttext and the contour of text. The initial frame\nsynthesis process is illustrated in Fig. 2.\nC. Subsequent frame prediction\n1) Patch cutting: The source exemplar contains both source\nstructure information and source texture effects where we only\nwant the model to learn the texture information and ignore the\nother. To achieve that, we cut overlapping patches for every\nframe in the source video and the target image, and process\nthem independently in later procedures.\nThe purpose of patch cutting is to decompose the input\nimage into structure-agnostic patches. We can also consider\nthis as a way of data augmentation as it produces huge amounts\nof training data for the model. The patches are overlapped\nsince we want to make the model be aware of the continuity\nbetween its neighbours and encourage the model to generate\nsmooth results. Consequently, a smaller patch cutting stride\nleads to better results and the stride is set to 1 in our method.\nThe choice of patch size is crucial for patch-based synthesis\nmethods like the proposed DynTexture. A smaller patch size\nmakes the patches less discriminative and causes a large\nportion of repetitive patterns or even misplaced patterns. On\n4\nthe contrary, a large patch size tends to contain too much\ninformation of the source structure and thus affects the target\nstructure of the synthesized video, which is unsatisfactory. We\nexperimentally choose the patch size as 16 by 16 which results\nin a good trade-off between better patch discrimination and\nless structural damage.\n2) Compressing patches by VQ-VAE: Using Transformers\nto predict long subsequent patch sequences requires us to learn\na rich codebook to effectively compress patches. VQ-VAE\nproposed in [19] is able to reconstruct sufficiently realistic\nimages by representing them with its codebook entry codes.\nThereby, we employ VQ-VAE to learn the texture patches\nfrom the source exemplar and compress the patches into\ncompact latent codes (i.e., code-book entry indices) for further\nprocessing. To make it adaptive for the small patch input, we\nslightly modify the original VQ-VAE model to a small-scale\nversion, which is trained by computing the reconstruction loss,\nthe codebook loss, and the commitment loss:\nLV QV AE = ||x − D(e)||2\n2 + ||sg[E(x)] − e||2\n2+\nβ||sg[e] − E(x)||2\n2,\n(1)\nwhere E and D are the encoder and the decoder of VQ-VAE,\nrespectively, x is the input patch, e is the latent embedding\nof x, β is the coefficient of the commitment loss, and sg\ndenotes the stop-gradient operation. The objective loss func-\ntion LV QV AE consists of three terms where the first term is\nthe reconstruction loss that supervises VQ-VAE to learn good\nrepresentations to reconstruct the patches. The second term\nand the third term are the codebook loss and the commitment\nloss, respectively, guiding the model to learn a good codebook.\nThe encoder and the decoder of VQ-VAE are trained on\nthe patches from source frames, thus the patches can be rep-\nresented in terms of the codebook indices of their encodings.\nSpecifically, given a patch x, we denote the codebook indices\nof x as Ix. Elements in Ix are the nearest entries among\nall elements in the codebook with respect to Zq, and can be\nformerly calculated as:\nIxij = arg min\nθ∈Θ\nDist(Zqij, Zθ),\n(2)\nDist(Zqij, Zθ) =\n\r\rZqij − Zθ\n\r\r\n2 ,\n(3)\nwhere Zq is the quantized encoding of x, Zθ is the θ-th\nelement in the codebook, Θ denotes the size of codebook,\nDist denotes the distance calculation function, and i and j\ndenote the indices of element locations.\nNotice that the patch x can be easily recovered by firstly\nmapping Ix to Zq, and then decoding Zq via the decoder. We\nwill recover the predicted subsequent patch index sequence in\nthe same manner.\nUtilizing VQ-VAE to encode the patches into latent codes\nbrings two benefits: First, VQ-VAE learns the texture informa-\ntion from structure-agnostic patches and represents the patches\nas high-quality latent codes, and it is able to generate new\npatches through manipulations on the latent codes. Second,\nVQ-VAE achieves a high compressing rate by converting patch\nimages to latent codes, which markedly reduces the prediction\nburden of the deep sequence forecasting module, and thus\nFig. 4. Comparison between simple average and Gaussian weighted average\nmerging strategies. Gaussian weighted average obviously obtains higher-\nquality results.\nenables long sequence prediction with limited computing and\nmemory resources.\n3) Subsequent sequence prediction via Transformers:\nTransformers [11] have shown tremendous successes in mod-\neling discrete data such as natural languages. The architecture\nof Transformers employs multi-head self-attention blocks fol-\nlowed by point-wise MLP feed-forward blocks. It is partic-\nularly suitable for the generation of long discrete sequences\nwhich coincides with our desire for predicting the discrete\nlatent codes of subsequent sequences. After encoding the\ninitial frame patches into discrete latent codes, we utilize\nTransformer to predict the subsequent latent code sequences.\nSpecifically, the indices of initial frame patches are fed into\nthe Transformer model and then the predicted indices of the\nsubsequent patch sequence are outputted. During the training\nstage, we concatenate the patch indices of the subsequent se-\nquence into a long sequence as the target sequence. Formally,\nthe Transformer model used here aims to fit the following\nfunction:\nP(I0, I1, ..., IF |I0) =\nF\nY\ni=1\nP(Ii|I0),\n(4)\nwhere P denotes the probability distribution, Ii denotes the\npatch index of the i-th frame, and F denotes the total number\nof frames.\nThe cross-entropy loss is used to train the model:\nLT ransformer = −\nX\np(Ipred) log q(IGT ),\n(5)\nwhere Ipred and IGT are the predicted and ground-truth\npatch indices, respectively, whose distributions are denoted by\np(Ipred) and q(IGT ), respectively.\nOnce our model has learnt the correspondence between\nthe indices of the initial frame and those of the subsequent\nframes, it will ensure the temporal consistency of the decoded\nvideo. As a result, each synthesized image patch will imitate\nthe appearance variance learnt from the source patches while\nacquiring motion properties implicitly.\n4) Patch decoding and merging: After getting the predicted\npatch indices in the subsequent sequence, we use the trained\ndecoder of VQ-VAE to decode the predicted patch indices and\nassign them to the corresponding frames.\n5\nSince there exist overlapping regions between adjacent\npatches, a good patch merging strategy is crucial for patch-\nbased image synthesis. As shown in Fig. 4, applying simple\naverage to the merging process causes blurry effects. On the\ncontrary, Gaussian weighted average obtains decent results so\nthat we utilize it to merge these patches to generate the output\nframes. We experimentally set the standard deviation of the\nGaussian kernel, which affects the sharpness of the merged\nimage, to 4 in our method to obtain the best performance.\nMore specifically, the value of a pixel in the synthesized image\nis computed as:\nPixy =\nP\npxy>0 wpxypxy\nP\npxy>0 wpxy\n,\n(6)\nwhere i = 1, . . . , T denotes the index of a frame, pxy means\nthe value of the pixel (x,y) in the patch p, and wpxy denotes\nthe pixel’s Gaussian weights and factors, which is defined as:\nwpxy =\n1\n2πσ2 e− (x−m)2+(y−n)2\n2σ2\n,\n(7)\nwhere σ is the standard deviation of the Gaussian kernel and\n(m,n) denotes the center of the patch p.\nIV. EXPERIMENTS\nOur approach is initially designed for dynamic text effects\ntransfer, but it can also be applied to other texture transfer\napplications. In this section, we first evaluate the effectiveness\nof our approach in handling the task of dynamic text effects\ntransfer with various font styles and several representative\ntypes of glyphs (e.g., English and Chinese characters), and\nillustrate its superiority over state-of-the-art methods. Then,\nwe further demonstrate the capability of our method in other\napplications such as image animation.\nA. Dynamic text effects transfer\nWe apply our dynamic texture transfer approach to transfer\nthe dynamic text effects of many stylized examples to the\nglyph images of English and Chinese characters. We demon-\nstrate that various complicated dynamic effects such as burning\nflame, flowing water, and others can be accurately transferred\nby our approach. Some results are shown in Fig. 5 and 6, and\nmore can be found in the supplemental material.\nB. Comparison\n1) Qualitative comparison: In this subsection, we compare\nour method with other existing approaches [17] [21] [1] [9]\nfor dynamic typography generation.\nThe synthesis results of some representative frames are\ndepicted in Fig. 11. Flow-guided synthesis [21] suffers from\nsevere texture distortions and causes ghosting artifacts due to\nthe error accumulation of flows.\n[1] fails to produce stable\ndynamic effects and synthesizes videos that are temporally\ninconsistent.\n[17] suffers from temporal inconsistency and\ndistinctive artifacts. Distorted textures are inevitably generated\nby DynTypo [9] in some regions due to the side effects brought\nby the common NNF computed on keyframes. Our method\nsynthesizes more natural and visually-pleasing results with the\nSource\nTarget\nFig. 5. Our results on flame dynamic effects transfer.\nSource\nTarget\nFig. 6. Our results on water dynamic effects transfer.\ndesired text effects while keeping both spatial and temporal\nconsistency.\nMoreover, Fig. 7 compares the performance of our method\nand DynTypo [9] on transferring dynamic and moving flame\neffects. When the text contains motions and no longer aligns\nwith the original semantic map, the video synthesized by\nDynTypo contains significant artifacts and inconsistency be-\ntween frames while our method still provides decent results,\nindicating its superiority and versatility.\nAs shown in Fig 8, we provide a detailed comparison for\nthe generated texture quality of our method with other texture\ntransfer methods including Pix2Pix [22] (we cut patches to\ntrain Pix2Pix for the one-shot learning task), SinGAN [4] (we\nchange its random variable inputs to semantic map as structure\nguidance), and DynTypo [9]. Fig 9 shows an extensive com-\nparison between our method and DynTypo [9]. Our synthesis\nresults are with the best texture quality and the most natural\nstructure.\n2) Quantitative comparison: Quantitatively evaluating the\nresults of one-shot learning tasks with no ground truth avail-\nable is always tough. The similarity metrics, like SSIM,\nare invalid without ground truth and the perceptual quality\nmetrics, like FID, are also unsuited for one-shot learning tasks\nincluding our work, since the data quantity is far too small to\nobtain reasonable feature distribution.\nDue to the fact that our method is implemented based\non patches, we found the LPIPS (Learned Perceptual Image\nPatch Similarity) metric suitable to measure the quality of\nsynthesized videos. Hence, we transfer the flame dynamic\ntexture to several representative types of glyphs (e.g., English\nand Chinese characters), 30 generated samples in total, and\n6\nsource\ntarget\n[Men et al. \n2019]\nOurs\nFig. 7. Comparison on moving flame dynamic effects. The exemplar moves\nto lower right in the driving video.\nFig. 8. Texture transfer quality comparison.\nSource\nTarget\n[Men et al. \n2019]\nOurs\nOurs\n[Men et al. \n2019]\nFig. 9. Comparison on flame dynamic effects with Dyntypo. Our DynTexture\nachieves better spatial consistency of the dynamic effect as well as more\nnatural flame shapes.\ncalculate the LPIPS score between the single source video\nframes and the generated results of [17], [9], and ours. As\ndemonstrated in Table I, consistent with our visually-pleasing\nresults shown in the paper, our method obtain the best LPIPS\nscore. Moreover, we also conduct a user study where each\nparticipant is asked to select the one that is more visually-\npleasing from synthesis results generated by our method and\nTABLE I\nQUANTITATIVE COMPARISON OF DIFFERENT METHODS.\nModel\nLPIPS↓\nUser Preference↑\nFiser et el. 2017 [17]\n0.51\n-\nMen et al. 2019 [9]\n0.095\n20.74%\nOurs\n0.085\n79.26%\nFig. 10. Animation results: cartoons (the first two rows), portraits (the last\ntwo rows). Controllable semantic maps serve as structure guidance.\n(Men et al. 2019) [9]. Our experimental results show that, with\n64 participants, our method obtains the best preference rate of\n79.26%.\nC. Other potential applications\nAlthough the proposed DynTexture is initially designed\nfor dynamic text effects transfer, it does not involve special\nconfigurations for typography inputs. Consequently, the pro-\nposed patch-cutting and prediction scheme is applicable to\nvarious kinds of dynamic effects. For instance, taking semantic\nmaps as the structure guidance, our approach can modify\nthe layout of a source image and animate it according to\ndriving videos. In Fig. 10, We demonstrate two examples for\nlayout modification and animation of cartoons and portraits,\nrespectively. In addition, since the proposed deep sequence\nforecasting module works independently, it can be employed\nto handle the animation task solely.\nV. EXPERIMENTAL SETTINGS AND RUNTIME\nWe provide details of our experimental settings. Specifically,\nour patch quantization module is adapted from VQ-VAE. In\norder to efficiently model small patches, both the encoder and\ndecoder consist of 3 residual convolution blocks, respectively,\nthe number of patch indices is set to 16, the codebook size\nis set to 256. For subsequent sequence prediction, we used\nminiGPT with 6 layers and the number of multi-heads for\n7\nFig. 11. Comparison on flame dynamic effects generated by different meth-\nods. Our method synthesizes more natural and visually-pleasing results with\nthe desired text effects while keeping both spatial and temporal consistency.\nTABLE II\nPARAMETER STUDY OF TRANSFORMER SETTINGS. ACCURACY SCORES\nARE THE PERCENT RATIO OF THE CORRECTLY PREDICTED PATCHES ON\nTHE VALIDATION SET OF TRAINING DATA. THE GPU COSTS ARE\nMEASURED UNDER THE BATCH SIZE OF 32 (PATCHES). THE RUNTIMES\nARE MEASURED BY SECONDS PER EPOCH DURING TRAINING. WE ADOPT\nTHE SETTING OF 6 TRANSFORMER LAYERS WITH 8 ATTENTION HEADS\nFOR LESS GPU AND RUNTIME COST WITH HIGH ACCURACY.\nLayers/Heads\nAccuracy↑\nGPU cost↓\nruntime↓\n3/4\n89.8%\n1740 Mib\n257\n6/8\n98.6%\n3791 Mib\n514\n12/16\n99.2%\n9533 Mib\n1103\nself-attention in each layer is set to 8 (Table II shows the\nchoice of the size of the Transformer). The vocabulary size is\ncoherent with the codebook size of VQ-VAE, which is set to\n256 in our case. We trained our model with a batch size of\n32, using the Adam optimizer whose learning rate is selected\nas 2.5e-6. The training process takes around 36 hours using\ntwo Nvidia GTX2080ti GPUs and the testing takes around\n30 minutes per video both depending on the image resolution\n(patch quantity).\nVI. CONCLUSION\nIn this paper, we presented a novel model that is able\nto achieve automatic dynamic texture transfer utilizing both\nPatchMatch and Transformers. To the best of our knowledge,\nthe proposed method is the first neural-based approach to\ntackle the one-shot dynamic texture transfer problem. Exper-\nimental results demonstrated the effectiveness and superiority\nof our method over the state of the art, synthesizing visually-\npleasing results for complicated dynamic effects transfer and\nmoving dynamic effects transfer which cannot be properly\nhandled by existing approaches. In addition, the proposed\npatch-cutting and prediction scheme is also applicable to other\ntypes of dynamic effects, showing great potentials in motivat-\ning researchers working on the one-shot video generation task,\nas well as inspiring future works in video style transfer and\nimage animation.\nACKNOWLEDGMENTS\nThis work was supported by Beijing Nova Program of Sci-\nence and Technology (Grant No.: Z191100001119077), Center\nFor Chinese Font Design and Research, and Key Laboratory\nof Science, Technology and Standard in Press Industry (Key\nLaboratory of Intelligent Press Media Technology)\nREFERENCES\n[1] S. Yang, J. Liu, Z. Lian, and Z. Guo, “Awesome typography: Statistics-\nbased text effects transfer,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2017, pp. 7464–7473.\n[2] Y. Men, Z. Lian, Y. Tang, and J. Xiao, “A common framework for\ninteractive texture transfer,” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition, 2018, pp. 6353–6362.\n[3] S. Yang, J. Liu, W. Yang, and Z. Guo, “Context-aware text-based\nbinary image stylization and synthesis,” IEEE Transactions on Image\nProcessing, vol. 28, no. 2, pp. 952–964, 2018.\n[4] T. R. Shaham, T. Dekel, and T. Michaeli, “Singan: Learning a generative\nmodel from a single natural image,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2019, pp. 4570–4580.\n[5] J. Xie, S.-C. Zhu, and Y. Nian Wu, “Synthesizing dynamic patterns\nby spatial-temporal generative convnet,” in Proceedings of the ieee\nconference on computer vision and pattern recognition, 2017, pp. 7093–\n7101.\n[6] M. Tesfaldet, M. A. Brubaker, and K. G. Derpanis, “Two-stream\nconvolutional networks for dynamic texture synthesis,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition,\n2018, pp. 6703–6712.\n[7] A. Siarohin, S. Lathuili`ere, S. Tulyakov, E. Ricci, and N. Sebe, “First\norder motion model for image animation,” Advances in Neural Informa-\ntion Processing Systems, vol. 32, pp. 7137–7147, 2019.\n[8] C. Thomas, Y. Song, and A. Kovashka, “Learning to transfer visual\neffects from videos to images,” arXiv preprint arXiv:2012.01642, 2020.\n[9] Y. Men, Z. Lian, Y. Tang, and J. Xiao, “Dyntypo: Example-based dy-\nnamic text effects transfer,” in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2019, pp. 5870–5879.\n[10] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, “Patch-\nmatch: A randomized correspondence algorithm for structural image\nediting,” ACM Trans. Graph., vol. 28, no. 3, p. 24, 2009.\n[11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint\narXiv:1706.03762, 2017.\n[12] S. Yang, Z. Wang, Z. Wang, N. Xu, J. Liu, and Z. Guo, “Controllable\nartistic text style transfer via shape-matching gan,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2019, pp.\n4442–4451.\n[13] C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkelstein, “The\ngeneralized patchmatch correspondence algorithm,” in European Con-\nference on Computer Vision.\nSpringer, 2010, pp. 29–43.\n8\n[14] S. Yang, J. Liu, W. Wang, and Z. Guo, “Tet-gan: Text effects transfer via\nstylization and destylization,” in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 33, no. 01, 2019, pp. 1238–1245.\n[15] K. S. Bhat, S. M. Seitz, J. K. Hodgins, and P. K. Khosla, “Flow-based\nvideo synthesis and editing,” in ACM SIGGRAPH 2004 Papers, 2004,\npp. 360–363.\n[16] P. B´enard, F. Cole, M. Kass, I. Mordatch, J. Hegarty, M. S. Senn,\nK. Fleischer, D. Pesare, and K. Breeden, “Stylizing animation by\nexample,” ACM Transactions on Graphics (TOG), vol. 32, no. 4, pp.\n1–12, 2013.\n[17] J. Fiˇser, O. Jamriˇska, D. Simons, E. Shechtman, J. Lu, P. Asente,\nM. Luk´aˇc, and D. S`ykora, “Example-based synthesis of stylized facial\nanimations,” ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp.\n1–11, 2017.\n[18] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-\nresolution image synthesis,” arXiv preprint arXiv:2012.09841, 2020.\n[19] A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu, “Neural discrete\nrepresentation learning,” arXiv preprint arXiv:1711.00937, 2017.\n[20] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, “Videogpt: Video gener-\nation using vq-vae and transformers,” arXiv preprint arXiv:2104.10157,\n2021.\n[21] V. Kwatra, I. Essa, A. Bobick, and N. Kwatra, “Texture optimization\nfor example-based synthesis,” in ACM SIGGRAPH 2005 Papers, 2005,\npp. 795–802.\n[22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation\nwith conditional adversarial networks,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR), July\n2017.\nGuo Pu received the master’s degree from Peking\nUniversity, China, in 2022. He is currently a Ph.D\ncandidate in Peking University, China, working on\nhis Ph.D thesis at the Wangxuan Institute of Com-\nputer Technology, Peking University, China. His\nresearch interests include computer vision, image\nsynthesis and texture transfer. He has published\npapers in related journals and conferences such as\nTPAMI, SIGGRAPH, CVPR, etc.\nShiyao Xu currently a third-year master student at\nWangxuan Institute of Computer Technology, Peking\nUniversity, China. Her research interests lie in build-\ning the bridge between 2D and 3D, specially about\nimage generation and neural rendering.\nXixin Cao received the Ph.D. degree in computer\nscience in 2000. He is a professor affiliated with\nSchool of Software and Microelectronics, Peking\nUniversity, Beijing, China. His research interests\ninclude digital image processing, computer vision\nand digital multimedia SOC design. He has pub-\nlished more than 50 papers in significant journals\nand conferences.\nZhouhui Lian received the Ph.D. degree from\nBeihang University, China, in 2011. He worked\nas a guest researcher at NIST, Gaithersburg, USA,\nfrom 2009 to 2011. He is currently an associate\nprofessor at the Wangxuan Institute of Computer\nTechnology, Peking University, China. His main re-\nsearch interests include computer graphics, computer\nvision, and AI. He has published more than 70\npapers in prestigious journals and conferences such\nas TOG, TPAMI, IJCV, SIGGRAPH/SIGGRAPH\nAsia, CVPR, etc.\n"
}