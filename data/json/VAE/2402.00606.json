{
    "optim": "1 Dynamic Texture Transfer using PatchMatch and Transformers Guo Pu, Shiyao Xu, Xixin Cao, and Zhouhui Lian Member, IEEE, Abstract—How to automatically transfer the dynamic texture of a given video to the target still image is a challenging and ongoing problem. In this paper, we propose to handle this task via a simple yet effective model that utilizes both PatchMatch and Transformers. The key idea is to decompose the task of dynamic texture transfer into two stages, where the start frame of the target video with the desired dynamic texture is synthesized in the first stage via a distance map guided texture transfer module based on the PatchMatch algorithm. Then, in the second stage, the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers equipped with VQ-VAE for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. Experimental results demonstrate the effectiveness and superiority of the proposed method in dynamic texture transfer compared to the state of the art. Index Terms—Texture Transfer, video synthesis, image gener- ation. I. INTRODUCTION N OWADAYS, with the rapid development of computer and network technologies, dynamic textures have become popular in a variety of media, such as films, digital posters, advertisements, and online chatting, making contents more appealing and varied. However, designing a set of dynamic special effects takes numerous times and efforts even for skilled designers. Taking artistic dynamic texts as an example, font designers need to design the special text effect frame by frame, and apply this dynamic effect to different characters in the entire font library, which is extremely labor-intensive and time-consuming. Hence, the aim of this paper is to automatically transfer the dynamic effect of a given video sample to a target shape or image, significantly improving the designing efficiency of dynamic textures. The above-mentioned process is called dynamic texture transfer. Compared with static texture transfer, both the in- ternal spatial information of the image and the temporal information across all frames need to be considered. What is more, the flame flickering and shaking problems that appear in the video synthesis process also need to be avoided. Most current texture transfer methods [1], [2], [3], [4] aim at handling static images which do not apply temporal Guo Pu, Shiyao Xu and Zhouhui Lian are with Wangxuan Institute of Computer Technology, Peking University, Beijing 100871, China. Xixin Cao is with School of Software and Microelectronics, Peking University, Beijing 100871, China Corresponding Author: Zhouhui Lian (email: lianzhouhui@pku.edu.cn) Transfer !!! Fig. 1. An example of dynamic texture transfer. Given a sample video and a target still image, the proposed method is able to synthesize the target video by transferring the dynamic texture of the sample video into the target image. constraints thus fail to satisfactorily address the dynamic tex- ture transfer problem. Moreover, existing neural-based meth- ods [5], [6], [7], [8] typically require huge amounts of data to train their models which are incapable of handling the one-shot effects transfer task. Furthermore, these neural-based models are required to resize all images to a certain aligned scale during training which causes detail losing of delicate textures, while our method works in a full-resolution fashion and thus is capable of transferring complex textures on their original resolution. The most relevant work to our paper is [9], where Men et al. [9] proposed to stylize a still semantic map to the target video. However, since a common Nearest- neighbor Field (NNF) is adopted to guide the texture synthesis procedure across all keyframes simultaneously, their method fails to apply to complex dynamic effects (e.g. with motions). In addition, the correlation of video frames is not properly exploited and thus there still exist some incoherent regions in their synthesis results. To solve the above issues, we propose DynTexture, a neural- based approach to automatically transfer various dynamic effects. As shown in Fig. 1, our method requires only a source texture video and a target semantic map as input, then it can automatically generate the target video with the same texture effect as the source video. To the best of our knowledge, the proposed method is the first neural-based approach to tackle this one-shot dynamic texture transfer problem. To animate the binary semantic image, we first introduce an attention term called distance map, which can help to better preserve the structural informa- tion. With the guidance of distance maps, the offset and the correspondence between inputs are calculated and the initial target frame can then be synthesized via the PatchMatch [10] algorithm. Afterwards, we cut the start frame into structure- agnostic patches and calculate their embeddings to make better use of the texture continuity and the patch distributions inside the image. In the second stage, we use the Transformer [11] arXiv:2402.00606v1  [cs.CV]  1 Feb 2024 2 model to predict the discretized code sequences, which is capable of capturing the long-distance dependencies between frames. In summary, our major contributions are threefold: • We obtain impressive results in the dynamic text effects transfer task with a new texture transfer and animation scheme that consists of a distance map guided texture transfer module and a novel deep sequence forecasting module. Extensive experiments demonstrate our method’s superiority over state-of-the-art methods and versatility in various texture transfer and animation tasks. • We tackle the challenge of insufficient data when training the deep sequence forecasting module through over- lapping patch splitting and merging data augmentation, enabling the model to effectively resolve the one-shot dynamic texture transfer problem. • The independence of the texture rendering module and the sequence forecasting module makes our method capa- ble of synthesizing more complex dynamic effects (e.g., with motions) that cannot be properly handled by existing state-of-the-art approaches (e.g., [9]). II. RELATED WORK Dynamic texture transfer has been a challenging task. At present, there exist very few works that try to solve this problem. In this section, we briefly survey the texture transfer and animating literature and summarize the current state of the art. The following highlights the techniques most closely related to ours. The still texture effects transfer task has been well resolved by existing approaches (e.g., [1], [2], [3], [12]) that take the input semantic map as structure guidance to learn the correspondence between the source and target images. Yang et al. [1] proposed to transfer the text effects by analyzing the high regularity of the spatial distribution for target effects. Actually their work can be seen as the extension of [10], [13]. They performed transferring by calculating the similarity attractively between patches to find the best matching. As for deep-learning-based methods, Yang et al. [14] designed a GAN-based network to accomplish both the objective of style transfer and style removal so that it can learn to disentangle and recombine the texture of content. However, like most deep learning models, their work requires a lot of paired data and is unsuited to handle the one-shot texture transfer task in our case. Shaham et al. [4] trained the model only on one single image to capture the internal distribution of patches, using a pyramid of fully convolutional GANs, to capture details at different scales. However, when applying the method [4] on complex cases, for example, the burning flame with dynamic texture effects, the generated results are blurry and can hardly preserve the desired structure. Since temporal consistency is not considered, the above-mentioned methods fail to handle the task of dynamic effects transfer. To take temporal constraints into consideration, dynamic effects transfer methods like [15] perform dynamic effects transfer by analyzing the motion of textured particles in the video while the method is not a fully automatic method PatchMatch Initial Source  Frame Generated Initial  Target Frame Calculate Distance Fig. 2. Utilizing distance information to guide the PatchMatch algorithm, letting the flow of information outward from the boundary. due to the requirement of user-specified flows. Tesfaldet et al. [6] utilized a pre-trained convolutional network to model the dynamics of per-frame appearances of textures to generate target textures. Xie et al. [5] defined a probability distribution over video sequences and introduced the temporal constraint through multi-scale model layers to capture spatial-temporal patterns. However, these methods require plenty of training data and struggle to capture the details of textures, thus failing to deal with the one-shot texture transfer task. Most image animation approaches [16], [17], [9], [8], [7] either require large amounts of videos for training, failing to work in the one-shot learning fashion, or have strong restrictions for the input objects such as human poses, thus cannot be applied to synthesize dynamic textures. Recently, Men et al. [9] extended the NNF search of PatchMatch [10] to the spatial-temporal domain and obtained impressive results in dynamic text effects transfer. They re- stricted the procedure of patch searching within the extracted keyframes to maintain both spatial and temporal consistencies, in the meantime losing inter-frame information which causes incoherent structures in their synthesis results. Moreover, their method only searches for a single global NNF, and thus fails to process dynamic textures with moving samples which shift across source videos. Transformer-based methods are emerging in Computer Vi- sion tasks, demonstrating their inherent advantages in captur- ing relationships and dependencies between sequences. Esser et al. [18] introduced the convolutional VQ-VAE [19] to learn compressed discrete representations for images and then modeled learnt embeddings with Transformers to generate controllable realistic images. Yan et al. [20] used a simple GPT-like architecture to model the discrete latent codes learnt from videos by VQ-VAE [19] to generate new videos. Con- strained by the compressing ability of VQ-VAE, their results are limited to a very low resolution. III. METHOD DESCRIPTION In this section, we first formulate the task of dynamic texture transfer (Section 3.1) and then present the detailed method description of our approach (Section 3.2 and 3.3), which is named as DynTexture (Dynamic Texture transfer) for short. Fig. 3 demonstrates an overview of our proposed DynTex- ture. The key idea is to decompose the one-shot dynamic 3 Transformer Decoder PatchMatch Initial Source Frame Generated Initial Frame Source Frames Predicted Frames Source Text and  Distance Map Target Text and  Distance Map Codebook Patch  Cutting Gaussian Patch  Merging Encoder Predicted Latents Discrete Latents Fig. 3. Overview of the proposed DynTexture, which is designed as a two-stage architecture, where the distance map guided texture rendering module generates the initial frame, and the novel deep sequence forecasting module predicts and synthesizes the subsequent frames based on the previously-synthesized initial frame. texture transfer task into two stages where the start frame of the target video with the desired dynamic texture is synthesized in the first stage (Section 3.2) via a distance map guided tex- ture transfer module based on the PatchMatch algorithm [10]. Then, in the second stage (Section 3.3), the synthesized image is decomposed into structure-agnostic patches, according to which their corresponding subsequent patches can be predicted by exploiting the powerful capability of Transformers [11] equipped with VQ-VAE [19] for processing long discrete sequences. After getting all those patches, we apply a Gaussian weighted average merging strategy to smoothly assemble them into each frame of the target stylized video. A. Problem formulation and analysis The proposed DynTexture can be used to handle various types of dynamic texture transferring tasks. For the sake of convenience, we formulate the task of dynamic texture transfer under the scenario of dynamic text effects transfer. More specifically, given a source text image Stext, its styl- ized animation Sstyle and a target image Ttext, the goal of dynamic text effects transfer is to synthesize the target stylized animation Tstyle such that Stext:Sstyle::Ttext:Tstyle, as shown in Fig. 1. We decompose the task of dynamic text effects transfer into two sub-problems: still image texture transfer and dynamic effects animation. For the first sub- problem, PatchMatch-based methods like [1] have achieved impressive results for texture transfer in still images. We tackle the still image texture transfer based on PatchMatch as well. As for how to transfer the dynamic effects to the still target image, we take advantage of the Transformers’ long sequence prediction ability, decomposing the image into large amounts of structure-agnostic patches and training Transformers to learn the correlation between frames. B. Initial frame synthesis We use the distance map guided PatchMatch [10] algorithm to search an NNF for the initial frame and generate the initial frame with the desired texture effect. For still text effects transfer, the target stylized image is synthesized with the semantic guidance Ttext. Obviously, the patches in the text contour contain more features and could easily find proper correspondences from the source. By exploiting the preserving and propagating features of the PatchMatch algorithm, we can find the optimal matches by enforcing texture continuity within and near the contour. This guarantees the spatial consistency of the initial frame. Unfortunately, the naive utilization of PatchMatch [10] in the application of text effects transfer fails to transfer com- plicated textures without semantic guidance outside the text (see the second row of Fig. 11). Inspired by [9], we calculate a distance map as the propagation guidance to enable the texture effect to spread around the text contour. Following [9], the distance is calculated as the normalized distance between each pixel of Ttext and the contour of text. The initial frame synthesis process is illustrated in Fig. 2. C. Subsequent frame prediction 1) Patch cutting: The source exemplar contains both source structure information and source texture effects where we only want the model to learn the texture information and ignore the other. To achieve that, we cut overlapping patches for every frame in the source video and the target image, and process them independently in later procedures. The purpose of patch cutting is to decompose the input image into structure-agnostic patches. We can also consider this as a way of data augmentation as it produces huge amounts of training data for the model. The patches are overlapped since we want to make the model be aware of the continuity between its neighbours and encourage the model to generate smooth results. Consequently, a smaller patch cutting stride leads to better results and the stride is set to 1 in our method. The choice of patch size is crucial for patch-based synthesis methods like the proposed DynTexture. A smaller patch size makes the patches less discriminative and causes a large portion of repetitive patterns or even misplaced patterns. On 4 the contrary, a large patch size tends to contain too much information of the source structure and thus affects the target structure of the synthesized video, which is unsatisfactory. We experimentally choose the patch size as 16 by 16 which results in a good trade-off between better patch discrimination and less structural damage. 2) Compressing patches by VQ-VAE: Using Transformers to predict long subsequent patch sequences requires us to learn a rich codebook to effectively compress patches. VQ-VAE proposed in [19] is able to reconstruct sufficiently realistic images by representing them with its codebook entry codes. Thereby, we employ VQ-VAE to learn the texture patches from the source exemplar and compress the patches into compact latent codes (i.e., code-book entry indices) for further processing. To make it adaptive for the small patch input, we slightly modify the original VQ-VAE model to a small-scale version, which is trained by computing the reconstruction loss, the codebook loss, and the commitment loss: LV QV AE = ||x − D(e)||2 2 + ||sg[E(x)] − e||2 2+ β||sg[e] − E(x)||2 2, (1) where E and D are the encoder and the decoder of VQ-VAE, respectively, x is the input patch, e is the latent embedding of x, β is the coefficient of the commitment loss, and sg denotes the stop-gradient operation. The objective loss func- tion LV QV AE consists of three terms where the first term is the reconstruction loss that supervises VQ-VAE to learn good representations to reconstruct the patches. The second term and the third term are the codebook loss and the commitment loss, respectively, guiding the model to learn a good codebook. The encoder and the decoder of VQ-VAE are trained on the patches from source frames, thus the patches can be rep- resented in terms of the codebook indices of their encodings. Specifically, given a patch x, we denote the codebook indices of x as Ix. Elements in Ix are the nearest entries among all elements in the codebook with respect to Zq, and can be formerly calculated as: Ixij = arg min θ∈Θ Dist(Zqij, Zθ), (2) Dist(Zqij, Zθ) = \r\rZqij − Zθ \r\r 2 , (3) where Zq is the quantized encoding of x, Zθ is the θ-th element in the codebook, Θ denotes the size of codebook, Dist denotes the distance calculation function, and i and j denote the indices of element locations. Notice that the patch x can be easily recovered by firstly mapping Ix to Zq, and then decoding Zq via the decoder. We will recover the predicted subsequent patch index sequence in the same manner. Utilizing VQ-VAE to encode the patches into latent codes brings two benefits: First, VQ-VAE learns the texture informa- tion from structure-agnostic patches and represents the patches as high-quality latent codes, and it is able to generate new patches through manipulations on the latent codes. Second, VQ-VAE achieves a high compressing rate by converting patch images to latent codes, which markedly reduces the prediction burden of the deep sequence forecasting module, and thus Fig. 4. Comparison between simple average and Gaussian weighted average merging strategies. Gaussian weighted average obviously obtains higher- quality results. enables long sequence prediction with limited computing and memory resources. 3) Subsequent sequence prediction via Transformers: Transformers [11] have shown tremendous successes in mod- eling discrete data such as natural languages. The architecture of Transformers employs multi-head self-attention blocks fol- lowed by point-wise MLP feed-forward blocks. It is partic- ularly suitable for the generation of long discrete sequences which coincides with our desire for predicting the discrete latent codes of subsequent sequences. After encoding the initial frame patches into discrete latent codes, we utilize Transformer to predict the subsequent latent code sequences. Specifically, the indices of initial frame patches are fed into the Transformer model and then the predicted indices of the subsequent patch sequence are outputted. During the training stage, we concatenate the patch indices of the subsequent se- quence into a long sequence as the target sequence. Formally, the Transformer model used here aims to fit the following function: P(I0, I1, ..., IF |I0) = F Y i=1 P(Ii|I0), (4) where P denotes the probability distribution, Ii denotes the patch index of the i-th frame, and F denotes the total number of frames. The cross-entropy loss is used to train the model: LT ransformer = − X p(Ipred) log q(IGT ), (5) where Ipred and IGT are the predicted and ground-truth patch indices, respectively, whose distributions are denoted by p(Ipred) and q(IGT ), respectively. Once our model has learnt the correspondence between the indices of the initial frame and those of the subsequent frames, it will ensure the temporal consistency of the decoded video. As a result, each synthesized image patch will imitate the appearance variance learnt from the source patches while acquiring motion properties implicitly. 4) Patch decoding and merging: After getting the predicted patch indices in the subsequent sequence, we use the trained decoder of VQ-VAE to decode the predicted patch indices and assign them to the corresponding frames. 5 Since there exist overlapping regions between adjacent patches, a good patch merging strategy is crucial for patch- based image synthesis. As shown in Fig. 4, applying simple average to the merging process causes blurry effects. On the contrary, Gaussian weighted average obtains decent results so that we utilize it to merge these patches to generate the output frames. We experimentally set the standard deviation of the Gaussian kernel, which affects the sharpness of the merged image, to 4 in our method to obtain the best performance. More specifically, the value of a pixel in the synthesized image is computed as: Pixy = P pxy>0 wpxypxy P pxy>0 wpxy , (6) where i = 1, . . . , T denotes the index of a frame, pxy means the value of the pixel (x,y) in the patch p, and wpxy denotes the pixel’s Gaussian weights and factors, which is defined as: wpxy = 1 2πσ2 e− (x−m)2+(y−n)2 2σ2 , (7) where σ is the standard deviation of the Gaussian kernel and (m,n) denotes the center of the patch p. IV. EXPERIMENTS Our approach is initially designed for dynamic text effects transfer, but it can also be applied to other texture transfer applications. In this section, we first evaluate the effectiveness of our approach in handling the task of dynamic text effects transfer with various font styles and several representative types of glyphs (e.g., English and Chinese characters), and illustrate its superiority over state-of-the-art methods. Then, we further demonstrate the capability of our method in other applications such as image animation. A. Dynamic text effects transfer We apply our dynamic texture transfer approach to transfer the dynamic text effects of many stylized examples to the glyph images of English and Chinese characters. We demon- strate that various complicated dynamic effects such as burning flame, flowing water, and others can be accurately transferred by our approach. Some results are shown in Fig. 5 and 6, and more can be found in the supplemental material. B. Comparison 1) Qualitative comparison: In this subsection, we compare our method with other existing approaches [17] [21] [1] [9] for dynamic typography generation. The synthesis results of some representative frames are depicted in Fig. 11. Flow-guided synthesis [21] suffers from severe texture distortions and causes ghosting artifacts due to the error accumulation of flows. [1] fails to produce stable dynamic effects and synthesizes videos that are temporally inconsistent. [17] suffers from temporal inconsistency and distinctive artifacts. Distorted textures are inevitably generated by DynTypo [9] in some regions due to the side effects brought by the common NNF computed on keyframes. Our method synthesizes more natural and visually-pleasing results with the Source Target Fig. 5. Our results on flame dynamic effects transfer. Source Target Fig. 6. Our results on water dynamic effects transfer. desired text effects while keeping both spatial and temporal consistency. Moreover, Fig. 7 compares the performance of our method and DynTypo [9] on transferring dynamic and moving flame effects. When the text contains motions and no longer aligns with the original semantic map, the video synthesized by DynTypo contains significant artifacts and inconsistency be- tween frames while our method still provides decent results, indicating its superiority and versatility. As shown in Fig 8, we provide a detailed comparison for the generated texture quality of our method with other texture transfer methods including Pix2Pix [22] (we cut patches to train Pix2Pix for the one-shot learning task), SinGAN [4] (we change its random variable inputs to semantic map as structure guidance), and DynTypo [9]. Fig 9 shows an extensive com- parison between our method and DynTypo [9]. Our synthesis results are with the best texture quality and the most natural structure. 2) Quantitative comparison: Quantitatively evaluating the results of one-shot learning tasks with no ground truth avail- able is always tough. The similarity metrics, like SSIM, are invalid without ground truth and the perceptual quality metrics, like FID, are also unsuited for one-shot learning tasks including our work, since the data quantity is far too small to obtain reasonable feature distribution. Due to the fact that our method is implemented based on patches, we found the LPIPS (Learned Perceptual Image Patch Similarity) metric suitable to measure the quality of synthesized videos. Hence, we transfer the flame dynamic texture to several representative types of glyphs (e.g., English and Chinese characters), 30 generated samples in total, and 6 source target [Men et al.  2019] Ours Fig. 7. Comparison on moving flame dynamic effects. The exemplar moves to lower right in the driving video. Fig. 8. Texture transfer quality comparison. Source Target [Men et al.  2019] Ours Ours [Men et al.  2019] Fig. 9. Comparison on flame dynamic effects with Dyntypo. Our DynTexture achieves better spatial consistency of the dynamic effect as well as more natural flame shapes. calculate the LPIPS score between the single source video frames and the generated results of [17], [9], and ours. As demonstrated in Table I, consistent with our visually-pleasing results shown in the paper, our method obtain the best LPIPS score. Moreover, we also conduct a user study where each participant is asked to select the one that is more visually- pleasing from synthesis results generated by our method and TABLE I QUANTITATIVE COMPARISON OF DIFFERENT METHODS. Model LPIPS↓ User Preference↑ Fiser et el. 2017 [17] 0.51 - Men et al. 2019 [9] 0.095 20.74% Ours 0.085 79.26% Fig. 10. Animation results: cartoons (the first two rows), portraits (the last two rows). Controllable semantic maps serve as structure guidance. (Men et al. 2019) [9]. Our experimental results show that, with 64 participants, our method obtains the best preference rate of 79.26%. C. Other potential applications Although the proposed DynTexture is initially designed for dynamic text effects transfer, it does not involve special configurations for typography inputs. Consequently, the pro- posed patch-cutting and prediction scheme is applicable to various kinds of dynamic effects. For instance, taking semantic maps as the structure guidance, our approach can modify the layout of a source image and animate it according to driving videos. In Fig. 10, We demonstrate two examples for layout modification and animation of cartoons and portraits, respectively. In addition, since the proposed deep sequence forecasting module works independently, it can be employed to handle the animation task solely. V. EXPERIMENTAL SETTINGS AND RUNTIME We provide details of our experimental settings. Specifically, our patch quantization module is adapted from VQ-VAE. In order to efficiently model small patches, both the encoder and decoder consist of 3 residual convolution blocks, respectively, the number of patch indices is set to 16, the codebook size is set to 256. For subsequent sequence prediction, we used miniGPT with 6 layers and the number of multi-heads for 7 Fig. 11. Comparison on flame dynamic effects generated by different meth- ods. Our method synthesizes more natural and visually-pleasing results with the desired text effects while keeping both spatial and temporal consistency. TABLE II PARAMETER STUDY OF TRANSFORMER SETTINGS. ACCURACY SCORES ARE THE PERCENT RATIO OF THE CORRECTLY PREDICTED PATCHES ON THE VALIDATION SET OF TRAINING DATA. THE GPU COSTS ARE MEASURED UNDER THE BATCH SIZE OF 32 (PATCHES). THE RUNTIMES ARE MEASURED BY SECONDS PER EPOCH DURING TRAINING. WE ADOPT THE SETTING OF 6 TRANSFORMER LAYERS WITH 8 ATTENTION HEADS FOR LESS GPU AND RUNTIME COST WITH HIGH ACCURACY. Layers/Heads Accuracy↑ GPU cost↓ runtime↓ 3/4 89.8% 1740 Mib 257 6/8 98.6% 3791 Mib 514 12/16 99.2% 9533 Mib 1103 self-attention in each layer is set to 8 (Table II shows the choice of the size of the Transformer). The vocabulary size is coherent with the codebook size of VQ-VAE, which is set to 256 in our case. We trained our model with a batch size of 32, using the Adam optimizer whose learning rate is selected as 2.5e-6. The training process takes around 36 hours using two Nvidia GTX2080ti GPUs and the testing takes around 30 minutes per video both depending on the image resolution (patch quantity). VI. CONCLUSION In this paper, we presented a novel model that is able to achieve automatic dynamic texture transfer utilizing both PatchMatch and Transformers. To the best of our knowledge, the proposed method is the first neural-based approach to tackle the one-shot dynamic texture transfer problem. Exper- imental results demonstrated the effectiveness and superiority of our method over the state of the art, synthesizing visually- pleasing results for complicated dynamic effects transfer and moving dynamic effects transfer which cannot be properly handled by existing approaches. In addition, the proposed patch-cutting and prediction scheme is also applicable to other types of dynamic effects, showing great potentials in motivat- ing researchers working on the one-shot video generation task, as well as inspiring future works in video style transfer and image animation. ACKNOWLEDGMENTS This work was supported by Beijing Nova Program of Sci- ence and Technology (Grant No.: Z191100001119077), Center For Chinese Font Design and Research, and Key Laboratory of Science, Technology and Standard in Press Industry (Key Laboratory of Intelligent Press Media Technology) REFERENCES [1] S. Yang, J. Liu, Z. Lian, and Z. Guo, “Awesome typography: Statistics- based text effects transfer,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017, pp. 7464–7473. [2] Y. Men, Z. Lian, Y. Tang, and J. Xiao, “A common framework for interactive texture transfer,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018, pp. 6353–6362. [3] S. Yang, J. Liu, W. Yang, and Z. Guo, “Context-aware text-based binary image stylization and synthesis,” IEEE Transactions on Image Processing, vol. 28, no. 2, pp. 952–964, 2018. [4] T. R. Shaham, T. Dekel, and T. Michaeli, “Singan: Learning a generative model from a single natural image,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4570–4580. [5] J. Xie, S.-C. Zhu, and Y. Nian Wu, “Synthesizing dynamic patterns by spatial-temporal generative convnet,” in Proceedings of the ieee conference on computer vision and pattern recognition, 2017, pp. 7093– 7101. [6] M. Tesfaldet, M. A. Brubaker, and K. G. Derpanis, “Two-stream convolutional networks for dynamic texture synthesis,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 6703–6712. [7] A. Siarohin, S. Lathuili`ere, S. Tulyakov, E. Ricci, and N. Sebe, “First order motion model for image animation,” Advances in Neural Informa- tion Processing Systems, vol. 32, pp. 7137–7147, 2019. [8] C. Thomas, Y. Song, and A. Kovashka, “Learning to transfer visual effects from videos to images,” arXiv preprint arXiv:2012.01642, 2020. [9] Y. Men, Z. Lian, Y. Tang, and J. Xiao, “Dyntypo: Example-based dy- namic text effects transfer,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 5870–5879. [10] C. Barnes, E. Shechtman, A. Finkelstein, and D. B. Goldman, “Patch- match: A randomized correspondence algorithm for structural image editing,” ACM Trans. Graph., vol. 28, no. 3, p. 24, 2009. [11] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all you need,” arXiv preprint arXiv:1706.03762, 2017. [12] S. Yang, Z. Wang, Z. Wang, N. Xu, J. Liu, and Z. Guo, “Controllable artistic text style transfer via shape-matching gan,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019, pp. 4442–4451. [13] C. Barnes, E. Shechtman, D. B. Goldman, and A. Finkelstein, “The generalized patchmatch correspondence algorithm,” in European Con- ference on Computer Vision. Springer, 2010, pp. 29–43. 8 [14] S. Yang, J. Liu, W. Wang, and Z. Guo, “Tet-gan: Text effects transfer via stylization and destylization,” in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 1238–1245. [15] K. S. Bhat, S. M. Seitz, J. K. Hodgins, and P. K. Khosla, “Flow-based video synthesis and editing,” in ACM SIGGRAPH 2004 Papers, 2004, pp. 360–363. [16] P. B´enard, F. Cole, M. Kass, I. Mordatch, J. Hegarty, M. S. Senn, K. Fleischer, D. Pesare, and K. Breeden, “Stylizing animation by example,” ACM Transactions on Graphics (TOG), vol. 32, no. 4, pp. 1–12, 2013. [17] J. Fiˇser, O. Jamriˇska, D. Simons, E. Shechtman, J. Lu, P. Asente, M. Luk´aˇc, and D. S`ykora, “Example-based synthesis of stylized facial animations,” ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1–11, 2017. [18] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high- resolution image synthesis,” arXiv preprint arXiv:2012.09841, 2020. [19] A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu, “Neural discrete representation learning,” arXiv preprint arXiv:1711.00937, 2017. [20] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas, “Videogpt: Video gener- ation using vq-vae and transformers,” arXiv preprint arXiv:2104.10157, 2021. [21] V. Kwatra, I. Essa, A. Bobick, and N. Kwatra, “Texture optimization for example-based synthesis,” in ACM SIGGRAPH 2005 Papers, 2005, pp. 795–802. [22] P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros, “Image-to-image translation with conditional adversarial networks,” in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017. Guo Pu received the master’s degree from Peking University, China, in 2022. He is currently a Ph.D candidate in Peking University, China, working on his Ph.D thesis at the Wangxuan Institute of Com- puter Technology, Peking University, China. His research interests include computer vision, image synthesis and texture transfer. He has published papers in related journals and conferences such as TPAMI, SIGGRAPH, CVPR, etc. Shiyao Xu currently a third-year master student at Wangxuan Institute of Computer Technology, Peking University, China. Her research interests lie in build- ing the bridge between 2D and 3D, specially about image generation and neural rendering. Xixin Cao received the Ph.D. degree in computer science in 2000. He is a professor affiliated with School of Software and Microelectronics, Peking University, Beijing, China. His research interests include digital image processing, computer vision and digital multimedia SOC design. He has pub- lished more than 50 papers in significant journals and conferences. Zhouhui Lian received the Ph.D. degree from Beihang University, China, in 2011. He worked as a guest researcher at NIST, Gaithersburg, USA, from 2009 to 2011. He is currently an associate professor at the Wangxuan Institute of Computer Technology, Peking University, China. His main re- search interests include computer graphics, computer vision, and AI. He has published more than 70 papers in prestigious journals and conferences such as TOG, TPAMI, IJCV, SIGGRAPH/SIGGRAPH Asia, CVPR, etc. "
}