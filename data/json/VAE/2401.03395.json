{
    "optim": "International Journal of Computer Vision manuscript No.\n(will be inserted by the editor)\nDeep Learning-based Image and Video Inpainting: A Survey\nWeize Quan 1,2 · Jiaxi Chen 1,2 · Yanli Liu 3 · Dong-Ming Yan 1,2, \u0000 ·\nPeter Wonka 4\nReceived: date / Accepted: date\nAbstract Image and video inpainting is a classic prob-\nlem in computer vision and computer graphics, aim-\ning to fill in the plausible and realistic content in the\nmissing areas of images and videos. With the advance\nof deep learning, this problem has achieved significant\nprogress recently. The goal of this paper is to compre-\nhensively review the deep learning-based methods for\nimage and video inpainting. Specifically, we sort ex-\nisting methods into different categories from the per-\nspective of their high-level inpainting pipeline, present\ndifferent deep learning architectures, including CNN,\nVAE, GAN, diffusion models, etc., and summarize tech-\nniques for module design. We review the training objec-\ntives and the common benchmark datasets. We present\nevaluation metrics for low-level pixel and high-level per-\nceptional similarity, conduct a performance evaluation,\nand discuss the strengths and weaknesses of represen-\ntative inpainting methods. We also discuss related real-\nworld applications. Finally, we discuss open challenges\nand suggest potential future research directions.\nKeywords Image inpainting · Video inpainting · Deep\nlearning · Generation\n\u0000 D.-M. Yan\nyandongming@gmail.com\n1 MAIS\n&\nNLPR,\nInstitute\nof\nAutomation,\nChinese\nAcademy of Sciences, Beijing, China\n2 School of Artificial Intelligence, University of Chinese\nAcademy of Sciences, Beijing, China\n3 College\nof\nComputer\nScience,\nSichuan\nUniversity,\nChengdu, China\n4 Computer, Electrical and Mathematical Science and En-\ngineering Division, King Abdullah University of Science\nand Technology, Thuwal, Saudi Arabia\nFig. 1: Application examples of inpainting techniques:\nphoto restoration (top left: image from (Bertalmio\net\nal.,\n2000)),\ntext\nremoval\n(top\nright:\nimage\nfrom (Bertalmio et al., 2000)), undesired target re-\nmoval (bottom left: image from (Chen, 2018)), and face\nverification (bottom right: image from\n(Zhang et al.,\n2018c)).\n1 Introduction\nImage and video inpainting (Masnou and Morel, 1998;\nBertalmio et al., 2000) refers to the task of restoring\nmissing/occluded regions of a digital image or video\nwith plausible and natural content. Inpainting is an\nunderconstrained problem with multiple plausible so-\nlutions, especially if there are large missing regions. In-\npainting has many important applications in multiple\nfields, such as cultural relic restoration, virtual scene\nediting, digital forensics, and film and television pro-\nduction, etc. Fig. 1 shows some important applications\nof inpainting techniques. Video is composed of multiple\nimages exhibiting temporal coherence, therefore, video\ninpainting is closely related to image inpainting, where\nthe former often learns from or extends the latter. For\nthis reason, we simultaneously review image and video\narXiv:2401.03395v1  [cs.CV]  7 Jan 2024\n2\nWeize Quan 1,2 et al.\nFig. 2: The rough number of papers on image and video\ninpainting per year.\ninpainting in this survey, and the number of papers is\nshown in Fig. 2.\nEarly image inpainting methods mainly depend on\nlow-level features of corrupted images, including PDE-\nbased methods (Bertalmio et al., 2000; Ballester et al.,\n2001; Tschumperl´e and Deriche, 2005) and patch-based\nmethods (Efros and Leung, 1999; Barnes et al., 2009;\nDarabi et al., 2012; Huang et al., 2014; Herling and\nBroll, 2014; Guo et al., 2018). PDE-based approaches\nusually propagate the information from the boundary\nto create a smooth inpainting. It is possible to prop-\nagate edge information, but it is hard to inpaint tex-\ntures. Instead of only considering the boundary infor-\nmation, patch-based approaches recover the unknown\nregions by matching and duplicating similar patches of\nknown regions. For smaller areas, patch-based methods\ncan inpaint textures and also inpaint complete objects\nif similar objects are available in other image regions.\nHowever, these traditional methods have limited abil-\nity to generate new semantically plausible content, es-\npecially for large missing regions and missing regions\nthat are not similar to other image regions. A compre-\nhensive review on classical image inpainting methods\nis beyond our scope, and we refer readers to the sur-\nveys (Guillemot and Meur, 2014; Jam et al., 2021) for\nmore details.\nBy contrast, deep learning holds the promise to in-\npaint large regions and also inpaint new plausible con-\ntent that was learned from a larger set of images. In the\nbeginning convolutional neural networks (CNNs) and\ngenerative adversarial networks (GANs) were the most\npopular choices in the inpainting literature. CNNs are\na class of feed-forward neural networks that consist of\nconvolutional, activation, and down-/up-sampling lay-\ners. They learn a highly non-linear mapping from the in-\nput image to the output image. GANs are a type of gen-\nerative model consisting of a generator and a discrim-\ninator that estimates the data distribution through an\nadversarial process. Recently, more attention has been\npaid to the transformer architecture and generative dif-\nfusion models (Sohl-Dickstein et al., 2015; Ho et al.,\n2020). Transformers are a prevalent network architec-\nture based on parallel multi-head attention modules.\nCompared to the locality of CNNs, transformers have\na better ability for contextual understanding. Diffusion\nprobabilistic models are a type of latent variable model,\nwhich mainly contain the forward process, the reverse\nprocess, and the sampling procedure. Diffusion models\nlearn to reverse a stochastic process (i.e., diffusion pro-\ncess) that progressively destroys data via adding noise.\nThese deep learning-based image inpainting methods\ncan achieve attractive results that surpass traditional\nmethods in many aspects. From the perspective of the\nhigh-level inpainting pipeline, existing inpainting meth-\nods can be classified into three categories: a single-shot\nframework, a two-stage framework, and a progressive\nframework. Orthogonal to these main approaches, dif-\nferent technical methods can be observed in their real-\nization, including mask-aware design, attention mecha-\nnisms, multi-scale aggregation, transform domain, deep\nprior guidance, multi-task learning, structure represen-\ntations, loss functions, etc.\nCompared with images, video data has an additional\ntime dimension. Therefore, video inpainting not only\nfills in reasonable content in the missing regions for\neach frame but also aims to recover a temporally consis-\ntent solution. Because of this close relationship between\nimage inpainting and video inpainting, many technical\nideas used in image inpainting are often applied and ex-\ntended to solve video inpainting tasks. Traditional video\ninpainting methods are usually based on patch sam-\npling and synthesis (Wexler et al., 2007; Granados et al.,\n2012; Newson et al., 2014; Huang et al., 2016). These\nmethods have limited ability to synthesize consistent\ncontent and capture complex motion and are often com-\nputationally expensive. To address these shortcomings,\nmany deep learning-based methods have been proposed\nand achieved significant progress. There mainly exist\nfour research directions: 3D CNN-based methods, shift-\nbased methods, flow-guided methods, and attention-\nbased methods. The core idea of these methods is to\ntransfer information from neighboring frames to the\ntarget frame. 3D CNNs are the direct extension of 2D\nCNNs and work in an end-to-end manner. However,\nthey often suffer from spatial misalignment and high\ncomputational cost. Shift-based methods can address\nthese limitations to some extent, but within a limited\nDeep Learning-based Image and Video Inpainting: A Survey\n3\ntemporal window only. Flow-guided approaches can pro-\nduce higher resolution and temporally consistent results\nbut are vulnerable to imperfect optical flow completion\ndue to occlusion and complex motion. Attention-based\nmethods fuse known information from short and long\ndistances. Unfortunately, inaccurate attention score es-\ntimation often leads to blurry results.\nTo our knowledge, there are several papers that re-\nview the deep learning-based inpainting works in the\nliterature. Elharrouss et al. (2020) categorizes image\ninpainting methods into sequential-based, CNN-based,\nand GAN-based methods, and reviews related papers.\nTo improve on their work, we also discuss common\nmethodological approaches, loss functions, and evalua-\ntion metrics. We also add more discussion about further\nresearch directions and include newer work. Jam et al.\n(2021) reviews the traditional and deep learning-based\nimage inpainting methods. However, they paid much\nattention to the traditional methods but have signifi-\ncantly fewer deep learning-based works compared to our\nsurvey. Weng et al. (2022) reviews some GAN-based im-\nage inpainting methods, but is generally shorter. More-\nover, these existing surveys do not review the image\nand video inpainting simultaneously.\n2 Image Inpainting\nFor the restoration of missing regions in an image, the\nresults sometimes are not unique, especially for large\nmissing areas. Consequently, there mainly exists two\nlines of research in the literature: (1) deterministic im-\nage inpainting and (2) stochastic image inpainting. Given\na corrupted image, deterministic image inpainting meth-\nods only output an inpainted result while stochastic im-\nage inpainting methods can output multiple plausible\nresults with a random sampling process. Inspired by\nmulti-modal learning, some researchers have recently\nfocused on text-guided image inpainting by providing\nadditional information with text prompts.\n2.1 Deterministic Image Inpainting\nFrom the perspective of a high-level inpainting pipeline,\nexisting works for deterministic image inpainting usu-\nally adopt three types of frameworks: single-shot, two-\nstage, and progressive. The single-shot framework usu-\nally adopts a generator network with a corrupted im-\nage as input and an inpainted image as output; The\ntwo-stage framework mainly consists of two generators,\nwhere the first generator achieves a rough result and\nthen the second generator improves upon it; The pro-\nFig. 3: Representative pipeline of the single-shot in-\npainting framework. The generator takes as input the\nconcatenation of a binary mask and a corrupted image\nand outputs the completed image. Training objectives\nare used for training the generator.\ngressive framework applies one or more generators to\niteratively recover missing regions along the boundary.\n2.1.1 Single-shot framework\nMany existing inpainting methods adopt a single-shot\nframework, as shown in Fig. 3. It essentially learns a\nmapping from a corrupted image to a complete image.\nThe framework usually consists of generators and cor-\nresponding training objectives.\nGenerators. To improve the inpainting ability of\nthe generator, there exist several lines of research: mask-\naware design, attention mechanism, multi-scale aggre-\ngation, transform domain, encoder-decoder connection,\nand deep prior guidance.\n(1) Mask-aware design.\nThe missing regions (indicated with a binary mask)\nhave different shapes and convolutional operations over-\nlapping with these missing regions may be the source\nof visual artifacts. Therefore, some researchers proposed\nmask-aware solutions for classical convolutional opera-\ntion and normalization. Inspired by the inherent spa-\ntially varying property of image inpainting, Ren et al.\n(2015) designed a Shepard interpolation layer where the\nfeature map and mask both conduct the same convo-\nlution operation. Its output is the fraction of feature\nconvolution and mask convolution results. Mask con-\nvolution can simultaneously update the mask. To bet-\nter handle various irregular holes and evolve the hole\nduring mask updating, Liu et al. (2018) proposed a\nmask-guided convolution operation, i.e., partial con-\nvolution, which distinguishes between the valid region\nand hole in a convolutional window. Xie et al. (2019)\nproposed trainable bidirectional attention maps to ex-\ntend the partial convolution (Liu et al., 2018), which\ncan adaptively learn the feature re-normalization and\nmask-updating.\nDifferent from the feature normalization considered\nby previous methods, Yu et al. (2020) focused on the\n4\nWeize Quan 1,2 et al.\nmean and variance shift-related normalization and in-\ntroduced a spatial region-wise normalization into the\ninpainting network. Wang et al. (2020c) designed a vi-\nsual consistency network for blind image inpainting.\nThey first predicted the damaged regions yielding a\nmask, and then applied an inpainting network with\nthe proposed probabilistic context normalization, which\ntransfers the mean and variance from known features to\nunknown parts building on different layers. Inspired by\nfilling holes with pixel priorities (Criminisi et al., 2004;\nZhang et al., 2019b), Wang et al. (2021c) used a struc-\nture priority (in low-resolution features) and a texture\npriority (in high-resolution features) in partial convo-\nlution (Liu et al., 2018). Wang et al. (2021a) proposed\na dynamic selection network to utilize the valid pixels\nbetter. Specifically, they designed a validness migrat-\nable convolution to dynamically sample the convolu-\ntional locations, and a regional composite normaliza-\ntion module to adaptively composite batch, instance,\nand layer normalization on mask-based selective fea-\nture maps. Zhu et al. (2021) learned to derive the con-\nvolutional kernel from the mask for each convolutional\nwindow and proposed a point-wise normalization that\nproduces the mask-aware scale and bias for batch nor-\nmalization.\n(2) Attention mechanism.\nAttention is a prevalent tool to model correlation in\nthe field of natural language processing Vaswani et al.\n(2017) and computer vision (Wang et al., 2018b; Fu\net al., 2019). Attention is better at accessing features\nof distant spatial locations than convolution. In the lit-\nerature, Yu et al. (2018) was the first to introduce a con-\ntextual attention mechanism to image inpainting. This\npioneering work inspired many following works. To en-\nhance both visual and semantic coherence, Zeng et al.\n(2019) proposed a pyramid-context encoder network\nwith an attention transfer method, where the attention\nscore computed in a high-level feature is used for low-\nlevel feature updating. Instead of using one fixed patch\nsize for attention computation, Wang et al. (2019b) pro-\nposed a multi-scale contextual attention model with\ntwo different patch sizes followed by a channel atten-\ntion block (Hu et al., 2018). Wang et al. (2020b) in-\ntroduced a multistage attention module that performs\nlarge patch swapping in the first stage and small patch\nswapping in the next stage. Qin et al. (2021) combined\nspatial-channel attention (Chen et al., 2017) and a spa-\ntial pyramid structure to construct a multi-scale atten-\ntion unit (MSAU). This unit separately conducts spa-\ntial attention on four feature maps obtained by different\ndilation convolutions and then applies augmented chan-\nnel attention on concatenated attentive features. Zhang\net al. (2022e) proposed a structure and texture interac-\ntion network for image inpainting. They designed a tex-\nture spatial attention module to recover texture details\nwith robust attention scores guided by coarse structures\nand introduced a structure channel excitation module\nto recalibrate structures according to the difference be-\ntween coarse and refined structures.\nIn addition, some recent works proposed image in-\npainting networks based on vision transformers (Doso-\nvitskiy et al., 2021). Deng et al. (2021) proposed a con-\ntextual transformer network to complete the corrupted\nimages. Their network mainly depends on the multi-\nscale multi-sub-head attention, which is extended from\nthe original multi-head attention proposed by (Vaswani\net al., 2017). Cao et al. (2022) incorporated rich prior\ninformation from the ViT-based masked autoencoder\n(MAE) (He et al., 2022) into image inpainting. Specifi-\ncally, the pre-trained MAE model provides the features\nprior to the encoder of the inpainting network and the\nattention prior to making the long-distance relationship\nmodeling easier. Instead of using shallow projections or\nlarge receptive field convolutions to sequence the incom-\nplete image, Zheng et al. (2022a) designed a restrictive\nCNN head with a small and non-overlapping receptive\nfield as token representation. Deng et al. (2022) mod-\nified multi-head self-attention by inserting a Laplace\ndistance prior, which computes the similarity consider-\ning the features and their locations simultaneously.\n(3) Multi-scale aggregation.\nIn the literature on image processing, multi-scale aggre-\ngation is a common method to fuse information from\ndifferent resolutions. Wang et al. (2018c) designed a\ngenerative multi-column inpainting network, consisting\nof three convolution branches with different filter ker-\nnel sizes, to fuse multi-scale feature representations. To\ncreate a smooth transition between the inpainted re-\ngions with existing content, Hong et al. (2019) pro-\nposed a deep fusion network with multiple fusion mod-\nules and reconstruction loss applied on multi-scale lay-\ners. The fusion module merged predicted content with\nthe input image via a learnable alpha composition. Hui\net al. (2020) proposed a dense multi-scale fusion mod-\nule, which fuses hierarchical features obtained by multi-\nple convolutional branches with different dilation rates.\nZheng et al. (2021b) designed a progressive multi-scale\nfusion module to extract multi-scale features in parallel\nand progressively fuse these features, yielding more rep-\nresentative local features. Inspired by the high-resolution\nnetwork (HRNet) for visual recognition (Sun et al.,\n2019; Wan et al., 2021), Wang et al. (2021c) introduced\na parallel multi-resolution fusion network for image in-\npainting. This network can simultaneously conduct in-\npainting in multiple resolutions with mask-aware and\nattention-guided representation fusion methods. Phutke\nDeep Learning-based Image and Video Inpainting: A Survey\n5\nand Murala (2021) also followed a multi-path design,\nwhere they introduce four concurrent branches with\ndifferent resolutions in the encoder. A residual module\nwith diverse receptive fields is designed as the building\nblock of the encoder. Cao and Fu (2021) proposed a\nmulti-scale sketch tensor network for man-made scene\ninpainting. This network reconstructs different types\nof structures by adding constraints on predicted lines,\nedges, and coarse images with different scales. Differ-\nent from the mask-blind processing (Li et al., 2020b;\nQin et al., 2021) of multi-scale features produced by\nconvolution with different dilation rates, Zeng et al.\n(2022) carefully designed a gated residual connection,\nwhich considers the difference between holes and valid\nregions. They also proposed a soft mask-guided Patch-\nGAN, where the discriminator is trained to predict the\nsoft mask obtained by Gaussian filtering.\n(4) Transform domain.\nInstead of conducting image inpainting in the spatial\ndomain, some existing works designed inpainting frame-\nworks in a transformed domain via the DWT (discrete\nwavelet transform) (Daubechies, 1990) and the FFT\n(fast Fourier transform). Wang et al. (2020a) recast the\nimage inpainting problem as predicting low-frequency\nsemantic structures and high-frequency texture details.\nSpecifically, they decomposed the corrupted image into\ndifferent frequency components via the Haar wavelet\ntransform (Mallat, 1989), designed a multi-frequency\nprobabilistic inference model to predict the frequency\ncontent in missing regions, and inversely transformed\nback to image space. Yu et al. (2021a) adopted a simi-\nlar inpainting pipeline. For the multi-frequency comple-\ntion, they proposed a frequency region attentive nor-\nmalization module to align and fuse the features with\ndifferent frequencies and applied two discriminators to\ntwo high-frequency streams. Li et al. (2021) extracted\nhigh-frequency subbands as the texture and introduced\na DWT loss to constrain the fidelity of low- and high-\nfrequency subbands. LaMa (Suvorov et al., 2022) com-\nbined the residual design (He et al., 2016) and fast\nFourier convolution (Chi et al., 2020) to construct a fast\nFourier convolution residual block, which is integrated\ninto the encoder-decoder network to handle large mask\ninpainting. Lu et al. (2022) further improved LaMa by\nintroducing various types of masks and adding the fo-\ncal frequency loss (Jiang et al., 2021) to constrain the\nspectrum of the images.\n(5) Encoder-decoder connection.\nSome works modify the basic encoder-decoder architec-\nture by introducing carefully designed feature connec-\ntions. Shift-Net (Yan et al., 2018) modified the U-Net\narchitecture by introducing a specific shift-connection\nlayer, which shifts the encoder features of the valid re-\ngion to the missing regions with a guidance loss. Dol-\nhansky and Ferrer (2018) introduced an eye inpaint-\ning network that merges the identifying information of\nthe reference image encoding as a code. Shen et al.\n(2019) designed a densely connected generative net-\nwork for semantic image inpainting. They combined\nfour symmetric U-Nets with dense skip connections.\nLiu et al. (2020) introduced a mutual encoder-decoder\nCNN, fusing the texture and structure features (from\nthe shallow and deep layers of the encoder), to jointly\nrestore the structure and texture with feature equaliza-\ntion. Similarly, Guo et al. (2021) designed a two-stream\nimage inpainting network, which combines a structure-\nconstrained texture synthesis submodel and a texture-\nguided structure reconstruction submodel. In addition,\nthey introduced a bi-directional gated feature fusion\nmodule and a contextual feature aggregation module to\nfuse and refine the resulting images. Feng et al. (2022)\ninserted generative memory into the classical encoder-\ndecoder network to jointly exploit the high-level se-\nmantic reasoning and the pixel-level content reasoning.\nBased on (Liu et al., 2020), Liu et al. (2022) inferred\nthe texture and structure with a content-consistent ref-\nerence image through a feature alignment module.\n(6) Deep prior guidance.\nTo enhance the performance of the inpainting gener-\nator, some works have explored the deep prior from\na single image or a large image database. Lempitsky\net al. (2018) utilized a randomly-initialized generator\nnetwork as the prior to completing the corrupted im-\nage by only reconstructing the known regions. Gu et al.\n(2020) proposed mGANprior by incorporating a pre-\ntrained GAN as prior for image inpainting. Specifically,\nthis method reconstructs the incorrupt regions while\nfilling in the missing areas by adaptively merging multi-\nple generative feature maps from different latent codes.\nRichardson et al. (2021) developed a pixel2style2pixel\n(pSp) framework for image inpainting. They introduced\nan encoder consisting of a feature pyramid and multiple\nmapping networks to encode the damaged image into\nextended latent space W+ (18 512-dimensional style\nvectors), which is the extension of latent space W (Kar-\nras et al., 2019), and reused a pre-trained StyleGAN\ngenerator as priors to achieve the complete image. To\nhandle the large missing regions and complex seman-\ntics, Wang et al. (2022b) designed a dual-path image\ninpainting framework with GAN inversion (Xia et al.,\n2022). Given a corrupted image, the inversion path in-\nfers the close latent code and extracts the correspond-\ning multi-layer features from the trained GAN model,\nand the feed-forward path fills the missing regions by\nmerging the above semantic priors with a deformable\nfusion module. To guarantee the invariance of the valid\n6\nWeize Quan 1,2 et al.\narea in the corrupted and completed images, Yu et al.\n(2022b) modified the GAN inversion pipeline (Richard-\nson et al., 2021) by designing the mapping network with\na pre-modulation module and introducing F&W+ la-\ntent space, where F are the feature maps of the cor-\nrupted image.\nTraining objectives. The training objective is a\nvery important component of deep learning-based im-\nage inpainting methods. Pixel-wise reconstruction loss,\nperceptual loss (Johnson et al., 2016), style loss (Gatys\net al., 2016), and adversarial loss (Goodfellow et al.,\n2014) are the prevalent training objectives. The ad-\nversarial loss is obtained by a discriminator network.\nPathak et al. (2016) and Li et al. (2019b) adopted the\ndiscriminator (stacked convolution and down-sampling)\nfrom DCGAN (Radford et al., 2016). Considering Pathak\net al. (2016)’s method struggles to maintain local con-\nsistency with the surrounding regions, Iizuka et al. (2017)\nproposed local and global discriminators, which gener-\nate more realistic contents. Yu et al. (2018) proposed\na patch-based discriminator, which can be regarded as\nthe generalized version of local and global discrimina-\ntors (Iizuka et al., 2017). This patch-based discrimina-\ntor is subsequently used in many following works. Liu\net al. (2021c) designed two discriminators with small-\nand large-scale receptive fields to guide the inpainting\nnetwork for fine-grained image detail generation.\nBesides, researchers have also introduced some care-\nfully designed losses. Li et al. (2017) introduced a se-\nmantic parsing loss for face completion. Yeh et al. (2017)\nproposed context and prior losses to search the clos-\nest encoding in the latent image manifold for inferring\nthe missing content. Vo et al. (2018) proposed a struc-\ntural reconstruction loss, which is the combination of\nreconstruction errors in pixel and feature space. For ex-\nplicitly exploring the structural and textural coherence\nbetween filled contents and their surrounding contexts,\nLi et al. (2019a) utilized the local intrinsic dimension-\nality (Houle, 2017a,b) in the image- and patch-level to\nmeasure and constrain the alignment between data sub-\nmanifolds of inpainted contents and those of the valid\npixels. To stabilize the training process of face inpaint-\ning, i.e., weakening gradient vanishing and model col-\nlapse, Han and Wang (2021) trained the generator via\nneuro-evolution and optimized the generator’s parame-\nters by mutation and crossover.\nSome researchers introduced additional training ob-\njectives via multi-task learning. Liao et al. (2018a) pre-\nsented a novel collaborative framework by training a\ngenerator simultaneously on multiple tasks, i.e., face\ncompletion, landmark detection, and semantic parsing.\nTo enhance the inpainting capability of the network for\nimage structure, Yang et al. (2020) designed a struc-\nture restoration branch in the decoder and explicitly\ninserted the structure features into the primary inpaint-\ning process. Appropriate semantic guidance is a suit-\nable tool for image inpainting (Song et al., 2018b), in-\nspired by this, Liao et al. (2020, 2021b,a) proposed a\nunified framework to jointly predict the segmentation\nmaps and recover the corrupted images. Specifically,\nLiao et al. (2020, 2021b) designed a semantic guidance\nand evaluation network that iteratively updates and\nevaluates a semantic map and infers the missing con-\ntents in multiple scales. However, this method may cre-\nate implausible textures and blurry boundaries, espe-\ncially on mixed semantic regions. To solve this problem,\nLiao et al. (2021a) devised a semantic-wise attention\npropagation module to apply the attention operation\non the same semantic regions. They also introduced two\ncoherence losses to constrain the consistency between\nthe semantic map and the structure and texture of the\ninpainted image. Zhang et al. (2020b) studied how to\nimprove the visual quality of inpainted images and pro-\nposed a pixel-wise dense detector for image inpaint-\ning. This detection-based framework can localize the\nartifacts of completed images, and the corresponding\nposition information is combined with the reconstruc-\ntion loss to better guide the training of the inpainting\nnetwork. Zhang et al. (2021) introduced the semantic\nprior estimation as a pretext task with a pre-trained\nmulti-label classification model, and then utilized the\nlearned semantic priors to guide the inpainting pro-\ncess through a spatially-adaptive normalization mod-\nule (Park et al., 2019). Yu et al. (2022a) jointly solved\nimage reconstruction, semantic segmentation, and edge\ntexture generation. Each branch is implemented with\na transformer network, and a multi-scale spatial-aware\nattention block is developed to guide the main image\ninpainting branch from the other two branches. Similar\nto (Zhang et al., 2020b), Zhang et al. (2022d) first local-\nized the perceptual artifacts from the completed image,\nand then used this information to guide the iterative\nrefinement process. They also manually annotated an\ninpainting artifact dataset.\n2.1.2 Two-stage framework\nCoarse-to-fine methods. This kind of method first\napplies a generator to fill the holes with coarse con-\ntents, and then refine them via the second generator,\nas shown in Fig. 4(a). Yu et al. (2018) modified the\ngenerative inpainting framework with cascaded coarse\nand refinement networks. In the refinement stage, they\ndesigned a contextual attention module modeling the\nlong-term correlation to facilitate the inpainting pro-\ncess.\nDeep Learning-based Image and Video Inpainting: A Survey\n7\n(a) Coarse-to-fine\n(b) Structure-then-texture\nFig. 4: Two types of the two-stage inpainting framework: (a) coarse-to-fine (Yu et al., 2018) where the first network\npredicts an initial coarse result and the second network predicts a refined result; (b) structure-then-texture (Nazeri\net al., 2019) where the first network predicts a structure map and the second network predicts a complete image.\nAn apparent difference between these two types is that the structure-then-texture methods explicitly predict the\nstructure map in the first stage.\nMany later works refined different aspects of this\nclassical coarse-to-fine framework. Inspired by mask-\naware convolution (Liu et al., 2018) for irregular holes,\nYu et al. (2019) improved the previous network (Yu\net al., 2018) by introducing gated convolution that adap-\ntively perceives the mask location. In the coarse stage,\nMa et al. (2019) proposed region-wise convolutions and\na non-local operation to process the discrepancy and\ncorrelation between intact and damaged areas. PEPSI\nSagong et al. (2019) modified the two-stage feature en-\ncoding processes in (Yu et al., 2018) by sharing the\nencoding network and organizing the coarse and fine\ninpainting network in a parallel manner. PEPSI can\nenhance the inpainting capability while reducing the\nnumber of convolution operations and computational\nresources. To further reduce the network parameters,\nShin et al. (2021) extended PEPSI by replacing the orig-\ninal dilated convolutional layers (Yu and Koltun, 2016)\nwith a so-called rate-adaptive version, which shares the\nweights for each layer but produces dynamic features\nvia dilation rates-related scaling and shifting opera-\ntions. The contextual attention proposed by (Yu et al.,\n2018) has a limited ability to model the relationships\nbetween patches inside the holes, therefore, Liu et al.\n(2019) introduced a coherent semantic attention layer,\nwhich can enhance the semantic relevance and feature\ncontinuity in the attention computation of hole regions.\nIn (Yu et al., 2018), several dilated convolutions are\napplied to enlarge the receptive field. Li et al. (2020b)\nreplaced the dilated convolution with a spatial pyra-\nmid dilation ResNet block with eight different dilation\nrates to extract multi-scale features. Navasardyan and\nOhanyan (2020) designed a patch-based onion convo-\nlution mechanism to continuously propagate informa-\ntion from known regions to the missing ones. This con-\nvolution mechanism can capture long-range pixel de-\npendencies and achieve high efficiency and low latency.\nWadhwa et al. (2021) proposed a hypergraph convo-\nlution with a trainable incidence matrix to generate\nglobally semantic completed images and replaced the\nregular convolutions with gated convolution in the dis-\ncriminator to enhance the local consistency of inpainted\nimages.\nDue to the computational overhead and the lack of\nsupervision for the contextual attention in (Yu et al.,\n2018), Zeng et al. (2021b) removed this attention block\nand learned its patch-borrowing behavior with a so-\ncalled contextual reconstruction loss. Based on the in-\nsight that recovering different types of missing areas\nneed a different scope of neighboring areas, Quan et al.\n8\nWeize Quan 1,2 et al.\n(2022) designed a local and global refinement network\nwith small and large receptive fields, which can be di-\nrectly applied to the end of existing networks to further\nenhance their inpainting capability. Kim et al. (2022)\ndeveloped a coarse-super-resolution-refine pipeline, where\nthey add a super-resolution network to reconstruct finer\ndetails after the coarse network and introduce a pro-\ngressive learning mechanism to repair larger holes.\nSome works adopt a coarse-to-fine framework to ob-\ntain high-resolution inpainting. Yang et al. (2017) de-\nsigned a two-stage inpainting framework consisting of\na content network and a texture network. The former\npredicts the holistic content in the low resolution (128×\n128) and the latter iteratively optimizes the texture\ndetails of missing regions from low to high resolution\n(512 × 512). Song et al. (2018a) developed an image-\nto-feature network to infer coarse results, and then de-\nsigned a patch-swap method to refine the coarse fea-\ntures. The swapped feature map is translated to a com-\nplete image via a Feature2Image network. In addition,\nthis framework can be directly used for high-resolution\ninpainting by upsampling the complete image as the\ninput of refine stage with a multi-scale inference. Yi\net al. (2020) proposed a contextual residual aggrega-\ntion mechanism for ultra high-resolution image inpaint-\ning (up to 8K). Specifically, a low-resolution inpaint-\ning result was first predicted via a two-stage coarse-\nto-fine network and then the high-resolution result was\ngenerated by adding the large blurry image with the\naggregated residuals, which are obtained by aggregat-\ning weighted high-frequency residuals from contextual\npatches. Zhang et al. (2022c) focused on image inpaint-\ning for 4K or more resolution. They first fill the hole\nvia LaMa (Suvorov et al., 2022), predict depth, struc-\nture, and segmentation map from the initially com-\npleted image, then generate multiple candidates with\na multiply-guided PatchMatch (Barnes et al., 2009),\nand finally choose a good output using the proposed\nauto-curation network. To complete the high-resolution\nimage with limited resources, these methods first pre-\ndicted the coarse content at the low-resolution level and\nthen refine the texture details at the high-resolution\nlevel (sometimes with multi-scale inferences).\nOther works also follow the basic coarse-to-fine strat-\negy, but they are clearly different from the framework\nproposed by (Yu et al., 2018). After obtaining the coarse\nresult with an initial prediction network, Li et al. (2019d)\napplied a super-resolution network as the refinement\nstage to produce high-frequency details. Roy et al. (2021)\npredicted the coarse results in the frequency domain by\nlearning the mapping of the DFT of the corrupted im-\nage and its ground truth. Based on the insight that\npatch-based methods (Barnes et al., 2009; He and Sun,\n2012) fill the missing regions with high-quality texture\ndetails, Xu et al. (2021) proposed a texture memory-\naugmented patch synthesis network with a patch dis-\ntribution loss after the coarse inpainting network.\nStructure-then-texture methods. Structure and\ntexture are two important components of the image,\ntherefore, some works decompose the image inpainting\nas the structure inference and the texture restoration,\nas shown in Fig. 4(b). Sun et al. (2018b) designed a two-\nstage head inpainting obfuscation network. The first\nstage generates facial landmarks and the second stage\nrecovers the head image guided by the landmarks. Song\net al. (2019) first estimated the facial geometry includ-\ning landmark heatmaps and parsing maps, and then\nconcatenated these results with a corrupted face image\nas the input of the complete network to recover face\nimages and disentangle masks. Liao et al. (2018b) and\nNazeri et al. (2019) both proposed an edge-guided im-\nage inpainting method, which first estimates the edge\nmap for the missing regions, and then utilizes this edge\nmap prior to predicting the texture details. Similarly,\nXiong et al. (2019) explicitly disentangled the image\ninpainting problem into two sub-tasks of foreground\ncontour prediction and content completion. To improve\nthe structural guidance of coarse edge maps, Ren et al.\n(2019) introduced another representation of the struc-\nture, i.e., the edge-preserving smoothing via filtering\noperation. Based on the structure reconstruction of the\nfirst network, they inpainted missing regions using ap-\npearance flow. Shao et al. (2020) combined the edge\nmap and color aware map as the representation of the\nstructure, where the former is captured via the Canny\noperator (Canny, 1986) and the latter is obtained through\nGaussian blur with a large kernel. For the specific Manga\ninpainting, Xie et al. (2021) first completed a semantic\nstructure map, including the structural lines and the\nScreenVAE map (a point-wise representation of screen-\ntones) (Xie et al., 2020), using a semantic inpainting\nnetwork. Then, the completed semantic map is used for\nguiding the appearance synthesis. Wang et al. (2021b)\ndesigned an external-internal learning inpainting frame-\nwork. It first reconstructs the structures in the monochro-\nmatic space using the knowledge externally learned from\nlarge datasets. Based on internal learning, then, it ap-\nplies a multi-stage network to recover the color infor-\nmation via iterative optimization. Besides the edge map\nused in (Nazeri et al., 2019), Yamashita et al. (2022) in-\ncorporated the depth image to provide the boundaries\nbetween different objects. Their method first completed\nthe masked edge and depth images separately and then\nrecovered the missing regions via an RGB image in-\npainting network taking as input the concatenation of\nmasked images, inpainted edges, and depth images. To\nDeep Learning-based Image and Video Inpainting: A Survey\n9\nFig. 5: Progressive image inpainting. The image comes\nfrom (Zhang et al., 2018a).\ncontain richer structural information, Wu et al. (2022)\nchoose the local binary pattern (LBP) (Ojala et al.,\n1996, 2002), which describes the distribution informa-\ntion of edges, speckles, and other local features (Zhang\net al., 2010). In (Wu et al., 2022), the first network in-\nfers the LBP information of the holes, and the second\nnetwork with spatial attention conducts the actual im-\nage inpainting. Dong et al. (2022) utilized a transformer\nto complete the holistic structure in a grayscale space\nand proposed a masking positional encoding for large\nirregular masks.\nIn addition, semantic segmentation maps are also\nused as the proxy of structure (Song et al., 2018b; Qiu\net al., 2021; Zhou et al., 2021). Song et al. (2018b) in-\ntroduced the semantic segmentation information into\nthe image inpainting process to improve the recovered\nboundary between different class regions. They first\npredict the segmentation map of missing regions via\na U-Net and then recover the missing contents with the\nguidance of the above inpainted semantic map using the\nsecond generator network. Song et al. (2018b) utilized\nthe pre-classification algorithm (Felzenszwalb and Hut-\ntenlocher, 2004) to extract a semantic structure map.\nAfter the completion of the semantic map, they em-\nployed a spatial-channel attention module to generate\nthe texture information. Zhou et al. (2021) first pre-\ndicted the complete segmentation map via a segmen-\ntation reconstructor, and then recovered fine-grained\ntexture details with an image generator based on a re-\nlation network. The relation network is an extension of\nSPADE (Park et al., 2019) to better modulate features\nvia spatially-adaptive normalization with the relation\ngraph.\n2.1.3 Progressive frameworks\nFollowing the basic idea of traditional inpainting meth-\nods, some works have been proposed to exploit progres-\nsive inpainting with deep models. As shown in Fig. 5,\nthe progressive methods iteratively fill in the holes from\nthe boundary to the center of the holes, and the miss-\ning area gradually becomes smaller until it disappears.\nZhang et al. (2018a) formulated image inpainting as\na sequential problem, where the missing regions are\nfilled in four inpainting phases. They designed an LSTM\n(long short-term memory) (Hochreiter and Schmidhu-\nber, 1997)-based framework to string these four inpaint-\ning phases together. However, this method cannot han-\ndle irregular holes common in real-world applications.\nGuo et al. (2019) devised a residual architecture to\nprogressively update irregular masks and introduced\na full-resolution network to facilitate feature integra-\ntion and texture reconstruction. Inspired by structure-\nguided inpainting methods (Nazeri et al., 2019; Xiong\net al., 2019), Li et al. (2019c) proposed a progressive\nreconstruction with a visual structure network to in-\ncorporate structure information into the visual features\nstep by step, which can generate a more structured im-\nage. Progressive inpainting methods have the potential\nto fill in large holes, however, it is still difficult due to\nthe lack of constraints on the hole center. To handle\nthis drawback, Li et al. (2020c) designed a recurrence\nfeature reasoning network with consistent attention and\nweighted feature fusion. This network recurrently infers\nand gathers the hole boundaries of the feature map so\nas to progressively strengthen the constraints for esti-\nmating internal contents. Zeng et al. (2020b) proposed\nan iterative inpainting method with confidence feed-\nback for high-resolution images. SRInpaintor (Li et al.,\n2022a) combined super-resolution and the transformer\nin a progressive pipeline. It reasons about the global\nstructure in low resolution, and progressively refines the\ntexture details in high resolution.\nTo this end, we organize the important and preva-\nlent technical aspects for the network design, as shown\nin Table 1.\n2.2 Stochastic Image Inpainting\nImage inpainting is an underdetermined inverse prob-\nlem. Therefore, multiple plausible solutions exist. We\nuse the term stochastic image inpainting to refer to\nmethods capable of producing multiple solutions with\na random sampling process.\nVAE-based methods. A variational autoencoder\n(VAE) (Kingma and Welling, 2014) is a generative model\nthat combines an encoder and a decoder. The encoder\n10\nWeize Quan 1,2 et al.\nTable 1: The summary of important techniques for deep learning-based image inpainting.\nAspects\nBlocks\nCore idea\nmask-aware convolution\nShepard interpolation (Ren et al., 2015)\ntranslation variant interpolation\npartial convolution (Liu et al., 2018)\nconvolution on valid regions\ngated convolution (Yu et al., 2019)\nadaptive gating\npriority-guided partial convolution (Wang et al., 2021c)\nstructure and texture priority\nAttention\ncontextual attention (Yu et al., 2018)\nbackground patches with high\nsimilarity to the coarse prediction\ncoherent semantic attention (Liu et al., 2019)\ncorrelation between patches within the hole\nmulti-scale attention module (Wang et al., 2019b)\nattention with two patch sizes\nmulti-scale attention uint (Qin et al., 2021)\nattention with four different dilation rates\nNormalization\nregion normalization (Yu et al., 2020)\nspatial and region-wise\nprobabilistic context normalization (Wang et al., 2020c)\ntransfer mean and variance\nregional composite normalization (Wang et al., 2021a)\nbatch, instance, and layer normalization\npoint-wise normalization (Zhu et al., 2021)\nmask-ware batch normalization\nfrequency region attentive normalization (Zhu et al., 2021)\nalign low- and high-frequency features\nDiscriminator\nglobal discriminator (Pathak et al., 2016)\nentire image\nlocal discriminator (Iizuka et al., 2017)\ncorrupted region\npatch-based discriminator (PatchDis) (Yu et al., 2019)\neense local patches\nconditional multi-scale discriminator (Li et al., 2020b)\nPatchDis with two different scales\nsoft mask-guided PatchDis (Zeng et al., 2022)\ncentral parts of the missing regions\nlearns an appropriated latent space and the decoder\ntransforms sampled latent representations back into new\ndata. Zheng et al. (2019) proposed a two-branch com-\npletion network, where the reconstructive branch mod-\nels the prior distribution of missing parts and recon-\nstructs the original complete image from this distri-\nbution. The generative branch infers the latent con-\nditional prior distribution for the missing areas. This\nframework is optimized by balancing the variance of\nthe conditional distribution and the reconstruction of\nthe original training data. Zheng et al. (2021a) extended\nthis work by estimating the distributions in a separate\ntraining stage and introducing the patch-level short-\nlong term attention module. For stochastic fashion im-\nage inpainting, Han et al. (2019) decomposed the in-\npainting process as the shape and appearance genera-\ntion. The network design for these two generation tasks\nmainly adopts the VAE architecture. Based on a pre-\ntrained VAE on facial images, Tu and Chen (2019) first\nsearched for the possible set of solutions in the coding\nvector space for the corrupted image, and then recov-\ners possible face images with the decoder of the VAE.\nZhao et al. (2020) proposed an instance-guided condi-\ntional image-to-image translation network to learn con-\nditional completion distribution. Specifically, they first\nencode the instance and masked images into two prob-\nability feature spaces, and then design a cross-semantic\nattention layer to fuse two feature maps. A decoder is\nfinally used to generate the inpainted image. However,\nHan et al. (2019) and Zhao et al. (2020) often suffer\nfrom distorted structures and blurry textures due to the\njoint optimization of structure and appearance. Peng\net al. (2021) designed a two-stage pipeline, where the\nfirst stage produces multiple coarse results with differ-\nent structures based on a hierarchical vector quantized\nvariational auto-encoder, and the second stage synthe-\nsizes the texture under the guidance of the discrete\nstructural features.\nGAN-based methods. GAN (Goodfellow et al.,\n2014) learns the data distribution via an adversarial\nprocess. A generator is applied to transform sampled\nGaussian random noise into image space and a dis-\ncriminator is used to differentiate the real sample and\nfake sample. Based on the premise that the degree of\nfreedom increases from the hole boundary to the hole\ncenter, Liu et al. (2021a) introduced a spatially prob-\nabilistic diversity normalization to modulate the pixel\ngeneration with diversity maps. Considering that min-\nimizing the classical reconstruction loss hampers the\ndiversity of results, they also proposed a perceptual di-\nversity loss that maximizes the distance of two gen-\nerated images in the feature space. By combining the\nimage-conditional and unconditional generative archi-\ntectures, Zhao et al. (2021) proposed a co-modulated\nGAN for large-scale image inpainting. Technically, they\nencode the incomplete input image into a conditional\nlatent vector, which is then concatenated with the orig-\ninal style vector of StyleGAN2 (Karras et al., 2020).\nTo enhance the diversity and control of image inpaint-\ning, Zeng et al. (2021a) applied the patch matching\nfrom the training samples on the basis of coarse in-\npainted results. In particular, they designed the near-\nest neighbor-based pixel-wise global matching (from a\nsingle image) and compositional matching (from mul-\ntiple images). Inspired by CoModGAN (Zhao et al.,\n2021), Zheng et al. (2022b) proposed a cascaded mod-\nDeep Learning-based Image and Video Inpainting: A Survey\n11\nulation GAN, which combines the global modulation\nand the spatially-adaptive modulation in each scale of\nthe decoder, and replaces the common convolution with\nfast Fourier convolution (Chi et al., 2020) in the en-\ncoder. To directly complete the high-resolution image,\nLi et al. (2022b) proposed a mask-aware transformer\nmodule with a dynamic mask updating as (Liu et al.,\n2018). This module conducts non-local interactions only\nusing partially valid tokens in a shifted-window man-\nner Liu et al. (2021d). Following (Chen et al., 2019;\nKarras et al., 2019), they developed a style manipula-\ntion module for stochastic generations.\nFlow-based methods. Normalizing Flows (Tabak\nand Vanden-Eijnden, 2010; Dinh et al., 2014; Rezende\nand Mohamed, 2015) are a generative method that con-\nstructs a complex probability distribution by assem-\nbling a sequence of invertible mappings. Inspired by\nGlow (Kingma and Dhariwal, 2018) and its conditional\nextension (Lugmayr et al., 2020), Wang et al. (2022a)\nproposed a conditional normalizing flow network to learn\nthe probability distribution of structure priors. Then,\nanother generator is applied to produce the final com-\nplete image with rich texture.\nMLM-based methods. To produce a stochastic\nstructure in the missing region, Yu et al. (2021b) and\nWan et al. (2021) adopted a sequence prediction pipeline\nbased on a masked language model (MLM). Yu et al.\n(2021b) proposed a bidirectional and auto-regressive\ntransformer as the low-resolution stochastic-structure\ngenerator, which predicts masked token (missing re-\ngions) via a top-K sampling strategy during inference.\nThen, a texture generator was applied to generate mul-\ntiple inpainted results. Similarly, Wan et al. (2021) pro-\nposed a Transformer-CNN framework. They first ap-\nply a transformer training with MLM objective to pro-\nduce a low-resolution image with pluralistic structures\nand some coarse textures, and then utilize an encoder-\ndecoder network to enhance the local texture details of\nthe high-resolution complete image.\nDiffusion model-based methods. Diffusion mod-\nels (DM) are emerging generative models for image\nsynthesis. Here, we only review diffusion model-based\ninpainting methods, and we refer readers to the sur-\nveys (Yang et al., 2023; Croitoru et al., 2023) about a\ncomprehensive introduction to diffusion models. Gen-\nerally, diffusion-based inpainting models employ a U-\nNet architecture. The training objectives are usually\nbased on LDM = Ex,ϵ∈N (0,1),t[∥ϵ − ϵθ(xt, t)∥2\n2], where\nt = 1 . . . T, xt is a noised version of x, and ϵθ(·, t) is a\nneural network. In the literature, existing works mainly\nfocused on the sampling strategy design and the com-\nputational cost reduction.\n(1) Sampling strategy design.\nBased on an unconditionally pre-trained denoising dif-\nfusion probabilistic model (DDPM) (Ho et al., 2020),\nLugmayr et al. (2022) modified the standard denoising\nstrategy by sampling the masked regions from the dif-\nfusion model and sampling the unmasked areas from\nthe given image. To preserve the background and im-\nprove the consistency, Xie et al. (2023) added an extra\nmask prediction to the diffusion model. In the inference\nstage, the predicted mask is used to guide the sampling\nprocess.\n(2) Computational cost reduction.\nInstead of applying the diffusion process in pixel space,\nEsser et al. (2021) utilized a multinomial diffusion pro-\ncess (Hoogeboom et al., 2021; Austin et al., 2021) on\na discrete latent space and autoregressively factorized\nmodels for the reverse process. These designs enable\nImageBART to generate high-resolution images, e.g.,\n300 × 1800. Similarly, Rombach et al. (2022) proposed\na latent diffusion model (LDM) to reduce the training\ncost of DMs while boosting visual quality, which can be\napplied to the image inpainting task at a high resolution\nof 10242 pixels. To overcome the limitation of massive\niterations in the diffusion model, Li et al. (2022c) pro-\nposed a spatial diffusion model (SDM) with decoupled\nprobabilistic modeling, where the mean term refers to\nthe inpainted result and the variance term measures\nthe uncertainty. Instead of starting with random Gaus-\nsian noise in the reverse conditional diffusion, Chung\net al. (2022) remarkably reduced the number of sam-\npling steps with a better initialization by starting from\nforward-diffused data.\n2.3 Text-guided Image Inpainting\nText-guided image inpainting takes an incomplete im-\nage and text description as input and generates text-\naligned inpainting results. The main challenge lies in\nhow to fuse the text and image semantic features, and\nhow to focus on effective information in the text. Zhang\net al. (2020a) proposed a dual attention mechanism to\nobtain the semantic feature of the masked region by\nfinding unmatched words compared to the image and\napplied DAMSM loss (Xu et al., 2018b) to measure the\nsimilarity of text and image. Lin et al. (2020) intro-\nduced an image-adaptive word demand module that re-\nmoves redundant information and aggregates text fea-\ntures in the coarse stage. They also proposed a text-\nguided attention loss that pays more attention to the\nreconstruction of the region affected by the text. Zhang\net al. (2020c) encoded text and image to sequential data\nand exploited the transformer architecture to let cross-\nmodal features interact. To ensure that the inpainted\n12\nWeize Quan 1,2 et al.\nFig. 6: Representative examples of masks.\nimage matches the text, they took the masked text and\ninpainted image as input to restore the text prompt. Wu\net al. (2021) incorporated word-level and sentence-level\ntextual features into a two-stage generator by introduc-\ning a dual-attention module. To eliminate the effection\nof the background, the mask reconstruction module was\ndevised to recover the corrupted object mask. Xie et al.\n(2022) applied multi-head self-attention as text-image\ninteractive encoder. They created a semantic relation\ngraph to compute non-Euclidean semantic relations be-\ntween text and image, and used graph convolution to\naggregate node features. Li et al. (2023) followed a\ncoarse-to-fine image inpainting framework. They first\nemployed a visual-aware textual filtering mechanism to\nadaptively concentrate on required words and then in-\nserted filtered text features into the coarse network. Un-\nlike (Zhang et al., 2020c), they directly reconstructed\ntext descriptions from inpainted images to guarantee\nmulti-modal semantic alignment. To better preserve the\nnon-defective regions during the text guidance, Ni et al.\n(2023) proposed a defect-free VQGAN to control recep-\ntive spreading and a sequence-to-sequence module to\nenable visual-language learning from multiple different\nperspectives, including text descriptions, low-level pix-\nels, and high-level tokens. Recent methods are based on\ndiffusion models.Shukla et al. (2023) focused on how to\ngenerate a high-quality text prompt to guide a text-\nto-image model-based inpainting network by analyz-\ning inter-object relationships. They first constructed a\nscene graph based on object detector outputs and ex-\npanded it via a graph convolution network to obtain\nthe features of the corrupted node. Finally, the gener-\nated text prompt and masked image were fed to the\ndiffusion model to obtain the inpainted result. Wang\net al. (2023) found that object masks would force the\ninpainted images to rely more on text descriptions in-\nstead of the random mask. Then, they proposed Imagen\nEditor fine-tuned from Imagen (Saharia et al., 2022b)\nwith a new convolutional layer and designed an object\nmasking strategy for better training. To facilitate the\nsystematic evaluation of text-guided image inpainting,\nthey established a benchmark called EditBench.\n2.4 Inpainting Mask\nIn the development of image inpainting techniques, var-\nious artificial masks have been introduced. These masks\ncan be roughly divided into two categories: regular masks\nand irregular masks. Fig. 6 summarizes these masks,\nwhere white pixels indicate missing regions.\nRegular masks. A square hole that blocks the cen-\nter area or random location are generally easier to con-\nstruct. Lugmayr et al. (2022) introduced more regular\nmasks, including Super-Resolution 2× (reserving pixels\nwith a stride of 2), Alternating lines (removing every\nsecond row), Expand (leaving a small center crop of the\ninput image), and Half (masking the half of the input\nimage).\nDeep Learning-based Image and Video Inpainting: A Survey\n13\nIrregular masks. Letter masks ((Bertalmio et al.,\n2000; Bian et al., 2022)) and object-shaped masks ((Cri-\nminisi et al., 2004; Yi et al., 2020)) are particularly de-\nsigned for specific tasks, for example, caption removal\nand object removal. Liu et al. (2018) introduced free-\nform masks, where the former collected random streaks\nand arbitrary holes from the results of the occlusion/dis-\nocclusion mask estimation method. The irregular masks\nshared by (Liu et al., 2018) are very common in the\nexisting inpainting methods. Suvorov et al. (2022) fur-\nther split free-form masks into narrow masks, large wide\nmasks, and large box masks, where two types of large\nmasks are generated via an aggressive mask method\nsampling polygonal chains with a high random width\nand rectangles of random aspect ratios, respectively.\n2.5 Loss Functions\nFor image inpainting, the loss functions affect features\nof different sizes. At the lowest level, a pixel reconstruc-\ntion loss aims to recover the exact pixel values. We\nfurther discuss the total-variational (TV) loss (Rudin\net al., 1992), feature consistency loss, the perceptual\nloss (Johnson et al., 2016), style loss (Gatys et al.,\n2016), and adversarial loss (Goodfellow et al., 2014).\nAs input, an inpainting network accepts an input\nimage Iin and a binary mask M describing the missing\nregions (where 0 means the valid pixel and 1 means the\nmissing pixel). The output of the network is a complete\nimage Iout. The loss functions are formulated as follows.\nPixel-wise reconstruction loss. In the literature,\nthe pixel-wise reconstruction loss often has two types:\nℓ1 loss (Eq. (1)) and weighted ℓ1 loss (Eq. (3)). The key\npoint is how the valid and unknown regions differ in the\nloss function. The detailed formulations are as follows,\nLwpr = ||(Igt − Iout)||1.\n(1)\nwhere Igt is the ground-truth complete image.\nLvalid =\n1\nsum(1 − M)||(Igt − Iout) ⊙ (1 − M)||1,\nLhole =\n1\nsum(M)||(Igt − Iout) ⊙ M||1,\n(2)\nwhere ⊙ is the element-wise product operation, and\nsum(M) is the number of non-zero elements in M.\nThen the weighted ℓ1 loss is formulated as\nLpr = Lvalid + α · Lhole,\n(3)\nwhere α is the balancing factor. It is well known that\nthe ℓ1 loss can capture the low-frequency components,\nwhereas it struggles to restore the high-frequency com-\nponents (Isola et al., 2017; Ledig et al., 2017).\nTotal-variation loss. Total-variation loss can be\napplied to ameliorate the potential checkerboard arti-\nfacts introduced by the perceptual loss. The formula-\ntion is:\nLtv = ||Imer(i, j + 1) − Imer(i, j)||1\n+||Imer(i + 1, j) − Imer(i, j)||1.\n(4)\nwhere Imer = Iout ⊙ M + Igt ⊙ (1 − M) is the merged\n(completed) image.\nFeature consistency loss. This loss constrains ex-\ntracted feature maps of the prediction with guidance\nfrom ground truth images:\nLfc =\nX\ny∈Ω\n||Φm(Iin)y − Φn(Igt)y||2\n2.\n(5)\nwhere Ω is the missing regions, Φm(·) is the feature\nmap of the selected layer in the inpainting network,\nand Φn(·) is the feature map of the corresponding layer\nin the inpainting network or pre-trained VGG models.\nΦm(·) and Φn(·) must have the same shape.\nPerceptual loss. The perceptual loss is first pro-\nposed in style transfer and super-resolution tasks. This\nloss measures the semantic/content difference between\ninpainted and ground-truth images, and thus encour-\nages the inpainting generator to restore the semantics\nof missing regions. The perceptual loss is computed in\nhigh-level feature representations and is formulated as:\nLper =\nX\ni\n||Ψi(Iout)−Ψi(Igt)||1+||Ψi(Imer)−Ψi(Igt)||1,\n(6)\nwhere Ψi(∗) is the feature map of i-th layer in the VGG-\n16/19 network (Simonyan and Zisserman, 2014) pre-\ntrained on ImageNet (Deng et al., 2009). Instead of us-\ning the common VGG network, Suvorov et al. (2022)\nsuggested using a base network with a fast-growing re-\nceptive field for large-mask inpainting and utilized the\npre-trained segmentation network (ResNet50 with di-\nlated convolutions (Zhou et al., 2018)) to compute the\nso-called high receptive field perceptual loss. Note that,\nsome works used 2-norm in Eq. (6) to compute percep-\ntual loss.\nStyle loss. Similar to the perceptual loss, the style\nloss also depends on higher-level features extracted from\na pre-trained network. This loss is applied to penal-\nize the style difference between inpainted and ground-\ntruth images, e.g., texture details and common pat-\nterns. Mathematically, the style loss measures the sim-\nilarities of Gram matrices of image features, instead of\n14\nWeize Quan 1,2 et al.\nFig. 7: Some examples of image inpainting datasets.\nthe feature reconstruction in the perceptual loss. The\ndetailed formulation can be written,\nLsty =\nX\ni\n||Φi(Iout)−Φi(Igt)||1+||Φi(Imer)−Φi(Igt)||1,\n(7)\nwhere Φi(·) = Ψi(·)Ψi(·)T is the Gram matrix (Gatys\net al., 2016).\nBesides using Gram matrices to model the style in-\nformation, the mean and standard deviation of image\nfeatures are commonly used in style transfer (Huang\nand Belongie, 2017; Deng et al., 2020). The formula-\ntion is written as,\nLsty mean =\nX\ni\n||µ(Ψi(Iout)) − µ(Ψi(Igt))||2\n+||µ(Ψi(Imer)) − µ(Ψi(Igt))||2,\nLsty std =\nX\ni\n||σ(Ψi(Iout)) − σ(Ψi(Igt))||2\n+||σ(Ψi(Imer)) − σ(Ψi(Igt))||2,\nLsty meanstd = Lsty mean + Lsty std,\n(8)\nwhere µ(∗), σ(∗) are the mean and standard devia-\ntion, computed over spatial dimensions independently\nfor each sample and each channel.\nAdversarial loss. GANs (Goodfellow et al., 2014)\nare widely used in many image generation tasks. They\nemploy an adversarial loss to force the output distribu-\ntion to be close to the “real” distribution. The adver-\nsarial loss can counteract blurry results and enhance\nthe visual realism of the output image. Therefore, it\nis often applied in GAN-based inpainting networks. To\ncompute the adversarial loss, a discriminator network\n(D) is necessary, which interacts with the generator net-\nwork (G). The hinge version (Lim and Ye, 2017) of the\nadversarial loss can be formulated as:\nLD = EI∼pdata(I)\nh\nmax(0, 1 − D(Igt))\ni\n+EImer∼pImer (Imer)\nh\nmax(0, 1 + D(Imer))\ni\n,\n(9)\nwhere D(Igt) and D(Imer) are the logits output from\ndiscriminator D. The objective function for generator\nG can be denoted as:\nLG = −EImer∼pImer (Imer)\nh\nD(Imer)\ni\n.\n(10)\nExcept for the above hinge version, other types of ad-\nversarial losses are also adopted: GAN (Goodfellow et al.,\n2014), WGAN (Arjovsky et al., 2017), LSGAN (Mao\net al., 2017), etc.\n2.6 Datasets\nIn the literature, there are six prevalent and public\ndatasets for evaluating image inpainting. These datasets\ncover various types of images, including faces (CelebA\nand CelebA-HQ), real-world encountered scenes (Places2),\nstreet scenes (Paris), texture (DTD), and objects (Im-\nageNet). Several examples are shown in Fig. 7. The de-\ntails of the datasets are described below.\n– CelebA dataset (Liu et al., 2015): A large-scale face\nattribute dataset that contains 10,177 identities, each\nof which has about 20 images. In total, CelebA has\n202,599 face images, each with 40 attribute annota-\ntions.\n– CelebA-HQ dataset (Karras et al., 2018): The high-\nquality version of CelebA (Liu et al., 2015) with\nJPEG artifacts removal, super-resolution operation,\nand cropping, etc. This dataset consists of 30,000\nface images.\nDeep Learning-based Image and Video Inpainting: A Survey\n15\n– Places2 dataset (Zhou et al., 2017): A large-scale\nscene recognition dataset. Places365-Standard has\n365 scene categories. The training set has 1,803,460\nimages and the validation set contains 18,250 im-\nages.\n– Paris StreetView dataset (Doersch et al., 2012): This\ndataset consists of street-level imagery. It contains\n14,900 images for training and 100 images for test-\ning.\n– DTD dataset (Cimpoi et al., 2014): A describable\ntexture dataset consisting of 5,640 images. Accord-\ning to human perception, these images are divided\ninto 47 categories with 120 images per category.\n– ImageNet dataset (Deng et al., 2009): A large-scale\nbenchmark for object category classification. There\nare about 1.2 million training images and 50 thou-\nsand validation images.\n2.7 Evaluation Protocol\nThe evaluation metrics can be classified into two cat-\negories: pixel-aware metrics and (human) perception-\naware metrics. The former focus on the precision of\nreconstructed pixels, including ℓ1 error, ℓ2 error, and\nPSNR (peak signal-to-noise ratio), SSIM (the struc-\ntural similarity index) (Wang et al., 2004), and MS-\nSSIM (multi-scale SSIM) (Wang et al., 2003). The latter\npay more attention to the visual perception quality, in-\ncluding FID (Fr´echet inception distance) (Heusel et al.,\n2017), LPIPS (learned perceptual image patch similar-\nity) (Zhang et al., 2018b), P/U-IDS (paired/unpaired\ninception discriminative score) (Zhao et al., 2021), and\nuser study results. The detailed descriptions are given\nin the following.\n– ℓ1 error: The mean absolute differences between the\ncomplete image (Ic) and the ground-truth image\n(Ig).\n– ℓ2 error: The mean squared differences between the\ncomplete image and the ground-truth image.\n– PSNR: It is mainly used to measure the quality of\nreconstruction of the complete image. Its formula-\ntion is PSNR = 20 · log10(255) − 10 · log10(MSE),\nwhere MSE is the mean squared error between the\ncomplete image and the ground-truth image.\n– SSIM: Instead of estimating absolute errors, SSIM\nmeasures the similarity in structural information by\nincorporating luminance masking and contrast mask-\ning. It is written as SSIM =\n(2µIcµIg +c1)(2σIcIg +c2)\n(µ2\nIc+µ2\nIg +c1)(σ2\nIc+σ2\nIg +c2),\nwhere µ and σ refer to the average and the variance,\nrespectively; and c1 = 0.012 and c2 = 0.032 are two\nvariables to stabilize the division.\n– MS-SSIM: Dosselmann and Yang (2011) illustrated\nthat SSIM is very close to the windowed mean squared\nerror and Wang et al. (2003) highlighted the single-\nscale nature of SSIM as a drawback. As an alterna-\ntive, MS-SSIM embraces more flexibility for image\nquality assessment. To compute the MS-SSIM, two\ninput images are iteratively processed with low-pass\nfilters and downsampled with a stride of 2 (in to-\ntal, five scales). Then, the contrast comparison and\nstructure comparison are computed at each scale\nand the luminance comparison is calculated at the\nlast scale. These measurements are combined with\nappropriate weights (Wang et al., 2003).\n– FID: The Fr´echet inception distance compares two\nsets of images. It computes a Gaussian with mean\nand covariance (m, C) and a Gaussian (mg, Cg)\nfrom deep features of the set of completed images\nand the set of ground-truth images. Specifically, FID\nis defined as FID = ∥m − mg∥2\n2 + Tr(C + Cg −\n2(CCg)\n1\n2 ).\n– LPIPS: The distance of multi-layer deep features\nof complete and ground-truth images. Let Fc, Fg ∈\nRHl×Wl×Cl denote the channel-wise normalized fea-\ntures in the l-th layer, the LPIPS is given by LPIPS =\nP\nl\n1\nHlWl\nP\nh,w∥Wl ⊙(Fl\nchw −Fl\nghw)∥2\n2, where Wl ∈\nRCl is the channel weight vector.\n– P/U-IDS: The linear separability of complete and\nground-truth images in a pre-trained feature space.\nLet ϕ(·) denote the Inception v3 model mapping\nthe image to the 2048D feature space, f(·) be the\ndecision function of the SVM, the P-IDS is formu-\nlated as P-IDS = Pr{f(ϕ(Ic)) > f(ϕ(Ig)}. Due to\nthe unpaired nature, U-IDS is obtained by directly\ncalculating the misclassification rate.\n– User Study: FID, LPIPS, and P/U-IDS cannot be\nable to comprehensively evaluate the visual quality\nof complete images, therefore, a user study is often\nconducted to complement the above metrics. User\nstudies typically let a human chooses a preferred\nimage among two (or multiple) images generated\nfrom two (or multiple) competitors. Based on the\ncollected votes, the preference ratio is calculated for\ncomparison.\n2.8 Performance Evaluation\n2.8.1 Representative Image Inpainting Methods\nWe qualitatively and quantitatively compare some rep-\nresentative image inpainting methods: RFR (Li et al.,\n2020c), MADF (Zhu et al., 2021), DSI (Peng et al.,\n2021), CR-Fill (Zeng et al., 2021b), CoModGAN (Zhao\n16\nWeize Quan 1,2 et al.\nTable 2: Quantitative comparison of several representative image inpainting methods on CelebA-HQ and Places2.\n‡ Higher is better. † Lower is better. From M1 to M6, the mask ratios are 1%-10%, 10%-20%, 20%-30%, 30%-40%,\n40%-50%, and 50%-60%, respectively. Because of the heavy inference time, we do not show the results of RePaint\nfor M1, M2, M4, and M6.\nDataset\nCelebA-HQ\nPlaces2\nMask\nM1\nM2\nM3\nM4\nM5\nM6\nM1\nM2\nM3\nM4\nM5\nM6\nℓ1(%) †\nRFR\n1.59\n2.47\n3.58\n4.90\n6.44\n9.47\n0.83\n2.20\n3.93\n5.83\n7.96\n11.37\nMADF\n0.47\n1.30\n2.40\n3.72\n5.26\n8.43\n0.80\n2.18\n3.96\n5.91\n8.10\n11.68\nDSI\n0.60\n1.65\n3.08\n4.80\n6.83\n11.11\n0.88\n2.42\n4.48\n6.75\n9.32\n13.82\nCR-Fill\n0.79\n2.15\n3.95\n6.01\n8.33\n13.18\n0.78\n2.17\n4.02\n6.11\n8.46\n12.43\nCoModGAN\n0.48\n1.38\n2.66\n4.28\n6.20\n10.53\n0.72\n2.05\n3.83\n5.89\n8.27\n12.58\nLGNet\n0.46\n1.28\n2.38\n3.72\n5.27\n8.38\n0.68\n1.89\n3.51\n5.33\n7.41\n10.86\nMAT\n0.83\n1.74\n3.00\n4.52\n6.30\n9.98\n1.07\n2.53\n4.48\n6.69\n9.20\n13.70\nRePaint\n-\n-\n3.37\n-\n7.47\n-\n-\n-\n4.96\n-\n10.01\n15.27\nPSNR ‡\nRFR\n36.39\n31.87\n29.07\n26.87\n25.09\n22.51\n35.74\n30.24\n27.24\n25.13\n23.48\n21.33\nMADF\n39.68\n33.77\n30.42\n27.95\n25.99\n23.07\n36.17\n30.37\n27.17\n25.00\n23.31\n21.10\nDSI\n37.68\n31.74\n28.39\n25.88\n23.91\n20.87\n35.40\n29.47\n26.15\n23.91\n22.19\n19.75\nCR-Fill\n35.67\n29.87\n26.60\n24.29\n22.53\n19.70\n36.35\n30.32\n26.96\n24.63\n22.85\n20.50\nCoModGAN\n39.56\n33.15\n29.41\n26.62\n24.49\n21.16\n37.00\n30.82\n27.35\n24.92\n23.05\n20.43\nLGNet\n40.04\n33.99\n30.54\n27.99\n26.01\n23.12\n37.62\n31.61\n28.18\n25.84\n24.05\n21.69\nMAT\n38.44\n32.62\n29.21\n26.70\n24.72\n21.78\n35.66\n29.76\n26.41\n24.09\n22.30\n19.81\nRePaint\n-\n-\n28.38\n-\n23.16\n-\n-\n-\n26.04\n-\n21.72\n18.99\nSSIM ‡\nRFR\n0.991\n0.976\n0.957\n0.932\n0.902\n0.834\n0.983\n0.952\n0.911\n0.862\n0.805\n0.699\nMADF\n0.995\n0.984\n0.967\n0.945\n0.917\n0.848\n0.984\n0.953\n0.910\n0.859\n0.800\n0.690\nDSI\n0.992\n0.976\n0.951\n0.918\n0.877\n0.778\n0.982\n0.945\n0.892\n0.832\n0.763\n0.636\nCR-Fill\n0.988\n0.965\n0.931\n0.890\n0.842\n0.729\n0.985\n0.954\n0.909\n0.855\n0.794\n0.675\nCoModGAN\n0.994\n0.981\n0.960\n0.929\n0.891\n0.792\n0.987\n0.957\n0.914\n0.860\n0.796\n0.671\nLGNet\n0.995\n0.985\n0.968\n0.945\n0.917\n0.849\n0.988\n0.963\n0.925\n0.878\n0.823\n0.714\nMAT\n0.993\n0.980\n0.959\n0.931\n0.897\n0.814\n0.983\n0.948\n0.898\n0.839\n0.772\n0.645\nRePaint\n-\n-\n0.952\n-\n0.867\n-\n-\n-\n0.892\n-\n0.750\n0.606\nMS-SSIM ‡\nRFR\n0.992\n0.976\n0.956\n0.933\n0.900\n0.830\n0.986\n0.960\n0.924\n0.880\n0.828\n0.731\nMADF\n0.994\n0.983\n0.966\n0.942\n0.913\n0.846\n0.987\n0.961\n0.923\n0.877\n0.824\n0.722\nDSI\n0.992\n0.976\n0.952\n0.919\n0.878\n0.784\n0.984\n0.952\n0.905\n0.850\n0.785\n0.664\nCR-Fill\n0.987\n0.963\n0.928\n0.887\n0.839\n0.732\n0.987\n0.960\n0.920\n0.872\n0.814\n0.704\nCoModGAN\n0.994\n0.980\n0.958\n0.926\n0.888\n0.793\n0.988\n0.961\n0.921\n0.870\n0.810\n0.692\nLGNet\n0.995\n0.984\n0.968\n0.945\n0.917\n0.851\n0.990\n0.968\n0.935\n0.894\n0.844\n0.744\nMAT\n0.994\n0.980\n0.960\n0.932\n0.898\n0.818\n0.986\n0.957\n0.913\n0.859\n0.796\n0.676\nRePaint\n-\n-\n0.953\n-\n0.870\n-\n-\n-\n0.903\n-\n0.771\n0.633\nFID †\nRFR\n0.86\n1.68\n2.67\n3.77\n5.21\n7.60\n2.62\n5.99\n9.47\n12.90\n16.62\n22.13\nMADF\n0.52\n1.55\n3.28\n5.43\n8.35\n13.54\n2.15\n5.58\n9.20\n13.08\n17.36\n24.42\nDSI\n0.59\n1.58\n3.01\n4.50\n6.51\n9.76\n2.51\n6.52\n11.35\n15.99\n21.75\n29.38\nCR-Fill\n1.06\n2.86\n5.26\n7.79\n11.23\n19.52\n2.37\n6.24\n10.54\n15.17\n20.36\n26.43\nCoModGAN\n0.44\n1.25\n2.45\n3.65\n5.03\n6.89\n2.11\n5.63\n9.58\n13.65\n17.68\n22.58\nLGNet\n0.39\n1.06\n2.08\n3.16\n4.61\n7.07\n1.97\n5.25\n8.90\n13.02\n17.60\n25.99\nMAT\n0.41\n1.13\n2.05\n2.96\n4.05\n5.43\n2.13\n5.47\n9.26\n13.00\n16.62\n21.88\nRePaint\n-\n-\n2.14\n-\n4.24\n-\n-\n-\n8.85\n-\n15.90\n21.58\nLPIPS †\nRFR\n0.015\n0.028\n0.042\n0.060\n0.081\n0.118\n0.021\n0.047\n0.074\n0.106\n0.142\n0.201\nMADF\n0.009\n0.025\n0.048\n0.077\n0.109\n0.168\n0.014\n0.038\n0.068\n0.102\n0.141\n0.209\nDSI\n0.010\n0.026\n0.048\n0.074\n0.104\n0.160\n0.018\n0.047\n0.085\n0.125\n0.169\n0.242\nCR-Fill\n0.017\n0.043\n0.074\n0.107\n0.143\n0.212\n0.016\n0.042\n0.076\n0.114\n0.156\n0.226\nCoModGAN\n0.008\n0.022\n0.041\n0.065\n0.092\n0.143\n0.016\n0.044\n0.080\n0.121\n0.164\n0.236\nLGNet\n0.006\n0.017\n0.031\n0.048\n0.069\n0.108\n0.014\n0.035\n0.064\n0.096\n0.132\n0.198\nMAT\n0.007\n0.019\n0.035\n0.054\n0.077\n0.120\n0.014\n0.040\n0.073\n0.111\n0.152\n0.224\nRePaint\n-\n-\n0.038\n-\n0.093\n-\n-\n-\n0.077\n-\n0.167\n0.259\nDeep Learning-based Image and Video Inpainting: A Survey\n17\nFig. 8: Qualitative comparison of representative image inpainting methods on CelebA-HQ (the first three rows)\nand Places2 (the last four rows).\nTable 3: Model computational complexity statistics.\nModel\n#Parameter GPU Memory Infer. time\nRFR\n30.59 M\n1.23 G\n28.95 ms\nMADF\n85.14 M\n2.42 G\n15.59 ms\nDSI\n70.32 M\n6.54 G\n40.20 s\nCR-Fill\n4.10 M\n0.96 G\n9.18 ms\nCoModGAN\n79.80 M\n1.71 G\n42.24 ms\nLGNet\n115.00 M\n1.52 G\n13.59 ms\nMAT\n59.78 M\n1.69 G\n78.35 ms\nRePaint\n552.81 M\n4.14 G\n6 min 30 s\net al., 2021), LGNet (Quan et al., 2022), MAT (Li\net al., 2022b), RePaint (Lugmayr et al., 2022). The test\nmask is from (Liu et al., 2018). Specifically, RFR fol-\nlows a progressive inpainting strategy, MADF adopts a\nmask-aware design, DSI generates stochastic structures\nwith hierarchical vq-vae, CR-Fill designs an attention-\nfree generator, CoModGAN embeds the known content\nof corrupted images into style vectors of styleGAN2,\nLGNet introduces local and global refinement networks\nwith different receptive fields, MAT designs a mask-\naware transformer architecture, and RePaint utilizes a\npre-trained unconditional diffusion model.\nTable 2 reports the quantitative results of these ad-\nvanced image inpainting methods on CelebA-HQ and\nPlaces2 datasets. In this experiment, we use the irreg-\nular masks shared by (Liu et al., 2018) for the evalu-\nation. From this table, we can find that MS-SSIM is\nvery close to SSIM in the CelebA-HQ dataset; MS-\nSSIM is consistently higher than SSIM in the Places2\ndataset and this phenomenon is more apparent for large\nmasks. The reason may be that face images are rel-\n18\nWeize Quan 1,2 et al.\nTable 4: Quantitative comparison of different loss functions on CelebA-HQ (“C”) and Paris StreetView (“P”). ‡\nHigher is better. † Lower is better. “16” refers to Eq. (3) with α = 6, and the remaining loss settings both include\n“16” (We omit it for simplicity). “percept” refers to Eq. (6) based on pretrained VGG16; “resnetpl” refers to\nEq. (6) based on the pre-trained segmentation network ResNet50, which is proposed by (Suvorov et al., 2022).;\n“style” refers to Eq. (7); “stylemeanstd” refers to Eq. (8); “percept style” refers to Eq. (6) plus Eq. (7); “lsgan”\nrefers to Eq. (9) and (10). Different percentage numbers in the first row refer to the hole ratios, where a large\nnumber implies large missing regions. Following the common setting, the test mask is from (Liu et al., 2018).\nMask\n1%-10%\n10%-20%\n20%-30%\n30%-40%\n40%-50%\n50%-60%\nDataset\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nC\nP\nℓ1(%) †\n16\n0.46\n0.57\n1.26\n1.53\n2.34\n2.81\n3.63\n4.25\n5.14\n5.93\n8.37\n9.07\npercept\n0.45\n0.57\n1.24\n1.53\n2.30\n2.81\n3.58\n4.26\n5.08\n5.95\n8.33\n9.11\nresnetpl\n0.45\n0.59\n1.25\n1.58\n2.32\n2.88\n3.60\n4.35\n5.10\n6.06\n8.35\n9.21\nstyle\n0.48\n0.59\n1.33\n1.60\n2.47\n2.97\n3.85\n4.53\n5.45\n6.37\n8.86\n9.78\nstylemeanstd\n0.46\n0.59\n1.27\n1.60\n2.39\n2.96\n3.75\n4.51\n5.35\n6.34\n8.80\n9.75\npercept style\n0.47\n0.60\n1.30\n1.63\n2.43\n3.00\n3.79\n4.58\n5.40\n6.42\n8.83\n9.84\nlsgan\n0.47\n0.59\n1.30\n1.61\n2.42\n2.97\n3.76\n4.52\n5.33\n6.34\n8.67\n9.72\nPSNR ‡\n16\n40.03\n38.74\n34.13\n33.17\n30.76\n29.92\n28.27\n27.67\n26.29\n25.88\n23.23\n23.28\npercept\n40.14\n38.77\n34.19\n33.17\n30.81\n29.91\n28.31\n27.65\n26.33\n25.85\n23.23\n23.25\nresnetpl\n40.12\n38.56\n34.18\n33.03\n30.80\n29.83\n28.29\n27.61\n26.31\n25.83\n23.23\n23.26\nstyle\n39.63\n38.36\n33.65\n32.71\n30.24\n29.37\n27.72\n27.07\n25.74\n25.22\n22.71\n22.62\nstylemeanstd\n39.91\n38.38\n33.89\n32.81\n30.42\n29.49\n27.85\n27.20\n25.83\n25.35\n22.72\n22.70\npercept style\n39.78\n38.20\n33.74\n32.60\n30.31\n29.30\n27.76\n27.01\n25.76\n25.20\n22.68\n22.58\nlsgan\n39.71\n38.40\n33.73\n32.78\n30.39\n29.50\n27.91\n27.18\n25.96\n25.35\n22.95\n22.72\nSSIM ‡\n16\n0.995\n0.991\n0.985\n0.973\n0.969\n0.946\n0.948\n0.911\n0.921\n0.867\n0.847\n0.767\npercept\n0.995\n0.991\n0.985\n0.973\n0.970\n0.946\n0.949\n0.911\n0.921\n0.866\n0.847\n0.765\nresnetpl\n0.995\n0.991\n0.985\n0.972\n0.970\n0.945\n0.948\n0.909\n0.921\n0.865\n0.848\n0.764\nstyle\n0.995\n0.991\n0.983\n0.971\n0.966\n0.940\n0.943\n0.902\n0.913\n0.853\n0.834\n0.746\nstylemeanstd\n0.995\n0.991\n0.984\n0.971\n0.968\n0.941\n0.944\n0.903\n0.914\n0.854\n0.835\n0.747\npercept style\n0.995\n0.990\n0.984\n0.970\n0.967\n0.940\n0.943\n0.901\n0.913\n0.852\n0.834\n0.745\nlsgan\n0.995\n0.991\n0.984\n0.971\n0.967\n0.941\n0.944\n0.903\n0.915\n0.854\n0.839\n0.746\nFID †\n16\n0.56\n4.74\n1.57\n13.74\n3.31\n26.55\n5.38\n40.79\n8.37\n57.49\n15.18\n86.51\npercept\n0.53\n4.64\n1.51\n13.41\n3.20\n26.13\n5.22\n40.35\n8.18\n57.22\n14.63\n88.10\nresnetpl\n0.52\n4.62\n1.47\n13.23\n3.11\n25.60\n5.13\n39.08\n7.99\n54.75\n13.81\n83.93\nstyle\n0.42\n3.91\n1.13\n10.67\n2.25\n19.65\n3.38\n28.87\n5.00\n39.09\n7.90\n57.00\nstylemeanstd\n0.44\n4.21\n1.21\n11.42\n2.38\n20.54\n3.65\n29.68\n5.36\n39.59\n8.55\n56.38\npercept style\n0.40\n3.98\n1.13\n10.91\n2.26\n19.76\n3.42\n29.12\n5.07\n39.28\n7.87\n57.07\nlsgan\n0.54\n4.26\n1.57\n11.72\n3.34\n21.54\n5.57\n31.21\n8.85\n41.80\n16.03\n60.01\nLPIPS †\n16\n0.011\n0.016\n0.032\n0.048\n0.063\n0.091\n0.102\n0.142\n0.144\n0.197\n0.222\n0.298\npercept\n0.010\n0.015\n0.029\n0.045\n0.057\n0.086\n0.092\n0.134\n0.129\n0.185\n0.200\n0.279\nresnetpl\n0.009\n0.015\n0.027\n0.044\n0.052\n0.083\n0.083\n0.126\n0.116\n0.174\n0.174\n0.259\nstyle\n0.007\n0.011\n0.018\n0.031\n0.033\n0.056\n0.052\n0.086\n0.073\n0.120\n0.117\n0.186\nstylemeanstd\n0.008\n0.013\n0.021\n0.035\n0.037\n0.062\n0.055\n0.092\n0.076\n0.125\n0.119\n0.189\npercept style\n0.006\n0.011\n0.018\n0.032\n0.033\n0.057\n0.052\n0.087\n0.074\n0.122\n0.118\n0.188\nlsgan\n0.009\n0.013\n0.028\n0.036\n0.054\n0.066\n0.086\n0.098\n0.123\n0.135\n0.196\n0.208\natively regular and uniform compared to the natural\nscene images in Places2, and thus the latter is more\nsensitive to structural similarity with different scales.\nAmong these methods, MAT and RePaint have rela-\ntively superior FID, especially for large masks (> 30%),\nwhile CoModGAN and LGNet perform better in PSNR.\nFor DSI, the inpainting performance on CelebA-HQ is\nslightly better than that on Places2, and the possible\nreason is that the structure of face images is easier to\nmodel than diverse natural scene images. CR-Fill has\nlimited inpainting performance.\nFig. 8 shows the visual results of some represen-\ntative image inpainting methods on CelebA-HQ and\nPlaces2 datasets. MADF adopts a mask-aware design,\nwhich can predict reasonable structures (the second and\nthird rows), but has limited ability for detail restora-\ntion. By contrast, MAT has better inpainting perfor-\nmance with mask-aware transformer blocks. Through\nintroducing local and global refinement with different\nreceptive fields, LGNet can perceive local details (the\nblack stroke in the first row) and global structure (the\nsecond row). For large missing regions, RFR can re-\ncover the helicopter rotor blade (the fourth row) and\nDeep Learning-based Image and Video Inpainting: A Survey\n19\nFig. 9: Qualitative comparison of different loss functions on CelebA-HQ (the first two rows) and Paris StreetView\n(the last two rows). “StyleMS” refers to “stylemeanstd”; “Per Style” refers to “Percept style”.\nwaterfall (the fifth row) with progressive inpainting.\nWith the help of the generative capability of the uncon-\nditional modulated model (StyleGAN2), CoModGAN\ndemonstrates relatively good inpainting performance\n(the fourth and sixth rows). DSI can perceive the struc-\nture with hierarchical VQ-VAE (the third and fourth\nrows). Based on the powerful generation ability of the\ndiffusion model, RePaint can correctly infer the missing\nbackground (the sixth row) and the human body (the\nseventh row). Interestingly, it may have an incorrect se-\nmantic prediction (a woman’s head in the waterfall of\nthe fifth row). Due to the implicit attention mechanism\nand simple network, CR-Fill achieves comparatively in-\nferior inpainted results, which is also consistent with the\nquantitative comparisons as shown in Table 2.\nIn addition, we evaluate the computational com-\nplexity of the representative inpainting methods in terms\nof the number of parameters, GPU memory of single im-\nage inference, and inference time on a GPU (the time of\na forward pass through the networks. The statistical re-\nsults are shown in Table 3. CR-Fill implicitly learns the\npatch-borrowing behavior without an attention layer,\nits model is the smallest and thus needs less GPU mem-\nory and running time. Because RePaint is based on a\ndiffusion model, it has the largest number of parame-\nters and a very long inference time. The GPU memory\nand inference time of DSI are also very high. LGNet\nfollows a coarse-to-fine framework with local and global\nrefinement, therefore, the number of parameters is high.\nThe running time of MAT and CoModGAN is relatively\nhigh because the former conducts many attention com-\nputations and the latter has multiple style modulations\nwith progressive growing. RFR and MADF are in the\nmiddle.\n2.8.2 Loss Functions\nAs summarized in Sec. 2.5, many loss functions have\nbeen proposed for image inpainting. In this part, we\nevaluate the effect of each loss term. We train an in-\npainting network with different loss settings on the CelebA-\nHQ and Paris StreetView datasets. This network con-\nsists of two downsampling layers, 11 ResNet residual\nblocks with dilation, and two upsampling layers. The\ncorresponding numerical results are reported in Table 4.\nIn the case of masks at 1%-10%, the SSIM values of dif-\nferent loss settings are (almost) the same for CelebA-\nHQ and Paris StreetView datasets. The reason is that\ndifferent loss settings only have a slight impact on the\ninpainting of very small missing regions. We can see\nthat pixel-wise reconstruction loss (“16”) provides the\nbaseline performance. After adding the perceptual loss\n(“percept”), FID and LPIPS are improved. Compared\nwith “percept”, “resnetpl” achieves slightly better re-\nsults, especially for the large mask. The style loss can\nremarkably decrease the FID and LPIPS at the ex-\npense of PSNR and SSIM. In other words, there ex-\nists a trade-off between pixel-wise reconstruction loss\n20\nWeize Quan 1,2 et al.\nFig. 10: Two representative examples of object removal.\nand style loss, where the former focuses on low-level\npixel recovery, and the latter emphasizes visual qual-\nity. A similar finding is reported and studied in (Blau\nand Michaeli, 2018). In addition, combining the per-\nceptual loss with style loss (“percept style”) has a very\nslight effect on the results compared to only style loss\n(“style”). The style loss based on Gram matrix (“style”)\nand style loss based on mean and standard deviation\n(“stylemeanstd”) have comparable results. Comparing\nwith adversarial loss (“lsgan”), style loss (“style”) ob-\ntain significantly lower FID and LPIPS.\nFig. 9 illustrates the corresponding qualitative com-\nparison. “16” fills the missing regions with smooth struc-\ntures and textures. After introducing the content loss,\nthis phenomenon is slightly improved, for example, the\nnose and mouth of the first row are better recovered\nin column “Percept”. Compared with “Percept”, the\ninpainted results of “Resnetpl” have slightly improved\nvisual quality, which is attributed to the perceptual loss\ncomputation with higher receptive field (Suvorov et al.,\n2022). We can find that the results of “Style” are sig-\nnificantly superior to the previous three columns, es-\npecially for the restoration of texture details. This is\nconsistent with the numerical results in Table 4. For\nthree settings with style loss, i.e., “Style”, “StyleMS”,\nand “Per Style”, “Style” and “Per Style” are on par,\n“StyleMS” is slightly worse. The performance of “LS-\nGan” is in between “Percept” and “Style”.\n2.9 Inpainting-based Applications\nImage inpainting can be used in many real-world appli-\ncations, such as object removal, text editing, old photo\nrestoration, image compression, text-guided image edit-\ning, etc.\nFig. 11: Representative samples of text editing.\n2.9.1 Object Removal\nAlmost all image editing tools include the function of\nobject removal, which is directly accomplished with im-\nage inpainting. To illustrate the capability of several\ncurrent inpainting methods on the object removal ap-\nplication, we apply the respective trained models to re-\nmove objects from selected real-world images with dif-\nferent scenes, and the corresponding results are shown\nin Fig. 10. The first row is generated by CNN-based\nmethod (Suvorov et al., 2022) and the second one is in-\npainted by a transformer-based method (Zheng et al.,\n2022a). These two methods can achieve visually re-\nalistic results, successfully removing the objects high-\nlighted with shadow markers.\n2.9.2 Text Editing\nOn social media sites, users often share their pictures\nand also want to hide their personal information for\nprivacy. For real-time text translation applications in\nsmartphones, the original content needs to be replaced\nwith the translated version. These text editing-related\ntasks can be solved via inpainting techniques. Fig. 11(a)\nshows the results of text removal with the method pro-\nposed by (Quan et al., 2022), and Fig. 11(b) illustrates\ntwo samples of text replacement from (Wu et al., 2019).\nThese results have a pleasing visual quality.\n2.9.3 Old Photo Restoration\nPhotos are helpful to record important moments. Un-\nfortunately, some photos are damaged over time, result-\ning in various missing regions. Image inpainting can be\nused to recover these incomplete photos automatically.\nIt is difficult to collect paired training data for this task,\ntherefore, we synthesize old photos using the Pascal\nVOC dataset Everingham et al. (2015) inspired by Wan\net al. (2020). Specifically, we collect some paper and\nDeep Learning-based Image and Video Inpainting: A Survey\n21\nFig. 12: Several representative examples of old photo\nrestoration.\nscratch texture images to simulate the realistic defects\nin the old photos. To blend the above texture images\nwith the VOC images, we randomly choose a mode from\nthree candidates (screen, lighten-only, and layer addi-\ntion) with a random opacity. In addition, some opera-\ntions, e.g., random flipping, random position, rescaling,\ncropping, etc, are also used for augmenting the diversity\nof texture images. To this end, the paired samples of the\noriginal VOC images and the corresponding blended re-\nsults are used for training the inpainting network (Quan\net al., 2022). Fig. 12 shows several examples of old photo\nrestoration, where the inpainting method restores the\noriginal appearance of old photos.\n2.9.4 Image Compression\nImage compression is a fundamental image processing\ntechnique to reduce the cost of storage or transmission\nof digital images. This technique mainly consists of two\nstages: compression and reconstruction. The former re-\nduces the data size to obtain a sparse image representa-\ntion and the latter reconstructs the original image. Dif-\nferent from waveform-based methods, Carlsson (1988)\nproposed a sketch-based method to obtain a sparse rep-\nresentation and reconstructed images via an interpo-\nlation process. Gali´c et al. (2008) introduced partial\ndifferential equation (PDE)-based inpainting to image\ncompression, where image coding and decoding both\nare based on edge-enhancing anisotropic diffusion. Re-\ncently, some researchers (Baluja et al., 2019; Dai et al.,\n2020; Schrader et al., 2023) applied deep learning meth-\nods to generate the sampling mask and reconstruct the\nimage with an inpainting network. Fig. 13 shows several\nexamples of image compression with inpainting, where\nthe reconstructed images have good quality based on\nadaptive sparse sampling with inpainting.\nFig. 13: Two representative examples of image compres-\nsion with inpainting. From left to right: input image,\nsampling mask, sampled image, and reconstructed im-\nage. Images come from (Dai et al., 2020).\nFig. 14: Selected examples of text-based image editing.\nEach group includes the input image with mask (red\ntransparent) and text prompts and edited results. Im-\nages come from (Xie et al., 2023).\n2.9.5 Text-guided image editing\nImage inpainting is a basic processing tool for image\nediting. Recent generative models based on probabilis-\ntic diffusion have the powerful capability of text-to-\nimage generation, which provides the potential for text-\nguided image editing with diffusion model-based im-\nage inpainting approaches. For example, diffusion-based\nSmartBrush (Xie et al., 2023) edited images with the\nguidance of text and shape. Fig. 14 illustrates several\nsamples generated by SmartBrush. The first row adds\nnew objects and the second row replaces original ob-\njects with new contents. We can see that the edited\nresults have high visual realism and are consistent with\ntext prompts.\n22\nWeize Quan 1,2 et al.\n3 Video Inpainting\n3.1 Method\nUnlike images, videos have an additional temporal di-\nmension which provides extra information about ob-\njects or camera movement. This information helps net-\nworks to obtain a better understanding of the context\nof the video. Therefore, the video inpainting task aims\nto ensure both spatial consistency and temporal co-\nherence. Existing deep learning-based video inpainting\nmethods can be roughly divided into four categories: 3D\nCNN-based approaches, shift-based approaches, flow-\nguided approaches, and attention-based approaches. We\nrefer the readers to more conventional methods in (Ilan\nand Shamir, 2015).\n3.1.1 3D CNN-based Approaches\nTo deal with the temporal dimension, researchers pro-\nposed 3D CNN-based approaches, which often combine\ntemporal restoration and image inpainting. Wang et al.\n(2019a) proposed a two-stage pipeline to jointly infer\ntemporal structure and spatial texture details. The first\nsub-network processes the low-resolution videos with a\n3D CNN, and the second sub-network completes the\noriginal-resolution video frames with an extended 2D\ninpainting network (Iizuka et al., 2017). Inspired by\nthe gated convolution in image inpainting (Yu et al.,\n2019), Chang et al. (2019a) proposed a 3D gated con-\nvolution and a temporal SN-PatchGAN for free-form\nvideo inpainting. They also integrated the perceptual\nloss (Johnson et al., 2016) and style loss (Gatys et al.,\n2016) into the training objective. Hu et al. (2020) pro-\nposed a two-stage video inpainting network, where they\nobtain a coarse inpainting result with a 3D CNN and\nthen fuse inpainting proposals generated by matching\nvalid pixels and pixels in coarse inpainting results.\n3.1.2 Shift-based Approaches\nConsidering the high computational cost of 3D con-\nvolution, Lin et al. (2019) proposed a generic tempo-\nral shift module (TSM) to capture temporal relation-\nships with high efficiency. This technique is extended\nfor video inpainting. Chang et al. (2019b) developed\na learnable gated TSM, which combines a TSM with\nlearnable shifting kernels and gated convolution (Yu\net al., 2019). They also equipped the 2D convolution\nlayers in SN-PatchGAN (Yu et al., 2019) with gated\nTSM. However, TSM often leads to blurry content due\nto misaligned features. To solve this, Zou et al. (2021)\nproposed a spatially-aligned TSM (TSAM), aligning\nfeatures to the current frame after shifting features. The\nalignment process is based on estimated flow with a va-\nlidity mask. Ouyang et al. (2021) applied an internal\nlearning strategy for video inpainting, which implicitly\nlearns the information shift from valid regions to un-\nknown parts in a single video sample. They also de-\nsigned the gradient regularization term and the anti-\nambiguity loss term for temporal consistency recon-\nstruction and realistic detail generation. Ke et al. (2021)\npresented an occlusion-aware video object inpainting\nmethod. Specifically, they completed the object shape\nwith a transformer-based network, recovered the flow\nwithin the completed object region under the guidance\nof the object contour, and filled missing content with an\nocclusion-aware TSM after the flow-guided pixel prop-\nagation.\n3.1.3 Flow-guided Approaches\nOptical flow is a common tool to model the temporal in-\nformation in videos, which is also applied to solve video\ninpainting. Based on the completed flow, the missing\npixels in the current frame can be filled by propagat-\ning pixels from neighboring frames. Kim et al. (2019b)\nmodeled the video inpainting task as a multi-to-single\nframe inpainting problem and proposed a 3D-2D encoder-\ndecoder network VINet. This network includes several\nflow and mask sub-networks in a progressive manner.\nThey also introduced the flow and warp loss to further\nenforce temporal consistency. Chang et al. (2019c) pro-\nposed a three-stage video inpainting framework consist-\ning of a warping network, an inpainting network, and\na refinement network. In the warping network, bilinear\ninterpolation is used to recover background flow with-\nout learning. Then the refinement network selected the\nbest candidate from two frames completed by warp-\ning and inpainting network to generate the final out-\nput. Zhang et al. (2019a) applied internal learning to\ninfer both frames and flow from input random noise\nand used flow generation loss to enhance temporal co-\nherence. Xu et al. (2019) proposed a flow-guided com-\npletion framework consisting of three steps. It first fills\nthe incomplete optical flow with stacked CNN networks,\nthen propagates pixels from known regions to holes with\ninpainted flow guidance, and finally completes unseen\nregions with an image inpainting network (Yu et al.,\n2019). To reduce the over-smoothing in the boundary\nregions during flow completion, they leveraged hard\nflow example mining to encourage the network to pro-\nduce sharp edges. To solve the same problem, Gao et al.\n(2020) explicitly completed motion edges and used them\nto guide flow completion. In addition, they introduced\nDeep Learning-based Image and Video Inpainting: A Survey\n23\na non-local flow connection to enable content propaga-\ntion from distant frames.\nThese previous methods cannot guarantee the con-\nsistency of flow, and even small errors in the flow may\nlead to geometric distortion in the video. Inspired by\nthis, Lao et al. (2021) transformed the background of a\n3D scene to a 2D scene template and learned the map-\nping of the template to the mask in the image. Given\nthat the complex motion of objects between consecu-\ntive frames will increase the difficulty to recover flow,\nZhang et al. (2022b) introduced an inertia prior in flow\ncompletion to align and aggregate flow features. To al-\nleviate the spatial incoherence problem, they proposed\nan adaptive style fusion network to correct the distribu-\ntion in the warped regions with the guidance of feature\ndistribution in valid regions. Kang et al. (2022) offset\nthe weaknesses of the error accumulation of a multi-\nstage pipeline in flow-based methods by introducing an\nerror compensation strategy, which iteratively detects\nand corrects the inconsistency errors during the flow-\nguided pixel propagation.\nThe above hand-crafted flow-based methods restored\nvideos with high computation and memory consump-\ntion because these processes cannot be accelerated by\nGPU. To speed up training and inference, Li et al.\n(2022d) proposed an end-to-end framework. They prop-\nagated features based on completed flow in low resolu-\ntion and used deformable convolution to decrease the\ndistortion caused by errors in flow. The temporal focal\ntransformer blocks were stacked to aggregate local and\nnon-local features.\n3.1.4 Attention-based Approaches\nThe attention mechanism is often applied to model the\ncontextual information and enlarge the spatial-temporal\nwindow. Oh et al. (2019) recurrently calculated the at-\ntention scores between the target and reference frames,\nand progressively filled holes of the target frame from\nthe boundary. Lee et al. (2019) firstly aligned frames by\nan affine transformation, and then copied pixels based\non the similarity between the target frame and aligned\nreference frames. Woo et al. (2020) proposed a coarse-\nto-fine framework for video inpainting. The first stage\nroughly recovers the target holes based on the com-\nputed homography between the target and reference\nframes, and the second stage refines the filled contents\nwith non-local attention. They also introduced an op-\ntical flow estimator to enhance temporal consistency.\nConsidering the motion of the foreground objects is di-\nverse, the choice of reference frames becomes more im-\nportant. While other methods take neighboring frames\nor frames in a specific distance as reference frames, Li\net al. (2020a) dynamically updated long-term reference\nframes after aggregating short-term aligned features.\nInstead of a frame-by-frame inpainting strategy, Zeng\net al. (2020a) adopted a “multi-to-multi” mechanism to\nfill in the holes in all input frames. Specifically, they pro-\nposed a spatial-temporal transformer network (STTN)\nto compute attention in both spatial and temporal di-\nmensions. Based on STTN (Zeng et al., 2020a), Liu\net al. (2021b) separated feature maps into overlapping\npatches, enabling more interactions between neighbor-\ning patches. In addition, they modified the common\ntransformer block by inserting soft split and soft com-\nposition modules into the feed-forward network. Chen\net al. (2021) proposed an interactive video inpainting\nmethod to jointly perform object segmentation and video\ninpainting with user guidance. For network design, they\nintroduce a spatial time attention block to update the\ntarget frames’ features with the reference frames’ fea-\ntures. Zhang et al. (2022a) designed a flow-guided trans-\nformer to combine the flow and the attention. They first\nutilized the completed flow to propagate pixels from\nneighboring frames, and then synthesized the remaining\nmissing regions with a flow-guided spatial transformer\nand a temporal transformer.\nThese attention-based methods still suffer from blurry\ncontent in high frequency due to mapping videos into\na continuous feature space. By learning a specific code-\nbook for each video and using subscripts of code to\nrepresent images, Ren et al. (2022) transformed videos\nto a discrete latent space. Then a discrete latent trans-\nformer was applied to infer content in masked regions.\nTable 5 summarizes the technical details of existing\nvideo inpainting methods.\n3.2 Loss Functions\nVideo inpainting is very close to image inpainting. There-\nfore, many loss functions for training image inpainting\nnetworks are also applied to train video inpainting mod-\nels, including reconstruction loss, GAN loss, perceptual\nloss, and style loss. To complete the corrupted flow, two\nlosses are often used:\nFlow loss. Similar to the image reconstruction loss,\nthe flow loss measures the difference between inpainted\nflow and its ground-truth version, which is defined as:\nLflow = ||Oi,j ⊙ (Fi,j − ˆFi,j)||1,\n(11)\nwhere ˆFi,j is the inpainted optical flow from frame i to\nframe j, Fi,j is the ground-truth flow estimated by pre-\ntrained flow estimation networks, e.g., FlowNet2 (Ilg\net al., 2017) and PWC-Net (Sun et al., 2018a), and Oi,j\ndenotes the occlusion map obtained by the forward-\nbackward consistency check.\n24\nWeize Quan 1,2 et al.\nTable 5: Summary of video inpainting methods. Like image inpainting, we also split existing video inpainting\napproaches into three types according to the number of stages: 1) one-stage framework ( 1 ) usually designs\na generator to recover the missing contents for each frame; 2) two-stage framework ( 2 ) often consists of two\nnetworks for different purposes; and 3) multi-stage framework ( m ) splits video inpainting into multiple steps.\nCategory\nMethod\nStage\nLoss details\nL1 loss\nGAN loss\nPerceptual loss\nStyle loss\nTV loss\nFlow loss\nWarp loss\n3D CNN\nWang et al. (2019a)\n2\n✓\nChang et al. (2019a)\n1\n✓\n✓\n✓\n✓\nHu et al. (2020)\n2\n✓\n✓\nShift\nChang et al. (2019b)\n1\n✓\n✓\n✓\n✓\nZou et al. (2021)\n1\n✓\n✓\n✓\n✓\nOuyang et al. (2021)\n1\n✓\nKe et al. (2021)\nm\n✓\n✓\n✓\n✓\nFlow\nKim et al. (2019b)\n1\n✓\n✓\n✓\nChang et al. (2019c)\nm\n✓\n✓\n✓\nZhang et al. (2019a)\n1\n✓\n✓\n✓\n✓\nXu et al. (2019)\nm\n✓\nGao et al. (2020)\nm\n✓\n✓\nLao et al. (2021)\n2\n✓\n✓\nZhang et al. (2022b)\nm\n✓\n✓\n✓\n✓\nLi et al. (2022d)\n1\n✓\n✓\n✓\nKang et al. (2022)\nm\n✓\n✓\n✓\nAttention\nOh et al. (2019)\nm\n✓\n✓\n✓\n✓\nLee et al. (2019)\n2\n✓\n✓\n✓\n✓\nWoo et al. (2020)\n2\n✓\n✓\n✓\n✓\nLi et al. (2020a)\n1\n✓\n✓\n✓\nZeng et al. (2020a)\n1\n✓\n✓\nLiu et al. (2021b)\n1\n✓\n✓\nChen et al. (2021)\n2\n✓\n✓\n✓\nZhang et al. (2022a)\n2\n✓\n✓\n✓\n✓\nRen et al. (2022)\n2\n✓\n✓\nTable 6: Quantitative comparisons of representative video inpainting methods on YouTube-VOS and DAVIS\ndataset. ‡ Higher is better. † Lower is better. *: our results using the method described in STTN (Zeng et al.,\n2020a), and numerical differences may be due to different optical flow models during evaluation.\nMethods\nYouTube-VOS\nDAVIS\nPSNR‡\nSSIM‡\nVFID†\nFWE(×10−2)†\nPSNR‡\nSSIM‡\nVFID†\nFWE(×10−2)†\nVINet (Kim et al., 2019b)\n29.20\n0.9434\n0.072\n0.1490 / -\n28.96\n0.9411\n0.199\n0.1785 / -\nDFVI (Xu et al., 2019)\n29.16\n0.9429\n0.066\n0.1509 / -\n28.81\n0.9404\n0.187\n0.1880 / 0.1608*\nLGTSM (Chang et al., 2019b)\n29.74\n0.9504\n0.070\n0.1859 / -\n28.57\n0.9409\n0.170\n0.2566 / 0.1640*\nCAP (Lee et al., 2019)\n31.58\n0.9607\n0.071\n0.1470 / -\n30.28\n0.9521\n0.182\n0.1824 / 0.1533*\nFGVC (Gao et al., 2020)\n29.68\n0.9396\n0.064\n- / 0.0858*\n30.24\n0.9444\n0.143\n- / 0.1530*\nSTTN (Zeng et al., 2020a)\n32.34\n0.9655\n0.053\n0.1451 / 0.0884*\n30.67\n0.9560\n0.149\n0.1779 / 0.1449*\nFuseFormer (Liu et al., 2021b)\n33.16\n0.9673\n0.051\n- / 0.0875*\n32.54\n0.9700\n0.138\n- / 0.1336*\nFGT (Zhang et al., 2022a)\n32.11\n0.9598\n0.054\n- / 0.0860*\n32.39\n0.9633\n0.1095\n- / 0.1517*\nISVI (Zhang et al., 2022b)\n32.80\n0.9611\n0.048\n- / 0.0856*\n33.70\n0.967\n0.1028\n- / 0.1509*\nE2FGVI (Li et al., 2022d)\n33.50\n0.9692\n0.046\n- / 0.0864*\n32.71\n0.9700\n0.096\n- / 0.1383*\nWarp loss. This loss encourages image-flow consis-\ntency:\nLwarp = ||Ii − Ij( ˆFi,j)||1,\n(12)\nwhere Ij( ˆFi,j) refers to the warped result of the frame Ij\nusing the generated flow ˆFi,j through backward warp-\ning.\n3.3 Datasets\nFor video inpainting, three common video datasets, i.e.,\nFaceForensics (R¨ossler et al., 2018), DAVIS (Perazzi\net al., 2016) and YouTube-VOS (Xu et al., 2018a), are\nused for training and evaluation.\n– FaceForensics: A face forgery detection video dataset\nconsisting of 1,004 videos. Among them, 854 videos\nDeep Learning-based Image and Video Inpainting: A Survey\n25\nFig. 15: Qualitative comparisons of representative video inpainting methods on YouTube-VOS and DAVIS dataset.\nThe light blue mask highlights the corrupted regions. The first three columns are random masks and the remaining\ntwo columns are object masks.\nare used for training and the rest are used for eval-\nuation.\n– DAVIS dataset: A densely annotated video segmen-\ntation dataset contains 150 videos with challenging\nmotion-blur and appearance motions. For the data\nsplit, 60 videos are used for training and 90 videos\nfor testing.\n– YouTube-VOS dataset: A large-scale video object\nsegmentation dataset containing 4,453 video clips\nand 94 object categories. The video clips have on\naverage 150 frames and show various scenes. The\noriginal data split, i.e., 3,471/474/508, is adopted\nfor experimental comparisons.\n3.4 Evaluation Protocol\nVideo contains many image frames, therefore, the two\nmost widely-used metrics in image inpainting (i.e., PSNR\nand SSIM) are also used for video quality assessment.\nIn addition, there are two other video-specific metrics\n(considering the temporal aspect), i.e., flow warping\nerror (FWE) Lai et al. (2018) and video-based Fr´echet\ninception distance (VFID) Wang et al. (2018a). The for-\nmer evaluates the temporal stability of inpainted videos\nand the latter measures the perceptual realism in the\nvideo setting.\n– FWE: The flow warping error between two con-\nsecutive video frames is calculated as E(It, It+1) =\n1\nPN\nn=1 Mn\nt\nPN\nn=1 Mn\nt ||In\nt − ˆIn\nt+1||2\n2, where Mt is a bi-\nnary mask indicating non-occluded areas and ˆIt+1 is\n26\nWeize Quan 1,2 et al.\nthe warped frame of It+1. The non-occlusion mask\ncan be estimated by using the method Ruder et al.\n(2016). Then, the warping error of a video is defined\nas the average error over the entire frames, and the\nformulation is E =\n1\nT −1\nPT −1\nt=1 E(It, It+1).\n– VFID: A variant of FID for video evaluation. In-\nstead of using a pre-trained image recognition net-\nwork, the spatiotemporal feature map of each video\nis extracted via a pre-trained video recognition net-\nwork, e.g., I3D Carreira and Zisserman (2017). Then,\nthe VFID is calculated following the same procedure\nas the FID.\n3.5 Performance Evaluation\nIn this section, we report the performance evaluation of\nrepresentative video inpainting methods.\nTable 6 shows the numerical results on YouTube-\nVOS and DAVIS datasets. We use the evaluated masks\nshared by (Liu et al., 2021b). Early video inpainting\nmethods based on 3D convolution (e.g., VINet (Kim\net al., 2019b)) and shift (e.g., LGTSM (Chang et al.,\n2019b)) have relatively limited inpainting performance.\nAfter introducing optical flow and attention mecha-\nnisms, the quality of video inpainting is remarkably im-\nproved. DFVI (Xu et al., 2019) generates the baseline\nresult with flow guidance, and FGVC (Gao et al., 2020)\nachieves better performance by completing flow with\nsharp edges and propagating information from distant\nframes. ISVI (Zhang et al., 2022b) obtains more ex-\nact flow completion under the inertia prior, and thus\nenhances the inpainting quality. STTN (Zeng et al.,\n2020a) and FuseFormer (Liu et al., 2021b) both design\nvideo inpainting frameworks through stacking multiple\ntransformer blocks with multi-scale attention and dense\npatch-wise attention, respectively. FGT (Zhang et al.,\n2022a) and E2FGVI (Li et al., 2022d) combine the flow\ncompletion and transformer as a whole, and the end-to-\nend pipeline as adopted by E2FGVI is slightly better.\nFig. 15 illustrates some inpainted results with differ-\nent types of scenes and masks. From the first and sec-\nond columns, we find that the flow-based pixel propa-\ngation methods, including FGVC, FGT, and ISVI, have\na good ability to recover the texture details and objects\nwith the guidance of neighboring frames. Through con-\ntextual correlation modeling, transformer-based video\ninpainting methods, such as STTN, FuseFormer, and\nE2FGVI, can complete the structure of objects, e.g.,\nthe window of a bus in the third column. Compared\nto STTN, FuseFormer introduces more dense atten-\ntion computation (with overlapping), which can help\nthe global structure recovery, e.g., the trunk in the\nfourth column and the post in the last column. In the\nFig. 16: Several representative examples of blind video\ndecaptioning produced by (Chu et al., 2021).\nfourth column, the coverage area is better filled with\nthe realistic grass texture by the ISVI method, which is\nattributed to the more accurate flow completion com-\npared to FGVC and FGT.\n3.6 Applications\n3.6.1 Blind Video Decaptioning\nBlind video decaptioning aims to automatically remove\nsubscripts and recover the occluded regions in videos\nwithout mask information. Kim et al. (2019a) designed\nan encoder-decoder framework based on 3D convolu-\ntion. They applied residual learning to directly touch\nthe corrupted regions and leveraged feedback connec-\ntions to enforce temporal coherence with the warping\nloss. However, this method often suffers from the prob-\nlem of incomplete subtitle removal. Chu et al. (2021)\nproposed a two-stage video decaptioning network in-\ncluding a mask extraction module and a frame attention-\nbased decaptioning module. Several examples produced\nby (Chu et al., 2021) are shown in Fig. 16. The regions\noriginally covered by subtitles are filled with plausible\ncontent.\n3.6.2 Dynamic Object Removal\nA common practical application of video inpainting tech-\nnology is to automatically remove undesired objects,\nwhich are static or dynamic at the time of recording.\nIn this part, we show two examples of dynamic object\nremoval with the recent video inpainting methods (Liu\net al., 2021b; Ren et al., 2022; Kang et al., 2022). As\nshown in Fig. 17, the regions covered by dynamic ob-\njects can be filled with plausible content.\nDeep Learning-based Image and Video Inpainting: A Survey\n27\nFig. 17: Three examples of dynamic object removal\nproduced by FuseFormer (Liu et al., 2021b), Dl-\nFormer (Ren et al., 2022), and ECFVI (Kang et al.,\n2022).\n4 Future Work\nImage and video inpainting essentially is a conditional\ngenerative task, therefore, the common generative mod-\nels, such as VAE and GAN, are often adopted by the ex-\nisting inpainting methods. Currently, diffusion models\nhave become the most popular generative models with\npowerful capability of content synthesis. DMs would\nhave the potential to improve the performance of image\nand video inpainting and may attract a lot of research\neffort in the future. For this promising direction, several\nchallenging problems need to be solved.\nHow to use large pre-trained diffusion models\n(e.g., denoising diffusion) for image inpainting?\nDMs synthesize an image by a sequential application\nof denoising steps, which are conducted in pixel or la-\ntent space. For the inpainting task, the core idea is to\nfill in the missing regions while preserving the origi-\nnally valid content. Some researchers have made prelim-\ninary attempts, such as Palette (Saharia et al., 2022a),\nBlended Diffusion (Avrahami et al., 2022), and Con-\ntrolNet (Zhang and Agrawala, 2023), etc. One research\nchallenge is how to inject conditioning information into\nthe denoising processes of large pre-trained diffusion\nmodels. Following the pipeline of diffusion models, they\nneed many iterations to generate the final image and\nthus require a longer inference time compared to ex-\nisting VAE- and GAN-based approaches. Another re-\nsearch challenge is to implement fast inpainting meth-\nods based on diffusion. Also, while video-based gener-\native diffusion models are still in their infancy, it is\nexpected that large pre-trained video generation mod-\nels will become available in the near future. Leveraging\nthese models for video inpainting will be an interesting\ntask once these models become available.\nHow to use large pre-trained models for joint\ntext and image embedding (e.g., the latest CLIP\nstyle architecture) for image inpainting?\nMainstream inpainting methods are uncontrollable, where\nthe inpainted content is unknown in advance and some-\ntimes this is undesired for users. Reference-based in-\npainting cannot fully satisfy this requirement. On the\nother hand, recent studies (Rombach et al., 2022; Hertz\net al., 2022; Parmar et al., 2023) have shown that large\npre-trained diffusion models with massive text-image\npairs can synthesize high-quality images with rich low-\nlevel attributes and details. In addition,\nZhao et al.\n(2023) implied that such pre-trained DMs also contain\nhigh-level visual concepts. As a result, text-guided in-\npainting based on the large pre-trained text-to-image\ndiffusion models would be able to fill the content un-\nder the control of users. The first problem is to design\nthe appropriate prompt exactly indicating the user’s\nintention. It is also challenging to merge the image em-\nbedding from the user prompt with the corresponding\nembedding of the input corrupted image. In addition,\ntext-based video inpainting will be a great avenue for\nfuture work.\nHow to scale up training to datasets of 5B\nimages (e.g. LAION)?\nDeep learning models are hungry for training datasets.\nCurrently, advanced diffusion models are pre-trained\non large-scale datasets containing millions or even bil-\nlions of text-image data pairs. However, these mod-\nels are mainly dominated by several industrial research\nlabs, where the datasets and training processes are not\ntransparent to the research community. Very recently,\nthe largest text-image dataset LAION-5B (Schuhmann\net al., 2022) containing 5.8 billion samples is publicly\navailable. In future work, it is worth designing efficient\nmethods for image and video inpainting that are trained\non such very large datasets directly.\nHow to utilize image data and pre-trained\nimage inpainting models to improve the models\nof video inpainting? In addition to considering the\nspatial aspect as in image inpainting, video inpainting\nalso needs to consider the temporal aspect. Therefore,\nit is important and beneficial to transfer the inpainting\nability from image to video. A simple and direct solu-\ntion is to take the result of image inpainting on each\nframe as the initialization and then revise the spatial\nand temporal consistency via carefully designed deep\nmodels. Another possible research line is to take the\nwell-trained image inpainting models as the backbone\nand aggregate the multiple frames in the feature space\nwith appropriate modules, such as deformable convo-\nlution or attention. It’s still worth exploring combining\nthe pre-trained image inpainting model with deep video\nprior.\nHow to create a large video dataset of 5B\nvideos and leverage it for video inpainting?\n28\nWeize Quan 1,2 et al.\nLike image inpainting, taking advantage of large pre-\ntrained text-video diffusion models may be a new re-\nsearch direction for video inpainting. However, current\ntext-video DMs are trained on datasets with 10 million\ncaptioned videos, which inevitably limits the generation\nand generalization ability of DMs. One potential direc-\ntion of future research is to collect large-scale text-video\ndatasets (e.g., 5B pairs) and design the pre-training\nmethods scaling up to this amount. As we all know,\nvideo inpainting is more difficult compared to its image\ncounterpart. Therefore, it is valuable to spend time on\nall aspects of large video datasets: building large pub-\nlicly available video datasets, generating large diffusion\nmethods for video synthesis and using these pre-trained\nmethods for video inpainting, and separately designing\nand training large-scale video architectures directly.\n5 Concluding Remarks\nThe prevalence of visual data, including images and\nvideo, promotes the development of related processing\ntechnologies, e.g., image and video inpainting. Due to\ntheir practical applications in many fields, these tech-\nniques have attracted great attention from both the in-\ndustrial and research communities over the past decade.\nWe presented a review of deep learning-based methods\nfor image and video inpainting. Specifically, we out-\nline different aspects of the research, including a tax-\nonomy of existing methods, training objectives, bench-\nmark datasets, evaluation protocols, performance eval-\nuation, and real-world applications. Future research di-\nrections are also discussed.\nAlthough current deep learning-based inpainting ap-\nproaches have achieved remarkable performance improve-\nment, there are still several limitations: (1) Uncertainty\nof artifacts. The results generated by inpainting meth-\nods often exhibit visual artifacts, which are difficult to\npredict and prevent. There is almost no research work\nto systematically and comprehensively study these ar-\ntifacts. (2) Specificity. Current inpainting models are\nusually trained on specific datasets, for example, face\nimages or natural scene images. In other words, models\ntrained on face images have bad predictions on nat-\nural scene images, and vice versa. Not enough mod-\nels are trained on large scale datasets such as LIAON.\n(3) Large-scale inpainting. Current advanced inpainting\nmethods still have limited performance on large-scale\nmissing regions. Many methods are based on attention\nmechanisms, which are more fragile in large-scale sce-\nnarios. (4) High training costs. Current deep learning-\nbased inpainting methods often need one or more weeks\non multiple GPUs, which places very high demands on\nresource consumption. (5) Long inference time. Diffu-\nsion model-based methods can achieve better inpainting\nperformance, however, they need a very long running\ntime, which limits the application scope of inpainting\ntechniques.\nDeep image/video inpainting techniques have a wide\nrange of real-world applications, however, they also raise\npotential ethical issues that need to be carefully con-\nsidered and addressed: (1) Security risks. Inpainting-\nbased visual data editing, e.g., object removal, may\nmaliciously be exploited, such as tampering with visual\ndata or altering evidence. (2) Ownership and copyright.\nWhen there is no appropriate authorization, deep in-\npainting techniques used to manipulate and enhance\nimages/videos could raise questions about ownership\nand copyright. The inpainting result may strongly re-\nsemble or be strongly inspired by copyrighted material.\n(3) Historical accuracy. Inpainting methods can be used\nfor the restoration of old photos/films or artworks. This\nprocess could raise risks of inadvertently changing the\ninitial creative intention or historical accuracy of the\ncontent, which requires careful verification by domain\nexperts. (4) Bias. If not properly trained, an inpaint-\ning model may introduce bias or unfairness, especially\nwhen the training data is biased or unrepresentative.\nThis has the potential to perpetuate social prejudices\nor inaccurately portray certain groups.\nReferences\nArjovsky M, Chintala S, Bottou L (2017) Wasser-\nstein Generative Adversarial Networks. In: Int. Conf.\nMach. Learn., vol 70, pp 214–223\nAustin J, Johnson DD, Ho J, Tarlow D, van den Berg R\n(2021) Structured Denoising Diffusion Models in Dis-\ncrete State-Spaces. In: Adv. Neural Inform. Process.\nSyst., vol 34, pp 17981–17993\nAvrahami O, Lischinski D, Fried O (2022) Blended Dif-\nfusion for Text-Driven Editing of Natural Images. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 18208–\n18218\nBallester C, Bertalmio M, Caselles V, Sapiro G, Verdera\nJ (2001) Filling-in by joint interpolation of vector\nfields and gray levels. IEEE Trans Image Process\n10(8):1200–1211\nBaluja S, Marwood D, Johnston N, Covell M (2019)\nLearning to Render Better Image Previews. In: IEEE\nInt. Conf. Image Process., pp 1700–1704\nBarnes C, Shechtman E, Finkelstein A, Goldman DB\n(2009) PatchMatch: A randomized correspondence\nalgorithm for structural image editing. ACM Trans\nGraph 28(3):24\nDeep Learning-based Image and Video Inpainting: A Survey\n29\nBertalmio M, Sapiro G, Caselles V, Ballester C (2000)\nImage inpainting. In: Proc. ACM SIGGRAPH, pp\n417–424\nBian X, Wang C, Quan W, Ye J, Zhang X, Yan DM\n(2022) Scene text removal via cascaded text stroke\ndetection and erasing. Computational Visual Media\n8:273–287\nBlau Y, Michaeli T (2018) The Perception-Distortion\nTradeoff. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 6228–6237\nCanny J (1986) A computational approach to edge\ndetection. IEEE Trans Pattern Anal Mach Intell\n(6):679–698\nCao C, Fu Y (2021) Learning a Sketch Tensor Space\nfor Image Inpainting of Man-Made Scenes. In: Int.\nConf. Comput. Vis., pp 14509–14518\nCao C, Dong Q, Fu Y (2022) Learning Prior Feature\nand Attention Enhanced Image Inpainting. In: Eur.\nConf. Comput. Vis.\nCarlsson S (1988) Sketch based coding of grey level im-\nages. Sign Process 15(1):57–83\nCarreira J, Zisserman A (2017) Quo Vadis, Action\nRecognition? A New Model and the Kinetics Dataset.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n4724–4733\nChang YL, Liu ZY, Lee KY, Hsu W (2019a) Free-\nform video inpainting with 3d gated convolution and\ntemporal patchgan. In: Int. Conf. Comput. Vis., pp\n9066–9075\nChang YL, Liu ZY, Lee KY, Hsu W (2019b) Learnable\ngated temporal shift module for deep video inpaint-\ning. In: Brit. Mach. Vis. Conf.\nChang YL, Yu Liu Z, Hsu W (2019c) Vornet: Spatio-\ntemporally consistent video inpainting for object re-\nmoval. In: IEEE Conf. Comput. Vis. Pattern Recog.\nWorksh., pp 1785–1794\nChen C, Cai J, Hu Y, Tang X, Wang X, Yuan C, Bai\nX, Bai S (2021) Deep Interactive Video Inpainting:\nAn Invisibility Cloak for Harry Potter. In: ACM Int.\nConf. Multimedia, p 862–870\nChen L, Zhang H, Xiao J, Nie L, Shao J, Liu W,\nChua TS (2017) SCA-CNN: Spatial and Channel-\nWise Attention in Convolutional Networks for Image\nCaptioning. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 6298–6306\nChen\nP\n(2018)\nVideo\nretouch:\nObject\nre-\nmoval.\nhttp://www.12371.cn/2021/02/08/\nARTI1612745858192472.shtml\nChen T, Lucic M, Houlsby N, Gelly S (2019) On Self\nModulation for Generative Adversarial Networks. In:\nInt. Conf. Learn. Represent.\nChi L, Jiang B, Mu Y (2020) Fast Fourier Convolution.\nIn: Adv. Neural Inform. Process. Syst., vol 33, pp\n4479–4488\nChu P, Quan W, Wang T, Wang P, Ren P, Yan DM\n(2021) Deep video decaptioning. In: Brit. Mach. Vis.\nConf.\nChung H, Sim B, Ye JC (2022) Come-Closer-Diffuse-\nFaster: Accelerating Conditional Diffusion Models for\nInverse Problems through Stochastic Contraction. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 12403–\n12412\nCimpoi M, Maji S, Kokkinos I, Mohamed S, Vedaldi\nA (2014) Describing Textures in the Wild. In: IEEE\nConf. Comput. Vis. Pattern Recog., pp 3606–3613\nCriminisi A, Perez P, Toyama K (2004) Region filling\nand object removal by exemplar-based image inpaint-\ning. IEEE Trans Image Process 13(9):1200–1212\nCroitoru FA, Hondru V, Ionescu RT, Shah M (2023)\nDiffusion models in vision: A survey. IEEE Trans\nPattern Anal Mach Intell 45(9):10850–10869\nDai Q, Chopp H, Pouyet E, Cossairt O, Walton M, Kat-\nsaggelos AK (2020) Adaptive image sampling using\ndeep learning and its application on x-ray fluores-\ncence image reconstruction. IEEE Trans Multimedia\n22(10):2564–2578\nDarabi S, Shechtman E, Barnes C, Goldman DB, Sen\nP (2012) Image Melding: combining inconsistent im-\nages using patch-based synthesis. ACM Trans Graph\n(Proc SIGGRAPH) 31(4):1–10\nDaubechies I (1990) The wavelet transform, time-\nfrequency localization and signal analysis. IEEE\nTrans Inf Theory 36(5):961–1005\nDeng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L (2009)\nImagenet: A large-scale hierarchical image database.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n248–255\nDeng Y, Tang F, Dong W, Sun W, Huang F,\nXu C (2020) Arbitrary Style Transfer via Multi-\nAdaptation Network. In: ACM Int. Conf. Multime-\ndia, p 2719–2727\nDeng Y, Hui S, Zhou S, Meng D, Wang J (2021)\nLearning Contextual Transformer Network for Im-\nage Inpainting. In: ACM Int. Conf. Multimedia, p\n2529–2538\nDeng Y, Hui S, Meng R, Zhou S, Wang J (2022) Hour-\nglass Attention Network for Image Inpainting. In:\nEur. Conf. Comput. Vis., pp 483–501\nDinh L, Krueger D, Bengio Y (2014) Nice: Non-linear\nindependent components estimation. Int Conf Learn\nRepresent Worksh\nDoersch C, Singh S, Gupta A, Sivic J, Efros AA (2012)\nWhat makes paris look like paris? ACM Trans Graph\n31(4):101:1–101:9\nDolhansky\nB,\nFerrer\nCC\n(2018)\nEye\nIn-painting\nwith Exemplar Generative Adversarial Networks. In:\n30\nWeize Quan 1,2 et al.\nIEEE Conf. Comput. Vis. Pattern Recog., pp 7902–\n7911\nDong Q, Cao C, Fu Y (2022) Incremental Transformer\nStructure Enhanced Image Inpainting With Masking\nPositional Encoding. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 11358–11368\nDosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D,\nZhai X, Unterthiner T, Dehghani M, Minderer M,\nHeigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An\nImage is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In: Int. Conf. Learn. Represent.\nDosselmann R, Yang XD (2011) A comprehensive as-\nsessment of the structural similarity index. Sign Im-\nage and Video Process 5:81–91\nEfros A, Leung T (1999) Texture synthesis by non-\nparametric sampling. In: Int. Conf. Comput. Vis.,\nvol 2, pp 1033–1038\nElharrouss O, Almaadeed N, Al-Maadeed S, Akbari Y\n(2020) Image Inpainting: A Review. Neural Process\nLetters 51(2):2007–2028\nEsser P, Rombach R, Blattmann A, Ommer B (2021)\nImageBART: Bidirectional Context with Multino-\nmial Diffusion for Autoregressive Image Synthesis.\nIn: Adv. Neural Inform. Process. Syst., vol 34, pp\n3518–3532\nEveringham M, Eslami SMA, Gool LV, Williams CKI,\nWinn J, Zisserman A (2015) The pascal visual object\nclasses challenge: A retrospective. Int J Comput Vis\n111:98–136\nFelzenszwalb PF, Huttenlocher DP (2004) Efficient\ngraph-based image segmentation. Int J Comput Vis\n(59):167–181\nFeng X, Pei W, Li F, Chen F, Zhang D, Lu G\n(2022) Generative memory-guided semantic reason-\ning model for image inpainting. IEEE Trans Circuit\nSyst Video Technol 32(11):7432–7447\nFu J, Liu J, Tian H, Li Y, Bao Y, Fang Z, Lu H (2019)\nDual Attention Network for Scene Segmentation. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 3141–\n3149\nGali´c I, Weickert J, Welk M, Bruhn A, Belyaev A, Sei-\ndel HP (2008) Image compression with anisotropic\ndiffusion. J Math Imaging Vis 31:255–269\nGao C, Saraf A, Huang JB, Kopf J (2020) Flow-edge\nguided video completion. In: Eur. Conf. Comput.\nVis., pp 713–729\nGatys LA, Ecker AS, Bethge M (2016) Image Style\nTransfer Using Convolutional Neural Networks. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 2414–\n2423\nGoodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-\nFarley D, Ozair S, Courville A, Bengio Y (2014) Gen-\nerative adversarial nets. In: Adv. Neural Inform. Pro-\ncess. Syst., pp 2672–2680\nGranados M, Kim KI, Tompkin J, Kautz J, Theobalt\nC (2012) Background Inpainting for Videos with Dy-\nnamic Objects and a Free-Moving Camera. In: Eur.\nConf. Comput. Vis., pp 682–695\nGu J, Shen Y, Zhou B (2020) Image Processing Us-\ning Multi-Code GAN Prior. In: IEEE Conf. Comput.\nVis. Pattern Recog., pp 3009–3018\nGuillemot C, Meur OL (2014) Image inpainting:\nOverview and recent advances. IEEE Sign Process\nMagazine 31(1):127–144\nGuo Q, Gao S, Zhang X, Yin Y, Zhang C (2018)\nPatch-based image inpainting via two-stage low\nrank approximation. IEEE Trans Vis Comput Graph\n24(6):2023–2036\nGuo X, Yang H, Huang D (2021) Image Inpainting via\nConditional Texture and Structure Dual Generation.\nIn: Int. Conf. Comput. Vis., pp 14134–14143\nGuo Z, Chen Z, Yu T, Chen J, Liu S (2019) Progressive\nImage Inpainting with Full-Resolution Residual Net-\nwork. In: ACM Int. Conf. Multimedia, p 2496–2504\nHan C, Wang J (2021) Face image inpainting with\nevolutionary generators. IEEE Sign Process Letters\n28:190–193\nHan X, Wu Z, Huang W, Scott MR, Davis L (2019)\nFiNet: Compatible and Diverse Fashion Image In-\npainting. In: Int. Conf. Comput. Vis., pp 4480–4490\nHe K, Sun J (2012) Statistics of Patch Offsets for Image\nCompletion. In: Eur. Conf. Comput. Vis., pp 16–29\nHe K, Zhang X, Ren S, Sun J (2016) Deep residual\nlearning for image recognition. In: IEEE Conf. Com-\nput. Vis. Pattern Recog., pp 770–778\nHe K, Chen X, Xie S, Li Y, Doll´ar P, Girshick R (2022)\nMasked Autoencoders Are Scalable Vision Learners.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n16000–16009\nHerling J, Broll W (2014) High-quality real-time video\ninpainting with pixmix. IEEE Trans Vis Comput\nGraph 20(6):866–879\nHertz A, Mokady R, Tenenbaum J, Aberman K, Pritch\nY, Cohen-Or D (2022) Prompt-to-prompt image\nediting with cross attention control. arXiv preprint\narXiv:220801626\nHeusel M, Ramsauer H, Unterthiner T, Nessler B,\nHochreiter S (2017) Gans trained by a two time-scale\nupdate rule converge to a local nash equilibrium. In:\nAdv. Neural Inform. Process. Syst., pp 6626–6637\nHo J, Jain A, Abbeel P (2020) Denoising Diffusion\nProbabilistic Models. In: Adv. Neural Inform. Pro-\ncess. Syst., vol 33, pp 6840–6851\nHochreiter S, Schmidhuber J (1997) Long short-term\nmemory. Neural Comput 9(8):1735–1780\nDeep Learning-based Image and Video Inpainting: A Survey\n31\nHong X, Xiong P, Ji R, Fan H (2019) Deep Fusion Net-\nwork for Image Completion. In: ACM Int. Conf. Mul-\ntimedia, pp 2033–2042\nHoogeboom E, Nielsen D, Jaini P, Forr´e P, Welling\nM (2021) Argmax Flows and Multinomial Diffusion:\nLearning Categorical Distributions. In: Adv. Neural\nInform. Process. Syst., vol 34, pp 12454–12465\nHoule ME (2017a) Local Intrinsic Dimensionality I: An\nExtreme-Value-Theoretic Foundation for Similarity\nApplications. In: Int. Conf. Similarity Search App.,\npp 64–79\nHoule ME (2017b) Local Intrinsic Dimensionality II:\nMultivariate Analysis and Distributional Support.\nIn: Int. Conf. Similarity Search App.\nHu J, Shen L, Sun G (2018) Squeeze-and-excitation net-\nworks. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 7132–7141\nHu YT, Wang H, Ballas N, Grauman K, Schwing AG\n(2020) Proposal-based video completion. In: Eur.\nConf. Comput. Vis., pp 38–54\nHuang JB, Kang SB, Ahuja N, Kopf J (2014) Image\ncompletion using planar structure guidance. ACM\nTrans Graph (Proc SIGGRAPH) 33(4):1–10\nHuang JB, Kang SB, Ahuja N, Kopf J (2016) Tem-\nporally coherent completion of dynamic video. ACM\nTrans Graph 35(6):1–11\nHuang X, Belongie S (2017) Arbitrary Style Transfer\nin Real-Time with Adaptive Instance Normalization.\nIn: Int. Conf. Comput. Vis., pp 1510–1519\nHui Z, Li J, Wang X, Gao X (2020) Image fine-grained\ninpainting. arXiv preprint arXiv:200202609\nIizuka S, Simo-Serra E, Ishikawa H (2017) Globally\nand locally consistent image completion. ACM Trans\nGraph (Proc SIGGRAPH) 36(4):1–14\nIlan S, Shamir A (2015) A survey on data-driven video\ncompletion. Comput Graph Forum 34(6):60–85\nIlg E, Mayer N, Saikia T, Keuper M, Dosovitskiy\nA, Brox T (2017) FlowNet 2.0: Evolution of Opti-\ncal Flow Estimation with Deep Networks. In: IEEE\nConf. Comput. Vis. Pattern Recog., pp 1647–1655\nIsola P, Zhu JY, Zhou T, Efros AA (2017) Image-to-\nimage translation with conditional adversarial net-\nworks. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 1125–1134\nJam J, Kendrick C, Walker K, Drouard V, Hsu JGS,\nYap MH (2021) A comprehensive review of past and\npresent image inpainting methods. Comput Vis Im-\nage Understand 203:103147\nJiang L, Dai B, Wu W, Loy CC (2021) Focal Frequency\nLoss for Image Reconstruction and Synthesis. In: Int.\nConf. Comput. Vis., pp 13899–13909\nJohnson J, Alahi A, Fei-Fei L (2016) Perceptual losses\nfor real-time style transfer and super-resolution. In:\nEur. Conf. Comput. Vis., pp 694–711\nKang J, Oh SW, Kim SJ (2022) Error Compensation\nFramework for Flow-Guided Video Inpainting. In:\nEur. Conf. Comput. Vis., pp 375–390\nKarras T, Aila T, Laine S, Lehtinen J (2018) Progres-\nsive Growing of GANs for Improved Quality, Stabil-\nity, and Variation. In: Int. Conf. Learn. Represent.\nKarras T, Laine S, Aila T (2019) A Style-Based Gen-\nerator Architecture for Generative Adversarial Net-\nworks. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 4396–4405\nKarras T, Laine S, Aittala M, Hellsten J, Lehtinen J,\nAila T (2020) Analyzing and Improving the Image\nQuality of StyleGAN. In: IEEE Conf. Comput. Vis.\nPattern Recog.\nKe L, Tai YW, Tang CK (2021) Occlusion-aware video\nobject inpainting. In: Int. Conf. Comput. Vis., pp\n14468–14478\nKim D, Woo S, Lee JY, Kweon IS (2019a) Deep\nblind video decaptioning by temporal aggregation\nand recurrence. In: IEEE Conf. Comput. Vis. Pat-\ntern Recog., pp 4263–4272\nKim D, Woo S, Lee JY, Kweon IS (2019b) Deep video\ninpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 5792–5801\nKim SY, Aberman K, Kanazawa N, Garg R, Wadhwa\nN, Chang H, Karnad N, Kim M, Liba O (2022) Zoom-\nto-Inpaint: Image Inpainting With High-Frequency\nDetails. In: IEEE Conf. Comput. Vis. Pattern Recog.\nWorksh., pp 477–487\nKingma DP, Dhariwal P (2018) Glow: Generative Flow\nwith Invertible 1x1 Convolutions. In: Adv. Neural In-\nform. Process. Syst., vol 31\nKingma DP, Welling M (2014) Auto-Encoding Varia-\ntional Bayes. In: Int. Conf. Learn. Represent.\nLai WS, Huang JB, Wang O, Shechtman E, Yumer\nE, Yang MH (2018) Learning Blind Video Temporal\nConsistency. In: Eur. Conf. Comput. Vis., pp 179–\n195\nLao D, Zhu P, Wonka P, Sundaramoorthi G (2021)\nFlow-Guided Video Inpainting with Scene Tem-\nplates. In: Int. Conf. Comput. Vis., pp 14599–14608\nLedig C, Theis L, Husz´ar F, Caballero J, Cunningham\nA, Acosta A, Aitken A, Tejani A, Totz J, Wang Z,\nShi W (2017) Photo-Realistic Single Image Super-\nResolution Using a Generative Adversarial Network.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n105–114\nLee S, Oh SW, Won D, Kim SJ (2019) Copy-and-paste\nnetworks for deep video inpainting. In: Int. Conf.\nComput. Vis., pp 4413–4421\nLempitsky V, Vedaldi A, Ulyanov D (2018) Deep Image\nPrior. In: IEEE Conf. Comput. Vis. Pattern Recog.,\n32\nWeize Quan 1,2 et al.\npp 9446–9454\nLi A, Qi J, Zhang R, Ma X, Ramamohanarao K\n(2019a) Generative Image Inpainting with Submani-\nfold Alignment. In: Int. Joint Conf. Artificial Intell.,\npp 811–817\nLi A, Zhao S, Ma X, Gong M, Qi J, Zhang R, Tao D,\nKotagiri R (2020a) Short-term and long-term context\naggregation network for video inpainting. In: Eur.\nConf. Comput. Vis., pp 728–743\nLi A, Zhao L, Zuo Z, Wang Z, Xing W, Lu D (2023)\nMigt: Multi-modal image inpainting guided with\ntext. Neurocomputing 520:376–385\nLi B, Zheng B, Li H, Li Y (2021) Detail-enhanced image\ninpainting based on discrete wavelet transforms. Sign\nProcess 189:108278\nLi CT, Siu WC, Liu ZS, Wang LW, Lun DPK (2020b)\nDeepGIN: Deep Generative Inpainting Network for\nExtreme Image Inpainting. In: Eur. Conf. Comput.\nVis. Worksh., pp 5–22\nLi F, Li A, Qin J, Bai H, Lin W, Cong R, Zhao\nY (2022a) Srinpaintor: When super-resolution meets\ntransformer for image inpainting. IEEE Trans Com-\nputational Imaging 8:743–758\nLi H, Li G, Lin L, Yu H, Yu Y (2019b) Context-\naware semantic inpainting. IEEE Trans Cybern\n49(12):4398–4411\nLi J, He F, Zhang L, Du B, Tao D (2019c) Progres-\nsive Reconstruction of Visual Structure for Image In-\npainting. In: Int. Conf. Comput. Vis., pp 5961–5970\nLi J, Wang N, Zhang L, Du B, Tao D (2020c) Recurrent\nFeature Reasoning for Image Inpainting. In: IEEE\nConf. Comput. Vis. Pattern Recog., pp 7757–7765\nLi W, Lin Z, Zhou K, Qi L, Wang Y, Jia J (2022b)\nMAT: Mask-Aware Transformer for Large Hole Im-\nage Inpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog.\nLi W, Yu X, Zhou K, Song Y, Lin Z, Jia J (2022c) Sdm:\nSpatial diffusion model for large hole image inpaint-\ning. arXiv preprint arXiv:221202963\nLi Y, Liu S, Yang J, Yang MH (2017) Generative Face\nCompletion. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 5892–5900\nLi Y, Jiang B, Lu Y, Shen L (2019d) Fine-grained Ad-\nversarial Image Inpainting with Super Resolution. In:\nInt. Joint Conf. Neural Networks, pp 1–8\nLi Z, Lu CZ, Qin J, Guo CL, Cheng MM (2022d) To-\nwards an end-to-end framework for flow-guided video\ninpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 17562–17571\nLiao H, Funka-Lea G, Zheng Y, Luo J, Zhou SK\n(2018a) Face Completion with Semantic Knowledge\nand Collaborative Adversarial Learning. In: Asian\nConf. Comput. Vis., vol 11361, pp 382–397\nLiao L, Hu R, Xiao J, Wang Z (2018b) Edge-Aware\nContext Encoder for Image Inpainting. In: Int. Conf.\nAcou. Speech Sign. Process., pp 3156–3160\nLiao L, Xiao J, Wang Z, Lin CW, Satoh S (2020) Guid-\nance and Evaluation: Semantic-Aware Image Inpaint-\ning for Mixed Scenes. In: Eur. Conf. Comput. Vis.\nLiao L, Xiao J, Wang Z, Lin CW, Satoh S (2021a) Im-\nage Inpainting Guided by Coherence Priors of Se-\nmantics and Textures. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 6539–6548\nLiao L, Xiao J, Wang Z, Lin CW, Satoh S (2021b)\nUncertainty-aware semantic guidance and estimation\nfor image inpainting. IEEE J Selected Topics Sign\nProcess 15(2):310–323\nLim JH, Ye JC (2017) Geometric gan. arXiv preprint\narXiv:170502894\nLin J, Gan C, Han S (2019) Tsm: Temporal shift mod-\nule for efficient video understanding. In: Int. Conf.\nComput. Vis., pp 7083–7093\nLin Q, Yan B, Li J, Tan W (2020) Mmfl: Multimodal\nfusion learning for text-guided image inpainting. In:\nACM Int. Conf. Multimedia, pp 1094–1102\nLiu G, Reda FA, Shih KJ, Wang TC, Tao A, Catanzaro\nB (2018) Image inpainting for irregular holes using\npartial convolutions. In: Eur. Conf. Comput. Vis.,\npp 85–100\nLiu H, Jiang B, Xiao Y, Yang C (2019) Coherent se-\nmantic attention for image inpainting. In: Int. Conf.\nComput. Vis., pp 4170–4179\nLiu H, Jiang B, Song Y, Huang W, Yang C (2020)\nRethinking Image Inpainting via a Mutual Encoder-\nDecoder with Feature Equalizations. In: Eur. Conf.\nComput. Vis.\nLiu H, Wan Z, Huang W, Song Y, Han X, Liao J\n(2021a) PD-GAN: Probabilistic Diverse GAN for Im-\nage Inpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 9367–9376\nLiu R, Deng H, Huang Y, Shi X, Lu L, Sun W, Wang X,\nDai J, Li H (2021b) FuseFormer: Fusing Fine-Grained\nInformation in Transformers for Video Inpainting. In:\nInt. Conf. Comput. Vis., pp 14040–14049\nLiu T, Liao L, Wang Z, Satoh S (2022) Reference-\nGuided Texture and Structure Inference for Image\nInpainting. In: IEEE Int. Conf. Image Process., pp\n1996–2000\nLiu W, Cao C, Liu J, Ren C, Wei Y, Guo H (2021c)\nFine-grained image inpainting with scale-enhanced\ngenerative adversarial network. Pattern Recog Letters\n143:81–87\nLiu Z, Luo P, Wang X, Tang X (2015) Deep learning\nface attributes in the wild. In: Int. Conf. Comput.\nVis., pp 3730–3738\nDeep Learning-based Image and Video Inpainting: A Survey\n33\nLiu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, Lin S,\nGuo B (2021d) Swin Transformer: Hierarchical Vi-\nsion Transformer using Shifted Windows. In: Int.\nConf. Comput. Vis., pp 9992–10002\nLu Z, Jiang J, Huang J, Wu G, Liu X (2022) GLaMa:\nJoint Spatial and Frequency Loss for General Im-\nage Inpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog. Worksh., pp 1301–1310\nLugmayr A, Danelljan M, Van Gool L, Timofte R\n(2020) SRFlow: Learning the Super-Resolution Space\nwith Normalizing Flow. In: Eur. Conf. Comput. Vis.,\np 715–732\nLugmayr A, Danelljan M, Romero A, Yu F, Timofte R,\nVan Gool L (2022) RePaint: Inpainting using Denois-\ning Diffusion Probabilistic Models. In: IEEE Conf.\nComput. Vis. Pattern Recog., pp 11451–11461\nMa Y, Liu X, Bai S, Wang L, He D, Liu A (2019)\nCoarse-to-Fine Image Inpainting via Region-wise\nConvolutions and Non-Local Correlation. In: Int.\nJoint Conf. Artificial Intell., pp 3123–3129\nMallat SG (1989) A theory for multiresolution sig-\nnal decomposition: the wavelet representation. IEEE\nTrans Pattern Anal Mach Intell 11(7):674–693\nMao X, Li Q, Xie H, Lau RY, Wang Z, Paul Smolley\nS (2017) Least Squares Generative Adversarial Net-\nworks. In: Int. Conf. Comput. Vis.\nMasnou S, Morel JM (1998) Level lines based disocclu-\nsion. In: IEEE Int. Conf. Image Process., vol 3, pp\n259–263\nNavasardyan S, Ohanyan M (2020) Image Inpainting\nwith Onion Convolutions. In: Asian Conf. Comput.\nVis.\nNazeri K, Ng E, Joseph T, Qureshi F, Ebrahimi M\n(2019) EdgeConnect: Structure Guided Image In-\npainting using Edge Prediction. In: Int. Conf. Com-\nput. Vis. Worksh.\nNewson A, Almansa A, Fradet M, Gousseau Y, P´erez\nP (2014) Video inpainting of complex scenes. SIAM\nJ Imaging Sciences 7(4):1993–2019\nNi M, Li X, Zuo W (2023) N¨UWA-LIP: Language-\nguided Image Inpainting with Defect-free VQGAN.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n14183–14192\nOh SW, Lee S, Lee JY, Kim SJ (2019) Onion-peel net-\nworks for deep video completion. In: Int. Conf. Com-\nput. Vis., pp 4403–4412\nOjala T, Pietik¨ainen M, Harwood D (1996) A com-\nparative study of texture measures with classifica-\ntion based on featured distributions. Pattern Recog\n29(1):51–59\nOjala T, Pietikainen M, Maenpaa T (2002) Multiresolu-\ntion gray-scale and rotation invariant texture classi-\nfication with local binary patterns. IEEE Trans Pat-\ntern Anal Mach Intell 24(7):971–987\nOuyang H, Wang T, Chen Q (2021) Internal video in-\npainting by implicit long-range propagation. In: Int.\nConf. Comput. Vis., pp 14579–14588\nPark T, Liu MY, Wang TC, Zhu JY (2019) Semantic\nImage Synthesis With Spatially-Adaptive Normaliza-\ntion. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 2332–2341\nParmar G, Singh KK, Zhang R, Li Y, Lu J, Zhu JY\n(2023) Zero-shot image-to-image translation. arXiv\npreprint arxiv230203027\nPathak D, Krahenbuhl P, Donahue J, Darrell T, Efros\nAA (2016) Context encoders: Feature learning by\ninpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 2536–2544\nPeng J, Liu D, Xu S, Li H (2021) Generating Di-\nverse Structure for Image Inpainting With Hierarchi-\ncal VQ-VAE. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 10770–10779\nPerazzi F, Pont-Tuset J, McWilliams B, Van Gool L,\nGross M, Sorkine-Hornung A (2016) A Benchmark\nDataset and Evaluation Methodology for Video Ob-\nject Segmentation. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 724–732\nPhutke SS, Murala S (2021) Diverse receptive field\nbased adversarial concurrent encoder network for im-\nage inpainting. IEEE Sign Process Letters 28:1873–\n1877\nQin J, Bai H, Zhao Y (2021) Multi-scale attention net-\nwork for image inpainting. Comput Vis Image Un-\nderstand 204:103155\nQiu J, Gao Y, Shen M (2021) Semantic-sca: Semantic\nstructure image inpainting with the spatial-channel\nattention. IEEE Access 9:12997–13008\nQuan W, Zhang R, Zhang Y, Li Z, Wang J, Yan DM\n(2022) Image inpainting with local and global refine-\nment. IEEE Trans Image Process 31:2405–2420\nRadford A, Metz L, Chintala S (2016) Unsupervised\nRepresentation Learning with Deep Convolutional\nGenerative Adversarial Networks. In: Int. Conf.\nLearn. Represent.\nRen J, Zheng Q, Zhao Y, Xu X, Li C (2022) DLFormer:\nDiscrete Latent Transformer for Video Inpainting. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 3511–\n3520\nRen JS, Xu L, Yan Q, Sun W (2015) Shepard Con-\nvolutional Neural Networks. In: Adv. Neural Inform.\nProcess. Syst., vol 28, p 901–909\nRen Y, Yu X, Zhang R, Li TH, Liu S, Li G (2019)\nStructureFlow: Image Inpainting via Structure-aware\nAppearance Flow. In: Int. Conf. Comput. Vis., pp\n181–190\n34\nWeize Quan 1,2 et al.\nRezende DJ, Mohamed S (2015) Variational Inference\nwith Normalizing Flows. In: Int. Conf. Mach. Learn.,\np 1530–1538\nRichardson E, Alaluf Y, Patashnik O, Nitzan Y, Azar\nY, Shapiro S, Cohen-Or D (2021) Encoding in Style:\na StyleGAN Encoder for Image-to-Image Transla-\ntion. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 2287–2296\nRombach R, Blattmann A, Lorenz D, Esser P, Ommer\nB (2022) High-Resolution Image Synthesis with La-\ntent Diffusion Models. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 10674–10685\nR¨ossler A, Cozzolino D, Verdoliva L, Riess C, Thies J,\nNießner M (2018) Faceforensics: A large-scale video\ndataset for forgery detection in human faces. arXiv\npreprint arXiv:180309179\nRoy H, Chaudhury S, Yamasaki T, Hashimoto T (2021)\nImage inpainting using frequency-domain priors. J\nElectronic Imaging 30(2):023016\nRuder M, Dosovitskiy A, Brox T (2016) Artistic\nStyle Transfer for Videos. In: German Conf. Pattern\nRecog., pp 26–36\nRudin LI, Osher S, Fatemi E (1992) Nonlinear total\nvariation based noise removal algorithms. Physica D:\nNonlinear Phenomena 60(1):259–268\nSagong Mc, Shin Yg, Kim Sw, Park S, Ko Sj (2019)\nPEPSI : Fast Image Inpainting With Parallel Decod-\ning Network. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 11352–11360\nSaharia C, Chan W, Chang H, Lee C, Ho J, Salimans T,\nFleet D, Norouzi M (2022a) Palette: Image-to-Image\nDiffusion Models. In: ACM SIGGRAPH Conf.\nSaharia C, Chan W, Saxena S, Li L, Whang J, Den-\nton E, Ghasemipour SKS, Gontijo-Lopes R, Ayan\nBK, Salimans T, Ho J, Fleet DJ, Norouzi M (2022b)\nPhotorealistic Text-to-Image Diffusion Models with\nDeep Language Understanding. In: Adv. Neural In-\nform. Process. Syst.\nSchrader K, Peter P, K¨amper N, Weickert J (2023) Ef-\nficient Neural Generation of 4K Masks for Homoge-\nneous Diffusion Inpainting. In: Int. Conf. Scale Space\nVariational Methods Comput. Vis., pp 16–28\nSchuhmann C, Beaumont R, Vencu R, Gordon CW,\nWightman R, Cherti M, Coombes T, Katta A, Mullis\nC, Wortsman M, Schramowski P, Kundurthy SR,\nCrowson K, Schmidt L, Kaczmarczyk R, Jitsev J\n(2022) LAION-5B: An open large-scale dataset for\ntraining next generation image-text models. In: Adv.\nNeural Inform. Process. Syst.\nShao H, Wang Y, Fu Y, Yin Z (2020) Generative image\ninpainting via edge structure and color aware fusion.\nSign Process: Image Communication 87:115929\nShen L, Hong R, Zhang H, Zhang H, Wang M (2019)\nSingle-Shot Semantic Image Inpainting with Densely\nConnected Generative Networks. In: ACM Int. Conf.\nMultimedia, p 1861–1869\nShin YG, Sagong MC, Yeo YJ, Kim SW, Ko SJ (2021)\nPepsi++: Fast and lightweight network for image in-\npainting. IEEE Trans Neural Networks Learn Syst\n32(1):252–265\nShukla T, Maheshwari P, Singh R, Shukla A, Kulka-\nrni K, Turaga P (2023) Scene Graph Driven Text-\nPrompt Generation for Image Inpainting. In: IEEE\nConf. Comput. Vis. Pattern Recog. Worksh., pp 759–\n768\nSimonyan K, Zisserman A (2014) Very deep convo-\nlutional networks for large-scale image recognition.\narXiv preprint arXiv:14091556\nSohl-Dickstein J, Weiss E, Maheswaranathan N, Gan-\nguli S (2015) Deep Unsupervised Learning using\nNonequilibrium Thermodynamics. In: Int. Conf.\nMach. Learn., vol 37, pp 2256–2265\nSong L, Cao J, Song L, Hu Y, He R (2019) Geometry-\nAware Face Completion and Editing. In: AAAI Conf.\nArtificial Intell., pp 2506–2513\nSong Y, Yang C, Lin Z, Liu X, Huang Q, Li H, Kuo\nCCJ (2018a) Contextual-Based Image Inpainting: In-\nfer, Match, and Translate. In: Eur. Conf. Comput.\nVis., pp 3–18\nSong Y, Yang C, Shen Y, Wang P, Huang Q, Kuo\nCCJ (2018b) SPG-Net: Segmentation Prediction and\nGuidance Network for Image Inpainting. In: Brit.\nMach. Vis. Conf.\nSun D, Yang X, Liu MY, Kautz J (2018a) PWC-Net:\nCNNs for Optical Flow Using Pyramid, Warping, and\nCost Volume. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 8934–8943\nSun K, Xiao B, Liu D, Wang J (2019) Deep High-\nResolution Representation Learning for Human Pose\nEstimation. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 5686–5696\nSun Q, Ma L, Joon Oh S, Gool LV, Schiele B, Fritz M\n(2018b) Natural and Effective Obfuscation by Head\nInpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 5050–5059\nSuvorov R, Logacheva E, Mashikhin A, Remizova A,\nAshukha A, Silvestrov A, Kong N, Goka H, Park K,\nLempitsky V (2022) Resolution-robust Large Mask\nInpainting with Fourier Convolutions. In: Winter\nConf. App. Comput. Vis., pp 3172–3182\nTabak EG, Vanden-Eijnden E (2010) Density estima-\ntion by dual ascent of the log-likelihood. Commun\nMath Sci 8(1):217 – 233\nTschumperl´e D, Deriche R (2005) Vector-valued im-\nage regularization with pdes: a common framework\nDeep Learning-based Image and Video Inpainting: A Survey\n35\nfor different applications. IEEE Trans Pattern Anal\nMach Intell 27(4):506–517\nTu CT, Chen YF (2019) Facial Image Inpainting\nwith Variational Autoencoder. In: Inter. Conf. In-\ntell. Robot. Control Engin., pp 119–122\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L,\nGomez AN, Kaiser u, Polosukhin I (2017) Attention\nis All You Need. In: Adv. Neural Inform. Process.\nSyst., p 6000–6010\nVo HV, Duong NQK, P´erez P (2018) Structural\nInpainting.\nIn:\nACM Int.\nConf.\nMultimedia,\np\n1948–1956\nWadhwa G, Dhall A, Murala S, Tariq U (2021) Hy-\nperrealistic Image Inpainting with Hypergraphs. In:\nWinter Conf. App. Comput. Vis., pp 3911–3920\nWan Z, Zhang B, Chen D, Zhang P, Chen D, Liao J,\nWen F (2020) Bringing Old Photos Back to Life. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 2747–\n2757\nWan Z, Zhang J, Chen D, Liao J (2021) High-Fidelity\nPluralistic Image Completion With Transformers. In:\nInt. Conf. Comput. Vis., pp 4692–4701\nWang C, Huang H, Han X, Wang J (2019a) Video In-\npainting by Jointly Learning Temporal Structure and\nSpatial Details. In: AAAI Conf. Artificial Intell., pp\n5232–5239\nWang C, Zhu Y, Yuan C (2022a) Diverse Image Inpaint-\ning with Normalizing Flow. In: Eur. Conf. Comput.\nVis., pp 53–69\nWang J, Wang C, Huang Q, Shi Y, Cai JF, Zhu Q,\nYin B (2020a) Image Inpainting Based on Multi-\nFrequency Probabilistic Inference Model. In: ACM\nInt. Conf. Multimedia, p 1–9\nWang N, Li J, Zhang L, Du B (2019b) MUSICAL:\nMulti-Scale Image Contextual Attention Learning for\nInpainting. In: Int. Joint Conf. Artificial Intell., pp\n3748–3754\nWang N, Ma S, Li J, Zhang Y, Zhang L (2020b) Multi-\nstage attention network for image inpainting. Pattern\nRecog 106:107448\nWang N, Zhang Y, Zhang L (2021a) Dynamic selec-\ntion network for image inpainting. IEEE Trans Image\nProcess 30:1784–1798\nWang S, Saharia C, Montgomery C, Pont-Tuset J, Noy\nS, Pellegrini S, Onoe Y, Laszlo S, Fleet DJ, Soricut R,\net al. (2023) Imagen editor and editbench: Advanc-\ning and evaluating text-guided image inpainting. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 18359–\n18369\nWang T, Ouyang H, Chen Q (2021b) Image Inpainting\nwith External-internal Learning and Monochromic\nBottleneck. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 5116–5125\nWang TC, Liu MY, Zhu JY, Liu G, Tao A, Kautz J,\nCatanzaro B (2018a) Video-to-Video Synthesis. In:\nAdv. Neural Inform. Process. Syst., vol 31\nWang W, Zhang J, Niu L, Ling H, Yang X, Zhang L\n(2021c) Parallel Multi-Resolution Fusion Network for\nImage Inpainting. In: Int. Conf. Comput. Vis., pp\n14559–14568\nWang W, Niu L, Zhang J, Yang X, Zhang L (2022b)\nDual-path Image Inpainting with Auxiliary GAN\nInversion. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 11411–11420\nWang X, Girshick RB, Gupta A, He K (2018b) Non-\nlocal Neural Networks. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 7794–7803\nWang Y, Tao X, Qi X, Shen X, Jia J (2018c) Image In-\npainting via Generative Multi-column Convolutional\nNeural Networks. In: Adv. Neural Inform. Process.\nSyst., pp 331–340\nWang Y, Chen YC, Tao X, Jia J (2020c) VCNet: A\nRobust Approach to Blind Image Inpainting. In: Eur.\nConf. Comput. Vis.\nWang Z, Simoncelli E, Bovik A (2003) Multiscale struc-\ntural similarity for image quality assessment. In:\nAsilomar Conf. Sign. Syst. Comput., vol 2, pp 1398–\n1402\nWang Z, Bovik AC, Sheikh HR, Simoncelli EP (2004)\nImage quality assessment: from error visibility to\nstructural similarity. IEEE Trans Image Process\n13(4):600–612\nWeng Y, Ding S, Zhou T (2022) A Survey on Improved\nGAN based Image Inpainting. In: Inter. Conf. Con-\nsumer Electronics and Comput. Engin., pp 319–322\nWexler Y, Shechtman E, Irani M (2007) Space-time\ncompletion of video. IEEE Trans Pattern Anal Mach\nIntell 29(3):463–476\nWoo S, Kim D, Park K, Lee JY, Kweon IS (2020) Align-\nand-Attend Network for Globally and Locally Coher-\nent Video Inpainting. In: Brit. Mach. Vis. Conf., pp\n1–13\nWu H, Zhou J, Li Y (2022) Deep generative model\nfor image inpainting with local binary pattern learn-\ning and spatial attention. IEEE Trans Multimedia\n24:4016–4027\nWu L, Zhang C, Liu J, Han J, Liu J, Ding E, Bai X\n(2019) Editing Text in the Wild. In: ACM Int. Conf.\nMultimedia, p 1500–1508\nWu X, Xie Y, Zeng J, Yang Z, Yu Y, Li Q, Liu W\n(2021) Adversarial learning with mask reconstruction\nfor text-guided image inpainting. In: ACM Int. Conf.\nMultimedia, pp 3464–3472\nXia W, Zhang Y, Yang Y, Xue JH, Zhou B, Yang MH\n(2022) Gan inversion: A survey. IEEE Trans Pattern\nAnal Mach Intell pp 1–17\n36\nWeize Quan 1,2 et al.\nXie C, Liu S, Li C, Cheng MM, Zuo W, Liu X, Wen\nS, Ding E (2019) Image Inpainting with Learnable\nBidirectional Attention Maps. In: Int. Conf. Comput.\nVis., pp 8858–8867\nXie M, Li C, Liu X, Wong TT (2020) Manga filling style\nconversion with screentone variational autoencoder.\nACM Trans Graph 39(6)\nXie M, Xia M, Liu X, Li C, Wong TT (2021) Seamless\nmanga inpainting with semantics awareness. ACM\nTrans Graph 40(4)\nXie S, Zhang Z, Lin Z, Hinz T, Zhang K (2023) Smart-\nBrush: Text and Shape Guided Object Inpainting\nWith Diffusion Model. In: IEEE Conf. Comput. Vis.\nPattern Recog., pp 22428–22437\nXie Y, Lin Z, Yang Z, Deng H, Wu X, Mao X, Li Q, Liu\nW (2022) Learning semantic alignment from image\nfor text-guided image inpainting. The Visual Com-\nputer 38(9-10):3149–3161\nXiong W, Yu J, Lin Z, Yang J, Lu X, Barnes C, Luo\nJ (2019) Foreground-Aware Image Inpainting. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 5833–\n5841\nXu N, Yang L, Fan Y, Yang J, Yue D, Liang Y, Price B,\nCohen S, Huang T (2018a) YouTube-VOS: Sequence-\nto-Sequence Video Object Segmentation. In: Eur.\nConf. Comput. Vis., pp 585–601\nXu R, Li X, Zhou B, Loy CC (2019) Deep flow-guided\nvideo inpainting. In: IEEE Conf. Comput. Vis. Pat-\ntern Recog., pp 3723–3732\nXu R, Guo M, Wang J, Li X, Zhou B, Loy CC (2021)\nTexture memory-augmented deep patch-based image\ninpainting. IEEE Trans Image Process 30:9112–9124\nXu T, Zhang P, Huang Q, Zhang H, Gan Z, Huang\nX, He X (2018b) Attngan: Fine-grained text to im-\nage generation with attentional generative adversar-\nial networks. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 1316–1324\nYamashita Y, Shimosato K, Ukita N (2022) Boundary-\nAware Image Inpainting With Multiple Auxiliary\nCues. In: IEEE Conf. Comput. Vis. Pattern Recog.\nWorksh., pp 619–629\nYan Z, Li X, Li M, Zuo W, Shan S (2018) Shift-Net:\nImage Inpainting via Deep Feature Rearrangement.\nIn: Eur. Conf. Comput. Vis., pp 3–19\nYang C, Lu X, Lin Z, Shechtman E, Wang O, Li H\n(2017) High-resolution image inpainting using multi-\nscale neural patch synthesis. In: IEEE Conf. Comput.\nVis. Pattern Recog., pp 6721–6729\nYang J, Qi Z, Shi Y (2020) Learning to Incorporate\nStructure Knowledge for Image Inpainting. In: AAAI\nConf. Artificial Intell., vol 34, pp 12605–12612\nYang L, Zhang Z, Song Y, Hong S, Xu R, Zhao Y,\nZhang W, Cui B, Yang MH (2023) Diffusion models:\nA comprehensive survey of methods and applications.\n2209.00796\nYeh RA, Chen C, Lim TY, Schwing AG, Hasegawa-\nJohnson M, Do MN (2017) Semantic Image Inpaint-\ning with Deep Generative Models. In: IEEE Conf.\nComput. Vis. Pattern Recog., pp 6882–6890\nYi Z, Tang Q, Azizi S, Jang D, Xu Z (2020) Contextual\nResidual Aggregation for Ultra High-Resolution Im-\nage Inpainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 7508–7517\nYu F, Koltun V (2016) Multi-Scale Context Aggrega-\ntion by Dilated Convolutions. In: Int. Conf. Learn.\nRepresent.\nYu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2018)\nGenerative image inpainting with contextual atten-\ntion. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 5505–5514\nYu J, Lin Z, Yang J, Shen X, Lu X, Huang TS (2019)\nFree-form image inpainting with gated convolution.\nIn: Int. Conf. Comput. Vis., pp 4471–4480\nYu T, Guo Z, Jin X, Wu S, Chen Z, Li W, Zhang Z, Liu\nS (2020) Region Normalization for Image Inpainting.\nIn: AAAI Conf. Artificial Intell., pp 12733–12740\nYu Y, Zhan F, Lu S, Pan J, Ma F, Xie X, Miao C\n(2021a) WaveFill: A Wavelet-Based Generation Net-\nwork for Image Inpainting. In: Int. Conf. Comput.\nVis., pp 14114–14123\nYu Y, Zhan F, WU R, Pan J, Cui K, Lu S, Ma F, Xie X,\nMiao C (2021b) Diverse Image Inpainting with Bidi-\nrectional and Autoregressive Transformers. In: ACM\nInt. Conf. Multimedia, p 69–78\nYu Y, Du D, Zhang L, Luo T (2022a) Unbiased Multi-\nmodality Guidance for Image Inpainting. In: Eur.\nConf. Comput. Vis., pp 668–684\nYu Y, Zhang L, Fan H, Luo T (2022b) High-Fidelity\nImage Inpainting with GAN Inversion. In: Eur. Conf.\nComput. Vis., pp 242–258\nZeng Y, Fu J, Chao H, Guo B (2019) Learning pyramid-\ncontext encoder network for high-quality image in-\npainting. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 1486–1494\nZeng Y, Fu J, Chao H (2020a) Learning Joint Spatial-\nTemporal Transformations for Video Inpainting. In:\nEur. Conf. Comput. Vis., Springer, pp 528–543\nZeng Y, Lin Z, Yang J, Zhang J, Shechtman E, Lu H\n(2020b) High-Resolution Image Inpainting with Iter-\native Confidence Feedback and Guided Upsampling.\nIn: Eur. Conf. Comput. Vis.\nZeng Y, Gong Y, Zhang J (2021a) Feature learning and\npatch matching for diverse image inpainting. Pattern\nRecog 119:108036\nZeng Y, Lin Z, Lu H, Patel VM (2021b) CR-Fill:\nGenerative Image Inpainting With Auxiliary Contex-\nDeep Learning-based Image and Video Inpainting: A Survey\n37\ntual Reconstruction. In: Int. Conf. Comput. Vis., pp\n14164–14173\nZeng Y, Fu J, Chao H, Guo B (2022) Aggregated con-\ntextual transformations for high-resolution image in-\npainting. IEEE Trans Vis Comput Graph pp 1–1\nZhang B, Gao Y, Zhao S, Liu J (2010) Local derivative\npattern versus local binary pattern: Face recognition\nwith high-order local pattern descriptor. IEEE Trans\nImage Process 19(2):533–544\nZhang H, Hu Z, Luo C, Zuo W, Wang M (2018a) Se-\nmantic Image Inpainting with Progressive Genera-\ntive Networks. In: ACM Int. Conf. Multimedia, p\n1939–1947\nZhang H, Mai L, Xu N, Wang Z, Collomosse J, Jin H\n(2019a) An internal learning approach to video in-\npainting. In: Int. Conf. Comput. Vis., pp 2720–2729\nZhang J, Niu L, Yang D, Kang L, Li Y, Zhao W, Zhang\nL (2019b) GAIN: Gradient Augmented Inpainting\nNetwork for Irregular Holes. In: ACM Int. Conf. Mul-\ntimedia, p 1870–1878\nZhang K, Fu J, Liu D (2022a) Flow-Guided Trans-\nformer for Video Inpainting. In: Eur. Conf. Comput.\nVis., pp 74–90\nZhang K, Fu J, Liu D (2022b) Inertia-Guided Flow\nCompletion and Style Fusion for Video Inpainting.\nIn: IEEE Conf. Comput. Vis. Pattern Recog., pp\n5982–5991\nZhang L, Agrawala M (2023) Adding conditional con-\ntrol to text-to-image diffusion models. 2302.05543\nZhang L, Chen Q, Hu B, Jiang S (2020a) Text-Guided\nNeural Image Inpainting. In: ACM Int. Conf. Multi-\nmedia, p 1302–1310\nZhang L, Barnes C, Wampler K, Amirghodsi S, Shecht-\nman E, Lin Z, Shi J (2022c) Inpainting at Mod-\nern Camera Resolution by Guided PatchMatch\nwith Auto-curation. In: Eur. Conf. Comput. Vis., pp\n51–67\nZhang L, Zhou Y, Barnes C, Amirghodsi S, Lin Z,\nShechtman E, Shi J (2022d) Perceptual Artifacts\nLocalization for Inpainting. In: Computer Vision –\nECCV 2022, pp 146–164\nZhang R, Isola P, Efros AA, Shechtman E, Wang O\n(2018b) The Unreasonable Effectiveness of Deep Fea-\ntures as a Perceptual Metric. In: IEEE Conf. Com-\nput. Vis. Pattern Recog., pp 586–595\nZhang R, Quan W, Wu B, Li Z, Yan DM (2020b) Pixel-\nwise dense detector for image inpainting. Comput\nGraph Forum 39(7)\nZhang R, Quan W, Zhang Y, Wang J, Yan DM (2022e)\nW-net: Structure and texture interaction for image\ninpainting. IEEE Trans Multimedia pp 1–12\nZhang S, He R, Sun Z, Tan T (2018c) Demeshnet: Blind\nface inpainting for deep meshface verification. IEEE\nTrans Inf Forensics Secur 13(3):637–647\nZhang W, Zhu J, Tai Y, Wang Y, Chu W, Ni B, Wang\nC, Yang X (2021) Context-Aware Image Inpainting\nwith Learned Semantic Priors. In: Int. Joint Conf.\nArtificial Intell., pp 1323–1329\nZhang Z, Zhao Z, Zhang Z, Huai B, Yuan J (2020c)\nText-guided image inpainting. In: ACM Int. Conf.\nMultimedia, pp 4079–4087\nZhao L, Mo Q, Lin S, Wang Z, Zuo Z, Chen H, Xing\nW, Lu D (2020) UCTGAN: Diverse Image Inpainting\nBased on Unsupervised Cross-Space Translation. In:\nIEEE Conf. Comput. Vis. Pattern Recog., pp 5740–\n5749\nZhao S, Cui J, Sheng Y, Dong Y, Liang X, Chang EI,\nXu Y (2021) Large Scale Image Completion via Co-\nModulated Generative Adversarial Networks. In: Int.\nConf. Learn. Represent.\nZhao W, Rao Y, Liu Z, Liu B, Zhou J, Lu J (2023)\nUnleashing text-to-image diffusion models for visual\nperception. arXiv preprint arXiv:230302153\nZheng C, Cham TJ, Cai J (2019) Pluralistic Image\nCompletion. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 1438–1447\nZheng C, Cham TJ, Cai J (2021a) Pluralistic free-form\nimage completion. Int J Comput Vis 129:2786–2805\nZheng C, Cham TJ, Cai J, Phung D (2022a) Bridging\nGlobal Context Interactions for High-Fidelity Image\nCompletion. In: IEEE Conf. Comput. Vis. Pattern\nRecog., pp 11512–11522\nZheng H, Zhang Z, Wang Y, Zhang Z, Xu M, Yang\nY, Wang M (2021b) GCM-Net: Towards Effective\nGlobal Context Modeling for Image Inpainting. In:\nACM Int. Conf. Multimedia, p 2586–2594\nZheng H, Lin Z, Lu J, Cohen S, Shechtman E, Barnes\nC, Zhang J, Xu N, Amirghodsi S, Luo J (2022b)\nImage Inpainting with Cascaded Modulation GAN\nand Object-Aware Training. In: Eur. Conf. Comput.\nVis., pp 277–296\nZhou B, Lapedriza A, Khosla A, Oliva A, Torralba A\n(2017) Places: A 10 million image database for scene\nrecognition. IEEE Trans Pattern Anal Mach Intell\n40(6):1452–1464\nZhou B, Zhao H, Puig X, Xiao T, Fidler S, Barriuso\nA, Torralba A (2018) Semantic understanding of\nscenes through the ade20k dataset. Int J Comput Vis\n127:302–321\nZhou X, Li J, Wang Z, He R, Tan T (2021) Image In-\npainting with Contrastive Relation Network. In: Int.\nConf. Pattern Recog., pp 4420–4427\nZhu M, He D, Li X, Li C, Li F, Liu X, Ding E, Zhang\nZ (2021) Image inpainting by end-to-end cascaded\nrefinement with mask awareness. IEEE Trans Image\nProcess 30:4855–4866\n38\nWeize Quan 1,2 et al.\nZou X, Yang L, Liu D, Lee YJ (2021) Progressive tem-\nporal feature alignment network for video inpaint-\ning. In: IEEE Conf. Comput. Vis. Pattern Recog.,\npp 16448–16457\n"
}