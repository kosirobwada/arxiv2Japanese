{
    "optim": "Combining Hierachical VAEs with LLMs for clinically meaningful timeline\nsummarisation in social media\nJiayu Song∗1, Jenny Chim∗1, Adam Tsakalidis1,3, Julia Ive1, Dana Atzil-Slonim2, Maria Liakata1,3\n1 Queen Mary University of London, London, UK\n2 Bar-Ilan University, Israel\n3 The Alan Turing Institute, London, UK\n{jiayu.song,c.chim,a.tsakalidis,j.ive,m.liakata}@qmul.ac.uk\ndana.slonim@gmail.com\nAbstract\nWe introduce a hybrid abstractive summari-\nsation approach combining hierarchical VAE\nwith LLMs (LlaMA-2) to produce clinically\nmeaningful summaries from social media user\ntimelines, appropriate for mental health moni-\ntoring. The summaries combine two different\nnarrative points of view: clinical insights in\nthird person useful for a clinician are generated\nby feeding into an LLM specialised clinical\nprompts, and importantly, a temporally sensi-\ntive abstractive summary of the user’s timeline\nin first person, generated by a novel hierarchical\nvariational autoencoder, TH-VAE. We assess\nthe generated summaries via automatic evalua-\ntion against expert summaries and via human\nevaluation with clinical experts, showing that\ntimeline summarisation by TH-VAE results in\nmore factual and logically coherent summaries\nrich in clinical utility and superior to LLM-only\napproaches in capturing changes over time.\n1\nIntroduction\nSocial media users discuss different aspects of their\nlives, providing important clues about their mental\nhealth. Previous work (De Choudhury et al., 2013;\nCoppersmith et al., 2014; Cohan et al., 2018; Chan-\ncellor and De Choudhury, 2020) have studied users’\nsocial media posts to help identify depression, bipo-\nlar disorder (Yates et al., 2017; Husseini Orabi et al.,\n2018) or self-harm (Zirikly et al., 2019), and there\nhave been efforts on multi-task learning to capture\nuser states at a particular moment in time (Benton\net al., 2017; Yang et al., 2023). Despite the impor-\ntance of longitudinal assessments of linguistic and\nother digital content for mental health clinical out-\ncomes (Velupillai et al., 2018), there is little work\non considering the evolution of an individual’s\nmental health over time through their social me-\ndia. Tsakalidis et al. (2022b,a) established the task\nof capturing changes (switches and escalations) in\n*Equal contribution.\nan individual’s mood over time and showed how\nidentifying these helps predict clinical assessments\nof suicidal ideation. However, currently clinicians\ndon’t have access to such information to assess\nindividuals’ mental-state and they mainly rely on\nself-reports completed by patients throughout psy-\nchotherapy (Crits-Christoph and Gibbons, 2021).\nAlthough standardized subjective measures are fun-\ndamental to mental health monitoring and research,\nthey have significant limitations, such as the extent\nof individuals’ self-awareness, their willingness to\ncomplete questionnaires, and the limited choice\nof responses (Kazdin, 2021). Providing concise\nsummaries that can capture fluctuations in individ-\nuals’ state-of-mind while emphasizing key clinical\nconcepts, can significantly assist in monitoring, pre-\nvention and early detection of mental health issues.\nSuch summaries would augment clinician capacity,\nprovide alternatives to standard questionnaires and\ncompensate for reduced access to mental health\nservices (Schwartz et al., 2023).\nTo the best of our knowledge we are the first to\npropose clinically meaningful summaries of social\nmedia user ‘timelines’ (sequences of chronologi-\ncally ordered posts by a user). Driven by the need\nto concisely summarise time-series language data\nwhich can span arbitrary lengths that exceed limits\nof many contemporary models and render purely\nextractive methods impractical, we propose a novel\nhybrid unsupervised abstractive method, Timeline\nHierarchical VAE (TH-VAE). Our system makes\nuse of a hierarchical variational autoencoder that\ncompresses timeline information into compact rep-\nresentations and a large language model (LLM),\ncreating a two-layer summary that combines two\ndifferent narrative points of view. Specifically: a\nhigh-level summary in third person useful for a\nclinician, is generated by feeding into an LLM\nspecialised clinical prompts and importantly a tem-\nporally sensitive abstractive summary of the user’s\ntimeline in first person (evidence summary), gen-\narXiv:2401.16240v2  [cs.CL]  16 Feb 2024\nerated by TH-VAE. The generation of the first per-\nson abstractive evidence summary via TH-VAE is\nguided by mental health related key-phrases ob-\ntained through instruction prompting by an LLM.\nThe final resulting high level summary covers as-\npects considered to be crucial by clinicians from\na wide range of therapeutic approaches, including\nindividuals’ diagnosis, intrapersonal and interper-\nsonal patterns and extent of mental state changes\nover time (Eells, 2022).1\nWe make the following contributions:\n• We develop a novel abstractive timeline sum-\nmarisation method (TH-VAE) based on adapt-\ning a hierarchical VAE model (NVAE)(§3.4) to\nlongitudinal social media data (user timelines).\n• We provide a new task, the creation of clini-\ncally meaningful summaries from social media\ndata. These summaries, generated in a hybrid\napproach, comprise high-level information in\nthird person consistent with clinical insights\n(diagnosis, inter- and intra- personal aspects,\nmoments of change) and evidence from a user’s\ntimeline, generated from the TH-VAE, support-\ning the assigned high-level insights. (§3)\n• We create a dataset of expert-written mental\nhealth summaries from longitudinal social me-\ndia data. A small sample of these is used to\nhelp with modeling (§3.5) and the rest is used\nfor evaluation (§4.3).\n• We provide a novel detailed evaluation method\nof the summaries based on preservation of clini-\ncal information, summary consistency, and use-\nfulness to clinicians, using semantic similarity\nbased metrics, NLI based inference, as well as\nexpert human evaluation (§4.3).\n• We conduct experiments using different un-\nsupervised summarisation methods based on\nLLMs and story generation (§4.2), showing su-\nperior performance for TH-VAE (§5).\n2\nRelated Work\nTimeline summarization aims at concisely sum-\nmarizing the evolution trajectory of a specific topic\nalong a timeline (Chen et al., 2019, 2023) and has\nprimarily focussed on news datasets. Methodolog-\nically it has involved both extractive and abstrac-\ntive methods; for example, Allan et al. (2001) de-\nfine temporal summaries by extracting a sentence\nper event in a news story while Li et al. (2021)\n1For a complete list, please see Table 6 in Appendix A.\nconstruct a multi-document event graph to capture\nlong distance dependencies between events, weight\nevents and extract an event summary sentence with\nmaximum event coverage. Li and Cardie (2014);\nChang et al. (2016); Wang et al. (2021); Hills et al.\n(2023a) detect important events in an individual’s\ntimeline and explore the event trajectory. In Ren\net al. (2013) timeline summarisation involves iden-\ntifying users’ interests by defining a social circle\nfrom a set of friends and selecting salient tweets to\nobtain an extractive summary. Chang et al. (2016)\nalso uses extractive summarisation and selects sen-\ntences based on different features (e.g., popularity-\nbased, temporal). Work in abstractive timeline sum-\nmarisation (Martschat and Markert, 2018; Steen\nand Markert, 2019) involves identifying clusters of\nnews or events to generate abstractive summaries\nfrom, or memory-based timeline summarisation to\ntrack the trajectory of events (Chen et al., 2019).\nBy contrast we consider a user’s timeline, a series\nof posts shared by an individual over a period of\ntime (Tsakalidis et al., 2022b). Such timelines do\nnot exhibit obvious or consistent topics, contain\nfew events and an explosion of emotions. Our goal\nin user timeline summarisation is to capture impor-\ntant information and synthesise it.\nSummaries in Mental Health. Although sum-\nmaries are clinically crucial for compiling informa-\ntion about individuals, there is limited literature on\nthe subject, with the primary focus being on expert-\ngenerated case study summarization (Eells, 2022).\nOnly recently, researchers have started to use NLP\ncapabilities to automatically generate summaries\nin the clinical domain. Manas et al. (2021) demon-\nstrated the usefulness of generating summarised\ndiagnoses from a single-session interview. Srivas-\ntava et al. (2022) summarised psychotherapy con-\nversations at the level of single counseling sessions\nproposing that summaries should exploit domain\nknowledge and conversational elements. On social\nmedia, Sotudeh et al. (2022) generated summaries\nof individual Reddit posts, relying on formatting\nconventions (i.e. TLDR) to extract short summaries\nprovided by the users themselves without further\ncontent constraints. Yang et al. (2023) instruction-\ntuned LLMs to generate mental health analyses\nfrom static social media text. By contrast our work\nsummarises user timelines and combines informa-\ntion from social media posts based on high-level\nexpert domain knowledge, important for evaluating\nindividuals’ progression over time.\nSummarising with LLMs. Current work on LLM-\nbased summarisation focuses on news articles or\ninstructional texts (Goyal et al., 2022; Zhang et al.,\n2023; Maynez et al., 2023), using simple prompts\n(e.g. “Summarize the following article:”). Wang\net al. (2023) took a multi-step approach, extracting\nevent information from news via curated guiding\nquestions then summarising the prompted outputs.\nIn our work, we summarise longitudinal user gen-\nerated content and use clinically-informed prompts\nto generate high-level mental health observations.\nSummary Evaluation. Existing mental health\nsummarisation works utilised natural language gen-\neration metrics, for example using ROUGE (Lin,\n2004) to measure n-gram overlap against refer-\nence documents (Manas et al., 2021; Srivastava\net al., 2022; Sotudeh et al., 2022). Srivastava et al.\n(2022) additionally applied BLEURT (Scialom\net al., 2021), a learned metric trained on ratings,\nQuestEval (Scialom et al., 2021), a metric based on\nquestion generation and answering, and MHIC, a\nmetric that they defined to assess information cap-\ntured in counselling summaries based on ROUGE.\nContrary to prior work, our task involves two-\nlayer mental health summaries combining first-\nperson social media content with high-level clinical\nconcepts in third person, posing unique evaluation\nchallenges. For example, data noisiness makes met-\nrics learned on well-formed texts unsuitable, and\nevaluation must assess consistency both between\nsummary layers and within the detailed high-level\nsummary itself. To this end, we extend the line of\nwork leveraging natural language inference (NLI)\nin summary factuality and consistency evaluation\n(Maynez et al., 2020; Laban et al., 2022).\n3\nMethodology\nTask Given a user’s timeline (a series of posts be-\ntween two dates (Tsakalidis et al., 2022b)), the goal\nis to generate an abstractive summary that reflects\nthe user’s mental state and how it changes over time.\nThis summary includes high-level information use-\nful for clinicians in third person, and corresponding\nevidence from the timeline in first person.\n3.1\nArchitecture Overview\nFig. 1 shows the summary generation process. It\nconsists of two sub-processes:\n(1) Abstractive generation of the timeline/evidence\nsummary (§3.4). We use three different unsuper-\nvised methods for creating the timeline summary in\npost0\n...\nTimeline summary\npost1\npostn\nLLM\nInstruction  \nprompts\nTimeline\nSummarisation Model\nresponse0\nHigh-level mental health summary\n...\nresponse1\nresponsen\nLLM\nMH summary0\nMH summary1\nMH summaryn\nLLM\nInstruction prompt (map)\nInstruction prompt (reduce)\nTH-VAE/LLaMA/skeleton\n(1)\n(2)\n(3)\nYour goal is to describe the\nindividual's mental state and\nidentify potential indicators\n...\nYour goal is to identify the\nperson's main intrapersonal\nand interpersonal pattern ...\nYour goal is to understand and\nsummarise changes over time in\nthis individual's well-being\nand functioning ...\nDiagnosis\nIntra- &\nInterpersonal\nMoments\nof\nChange\nFigure 1: Prompting framework for generating high-\nlevel summaries. Taking a first-person summarised time-\nline as input, we (1) prompt the LLM around different\nclinical topics, (2) summarise extracted inferences into\nprose per topic, and (3) combine the topic-specific inter-\nmediate summaries into a coherent, distilled document.\nFigure 2: Each timeline is separated into several seg-\nments based on ’MoC’. We highlight the key phrases.\nfirst person: Timeline hierarchical VAE (TH-VAE\n§3.2), our key methodological novelty; LLaMA\n(§4.2); a method from story generation (§4.2).\n(2) Generation of the High-level summary (§3.5).\nWe feed the generated timeline/evidence summary\ninto an instruction-tuned LLM (Llama), where\nprompts originate from a small sample of expert\nhuman annotation (§3.2), and generate high-level\nsummaries covering clinical aspects such as diag-\nnosis, inter- and intra- personal relationships and\nfluctuations in mood. The following subsections\ndescribe our novel timeline summarisation method\nusing an adapted hierarchical VAE (TH-VAE).\n3.2\nInput to Timeline Summarisation\nThe input to TH-VAE and the other timeline\nsummarisation methods is a user’s timeline, an-\nnotated with Moments of Change in mood\n(MoC)(Tsakalidis et al., 2022b).\nMoC annota-\ntions consist of Switches (sudden mood shifts, de-\nnoted by ‘IS’–In Switch– and ‘ISB’–In Switch\nBeginning– tags), and Escalations (gradual mood\nFigure 3: Overview of TH-VAE. The left of the dotted line shows the construction of the k-sentence representation\nused only during generation, informed by the key-phrases, while the right side shows the hierarchical structure of\nTH-VAE, and its components.① and ② represent the input during training and generation respectively.\nprogression, denoted by ‘IE’–In Escalation– and\n‘IEP’–In Escalation Peak– tags). We split the whole\ntimeline (see Fig. 2) into several segments (sub-\ntimelines) based on ’MoC’, so that consecutive\nposts with the same label (‘IE’ or ‘IEP’),(‘ISB’ or\n‘IS’) or ‘0’ are grouped together. This assumes each\nsegment consists of posts of a similar mood type,\nwhich facilitates capturing different features and\nrelations between them. This is somewhat simi-\nlar to news timeline summarisation which clusters\naround stories or events, with the additional chal-\nlenge that mood features are more evasive and we\nhope to model these through latent variables.\nKey phrases We asked clinical psychologists to\nannotate key phrases indicative of users’ mental\nhealth in three timelines. These phrases include\nmood related clues but also information on inter-\npersonal relationships, behaviors or events related\nto a user’s mental state (see highlights in Fig. 2).\nWe take these annotated timeline/key phrases pairs\nas examples and prompt LLaMA (Touvron et al.,\n2023) to annotate the rest of the timelines.\nTimeline summary representation For each\nsegment si, we input its corresponding key\nphrase sequence {e1, ..., ej, ..., en} into a GRU\nencoder (Cho et al., 2014) to get the key phrases\nencoding v = GRU([e1; ...; en]), which is repre-\nsented by the last hidden state of the GRU (see left\npart of Fig3). We calculate the similarity between\nv and each word embedding wi in the segment as\nthe weight αi:\nαi =\ncos(v, wi)\nPm\ni′=1 cos(v, wi′).\nThus\nsi\ncan\nbe\nrepresented\nby\na\nse-\nries\nof\nweighted\nword\nembeddings\n{α1w1, α2w2, ..., αmwm},\nwhere\nm\nis\nthe\nlength of si.\nWe encode it with the GRU\nencoder\nto\nget\nthe\nsegment\nrepresentation\nsenci=GRU([α1w1; α2w2; ...; αmwm]).\nIf the\ntimeline is divided into k segments, we can\nget\nk\nsegment encodings\nsenc1, ..., senck\nin\nthis way.\nWe concatenate these encodings in\nchronological order to get a segment sequence\n{senc1, senc2, ..., senck}, apply an average pooling\noperation (Avg Pool)(Lin et al., 2013) over the\noutput of the GRU encoder (See right part of Fig3)\nand feed it into the hierarchical part of TH-VAE to\ngenerate a timeline summary.\n3.3\nOverview of TH-VAE\nDue to the lack of gold summaries for training\npurposes, we have to construct the summary dis-\ntribution without any guidance. Thus we need a\nmodel that can learn an expressive distribution for\na long timeline (the longest timeline has 124 posts,\nand the longest of these posts has over 300 words).\nWe also need to construct a mental health related\nsummary distribution that can capture different fea-\ntures and establish the long-range dependencies\nbetween these features in the timeline. We propose\nTH-VAE, an unsupervised abstractive timeline sum-\nmarization model adapted from NVAE (Vahdat and\nKautz, 2020), to construct a more expressive prior\nfor a user timeline.\nIn the learning process, we split the timeline into\nseveral segments (sub-timelines, §3.4), considered\nto contain consecutive posts with similar mood,\nand train TH-VAE to learn the distribution of each\nsegment s by reconstructing it.\nWhen generating the evidence summary, we still\ntreat each segment as a unit. To help the model fo-\ncus on important information during generation we\nintroduce the notion of key phrases (§3.2). We use\nan automatic method based on an LLM to extract\nmental health related key phrases from each seg-\nment and encode key phrase-segment pairs with an\nattention mechanism. We concatenate the sequence\nof segment representations of a timeline in chrono-\nlogical order and input it into the hierarchical struc-\nture of TH-VAE to generate the timeline/evidence\nsummary (See Fig. 3, left part).\n3.4\nDocument Reconstruction via TH-VAE\nThe vanilla VAE assumes a prior p(z) of document\nx over latent variables z to be a Normal Gaus-\nsian distribution, and parameterizes an approxi-\nmate posterior distribution qϕ(z|x) given text x.\nIt uses KL (Kullback–Leibler divergence) to cal-\nculate the distance between p(z) and qϕ(z|x) and\ngradually reduces the distance between them in\ntraining. Finally, it samples from the hypothesised\nposterior distribution and generates the document\nx. It has been shown that the vanilla VAE can\nlead to over-regularising the posterior distribution,\nresulting in latent representations that do not repre-\nsent well the structure of the data (Klushyn et al.,\n2019; Alemi et al., 2018; Sønderby et al., 2016;\nRanganath et al., 2016; Vahdat and Kautz, 2020).\nHowever, for a long document assuming its distri-\nbution to be a Gaussian does not provide enough\nexpressive power; we need to be able to consider\nthe structure of different semantic elements and the\nrelationship between them.\nThe deep Hierarchical VAE (NVAE) (Vahdat and\nKautz, 2020), introduced for images, increases ex-\npressiveness by introducing several latent variables\nto generate large high-quality images, demonstrates\nthe superiority of the hierarchical VAE. Here, we\nadapt this model for long documents, resulting in\nTimeline Hierarchical VAE (TH-VAE), and use it\nas the basis of constructing mental health related\ntimeline representations.\n3.4.1\nHierarchical Component\nTH-VAE increases the expressiveness of the\napproximate posterior and prior by partition-\ning the latent variable z into l latent variables\nz={z1, z2, ..., zl}(Vahdat and Kautz, 2020). The\nprior is represented by p(z) = Q\nl p(zl|z<l) and\nit parameterises the approximate posterior distri-\nbution qϕ(z|x) = Q\nl qϕ(zl|z<l, x) which are rep-\nresented by factorial Normal distributions. This\nobjective is to maximise its lower bound as:\nL(θ; x) = −KL(qϕ(z1|x)||p(z1))\nL\nX\nl=2\nEqϕ(z<l|x)[−KL(qϕ(zl|x, z < l)||p(zl|z < l))]\n+Eqϕ(z|x)[log pθ(x|z)].\nBefore going into the hierarchical architecture, we\nuse a GRU encoder to encode the segment, to re-\nduce the impact from padding. Then we add an\nAvg Pool (Lin et al., 2013) over the output of the\nGRU encoder to fix the input length. Both TH-VAE\nand NVAE use multiple residual cells to construct\nthe hierarchical structure. In TH-VAE we simplify\nresidual cells to work with textual data rather than\nimages, and keep the optimization strategies in\nNVAE, i.e., BN (batch normalization) with Swish\nActivation and Squeeze and Excitation (SE). We\nuse two different residual cells: residual cell1 and\nresidual cell2. The input representations first go\nthrough a block which focuses on capturing the fea-\ntures of a segment and consists of residual cell1.\nTo form residual cell1 we use series BN, conv\n(CNN with one kernel size), SE as well as convmul\n(CNN with multiple kernel sizes), where the latter\nhelps with capturing the different features. Then,\nthe output of the block will go into the layered\ngroups (see Fig. 3–right), responsible for learning\nto capture the relationship between different fea-\ntures in segments and long-range dependencies be-\ntween them. Each group is used to encode the sub-\nlatent variables zi and consists of residual cell2s.\nSince convmul increased parameters without added\nbenefit, we only use conv in residual cell2. Fi-\nnally we add another block to integrate information.\nDuring training, the whole hierarchical architecture\nis used to learn the distribution of each segment,\nby learning features and long-range dependencies\nwithin them via segment reconstruction (as shown\nin the right part of Fig3). Then during generation a\nsequence of segments (a whole timeline) is input\nto TH-VAE to generate similarly structured text.\nThe left part of Fig3 shows the process of encod-\ning the sequence of segments. When decoding, we\nuse the same decoder component as in (Song et al.,\n2022), comprising a transformer decoder (we load\npre-trained parameters from BART) followed by a\nGRU decoder.\n3.5\nHigh-level Mental Health Summarization\nWe focus on information considered important in\nsummarising individuals’ mental states according\nto therapeutic approaches (Eells, 2022). Although\nall users broadly talk about mental health related\ntopics in this dataset, the extent to which clinical\nconcepts appear in each one varies due to natural\nindividual differences. As such, when annotators\nwrite gold summaries and when we generate model-\nwritten ones, we focus on clinical information that\nis present, ignoring true negatives.\nWe prompt an instruction-tuned LLM follow-\ning a multi-stage framework (Fig. 1) to generate\nhigh-level mental health summaries based on time-\nline summaries. In the map stage, we instruct the\nmodel to provide inferences based on the timeline\nsummary focusing on clinical topics (Appendix\nA, Table 6), such as presenting issues, inter/intra-\npersonal patterns, and moments of change. Instruc-\ntions and prompts are in Appendix B. In the reduce\nstage, we iteratively prompt the model to synthe-\nsise extracted observations into a concise summary.\n4\nExperiments\n4.1\nEvaluation Dataset Creation\nWe work with three clinical psychology graduate\nstudents who are fluent in English to create gold\nevidence-supported summaries. We use the dataset\ncollected by Tsakalidis et al. (2022b) comprising\n500 anonymised user timelines from Talklife. The\nnumber of posts in each timeline varies ([12-124]).\nWe sample 30 timelines for annotators to highlight\ninformation related to individuals’ mental states\nand write high-level summaries which include diag-\nnosis, intra- and interpersonal patterns and mental\nstate changes over time. We use these for evalua-\ntion and 3 additional held out timelines for develop-\nment and in-context learning key phrase extraction.\n4.2\nModels & Baselines\nWe compare our method against existing models for\nunsupervised abstractive opinion summarisation.\nFor experiment settings, model specifications, and\nprompts refer to Appendix A and B.\nSkeleton-based model is an unsupervised method\nproposed for story generation which encodes the\nskeleton (phrases that express the key meaning of\nsentences) to generate a detailed and polished sen-\ntence (Xu et al., 2018). We include it as one of\nthe models to compare against as like TH-VAE it\nuses key phrases to generate a story/timeline in an\nunsupervised way. The key phrases provided are\nthe same as for TH-VAE.\nLLaMA We prompt a LLM to extract key phrases\nand then write TLDR-type summaries (Völske\net al., 2017) focusing on the key phrases. Result-\ning summaries are similar to concise user-authored\nones commonly found in social media data.\nHigh-level Summary To obtain corresponding\nmental health summaries, we feed timeline sum-\nmaries generated via TH-VAE and the above base-\nlines into the LLM prompting framework outlined\nin §3.5. In addition, to see the benefits of time-\nline summarisation and specific clinical prompts,\nwe implement a high-level and prompt-only naive\nbaseline. It involves splitting timelines into chunks,\nprompting the LLM to write a mental health sum-\nmary of each chunk, and rewriting the chunk-level\nsummaries into a single coherent document.\n4.3\nEvaluation\nWe use summaries by clinical experts (§4.1) in au-\ntomatic evaluation. In human evaluation we work\nwith the same experts, where they rated summaries\nfor factual consistency, salient meaning preserva-\ntion, and facets of usefulness.2 Details on proce-\ndure and metrics are in Appendix A.2-A.3.\nSalient information preservation.\nWe adapt\nMHIC (Srivastava et al., 2022) to assess whether\ntimeline summaries capture clinically relevant in-\nformation. Given evidence E and timeline sum-\nmary sentences T, we average the maximum recall-\noriented BERTScore (Zhang et al., 2020):\nMHICsem =\n1\n|E|\nX\ne∈E\nmax\nt∈T RBERT(e, t)\nFactual consistency. To measure whether timeline\nsummaries are consistent with original timelines,\n2We merged aspects in human evaluation after a pilot,\nbased on expert feedback. Given the LLM’s ability to output\nwell-formed text, the cognitively taxing nature of the task, and\ntime constraints, we prioritised aspects that demand domain\nexpertise rather than general linguistic quality (e.g. fluency).\nTimeline Summary\nHigh-level Summary\nI hate school. im so scared.i love the cat so\nmuch. i’m gonna go be counter for a while.\nunknown good night all! i hate my nose. fuck\nthe sat. i got a commitment ring 3 i’ve got\ncounterbodied under my eyes. i feel alone i\nfeel like everyone hates me. i hate saying this\nbecause it sounds bad, but i really want some-\none to pay attention to me. solid my mom\nonly cares about my brothers learning disabil-\nities and her new boyfriend expressive. my\nboyfriend goes throug of ptsd like depression\nsymptoms, so he gets distant sometimes levels.\ni literally have no friends argue. i’ve always\nbeen quiet but sometimes i just want someone\nnever pay attention to me. i argue to know\nonly when’t be a body. i have to know. i’ll\nhave to be happy.\nThe individual is likely experiencing a range of mental health issues, including depression, anxiety,\nand low self-esteem. They report feeling isolated, lonely, and frustrated, with a history of stressful life\nevents. The individual’s mood appears to be low, with expressions of sadness, frustration, and anger.\nIt is evident that they have a profound craving for attention and connection with others, as well\nas a yearning for acceptance and validation. However, their fear of rejection and abandonment\nhinders them from forming and maintaining healthy relationships. Moreover, their self-criticism and\npreoccupation with perceived shortcomings indicate a lack of self-compassion and acceptance of their\nstrengths and vulnerabilities.\nThe individual’s mood and well-being have been observed to fluctuate over time, with both positive\nand negative changes experienced. The individual has expressed a range of emotions, including\nsadness, loneliness, and frustration, as well as moments of happiness and positivity. Noteworthy\npositive changes include their excitement about having a cat and receiving a commitment ring, which\nare associated with positive emotions and a sense of joy. However, the individual also struggles with\nschool and experiences anxiety and depression, which are linked to negative emotions such as sadness,\nfear, and frustration.\nTable 1: Example TH-VAE timeline summary and its high-level summary. Examples for all systems in Appendix C.\nwe apply the faithfulness score used in traditional\nsummary evaluation with a modified procedure that\nsplits timelines into chunks. Given a chunked time-\nline D and its timeline summary T, for every sen-\ntence t in T, we calculate the maximum probability\nof a timeline chunk d in D entailing t using a NLI\nmodel and average across all summary sentences.\nFCTimeline = 1\n|T|\nX\nt∈T\nmax\nd∈D NLI(Entail|d, t)\nNext we assess the consistency of high-level model-\ngenerated summaries S with human-written ones\nG, where consistency is the absence of contradic-\ntion. We define C to be a function that quantifies\nthe consistency of text B based on text A:\nC(A, B) =\n1\n|A|·|B|\nP\na∈A\nP\nb∈B (1 − NLI(Contradict|a, b))\nWe calculate the consistency of high-level sum-\nmaries to gold summaries as FCExpert = C(G, S).\nEvidence appropriateness. We measure the con-\nsistency of high-level summaries S to their accom-\npanying timeline summaries T via EA = C(T, S).\nCoherence. We estimate how easy it is to follow\nthe summary and how effectively the mental health\nsummary integrates information from the timeline\nsummary using BARTScore (Yuan et al., 2021).\nWe evaluate logical coherence via intra-summary\nNLI (IntraNLI), taking the mean consistency of\neach sentence against all other sentences to assess\nthe logical interconnection of information within\nthe mental health summary.\nFluency. We separately estimate fluency for time-\nline and high-level summaries using perplexity\n(PPL) under GPT-2-XL (Radford et al., 2019).\nUsefulness. Summaries should help the clinician\nunderstand the client’s condition. This is assessed\nAspect\nMetric\nLLaMA\nTH-VAE\nSkeleton\nNaive\nSMP\nMHICsem\n.65\n.66\n.57\n–\nFC\nFCTimeline\n.63\n.63\n.21\n–\nFCExpert\n.95\n.96\n.95\n.93\nEA\nEA\n.97\n.97\n.95\n–\nCoherence\nIntraNLI\n.95\n.96\n.95\n.93\nBARTScore\n-2.96\n-3.10\n-3.09\n–\nFluency\nPPLTimeline (↓)\n13.80\n56.33\n31.82\n–\nPPLHigh-level (↓)\n9.32\n9.30\n9.45\n11.38\nTable 2: Automatic evaluation for salient meaning\npreservation (SMP), factual consistency (FC), evidence\nappropriateness (EA), coherence, and fluency. Higher\nis better, except for PPL. BARTScore uses log likeli-\nhood, hence higher (less negative) is better. Best in bold,\nsignificant improvement over second-best underlined.\nvia human evaluation only, with respect to general\nusefulness and specific categories (diagnosis, intra-\nand interpersonal patterns and MoC). Details are\navailable in the Appendix in Table 6.\n5\nResults\n5.1\nAutomatic evaluation\nTable 1 shows example summaries. We perform\ntwo-tailed permutation tests in our comparisons\nreporting statistical significance at α = .05. TH-\nVAE and LLaMA generated significantly higher\nquality summaries compared to other baselines.\nTH-VAE and LLaMA were comparable on most\nmetrics, preserving mental health information\n(MHICsem) while similarly consistent with the\nsource (FCTimeline) in timeline summaries and fac-\ntually consistent with human-written references in\nhigh-level mental health summaries (FCExpert).\nTwo-tailed\npermutation\ntests\nshowed\nthat\nLLaMA timeline summaries were significantly\nmore fluent (PPLTimeline), in line with its tendency\nto normalise text (see examples, Appendix B).\nThese tests also indicate that high-level summaries\nwere comparably coherent in terms of ease of read-\ning and integrating information from timeline sum-\nmaries (BARTScore). This is expected since all\nmethods used the same prompting framework to\ngenerate high-level summaries. However, TH-VAE\nachieved significantly higher IntraNLI, suggesting\nits timeline summaries allow for more logically\ncoherent synthesis of detailed clinical information.\n5.2\nHuman evaluation\nWe selected three systems for human evaluation:\nLLaMA, TH-VAE, and the naive LLaMA baseline.\nThis allows us to compare top-performing models\nand understand how removing timeline summarisa-\ntion and clinical prompting steps may impact sum-\nmary quality perceived by human judges. TH-VAE\nproduced summaries considered the most factu-\nally consistent and useful in summarising changes\n(MoC) among compared models. Human judges\nfound LLaMA summaries generated with clinical\nprompts to be most useful in other usefulness cri-\nteria, whereas LLaMA with a simple summarisa-\ntion prompt was consistently least useful. Notably,\nLLaMA summaries without clinical prompts were\nrated as more factually consistent than those with\nclinical prompts, suggesting they adhered to the\nsource timeline, but were impacted by lack of guid-\nance (Table 3). A detailed qualitative evaluation in\nAppendix A.6 shows that Llama timelines present\nmore hallucinations than TH-VAE.\nAspect\nLLaMA\nTH-VAE\nNaive\nFactual Consistency\n3.08\n3.35\n3.28\nUsefulness (General)\n3.38\n3.28\n2.55\n(Diagnosis)\n3.40\n3.25\n2.93\n(Inter-& Intrapersonal)\n3.48\n3.33\n2.23\n(MoC)\n3.30\n3.35\n1.18\nTable 3: Human evaluation results based on 5-point\nLikert scales (1 is worst, 5 is best). Best in bold.\n5.3\nAblation\nWe performed ablation studies to investigate the\nimportance of key phrases (§3.2) and elaborate\nclinical prompts for the final summary generation\n(§3.5) in TH-VAE and LLaMA. Details are in Ap-\npendix A.4, Tables 4 and 5. We experimented\nwith (a) removing keyphrases but keeping the clin-\nical prompts and (b) keeping the keyphrases, but\nprompting the LLM to summarise the high-level\nsummary directly without any guiding topics.\nIn both systems, removing keyphrases results\nin timeline summaries capturing less salient in-\nformation (MHICsem), and degraded logical con-\nnectedness (IntraNLI), evidence appropriateness\n(EA), and factual consistency with gold summaries\n(FCExpert), showing that keyphrases help focus gen-\neration on mental health related information. In\nTH-VAE, removing keyphrases made timeline sum-\nmaries less consistent with the source (FCTimeline),\nand we observed the same trend to a greater ef-\nfect when clinical prompts are removed. Thus, the\nelaborate prompt does provide an efficient clinical\nguidance for the LLM to generate summaries.\nIn LLaMA, removing keyphrases improves time-\nline summary faithfulness (FCTimeline) at the ex-\npense of clinical information (MHICsem). This\nshows the role of keyphrases guided by domain ex-\npertise as anchors in summaries of long texts. Con-\nsistency with experts (FCExpert) are similar across\nablation settings but highest when both are em-\nployed, underlining the importance of using these\ncomponents in conjunction.\n6\nConclusions\nWe present a novel method for hybrid abstractive\nsummarisation using hierarchical VAE and LLMs\nand the first approach to creating clinically mean-\ningful mental health summaries from users’ so-\ncial media timelines. Our approach results in sum-\nmaries with a dual narrative perspective: high-level\nthird person information useful for clinicians is\ncombined with first person corresponding evidence\nfrom users’ timelines. Abstractive timeline sum-\nmarisation is performed by three different systems\n(LLM-, TH-VAE- and skeleton-based) whose gen-\neration is guided by key-phrases obtained by an\nLLM through instruction prompting. High-level\nclinical summaries in third-person are generated\nby feeding the timeline summaries from all three\nsystems into an LLM. Our proposed timeline sum-\nmariser, TH-VAE, based on a hierarchical VAE for\nlong texts, can capture long dependencies between\nsub-timelines and while LLM timeline summaries\nare the most fluent, they lag behind TH-VAE on\nlogical coherence and factuality. From a clinical\npsychology viewpoint our work enables clinician\naccess to consented clients’ social media data al-\nlowing them to understand changes in their mental\nstate over time. Importantly it enables generation of\nautomated summaries emphasizing essential clin-\nical concepts which can aid mental health profes-\nsionals to quickly grasp an individual’s psychologi-\ncal condition and progression.\nLimitations\nOur work considers the segmentation of timelines\nin terms of moments of change as changes in an\nindividual’s mood judged on the basis of their self-\ndisclosure of their well-being. This is faced by\ntwo limiting factors: (a) users may not be self-\ndisclosing important aspects of their daily lives and\n(b) while also (Hills et al., 2023b) segment user\ntimelines based on moments of change in mood\nthere may be other appropriate ways to effectively\nsegment timelines into semantically related tem-\nporal units. For example timelines could be seg-\nmented based on symptoms or life events which\ncould also be evolving over time. Empirically we\nhave not found topics to be an effective way of\nidentifying sub-timelines and segments within a\ntimeline but the best way of segmenting the time-\nlines is an open research direction.\nThough our models could be tested in cases of\nnonself-disclosure (given the appropriate ground\ntruth labels), the analysis and results presented in\nthis work should not be used to infer any conclusion\non such cases.\nWhile we believe our methods for clinically\nmeaningful longitudinal summarisation of social\nmedia data for mental health monitoring to be ap-\nplicable to non-social media longitudinal data such\nas therapy sessions, this remains future work.\nIn the present study, we have conducted a com-\nparison between timeline summarization using\nTH-VAE, skeleton-based and LLM-generated sum-\nmaries. A further qualitative evaluation by a senior\nclinical therapist found that the summaries gener-\nated by Llama often reached conclusions that were\nnot sufficiently supported by the evidence provided\nin the timeline, and were lower in factual consis-\ntency than the TH-VAE. The TH-VAE and Llama\nwere effective in summarizing the intrapersonal\nand interpersonal patterns and moments of change,\nbut their depiction of diagnostic aspects was only\nmoderately accurate, characterized by some inac-\ncuracies and omissions. These findings will help\npinpoint areas where our models can be enhanced\nand refined.\nEthics Statement\nEthics institutional review board (IRB) approval\nwas obtained from the corresponding ethics board\nof the lead University prior to engaging in this\nresearch study. Our work involves ethical consider-\nations around the analysis of user generated content\nshared on a peer support network (TalkLife). A li-\ncense was obtained to work with the user data from\nTalkLife and a project proposal was submitted to\nthem in order to embark on the project. The current\npaper focuses on the summarisation of users’ social\nmedia timelines for mental health monitoring, by\nusing moments of change (MoC) in mood as the\nanchors to segment timelines. These changes in-\nvolve recognising sudden shifts in mood (switches\nor escalations). Expert clinical annotators were\npaid fairly in line with University payscales. They\nwere alerted about potentially encountering disturb-\ning content and were advised to take breaks. The\nannotations are used to provide examples to an in\nhouse LLMand evaluate natural language process-\ning models for creating mental health summaries\nbased on users social media timelines. Working\nwith datasets such as TalkLife and data on online\nplatforms where individuals disclose personal infor-\nmation involves ethical considerations (Mao et al.,\n2011; Keküllüoglu et al., 2020). Such consider-\nations include careful analysis and data sharing\npolicies to protect sensitive personal information.\nThe data has been de-identified both at the time\nof sharing by TalkLife but also by the research\nteam to make sure that no user handles and names\nare visible. Any examples used in the paper are\nparaphrased (generated summaries). Potential risks\nfrom the application of our work in being able to\nsummarise the mental health of individuals based\non their social media timelines are akin to those in\nearlier work on personal event identification from\nsocial media and the detection of suicidal ideation.\nPotential mitigation strategies include restricting\naccess to the code base and corpus used for evalu-\nation by requiring an NDA, as with other mental\nhealth datasets.\nThe final high level summaries in all cases are\nobtained by feeding the timeline summaries into an\nLLM. Given that LLMs are susceptible to factual\ninaccuracies, often referred to as ’hallucinations,’\nand tend to exhibit biases, the clinical summaries\nthey generate may contain errors that could have\nserious consequences in the realm of mental health\ndecision-making. These inaccuracies can encom-\npass anything from flawed interpretations of the\ntimeline data to incorrect diagnoses and even rec-\nommendations for potentially harmful treatments.\nMental health professionals must exercise caution\nwhen relying on such generated clinical summaries.\nThese summaries should not serve as substitutes\nfor therapists in making clinical judgments. In-\nstead, well-trained therapists must skillfully incor-\nporate these summaries into their clinical thought\nprocesses and practices. Significant efforts are re-\nquired to establish the scientific validity of the clini-\ncal benefits offered by these summaries before they\ncan be integrated into routine clinical practice.\nReferences\nAlexander Alemi, Ben Poole, Ian Fischer, Joshua Dillon,\nRif A Saurous, and Kevin Murphy. 2018. Fixing a\nbroken elbo. In International conference on machine\nlearning, pages 159–168. PMLR.\nJames Allan, Rahul Gupta, and Vikas Khandelwal. 2001.\nTemporal summaries of news topics. In SIGIR 2001:\nProceedings of the 24th Annual International ACM\nSIGIR Conference on Research and Development in\nInformation Retrieval, September 9-13, 2001, New\nOrleans, Louisiana, USA, pages 10–18. ACM.\nAdrian Benton, Margaret Mitchell, and Dirk Hovy. 2017.\nMultitask learning for mental health conditions with\nlimited social media data. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 1, Long\nPapers, pages 152–162, Valencia, Spain. Association\nfor Computational Linguistics.\nStevie Chancellor and Munmun De Choudhury. 2020.\nMethods in predictive techniques for mental health\nstatus on social media: a critical review. NPJ digital\nmedicine, 3(1):43.\nYi Chang, Jiliang Tang, Dawei Yin, Makoto Yamada,\nand Yan Liu. 2016. Timeline summarization from\nsocial media with life cycle models. In IJCAI, pages\n3698–3704.\nXiuying Chen, Zhangming Chan, Shen Gao, Meng-\nHsuan Yu, Dongyan Zhao, and Rui Yan. 2019. Learn-\ning towards abstractive timeline summarization. In\nIJCAI, pages 4939–4945.\nXiuying Chen, Mingzhe Li, Shen Gao, Zhangming\nChan, Dongyan Zhao, Xin Gao, Xiangliang Zhang,\nand Rui Yan. 2023. Follow the timeline! generating\nan abstractive and extractive timeline summary in\nchronological order. ACM Transactions on Informa-\ntion Systems, 41(1):1–30.\nKyunghyun Cho, Bart Van Merriënboer, Caglar Gul-\ncehre, Dzmitry Bahdanau, Fethi Bougares, Holger\nSchwenk, and Yoshua Bengio. 2014.\nLearning\nphrase representations using rnn encoder-decoder for\nstatistical machine translation. Proceedings of the\n2014 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2014, October 25-29,\n2014, Doha, Qatar, A meeting of SIGDAT, a Special\nInterest Group of the ACL.\nArman Cohan, Bart Desmet, Andrew Yates, Luca Sol-\ndaini, Sean MacAvaney, and Nazli Goharian. 2018.\nSMHD: a large-scale resource for exploring online\nlanguage usage for multiple mental health condi-\ntions. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1485–\n1497, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nGlen Coppersmith, Mark Dredze, and Craig Harman.\n2014. Quantifying mental health signals in twitter.\nIn Proceedings of the workshop on computational\nlinguistics and clinical psychology: From linguistic\nsignal to clinical reality, pages 51–60.\nPaul Crits-Christoph and Mary Beth Connolly Gibbons.\n2021. Psychotherapy process-outcome research: Ad-\nvances in understanding causal connections. Bergin\nand Garfield’s handbook of psychotherapy and be-\nhavior change, pages 263–296.\nMunmun De Choudhury, Michael Gamon, Scott Counts,\nand Eric Horvitz. 2013. Predicting depression via\nsocial media. In Proceedings of the international\nAAAI conference on web and social media, volume 7,\npages 128–137.\nTracy D Eells. 2022. Handbook of psychotherapy case\nformulation. Guilford Publications.\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022.\nNews summarization and evaluation in the era of\ngpt-3. arXiv preprint.\nAnthony Hills, Adam Tsakalidis, Federico Nanni, Ioan-\nnis Zachos, and Maria Liakata. 2023a. Creation and\nevaluation of timelines for longitudinal user posts.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 3791–3804, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nAnthony Hills, Adam Tsakalidis, Federico Nanni, Ioan-\nnis Zachos, and Maria Liakata. 2023b. Creation and\nevaluation of timelines for longitudinal user posts.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 3791–3804, Dubrovnik, Croatia.\nAssociation for Computational Linguistics.\nAhmed Husseini Orabi, Prasadith Buddhitha, Mahmoud\nHusseini Orabi, and Diana Inkpen. 2018. Deep learn-\ning for depression detection of Twitter users. In Pro-\nceedings of the Fifth Workshop on Computational\nLinguistics and Clinical Psychology: From Keyboard\nto Clinic, pages 88–97, New Orleans, LA. Associa-\ntion for Computational Linguistics.\nAlan E Kazdin. 2021. Extending the scalability and\nreach of psychosocial interventions.\nDilara Keküllüoglu, Walid Magdy, and Kami Vaniea.\n2020. Analysing privacy leakage of life events on\ntwitter. In Proceedings of the 12th ACM Conference\non Web Science, pages 287–294.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. 3rd International\nConference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference\nTrack Proceedings.\nAlexej Klushyn, Nutan Chen, Richard Kurle, Botond\nCseke, and Patrick van der Smagt. 2019. Learning\nhierarchical priors in vaes. Advances in neural infor-\nmation processing systems, 32.\nPhilippe Laban, Tobias Schnabel, Paul N. Bennett, and\nMarti A. Hearst. 2022. SummaC: Re-visiting NLI-\nbased models for inconsistency detection in summa-\nrization. Transactions of the Association for Compu-\ntational Linguistics, 10:163–177.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nJiwei Li and Claire Cardie. 2014. Timeline generation:\nTracking individuals on twitter. In Proceedings of the\n23rd International Conference on World Wide Web,\nWWW ’14, page 643–652, New York, NY, USA.\nAssociation for Computing Machinery.\nManling Li, Tengfei Ma, Mo Yu, Lingfei Wu, Tian Gao,\nHeng Ji, and Kathleen McKeown. 2021. Timeline\nsummarization based on event graph compression via\ntime-aware optimal transport. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 6443–6456, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nMin Lin, Qiang Chen, and Shuicheng Yan. 2013. Net-\nwork in network. CoRR, abs/1312.4400.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRo{bert}a: A robustly optimized {bert} pretraining\napproach.\nGaur Manas,\nVamsi Aribandi,\nUgur Kursuncu,\nAmanuel Alambo, Valerie L Shalin, Krishnaprasad\nThirunarayan, Jonathan Beich, Meera Narasimhan,\nand Amit Sheth. 2021. Knowledge-infused abstrac-\ntive summarization of clinical diagnostic interviews:\nFramework development study. JMIR Ment Health,\n8(5):e20865.\nHuina Mao, Xin Shuai, and Apu Kapadia. 2011. Loose\ntweets: An analysis of privacy leaks on twitter. In\nProceedings of the 10th Annual ACM Workshop on\nPrivacy in the Electronic Society, WPES ’11, page\n1–12, New York, NY, USA. Association for Comput-\ning Machinery.\nSebastian Martschat and Katja Markert. 2018. A tempo-\nrally sensitive submodularity framework for timeline\nsummarization. In Proceedings of the 22nd Confer-\nence on Computational Natural Language Learning,\npages 230–240, Brussels, Belgium. Association for\nComputational Linguistics.\nJoshua Maynez, Priyanka Agrawal, and Sebastian\nGehrmann. 2023.\nBenchmarking large language\nmodel capabilities for conditional generation.\nIn\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 9194–9213, Toronto, Canada.\nAssociation for Computational Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nRajesh Ranganath, Dustin Tran, and David Blei. 2016.\nHierarchical variational models. In International con-\nference on machine learning, pages 324–333. PMLR.\nZhaochun Ren, Shangsong Liang, Edgar Meij, and\nMaarten de Rijke. 2013. Personalized time-aware\ntweets summarization. In Proceedings of the 36th in-\nternational ACM SIGIR conference on Research and\ndevelopment in information retrieval, pages 513–522.\nBrian Schwartz, Jessica Uhl, and Dana Atzil-Slonim.\n2023. Assessments and measures in psychotherapy\nresearch: going beyond self-report data. Frontiers in\nPsychiatry, 14:1276222.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, Jacopo Staiano, Alex Wang,\nand Patrick Gallinari. 2021. QuestEval: Summariza-\ntion asks for fact-based evaluation. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natu-\nral Language Processing, pages 6594–6604, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nCasper Kaae Sønderby, Tapani Raiko, Lars Maaløe,\nSøren Kaae Sønderby, and Ole Winther. 2016. Lad-\nder variational autoencoders. Advances in neural\ninformation processing systems, 29.\nJiayu Song, Iman Munire Bilal, Adam Tsakalidis, Rob\nProcter, and Maria Liakata. 2022. Unsupervised opin-\nion summarisation in the Wasserstein space. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, pages 8592–\n8607, Abu Dhabi, United Arab Emirates. Association\nfor Computational Linguistics.\nSajad Sotudeh, Nazli Goharian, and Zachary Young.\n2022. MentSum: A resource for exploring summa-\nrization of mental health online posts. In Proceedings\nof the Thirteenth Language Resources and Evalua-\ntion Conference, pages 2682–2692, Marseille, France.\nEuropean Language Resources Association.\nAseem Srivastava, Tharun Suresh, Sarah P. Lord,\nMd Shad Akhtar, and Tanmoy Chakraborty. 2022.\nCounseling summarization using mental health\nknowledge guided utterance filtering. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowl-\nedge Discovery and Data Mining, KDD ’22, page\n3920–3930, New York, NY, USA. Association for\nComputing Machinery.\nJulius Steen and Katja Markert. 2019. Abstractive time-\nline summarization. In Proceedings of the 2nd Work-\nshop on New Frontiers in Summarization, pages 21–\n31, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAdam Tsakalidis, Jenny Chim, Iman Munire Bilal, Ayah\nZirikly, Dana Atzil-Slonim, Federico Nanni, Philip\nResnik, Manas Gaur, Kaushik Roy, Becky Inkster,\nJeff Leintz, and Maria Liakata. 2022a. Overview of\nthe CLPsych 2022 shared task: Capturing moments\nof change in longitudinal user posts. In Proceedings\nof the Eighth Workshop on Computational Linguistics\nand Clinical Psychology, pages 184–198, Seattle,\nUSA. Association for Computational Linguistics.\nAdam Tsakalidis, Federico Nanni, Anthony Hills, Jenny\nChim, Jiayu Song, and Maria Liakata. 2022b. Identi-\nfying moments of change from longitudinal user text.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 4647–4660, Dublin, Ireland.\nAssociation for Computational Linguistics.\nArash Vahdat and Jan Kautz. 2020.\nNvae: A deep\nhierarchical variational autoencoder. Advances in\nneural information processing systems, 33:19667–\n19679.\nSumithra Velupillai, Hanna Suominen, Maria Liakata,\nAngus Roberts, Anoop D Shah, Katherine Morley,\nDavid Osborn, Joseph Hayes, Robert Stewart, Johnny\nDowns, et al. 2018. Using clinical natural language\nprocessing for health outcomes research: overview\nand actionable suggestions for future advances. Jour-\nnal of biomedical informatics, 88:11–19.\nMichael Völske, Martin Potthast, Shahbaz Syed, and\nBenno Stein. 2017. TL;DR: Mining Reddit to learn\nautomatic summarization.\nIn Proceedings of the\nWorkshop on New Frontiers in Summarization, pages\n59–63, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nShang Wang, Zhiwei Yang, and Yi Chang. 2021. Bring-\ning order to episodes: Mining timeline in social me-\ndia. Neurocomputing, 450:80–90.\nYiming Wang, Zhuosheng Zhang, and Rui Wang. 2023.\nElement-aware summarization with large language\nmodels: Expert-aligned evaluation and chain-of-\nthought method. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8640–8665,\nToronto, Canada. Association for Computational Lin-\nguistics.\nJingjing Xu, Xuancheng Ren, Yi Zhang, Qi Zeng, Xi-\naoyan Cai, and Xu Sun. 2018. A skeleton-based\nmodel for promoting coherence among sentences in\nnarrative story generation. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 4306–4315, Brussels,\nBelgium. Association for Computational Linguistics.\nKailai Yang, Shaoxiong Ji, Tianlin Zhang, Qianqian\nXie, Ziyan Kuang, and Sophia Ananiadou. 2023. To-\nwards interpretable mental health analysis with large\nlanguage models. In Proceedings of the 2023 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 6056–6077, Singapore. Associa-\ntion for Computational Linguistics.\nAndrew Yates, Arman Cohan, and Nazli Goharian. 2017.\nDepression and self-harm risk assessment in online\nforums. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2968–2978, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text genera-\ntion. In Advances in Neural Information Processing\nSystems, volume 34, pages 27263–27277. Curran As-\nsociates, Inc.\nTianyi Zhang, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nTianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,\nKathleen McKeown, and Tatsunori B. Hashimoto.\n2023. Benchmarking large language models for news\nsummarization.\nAyah Zirikly, Philip Resnik, Ozlem Uzuner, and Kristy\nHollingshead. 2019. Clpsych 2019 shared task: Pre-\ndicting the degree of suicide risk in reddit posts. In\nProceedings of the sixth workshop on computational\nlinguistics and clinical psychology, pages 24–33.\nA\nAppendix\nA.1\nExperimental Settings\nTH-VAE We load pre-trained parameters from\nBART-BASE (Lewis et al., 2020) for pre-trained\nword embedding and 6 transformer decoder layers\nin the model. We set the dimensional size of zi to\nbe the same as the size of word embeddings (768).\nWe set the number of latent variables l as 5, which\nhas the best performance on our dataset. In addi-\ntion, we set the number of cells in block is 3, and\nthe number of cells in each group is 1. We use the\nAdam optimizer (Kingma and Ba, 2015) (learning\nrate: 5×10−4).\nLLM Our experiments use 4bit-quantized LLAMA-\n2 (Touvron et al., 2023). For keyphrase extraction,\nwe use few-shot prompting on the base pre-trained\nmodel LLAMA-2-13B.\nIn zero-shot prompting\ntasks with detailed instructions (i.e. mental health\nrelated inferences), we use the chat version of the\nmodel LLAMA-2-13B-CHAT to take advantage of\nits fine-tuning on instruction datasets and human\npreferences.\nWe trained TH-VAE with 2 hours on 1 GPU,\nand spent 20 GPU hours for generating high-level\nsummaries.\nA.2\nEvaluation Metrics\nNLI\nOn metrics that require NLI, we use a\nROBERTA model (Liu et al., 2020) fine-tuned\non fact verification and NLI(Nie et al., 2020):\nhttps://huggingface.co/ynie/roberta-large-snli_\nmnli_fever_anli_R1_R2_R3-nli.\nWhen evaluating\nevidence appropriateness, we consider text from\nthe timeline summary to be the premise and text\nfrom the high-level summary to be the hypothesis.\nWhen running the NLI model, we prefix every\nsentence in the timeline summary with \"The indi-\nvidual wrote:\". While we did not find statistically\nsignificant differences between the selected prefix\nvs.\nno prefix and vs.\nsimilar alternatives, we\ndecided on prefixing as empirically it seemed to\nhelp the NLI model on noisy premises.\nSalient Meaning Preservation:\nMHIC\nWe\nmake the following changes to MHIC (Srivastava\net al., 2022). First, instead of ROUGE we measure\nsemantic embedding similarity using BERTScore.\nSecond, instead of computing separate scores based\non hard utterance categories, we compute a unified\none using the semantic intersection of information\nhighlighted by annotators.\nWe find the intersection of highlighted timeline\nspans among annotators by (1) directly extracting\nintersecting substrings, (2) computing pairwise co-\nsine similarity across evidence spans, keeping pairs\nwith similarity >= .60, then selecting the shorter\nspan from each pair, and (3) deduplicating evi-\ndences from these steps. We use the sentence-\ntransformers library and MSMARCO-DISTILBERT-\nBASE-V3 embeddings.\nFactual Consistency\nFor FCTimeline, we chunk\ntimeline texts with a cutoff of 60 tokens to match\ninput lengths in the NLI model’s training data.\nA.3\nAnnotation & Human Evaluation\nTraining\nWe ran training sessions for both sum-\nmarisation and evaluation tasks under the supervi-\nsion of a senior clinical expert to ensure annotators\nclearly understood task requirements.\nSummarisation\nDuring the training session, the\nannotation team were introduced to the dataset and\ntask, and were provided with guidelines. After re-\nviewing the guidelines and held out examples, we\nworked on a timeline reserved for annotator train-\ning together. The annotators separately worked on\nanother timeline reserved for training. We com-\npared annotations during the second training ses-\nsion. Once we were confident that the team had a\nshared understanding of the task requirements, the\nannotators proceeded to actual timelines used for\ntesting in this paper.\nEvaluation\nWe provided the annotators with\nguidelines and introduced the evaluation task as\nwell as criteria (see Appendix D) in the first train-\ning session. We checked agreement on a small set\nof timelines, then after discussion and clarifications\non a second session they were asked to proceed to\nrating summaries on the remaining test timelines.\nDuring evaluation, annotators were presented\ndata on a timeline-by-timeline basis. When rating\nsummaries for a timeline, they would receive the\nsummaries in a randomly shuffled order.\nAspect\nMetric\nTH-VAE\n-keyphrases\n-clinical prompts\nSMP\nMHICsem\n.66\n.62\n–\nFC\nFCTimeline\n.63\n.52\n–\nFCExpert\n.96\n.95\n.91\nEA\nEA\n.97\n.94\n.93\nCoherence\nIntraNLI\n.96\n.95\n.94\nBARTScore\n-3.10\n-3.08\n-2.74\nFluency\nPPLTimeline (↓)\n56.33\n81.45\n–\nPPLHigh-level (↓)\n9.30\n9.38\n13.62\nTable 4: Ablation results. Best in bold. TH-VAE with-\nout clinical prompts uses the same timeline summary as\nTH-VAE so repeated metrics were removed for brevity.\nAspect\nMetric\nLLaMA\n-keyphrases\n-clinical prompts\nNaive\nSMP\nMHICsem\n.65\n.59\n–\n–\nFC\nFCTimeline\n.63\n.68\n–\n–\nFCExpert\n.95\n.93\n.93\n.93\nEA\nEA\n.97\n.93\n.94\n–\nCoherence\nIntraNLI\n.95\n.89\n.90\n.93\nBARTScore\n-2.96\n-2.48\n-2.61\n–\nFluency\nPPLTimeline (↓)\n13.80\n11.38\n–\n–\nPPLHigh-level (↓)\n9.32\n13.78\n11.62\n11.38\nTable 5: Ablation results. Best in bold. LLaMA without\nclinical prompts uses the same timeline summary as\nLLaMA so repeated metrics were removed for brevity.\nNaive uses neither keyphrases nor clinical prompts.\nA.4\nAblation Results\nA.5\nClinical Concepts\nDiagnosis\nPresenting issues (what bothers the person and causes distress; triggers).\nMental health symptoms, level of functioning, well-being.\nPhysical symptoms.\nRisk assessment (previous suicidal attempts, intent to suicide, access to\nlethal means; hopelessness, social isolation, recent loss, impulsivity,\ndramatic mood swings).\nMotivation to change.\nLifestyle (diet, physical activity, sleep, alcohol/drug/tobacco use,\noccupation, environment, screen time, healthcare practices).\nAgency, coping mechanisms, strengths and resources (what helps\nthe person, how they typically cope with stress and difficulties, resilience).\nMeaning/goals/direction in life.\nBehaviour (adaptive and maladaptive behavioural patterns).\nImportant events (present and past events in life; traumatic events).\nIntrapersonal and Interpersonal patterns\nMain need/wish/desire.\nInterpersonal relationships (repetitive interpersonal pattern;\nconflicts; how others are perceived; social support).\nSelf perception, self esteem.\nMoments of change\nEmotion (sad, happy, etc).\nArousal level (high/low).\nEmotion regulation strategies.\nSwitches (drastic change of one’s mood).\nEscalations (intensification in one’s mood).\nSelf understanding (insights about the self and the relationship; ability to\nreflect and understand repetitive patterns).\nTable 6: Clinical concepts important to therapeutic ap-\nproaches. Our task is to capture and summarize them if\nsuch information is present in user timelines.\nA.6\nQualitative discussion of clinical\nsummaries\nThe TH-VAE and Llama-best models offered mod-\nerately insightful details regarding the individual’s\ndiagnosis. Their summaries accurately captured the\ngeneral aspects of the diagnosis, focusing mainly\non evident symptoms while overlooking some criti-\ncal elements. The Llama-best model often reached\nconclusions that were not sufficiently supported by\nthe evidence provided in the timeline. For example,\nboth models noted the individual’s depression, self-\nharm, and suicidal thoughts but failed to recognize\na clear eating disorder. Additionally, the Llama-\nbest suggested PTSD without substantial evidence\nin the provided timeline. However, these models\nwere useful in shedding light on the individual’s\nself and relational dynamics over time. In contrast,\nthe basic-prompt model presented a very broad\nsummary, missing several vital details and failing\nto reflect significant clinical concepts. On the other\nhand, the TH-VAE and Llama-best produced more\ncomprehensive summaries, effectively highlighting\ncrucial aspects of the individual’s self-perception,\ninterpersonal relationships, and moments of change.\nOverall, from a clinical point of view, the quality\nof the summaries generated by the TH-VAE and\nLlama-best models were quite similar. The Llama\nbest was only slightly lower in factual consistency\nthan the TH-VAE. The TH-VAE and Llama-best\nmodels were effective in summarizing the intrap-\nersonal and interpersonal patterns and moments of\nchange, but their depiction of diagnostic aspects\nwas only moderately accurate, characterized by\nsome inaccuracies and omissions.\nAppendix B. Instruction Prompts \nB.1 Keyphrase Extraction \nB.2 Timeline Summarisation: LLaMA \nB.3 High-level only: LLaMA Naive Baseline \nB.4 Map Prompt: Diagnosis \nTask: Choose key phrases in the following posts. \nText: {example post 1} \nKeyphrases:{expert key phrases 1} \nText: {example post 2} \nKeyphrases:{expert key phrases 2} \nText: {concatenated posts to annotate} \nKeyphrases:\nWrite a TLDR as the user (first-person), focusing on the keyphrases. \nKeyphrases: {extracted keyphrases} \n{concatenated posts to summarise} \nTLDR:\nYou are a helpful assistant to an expert therapist who reads social media \nchronological text written by an individual who has mental health concerns. \nSummarize the texts below: \n{timeline chunk concatenated}\nYour goal is to describe the individual’s mental state and identify potential \nindicators that may suggest a mental health diagnosis, considering the following \naspects: \n1. Presenting Issues: What are the main concerns or stressors evident in the \nindividual's posts? What triggers seem to affect their mental state? \n2. Mental Health Symptoms and Functioning: Does the individual exhibit any mental \nhealth symptoms? How are their mood, energy levels, and interest in usual \nactivities? Are there noticeable changes in sleep patterns, appetite, \nconcentration, or social interactions? How do they describe their overall well-\nbeing and functioning in daily activities? \n3. Mental Health Treatment History: Has the individual been in contact with \nmental health professionals such as psychiatrists or psychotherapists? Are there \nmentions of current or past outpatient or inpatient mental health treatments? Do \nthey reference taking psychiatric medications? \n4. Physical Health: Are there any current or past physical health issues, medical \nconditions, hospitalizations, or surgeries mentioned? \n5. Risk Assessment: Is there evidence of previous suicidal attempts or current \nsuicidal thoughts? Do they have access to lethal means? What level of \nhopelessness is expressed? Do they discuss social isolation, recent losses, \nimpulsivity, or dramatic mood swings? \n6. Lifestyle Factors: What do the individual’s posts reveal about their lifestyle \nhabits, such as diet, physical activity, sleep patterns, occupation, environment, \nscreen time, and healthcare practices? \n7. Substance Use: Are there any references by the individual to the use of \nsubstances like alcohol, drugs, or tobacco? If so, how frequently do they use \nthese substances? \n8. Significant Life Events and Family History: Are there references to \nsignificant life events like divorce, loss of a close person, experiences of \nabuse, or neglect? Is there any mention of psychiatric problems or treatments \namong family members? \n9. Motivation and Coping Strategies: What does the individual express about their \nmotivation for change? How do they cope with stress and difficulties? What \nstrengths and resources do they have? What seems to help them? How resilient do \nthey appear? Do they discuss having direction, meaning, or goals in their life? \nYou must not make anything up. Keep the description concise and only describe \nobservations if they are fully supported by the text. \nHere are the texts: \n{Timeline summary}\n15\nB.5 Map Prompt: Intrapersonal and Interpersonal Patterns \nB.6 Map Prompt: Moments of Change (MoC) \nB.7 Reduce Prompt \nYour goal is to identify the person's main intrapersonal and interpersonal \npattern, considering the following aspects: \n1. Wish/Need/desire/intention/expectation:  What is the person’s most dominant \nneed, desire, intention, expectation from others and from themselves? Are there \nany other needs or wishes that might be indicated in a less obvious way? How well \ndoes the individual communicate their needs/ wishes with others? \n2. Response of Others: How does this person typically perceive the emotions, \nbehaviors, and thoughts of others? Are there any other perceptions of the other \nthat might be indicated in a less obvious way? Is the individual capable of \nacknowledging the complex nature of others? \n3. Response of self to others: How does this person tend to feel and react to \nothers? Are there any other reactions towards others that might be indicated in a \nless obvious way? \n4. Response of self to self: What is the individual’s most dominant emotion, \nbehavior and cognition toward oneself? Are there any other emotions and \ncognitions towards the self that might be indicated in a less obvious way? What \nis the level of self-compassion and acceptance of strengths and vulnerabilities? \n5. Patterns: What is this individual’s predominant dysfunctional intrapersonal \nand interpersonal pattern? What is this individual’s predominant adaptive \nintrapersonal and interpersonal pattern? \nYou must not make anything up. Keep the description concise and only describe \nobservations if they are fully supported by the text. \nHere are the texts: \n{Timeline summary}\nYour goal is to understand and summarise changes over time in this individual’s \nmood, well-being and functioning (individual/self well being; interpersonal well-\nbeing including family and close relationships; social well-being, including \nwork, school and friends). From the following text, identify whether there are \nchanges in the individual's expressed mood, well-being, and functioning. \na. Overall, does the mood/ well being/ functioning stay consistent or fluctuate \nover time? \nb. Are there specific positive to negative changes? Which events (personal or \ninterpersonal) are associated with these changes? \nc. Are there specific negative to positive changes? Which events (personal or \ninterpersonal) are associated with these changes? \nAnswer each concisely and reply None if not available. You must not make anything \nup. Keep the description concise and only describe observations if they are fully \nsupported by the text. \nHere are the texts: \n{Timeline summary}\nRewrite these sentences about the individual so that they form a single clear and \ncoherent document. Combine redundant sentences, but do not remove any clinical \ninformation or information about interpersonal relationships. You must not make \nanything up. \n{Previously generated summaries}\n16\nAppendix C. Example Summaries \nC.1 Timeline Summaries \nTH-VAE\nI hate school. im so scared.i love the cat so much. i'm gonna go be counter for a while. \nunknown good night all! i hate my nose. fuck the sat. i got a commitment ring 3 i've got \ncounterbodied under my eyes. i feel alone i feel like everyone hates me. i hate saying \nthis because it sounds bad, but i really want someone to pay attention to me. solid my \nmom only cares about my brothers learning disabilities and her new boyfriend \nexpressive. my boyfriend goes throug of ptsd like depression symptoms, so he gets \ndistant sometimes levels. i literally have no friends argue. i've always been quiet but \nsometimes i just want someone never pay attention to me. i argue to know only when't \nbe a body. i have to know. i'll have to be happy. \nLLaMA\nNeed help passing a breathalyzer test for homecoming, worried about proposal, \nstruggling with school, and feeling insecure about appearance. I love my cat so much, \nbut I have no motivation and can't get out of bed. I really enjoy making others happy, \nbut I struggle with feeling alone and like everyone hates me. What is a healthy weight \nfor a 5'2\\\" 17 year old girl? Is there anything besides vodka that can help me get through \nhomecoming? Someone please pay attention to me. I feel ignored by my family and my \nchat isn't working. I have no friends and my boyfriend has PTSD-like symptoms. I \npromise I'm not ignoring you, I just need someone to talk to. My cat snoozes and it's \nwonderful. “I just wanna be kissed\" but I hate my nose and have dark circles under my \neyes.\nSkeleton\ndon't know what to do anymore i don't want to go to homecoming but i can't help my \nown time i feel like i'm going to try to share my feelings but i know that i'm feeling \nalone but i just want someone to  pay attention to me. i love my cat so much. i can tell \nyou that i feel good at the same time i'm in love with my partner and i'm glad i could \nturn up to be my best friend and i know what it happened to me i bet you're going to \nmake me feel better cause i'm sure if i can turn my homecoming if i do help i will be \nhappy but i like a lot of people i want to tell you i appreciate you but i won't help you \njust want you to go to sleep but i didn't really want to talk to you my way to help my \nmom so i can help you and make you feel better at the end of my life. i love him so \nmuch and i bet it's hard to me but i hope you were a good time but i need to be able to \nsee you but you don't like you but it's okay to see your\n17\nC.2 High-level Summaries \nTH-VAE\nThe individual is likely experiencing a range of mental health issues, including \ndepression, anxiety, and low self-esteem. They report feeling isolated, lonely, and \nfrustrated, with a history of stressful life events. The individual's mood appears to be \nlow, with expressions of sadness, frustration, and anger. \nIt is evident that they have a profound craving for attention and connection with others, \nas well as a yearning for acceptance and validation. However, their fear of rejection and \nabandonment hinders them from forming and maintaining healthy relationships. \nMoreover, their self-criticism and preoccupation with perceived shortcomings indicate a \nlack of self-compassion and acceptance of their strengths and vulnerabilities. \nThe individual's mood and well-being have been observed to fluctuate over time, with \nboth positive and negative changes experienced. The individual has expressed a range of \nemotions, including sadness, loneliness, and frustration, as well as moments of \nhappiness and positivity. Noteworthy positive changes include their excitement about \nhaving a cat and receiving a commitment ring, which are associated with positive \nemotions and a sense of joy. However, the individual also struggles with school and \nexperiences anxiety and depression, which are linked to negative emotions such as \nsadness, fear, and frustration.\nLLaMA\nBased on the information provided, the individual is experiencing a range of mental \nhealth symptoms, including depressive and anxiety symptoms, low self-esteem, and \nsuicidal ideation, which are likely interfering with their daily functioning and quality of \nlife. Addressing lifestyle habits such as excessive screen time and lack of physical \nactivity may also be helpful in managing symptoms.  \nThe individual's primary intrapersonal and interpersonal patterns appear to be a mix of \ndependent and self-centered tendencies, with a lack of self-awareness and self-\nregulation. These patterns are characterized by negative self-talk, low self-esteem, and a \nfear of being alone, leading to a reliance on alcohol to cope with stress. The individual \nalso exhibits difficulty regulating their emotions and behaviors, and seeks validation and \nattention from others while having difficulty setting healthy boundaries. \nThe individual experiences fluctuating mood and well-being, with feelings of worry, \ninsecurity, and loneliness, as well as moments of joy and happiness when they make \nothers happy. Despite these positive interactions, the individual struggles with negative \nthoughts and feelings of being ignored by family and friends. They desire positive \nchanges and may benefit from seeking professional help to address symptoms of \ndepression, anxiety, and low self-esteem.\n18\nSkeleton\nBased on the provided social media texts, the individual is struggling with multiple \nmental health issues, including depression, anxiety, low self-esteem, and possible \nborderline personality disorder. Their mood appears low, with expressions of feeling \nalone, lost, and tired, which may indicate depression and anxiety. The individual's desire \nto \"feel better\" and \"turn up\" suggests a willingness to seek help and improve their \nmental state. The lack of direct references to mental health treatment history and their \nexpressed desire for improvement suggest that they may be open to seeking professional \nhelp. Their statements about feeling alone and lost, as well as their desire to \"turn up,\" \nmay indicate a higher risk of suicide. \nThe individual longs for human connection and yearns for validation from others, yet \nthey feel overwhelmed by their own emotions. They find solace in their attachment to \ntheir cat and appreciate their partner's support. However, they still experience a sense of \nloneliness and disconnection, which they introspectively examine. The individual's \nprimary intrapersonal patterns may be characterized by isolation and disconnection, \nwhile their adaptive patterns may involve self-awareness and introspection. \nThe individual's mood and well-being appear to fluctuate over time, with both positive \nand negative emotions being experienced. They express feelings of happiness and \nappreciation for their partner and pets, but also mention feeling alone and isolated, \nwhich may suggest a decline in their well-being. The text is written in a stream of \nconsciousness style, providing a glimpse into the individual's inner thoughts and \nemotions, but it does not provide a complete picture of their mood and well-being.\nLLaMA \nNaive\nBased on the information provided, it appears that the individual is seeking emotional \nsupport and connection. They mention feeling alone and isolated, and express a desire \nfor someone to pay attention to them and listen to their needs. The individual is feeling \noverwhelmed and frustrated, expressing a desire to leave and pursue their dreams. They \nare struggling with negative self-talk and body image issues, feeling ignored and alone \ndespite expressing happiness and positivity. They use humor and sarcasm to cope with \ntheir emotions, but also mention feeling scared and insecure. The individual is seeking \nalternative ways to cope with their feelings besides drinking vodka.\n19\nAppendix D. Human Evaluation Criteria \nFactual consistency \nA factually consistent \nsummary accurately reflects \nthe content of the timeline. \nIt does not contain \ninformation that is not \npresent in the timeline.\n1 - Not at all factually consistent: The summary contains significant \ninaccuracies or misrepresentations, completely misaligning with the \ntimeline's content. \n2 - Mostly not factually consistent: The summary contains significant \ninaccuracies or misrepresentations, poorly reflecting the timeline's \ncontent. \n3 - Somewhat factually consistent: The summary is somewhat \naccurate, with several inaccuracies or omissions, but retains a basic \nreflection of the timeline’s content. \n4 - Mostly factually consistent: The summary is largely accurate, \nwith minor inaccuracies or omissions that do not majorly distort \noverall understanding. \n5 - Fully factually consistent: The summary is completely accurate, \nperfectly aligning with the timeline's content without discrepancies.\nGeneral usefulness and \nSalient meaning \npreservation \nA useful summary should \nhelp the clinician understand \nthe client’s condition. It \nshould contain the most \nclinically important \ninformation of the timeline. \nIt does not include parts of \nthe timeline that are less \nimportant.\n1 - Not at all useful: The summary fails to capture any essential \ninformation, significantly misrepresenting or omitting critical aspects \nof the individual’s condition. \n2 - Slightly useful: The summary includes some important details but \nprimarily focuses on irrelevant or less critical information. \n3 - Moderately useful: The summary captures important information \nbut still includes less relevant details or omits minor key elements. \n4 - Very useful: The summary highlights most of the crucial \ninformation, with only minor irrelevant details. \n5 - Extremely useful: The summary encapsulates all critical \ninformation, providing a comprehensive and clear understanding of \nthe individual’s condition, without providing irrelevant information.\nUsefulness (diagnosis) \nThe summary provides \nuseful information about the \nindividual's diagnosis (such \nas presenting issues, mental \nhealth & physical \nsymptoms, risk assessment, \nbehaviour).\n1 - Not at all useful: The summary fails to provide information \nregarding the individual's diagnosis, or it clearly distorts the \nindividual’s diagnosis by incorrectly identifying diagnostic elements. \n2 - Slightly useful: The summary provides minimal information \nrelated to the individual’s diagnosis. While the summary includes \nsome correct diagnostic elements, it generally contains irrelevant or \nincorrect details and omissions. \n3 - Moderately useful: The summary is generally accurate about the \nindividual’s diagnosis but only describes the more obvious aspects, \nwith some information possibly missing or unclear. \n4 - Very useful: The summary accurately identifies the individual’s \ndiagnosis and captures almost all the essential diagnostic information \nwith only minor gaps. \n5 - Extremely useful: The summary is comprehensive and accurately \ndetails all aspects of the individual's diagnosis.\n20\nUsefulness (interpersonal \nand intrapersonal pattern) \nThe summary provides \nhelpful information about \nthe individuals' main needs \nand patterns of self and \nother relationships.\n1 - Not at all useful: The summary provides no insight into the \nindividual's interpersonal and intrapersonal patterns. \n2 - Slightly useful: The summary provides a minimal understanding \nof interpersonal and intrapersonal patterns. \n3 - Moderately useful: The summary covers some key aspects of the \nindividual's interpersonal and intrapersonal patterns but may lack \ndepth or miss important elements. \n4 - Very useful: The summary provides a comprehensive overview of \nthe individual’s interpersonal and intrapersonal patterns, with only \nslight gaps or generalizations. \n5 - Extremely useful: The summary gives a detailed and complete \nunderstanding of the individual's interpersonal and intrapersonal \npatterns.\nUsefulness \n(moments of change) \nThe summary provides \nuseful information about the \nindividual's changes over \ntime in emotion/cognition \nand behaviour. Where \nappropriate, it should help \nconnect information \nbetween events and the \nindividual’s responses.\n1 - Not at all useful: The summary fails to provide any accurate \ninformation about whether there are changes in the individual over \ntime. \n2 - Slightly useful: The summary includes information about \nchanges, but they are generally inaccurate and overlook key \ndevelopments/connections, or they generally contain irrelevant \ninformation. \n3 - Moderately useful: The summary accurately describes whether \nthere are changes, although there may be some weaknesses or \nomissions as well as irrelevant information. \n4 - Very useful: The summary accurately describes whether there are \nchanges and where available offers helpful insights. \n5 - Extremely useful: The summary accurately describes whether \nthere are changes and where available provides clear, well-connected \ninsights about the individual’s development over time.\n21\n"
}