{
    "optim": "CFASL: Composite Factor-Aligned Symmetry Learning for Disentanglement in\nVariational AutoEncoder\nHee-Jun Jung1 , Jaehyoung Jeong1 and Kangil Kim1∗\n1Gwangju Institute of Science and Technology\n{jungheejun93, jaehyoung98}@gm.gist.ac.kr, kikim01@gist.ac.kr,\nAbstract\nSymmetries of input and latent vectors have pro-\nvided valuable insights for disentanglement learning\nin VAEs. However, only a few works were pro-\nposed as an unsupervised method, and even these\nworks require known factor information in training\ndata. We propose a novel method, Composite Factor-\nAligned Symmetry Learning (CFASL), which is in-\ntegrated into VAEs for learning symmetry-based\ndisentanglement in unsupervised learning without\nany knowledge of the dataset factor information.\nCFASL incorporates three novel features for learn-\ning symmetry-based disentanglement: 1) Injecting\ninductive bias to align latent vector dimensions to\nfactor-aligned symmetries within an explicit learn-\nable symmetry codebook 2) Learning a compos-\nite symmetry to express unknown factors change\nbetween two random samples by learning factor-\naligned symmetries within the codebook 3) Inducing\ngroup equivariant encoder and decoder in training\nVAEs with the two conditions. In addition, we pro-\npose an extended evaluation metric for multi-factor\nchanges in comparison to disentanglement evalua-\ntion in VAEs. In quantitative and in-depth quali-\ntative analysis, CFASL demonstrates a significant\nimprovement of disentanglement in single-factor\nchange, and multi-factor change conditions com-\npared to state-of-the-art methods.\n1\nIntroduction\nDisentangling representations by intrinsic factors of datasets\nis a crucial issue in machine learning literature [Bengio et al.,\n2013]. In Variational Autoencoder (VAE) frameworks, a preva-\nlent method to handle the issue is to factorize latent vector\ndimensions to encapsulate specific factor information [Kingma\nand Welling, 2013; Higgins et al., 2017; Chen et al., 2018;\nKim and Mnih, 2018; Jeong and Song, 2019; Shao et al., 2020;\nShao et al., 2022]. Although their effective disentanglement\nlearning methods, [Locatello et al., 2019] raises the serious\ndifficulty of disentanglement without sufficient inductive bias.\n∗corresponding author\nIn VAE literature, recent works using group theory offer\na possible solution to inject such inductive bias by decom-\nposing group symmetries [Higgins et al., 2018] in the latent\nvector space. To implement group equivariant VAE, [Win-\nter et al., 2022a; Nasiri and Bepler, 2022] achieve the trans-\nlation and rotation equivariant VAE. The other branch im-\nplements the group equivariant function [Yang et al., 2022;\nKeller and Welling, 2021a] over the pre-defined group ele-\nments. All of the methods effectively improve disentangle-\nment by adjusting symmetries, but they focused on learning\nsymmetries among observations to inject inductive bias rather\nthan factorizing group elements to align them on a single factor\nand a single dimension changes, as introduced in the definition\nprovided in [Higgins et al., 2018].\nIn current works, unsupervised learning approaches of\ngroup equivariant models are introduced. [Miyato et al., 2022;\nQuessard et al., 2020] represent the symmetries on the la-\ntent vector space, which correspond to the symmetries on\nthe input space, by considering the sequential observations.\nAlso, [Winter et al., 2022b] proposes the group invariant and\nequivariant representations with different modules to learn\nthe different groups of dataset structure. However, these ap-\nproaches, despite being unsupervised learning, require the\nfactor information of the dataset to construct the sequential\ninput and to set different modules for learning symmetries.\nThis paper introduces a novel disentanglement method\nfor Composite Factor-Aligned Symmetry Learning (CFASL)\nwithin VAE frameworks, aimed at addressing the challenges\nencountered in unsupervised learning scenarios, particularly\nthe absence of explicit knowledge about the factor structure in\ndatasets. Our methodology follow as 1) a network architecture\nto learn an explicit codebook of symmetries, responsible for\neach single factor change, called factor-aligned symmetries,\n2) training losses to inject inductive bias to be an explicit code-\nbook where each factor-aligned symmetry only impacts to a\nsingle dimension value of latent vectors for disentangled repre-\nsentations, 3) learning composite symmetries by prediction of\nsingle factor changes itself without information of factor labels\nfor unsupervised learning, 4) implementing group equivariant\nencoder and decoder functions that factor-aligned symmetry\naffects latent vector space, 5) an extended metric (m-FVMk)\nto evaluate disentanglement in the multi-factor change con-\ndition. We conduct quantitative and qualitative analyses of\nour method onn common benchmarks of disentanglement in\narXiv:2401.08897v2  [cs.LG]  19 Jan 2024\nVAEs.\n2\nLimits of Disentanglement Learning of VAE\nBy the definition of disentangled representation [Bengio et\nal., 2013; Higgins et al., 2018], the disentangled representa-\ntions distribute on the flattened surface as shown in Fig. 1g\nbecause each change of the factor only affects to single\ndimension of latent vector.\nHowever, the previous meth-\nods [Higgins et al., 2017; Chen et al., 2018; Shao et al., 2020;\nZhu et al., 2021; Yang et al., 2022] show the entangled\nrepresentations on their latent vector space as shown in\nFig. 1a-1c. Even though the group theory-based methods\nimprove the disentanglement performance [Zhu et al., 2021;\nYang et al., 2022], these are still struggling with the same\nproblem as shown in Fig. 1d and 1e. In addition, symme-\ntries are represented on the latent vector space for disentan-\ngled representations. In current works [Miyato et al., 2022;\nKeller and Welling, 2021a; Quessard et al., 2020], the se-\nquential observation is considered with unsupervised learn-\ning. However, these works need the knowledge of sequential\nchanges of images to set up inputs manually.\nTo enhance these two problems of disentanglement learning\nof group theory-based methods, addressing two questions is\ncrucial:\n1. Do the explicitly defined symmetries impact to the struc-\nturing of a disentangled space as depicted in Fig. 1g?\n2. can these symmetries be represented through unsuper-\nvised learning without any prior knowledge of factor\ninformation?\n3\nMethods\n3.1\nInput: A Pair of Two Samples\nTo learn the symmetries between inputs with unknown fac-\ntors changes, we randomly pair the two samples as an in-\nput. During the training, samples in the mini-batch X|B|\nare divided into two parts X1\n|B|\n=\n{x1\n1, x1\n2, . . . , x1\n|B|\n2 },\nand X2\n|B|\n=\n{x2\n1, x2\n2, . . . , x2\n|B|\n2 }, where |B| is a mini-\nbatch size.\nIn the next, our model pairs the samples\n(x1\n1, x2\n1), (x1\n2, x2\n2), . . . , (x1\n|B|\n2 , x2\n|B|\n2 ) and is used for learning\nsymmetries between the elements of each pair.\n3.2\nFactor-Aligned Symmetry Learning with\nInductive Bias\nWe define the factor-aligned symmetry that represents a corre-\nsponding factor change on the latent vector space. For factor-\naligned symmetry, we compose the symmetry codebook and\ninject inductive bias via Parallel loss Lpl and Perpendicu-\nlar loss Lpd that matches each symmetry to a single factor\nchanges. Then we add sparsity loss Ls to the losses for disen-\ntangled representations as shown in Fig. 3. It aligns a single\nfactor change to an axis of latent vector space. Also, we im-\nplement the commutative loss Lc to reduce the computational\ncosts for matrix exponential multiplication.\nExplicit and Learnable Symmetry Representation for In-\nductive Bias Injection\nTo allow the direct injection of in-\nductive bias into symmetries, we implement an explicit and\ntrainable codebook for symmetry representation. we consider\nthe symmetry group on the latent vector space as a subgroup\nof the general lie group GL(n) under a matrix multiplica-\ntion. The codebook G = {G1, G2, . . . , Gk} is composed of\nsections Gi, which are affect to a different single factor, where\nk ∈ {1, 2, . . . |S|}, and |S| is the number of sections. The\nsection Gi is composed of Lie algebra {gi\n1, gi\n2, . . . , gi\nl}, where\ngi\nj ∈ R|D|×|D|, l ∈ {1, 2, . . . , |SS|}, |SS| is the number of el-\nements in each section, and |D| is a dimension size of latent z.\nWe assume that each Lie algebra consists of linearly indepen-\ndent bases B = {Bi|Bi ∈ Rn×n, P\ni αiBi ̸= 0, αi ̸= 0}:\ngi\nj = P\nb αi,j\nb Bb, where b ∈ {1, 2, . . . , kl}. Then the di-\nmension of the element of the codebook is equal to |B| and\nthe dimension of the Lie group composited by the codebook\nelement is also |B|. To utilize previously studied effective\nexpression of symmetry for disentanglement, we set the sym-\nmetry to be continuous [Higgins et al., 2022] and invert-\nible via matrix exponential form [Xiao and Liu, 2020] as\ngi\nj = egi\nj = P∞\nk=0\n1\nk!(gi\nj)k to construct the Lie group [Hall,\n2015].\nInductive Bias: Group Elements of the Same Section Im-\npacts on the Same Factor Changes\nWe add a bias that\nlatent vector changes by two symmetries for the same factor\nshould be parallel (z−gi\njz ∥ z−gi\nlz for ith section) as shown\nin Fig. 3a. We define a loss function to make them parallel as:\nLpl =\n|S|\nX\ni=1\n|SS|\nX\nj,k=1\nlog\n< z − gi\njz, z − gi\nkz >\n||z − gi\njz||2 · ||z − gi\nkz||2\n,\n(1)\nwhere gi\nj = egi\nj, < ·, · > is a dot product, and || · ||2 is a L2\nnorm.\nInductive Bias: Group Elements of the Different Section\nImpacts on the Different Factor Changes\nSimilarly to the\nparallel loss, we inject another bias that changes by two sym-\nmetries for different factors should satisfy the orthogonality\n( z − gi\njz ⊥ z − gk\nl z for different ith and kth sections) as\nshown in Fig. 3c. The loss for inducing the orthogonality is\nLpd =\n|S|\nX\ni,k=1,i̸=k\n|SS|\nX\nj,l=1\n< z − gi\njz, z − gk\nl z >\n||z − gi\njz||2 · ||z − gk\nl z||2\n.\n(2)\nThis loss is computationally expensive to calculate (O(|S|2 ·\n|SS|2)), so we randomly select a (j, l) pair of symmetries of\ndifferent sections. This random selection still holds the or-\nthogonality, because if all elements in the same section satisfy\nEquation 1 and a pair of elements from a different section\n(Gi, Gj) satisfies Equation 2, then any pair of the element\n(gi ∈ Gi, gj ∈ Gj) satisfies the Equation 2. More details are\nin Appendix B.\nInductive Bias: Align Each Factor Changes to the Axis of\nLatent Space for Disentangled Representations\nFactoriz-\ning latent dimensions to represent the change of independent\nfactors is an attribute of disentanglement defined in [Bengio et\n(a) β-VAE\n(b) β-TCVAE\n(c) Control-VAE\n(d) CLG-VAE\n(e) G-VAE\n(f) CFASL (ours)\n(g) Ideal\nFigure 1: Distribution of latent vectors for dimensions responsible for Shape, X-pos, Y-pos factors in the dSprites dataset. The groupified-VAE\nmethod is applied to β-TCVAE because this model shows a better evaluation score. The results show disentanglement for Shape from\nthe combination of the other two factors by coloring three shapes (square, ellipse, heart) as red, blue, and green color, respectively. Each\n3D-plot shows the whole distribution. We fix Scale and Orientation factor values, and plot randomly sampled 640 inputs (20.8% of all\npossible observations (32 × 32 × 3 = 3, 072)). We select the dimensions responsible for the factors by selecting the largest value of the\nKullback-Leibuler divergence between the prior and the posterior.\nal., 2013] and derived by ELBO term in VAE training frame-\nworks [Chen et al., 2018; Kim and Mnih, 2018]. However, it\nis noteworthy that neither the parallel loss nor the perpendic-\nular loss exhibits specific constraints aimed at inducing this\ncharacteristic as shown in Fig. 3a, 3c. To guide symmetries to\nhold the attribute, we enforce the change ∆i\njz = z − gi\njz to\nbe a parallel shift to a unit vector as Fig. 3b, 3d via sparsity\nloss defined as\nLs =\n|S|\nX\ni=1\n|SS|\nX\nj\nh\u0002 |D|\nX\nk=1\n(∆i\njzk)2\u00032 − max\nk ((∆i\njzk)2)2i\n,\n(3)\nwhere ∆i\njzk is a kth dimension value.\n3.3\nComposition of Factor-Aligned Symmetries via\nTwo-Step Attention for Unsupervised Learning\nFirst Step: Select Factor-Aligned Symmetry\nIn the first\nstep, the model generate the factor-aligned symmetries of\neach section through the attention score: gi\nc = egi\nc, where\ngi\nc = P|S.S|\nj=1 attni\njgi\nj, attni\nj = softmax([M; Σ]W i\nc + bi\nc)\n[M; Σ] = [µ1; σ1; µ2; σ2], W i\nc ∈ R4|D|×|SS| and bi\nc ∈\nR|SS| are learnable parameters, and i ∈ {1, 2, . . . |S|}.\nSecond Step: Section Selection\nIn the second step of our\nproposed model, we enforce the prediction of factors deemed\nto have undergone changes. We assume that if some factor\nvalue of two inputs is equal, then the variance of the corre-\nsponding latent vector dimension value is smaller compared\nto others. Based on this assumption, we define the target (T)\nfor factor prediction: if z1,i − z2,i > threshold, then we set\nTi as 1 and 0 otherwise, where Ti is a ith dimension value of\nT ∈ R|D|, zj,i is an ith dimension value of zj, and we set\nthe threshold as a hyper-parameter. For section prediction, we\nutilize the cross-entropy loss:\nLp =\n|S|\nX\ni=1\nX\nc∈C\n1[Ti = c] · log softmax(pi\ns),\n(4)\nwhere pi\ns = [M; Σ]W i\ns + bi\ns, W i\ns ∈ R4|D|×2 and bi\ns ∈ R2\nare learnable parameters, and c ∈ {0, 1}.\nTo infer the activated section of the symmetries codebook,\nwe utilize the Gumbel softmax function to handle binary on-\nand-off scenarios, akin to a switch operation:\nsw(G(pi\ns)) =\n\u001aG(pi\ns,2)\nif pi\ns,2 ≥ 0.5\n1 − G(pi\ns,1) if pi\ns,2 < 0.5\n,\n(5)\nwhere pi\ns,j is a jth dimension value of pi\ns, and G(·) is the\nGumbel softmax with temperature as 1e-4.\nIntegration for Composite Symmetry\nFor the composite\nsymmetry gc, we compute it s the product of weighted sums\nof switch function sw(ps) and prediction distribution attn as:\ngc = Q|S|\ni=1 ˆgi\nc, where ˆgi\nc = esw(G(pi\ns))·gi\nc.\nCommutativity Loss for Computational Efficiency\nIn the\ncomputation of the composite symmetry gc, the production\nQ|S|\ni=1 ˆgi\nc is a computationally expensive Taylor series repeated\nfor all (i, j) pairs. To reduce the cost by repetition, we enforce\nall pairs of basis gj\ni to be commutative to convert the produc-\ntion to e\nP\ni gi\nc (By the matrix exponential property: eAeB =\neA + B as AB = BA, where A, B ∈ Rn×n). The loss for\nthe commutativity is Lc = P|S|\ni,k=1\nP|SS|\nj,l=1 gi\njgk\nl − gk\nl gi\nj → 0.\n3.4\nEquivariance Induction of Composite\nSymmetries\nHow to Induce Equivariance?\nMotivated by the implemen-\ntations of equivariant mapping in prior studies [Yang et al.,\n2022; Miyato et al., 2022] for disentanglement learning, we\nimplement an equivariant encoder and decoder that satisfies\nqϕ(ψi ∗ x) = gi ◦ qϕ(x) and pθ(gi ◦ z) = ψi ∗ pθ(z) respec-\ntively, where qϕ is an encoder, and pθ is the decoder. In the\nnotation, ψi and gi are group elements of the group (Ψ, ∗)\nand (G, ◦) respectively, and both groups are isomorphic. Each\ngroup acts on the input and latent vector space with group\naction ∗, and ◦, respectively. We specify the form of symmetry\ngi and ◦ as an invertible matrix, and group action as matrix\nmultiplication on the latent vector space. Then, the encoder\nequivariant function can be rewritten by multiplying the inver-\nsion of gi on both sides and z can be replaced with the qϕ(x)\nin the decoder equivariant function as\nqϕ(x) = g−1\ni\n◦ qϕ(ψi ∗ x)\n⇐⇒ qϕ(x) − g−1\ni\n◦ qϕ(ψi ∗ x) → 0\n(for encoder).\n(6)\npθ(gi ◦ qϕ(x)) = ψi ∗ pθ(qϕ(x))\n⇐⇒ pθ(gi ◦ qϕ(x)) − ψi ∗ pθ(qϕ(x)) → 0 (for decoder),\n(7)\nwhere xj = ψi→j ∗ xi. For the equivariant encoder and\ndecoder, we differently propose the single forward process\nby the encoder and decoder objective functions compared to\nprevious work [Yang et al., 2022].\nFigure 2: The overall architecture of our proposed method. The loss function is divided into four parts: 1) commutative loss (Lc), 2)\nperpendicular, parallel, and sparsity loss (Lpd, Lpl, and Ls) in Equation 1-3, 3) factor prediction loss (Lp) in Equation 4), and 4) equivariant\nloss (Lee, and Lde) in Equation 8). MLP is a multi-layer perceptron, and, tr is a threshold. Attention score attne, sw(·), and pi\ns are introduced\nin section 3.3.\nEquivariance Loss for Encoder and Decoder\nIn order for\nthe equivariant function between the input and latent vector\nspace, the mapping function qϕ(·) must satisfy Equation 6.\nTherefore, we directly induce an equivariant encoder between\ninput and latent space with MSE loss (Lee). Additionally, we\ninduce the equivariant decoder (Lde) with MSE loss following\nEquation 7:\nLequiv = Lee + ϵLde = MSE(qϕ(x1\ni ), g−1\ni\n◦ qϕ(x2\ni ))\n+ ϵMSE(pθ(gi ◦ (qϕ(x1\ni )), ψi ∗ pθ(qϕ(x1\ni )),\n(8)\nwhere x2\ni = ψi ∗ x1\ni . During the training, we replace the\npθ(qϕ(x1\ni )) as a x1\ni because the ELBO term includes the re-\nconstruction error between pθ(qϕ(x1\ni )) and x1\ni to be close to\nzero.\nObjective and Base model\nOur method can be plugged\ninto existing VAE frameworks, where the objective function is\nintegrated additively as follows:\nL(ϕ, θ; x) = LV AE + Lcodebook + Lequiv,\n(9)\nwhere LV AE is the loss function of a VAE framework (Ap-\npendix A). The other loss Lcodebook = Lpl +Lpd +Ls +Lc +\nLp and Lequiv = Lee + ϵLde where ϵ is a hyper-parameter,\nwhich are introduced in the following subsections.\n3.5\nExtended Evaluation Metric: m-FVM Metric\nfor Disentanglement in Multi-Factor Change\nWe define the multi-factor change condition as simultaneously\naltering more than two factors in the transformation between\ntwo samples or representations. To the best of our knowl-\nedge, there is no evaluation metric for disentanglement in\nmulti-factor change, so we propose the extended version of\nthe Factor-VAE metric (FVM) score called as multi-FVM\nscore (m-FVMk), where k ∈ {2, 3, . . . , |F| − 1}, and |F|\nis a number of factors. Similar to FVM, 1) we randomly\nchoose the k fixed factors (Fi, Fj, . . .). 2) We sample each fac-\ntor’s value (fi, fj, . . .) and fix the corresponding factor dimen-\nsion value in the mini-batch, where fi ∈ {1, 2, . . . |Fi|}, fj ∈\n{1, 2, . . . |Fj|}, . . ., |Fi| and |Fj| is a maximum value of each\nfactor label. 3) Subsequently, we estimate the standard devia-\ntion (std.) of each dimension to find the number of k lowest\nstd. dimension (zl1, zl2, . . .) in one epoch. 4) We then count\neach pair of selected dimensions by std. values (the num-\nber of (zl1, zl2, . . .), which are corresponded to fixed factors).\n5) In the last, we add the maximum value of the number of\n(zl1, zl2, . . .) on all fixed factor cases, and divide with epoch.\n4\nRelated Work\nDisentanglement Learning\nDiverse works for unsupervised\ndisentanglement learning have elaborated in the machine learn-\ning field. The VAE based approaches have factorized latent\nvector dimensions with weighted hyper-parameters or con-\ntrollable weighted values to penalize Kullback-Leibler diver-\ngence (KL divergence) [Higgins et al., 2017; Shao et al., 2020;\nShao et al., 2022]. Extended works penalize total correlation\nfor factorizing latent vector dimensions with divided KL diver-\n(a) parallel loss\n(b) (a) + sparsity loss\n(c) perpendicular loss\n(d) (c) + sparsity loss\nFigure 3: Roles of parallel, perpendicular, and sparsity loss on symmetries in the codebook for adjusting representation change. Parallel loss is\nfor symmetries of the same section, and perpendicular loss is for different sections. Each axis (x and y) only affects to a single factor.\n3D Car\nFVM\nbeta VAE\nMIG\nSAP\nDCI\nm-FVM2\nm-FVM3\nm-FVM4\nβ-VAE\n91.83(±4.39)\n100.00(±0.00)\n11.44(±1.07)\n0.63(±0.24)\n27.65(±2.50)\n61.28(±9.40)\n-\n-\nβ-TCVAE\n92.32(±3.38)\n100.00(±0.00)\n17.19(±3.06)\n1.13(±0.37)\n33.63(±3.27)\n59.25(±5.63)\n-\n-\nFactor-VAE\n93.22(±2.86)\n100.00(±0.00)\n10.84(±0.93)\n1.35(±0.48)\n24.31(±2.30)\n50.43(±10.65)\n-\n-\nControl-VAE\n93.86(±5.22)\n100.00(±0.00)\n9.73(±2.24)\n1.14(±0.54)\n25.66(±4.61)\n46.42(±10.34)\n-\n-\nCLG-VAE\n91.61(±2.84)\n100.00(±0.00)\n11.62(±1.65)\n1.35(±0.26)\n29.55(±1.93)\n47.75(±5.83)\n-\n-\nCFASL\n95.70(±1.90)\n100.00(±0.00)\n18.58(±1.24)\n1.43(±0.18)\n34.81(±3.85)\n62.43(±8.08)\n-\n-\nsmallNORB\nFVM\nbeta VAE\nMIG\nSAP\nDCI\nm-FVM2\nm-FVM3\nm-FVM4\nβ-VAE\n60.71(±2.47)\n59.40(±7.72)\n21.60(±0.59)\n11.02(±0.18)\n25.43(±0.48)\n24.41(±3.34)\n15.13(±2.76)\n-\nβ-TCVAE\n59.30(±2.52)\n60.40(±5.48)\n21.64(±0.51)\n11.11(±0.27)\n25.74(±0.29)\n25.71(±3.51)\n15.66(±3.74)\n-\nFactor-VAE\n61.93(±1.90)\n56.40(±1.67)\n22.97(±0.49)\n11.21(±0.49)\n24.84(±0.72)\n26.43(±3.47)\n17.25(±3.50)\n-\nControl-VAE\n60.63(±2.67)\n61.40(±4.33)\n21.55(±0.53)\n11.18(±0.48)\n25.97(±0.43)\n24.11(±3.41)\n16.12(±2.53)\n-\nCLG-VAE\n62.27(±1.71)\n62.60(±5.17)\n21.39(±0.67)\n10.71(±0.33)\n22.95(±0.62)\n27.71(±3.45)\n17.16(±3.12)\n-\nCFASL\n62.73(±3.96)\n63.20(±4.13)\n22.23(±0.48)\n11.42(±0.48)\n24.58(±0.51)\n27.96(±3.00)\n17.37(±2.33)\n-\ndSprites\nFVM\nbeta VAE\nMIG\nSAP\nDCI\nm-FVM2\nm-FVM3\nm-FVM4\nβ-VAE\n73.54(±6.47)\n83.20(±7.07)\n13.19(±4.48)\n5.69(±1.98)\n21.49(±6.30)\n53.80(±10.29)\n50.13(±11.98)\n48.02(±8.98)\nβ-TCVAE\n79.19(±5.87)\n89.20(±4.73)\n23.95(±10.13)\n7.20(±0.66)\n35.33(±9.07)\n61.75(±6.71)\n57.82(±5.39)\n63.81(±9.45)\nFactor-VAE\n78.10(±4.45)\n84.40(±5.55)\n25.74(±10.58)\n6.37(±1.82)\n32.30(±9.47)\n58.39(±5.18)\n51.63(±2.88)\n53.71(±4.22)\nControl-VAE\n69.64(±7.67)\n82.80(±7.79)\n5.93(±2.78)\n3.89(±1.89)\n12.42(±4.95)\n38.99(±9.31)\n29.00(±10.75)\n19.33(±5.98)\nCLG-VAE\n82.33(±5.59)\n86.80(±3.43)\n23.96(±6.08)\n7.07(±0.86)\n31.23(±5.32)\n63.21(±8.13)\n48.68(±9.59)\n51.00(±8.13)\nCFASL\n82.30(±5.64)\n90.20(±5.53)\n33.62(±8.18)\n7.28(±0.63)\n46.52(±6.18)\n68.32(±0.13)\n66.25(±7.36)\n71.35(±12.08)\n3D Shapes\nFVM\nbeta VAE\nMIG\nSAP\nDCI\nm-FVM2\nm-FVM3\nm-FVM4\nβ-VAE\n84.33(±10.65)\n91.20(±4.92)\n45.80(±21.20)\n8.66(±3.80)\n66.05(±7.44)\n70.26(±6.27)\n61.52(±8.62)\n60.17(±8.48)\nβ-TCVAE\n86.03(±3.49)\n87.80(±3.49)\n60.02(±10.05)\n5.88(±0.79)\n70.38(±4.63)\n70.20(±4.08)\n63.79(±5.66)\n63.61(±5.90)\nFactor-VAE\n79.54(±10.72)\n95.33(±5.01)\n52.68(±22.87)\n6.20(±2.15)\n61.37(±12.46)\n66.93(±17.49)\n63.55(±18.02)\n57.00(±21.36)\nControl-VAE\n81.03(±11.95)\n95.00(±5.60)\n19.61(±12.53)\n4.76(±2.79)\n55.93(±13.11)\n62.22(±11.35)\n55.83(±13.61)\n51.66(±12.08)\nCLG-VAE\n83.16(±8.09)\n89.20(±4.92)\n49.72(±16.75)\n6.36(±1.68)\n63.62(±3.80)\n65.13(±5.26)\n58.94(±6.59)\n60.51(±7.62)\nCFASL\n89.70(±9.65)\n96.20(±4.85)\n62.12(±13.38)\n9.28(±1.92)\n75.49(±8.29)\n74.26(±2.82)\n67.68(±2.67)\n63.48(±4.12)\nTable 1: Disentanglement scores for single factor change (left 5 metrics) and multi-factor change (m-FVMs) with 10 random seeds.\n3D Car\nsmallNORB\ndSprites\n3D Shapes\nAvg.\nβ-VAE\n3.33\n4.86\n4.88\n3.38\n4.11\nβ-TCVAE\n2.50\n4.29\n2.50\n2.88\n3.04\nFactor-VAE\n3.17\n2.71\n3.38\n4.00\n3.31\nControl-VAE\n3.67\n3.86\n6.00\n5.50\n4.76\nCLG-VAE\n3.00\n3.86\n3.13\n4.13\n3.53\nCFASL\n1.00\n1.43\n1.13\n1.13\n1.17\nTable 2: Disentanglement performance rank. Each dataset rank is an\naverage of evaluation metrics, and Avg. is an average of all datasets.\ngence [Chen et al., 2018] and discriminator [Kim and Mnih,\n2018]. Differently, we induce disentanglement learning with\ngroup equivariant VAE for inductive bias.\nGroup Theory-Based Approaches for Disentangled Rep-\nresentation\nIn recent periods, various unsupervised disen-\ntanglement learning research proposes different approaches\nwith another definition of disentanglement, which is based\non the group theory [Higgins et al., 2018]. To learn the\nequivariant function, Topographic VAE [Keller and Welling,\n2021b] proposes the sequentially permuted activations on\nthe latent vector space called shifting temporal coherence,\nand Groupified VAE [Yang et al., 2022] method proposes\nthat inputs pass the encoder and decoder two times to im-\nplement permutation group equivariant VAE models. Also,\nCommutative Lie Group VAE (CLG-VAE) [Zhu et al., 2021;\nMercatali et al., 2022] maps latent vectors into Lie algebra\nwith one-parameter subgroup decomposition for inductive bias\nto learn the group structure from abstract canonical point to\ninputs. Differently, we propose the trainable symmetries that\nare extracted between two samples directly on the latent space\nwhile maintaining the equivariance function between input\nand latent vector space.\nSymmetry Learning with Equivariant Model\nLie group\nequivariant CNN [Dehmamy et al., 2021] and [Finzi et al.,\n2020] construct the In the other literature, several works ex-\ntract symmetries, which consist of matrices, between two\ninputs or objects. [Miyato et al., 2022] extracts the symme-\ntries between sequential or sequentially augmented inputs by\npenalizing the transformation of difference of the same time\ninterval. Other work extracts the symmetries by comparing\ntwo inputs, in which the differentiated factor is a rotation or\ntranslation, and implements symmetries with block diagonal\nmatrices [Bouchacourt et al., 2021]. Furthermore, [Marchetti\net al., 2023] decomposes the class and pose factor simultane-\nously by invariant and equivariant loss function with weakly\nsupervised learning. The unsupervised learning work [Winter\net al., 2022a] achieves class invariant and group equivariant\nfunction in less constraint conditions. Differently, we gener-\nally extend the a class invariant and group equivariant model in\nthe more complex disparity condition without any knowledge\nof the factors of datasets.\n5\nExperiments\n5.1\nSettings\nWe implement β-VAE [Higgins et al., 2017], β-TCVAE [Chen\net al., 2018], Factor-VAE [Kim and Mnih, 2018], control-\nVAE [Shao et al., 2020], and Commutative Lie Group VAE\n(CLG-VAE) [Zhu et al., 2021] for baseline. For common\nsettings to baselines, we set batch size 64, learning rate 1e-4,\nand random seed from {1, 2, . . . , 10} without weight decay.\nWe train for 3 × 105 iterations on dSprites smallNORB and\n3D Cars, and 5 × 105 iterations on 3D Shapes. Also, each\ndataset guarantees the commutativity of transformation. More\ndetails for experimental settings are in Appendix C.\n5.2\nQuantitative Analysis Results and Discussion\nDisentanglement Performance in Single and Multi-Factor\nChange\nWe evaluate four common disentanglement met-\nrics: FVM [Kim and Mnih, 2018], MIG [Chen et al., 2018],\nSAP [Kumar et al., 2018], and DCI [Eastwood and Williams,\n2018], and more details of evaluation settings are in Ap-\npendix C. As shown in Table 1, our method gradually im-\nproves the disentanglement learning in dSprites, 3D Cars, 3D\nShapes, and smallNORB datasets in most metrics. This result\nalso shows that our method positively affects single factor\nchange conditions. More details are in Appendix D.1.\nTo show the quantitative score of the disentanglement in\nmulti-factor change, we evaluate the m-FVMk, where max(k)\nis 2, 3, and 4 in 3D Cars, smallNORB, and dSprites datasets\nrespectively. As shown in Table 2, the proposed method shows\na statistically significant improvement, as indicated by the\nhigher average rank of dataset metrics compared to other ap-\nproaches. It implies that our method has the benefit of disen-\ntanglement learning in the multi-factor change condition. We\nprovide additional results in Appendix D.1.\nAblation Study\nTable 3 shows the ablation study to evaluate\nthe impact of each component of our method for disentangle-\nment learning. To compare factor-aligned losses (w/o Lpl, w/o\nLpd, w/o Ls, and w/o Lpl +Lpd +Ls), the best of among four\ncases is the w/o Lpl +Lpd +Ls and it implies that these losses\nare interrelated. In the case of w/o Lpl, the extraction of the\ncomposite symmetry gc becomes challenging due to the lack of\nunified roles among individual sections. Composite symmetry\ngc is affected by the second section selection method, which is\nwhether to use the section or not (0 or 1). Therefore, composite\nthat has a different role on elements in the same section strug-\ngles with constructing adequate composite symmetry gc. With\nthe similar perspective referred to w/o Lpl case, the coverage\nof code w/o Lpd is limited due to the absence of assurance that\neach section aligns with distinct factors. In the case of w/o Ls,\neach section assigns a different role and the elements of each\nsection align on the same factor, w/o Ls case is better than w/o\nLpl and w/o Lpd. Also, constructing the symmetries without\nthe equivariant model is meaningless because the model does\nnot satisfy Equation 6- 8. The w/o Lequiv naturally shows\nthe lowest results compared to other cases except w/o Lpd\nand Lpl. Moreover, the w/o Lp case shows the impact of the\nsecond section selection for unsupervised learning. Above all,\neach group exhibits a positive influence on disentanglement\nwhen compared to the base model (β-VAE). When combining\nall loss functions, our method consistently outperforms the\nothers across the majority of evaluation metrics. The inductive\nbias for symmetry changes (Lpl + Lpd) is less effective than\nthat for composition because the bias is only for symmetry\nchange control without latent dimension matching to a factor.\nThe inclusion of a sparsity loss effectively addresses this con-\ncern and yields the best improvement. More details are in the\nAppendix D.2.\nLp\nLc\nLe.\nLpl\nLpd\nLs\nFVM\nMIG\nSAP\nDCI\nm-FVM2\nβ-VAE\n✗\n✗\n✗\n✗\n✗\n✗\n88.19(±4.60)\n6.82(±2.93)\n0.63(±0.33)\n20.45(±3.93)\n42.36(±7.16)\n✗\n✓\n✓\n✓\n✓\n✓\n88.57(±6.68)\n7.18(±2.52)\n1.85(±1.04)\n18.39(±4.80)\n48.23(±5.51)\n✓\n✓\n✗\n✓\n✓\n✓\n88.56(±7.78)\n7.27(±4.16)\n1.31(±0.70)\n19.58(±4.45)\n42.63(±4.21)\n✓\n✓\n✓\n✗\n✓\n✓\n86.95(±5.96)\n7.11(±3.49)\n1.09(±0.40)\n18.35(±3.32)\n41.90(±7.80)\n✓\n✓\n✓\n✓\n✗\n✓\n85.42(±7.89)\n7.30(±3.73)\n1.15(±0.70)\n21.69(±4.70)\n41.90(±6.07)\n✓\n✓\n✓\n✗\n✗\n✗\n89.34(±5.18)\n9.44(±2.91)\n1.26(±0.40)\n23.14(±5.51)\n51.37(±9.29)\n✓\n✓\n✓\n✓\n✓\n✗\n90.71(±5.75)\n9.29(±3.74)\n1.07(±0.65)\n22.74(±5.06)\n45.84(±7.71)\n✓\n✓\n✓\n✓\n✓\n✓\n91.91(±3.45)\n9.51(±2.74)\n1.42(±0.52)\n20.72(±3.65)\n55.47(±10.09)\nTable 3: Ablation study for loss functions on 3D-Cars and β-VAE with 10 random seeds.\n(a) Generated images by composite symmetry and its factor-aligned symmetries. Image 1⃝ and 2⃝ are inputs, and image 3⃝ is\nan output from image 1⃝ (pθ(z1)). Image 5⃝ is a output of group element gc acted on z1 (p(gcz1)). Images 4⃝ are outputs of\ndecomposed composite symmetry gc acted on z1 sequentially.\n(b) Generated images by dimension change. Red and blue colored squares represent the value of latent vector dimensions of z1\nand z2. The images of baseline and CAFSL are the generated images from each latent vector.\n(c) Generalization over unseen pairs of images. We set pairs {(xi−1, xi)|1 ≤ i ≤ ||X||−1} then extract the symmetries between\nelements of each pair gp = {g(1,2), g(2,3), . . . g(k−1,k)} in inference step, where g(k−1,k) is a symmetry between zk−1 and zk.\nThe first row images are inputs (targets) and the second row images are the generated images by symmetry codebook.\nFigure 5: Qualitative Analysis to Generate Images from Latent Vectors in Various Conditions. More details are in the Appendix D.3.\n5.3\nQualitative Analysis Results and Discussion\nFigure 4: Heatmaps of Eigenvectors for\nlatent vector representations.\nIs Latent Vector\nSpace\nClose\nto\nDisentangled\nSpace?\nThe\nprevious result as\nshown in Fig. 1 is\na clear example\nof\nof\nwhether\nthe latent vector\nspace\nclosely\napproximates a disentangled space. The latent vector space\nof previous works (Fig. 1a-1e) are far from disentangled\nspace (Fig. 1g) but CFASL shows the closest disentangled\nspace compare to other methods.\nAlignment of Latent Dimensions to Factor-Aligned Sym-\nmetry\nIn the principal component analysis of latent vectors\nshown in Fig. 4, the eigenvectors V = [v1, v2, . . . , v|D|] are\nclose to one-hot vectors compared to the baseline, and the\ndominating dimension of the one-hot vectors are all different.\nThis result implies that the representation (factor) changes are\naligned to latent dimensions.\nFactor Aligned Symmetries\nTo verify the representation\nof learnable codebook over composite symmetries and factor-\naligned symmetries, we randomly select a sample pair as\nshown in Fig. 5a. The results imply that gc generated from the\ncodebook represents the composite symmetries between two\nimages ( 1⃝ and 2⃝) because the image 2⃝ and the generated\nimage 5⃝ by symmetry gc are similar (p(z2) ≈ p(gcz1)). Also,\neach factor-aligned symmetry (gi) generated from codebook\nsection affects a single factor changes as shown in images 4⃝.\nin Fig. 5a.\nFactor Aligned Latent Dimension\nTo analyze each factor\nchanges aligned to each dimension of latent vector space, we\nset the qualitative analysis as shown in 5b. We select two ran-\ndom samples (x1, x2), generate latent vectors z1 and z2, and\nselect the largest Kullback-Leibler divergence (KLD) value\ndimension from their posterior. Then, replacing the dimension\nvalue of z1 to the value of z2 one by one sequentially. As a\nresult, the shape and color factors are changed when a single\ndimension value is replaced within the baseline. However, our\nmethod results show no overlapped factor changes compared\nto baseline results. It implies that each latent vector dimension\nof the proposed method contains a single factor of information.\nUnseen Change Prediction in Sequential Data\nThe se-\nquential observation as [Miyato et al., 2022] is rarely observed\nin our methods, because of the random pairing during training\n(less 1 pair of observation). But their generated images via\ntrained symmetries of our method are similar to target images\nas shown in Fig. 5c. This result implies that our method is\nstrongly regularized for unseen change.\n6\nConclusion\nThis work tackles the difficulty of disentanglement learning\nof VAEs in unknown factors change conditions. We propose a\nnovel framework to learn composite symmetries from explicit\nfactor-aligned symmetries by codebook to directly represent\nthe multi-factor change of a pair of samples in unsupervised\nlearning. The framework enhances disentanglement by learn-\ning an explicit symmetry codebook, injecting three inductive\nbiases on the symmetries aligned to unknown factors, and\ninducing a group equivariant VAE model. We quantitatively\nevaluate disentanglement in the condition by a novel metric\n(m-FVMk) extended from a common metric for a single factor\nchange condition. This method significantly improved in the\nmulti-factor change and gradually improved in the single factor\nchange condition compared to state-of-the-art disentanglement\nmethods of VAEs. Also, training process does not need the\nknowledge of factor information of datasets. This work can\nbe easily plugged into VAEs and extends disentanglement to\nmore general factor conditions of complex datasets.\nReferences\n[Bengio et al., 2013] Yoshua Bengio, Aaron Courville, and\nPascal Vincent. Representation learning: a review and new\nperspectives. IEEE transactions on pattern analysis and\nmachine intelligence, 35(8):1798—1828, August 2013.\n[Bouchacourt et al., 2021] Diane\nBouchacourt,\nMark\nIbrahim, and St´ephane Deny. Addressing the topological\ndefects of disentanglement via distributed operators, 2021.\n[Burgess and Kim, 2018] Chris Burgess and Hyunjik Kim.\n3d shapes dataset. https://github.com/deepmind/3dshapes-\ndataset/, 2018.\n[Cao et al., 2022] Jinkun Cao, Ruiqian Nai, Qing Yang, Jialei\nHuang, and Yang Gao. An empirical study on disentan-\nglement of negative-free contrastive learning. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing\nSystems, 2022.\n[Chen et al., 2018] Ricky T. Q. Chen, Xuechen Li, Roger B\nGrosse, and David K Duvenaud. Isolating sources of dis-\nentanglement in variational autoencoders. In S. Bengio,\nH. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,\nand R. Garnett, editors, Advances in Neural Information\nProcessing Systems, volume 31. Curran Associates, Inc.,\n2018.\n[Dehmamy et al., 2021] Nima Dehmamy, Robin Walters,\nYanchen Liu, Dashun Wang, and Rose Yu. Automatic sym-\nmetry discovery with lie algebra convolutional network.\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Process-\ning Systems, 2021.\n[Eastwood and Williams, 2018] Cian Eastwood and Christo-\npher K. I. Williams. A framework for the quantitative\nevaluation of disentangled representations. In International\nConference on Learning Representations, 2018.\n[Finzi et al., 2020] Marc Finzi, Samuel Stanton, Pavel Iz-\nmailov, and Andrew Gordon Wilson. Generalizing con-\nvolutional neural networks for equivariance to lie groups on\narbitrary continuous data. arXiv preprint arXiv:2002.12880,\n2020.\n[Hall, 2015] B. Hall. Lie Groups, Lie Algebras, and Repre-\nsentations: An Elementary Introduction. Graduate Texts in\nMathematics. Springer International Publishing, 2015.\n[Higgins et al., 2017] Irina Higgins, Lo¨ıc Matthey, Arka\nPal, Christopher P. Burgess, Xavier Glorot, Matthew M.\nBotvinick, Shakir Mohamed, and Alexander Lerchner. beta-\nvae: Learning basic visual concepts with a constrained\nvariational framework. In ICLR, 2017.\n[Higgins et al., 2018] Irina Higgins, David Amos, David\nPfau, S´ebastien Racani`ere, Lo¨ıc Matthey, Danilo J.\nRezende, and Alexander Lerchner. Towards a definition\nof disentangled representations. CoRR, abs/1812.02230,\n2018.\n[Higgins et al., 2022] Irina Higgins, S´ebastien Racani`ere,\nand Danilo Rezende. Symmetry-based representations for\nartificial and biological general intelligence, 2022.\n[Jeong and Song, 2019] Yeonwoo Jeong and Hyun Oh Song.\nLearning discrete and continuous factors of data via al-\nternating disentanglement. In Kamalika Chaudhuri and\nRuslan Salakhutdinov, editors, Proceedings of the 36th In-\nternational Conference on Machine Learning, volume 97\nof Proceedings of Machine Learning Research, pages 3091–\n3099. PMLR, 09–15 Jun 2019.\n[Keller and Welling, 2021a] T. Anderson Keller and Max\nWelling. Topographic VAEs learn equivariant capsules.\nIn A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Process-\ning Systems, 2021.\n[Keller and Welling, 2021b] T. Anderson Keller and Max\nWelling.\nTopographic vaes learn equivariant capsules.\nCoRR, abs/2109.01394, 2021.\n[Kim and Mnih, 2018] Hyunjik Kim and Andriy Mnih. Dis-\nentangling by factorising. In Jennifer Dy and Andreas\nKrause, editors, Proceedings of the 35th International Con-\nference on Machine Learning, volume 80 of Proceedings\nof Machine Learning Research, pages 2649–2658. PMLR,\n10–15 Jul 2018.\n[Kingma and Welling, 2013] Diederik P Kingma and Max\nWelling. Auto-encoding variational bayes, 2013.\n[Kumar et al., 2018] Abhishek Kumar, Prasanna Sattigeri,\nand Avinash Balakrishnan. VARIATIONAL INFERENCE\nOF DISENTANGLED LATENT CONCEPTS FROM UN-\nLABELED OBSERVATIONS. In International Conference\non Learning Representations, 2018.\n[LeCun et al., 2004] Yann LeCun, Fu Jie Huang, and L´eon\nBottou. Learning methods for generic object recognition\nwith invariance to pose and lighting. In Proceedings of\nthe 2004 IEEE Computer Society Conference on Computer\nVision and Pattern Recognition, CVPR’04, page 97–104,\nUSA, 2004. IEEE Computer Society.\n[Liu et al., 2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and\nXiaoou Tang. Deep learning face attributes in the wild.\n2015 IEEE International Conference on Computer Vision\n(ICCV), pages 3730–3738, 2015.\n[Locatello et al., 2019] Francesco Locatello, Stefan Bauer,\nMario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard\nSch¨olkopf, and Olivier Bachem. Challenging common as-\nsumptions in the unsupervised learning of disentangled rep-\nresentations. In Kamalika Chaudhuri and Ruslan Salakhut-\ndinov, editors, Proceedings of the 36th International Con-\nference on Machine Learning, volume 97 of Proceedings\nof Machine Learning Research, pages 4114–4124. PMLR,\n09–15 Jun 2019.\n[Marchetti et al., 2023] Giovanni Luca Marchetti, Gustaf\nTegn´er, Anastasiia Varava, and Danica Kragic. Equivariant\nrepresentation learning via class-pose decomposition. In\nFrancisco Ruiz, Jennifer Dy, and Jan-Willem van de Meent,\neditors, Proceedings of The 26th International Conference\non Artificial Intelligence and Statistics, volume 206 of Pro-\nceedings of Machine Learning Research, pages 4745–4756.\nPMLR, 25–27 Apr 2023.\n[Matthey et al., 2017] Loic\nMatthey,\nIrina\nHig-\ngins,\nDemis\nHassabis,\nand\nAlexander\nLerchner.\ndsprites:\nDisentanglement\ntesting\nsprites\ndataset.\nhttps://github.com/deepmind/dsprites-dataset/, 2017.\n[Mercatali et al., 2022] Giangiacomo Mercatali, Andre Fre-\nitas, and Vikas Garg. Symmetry-induced disentanglement\non graphs. In S. Koyejo, S. Mohamed, A. Agarwal, D. Bel-\ngrave, K. Cho, and A. Oh, editors, Advances in Neural\nInformation Processing Systems, volume 35, pages 31497–\n31511. Curran Associates, Inc., 2022.\n[Michlo, 2021] Nathan Juraj Michlo. Disent - a modular dis-\nentangled representation learning framework for pytorch.\nGithub, 2021.\n[Miyato et al., 2022] Takeru Miyato, Masanori Koyama, and\nKenji Fukumizu. Unsupervised learning of equivariant\nstructure from sequences. In Alice H. Oh, Alekh Agarwal,\nDanielle Belgrave, and Kyunghyun Cho, editors, Advances\nin Neural Information Processing Systems, 2022.\n[Nasiri and Bepler, 2022] Alireza Nasiri and Tristan Bepler.\nUnsupervised object representation learning using trans-\nlation and rotation group equivariant VAE. In Alice H.\nOh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun\nCho, editors, Advances in Neural Information Processing\nSystems, 2022.\n[Quessard et al., 2020] Robin Quessard, Thomas Barrett, and\nWilliam Clements.\nLearning disentangled representa-\ntions and group structure of dynamical environments. In\nH. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and\nH. Lin, editors, Advances in Neural Information Process-\ning Systems, volume 33, pages 19727–19737. Curran Asso-\nciates, Inc., 2020.\n[Reed et al., 2015] Scott E Reed, Yi Zhang, Yuting Zhang,\nand Honglak Lee.\nDeep visual analogy-making.\nIn\nC. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Gar-\nnett, editors, Advances in Neural Information Processing\nSystems, volume 28. Curran Associates, Inc., 2015.\n[Shakerinava et al., 2022] Mehran Shakerinava, Arnab Ku-\nmar Mondal, and Siamak Ravanbakhsh. Structuring repre-\nsentations using group invariants. In Alice H. Oh, Alekh\nAgarwal, Danielle Belgrave, and Kyunghyun Cho, editors,\nAdvances in Neural Information Processing Systems, 2022.\n[Shao et al., 2020] Huajie Shao, Shuochao Yao, Dachun Sun,\nAston Zhang, Shengzhong Liu, Dongxin Liu, Jun Wang,\nand Tarek Abdelzaher. ControlVAE: Controllable varia-\ntional autoencoder. In Hal Daum´e III and Aarti Singh,\neditors, Proceedings of the 37th International Conference\non Machine Learning, volume 119 of Proceedings of Ma-\nchine Learning Research, pages 8655–8664. PMLR, 13–18\nJul 2020.\n[Shao et al., 2022] Huajie Shao, Yifei Yang, Haohong Lin,\nLongzhong Lin, Yizhuo Chen, Qinmin Yang, and Han\nZhao. Rethinking controllable variational autoencoders. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition (CVPR), pages 19250–19259,\nJune 2022.\n[Winter et al., 2022a] Robin Winter, Marco Bertolini, Tuan\nLe, Frank Noe, and Djork-Arn´e Clevert. Unsupervised\nlearning of group invariant and equivariant representations.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information\nProcessing Systems, 2022.\n[Winter et al., 2022b] Robin Winter, Marco Bertolini, Tuan\nLe, Frank Noe, and Djork-Arn´e Clevert. Unsupervised\nlearning of group invariant and equivariant representations.\nIn Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and\nKyunghyun Cho, editors, Advances in Neural Information\nProcessing Systems, 2022.\n[Xiao and Liu, 2020] Changyi Xiao and Ligang Liu. Genera-\ntive flows with matrix exponential. In Hal Daum´e III and\nAarti Singh, editors, Proceedings of the 37th International\nConference on Machine Learning, volume 119 of Proceed-\nings of Machine Learning Research, pages 10452–10461.\nPMLR, 13–18 Jul 2020.\n[Yang et al., 2022] Tao Yang, Xuanchi Ren, Yuwang Wang,\nWenjun Zeng, and Nanning Zheng. Towards building a\ngroup-based unsupervised representation disentanglement\nframework. In International Conference on Learning Rep-\nresentations, 2022.\n[Zhu et al., 2021] Xinqi Zhu, Chang Xu, and Dacheng Tao.\nCommutative lie group VAE for disentanglement learning.\nCoRR, abs/2106.03375, 2021.\nA\nLoss Function of Baseline\nAs shown in Table 4, we train the baselines with each objective\nfunction.\nVAEs\nLV AE\nβ-VAE\nEqϕ(z|x) log pθ(x|z) − βDKL(qϕ(z|x)||p(z))\nβ-TCVAE\nEqϕ(z|x) log pθ(x|z) − αDKL(q(z, n)||q(z)p(n))\n−βDKL(q(z)|| Q\nj a(zj)) − γ P\nj DKL(q(zj)||p(zj))\nFactor-VAE\n1\nN\nPN\ni [Eq(z|xi)[log p(xi|z)] − DKL(q(z|xi)||p(z))]\n−γDKL(q(z)|| Q\nj(zj))\nControl-VAE\nEqϕ(z|x) log pθ(x|z) − β(t)DKL(qϕ(z|x)||p(z))\nCLG-VAE\nEa(z|x)q(t|z) log p(x|z)p(z|t)\n−Eq(z|x)DKL(q(t|z)||p(t)) − Eq(z|x) log q(z|x)\nTable 4: Objective Function of the VAEs.\nB\nPerpendicular and Parallel Loss\nRelationship\nWe define parallel loss Lp to set two vectors in the same\nsection of the symmetries codebook to be parallel: z − gi\nj ∥\nz − gi\nj′z then,\nz − gi\njz = c(z − gi\nj′z)\n(10)\n⇒ (1 − c)z = (gi\nj − cgi\nj′)z\n(11)\n⇒ (1 − c)I = gi\nj − cgi\nj′ or [(1 − c)I + cgi\nj′ − gi\nj]z = 0,\n(12)\nwhere I is an identity matrix and constant c ∈ R. However,\nall latent z is not eigenvector of [(1 − c)I + cgi\nj′ − gi\nj]. Then,\nwe generally define symmetry as:\ngi\nj′ = 1\nc gi\nj + c − 1\nc\nI,\n(13)\nwhere i, j, and j′ are natural number 1 ≤ i ≤ |S|, 1 ≤ j, j′ ≤\n|SS|, and k ̸= j. Therefore, all symmetries in the same section\nare parallel then, any symmetry in the same section is defined\nby a specific symmetry in the same section.\nWe define orthogonal loss Lo between two vectors, which\nare in different sections, to be orthogonal: z −gi\njz ⊥ z −gk\nl z,\nwhere i ̸= k, 1 ≤ i, k ≤ |S|, and 1 ≤ j, l ≤ |SS|. By the\nEquation 13,\nz − gi\njz ⊥ z − gk\nl z\n(14)\n⇒ ( 1\nca\ngi\na + ca − 1\nca\nI)z − z ⊥ ( 1\ncb\ngk\nb + cb − 1\ncb\nI)z − z\n(15)\n⇒\n1\nca\n(gi\naz − z) ⊥ 1\ncb\n(gk\nb z − z),\n(16)\nwhere ca and cb are any natural number, and 1 ≤ a, b ≤ |SS|.\nTherefore, if two vectors from different sections are orthogonal\nand satisfied with Equation 13, then any pair of vectors from\ndifferent sections is always orthogonal.\nC\nExperiment Details\nDevice\nWe set the below settings for all experiments in a\nsingle Galaxy 2080Ti GPU for 3D Cars and smallNORB, and\na single Galaxy 3090 for dSprites 3D Shapes and CelebA.\nMore details are in README.md file.\nDatasets\n1) The dSprites dataset consists of 737,280 bi-\nnary 64 × 64 images with five independent ground truth fac-\ntors(number of values), i.e. x-position(32), y-position(32),\norientation(40), shape(3), and scale(6), [Matthey et al., 2017].\nAny composite transformation of x- and y-position, orienta-\ntion (2D rotation), scale, and shape is commutative. 2) The 3D\nCars dataset consists of 17,568 RGB 64 × 64 × 3 images with\nthree independent ground truth factors: elevations(4), azimuth\ndirections(24), and car models(183) [Reed et al., 2015]. Any\ncomposite transformation of elevations(x-axis 3D rotation),\nazimuth directions (y-axis 3D rotation), and models are com-\nmutative. 3) The smallNORB [LeCun et al., 2004] dataset\nconsists of total 96 × 96 24,300 grayscale images with four\nfactors, which are category(10), elevation(9), azimuth(18),\nlight(6) and we resize the input as 64 × 64. Any compos-\nite transformation of elevations(x-axis 3D rotation), azimuth\n(y-axis 3D rotation), light, and category is commutative. 4)\nThe 3D Shapes dataset consists of 480,000 RGB 64 × 64 × 3\nimages with six independent ground truth factors: orienta-\ntion(15), shape(4), floor color(10), scale(8), object color(10),\nand wall color(10) [Burgess and Kim, 2018]. 5) The CelebA\ndataset [Liu et al., 2015] consists of 202,599 images, and we\ncrop the center 128 × 128 area and then, resize to 64 × 64\nimages.\nEvaluation Settings\nWe set prune dims.threshold as 0.06,\n100 samples to evaluate global empirical variance in each\ndimension, and run it a total of 800 times to estimate the\nFVM score introduced in [Kim and Mnih, 2018]. For the\nother metrics, we follow default values introduced in [Michlo,\n2021], training and evaluation 10,000 and 5,000 times with 64\nmini-batches, respectively [Cao et al., 2022].\nModel\nHyper-parameter\nTuning\nWe\nimplement\nβ-\nVAE [Higgins et al., 2017], β-TCVAE [Chen et al., 2018],\ncontrol-VAE [Shao et al., 2020], Commutative Lie Group\nVAE (CLG-VAE) [Zhu et al., 2021], and Groupified-VAE (G-\nVAE) [Yang et al., 2022] for baseline. For common settings to\nbaselines, we set batch size 64, learning rate 1e-4, and random\nseed from {1, 2, . . . , 10} without weight decay. We train for\n3×105 iterations on dSprites smallNORB and 3D Cars, 6×105\niterations on 3D Shapes, and 106 iterations on CelebA. We\nset hyper-parameter β ∈ {1.0, 2.0, 4.0, 6.0} for β-VAE and β-\nTCVAE, fix the α, γ for β-TCVAE as 1 [Chen et al., 2018]. We\nfollow the ControlVAE settings [Shao et al., 2020], the desired\nvalue C ∈ {10.0, 12.0, 14.0, 16.0}, and fix the Kp = 0.01,\nKi = 0.001. For CLG-VAE, we also follow the settings [Zhu\net al., 2021] as λhessian = 40.0, λdecomp = 20.0, p = 0.2,\nand balancing parameter of lossrec group ∈ {0.1, 0.2, 0.5, 0.7}.\nFor G-VAE, we follow the official settings [Yang et al., 2022]\nwith β-TCVAE (β ∈ {10, 20, 30}), because applying this\nmethod to the β-TCVAE model usually shows higher perfor-\nmance than other models [Yang et al., 2022]. Then we select\nthe best case of models. We run the proposed model on the\nβ-VAE and β-TCVAE because these methods have no induc-\ntive bias to symmetries. We set the same hyper-parameters\nof baselines with ϵ ∈ {0.1, 0.01}, threshold ∈ {0.2, 0.5},\n|S| = |SS| = |D|, where |D| is a latent vector dimension.\nMore details for experimental settings.\nC.1\nBest Models for Quantitative Analysis\nIn this section, we show how we pick the best model among\nhyper-parameter tuning results. As shown in Table 5-7, we\nchoose the best model on each datasets.\nβ\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n1.0\n78.80(±6.61)\n65.13(±12.78)\n4.62(±3.21)\n2.67(±1.52)\n9.22(±3.05)\n2.0\n81.00(±7.62)\n64.78(±10.02)\n6.34(±3.66)\n3.37(±1.70)\n10.95(±4.42)\n4.0\n82.67(±7.28)\n73.54(±6.47)\n13.19(±4.48)\n5.69(±1.98)\n21.49(±6.30)\n6.0\n74.80(±10.46)\n63.20(±6.76)\n8.35(±2.95)\n2.43(±1.27)\n13.45(±5.07)\n(a) β-VAE\nβ\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n1.0\n77.20(±8.01)\n65.46(±8.79)\n4.32(±1.46)\n2.41(±1.30)\n9.34(±1.23)\n2.0\n78.20(±9.59)\n70.68(±11.16)\n11.74(±8.51)\n3.84(±2.83)\n16.80(±11.20)\n4.0\n87.40(±4.72)\n78.18(±7.31)\n19.47(±6.61)\n6.32(±1.70)\n30.05(±8.57)\n6.0\n89.20(±4.73)\n79.19(±5.87)\n23.95(±10.13)\n7.20(±0.66)\n35.33(±9.07)\n(b) β-TCVAE\nC\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n10.0\n82.80(± 7.79)\n69.64(±7.67)\n5.93(±2.78)\n3.89(±1.89)\n12.42(±4.95)\n12.0\n75.20(±5.43)\n68.00(±8.67)\n5.10(±2.24)\n2.49(±1.50)\n9.82(±3.69)\n14.0\n73.60(±9.03)\n61.58(±7.87)\n4.53(±2.60)\n2.11(±1.67)\n9.30(±1.89)\n16.0\n76.20(±8.14)\n63.28(±7.98)\n4.09(±2.00)\n2.08(±1.37)\n8.91(±1.88)\n(c) Control-VAE\nlossrec group\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n0.1\n86.80(±3.43)\n82.33(±5.59)\n23.96(±6.08)\n7.07(±0.86)\n31.23(±5.32)\n0.2\n88.20(±4.57)\n82.88(±3.55)\n20.39(±6.31)\n6.82(±1.80)\n28.28(±7.09)\n0.5\n88.20(±5.53)\n81.05(±7.51)\n20.63(±6.64)\n6.49(±1.98)\n27.45(±6.07)\n0.7\n88.00(±4.81)\n79.93(±8.16)\n18.95(±6.86)\n6.94(±1.19)\n27.27(±6.76)\n(d) Commutative Lie Group VAE\nTable 5: Baselines hyper-parameter tuning results on dSprites dataset with 10 random seeds.\nβ\nFVM\nMIG\nSAP\nDCI\nm-fvm2\n1.0\n88.19(±4.60)\n6.82(±2.93)\n0.63(±0.33)\n20.45(±3.93)\n42.36(±7.16)\n2.0\n88.51(±5.44)\n10.00(±3.84)\n0.79(±0.38)\n28.78(±7.28)\n50.98(±8.33)\n4.0\n90.95(±4.01)\n12.76(±1.19)\n0.61(±0.36)\n30.70(±3.06)\n55.76(±9.97)\n6.0\n91.83(±4.39)\n11.44(±1.07)\n0.63(±0.24)\n27.65(±2.50)\n61.28(±9.40)\n(a) β-VAE\nβ\nFVM\nMIG\nSAP\nDCI\nm-fvm2\n1.0\n89.85(±7.17)\n7.27(±3.94)\n0.71(±0.40)\n21.21(±6.26)\n41.99(±3.09)\n2.0\n91.29(±3.95)\n11.62(±3.70)\n0.79(±0.27)\n30.60(±5.23)\n54.87(±3.20)\n4.0\n92.70(±3.41)\n17.31(±2.91)\n1.07(±0.36)\n33.19(±3.38)\n59.12(±2.20)\n6.0\n92.32(±3.38)\n17.20(±3.06)\n1.13(±0.36)\n33.63(±3.27)\n59.25(±5.63)\n(b) β-TCVAE\nC\nFVM\nMIG\nSAP\nDCI\nm-fvm2\n10.0\n93.86(±5.12)\n9.73(±2.24)\n1.14(±0.54)\n25.66(±4.61)\n46.42(±10.34)\n12.0\n91.43(±5.32)\n8.65(±3.59)\n11.28(±0.70)\n21.05(±3.93)\n46.06(±11.20)\n14.0\n88.09(±5.46)\n6.11(±3.46)\n1.05(±0.57)\n22.09(±4.34)\n45.87(±10.84)\n16.0\n89.65(±6.87)\n8.12(±3.71)\n0.71(±0.36)\n20.89(±6.30)\n45.77(±10.23)\n(c) Control-VAE\nlossrec group\nFVM\nMIG\nSAP\nDCI\nm-fvm2\n0.1\n91.64(±3.91)\n10.68(±3.18)\n1.22(±0.47)\n31.24(±5.42)\n45.74(±8.68)\n0.2\n91.18(±3.18)\n11.45(±1.12)\n1.06(±0.25)\n31.09(±4.15)\n48.12(±6.04)\n0.5\n90.19(±3.46)\n10.90(±1.53)\n1.51(±0.30)\n30.68(±3.01)\n49.53(±8.44)\n0.7\n91.61(±2.84)\n11.62(±1.65)\n1.35(±2.61)\n29.55(±1.93)\n47.75(±5.83)\n(d) Commutative Lie Group VAE\nTable 6: Baselines hyper-parameter tuning results on 3D Cars dataset with 10 random seeds.\nβ\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n1.0\n59.40(±7.72)\n60.71(±2.47)\n21.61(±0.59)\n11.02(±0.18)\n25.43(±0.48)\n2.0\n56.80(±7.90)\n54.69(±2.96)\n19.97(±0.31)\n10.45(±0.24)\n21.15(±0.47)\n4.0\n52.40(±7.65)\n55.19(±1.73)\n19.14(±0.49)\n9.67(±0.24)\n20.54(±0.41)\n6.0\n52.67(±7.28)\n53.42(±1.54)\n18.05(±0.27)\n10.10(±0.28)\n21.03(±0.27)\n(a) β-VAE\nβ\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n1.0\n60.40(±5.48)\n59.30(±2.52)\n21.64(±0.51)\n11.11(±0.27)\n25.74(±0.29)\n2.0\n56.60(±9.24)\n59.48(±2.14)\n21.72(±0.44)\n11.08(±0.35)\n23.74(±0.33)\n4.0\n58.00(±6.86)\n56.40(±1.55)\n21.50(±0.62)\n10.98(±0.35)\n22.29(±0.73)\n6.0\n56.00(±8.17)\n55.46(±1.42)\n21.49(±0.52)\n10.50(±0.25)\n20.24(±0.51)\n(b) β-TCVAE\nC\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n10.0\n59.80(±5.77)\n60.34(±2.58)\n21.53(±0.33)\n10.91(±0.37)\n25.55(±0.49)\n12.0\n60.20(±10.60)\n61.00(±1.86)\n21.39(±0.41)\n11.25(±0.32)\n25.71(±0.37)\n14.0\n61.40(±4.33)\n60.63(±2.67)\n21.55(±0.53)\n11.18(±0.48)\n25.97(±0.43)\n16.0\n60.20(±7.69)\n60.50(±2.89)\n21.72(±0.31)\n11.30(±0.41)\n25.60(±0.33)\n(c) Control-VAE\nlossrec group\nbeta VAE\nFVM\nMIG\nSAP\nDCI\n0.1\n59.20(±5.75)\n59.54(±1.64)\n20.61(±0.41)\n10.93(±0.36)\n23.77(±0.60)\n0.2\n63.40(±9.14)\n59.74(±1.60)\n20.87(±0.36)\n10.80(±0.47)\n23.59(±0.63)\n0.5\n64.20(±8.24)\n61.28(±1.68)\n21.20(±0.53)\n10.58(±0.36)\n22.88(±0.52)\n0.7\n62.60(±5.17)\n62.26(±1.71)\n21.39(±0.67)\n10.71(±0.33)\n22.95(±0.62)\n(d) Commutative Lie Group VAE\nTable 7: Baselines hyper-parameter tuning results on smallNORB dataset with 10 random seeds.\nD\nAdditional Experiment\nD.1\nDisentanglement Performance\nStatistically Significant Improvements\nAs shown in Fig-\nure 6, our model significantly improves disentanglement learn-\ning.\n3D Shapes\nAs shown in Table 9, CFASL also shows an\nadvantage on multi-factor change.\nD.2\nAblation Studies\nHow Commutative Lie Group Improves Disetanglement\nLearning?\nThe Lie group is not commutative, however most\nfactors of the used datasets are commutative. For example,\n3D Shapes dataset factors consist of the azimuth (x-axis), yaw\n(z-axis), coloring, scale, and shape. Their 3D rotations are all\ncommutative. Also, other composite symmetries as coloring\nand scale are commutative. Even though we restrict the Lie\ngroup to be commutative, our model shows better results than\nbaselines as shown in Table 1.\nImpact of Hyper-Parameter tuning\nWe operate a grid\nsearch of the hyper-parameter ϵ. As shown in Figure 7a,\nthe Kullback-Leibler divergence convergences to the high-\nest value, when ϵ is large (ϵ = 1.0) and it shows less stable\nresults. It implies that the CFASL with larger ϵ struggles with\ndisentanglement learning, and is shown in Tabel 10a. Also, the\nLee in Figure 7b is larger than other cases, which implies that\nthe model struggles with extracting adequate composite sym-\nmetry because its encoder is far from the equivariant model\nand it is also shown in Table 10a. Even though ϵ = 0.01 case\nshows the lowest value in the most loss, Lde in Figure 7e is\nhigher than others and it also implies the model struggles with\nlearning symmetries, as shown in Table 10a because the model\ndoes not close to the equivariant model compare to ϵ = 0.1\ncase.\nImpact of Factor-Aligned Symmetry Size\nWe set the code-\nbook size as 100, and 10 to validate the robustness of our\nmethod. In Table 10b, the larger size shows better results than\nthe smaller one, and is more stable by showing a low standard\ndeviation.\n3D Cars\nLc\nwithout Lc\nx4.63\nx1.00\nTable 11: Complexity.\nImpact of Commutative\nLoss on Computational\nComplexity\nAs shown\nin Table 11, our methods\nreduce the composite symmetries computation. Matrix expo-\nnential is based on the Taylor series and it needs high compu-\ntation cost though its approximation is lighter than the Taylor\nseries. We need one matrix exponential computation for com-\nposite symmetries with commutative loss, in contrast, the\nother case needs the number of symmetry codebook elements\n|S| · |SS| for the matrix exponential and also |S| · |SS| − 1\ntime matrix multiplication.\nComparison of Plug-in Methods\nTo compare plug-in meth-\nods, we evaluate common disentanglement metrics on G-\nVAE [Shakerinava et al., 2022] and apply both methods to\nβ-TCVAE. As shown in Table 12, our method shows statis-\ntically significant improvements in disentanglement learning\nalthough β hyper-parameter of CFASL is smaller than G-VAE.\nAs shown in Table 8, we estimate the p-value over common\ndisentanglement metrics on each dataset. Most values show\nthat improvements in disentanglement learning are statistically\nsignificant.\np-value\nFVM\nMIG\nSAP\nDCI\ndSprites\n0.011\n0.005\n0.016\n0.001\n3D Cars\n0.006\n0.000\n0.97\n0.003\nsmallNORB\n0.000\n0.002\n0.000\n1.000\nTable 8: p-value estimation on each datasets.\n3D Shapes\nβ-VAE\nβ-TCVAE\nFactor-VAE\nControl-VAE\nCLG-VAE\nOURS\nm-FVM5\n80.26(±3.78)\n79.21(±5.87)\n76.69(±5.08)\n73.31(±6.54)\n73.61(±4.22)\n83.03(±2.73)\nTable 9: m-FVMs results.\nFigure 6: Disentanglement Scores with box plots.\nϵ\nFVM\nbeta VAE\nMIG\nSAP\nDCI\n0.01\n76.98(±8.63)\n87.33(±7.87)\n29.68(±11.38)\n6.96(±1.16)\n41.28(±11.93)\n0.1\n82.21(±1.34)\n90.33(±5.85)\n34.79(±3.26)\n7.45(±0.61)\n48.07(±5.62)\n1.0\n76.77(±7.05)\n78.33(±13.88)\n22.42(±11.14)\n6.02(±0.48)\n38.87(±7.83)\n(a) Hyper-parameter tuning with 6 random seeds.\n3D Cars\n|G|=100\n|G|=10\nFVM\n95.70(±1.90)\n48.63(±24.55)\nMIG\n18.58(±1.24)\n2.99(±6.04)\nSAP\n1.43(±0.18)\n0.29(±0.34)\nDCI\n34.81(±3.85)\n6.12(±10.44)\nFVM2\n62.43(±8.08)\n37.94(±10.01)\n(b) Codebook size impact\nTable 10: Table\nDatasets\nFVM\nMIG\nSAP\nDCI\nG-VAE\nCFASL\nG-VAE\nCFASL\nG-VAE\nCFASL\nG-VAE\nCFASL\ndSprites\n69.75(±13.66)\n82.30(±5.64)\n21.09(±9.20)\n33.62(±8.18)\n5.45(±2.25)\n7.28(±0.63)\n31.08(±10.87)\n46.52(±6.18)\n3D Car\n92.34(±2.96)\n95.70(±1.90)\n11.95(±2.16)\n18.58(±1.24)\n2.10(±0.96)\n1.43(±0.18)\n26.91(±6.24)\n34.81(±3.85)\nsmallNROB\n46.64(±1.45)\n61.15(±4.23)\n20.66(±1.22)\n22.23(±0.48)\n10.37(±0.51)\n11.12(±0.48)\n27.77(±0.68)\n24.59(±0.51)\nTable 12: Comparison of disentanglement scores of plug-in methods in single factor change.\n(a) kld (hyper)\n(b) encoder (hyper)\n(c) kld (w/o)\n(d) encoder (w/o)\n(e) decoder (hyper)\n(f) perpendicular (hyper)\n(g) decoder (w/o)\n(h) perpendicular (w/o)\n(i) parallel (hyper)\n(j) sparsity (hyper)\n(k) parallel (w/o)\n(l) sparsity (w/o)\nFigure 7: Loss curves: 1) HT: hyper-parameter tuning (ϵ ∈ {0.01, 0.1, 1.0}) with β-TCVAE based CFASL. 2) AB: ablation study with β-VAE\nbased CFASL.\nD.3\nAdditional Qualitative Analysis (Baseline vs.\nCFASL)\nFig. 8-9 show the qualitative results on 3D Cars introduced in\nFig. 5. Fig. 10-11, and Fig. 14 show the dSprites and small-\nNORB dataset results respectively. Additionally, we describe\nFig. 12-13results over 3D Shapes datasets respectively. We\nrandomly sample the images in all cases.\n3D Cars\nAs shown in Figure 8c, CFASL shows better re-\nsults than the baseline. In the 1st and 2nd rows, the baseline\nchanges shape and color factor when a single dimension value\nis changed, but ours clearly disentangle the representations.\nAlso in the 3rd row, the baseline struggles with separating\ncolor and azimuth but CFASL successfully separates the color\nand azimuth factors.\n• 1st row: our model disentangles the shape and color\nfactors when the 2nd dimension value is changed.\n• 2nd row: ours disentangles shape and color factors when\nthe 1st dimension value is changed.\n• 4th row: ours disentangles the color, and azimuth factors\nwhen the 2nd dimension value is changed.\ndSprites\nAs shown in Figure 10c, the CFASL shows better\nresults than the baseline. The CFASL significantly improves\nthe disentanglement learning as shown in the 4th and 5th\nrows. The baseline shows the multi-factor changes during a\nsingle dimension value is changed, while ours disentangles all\nfactors.\n• 1st row: ours disentangles the x- and y-pos factor when\nthe 2nd dimension value is changed.\n• 2nd row: ours disentangles the rotation and scale factor\nwhen the 2nd dimension value is changed.\n• 3rd row: ours disentangles the x- and y-pos, and rota-\ntion factor when the 1st and 2nd dimension values are\nchanged.\n• 4th row: ours disentangles the all factors when the 1st\nand 2nd dimension values are changed.\n3D Shapes\nAs shown in Figure 12c, the CFASL shows bet-\nter results than the baseline. In the 1st, 3rd, and 5th rows,\nour model clearly disentangles the factors while the baseline\nstruggles with disentangling multi-factors. Even though our\nmodel does not clearly disentangle the factors, compared to\nthe baseline, which is too poor for disentanglement learning,\nours improves the performance.\n• 1st row: our model disentangles the object color and\nfloor color factor when the 2nd and 3rd dimension values\nare changed.\n• 2nd row: ours disentangles shape factor in 1st dimen-\nsion, and object color and floor color factors at the 4th\ndimension value are changed.\n• 3rd row: ours disentangles the object color and floor\ncolor factor when the 3rd dimension value is changed.\n• 4th row: ours disentangles the scale, object color, wall\ncolor, and floor color factor when the 2nd and 3rd dimen-\nsion values are changed.\n• 5th row: ours disentangles the shape, object color, and\nfloor color factor when the 1st and 2nd dimension values\nare changed.\nsmallNORB\nEven though our model does not clearly disen-\ntangle the multi-factor changes, ours shows better results than\nthe baseline as shown in Figure 14c.\n• 1st row: our model disentangles the category and light\nfactor when the 2nd dimension value is changed.\n• 3rd row: ours disentangles category factor and azimuth\nfactors when the 5th dimension value is changed.\n(a) Generated images by composite symmetry on 3D Cars dataset. The images in the red box are inputs. The images in the blue\nbox at odd column are same as 3⃝ and even column are same as 5⃝ in Fig. 5a.\n(b) Generated images by its factor-aligned symmetries on 3D Cars datset. The images are same as 4⃝ in Fig 5a.\n(c) Generated images by dimension change on 3D Cars dataset.\nFigure 8: Generalization over unseen pairs of images on 3D Cars dataset.\nFigure 9: Fig. 8a shows the generation quality of composite symmetries results, Fig. 8b shows the disentanglement of symmetries by factors\nresults, and Fig. 8c shows the disentanglement of latent dimensions by factors results.\n(a) Generated images by composite symmetry on dSprites dataset. The images in the red box are inputs.\nThe images in the blue box at odd column are same as 3⃝ and even column are same as 5⃝ in Fig. 5a.\n(b) Generated images by its factor-aligned symmetries on dSprites datset. The images are same as 4⃝ in\nFig 5a.\n(c) Generated images by dimension change on dSprites dataset.\nFigure 10: Generalization over unseen pairs of images on dSprites dataset.\nFigure 11: Fig. 10a shows the generation quality of composite symmetries results, Fig. 10b shows the disentanglement of symmetries by\nfactors results, and Fig. 10c shows the disentanglement of latent dimensions by factors results.\n(a) Generated images by composite symmetry on 3DShapes dataset. The images in the red box are inputs. The\nimages in the blue box at odd column are same as 3⃝ and even column are same as 5⃝ in Fig. 5a.\n(b) Generated images by its factor-aligned symmetries on 3DShapes datset. The\nimages are same as 4⃝ in Fig 5a\n(c) Generated images by dimension change on 3DShapes dataset.\nFigure 12: Generalization over unseen pairs of images on 3DShapes dataset.\nFigure 13: Fig. 12a shows the generation quality of composite symmetries results, Fig. 12b shows the disentanglement of symmetries by\nfactors results, and Fig. 12c shows the disentanglement of latent dimensions by factors results.\n(a) Generated images by composite symmetry on smallNORB dataset. The images in the red box are inputs. The images in the blue box at odd\ncolumn are same as 3⃝ and even column are same as 5⃝ in Fig. 5a.\n(b) Generated images by its factor-aligned symmetries on smallNORB datset. The images are same as 4⃝ in Fig 5a\n(c) Generated images by dimension change on smallNORB dataset.\nFigure 14: Fig. 14a shows the generation quality of composite symmetries results, Fig. 14b shows the disentanglement of symmetries by\nfactors results, and Fig. 14c shows the disentanglement of latent dimensions by factors results.\n"
}