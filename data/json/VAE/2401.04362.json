{
    "optim": "Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example Kwan Yun∗ Youngseo Kim∗ Kwanggyoon Seo Chang Wook Seo Junyong Noh KAIST, Visual Media Lab Figure 1. Results of DiffSketch and distilled DiffSketchdistilled, trained with one example. The left sketches were generated by DiffSketch, while the right sketches were extracted from images using DiffSketchdistilled. Abstract We introduce DiffSketch, a method for generating a vari- ety of stylized sketches from images. Our approach focuses on selecting representative features from the rich semantics of deep features within a pretrained diffusion model. This novel sketch generation method can be trained with one manual drawing. Furthermore, efficient sketch extraction is ensured by distilling a trained generator into a streamlined extractor. We select denoising diffusion features through analysis and integrate these selected features with VAE fea- tures to produce sketches. Additionally, we propose a sam- pling scheme for training models using a conditional gen- erative approach. Through a series of comparisons, we verify that distilled DiffSketch not only outperforms exist- ing state-of-the-art sketch extraction methods but also sur- passes diffusion-based stylization methods in the task of ex- tracting sketches. 1. Introduction Sketching is performed in the initial stage of artistic cre- ation of drawing, serving as a foundational process for both *These authors contributed equally to this work conceptualizing and conveying artistic intentions. It also serves as a preliminary representation that visualizes the core structure and content of the eventual artwork. As sketches can exhibit distinct styles despite their basic form composed of simple lines, many studies in computer vision and graphics have attempted to train models for automati- cally extracting stylized sketches [2, 5, 25, 43, 55] that differ from abstract lines [30, 51, 54]. Majority of current sketch extraction approaches uti- lize image-to-image translation techniques to produce high- quality results. These approaches typically require a large dataset when training an image translation model from scratch, making it hard to personalize the sketch auto- colorization [6, 17, 63, 66] or sketch-based editting [24, 33, 44, 67]. On the other hand, recent research has explored the utilization of diffusion model [36, 40] features for down- stream tasks [16, 50, 60, 64]. Features derived from pre- trained diffusion models are known to contain rich seman- tics and spatial information [50, 60], which is known to help the training with limited data [3]. Previous studies have uti- lized these features extracted from a subset of layers, certain timesteps, or every specific intervals. Unfortunately, these hand-selected features often do not contain most of the in- 1 arXiv:2401.04362v1  [cs.CV]  9 Jan 2024 formation generated during the entire diffusion process. To this end, we propose Diffsketch, a new method that can extract representative features from a pretrained diffu- sion model and train the sketch extraction model with one data. For feature extraction from the denoising process, we statistically analyze the features and select those that can represent the whole feature information from the denoising process. Our new generator aggregates the features from multiple timesteps, fuses them with VAE features, and de- codes these fused features. The way we train the generator with synthetic features differs from that employed by previous diffusion-based styl- ization methods in that our method is specially designed for sketch extraction. While most diffusion-based stylization methods adopt the original pretrained diffusion model by swapping features [11, 50] or by inverting style into a cer- tain prompt [10, 39], these techniques do not provide fine control over the style of the sketch, making them unsuitable for extracting sketches in a desired style. In contrast, DiffS- ketch trains a generator model from scratch specifically for sketch extraction of a desired style. In addition to the newly proposed model architecture, we introduce a method for effective sampling during training. It is easy to train a network with data that share similar semantic information to ground truth data. However, re- lying solely on such data for training will hinder the full utilization of the capacity provided by the diffusion model. Therefore, we adopt a new sampling method to ensure train- ing with diverse examples while enabling effective training. Finally, we distill our network into a streamlined image-to- image translation network for improved inference speed and efficient memory usage. The resulting DiffSketchdistilled is the final network that is capable of performing a sketch ex- traction task. The contributions can be summarized as fol- lows: • We propose DiffSketch, a novel method that utilizes features from a pretrained diffusion model to generate sketches, learning from one manual sketch data. • Through analysis, we select the representative features during the diffusion process and utilize the VAE features as fine detailed input to the sketch generator. • We propose a new sampling method to train the model effectively with synthetic data. 2. Related Work 2.1. Sketch Extraction At its core, sketch extraction utilizes edge detection. Edge detection serves as the foundation not only for sketch ex- traction but also for tasks like object detection and segmen- tation [1, 65]. Initial edge detection studies primarily fo- cused on identifying edges based on abrupt variations in color or brightness [4, 55]. Although these techniques are direct and efficient without requiring extensive datasets to train on, they often produce outputs with artifacts, like scat- tered dots or lines. To make extracted sketches authentic, learning-based strategies have been introduced. These strategies excel at identifying object borders or rendering lines in distinct styles [21, 22, 25, 57, 58]. Chan et al. [5] took a step for- ward from prior techniques by incorporating the depth and semantic information of images to procure superior-quality sketches. In a more recent development, Ref2sketch [2] per- mits to extract stylized sketches using reference sketches through paired training. Semi-Ref2sketch [43] adopted con- trastive learning for semi-supervised training. All of these methods share the same limitation; they require a large amount of sketch data for training, which is hard to gather. Due to data scarcity, training a sketch extraction model is generally challenging. To address this challenge, our method is designed to train a sketch generator using just one manual drawing. 2.2. Diffusion Features for Downstream Task Diffusion models [12, 31] have shown cutting-edge results in tasks related to generating images conditioned on text prompt [35, 36, 40]. There have been attempts to analyze the features for utilization in downstream tasks such as seg- mentation [3, 16, 60], image editing [50], and finding dense semantic correspondence [26, 48, 64]. Most earlier studies chose a specific subset of features for their own downstream tasks. Recently, Luo et al. [26] proposed an aggregator that learns features from all layers and that uses equally sam- pled time steps. We advance a step further by analyzing and selecting the features from multiple timesteps, which repre- sent the overall features. We also propose a two-stage ag- gregation network and feature-fusing decoder utilizing ad- ditional information from VAE to generate finer details. 2.3. Deep Features for Sketch Extraction Most of recent sketch extraction methods utilize the deep features of a pretrained model for sketch extraction train- ing [2, 43, 61, 62]. While the approach of utilizing deep features from a pretrained classifier [14, 68] is widely used to measure perceptual similarity, vision-language models such as CLIP [34] are used to measure semantic similar- ity [5, 51]. These methods indirectly use the features by comparing them for the loss calculation during the training process instead of using them directly to generate a sketch. Unlike the previous approaches, we directly use the denois- ing diffusion features that contain rich information to ex- tract sketches for the first time. 3. Diffusion Features During a backward diffusion process, a latent or image with noise repeatedly invokes a UNet [37] to reduce the noise. 2 (a) Color by class (b) Color by timesteps Figure 2. Analysis on sampled features. PCA is applied to DDIM sampled features from different classes. (a) : features colored with human-labeled classes. (b) : features colored with denoising timesteps. The UNet produces several intermediate features with dif- ferent shapes. This collection of features contains rich in- formation about texture and semantics, which can be used to generate an image in various domains. For instance, fea- tures from the lower to intermediate layers of the UNet re- veal global structures and semantic regions, while features from higher layers exhibit fine and high-frequency infor- mation [26, 50]. Furthermore, features become more fine- grained over time steps [11]. As these features have dif- ferent information depending on their embedded layers and processed timesteps, it is important to select diverse features to fully utilize the information they provide. 3.1. Diffusion Features Selection Here, we first present a method for selecting features by analysis. Our approach involves selecting representative features from all the denoising timesteps and building our novel sketch generator, Gsketch to extract a sketch from an image by learning from a single data. To perform anal- ysis for this purpose, we first sampled 1,000 images ran- domly and collected all the features from multiple layers and timesteps during Denoising Diffusion Implicit Model (DDIM) sampling, with a total of 50 steps [47]. We conducted Principal component analysis (PCA) on these features from multiple classes and all timesteps to ex- amine the distribution of features depending on their se- mantics and timesteps. The PCA results are visualized in Figure 2. For our experiments, we manually classified the sampled images and their corresponding features into 17 classes with human perception, where each class contains more than 5 images. As illustrated by the left graphs in Fig- ure 2 (a), features from the same class tend to have similar characteristics, which can be seen as an additional proof to the previous literature finding that features contain seman- tic information [3, 60, 64]. There is also a smooth trajec- tory across timesteps as shown in Figure 2 (b). Therefore, selecting features from a hand-crafted interval can be more beneficial than using a single feature, as it provides richer information, as previously suggested [26]. Upon further ex- amination, we can observe that features tend to start at a similar point in their initial timesteps (t ≈ 50) and diverge thereafter (cyan box). In addition, during the initial steps, nearby values do not show a large difference compared to those in the middle (black box), while the final features ex- hibit distinct values even though they are on the same tra- jectory (orange box). These findings provide insights that can guide the selec- tion of representative features. As we aim to capture the most informative features across timesteps instead of using all features, we first conducted a K-means cluster analysis (K-means) [13] with Within Clusters Sum of Squares dis- tance (WCSS) to determine the number of representative clusters. One way to compute the K-means cluster with WCSS distance is to use the elbow method. However, we could not identify a clear visual elbow when 30 PCA com- ponents were used. Therefore, we used a combination of the Silhouette Score (SS) [38] and the Davies-Bouldin In- dex (DBI) [7]. For all features from each sampled image, we chose the first K that matched both k′’th highest SS score and k′’th lowest DBI score. From this process, we chose our K as 13 although this K value may vary with the number of diffusion sampling pro- cesses. We select the representative features from the center of each cluster to use them as input to our sketch generation network. To verify that the selected features indeed offer better representation compared to those selected from equal timesteps and random features, we calculated the minimum Euclidean distance from each projected feature to the se- lected 13 features across 1,000 images. We found that our method led to the minimum distance (18,615.6) among the distances achieved by using the equal timestep selection (19,004.9) and random selection (23,957.2). More expla- nations are provided in the supplementary material. 3.2. Diffusion Features Aggregation Inspired by feature aggregation networks for downstream tasks [26, 60], we build our two-level aggregation network and feature fusing decoder (FFD), both of which consti- 3 tute our new sketch generator Gsketch. The architectures of Gsketch and FFD are shown in Figure 4 (b) and (d), re- spectively. The diffusion features fl,t, generated on layer l and timestep t, are passed through the representative feature gate G∗. They are then upsampled to a certain resolution by Umd and Utp, and passed through a bottleneck layer Bl followed by being assigned with mixing weights w. The second aggregation network receives the first fused feature Ffst as an additional input feature. Ffst = T X t=0 lmd X l=1 wl,t · Bl(Umd(G∗(fl,t))), Ffin = T X t=0 L X l=lmd+1 wl,t · Bl(Utp(G∗(fl,t))) + L X l=lmd+1 wl · Bl(Utp(Ffst)) (1) Here, L is the total number of UNet layers, while lmd in- dicates the middle layer, which are set to be 12 and 9, re- spectively. Bottleneck layer Bl is shared across timesteps. T is the total number of timesteps. Ffst denotes the first level aggregated features and Ffin denotes the final aggre- gated features. These two levels of aggregation allow us to utilize the features in a memory efficient manner by mixing the features sequentially in a lower resolution first and then in a higher resolution. 3.3. VAE Decoder Features Unlike recent applications on utilizing diffusion features, where semantic correspondences are more important than high-frequency details, sketch generation utilizes both se- mantic information and high-frequency details such as tex- ture. As shown in Figure 3, VAE decoder features contain high-frequency details such as hair and wrinkles. From this observation, we designed our network to utilize VAE fea- tures following the aggregation of UNet features. Extended visualizations are provided in the supplementary material. Figure 3. Visualization of features from UNet and VAE in lower and higher resolution layers. Lower resolution layers are the first layers while higher resolution layers are the 11th for UNet and the 9th for VAE. We utilize all the VAE features from the residual blocks to build FFD. The aggregated features Ffin and VAE fea- tures are fused together to generate the output sketch. Specifically, in the fusing step i, VAE features with the same resolution are passed through the channel reduction layer followed by the convolution layer. These processed features are concatenated to the previously fused feature xi and the result is passed through the fusion layer to output xi+1. For the first step (i = 0), x0 is Ffin. All features in the same step has same resolution. We denote the number of total features at i as N without subscript for simplicity. This process is shown in Figure 4 (d) and can be expressed as follows: xi+1 = FUSE \"( N X n=1 Conv(CH(vi,n)) ) + xi # ˆIsketch = OUT \"( N X n=1 Conv(CH(vM,n)) ) + xM + Isource # (2) where CH is the channel reduction layer, Conv is the convolution layers, FUSE is the fusion layer, OUT is the final convolution layer applied before outputting a ˆIsketch, P and addition represent concatenation in the channel di- mension. Only at the last step (i = M), the source image, Isource is also concatenated to generate the output sketch. 4. DiffSketch DiffSketch learns to generate a pair of image and sketch through the process described below, which is also shown in Figure 4. 1. First, the user generates an image using a prompt with Stable Diffusion (SD) [36] and draws a corresponding sketch while its diffusion features F are kept. 2. The diffusion features F, its corresponding image Isource, and drawn sketch Isketch constitute a triplet data to train the sketch generator Gsketch with directional CLIP guidance. 3. With trained Gsketch, paired image and sketch can be generated with a condition. This becomes the input for the distilled network for fast sketch extraction. In the following subsections, we will describe the structure of sketch generator Gsketch (Sec. 4.1), its loss functions (Sec. 4.2), and the distilled network (Sec. 4.4). 4.1. Sketch Generator Our sketch generator Gsketch is built to utilize the features from the denoising diffusion process by performed UNet and the VAE as described in Secs. 3.2 and 3.3. Gsketch takes the representative features from UNet as input, and aggregate them and fuse them with the VAE decoder fea- tures vi,n to synthesizes the corresponding sketch ˆIsketch. 4 Figure 4. Overview of Diffsketch. The UNet features generated during the denoising process are fed to the Aggregation networks to be fused with the VAE features to generate a sketch corresponding to the image that Stable Diffusion generates. Unlike other image-to-image translation-based sketch ex- traction methods in which the network takes an image as input [2, 5, 43], our method accepts multiple deep features that have different spatial resolutions and channels. 4.2. Objectives To train Gsketch, we utilize the following loss functions: L = Lrec + λacrossLacross + λwithinLwithin (3) where λacross and λwithin are the balancing weights. Lacross and Lwithin are directional CLIP losses proposed in Mind- the-gap (MTG) [69], where Lwithin preserves the direction across the domain, by enforcing the difference between Isamp and Isource to be similar to that between Isampsketch and Isketch in CLIP embedding space. Similarly, Lacross en- forces the difference between Isampsketch and Isamp to be similar to that between Isketch and Isource. Lrec enforces the generated sketch from one known feature F and the ground truth sketch Isketch to be similar. While MTG uses an MSE loss for the pixel-wise reconstruction, we use an L1 distance to avoid blurry sketch results, which is impor- tant in the generation of stylized sketches. Our Lrec can be expressed as follows: Lrec = λL1LL1 + λLPIPSLLPIPS + λCLIPsimLCLIPsim (4) where λL1, λLPIPS, and λCLIPsim are the balancing weights. LCLIPsim calculates the semantic similarity in the cosine distance, LLPIPS [68] captures the perceptual similar- ity, and LL1 calculates the pixel-wise reconstruction. More details can be found in Sec. 5.1. 4.3. Sampling Scheme for Training Our method uses one source image and its corresponding sketch as the only ground truth when guiding the sketch style, using the direction of CLIP embeddings. Therefore, our losses rely on well-constructed CLIP manifold. When the domains of two images Isource and Isamp differ largely, the confidence in the directional CLIP loss becomes low in general (experiment details are provided in the supplemen- tary material). To fully utilize the capacity of the diffusion model and produce sketches in diverse domains, however, it is important to train the model on diverse examples. To ensure learning from diverse examples without de- creasing the CLIP loss confidence, we propose a novel sampling scheme, condition diffusion sampling for training (CDST). We envision that this sampling can be useful when training a model with a conditional generator. This method initially samples a data Isamp from one known condition C and gradually changes the sampling distribution to random by using a diffusion algorithm when training the network. The condition on the iteration iter (0 ≤ iter ≤ S) can be described as follows: αiter = r (1 − iter S ), βiter = r iter S , Citer = αiter αiter + βiter C + βiter αiter + βiter DSD, (5) where DSD represents the distribution of the pretrained SD, while S indicates the number of total diffusion dura- tion during training. 4.4. Distillation Once the sketch generator Gsketch is trained, DiffSketch can generate pairs of images and sketches in the trained style. This generation can be performed either randomly or with a specific condition. Due to the nature of the de- noising diffusion model, however, in which the result is re- fined through the denoising process, long processing time and high memory usage are required. Moreover, when 5 extracting sketches from images, the quality can be de- graded because of the inversion process. Therefore, to perform image-to-sketch extraction efficiently while ensur- ing high-quality results, we train DiffSketchdistilled using Pix2PixHD [52]. To train DiffSketchdistilled, we extract 30k pairs of im- age and sketch samples using our trained DiffSketch, ad- hering to CDST. Additionally, we employ regularization to ensure that the ground truth sketch Isketch can be gen- erated and discriminated effectively during the training of DiffSketchdistilled. With this trained model, images can be extracted in a given style much more quickly than with the original DiffSketch. 5. Experiments 5.1. Implementation Details We implemented DiffSketch and trained generator Gsketch on an Nvidia V-100 GPU for 1,200 iterations. When train- ing DiffSketch, we applied CDST with S in Eq. 4.3 to be 1,000. The model was trained with a fixed learning rate of 1e-4. The balancing weights λacross, λwithin, λL1, λLPIPS, and λCLIPsim are fixed at 1, 1, 30, 15, and 30, respectively. DiffSketchdistilled was trained on two A6000 GPUs using the same architecture and parameters from its original pa- per except for the output channel, where ours was set to one. We also added regularization on every 16 iterations. DiffSketchdistilled was trained with 30,000 pairs that were sampled from DiffSketch with CDST (S = 30, 000). LPIPS [68] and SSIM [53] were used for evaluation metrics, in both ablation study and comparison with base- lines. LPIPS was to calculate perceptual similarity with pre- trained classifier. SSIM was calculated for structural simi- larity of sketch image. 5.2. Datasets For training, DiffSketch requires a sketch corresponding to an image generated from SD. To facilitate a numerical com- parison, we established the ground truth for given images. Specifically, three distinct styles were employed for quan- titative evaluation: 1) HED [59] utilizes nested edge detec- tion and is one of the most widely used edge detection meth- ods. 2) XDoG [56] takes an algorithmic approach of using a difference of Gaussians to extract sketches. 3) Informative- anime [5] employs informative learning. This method is the state-of-the-art among single modal sketch extraction meth- ods and is trained on the Anime Colorization dataset [18], which consists of 14,224 sketches. For qualitative evalua- tion, we added hand-drawn sketches of two more styles. For testing, we employed the test set from BSDS500 dataset [29] and also randomly sampled an additional 1,000 images from the test set of Common Objects in Context (COCO) dataset [23]. As a result, our training set consisted of 3 sketches and the test dataset consisted of 3,600 pairs (1,200 pairs for each style) of image-sketch. Two hand- drawn sketches were used only for perceptual study because there is no ground truth to compare with. 5.3. Ablation Study We conducted an ablation study on each component of our method compared to the baselines as shown in Table 1. Ex- periment were performed to verify the contribution of each component; feature selections, CDST, losses, and FFD. To perform the ablation study, we randomly sampled 100 im- ages and extracted sketches with HED, XDog, and Anime- informative and paired them with all 100 images. All seeds were fixed to generate sketches from the same sample. The ablation study was conducted as follows. For Non- representative features, we randomly selected the features from the denoising timesteps while keeping the number of timesteps equal to ours (13). We performed this random selection and analysis twice. For one timestep feature, we only used the features from the final timestep t = 0. To produce a result without CDST, we executed random text prompt guidance for the diffusion sampling process during training. For the alternative loss approach, we contrasted L1 Loss with L2 Loss for pixel-level reconstruction, as pro- posed in MTG. To evaluate the effect of the FFD, we pro- duced sketches after removing the VAE features. The quantitative and qualitative results of the ablation study are shown in Table 1 and Figure 5, respectively. Ours achieved the higest average scores on both indices. Both Non-representative features achieved overall low scores in- dicating that representative feature selection helps obtain rich information. Similarly, using one time step features achieved lower scores than ours on average, showing the importance of including diverse features. W/O CDST scored lower than ours on both HED and XDoG styles. W/O L1 and FFD W/O features performed the worst due to the blurry and blocky output, respectively. The blocky results are due to lack of fine information from VAE. Condition Diffusion Sampling for Training While we tested on randomly generated images for quantitative eval- uation, our CDST can be applied to both training DiffSkech and sampling for training DiffSketchdistilled. Therefore, we performed an additional ablation study on CDST, com- paring Ours (trained and sampled with CDST), with W/O CDST (trained and sampled randomly). The outline of the sketch is clearly reproduced, following the style, when CDST is used as shown in Figure 6. 5.4. Comparison with Baselines We compared our method with 5 different alternatives in- cluding state-of-the-art sketch extraction methods [2, 43] and diffusion based methods [9, 19, 39]. Ref2sketch [2] 6 Table 1. Quantitative results on ablation with LPIPS and SSIM. Best scores are denoted in bold. Sketch Styles anime-informative HED XDoG Average Methods LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ Ours 0.2054 0.6835 0.2117 0.5420 0.1137 0.6924 0.1769 0.6393 Non-representative features 1 0.2154 0.6718 0.2383 0.5137 0.1221 0.6777 0.1919 0.6211 Non-representative features 2 0.2042 0.6869 0.2260 0.5281 0.1194 0.6783 0.1832 0.6311 One timestep features (t=0) 0.2135 0.6791 0.2251 0.5347 0.1146 0.6962 0.1844 0.6367 W/O CDST 0.2000 0.6880 0.2156 0.5341 0.1250 0.6691 0.1802 0.6304 W/O L1 0.2993 0.3982 0.2223 0.5011 0.1203 0.6547 0.2140 0.5180 FFD W/O VAE features 0.2650 0.5044 0.2650 0.4061 0.2510 0.3795 0.2603 0.4300 Target Source image GT Ours W/O L1 Non-representative  features 1 W/O CDST One timestep features (t=0) FFD W/O VAE features Non-representative  features 2 Figure 5. Visual examples of the ablation study. Ours generates higher quality results with details such as face, separated with hair region, compared to the alternatives. Figure 6. Visualization of additional ablation: Ours were trained and sampled with CDST. In contrast, W/O CDST were trained and sampled randomly. and Semi-Ref2sketch [43] are methods specifically de- signed to extract sketches in the style of a reference from a large pretrained network on diverse sketches in a supervised (Ref2sketch) and a semi-supervised (Semi- Ref2sketch) manner. DiffuseIT [19] is designed for image- to-image translation by disentangling style and content. DreamBooth [39] finetunes a Stable Diffusion model to generate personalized images, while Textural Inversion [10] optimizes an additional text embedding to generate a per- sonalized concept for a style or object. For DreamBooth and Textual Inversion, DDIM inversion was conducted to extract sketches. Table 2 presents the result of the quantitative evalua- tion that used BSDS500 and COCO datasets in a one-shot setting. Overall, ours achieved the best scores. While Semi-Ref2sketch scored higher on some of SSIM scores, the method relies on a large sketch dataset to train while ours requires only one. Figure 7 presents visual results produced by different methods. While Semi-Ref2sketch and Ref2sketch generated superior quality sketches to the results produced by others, they do not faithfully follow the style of the reference sketches, especially for dense styles. Diffusion-based methods sometimes overfit to the style image (DiffuseIT) or change the content of the im- ages (DreamBooth, Textual Inversion). DiffSketchdistilled generated superior results compared to these baselines, ef- fectively maintaining its styles and content. 5.5. Perceptual Study We conducted a user study to evaluate different sketch ex- traction methods on human perception. We recruited 45 participants to complete a survey that used test images from two datasets, processed in five different styles, to extract sketches. Each participant was presented with a total of 20 sets of source image, target sketch style, and resulting sketch. Participants were asked to choose one that best follows the given style while preserving the content of the source image. The result should not depend on demograph- ics distribution, therfore we did not focus on group of peo- ple as previous sketch studies [2, 5, 43]. As shown in Ta- ble 3, our method received the highest scores when com- pared with the alternative methods. Ours outperformed the diffusion-based methods by a large margin and even re- ceived a higher preference rating than the specialized sketch extraction method that was trained on a large sketch dataset. 6. Limitation and Conclusion We proposed DiffSketch, a novel method to train a sketch generator using representative features and extract sketches in diverse styles. For the first time, we conducted the task of 7 Figure 7. Qualitative comparison with alternative sketch extraction methods. Table 2. Quantitative comparison of different methods on BSDS500 and COCO datasets. BSDS500 - anime BSDS500 - HED BSDS500 - XDoG BSDS500 - average Methods LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ Oursdistilled 0.21746 0.49343 0.22706 0.59314 0.14280 0.64874 0.19577 0.57844 Ref2sketch 0.33621 0.46932 0.41993 0.31448 0.57096 0.13095 0.44237 0.30492 Semi-Ref2sketch 0.23916 0.50972 0.39675 0.34200 0.50447 0.30918 0.38013 0.38697 DiffuseIT 0.48365 0.29789 0.49217 0.19104 0.57335 0.11030 0.51639 0.19974 DreamBooth 0.80608 0.30149 0.74550 0.18523 0.72326 0.19465 0.75828 0.22712 Textual Inversion 0.82789 0.26373 0.77098 0.16416 0.64662 0.21953 0.74850 0.21581 COCO - anime COCO - HED COCO - XDoG COCO - average Methods LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ LPIPS↓ SSIM↑ Oursdistilled 0.17634 0.36021 0.20039 0.36093 0.14806 0.38319 0.17493 0.36811 Ref2sketch 0.32142 0.50517 0.37764 0.37230 0.56012 0.16835 0.41973 0.34861 Semi-Ref2sketch 0.21337 0.64732 0.32920 0.39487 0.47974 0.31894 0.34077 0.45371 DiffuseIT 0.46527 0.36092 0.47905 0.24611 0.56360 0.14595 0.50264 0.25099 DreamBooth 0.76399 0.30517 0.72278 0.22066 0.67909 0.21655 0.72195 0.24746 Textual Inversion 0.81458 0.29168 0.78835 0.19952 0.63215 0.22074 0.74503 0.23731 Table 3. Results from the user perceptual study given style exam- ple and the source image. The percentage indicates the selected frequency. Methods User Score Ours 68.67% Ref2sketch 6.00% Semi-Ref2sketch 18.56% DiffuseIT 0.22% DreamBooth 0.00% Textual Inversion 0.22% extracting sketches from the features of a diffusion model and demonstrated that our method outperforms previous state-of-the-art methods in extracting sketches. The ability to extract sketches in a diverse style, trained with one ex- ample, will have various use cases not only for artistic pur- poses but also for personalizing sketch-to-image retrieval and sketch-based image editing. We built our generator network specialized for generat- ing sketches by fusing aggregated features with the features from a VAE decoder. Consequently, our method works well with diverse sketches including dense sketches and outlines. Because our method not directly employ a loss function to compares stroke styles, however, it fails to generate highly abstract sketches or pointillism. One possible research di- rection could involve incorporating a new sketch style loss that does not require additional sketch data, such as penal- izing based on stroke similarity in close-ups. Although we focused on sketch extraction, our analysis of selecting representative features and the proposed train- ing scheme are not limited to the domain of sketches. Ex- tracting representative feature holds potential to improve applications leveraging diffusion features, including seman- tic segmentation, visual correspondence, and depth estima- tion. We believe this research direction promises to broaden the impact and utility of diffusion feature-based applica- tions. 8 References [1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji- tendra Malik. Contour detection and hierarchical image seg- mentation. IEEE transactions on pattern analysis and ma- chine intelligence, 33(5):898–916, 2010. 2 [2] Amirsaman Ashtari, Chang Wook Seo, Cholmin Kang, Si- hun Cha, and Junyong Noh. Reference based sketch extrac- tion via attention mechanism. ACM Transactions on Graph- ics (TOG), 41(6):1–16, 2022. 1, 2, 5, 6, 7 [3] Dmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, and Artem Babenko. Label-efficient se- mantic segmentation with diffusion models. arXiv preprint arXiv:2112.03126, 2021. 1, 2, 3 [4] John Canny. A computational approach to edge detection. IEEE Transactions on pattern analysis and machine intelli- gence, (6):679–698, 1986. 2 [5] Caroline Chan, Fr´edo Durand, and Phillip Isola. Learning to generate line drawings that convey geometry and semantics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7915–7925, 2022. 1, 2, 5, 6, 7 [6] Yuanzheng Ci, Xinzhu Ma, Zhihui Wang, Haojie Li, and Zhongxuan Luo. User-guided deep anime line art coloriza- tion with conditional adversarial networks. In Proceedings of the 26th ACM international conference on Multimedia, pages 1536–1544, 2018. 1 [7] David L Davies and Donald W Bouldin. A cluster separation measure. IEEE transactions on pattern analysis and machine intelligence, (2):224–227, 1979. 3, 1 [8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical im- age database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pages 248–255. IEEE, 2009. 1 [9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 6 [10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash- nik, Amit H Bermano, Gal Chechik, and Daniel Cohen- Or. An image is worth one word: Personalizing text-to- image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022. 2, 7, 1 [11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- age editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. 2, 3 [12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif- fusion probabilistic models. Advances in neural information processing systems, 33:6840–6851, 2020. 2 [13] Harold Hotelling. Analysis of a complex of statistical vari- ables into principal components. Journal of educational psy- chology, 24(6):417, 1933. 3 [14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep- tual losses for real-time style transfer and super-resolution. In Computer Vision–ECCV 2016: 14th European Confer- ence, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 694–711. Springer, 2016. 2 [15] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, 2019. 1 [16] Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi, Ali Mahdavi Amiri, and Ghassan Hamarneh. Slime: Seg- ment like me. arXiv preprint arXiv:2309.03179, 2023. 1, 2 [17] Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, and Sungjoo Yoo. Tag2pix: Line art colorization using text tag with secat and changing loss. In Proceedings of the IEEE/CVF inter- national conference on computer vision, pages 9056–9065, 2019. 1 [18] Taebum Kim. Anime sketch colorization pair. https: //www.kaggle.com/ktaebum/anime-sketch- colorization-pair, 2018. 6 [19] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representa- tion. In The Eleventh International Conference on Learning Representations, 2023. 6, 7, 1 [20] Elizaveta Levina and Peter Bickel. The earth mover’s dis- tance is the mallows distance: Some insights from statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, pages 251–256. IEEE, 2001. 6 [21] Chengze Li, Xueting Liu, and Tien-Tsin Wong. Deep extrac- tion of manga structural lines. ACM Transactions on Graph- ics (SIGGRAPH 2017 issue), 36(4):117:1–117:12, 2017. 2 [22] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and Deva Ramanan. Photo-sketching: Inferring contour draw- ings from images. In 2019 IEEE Winter Conference on Ap- plications of Computer Vision (WACV), pages 1403–1412. IEEE, 2019. 2 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft coco: Common objects in context, 2015. 6 [24] Feng-Lin Liu, Shu-Yu Chen, Yukun Lai, Chunpeng Li, Yue- Ren Jiang, Hongbo Fu, and Lin Gao. Deepfacevideoediting: Sketch-based deep editing of face videos. ACM Transactions on Graphics, 41(4):167, 2022. 1 [25] lllyasviel. sketchkeras. https://github.com/ lllyasviel/sketchKeras, 2017. 1, 2 [26] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn- ski, and Trevor Darrell. Diffusion hyperfeatures: Searching through time and space for semantic correspondence. arXiv preprint arXiv:2305.14334, 2023. 2, 3 [27] Kanti V Mardia. Measures of multivariate skewness and kur- tosis with applications. Biometrika, 57(3):519–530, 1970. 6 [28] Kanti V Mardia. Applications of some measures of mul- tivariate skewness and kurtosis in testing normality and ro- bustness studies. Sankhy¯a: The Indian Journal of Statistics, Series B, pages 115–128, 1974. 6 [29] David Martin, Charless Fowlkes, Doron Tal, and Jitendra Malik. A database of human segmented natural images 9 and its application to evaluating segmentation algorithms and measuring ecological statistics. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, pages 416–423. IEEE, 2001. 6 [30] Haoran Mo, Edgar Simo-Serra, Chengying Gao, Changqing Zou, and Ruomei Wang. General virtual sketching frame- work for vector line art. ACM Transactions on Graphics (TOG), 40(4):1–14, 2021. 1 [31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162–8171. PMLR, 2021. 2 [32] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 3 [33] Tiziano Portenier, Qiyang Hu, Attila Szabo, Siavash Ar- jomand Bigdeli, Paolo Favaro, and Matthias Zwicker. Faceshop: Deep sketch-based face image editing. arXiv preprint arXiv:1804.08972, 2018. 1 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervi- sion. In International conference on machine learning, pages 8748–8763. PMLR, 2021. 2 [35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Confer- ence on Machine Learning, pages 8821–8831. PMLR, 2021. 2 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684–10695, 2022. 1, 2, 4 [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U- net: Convolutional networks for biomedical image segmen- tation. In Medical Image Computing and Computer-Assisted Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18, pages 234–241. Springer, 2015. 2 [38] Peter J Rousseeuw. Silhouettes: a graphical aid to the inter- pretation and validation of cluster analysis. Journal of com- putational and applied mathematics, 20:53–65, 1987. 3, 1 [39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500– 22510, 2023. 2, 6, 7, 1 [40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479–36494, 2022. 1, 2 [41] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs, 2021. 6 [42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- man, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural In- formation Processing Systems, 35:25278–25294, 2022. 1 [43] Chang Wook Seo, Amirsaman Ashtari, and Junyong Noh. Semi-supervised reference-based sketch extraction using a contrastive learning framework. ACM Transactions on Graphics (TOG), 42(4):1–12, 2023. 1, 2, 5, 6, 7, 4 [44] Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, and Seungryong Kim. Midms: Matching interleaved diffusion models for exemplar-based image translation. arXiv preprint arXiv:2209.11047, 2022. 1 [45] Samuel Sanford Shapiro and Martin B Wilk. An analysis of variance test for normality (complete samples). Biometrika, 52(3/4):591–611, 1965. 6 [46] sharpei pups. 6.5 weeks old sharpei puppies. https:// www.youtube.com/watch?v=plIyQg6llp8, 2014. Accessed: 23-11-2023. 6 [47] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. 3, 1 [48] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng Phoo, and Bharath Hariharan. Emergent correspondence from image diffusion. arXiv preprint arXiv:2306.03881, 2023. 2 [49] TheSaoPauloSeries. S˜ao paulo city mini-documentary: (full hd) the s˜ao paulo series. https://www.youtube.com/ watch?v=A3pBJTTjwCM, 2013. Accessed: 23-11-2023. 6 [50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1921–1930, 2023. 1, 2, 3 [51] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro- man Christian Bachmann, Amit Haim Bermano, Daniel Cohen-Or, Amir Zamir, and Ariel Shamir. Clipasso: Semantically-aware object sketching. ACM Transactions on Graphics (TOG), 41(4):1–11, 2022. 1, 2 [52] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image syn- thesis and semantic manipulation with conditional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. 6 [53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si- moncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. 6 [54] Nora S Willett, Fernando de Goes, Kurt Fleischer, Mark Meyer, and Chris Burrows. Stylizing ribbons: Comput- ing surface contours with temporally coherent orientations. 10 IEEE Transactions on Visualization and Computer Graph- ics, 2023. 1 [55] Holger Winnem¨oller. Xdog: advanced image styliza- tion with extended difference-of-gaussians. In Proceed- ings of the ACM SIGGRAPH/eurographics symposium on non-photorealistic animation and rendering, pages 147–156, 2011. 1, 2 [56] Holger Winnem¨oller, Jan Eric Kyprianidis, and Sven C Olsen. Xdog: An extended difference-of-gaussians com- pendium including advanced image stylization. Computers & Graphics, 36(6):740–753, 2012. 6 [57] Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, and Xi- aohui Shen. Anime2sketch: A sketch extractor for anime arts with deep networks. https://github.com/ Mukosame/Anime2Sketch, 2021. 2 [58] Saining Xie and Zhuowen Tu. Holistically-nested edge de- tection. In Proceedings of the IEEE international conference on computer vision, pages 1395–1403, 2015. 2 [59] Saining Xie and Zhuowen Tu. Holistically-nested edge de- tection, 2015. 6, 1 [60] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao- long Wang, and Shalini De Mello. Open-vocabulary panop- tic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vi- sion and Pattern Recognition, pages 2955–2966, 2023. 1, 2, 3 [61] Ran Yi, Yong-Jin Liu, Yu-Kun Lai, and Paul L Rosin. Apdrawinggan: Generating artistic portrait drawings from face photos with hierarchical gans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10743–10752, 2019. 2 [62] Ran Yi, Yong-Jin Liu, Yu-Kun Lai, and Paul L Rosin. Unpaired portrait drawing generation via asymmetric cycle mapping. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8217–8225, 2020. 2 [63] Mingcheng Yuan and Edgar Simo-Serra. Line art coloriza- tion with concatenated spatial attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3946–3950, 2021. 1 [64] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola- nia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. A tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. arXiv preprint arXiv:2305.15347, 2023. 1, 2, 3 [65] Kaihua Zhang, Lei Zhang, Kin-Man Lam, and David Zhang. A level set approach to image segmentation with intensity in- homogeneity. IEEE transactions on cybernetics, 46(2):546– 557, 2015. 2 [66] Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and Chunping Liu. Two-stage sketch colorization. ACM Trans- actions on Graphics (TOG), 37(6):1–14, 2018. 1 [67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836–3847, 2023. 1 [68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht- man, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recogni- tion, pages 586–595, 2018. 2, 5, 6 [69] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka. Mind the gap: Domain gap control for single shot domain adaptation for generative adversarial networks. In Interna- tional Conference on Learning Representations, 2022. 5 11 Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example Supplementary Material Overview This supplementary material consists of 5 Sections. Section A describes implementation details (Sec. A). Sec. B pro- vides additional details and findings on diffusion features selection. Sec. C presents extended details of VAE decoder features. Sec. D contains the results of additional experi- ments on CDST. Lastly, Sec. E presents additional qualita- tive results with various style sketches. A. Implementation Details DiffSketch DiffSketch leverages the Stable Diffusion v1.4 sampled with DDIM [47] pretrained with the LAION- 5B [42] dataset, which produced images of resolution 512 × 512. With the pretrained Stable Diffusion, we use a to- tal of 50 time steps T for sampling. The training of DiffS- ketch was performed for 1200 iterations which required less than 3 hours on an Nvidia V100 GPU. For the training using HED [59], we concatenated the first two layers with the first three layers to stylize sketch. In the case of XDoG [55], we used Gary Grossi style. DiffSketchdistilled DiffSketchdistilled was developed to conduct sketch extraction efficiently with the streamlined generator. The training of DiffSketchdistilled was per- formed for 10 epochs for 30,000 sketch-image pairs gener- ated from DiffSKetch, following the CDST. The training of DiffSketchdistilled required approximately 5 hours on two Nvidia A6000 GPUs. The inference time of both DiffS- ketch and DiffSketchdistilled was 4.74 seconds and 0.0139 seconds, respectively, when tested on an Nvidia A5000 GPU with image with same resolutions. Comparison with Baselines For the baselines, the set- tings used in our study were based on the official code pro- vided by the authors and information obtained from their respective papers. For both Ref2Sketch [2] and Semi- ref2sketch [43], we used the given checkpoint, the offi- cial pre-trained model provided by the authors. For Dif- fuseIT [19], we also used the official code and check- point given by the authors in which the diffusion model was trained with the Imagenet [8] dataset, not FFHQ [15] because our comparison is not constrained to the face. For Dreambooth [39] and Textual Inversion [10], we used DDIM inversion [47] to invert the source image to the la- tent code of Stable Diffusion. B. Diffusion Features Selection To conduct K-means clustering for diffusion feature selec- tion, we first employed the elbow method, visualizing the results. However, a distinct elbow was not visually appar- ent, as shown in Figure 8. The left 6 images are WCSS values from randomly selected images out of our 1,000 test images. All 6 plots show similar patterns, making it hard to select a definitive elbow as stated in the main paper. The right image, which exhibits similar results, shows the aver- age of WCSS on all 1,000 images. Therefore, we chose to use the Silhouette score [38] and Davies-Bouldin index [7], which are two of the most widely used numerical method when choosing the optimal num- ber of clusters. However, they are two different methods, whose results do not always match with each other. We first visualized and found the contradicting results of these two methods as shown in Figure 9. Therefore, we chose to use the one that first matches the ith highest silhouette score and the ith lowest Davies-Bouldin index simultaneously. This process of choosing the optimal number of clusters can be written as follows : Algorithm 1 Finding the Optimal Number of Clusters 1: MAX clusters = Total time steps/2 2: sil indices ← sorted(range(MAX clusters), key = λk : silhouette scores[k], reverse = True) 3: db indices ← sorted(range(MAX clusters), key = λk : db scores[k], reverse = False) 4: for i ← 0 to MAX clusters do 5: if sil indices[i] in db indices[: i + 1] then 6: k optimal = sil indices[i]+1 7: break 8: end if 9: end for We conducted this process twice with two different num- bers of PCA components (10 and 30), yielding the results shown in Figure 10. The averages (13.26 and 13.34) and standard deviations (0.69 and 0.69) were calculated. As the mode value with both PCA components was 13, and the rounded average was also 13, we chose our optimal k to be 13. Using this number of clusters, we chose the representa- tive feature as the one nearest to the center of each cluster. From this process, we ended up with the following t val- ues: [0,3,8,12,16,21,25,28,32,35,39,43,47]. To verify the process, if an optimal number of clusters in each image can 1 Figure 8. Visualization of WCSS values according to the number used for K-means clustering. The left plots are the WCSS of the features from an randomly sampled image while the right plot shows the average WCSS values of the features from 1,000 randomly sampled images. image(a) image(b) image(d) image(c) image(e) Figure 9. Visualization of contradicting results of Silhouette scores and Davis Bouldin indices on five different images. really be globally adjusted, we compared our selected fea- tures against the baselines. These baselines included sam- pling at equal time intervals (t=[i*4+1 for i in the range of (0,13)]) and randomly selecting 13 values. We calculated the minimum Euclidean distance from each feature and con- firmed that our method resulted in the minimum distance across 1,000 randomly sampled images. This is illustrated in Table 4. Table 4. Sum of the minimum distances from all features Method Euclidean Distance Ours 18,615.6 Equal time steps 19,004.9 Random sample 23,957.2 In the main paper, we found several key insights through the visualization of features within the manually selected classes, which we summarize extensively here. First, se- mantically similar images lead to similar trajectories, al- though not identical. Second, features in the initial stage of the diffusion process (when t is approximately 50) retain Figure 10. Visualization on histogram for optimal k value with different number of PCA components. similar information despite significant differences in the re- sulting images. Third, features in the middle stage of the diffusion process (when t is around 25) exhibit larger differ- ences between adjacent features in their time steps. Lastly, the feature at the final time step (t=0) possesses distinc- tive information, varying significantly from previous values. This is also evident in the additional visualization presented in Figure 11. Our automatically selected features indicate a prioritiza- 2 Figure 11. Additional analysis on sampled features. PCA is applied to DDIM sampled features from different classes. Up : features colored with human-labeled classes. Down : features colored with denoising timesteps tion of the final feature (t=0), and that selection was made more from the middle stages than from the initial steps (t=[21,25,28] versus t=[43,47]). Our finding offer some guidance for manual feature selecting to consider the time steps, especially when memory is constrained. The order of the preference will on the last feature (t=0), a middle one (t is near 25), and the middle to final time steps while the features from initial steps are preferred less in general. For instance, when selecting four features from 50 time steps, a possible selection could be t=[0,12,25,37]. B.2 Features From Additional Models While we focused on T=50 DDIM sampling, for gener- alization, we examined different intervals (T=25, T=100) and different model. For these experiments, we randomly sampled 100 images. While our main experiments were conducted with manually classified images, we utilized DINOv2 [32], which was contrastively trained in a self- supervised manner and has learned visual semantics. With DINOv2, we separated the data into 15 different clusters and followed the process described in the main paper to plot the features. Here, we used 15 images from each cluster to calculate the PCA axis while we used 17 classes in the main experiments. The results, as shown in Figure 12 and Fig- ure 13, indicate that even with different sampling methods, the same conclusions regarding the sampling method can be drawn. The last feature exhibits a distinct value, while the features from the initial time step show similar values. In addition, we also tested on different model, Stable diffusion V2.1 which produce 768×768 images. Follow- ing the same process, we randomly sampled 100 images and clustered with DINOv2 and plot as shown in Figure 14. This result also shows that even with different model with different resolution, the same conclusions can be drawn, showing the scalability of our analysis. C. VAE Decoder Features VAE features fused with the Aggregation network features for FFD in the proposed model architecture. Figure 15 shows a visualization of the VAE features. We used a set of 20 generated face images and extracted features from dif- ferent decoder layers of the UNet and VAE decoder, at the last time step (t=0) similar to that of PNP [50]. We observe that the use of VAE decoder resulted in higher-frequency details than the UNet decoder. While the feature from UNet decoder contains semantic information, the features from VAE decoder produces finer details such as hair, wrinkles, and small letters. D. Condition Diffusion Sampling for Training D.1 Rationale Behind CDST An underlying assumption of CDST is that for a direc- tional CLIP loss, two images with a similar domain (Isource and Isamp in the main paper) leads to higher confidence 3 Color by clusters Color by time steps Figure 12. Additional analysis on sampled features. PCA is applied to 25 steps of DDIM sampled features with different clusters. Up : features colored with DINOv2 clusters. Down : features colored with denoising timesteps. Color by clusters Color by time steps Figure 13. Additional analysis on sampled features. PCA is applied to 100 steps of DDIM sampled features with different clusters. Up : features colored with DINOv2 clusters. Down : features colored with denoising timesteps. compared to two images with a different domain. To ex- amine this, we performed a confidence score test using 4SKST [43] which consists of four different sketch styles paired with color images. 4KST is suitable for the confi- dence score test because it contains images from two dif- ferent domains, photos, and anime images, in four different styles. We manually separated into photos and anime images since it was not labeled. Here, we computed a confidence score to determine if the directional clip loss is more re- liable when the calculated source images are in the same domain. We performed a test with three settings, measur- ing cosine similarity between the images IA (Photo) and IB (Anime) from different domains with the corresponding sketches SA and SB. All these images were encoded into the CLIP embedding. We employed two similarity scores Simwithin and Simacross in the same manner as the main paper (Sec.4.2). We calculated the similarity of the features 4 Color by clusters Color by time steps Figure 14. Additional analysis on Stable diffusion v2.1 sampled features. PCA is applied to 50 steps of DDIM sampled features with different clusters. Up : features colored with DINOv2 clusters. Down : features colored with denoising timesteps. Figure 15. Extended visualization of features from UNet and VAE. (a) shows the UNet decoder features in lower resolution (layers 1), intermediate resolution (layers 5), and higher resolution (layers 11). (b) shows the VAE decoder features in lower resolution (layers 1), intermediate resolution (layers 6), and higher resolution (layers 9). in the photo domain, in the anime domain, and across the two domains. The equation can be expressed as follows: Sim(X, Y ) = cos(−−−→ IXIY · −−−−→ SXSY ) + cos(−−−→ IXSX · −−−→ IY SY ) N (6) where cos(a · b) is the cosine similarity and N is the total number of cos calculation. X,Y corresponds to the images in each domain. With these computed similarities, the confidence score in domain A and domain B can be written as follows where Sim(ALL, ALL) denotes the average similarity of all im- 5 Table 5. Confidence scores on 4SKST with four different styles. Similarity Style1 Style2 Style3 Style4 Average confidence(Anime,Anime) 104.2608 102.8716 108.2026 101.3530 104.1720 confidence(Photo,Photo) 101.9346 98.8005 102.4516 100.5453 100.9330 confidence(Photo,Anime) 94.5036 94.0189 98.1867 92.3874 94.7742 ages: confidence(A,B) = Sim(A, B) Sim(ALL, ALL) × 100 (7) In Table 5, we show the confidence test results on four different style sketches. For all four styles, calculating the directional CLIP loss in the same domain produced higher confidence compared to the confidence computed across a different domain. Accordingly, we propose a sampling scheme, CDST to train the generator in the same domain at the initial stage of the training, which leads to higher confi- dence while widening its capacity in the latter iterations of training. D.2 Additional Experiment on CDST In the main paper, we used DSD for CDST. However, the distribution of the condition of a pretrained stable diffusion network is not known. Therefore, we approximate DSD by randomly sampling 1,000 text prompts from the LAION- 400M [41], which is a subset of the trained text-image pairs of the SD model. We then tokenized and embedded these prompts for preprocessing, following the process of the pre- trained SD model. We conducted PCA on these 1,000 sam- pled embeddings to extract 512 principal components. We then checked the normality of the sampled embeddings with all 512 principal component axes using the Shapiro-Wilk test [45] with a significance level of α = 5%. As a result, 214 components rejected the null hypothe- sis of normality. This indicates that each of its marginals cannot be assumed to be univariate normal. Next, we con- ducted the Mardia test [27, 28] with the same 1,000 sam- ples, taking into account skewness and kurtosis to check if the distribution is multivariate. The results failed to reject the null hypothesis of normality with a significance level of α = 5%. Therefore, we assumed DSD as a multivariate normal distribution for our sampling during training. We examined whether our calculated distribution of sta- ble diffusion (DSD) is similar to the ground truth em- bedding distribution of LAION-400M. For verification, we sampled 100k data from the embedded LAION-400M as a subset of ground truth. We also sampled same amount of embeddings from the multivariate normal distribution (Ours), univariate normal distribution for each axis, and a uniform distribution between the max and min values of the sampled embedded LAION-400M as a baseline. We used Earth moving distance (EMD) [20] and found out that the multivariate normal distribution lead the lowest distance, as shown in Table 6. Mij = ∥disti − distGT j∥2, ai = 1 len(dist), bj = 1 len(distGT ), W(dist, GTdist) = EMD(a, b, M). (8) This result does not prove that DSD has multivariate nor- mality, and the difference with the normal distribution is marginal. However, it is sufficient for our usage of the con- dition diffusion sampling for training. Table 6. Distance from GT embeddings. Method EMD Multivariate normal (Ours) 244.22 normal distribution for each axis 244.31 uniform distribution 1480.57 E. Qualitative Results We present additional results from the baseline compar- isons in Figure 16 and 17. Each figure shows the results that compared DiffSketchdistilled and the baseline methods on the COCO dataset [23] and the BSDS500 dataset [29], respectively. Addition to this, we also provide visual ex- amples of video sketch extraction results on diverse do- main including buildings, nature, and animals [46, 49] using DiffSketchdistilled in Figure 18 and supplementary video. 6 Figure 16. Qualitative comparison with alternative sketch extraction methods on COCO dataset. Figure 17. Qualitative comparison with alternative sketch extraction methods on BSDS500 dataset. 7 Figure 18. Qualitative examples of video sketch extraction. 8 "
}