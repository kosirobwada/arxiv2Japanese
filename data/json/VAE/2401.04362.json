{
    "optim": "Representative Feature Extraction During Diffusion Process\nfor Sketch Extraction with One Example\nKwan Yun∗\nYoungseo Kim∗\nKwanggyoon Seo\nChang Wook Seo\nJunyong Noh\nKAIST, Visual Media Lab\nFigure 1. Results of DiffSketch and distilled DiffSketchdistilled, trained with one example. The left sketches were generated by DiffSketch,\nwhile the right sketches were extracted from images using DiffSketchdistilled.\nAbstract\nWe introduce DiffSketch, a method for generating a vari-\nety of stylized sketches from images. Our approach focuses\non selecting representative features from the rich semantics\nof deep features within a pretrained diffusion model. This\nnovel sketch generation method can be trained with one\nmanual drawing. Furthermore, efficient sketch extraction is\nensured by distilling a trained generator into a streamlined\nextractor. We select denoising diffusion features through\nanalysis and integrate these selected features with VAE fea-\ntures to produce sketches. Additionally, we propose a sam-\npling scheme for training models using a conditional gen-\nerative approach.\nThrough a series of comparisons, we\nverify that distilled DiffSketch not only outperforms exist-\ning state-of-the-art sketch extraction methods but also sur-\npasses diffusion-based stylization methods in the task of ex-\ntracting sketches.\n1. Introduction\nSketching is performed in the initial stage of artistic cre-\nation of drawing, serving as a foundational process for both\n*These authors contributed equally to this work\nconceptualizing and conveying artistic intentions. It also\nserves as a preliminary representation that visualizes the\ncore structure and content of the eventual artwork.\nAs\nsketches can exhibit distinct styles despite their basic form\ncomposed of simple lines, many studies in computer vision\nand graphics have attempted to train models for automati-\ncally extracting stylized sketches [2, 5, 25, 43, 55] that differ\nfrom abstract lines [30, 51, 54].\nMajority of current sketch extraction approaches uti-\nlize image-to-image translation techniques to produce high-\nquality results. These approaches typically require a large\ndataset when training an image translation model from\nscratch, making it hard to personalize the sketch auto-\ncolorization [6, 17, 63, 66] or sketch-based editting [24, 33,\n44, 67]. On the other hand, recent research has explored the\nutilization of diffusion model [36, 40] features for down-\nstream tasks [16, 50, 60, 64]. Features derived from pre-\ntrained diffusion models are known to contain rich seman-\ntics and spatial information [50, 60], which is known to help\nthe training with limited data [3]. Previous studies have uti-\nlized these features extracted from a subset of layers, certain\ntimesteps, or every specific intervals. Unfortunately, these\nhand-selected features often do not contain most of the in-\n1\narXiv:2401.04362v1  [cs.CV]  9 Jan 2024\nformation generated during the entire diffusion process.\nTo this end, we propose Diffsketch, a new method that\ncan extract representative features from a pretrained diffu-\nsion model and train the sketch extraction model with one\ndata. For feature extraction from the denoising process, we\nstatistically analyze the features and select those that can\nrepresent the whole feature information from the denoising\nprocess. Our new generator aggregates the features from\nmultiple timesteps, fuses them with VAE features, and de-\ncodes these fused features.\nThe way we train the generator with synthetic features\ndiffers from that employed by previous diffusion-based styl-\nization methods in that our method is specially designed for\nsketch extraction. While most diffusion-based stylization\nmethods adopt the original pretrained diffusion model by\nswapping features [11, 50] or by inverting style into a cer-\ntain prompt [10, 39], these techniques do not provide fine\ncontrol over the style of the sketch, making them unsuitable\nfor extracting sketches in a desired style. In contrast, DiffS-\nketch trains a generator model from scratch specifically for\nsketch extraction of a desired style.\nIn addition to the newly proposed model architecture, we\nintroduce a method for effective sampling during training.\nIt is easy to train a network with data that share similar\nsemantic information to ground truth data. However, re-\nlying solely on such data for training will hinder the full\nutilization of the capacity provided by the diffusion model.\nTherefore, we adopt a new sampling method to ensure train-\ning with diverse examples while enabling effective training.\nFinally, we distill our network into a streamlined image-to-\nimage translation network for improved inference speed and\nefficient memory usage. The resulting DiffSketchdistilled is\nthe final network that is capable of performing a sketch ex-\ntraction task. The contributions can be summarized as fol-\nlows:\n• We propose DiffSketch, a novel method that utilizes\nfeatures from a pretrained diffusion model to generate\nsketches, learning from one manual sketch data.\n• Through analysis, we select the representative features\nduring the diffusion process and utilize the VAE features\nas fine detailed input to the sketch generator.\n• We propose a new sampling method to train the model\neffectively with synthetic data.\n2. Related Work\n2.1. Sketch Extraction\nAt its core, sketch extraction utilizes edge detection. Edge\ndetection serves as the foundation not only for sketch ex-\ntraction but also for tasks like object detection and segmen-\ntation [1, 65]. Initial edge detection studies primarily fo-\ncused on identifying edges based on abrupt variations in\ncolor or brightness [4, 55]. Although these techniques are\ndirect and efficient without requiring extensive datasets to\ntrain on, they often produce outputs with artifacts, like scat-\ntered dots or lines.\nTo make extracted sketches authentic, learning-based\nstrategies have been introduced.\nThese strategies excel\nat identifying object borders or rendering lines in distinct\nstyles [21, 22, 25, 57, 58]. Chan et al. [5] took a step for-\nward from prior techniques by incorporating the depth and\nsemantic information of images to procure superior-quality\nsketches. In a more recent development, Ref2sketch [2] per-\nmits to extract stylized sketches using reference sketches\nthrough paired training. Semi-Ref2sketch [43] adopted con-\ntrastive learning for semi-supervised training. All of these\nmethods share the same limitation; they require a large\namount of sketch data for training, which is hard to gather.\nDue to data scarcity, training a sketch extraction model\nis generally challenging.\nTo address this challenge, our\nmethod is designed to train a sketch generator using just\none manual drawing.\n2.2. Diffusion Features for Downstream Task\nDiffusion models [12, 31] have shown cutting-edge results\nin tasks related to generating images conditioned on text\nprompt [35, 36, 40]. There have been attempts to analyze\nthe features for utilization in downstream tasks such as seg-\nmentation [3, 16, 60], image editing [50], and finding dense\nsemantic correspondence [26, 48, 64]. Most earlier studies\nchose a specific subset of features for their own downstream\ntasks. Recently, Luo et al. [26] proposed an aggregator that\nlearns features from all layers and that uses equally sam-\npled time steps. We advance a step further by analyzing and\nselecting the features from multiple timesteps, which repre-\nsent the overall features. We also propose a two-stage ag-\ngregation network and feature-fusing decoder utilizing ad-\nditional information from VAE to generate finer details.\n2.3. Deep Features for Sketch Extraction\nMost of recent sketch extraction methods utilize the deep\nfeatures of a pretrained model for sketch extraction train-\ning [2, 43, 61, 62]. While the approach of utilizing deep\nfeatures from a pretrained classifier [14, 68] is widely used\nto measure perceptual similarity, vision-language models\nsuch as CLIP [34] are used to measure semantic similar-\nity [5, 51]. These methods indirectly use the features by\ncomparing them for the loss calculation during the training\nprocess instead of using them directly to generate a sketch.\nUnlike the previous approaches, we directly use the denois-\ning diffusion features that contain rich information to ex-\ntract sketches for the first time.\n3. Diffusion Features\nDuring a backward diffusion process, a latent or image with\nnoise repeatedly invokes a UNet [37] to reduce the noise.\n2\n(a) Color by class\n(b) Color by timesteps\nFigure 2. Analysis on sampled features. PCA is applied to DDIM sampled features from different classes. (a) : features colored with\nhuman-labeled classes. (b) : features colored with denoising timesteps.\nThe UNet produces several intermediate features with dif-\nferent shapes. This collection of features contains rich in-\nformation about texture and semantics, which can be used\nto generate an image in various domains. For instance, fea-\ntures from the lower to intermediate layers of the UNet re-\nveal global structures and semantic regions, while features\nfrom higher layers exhibit fine and high-frequency infor-\nmation [26, 50]. Furthermore, features become more fine-\ngrained over time steps [11]. As these features have dif-\nferent information depending on their embedded layers and\nprocessed timesteps, it is important to select diverse features\nto fully utilize the information they provide.\n3.1. Diffusion Features Selection\nHere, we first present a method for selecting features by\nanalysis.\nOur approach involves selecting representative\nfeatures from all the denoising timesteps and building our\nnovel sketch generator, Gsketch to extract a sketch from an\nimage by learning from a single data. To perform anal-\nysis for this purpose, we first sampled 1,000 images ran-\ndomly and collected all the features from multiple layers\nand timesteps during Denoising Diffusion Implicit Model\n(DDIM) sampling, with a total of 50 steps [47].\nWe conducted Principal component analysis (PCA) on\nthese features from multiple classes and all timesteps to ex-\namine the distribution of features depending on their se-\nmantics and timesteps. The PCA results are visualized in\nFigure 2. For our experiments, we manually classified the\nsampled images and their corresponding features into 17\nclasses with human perception, where each class contains\nmore than 5 images. As illustrated by the left graphs in Fig-\nure 2 (a), features from the same class tend to have similar\ncharacteristics, which can be seen as an additional proof to\nthe previous literature finding that features contain seman-\ntic information [3, 60, 64]. There is also a smooth trajec-\ntory across timesteps as shown in Figure 2 (b). Therefore,\nselecting features from a hand-crafted interval can be more\nbeneficial than using a single feature, as it provides richer\ninformation, as previously suggested [26]. Upon further ex-\namination, we can observe that features tend to start at a\nsimilar point in their initial timesteps (t ≈ 50) and diverge\nthereafter (cyan box). In addition, during the initial steps,\nnearby values do not show a large difference compared to\nthose in the middle (black box), while the final features ex-\nhibit distinct values even though they are on the same tra-\njectory (orange box).\nThese findings provide insights that can guide the selec-\ntion of representative features. As we aim to capture the\nmost informative features across timesteps instead of using\nall features, we first conducted a K-means cluster analysis\n(K-means) [13] with Within Clusters Sum of Squares dis-\ntance (WCSS) to determine the number of representative\nclusters. One way to compute the K-means cluster with\nWCSS distance is to use the elbow method. However, we\ncould not identify a clear visual elbow when 30 PCA com-\nponents were used. Therefore, we used a combination of\nthe Silhouette Score (SS) [38] and the Davies-Bouldin In-\ndex (DBI) [7]. For all features from each sampled image,\nwe chose the first K that matched both k′’th highest SS\nscore and k′’th lowest DBI score.\nFrom this process, we chose our K as 13 although this K\nvalue may vary with the number of diffusion sampling pro-\ncesses. We select the representative features from the center\nof each cluster to use them as input to our sketch generation\nnetwork. To verify that the selected features indeed offer\nbetter representation compared to those selected from equal\ntimesteps and random features, we calculated the minimum\nEuclidean distance from each projected feature to the se-\nlected 13 features across 1,000 images. We found that our\nmethod led to the minimum distance (18,615.6) among the\ndistances achieved by using the equal timestep selection\n(19,004.9) and random selection (23,957.2). More expla-\nnations are provided in the supplementary material.\n3.2. Diffusion Features Aggregation\nInspired by feature aggregation networks for downstream\ntasks [26, 60], we build our two-level aggregation network\nand feature fusing decoder (FFD), both of which consti-\n3\ntute our new sketch generator Gsketch. The architectures\nof Gsketch and FFD are shown in Figure 4 (b) and (d), re-\nspectively. The diffusion features fl,t, generated on layer l\nand timestep t, are passed through the representative feature\ngate G∗. They are then upsampled to a certain resolution\nby Umd and Utp, and passed through a bottleneck layer Bl\nfollowed by being assigned with mixing weights w. The\nsecond aggregation network receives the first fused feature\nFfst as an additional input feature.\nFfst =\nT\nX\nt=0\nlmd\nX\nl=1\nwl,t · Bl(Umd(G∗(fl,t))),\nFfin =\nT\nX\nt=0\nL\nX\nl=lmd+1\nwl,t · Bl(Utp(G∗(fl,t)))\n+\nL\nX\nl=lmd+1\nwl · Bl(Utp(Ffst))\n(1)\nHere, L is the total number of UNet layers, while lmd in-\ndicates the middle layer, which are set to be 12 and 9, re-\nspectively. Bottleneck layer Bl is shared across timesteps.\nT is the total number of timesteps. Ffst denotes the first\nlevel aggregated features and Ffin denotes the final aggre-\ngated features. These two levels of aggregation allow us to\nutilize the features in a memory efficient manner by mixing\nthe features sequentially in a lower resolution first and then\nin a higher resolution.\n3.3. VAE Decoder Features\nUnlike recent applications on utilizing diffusion features,\nwhere semantic correspondences are more important than\nhigh-frequency details, sketch generation utilizes both se-\nmantic information and high-frequency details such as tex-\nture. As shown in Figure 3, VAE decoder features contain\nhigh-frequency details such as hair and wrinkles. From this\nobservation, we designed our network to utilize VAE fea-\ntures following the aggregation of UNet features. Extended\nvisualizations are provided in the supplementary material.\nFigure 3. Visualization of features from UNet and VAE in lower\nand higher resolution layers. Lower resolution layers are the first\nlayers while higher resolution layers are the 11th for UNet and the\n9th for VAE.\nWe utilize all the VAE features from the residual blocks\nto build FFD. The aggregated features Ffin and VAE fea-\ntures are fused together to generate the output sketch.\nSpecifically, in the fusing step i, VAE features with the\nsame resolution are passed through the channel reduction\nlayer followed by the convolution layer. These processed\nfeatures are concatenated to the previously fused feature xi\nand the result is passed through the fusion layer to output\nxi+1. For the first step (i = 0), x0 is Ffin. All features in\nthe same step has same resolution. We denote the number\nof total features at i as N without subscript for simplicity.\nThis process is shown in Figure 4 (d) and can be expressed\nas follows:\nxi+1 = FUSE\n\"( N\nX\nn=1\nConv(CH(vi,n))\n)\n+ xi\n#\nˆIsketch = OUT\n\"( N\nX\nn=1\nConv(CH(vM,n))\n)\n+ xM + Isource\n#\n(2)\nwhere CH is the channel reduction layer, Conv is the\nconvolution layers, FUSE is the fusion layer, OUT is the\nfinal convolution layer applied before outputting a ˆIsketch,\nP and addition represent concatenation in the channel di-\nmension. Only at the last step (i = M), the source image,\nIsource is also concatenated to generate the output sketch.\n4. DiffSketch\nDiffSketch learns to generate a pair of image and sketch\nthrough the process described below, which is also shown\nin Figure 4.\n1. First, the user generates an image using a prompt with\nStable Diffusion (SD) [36] and draws a corresponding\nsketch while its diffusion features F are kept.\n2. The diffusion features F, its corresponding image\nIsource, and drawn sketch Isketch constitute a triplet data\nto train the sketch generator Gsketch with directional\nCLIP guidance.\n3. With trained Gsketch, paired image and sketch can be\ngenerated with a condition. This becomes the input for\nthe distilled network for fast sketch extraction.\nIn the following subsections, we will describe the structure\nof sketch generator Gsketch (Sec. 4.1), its loss functions\n(Sec. 4.2), and the distilled network (Sec. 4.4).\n4.1. Sketch Generator\nOur sketch generator Gsketch is built to utilize the features\nfrom the denoising diffusion process by performed UNet\nand the VAE as described in Secs. 3.2 and 3.3. Gsketch\ntakes the representative features from UNet as input, and\naggregate them and fuse them with the VAE decoder fea-\ntures vi,n to synthesizes the corresponding sketch ˆIsketch.\n4\nFigure 4. Overview of Diffsketch. The UNet features generated during the denoising process are fed to the Aggregation networks to be\nfused with the VAE features to generate a sketch corresponding to the image that Stable Diffusion generates.\nUnlike other image-to-image translation-based sketch ex-\ntraction methods in which the network takes an image as\ninput [2, 5, 43], our method accepts multiple deep features\nthat have different spatial resolutions and channels.\n4.2. Objectives\nTo train Gsketch, we utilize the following loss functions:\nL = Lrec + λacrossLacross + λwithinLwithin\n(3)\nwhere λacross and λwithin are the balancing weights. Lacross\nand Lwithin are directional CLIP losses proposed in Mind-\nthe-gap (MTG) [69], where Lwithin preserves the direction\nacross the domain, by enforcing the difference between\nIsamp and Isource to be similar to that between Isampsketch\nand Isketch in CLIP embedding space. Similarly, Lacross en-\nforces the difference between Isampsketch and Isamp to be\nsimilar to that between Isketch and Isource. Lrec enforces\nthe generated sketch from one known feature F and the\nground truth sketch Isketch to be similar. While MTG uses\nan MSE loss for the pixel-wise reconstruction, we use an\nL1 distance to avoid blurry sketch results, which is impor-\ntant in the generation of stylized sketches. Our Lrec can be\nexpressed as follows:\nLrec = λL1LL1 + λLPIPSLLPIPS + λCLIPsimLCLIPsim\n(4)\nwhere λL1, λLPIPS, and λCLIPsim are the balancing\nweights. LCLIPsim calculates the semantic similarity in the\ncosine distance, LLPIPS [68] captures the perceptual similar-\nity, and LL1 calculates the pixel-wise reconstruction. More\ndetails can be found in Sec. 5.1.\n4.3. Sampling Scheme for Training\nOur method uses one source image and its corresponding\nsketch as the only ground truth when guiding the sketch\nstyle, using the direction of CLIP embeddings. Therefore,\nour losses rely on well-constructed CLIP manifold. When\nthe domains of two images Isource and Isamp differ largely,\nthe confidence in the directional CLIP loss becomes low in\ngeneral (experiment details are provided in the supplemen-\ntary material). To fully utilize the capacity of the diffusion\nmodel and produce sketches in diverse domains, however, it\nis important to train the model on diverse examples.\nTo ensure learning from diverse examples without de-\ncreasing the CLIP loss confidence, we propose a novel\nsampling scheme, condition diffusion sampling for training\n(CDST). We envision that this sampling can be useful when\ntraining a model with a conditional generator. This method\ninitially samples a data Isamp from one known condition C\nand gradually changes the sampling distribution to random\nby using a diffusion algorithm when training the network.\nThe condition on the iteration iter (0 ≤ iter ≤ S) can be\ndescribed as follows:\nαiter =\nr\n(1 − iter\nS ), βiter =\nr\niter\nS ,\nCiter =\nαiter\nαiter + βiter\nC +\nβiter\nαiter + βiter\nDSD,\n(5)\nwhere DSD represents the distribution of the pretrained\nSD, while S indicates the number of total diffusion dura-\ntion during training.\n4.4. Distillation\nOnce the sketch generator Gsketch is trained, DiffSketch\ncan generate pairs of images and sketches in the trained\nstyle. This generation can be performed either randomly\nor with a specific condition. Due to the nature of the de-\nnoising diffusion model, however, in which the result is re-\nfined through the denoising process, long processing time\nand high memory usage are required.\nMoreover, when\n5\nextracting sketches from images, the quality can be de-\ngraded because of the inversion process.\nTherefore, to\nperform image-to-sketch extraction efficiently while ensur-\ning high-quality results, we train DiffSketchdistilled using\nPix2PixHD [52].\nTo train DiffSketchdistilled, we extract 30k pairs of im-\nage and sketch samples using our trained DiffSketch, ad-\nhering to CDST. Additionally, we employ regularization\nto ensure that the ground truth sketch Isketch can be gen-\nerated and discriminated effectively during the training of\nDiffSketchdistilled. With this trained model, images can be\nextracted in a given style much more quickly than with the\noriginal DiffSketch.\n5. Experiments\n5.1. Implementation Details\nWe implemented DiffSketch and trained generator Gsketch\non an Nvidia V-100 GPU for 1,200 iterations. When train-\ning DiffSketch, we applied CDST with S in Eq. 4.3 to be\n1,000. The model was trained with a fixed learning rate\nof 1e-4. The balancing weights λacross, λwithin, λL1, λLPIPS,\nand λCLIPsim are fixed at 1, 1, 30, 15, and 30, respectively.\nDiffSketchdistilled was trained on two A6000 GPUs using\nthe same architecture and parameters from its original pa-\nper except for the output channel, where ours was set to\none. We also added regularization on every 16 iterations.\nDiffSketchdistilled was trained with 30,000 pairs that were\nsampled from DiffSketch with CDST (S = 30, 000).\nLPIPS [68] and SSIM [53] were used for evaluation\nmetrics, in both ablation study and comparison with base-\nlines. LPIPS was to calculate perceptual similarity with pre-\ntrained classifier. SSIM was calculated for structural simi-\nlarity of sketch image.\n5.2. Datasets\nFor training, DiffSketch requires a sketch corresponding to\nan image generated from SD. To facilitate a numerical com-\nparison, we established the ground truth for given images.\nSpecifically, three distinct styles were employed for quan-\ntitative evaluation: 1) HED [59] utilizes nested edge detec-\ntion and is one of the most widely used edge detection meth-\nods. 2) XDoG [56] takes an algorithmic approach of using a\ndifference of Gaussians to extract sketches. 3) Informative-\nanime [5] employs informative learning. This method is the\nstate-of-the-art among single modal sketch extraction meth-\nods and is trained on the Anime Colorization dataset [18],\nwhich consists of 14,224 sketches. For qualitative evalua-\ntion, we added hand-drawn sketches of two more styles.\nFor testing, we employed the test set from BSDS500\ndataset [29] and also randomly sampled an additional 1,000\nimages from the test set of Common Objects in Context\n(COCO) dataset [23]. As a result, our training set consisted\nof 3 sketches and the test dataset consisted of 3,600 pairs\n(1,200 pairs for each style) of image-sketch. Two hand-\ndrawn sketches were used only for perceptual study because\nthere is no ground truth to compare with.\n5.3. Ablation Study\nWe conducted an ablation study on each component of our\nmethod compared to the baselines as shown in Table 1. Ex-\nperiment were performed to verify the contribution of each\ncomponent; feature selections, CDST, losses, and FFD. To\nperform the ablation study, we randomly sampled 100 im-\nages and extracted sketches with HED, XDog, and Anime-\ninformative and paired them with all 100 images. All seeds\nwere fixed to generate sketches from the same sample.\nThe ablation study was conducted as follows. For Non-\nrepresentative features, we randomly selected the features\nfrom the denoising timesteps while keeping the number of\ntimesteps equal to ours (13). We performed this random\nselection and analysis twice. For one timestep feature, we\nonly used the features from the final timestep t = 0. To\nproduce a result without CDST, we executed random text\nprompt guidance for the diffusion sampling process during\ntraining. For the alternative loss approach, we contrasted\nL1 Loss with L2 Loss for pixel-level reconstruction, as pro-\nposed in MTG. To evaluate the effect of the FFD, we pro-\nduced sketches after removing the VAE features.\nThe quantitative and qualitative results of the ablation\nstudy are shown in Table 1 and Figure 5, respectively. Ours\nachieved the higest average scores on both indices. Both\nNon-representative features achieved overall low scores in-\ndicating that representative feature selection helps obtain\nrich information. Similarly, using one time step features\nachieved lower scores than ours on average, showing the\nimportance of including diverse features.\nW/O CDST\nscored lower than ours on both HED and XDoG styles.\nW/O L1 and FFD W/O features performed the worst due\nto the blurry and blocky output, respectively. The blocky\nresults are due to lack of fine information from VAE.\nCondition Diffusion Sampling for Training\nWhile we\ntested on randomly generated images for quantitative eval-\nuation, our CDST can be applied to both training DiffSkech\nand sampling for training DiffSketchdistilled.\nTherefore,\nwe performed an additional ablation study on CDST, com-\nparing Ours (trained and sampled with CDST), with W/O\nCDST (trained and sampled randomly).\nThe outline of\nthe sketch is clearly reproduced, following the style, when\nCDST is used as shown in Figure 6.\n5.4. Comparison with Baselines\nWe compared our method with 5 different alternatives in-\ncluding state-of-the-art sketch extraction methods [2, 43]\nand diffusion based methods [9, 19, 39]. Ref2sketch [2]\n6\nTable 1. Quantitative results on ablation with LPIPS and SSIM. Best scores are denoted in bold.\nSketch Styles\nanime-informative\nHED\nXDoG\nAverage\nMethods\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nOurs\n0.2054\n0.6835\n0.2117\n0.5420\n0.1137\n0.6924\n0.1769\n0.6393\nNon-representative features 1\n0.2154\n0.6718\n0.2383\n0.5137\n0.1221\n0.6777\n0.1919\n0.6211\nNon-representative features 2\n0.2042\n0.6869\n0.2260\n0.5281\n0.1194\n0.6783\n0.1832\n0.6311\nOne timestep features (t=0)\n0.2135\n0.6791\n0.2251\n0.5347\n0.1146\n0.6962\n0.1844\n0.6367\nW/O CDST\n0.2000\n0.6880\n0.2156\n0.5341\n0.1250\n0.6691\n0.1802\n0.6304\nW/O L1\n0.2993\n0.3982\n0.2223\n0.5011\n0.1203\n0.6547\n0.2140\n0.5180\nFFD W/O VAE features\n0.2650\n0.5044\n0.2650\n0.4061\n0.2510\n0.3795\n0.2603\n0.4300\nTarget\nSource\nimage\nGT\nOurs\nW/O L1\nNon-representative \nfeatures 1\nW/O CDST\nOne timestep\nfeatures (t=0)\nFFD W/O\nVAE features\nNon-representative \nfeatures 2\nFigure 5. Visual examples of the ablation study. Ours generates higher quality results with details such as face, separated with hair region,\ncompared to the alternatives.\nFigure 6. Visualization of additional ablation: Ours were trained\nand sampled with CDST. In contrast, W/O CDST were trained and\nsampled randomly.\nand Semi-Ref2sketch [43] are methods specifically de-\nsigned to extract sketches in the style of a reference\nfrom a large pretrained network on diverse sketches in\na supervised (Ref2sketch) and a semi-supervised (Semi-\nRef2sketch) manner. DiffuseIT [19] is designed for image-\nto-image translation by disentangling style and content.\nDreamBooth [39] finetunes a Stable Diffusion model to\ngenerate personalized images, while Textural Inversion [10]\noptimizes an additional text embedding to generate a per-\nsonalized concept for a style or object. For DreamBooth\nand Textual Inversion, DDIM inversion was conducted to\nextract sketches.\nTable 2 presents the result of the quantitative evalua-\ntion that used BSDS500 and COCO datasets in a one-shot\nsetting.\nOverall, ours achieved the best scores.\nWhile\nSemi-Ref2sketch scored higher on some of SSIM scores,\nthe method relies on a large sketch dataset to train while\nours requires only one.\nFigure 7 presents visual results\nproduced by different methods.\nWhile Semi-Ref2sketch\nand Ref2sketch generated superior quality sketches to the\nresults produced by others, they do not faithfully follow\nthe style of the reference sketches, especially for dense\nstyles. Diffusion-based methods sometimes overfit to the\nstyle image (DiffuseIT) or change the content of the im-\nages (DreamBooth, Textual Inversion). DiffSketchdistilled\ngenerated superior results compared to these baselines, ef-\nfectively maintaining its styles and content.\n5.5. Perceptual Study\nWe conducted a user study to evaluate different sketch ex-\ntraction methods on human perception. We recruited 45\nparticipants to complete a survey that used test images from\ntwo datasets, processed in five different styles, to extract\nsketches. Each participant was presented with a total of\n20 sets of source image, target sketch style, and resulting\nsketch.\nParticipants were asked to choose one that best\nfollows the given style while preserving the content of the\nsource image. The result should not depend on demograph-\nics distribution, therfore we did not focus on group of peo-\nple as previous sketch studies [2, 5, 43]. As shown in Ta-\nble 3, our method received the highest scores when com-\npared with the alternative methods. Ours outperformed the\ndiffusion-based methods by a large margin and even re-\nceived a higher preference rating than the specialized sketch\nextraction method that was trained on a large sketch dataset.\n6. Limitation and Conclusion\nWe proposed DiffSketch, a novel method to train a sketch\ngenerator using representative features and extract sketches\nin diverse styles. For the first time, we conducted the task of\n7\nFigure 7. Qualitative comparison with alternative sketch extraction methods.\nTable 2. Quantitative comparison of different methods on BSDS500 and COCO datasets.\nBSDS500 - anime\nBSDS500 - HED\nBSDS500 - XDoG\nBSDS500 - average\nMethods\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nOursdistilled\n0.21746\n0.49343\n0.22706\n0.59314\n0.14280\n0.64874\n0.19577\n0.57844\nRef2sketch\n0.33621\n0.46932\n0.41993\n0.31448\n0.57096\n0.13095\n0.44237\n0.30492\nSemi-Ref2sketch\n0.23916\n0.50972\n0.39675\n0.34200\n0.50447\n0.30918\n0.38013\n0.38697\nDiffuseIT\n0.48365\n0.29789\n0.49217\n0.19104\n0.57335\n0.11030\n0.51639\n0.19974\nDreamBooth\n0.80608\n0.30149\n0.74550\n0.18523\n0.72326\n0.19465\n0.75828\n0.22712\nTextual Inversion\n0.82789\n0.26373\n0.77098\n0.16416\n0.64662\n0.21953\n0.74850\n0.21581\nCOCO - anime\nCOCO - HED\nCOCO - XDoG\nCOCO - average\nMethods\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nLPIPS↓\nSSIM↑\nOursdistilled\n0.17634\n0.36021\n0.20039\n0.36093\n0.14806\n0.38319\n0.17493\n0.36811\nRef2sketch\n0.32142\n0.50517\n0.37764\n0.37230\n0.56012\n0.16835\n0.41973\n0.34861\nSemi-Ref2sketch\n0.21337\n0.64732\n0.32920\n0.39487\n0.47974\n0.31894\n0.34077\n0.45371\nDiffuseIT\n0.46527\n0.36092\n0.47905\n0.24611\n0.56360\n0.14595\n0.50264\n0.25099\nDreamBooth\n0.76399\n0.30517\n0.72278\n0.22066\n0.67909\n0.21655\n0.72195\n0.24746\nTextual Inversion\n0.81458\n0.29168\n0.78835\n0.19952\n0.63215\n0.22074\n0.74503\n0.23731\nTable 3. Results from the user perceptual study given style exam-\nple and the source image. The percentage indicates the selected\nfrequency.\nMethods\nUser Score\nOurs\n68.67%\nRef2sketch\n6.00%\nSemi-Ref2sketch\n18.56%\nDiffuseIT\n0.22%\nDreamBooth\n0.00%\nTextual Inversion\n0.22%\nextracting sketches from the features of a diffusion model\nand demonstrated that our method outperforms previous\nstate-of-the-art methods in extracting sketches. The ability\nto extract sketches in a diverse style, trained with one ex-\nample, will have various use cases not only for artistic pur-\nposes but also for personalizing sketch-to-image retrieval\nand sketch-based image editing.\nWe built our generator network specialized for generat-\ning sketches by fusing aggregated features with the features\nfrom a VAE decoder. Consequently, our method works well\nwith diverse sketches including dense sketches and outlines.\nBecause our method not directly employ a loss function to\ncompares stroke styles, however, it fails to generate highly\nabstract sketches or pointillism. One possible research di-\nrection could involve incorporating a new sketch style loss\nthat does not require additional sketch data, such as penal-\nizing based on stroke similarity in close-ups.\nAlthough we focused on sketch extraction, our analysis\nof selecting representative features and the proposed train-\ning scheme are not limited to the domain of sketches. Ex-\ntracting representative feature holds potential to improve\napplications leveraging diffusion features, including seman-\ntic segmentation, visual correspondence, and depth estima-\ntion. We believe this research direction promises to broaden\nthe impact and utility of diffusion feature-based applica-\ntions.\n8\nReferences\n[1] Pablo Arbelaez, Michael Maire, Charless Fowlkes, and Ji-\ntendra Malik. Contour detection and hierarchical image seg-\nmentation. IEEE transactions on pattern analysis and ma-\nchine intelligence, 33(5):898–916, 2010. 2\n[2] Amirsaman Ashtari, Chang Wook Seo, Cholmin Kang, Si-\nhun Cha, and Junyong Noh. Reference based sketch extrac-\ntion via attention mechanism. ACM Transactions on Graph-\nics (TOG), 41(6):1–16, 2022. 1, 2, 5, 6, 7\n[3] Dmitry\nBaranchuk,\nIvan\nRubachev,\nAndrey\nVoynov,\nValentin Khrulkov, and Artem Babenko. Label-efficient se-\nmantic segmentation with diffusion models. arXiv preprint\narXiv:2112.03126, 2021. 1, 2, 3\n[4] John Canny. A computational approach to edge detection.\nIEEE Transactions on pattern analysis and machine intelli-\ngence, (6):679–698, 1986. 2\n[5] Caroline Chan, Fr´edo Durand, and Phillip Isola. Learning to\ngenerate line drawings that convey geometry and semantics.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 7915–7925, 2022. 1,\n2, 5, 6, 7\n[6] Yuanzheng Ci, Xinzhu Ma, Zhihui Wang, Haojie Li, and\nZhongxuan Luo. User-guided deep anime line art coloriza-\ntion with conditional adversarial networks. In Proceedings\nof the 26th ACM international conference on Multimedia,\npages 1536–1544, 2018. 1\n[7] David L Davies and Donald W Bouldin. A cluster separation\nmeasure. IEEE transactions on pattern analysis and machine\nintelligence, (2):224–227, 1979. 3, 1\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei.\nImagenet: A large-scale hierarchical im-\nage database. In Computer Vision and Pattern Recognition,\n2009. CVPR 2009. IEEE Conference on, pages 248–255.\nIEEE, 2009. 1\n[9] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 6\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-\nnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-\nOr.\nAn image is worth one word: Personalizing text-to-\nimage generation using textual inversion.\narXiv preprint\narXiv:2208.01618, 2022. 2, 7, 1\n[11] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman,\nYael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im-\nage editing with cross attention control.\narXiv preprint\narXiv:2208.01626, 2022. 2, 3\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840–6851, 2020. 2\n[13] Harold Hotelling. Analysis of a complex of statistical vari-\nables into principal components. Journal of educational psy-\nchology, 24(6):417, 1933. 3\n[14] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Percep-\ntual losses for real-time style transfer and super-resolution.\nIn Computer Vision–ECCV 2016: 14th European Confer-\nence, Amsterdam, The Netherlands, October 11-14, 2016,\nProceedings, Part II 14, pages 694–711. Springer, 2016. 2\n[15] Tero Karras, Samuli Laine, and Timo Aila. A style-based\ngenerator architecture for generative adversarial networks. In\nCVPR, 2019. 1\n[16] Aliasghar Khani, Saeid Asgari Taghanaki, Aditya Sanghi,\nAli Mahdavi Amiri, and Ghassan Hamarneh. Slime: Seg-\nment like me. arXiv preprint arXiv:2309.03179, 2023. 1,\n2\n[17] Hyunsu Kim, Ho Young Jhoo, Eunhyeok Park, and Sungjoo\nYoo. Tag2pix: Line art colorization using text tag with secat\nand changing loss. In Proceedings of the IEEE/CVF inter-\nnational conference on computer vision, pages 9056–9065,\n2019. 1\n[18] Taebum Kim.\nAnime sketch colorization pair.\nhttps:\n//www.kaggle.com/ktaebum/anime-sketch-\ncolorization-pair, 2018. 6\n[19] Gihyun Kwon and Jong Chul Ye.\nDiffusion-based image\ntranslation using disentangled style and content representa-\ntion. In The Eleventh International Conference on Learning\nRepresentations, 2023. 6, 7, 1\n[20] Elizaveta Levina and Peter Bickel. The earth mover’s dis-\ntance is the mallows distance: Some insights from statistics.\nIn Proceedings Eighth IEEE International Conference on\nComputer Vision. ICCV 2001, pages 251–256. IEEE, 2001.\n6\n[21] Chengze Li, Xueting Liu, and Tien-Tsin Wong. Deep extrac-\ntion of manga structural lines. ACM Transactions on Graph-\nics (SIGGRAPH 2017 issue), 36(4):117:1–117:12, 2017. 2\n[22] Mengtian Li, Zhe Lin, Radomir Mech, Ersin Yumer, and\nDeva Ramanan. Photo-sketching: Inferring contour draw-\nings from images. In 2019 IEEE Winter Conference on Ap-\nplications of Computer Vision (WACV), pages 1403–1412.\nIEEE, 2019. 2\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll´ar. Microsoft\ncoco: Common objects in context, 2015. 6\n[24] Feng-Lin Liu, Shu-Yu Chen, Yukun Lai, Chunpeng Li, Yue-\nRen Jiang, Hongbo Fu, and Lin Gao. Deepfacevideoediting:\nSketch-based deep editing of face videos. ACM Transactions\non Graphics, 41(4):167, 2022. 1\n[25] lllyasviel.\nsketchkeras.\nhttps://github.com/\nlllyasviel/sketchKeras, 2017. 1, 2\n[26] Grace Luo, Lisa Dunlap, Dong Huk Park, Aleksander Holyn-\nski, and Trevor Darrell. Diffusion hyperfeatures: Searching\nthrough time and space for semantic correspondence. arXiv\npreprint arXiv:2305.14334, 2023. 2, 3\n[27] Kanti V Mardia. Measures of multivariate skewness and kur-\ntosis with applications. Biometrika, 57(3):519–530, 1970. 6\n[28] Kanti V Mardia.\nApplications of some measures of mul-\ntivariate skewness and kurtosis in testing normality and ro-\nbustness studies. Sankhy¯a: The Indian Journal of Statistics,\nSeries B, pages 115–128, 1974. 6\n[29] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik.\nA database of human segmented natural images\n9\nand its application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In Proceedings Eighth IEEE\nInternational Conference on Computer Vision. ICCV 2001,\npages 416–423. IEEE, 2001. 6\n[30] Haoran Mo, Edgar Simo-Serra, Chengying Gao, Changqing\nZou, and Ruomei Wang. General virtual sketching frame-\nwork for vector line art.\nACM Transactions on Graphics\n(TOG), 40(4):1–14, 2021. 1\n[31] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162–8171. PMLR,\n2021. 2\n[32] Maxime Oquab, Timoth´ee Darcet, Th´eo Moutakanni, Huy\nVo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al.\nDinov2: Learning robust visual features without supervision.\narXiv preprint arXiv:2304.07193, 2023. 3\n[33] Tiziano Portenier, Qiyang Hu, Attila Szabo, Siavash Ar-\njomand Bigdeli, Paolo Favaro, and Matthias Zwicker.\nFaceshop: Deep sketch-based face image editing.\narXiv\npreprint arXiv:1804.08972, 2018. 1\n[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748–8763. PMLR, 2021. 2\n[35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821–8831. PMLR, 2021.\n2\n[36] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj¨orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684–10695, 2022. 1, 2, 4\n[37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-\nnet: Convolutional networks for biomedical image segmen-\ntation. In Medical Image Computing and Computer-Assisted\nIntervention–MICCAI 2015: 18th International Conference,\nMunich, Germany, October 5-9, 2015, Proceedings, Part III\n18, pages 234–241. Springer, 2015. 2\n[38] Peter J Rousseeuw. Silhouettes: a graphical aid to the inter-\npretation and validation of cluster analysis. Journal of com-\nputational and applied mathematics, 20:53–65, 1987. 3, 1\n[39] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,\nMichael Rubinstein, and Kfir Aberman. Dreambooth: Fine\ntuning text-to-image diffusion models for subject-driven\ngeneration.\nIn Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 22500–\n22510, 2023. 2, 6, 7, 1\n[40] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479–36494, 2022. 1, 2\n[41] Christoph Schuhmann, Richard Vencu, Romain Beaumont,\nRobert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:\nOpen dataset of clip-filtered 400 million image-text pairs,\n2021. 6\n[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu,\nCade Gordon,\nRoss Wightman,\nMehdi Cherti,\nTheo\nCoombes, Aarush Katta, Clayton Mullis, Mitchell Worts-\nman, et al. Laion-5b: An open large-scale dataset for training\nnext generation image-text models. Advances in Neural In-\nformation Processing Systems, 35:25278–25294, 2022. 1\n[43] Chang Wook Seo, Amirsaman Ashtari, and Junyong Noh.\nSemi-supervised reference-based sketch extraction using a\ncontrastive learning framework.\nACM Transactions on\nGraphics (TOG), 42(4):1–12, 2023. 1, 2, 5, 6, 7, 4\n[44] Junyoung Seo, Gyuseong Lee, Seokju Cho, Jiyoung Lee, and\nSeungryong Kim. Midms: Matching interleaved diffusion\nmodels for exemplar-based image translation. arXiv preprint\narXiv:2209.11047, 2022. 1\n[45] Samuel Sanford Shapiro and Martin B Wilk. An analysis of\nvariance test for normality (complete samples). Biometrika,\n52(3/4):591–611, 1965. 6\n[46] sharpei pups. 6.5 weeks old sharpei puppies. https://\nwww.youtube.com/watch?v=plIyQg6llp8, 2014.\nAccessed: 23-11-2023. 6\n[47] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 3, 1\n[48] Luming Tang, Menglin Jia, Qianqian Wang, Cheng Perng\nPhoo, and Bharath Hariharan.\nEmergent correspondence\nfrom image diffusion.\narXiv preprint arXiv:2306.03881,\n2023. 2\n[49] TheSaoPauloSeries. S˜ao paulo city mini-documentary: (full\nhd) the s˜ao paulo series. https://www.youtube.com/\nwatch?v=A3pBJTTjwCM, 2013. Accessed: 23-11-2023.\n6\n[50] Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali\nDekel.\nPlug-and-play diffusion features for text-driven\nimage-to-image translation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 1921–1930, 2023. 1, 2, 3\n[51] Yael Vinker, Ehsan Pajouheshgar, Jessica Y Bo, Ro-\nman Christian Bachmann, Amit Haim Bermano, Daniel\nCohen-Or, Amir Zamir, and Ariel Shamir.\nClipasso:\nSemantically-aware object sketching. ACM Transactions on\nGraphics (TOG), 41(4):1–11, 2022. 1, 2\n[52] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao,\nJan Kautz, and Bryan Catanzaro. High-resolution image syn-\nthesis and semantic manipulation with conditional gans. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 2018. 6\n[53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Si-\nmoncelli. Image quality assessment: from error visibility to\nstructural similarity. IEEE transactions on image processing,\n13(4):600–612, 2004. 6\n[54] Nora S Willett, Fernando de Goes, Kurt Fleischer, Mark\nMeyer, and Chris Burrows.\nStylizing ribbons: Comput-\ning surface contours with temporally coherent orientations.\n10\nIEEE Transactions on Visualization and Computer Graph-\nics, 2023. 1\n[55] Holger Winnem¨oller.\nXdog:\nadvanced image styliza-\ntion with extended difference-of-gaussians.\nIn Proceed-\nings of the ACM SIGGRAPH/eurographics symposium on\nnon-photorealistic animation and rendering, pages 147–156,\n2011. 1, 2\n[56] Holger Winnem¨oller, Jan Eric Kyprianidis, and Sven C\nOlsen.\nXdog: An extended difference-of-gaussians com-\npendium including advanced image stylization. Computers\n& Graphics, 36(6):740–753, 2012. 6\n[57] Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, and Xi-\naohui Shen. Anime2sketch: A sketch extractor for anime\narts with deep networks.\nhttps://github.com/\nMukosame/Anime2Sketch, 2021. 2\n[58] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection. In Proceedings of the IEEE international conference\non computer vision, pages 1395–1403, 2015. 2\n[59] Saining Xie and Zhuowen Tu. Holistically-nested edge de-\ntection, 2015. 6, 1\n[60] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiao-\nlong Wang, and Shalini De Mello. Open-vocabulary panop-\ntic segmentation with text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 2955–2966, 2023. 1, 2,\n3\n[61] Ran Yi, Yong-Jin Liu, Yu-Kun Lai, and Paul L Rosin.\nApdrawinggan: Generating artistic portrait drawings from\nface photos with hierarchical gans.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10743–10752, 2019. 2\n[62] Ran Yi, Yong-Jin Liu, Yu-Kun Lai, and Paul L Rosin.\nUnpaired portrait drawing generation via asymmetric cycle\nmapping. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 8217–8225,\n2020. 2\n[63] Mingcheng Yuan and Edgar Simo-Serra. Line art coloriza-\ntion with concatenated spatial attention. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 3946–3950, 2021. 1\n[64] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Pola-\nnia Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan\nYang. A tale of two features: Stable diffusion complements\ndino for zero-shot semantic correspondence. arXiv preprint\narXiv:2305.15347, 2023. 1, 2, 3\n[65] Kaihua Zhang, Lei Zhang, Kin-Man Lam, and David Zhang.\nA level set approach to image segmentation with intensity in-\nhomogeneity. IEEE transactions on cybernetics, 46(2):546–\n557, 2015. 2\n[66] Lvmin Zhang, Chengze Li, Tien-Tsin Wong, Yi Ji, and\nChunping Liu. Two-stage sketch colorization. ACM Trans-\nactions on Graphics (TOG), 37(6):1–14, 2018. 1\n[67] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 3836–3847, 2023. 1\n[68] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shecht-\nman, and Oliver Wang. The unreasonable effectiveness of\ndeep features as a perceptual metric. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 586–595, 2018. 2, 5, 6\n[69] Peihao Zhu, Rameen Abdal, John Femiani, and Peter Wonka.\nMind the gap: Domain gap control for single shot domain\nadaptation for generative adversarial networks. In Interna-\ntional Conference on Learning Representations, 2022. 5\n11\nRepresentative Feature Extraction During Diffusion Process\nfor Sketch Extraction with One Example\nSupplementary Material\nOverview\nThis supplementary material consists of 5 Sections. Section\nA describes implementation details (Sec. A). Sec. B pro-\nvides additional details and findings on diffusion features\nselection. Sec. C presents extended details of VAE decoder\nfeatures. Sec. D contains the results of additional experi-\nments on CDST. Lastly, Sec. E presents additional qualita-\ntive results with various style sketches.\nA. Implementation Details\nDiffSketch\nDiffSketch leverages the Stable Diffusion\nv1.4 sampled with DDIM [47] pretrained with the LAION-\n5B [42] dataset, which produced images of resolution 512\n× 512. With the pretrained Stable Diffusion, we use a to-\ntal of 50 time steps T for sampling. The training of DiffS-\nketch was performed for 1200 iterations which required less\nthan 3 hours on an Nvidia V100 GPU. For the training using\nHED [59], we concatenated the first two layers with the first\nthree layers to stylize sketch. In the case of XDoG [55], we\nused Gary Grossi style.\nDiffSketchdistilled\nDiffSketchdistilled was developed to\nconduct sketch extraction efficiently with the streamlined\ngenerator.\nThe training of DiffSketchdistilled was per-\nformed for 10 epochs for 30,000 sketch-image pairs gener-\nated from DiffSKetch, following the CDST. The training of\nDiffSketchdistilled required approximately 5 hours on two\nNvidia A6000 GPUs. The inference time of both DiffS-\nketch and DiffSketchdistilled was 4.74 seconds and 0.0139\nseconds, respectively, when tested on an Nvidia A5000\nGPU with image with same resolutions.\nComparison with Baselines\nFor the baselines, the set-\ntings used in our study were based on the official code pro-\nvided by the authors and information obtained from their\nrespective papers.\nFor both Ref2Sketch [2] and Semi-\nref2sketch [43], we used the given checkpoint, the offi-\ncial pre-trained model provided by the authors. For Dif-\nfuseIT [19], we also used the official code and check-\npoint given by the authors in which the diffusion model\nwas trained with the Imagenet [8] dataset, not FFHQ [15]\nbecause our comparison is not constrained to the face.\nFor Dreambooth [39] and Textual Inversion [10], we used\nDDIM inversion [47] to invert the source image to the la-\ntent code of Stable Diffusion.\nB. Diffusion Features Selection\nTo conduct K-means clustering for diffusion feature selec-\ntion, we first employed the elbow method, visualizing the\nresults. However, a distinct elbow was not visually appar-\nent, as shown in Figure 8. The left 6 images are WCSS\nvalues from randomly selected images out of our 1,000 test\nimages. All 6 plots show similar patterns, making it hard\nto select a definitive elbow as stated in the main paper. The\nright image, which exhibits similar results, shows the aver-\nage of WCSS on all 1,000 images.\nTherefore, we chose to use the Silhouette score [38] and\nDavies-Bouldin index [7], which are two of the most widely\nused numerical method when choosing the optimal num-\nber of clusters. However, they are two different methods,\nwhose results do not always match with each other. We first\nvisualized and found the contradicting results of these two\nmethods as shown in Figure 9. Therefore, we chose to use\nthe one that first matches the ith highest silhouette score and\nthe ith lowest Davies-Bouldin index simultaneously. This\nprocess of choosing the optimal number of clusters can be\nwritten as follows :\nAlgorithm 1 Finding the Optimal Number of Clusters\n1: MAX clusters = Total time steps/2\n2: sil indices ← sorted(range(MAX clusters), key =\nλk : silhouette scores[k], reverse = True)\n3: db indices ← sorted(range(MAX clusters), key =\nλk : db scores[k], reverse = False)\n4: for i ← 0 to MAX clusters do\n5:\nif sil indices[i] in db indices[: i + 1] then\n6:\nk optimal = sil indices[i]+1\n7:\nbreak\n8:\nend if\n9: end for\nWe conducted this process twice with two different num-\nbers of PCA components (10 and 30), yielding the results\nshown in Figure 10. The averages (13.26 and 13.34) and\nstandard deviations (0.69 and 0.69) were calculated. As the\nmode value with both PCA components was 13, and the\nrounded average was also 13, we chose our optimal k to be\n13. Using this number of clusters, we chose the representa-\ntive feature as the one nearest to the center of each cluster.\nFrom this process, we ended up with the following t val-\nues: [0,3,8,12,16,21,25,28,32,35,39,43,47]. To verify the\nprocess, if an optimal number of clusters in each image can\n1\nFigure 8. Visualization of WCSS values according to the number used for K-means clustering. The left plots are the WCSS of the features\nfrom an randomly sampled image while the right plot shows the average WCSS values of the features from 1,000 randomly sampled\nimages.\nimage(a)\nimage(b)\nimage(d)\nimage(c)\nimage(e)\nFigure 9. Visualization of contradicting results of Silhouette scores and Davis Bouldin indices on five different images.\nreally be globally adjusted, we compared our selected fea-\ntures against the baselines. These baselines included sam-\npling at equal time intervals (t=[i*4+1 for i in the range of\n(0,13)]) and randomly selecting 13 values. We calculated\nthe minimum Euclidean distance from each feature and con-\nfirmed that our method resulted in the minimum distance\nacross 1,000 randomly sampled images. This is illustrated\nin Table 4.\nTable 4. Sum of the minimum distances from all features\nMethod\nEuclidean Distance\nOurs\n18,615.6\nEqual time steps\n19,004.9\nRandom sample\n23,957.2\nIn the main paper, we found several key insights through\nthe visualization of features within the manually selected\nclasses, which we summarize extensively here. First, se-\nmantically similar images lead to similar trajectories, al-\nthough not identical. Second, features in the initial stage\nof the diffusion process (when t is approximately 50) retain\nFigure 10. Visualization on histogram for optimal k value with\ndifferent number of PCA components.\nsimilar information despite significant differences in the re-\nsulting images. Third, features in the middle stage of the\ndiffusion process (when t is around 25) exhibit larger differ-\nences between adjacent features in their time steps. Lastly,\nthe feature at the final time step (t=0) possesses distinc-\ntive information, varying significantly from previous values.\nThis is also evident in the additional visualization presented\nin Figure 11.\nOur automatically selected features indicate a prioritiza-\n2\nFigure 11. Additional analysis on sampled features. PCA is applied to DDIM sampled features from different classes. Up : features\ncolored with human-labeled classes. Down : features colored with denoising timesteps\ntion of the final feature (t=0), and that selection was made\nmore from the middle stages than from the initial steps\n(t=[21,25,28] versus t=[43,47]).\nOur finding offer some\nguidance for manual feature selecting to consider the time\nsteps, especially when memory is constrained. The order of\nthe preference will on the last feature (t=0), a middle one\n(t is near 25), and the middle to final time steps while the\nfeatures from initial steps are preferred less in general. For\ninstance, when selecting four features from 50 time steps, a\npossible selection could be t=[0,12,25,37].\nB.2 Features From Additional Models\nWhile we focused on T=50 DDIM sampling, for gener-\nalization, we examined different intervals (T=25, T=100)\nand different model. For these experiments, we randomly\nsampled 100 images. While our main experiments were\nconducted with manually classified images, we utilized\nDINOv2 [32], which was contrastively trained in a self-\nsupervised manner and has learned visual semantics. With\nDINOv2, we separated the data into 15 different clusters\nand followed the process described in the main paper to plot\nthe features. Here, we used 15 images from each cluster to\ncalculate the PCA axis while we used 17 classes in the main\nexperiments. The results, as shown in Figure 12 and Fig-\nure 13, indicate that even with different sampling methods,\nthe same conclusions regarding the sampling method can be\ndrawn. The last feature exhibits a distinct value, while the\nfeatures from the initial time step show similar values.\nIn addition, we also tested on different model, Stable\ndiffusion V2.1 which produce 768×768 images. Follow-\ning the same process, we randomly sampled 100 images\nand clustered with DINOv2 and plot as shown in Figure 14.\nThis result also shows that even with different model with\ndifferent resolution, the same conclusions can be drawn,\nshowing the scalability of our analysis.\nC. VAE Decoder Features\nVAE features fused with the Aggregation network features\nfor FFD in the proposed model architecture.\nFigure 15\nshows a visualization of the VAE features. We used a set of\n20 generated face images and extracted features from dif-\nferent decoder layers of the UNet and VAE decoder, at the\nlast time step (t=0) similar to that of PNP [50]. We observe\nthat the use of VAE decoder resulted in higher-frequency\ndetails than the UNet decoder. While the feature from UNet\ndecoder contains semantic information, the features from\nVAE decoder produces finer details such as hair, wrinkles,\nand small letters.\nD. Condition Diffusion Sampling for Training\nD.1 Rationale Behind CDST\nAn underlying assumption of CDST is that for a direc-\ntional CLIP loss, two images with a similar domain (Isource\nand Isamp in the main paper) leads to higher confidence\n3\nColor by clusters\nColor by time steps\nFigure 12. Additional analysis on sampled features. PCA is applied to 25 steps of DDIM sampled features with different clusters. Up :\nfeatures colored with DINOv2 clusters. Down : features colored with denoising timesteps.\nColor by clusters\nColor by time steps\nFigure 13. Additional analysis on sampled features. PCA is applied to 100 steps of DDIM sampled features with different clusters. Up :\nfeatures colored with DINOv2 clusters. Down : features colored with denoising timesteps.\ncompared to two images with a different domain. To ex-\namine this, we performed a confidence score test using\n4SKST [43] which consists of four different sketch styles\npaired with color images. 4KST is suitable for the confi-\ndence score test because it contains images from two dif-\nferent domains, photos, and anime images, in four different\nstyles.\nWe manually separated into photos and anime images\nsince it was not labeled. Here, we computed a confidence\nscore to determine if the directional clip loss is more re-\nliable when the calculated source images are in the same\ndomain. We performed a test with three settings, measur-\ning cosine similarity between the images IA (Photo) and\nIB (Anime) from different domains with the corresponding\nsketches SA and SB. All these images were encoded into\nthe CLIP embedding. We employed two similarity scores\nSimwithin and Simacross in the same manner as the main\npaper (Sec.4.2). We calculated the similarity of the features\n4\nColor by clusters\nColor by time steps\nFigure 14. Additional analysis on Stable diffusion v2.1 sampled features. PCA is applied to 50 steps of DDIM sampled features with\ndifferent clusters. Up : features colored with DINOv2 clusters. Down : features colored with denoising timesteps.\nFigure 15. Extended visualization of features from UNet and VAE. (a) shows the UNet decoder features in lower resolution (layers 1),\nintermediate resolution (layers 5), and higher resolution (layers 11). (b) shows the VAE decoder features in lower resolution (layers 1),\nintermediate resolution (layers 6), and higher resolution (layers 9).\nin the photo domain, in the anime domain, and across the\ntwo domains. The equation can be expressed as follows:\nSim(X, Y ) = cos(−−−→\nIXIY · −−−−→\nSXSY ) + cos(−−−→\nIXSX · −−−→\nIY SY )\nN\n(6)\nwhere cos(a · b) is the cosine similarity and N is the total\nnumber of cos calculation. X,Y corresponds to the images\nin each domain.\nWith these computed similarities, the confidence score\nin domain A and domain B can be written as follows where\nSim(ALL, ALL) denotes the average similarity of all im-\n5\nTable 5. Confidence scores on 4SKST with four different styles.\nSimilarity\nStyle1\nStyle2\nStyle3\nStyle4\nAverage\nconfidence(Anime,Anime)\n104.2608\n102.8716\n108.2026\n101.3530\n104.1720\nconfidence(Photo,Photo)\n101.9346\n98.8005\n102.4516\n100.5453\n100.9330\nconfidence(Photo,Anime)\n94.5036\n94.0189\n98.1867\n92.3874\n94.7742\nages:\nconfidence(A,B) =\nSim(A, B)\nSim(ALL, ALL) × 100\n(7)\nIn Table 5, we show the confidence test results on four\ndifferent style sketches. For all four styles, calculating the\ndirectional CLIP loss in the same domain produced higher\nconfidence compared to the confidence computed across\na different domain. Accordingly, we propose a sampling\nscheme, CDST to train the generator in the same domain at\nthe initial stage of the training, which leads to higher confi-\ndence while widening its capacity in the latter iterations of\ntraining.\nD.2 Additional Experiment on CDST\nIn the main paper, we used DSD for CDST. However, the\ndistribution of the condition of a pretrained stable diffusion\nnetwork is not known. Therefore, we approximate DSD by\nrandomly sampling 1,000 text prompts from the LAION-\n400M [41], which is a subset of the trained text-image pairs\nof the SD model. We then tokenized and embedded these\nprompts for preprocessing, following the process of the pre-\ntrained SD model. We conducted PCA on these 1,000 sam-\npled embeddings to extract 512 principal components. We\nthen checked the normality of the sampled embeddings with\nall 512 principal component axes using the Shapiro-Wilk\ntest [45] with a significance level of α = 5%.\nAs a result, 214 components rejected the null hypothe-\nsis of normality. This indicates that each of its marginals\ncannot be assumed to be univariate normal. Next, we con-\nducted the Mardia test [27, 28] with the same 1,000 sam-\nples, taking into account skewness and kurtosis to check if\nthe distribution is multivariate. The results failed to reject\nthe null hypothesis of normality with a significance level of\nα = 5%. Therefore, we assumed DSD as a multivariate\nnormal distribution for our sampling during training.\nWe examined whether our calculated distribution of sta-\nble diffusion (DSD) is similar to the ground truth em-\nbedding distribution of LAION-400M. For verification, we\nsampled 100k data from the embedded LAION-400M as\na subset of ground truth. We also sampled same amount\nof embeddings from the multivariate normal distribution\n(Ours), univariate normal distribution for each axis, and a\nuniform distribution between the max and min values of the\nsampled embedded LAION-400M as a baseline. We used\nEarth moving distance (EMD) [20] and found out that the\nmultivariate normal distribution lead the lowest distance, as\nshown in Table 6.\nMij = ∥disti − distGT j∥2,\nai =\n1\nlen(dist),\nbj =\n1\nlen(distGT ),\nW(dist, GTdist) = EMD(a, b, M).\n(8)\nThis result does not prove that DSD has multivariate nor-\nmality, and the difference with the normal distribution is\nmarginal. However, it is sufficient for our usage of the con-\ndition diffusion sampling for training.\nTable 6. Distance from GT embeddings.\nMethod\nEMD\nMultivariate normal (Ours)\n244.22\nnormal distribution for each axis\n244.31\nuniform distribution\n1480.57\nE. Qualitative Results\nWe present additional results from the baseline compar-\nisons in Figure 16 and 17. Each figure shows the results\nthat compared DiffSketchdistilled and the baseline methods\non the COCO dataset [23] and the BSDS500 dataset [29],\nrespectively. Addition to this, we also provide visual ex-\namples of video sketch extraction results on diverse do-\nmain including buildings, nature, and animals [46, 49] using\nDiffSketchdistilled in Figure 18 and supplementary video.\n6\nFigure 16. Qualitative comparison with alternative sketch extraction methods on COCO dataset.\nFigure 17. Qualitative comparison with alternative sketch extraction methods on BSDS500 dataset.\n7\nFigure 18. Qualitative examples of video sketch extraction.\n8\n"
}