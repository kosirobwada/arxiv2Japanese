{
    "optim": "Published as a conference paper at ICLR 2024\nDDMI:\nDOMAIN-AGNOSTIC\nLATENT\nDIFFUSION\nMODELS\nFOR SYNTHESIZING HIGH-QUALITY IM-\nPLICIT NEURAL REPRESENTATIONS\nDogyun Park, Sihyeon Kim, Sojin Lee, Hyunwoo J. Kim∗\nDepartment of Computer Science\nKorea University\nSeoul, South Korea\n{gg933,sh bs15,sojin lee,hyunwoojkim}@korea.ac.kr\nABSTRACT\nRecent studies have introduced a new class of generative models for synthesiz-\ning implicit neural representations (INRs) that capture arbitrary continuous sig-\nnals in various domains.\nThese models opened the door for domain-agnostic\ngenerative models, but they often fail to achieve high-quality generation. We\nobserved that the existing methods generate the weights of neural networks to\nparameterize INRs and evaluate the network with fixed positional embeddings\n(PEs). Arguably, this architecture limits the expressive power of generative mod-\nels and results in low-quality INR generation. To address this limitation, we pro-\npose Domain-agnostic Latent Diffusion Model for INRs (DDMI) that generates\nadaptive positional embeddings instead of neural networks’ weights. Specifically,\nwe develop a Discrete-to-continuous space Variational AutoEncoder (D2C-VAE),\nwhich seamlessly connects discrete data and the continuous signal functions in the\nshared latent space. Additionally, we introduce a novel conditioning mechanism\nfor evaluating INRs with the hierarchically decomposed PEs to further enhance\nexpressive power. Extensive experiments across four modalities, e.g., 2D images,\n3D shapes, Neural Radiance Fields, and videos, with seven benchmark datasets,\ndemonstrate the versatility of DDMI and its superior performance compared to\nthe existing INR generative models. Code will be released soon.\n1\nINTRODUCTION\nImplicit neural representation (INR) is a popular approach for representing arbitrary signals as a con-\ntinuous function parameterized by a neural network. INRs provide great flexibility and expressivity\neven with a simple neural network like a small multi-layer perceptron (MLP). INRs are virtually\ndomain-agnostic representations that can be applied to a wide range of signals domains, such as im-\nage (M¨uller et al., 2022; Tancik et al., 2020; Sitzmann et al., 2020), shape/scene model (Mescheder\net al., 2019; Peng et al., 2020; Park et al., 2019), video reconstruction (Nam et al., 2022b; Chen\net al., 2022), and novel view synthesis (Mildenhall et al., 2021; Martin-Brualla et al., 2021; Park\net al., 2021; Barron et al., 2021). Also, INR enables the continuous representation of signals at arbi-\ntrary scales and complex geometries. For instance, given an INR of an image, applying zoom-in/out\nor sampling an arbitrary-resolution image is readily achievable, leading to superior performance in\nsuper-resolution (Chen et al., 2021; Xu et al., 2021). Lastly, INR represents signals with high quality\nleveraging the recent advancements in parametric positional embedding (PE) (M¨uller et al., 2022;\nCao & Johnson, 2023).\nRecent research has expanded its attention to INR generative models using Normalizing\nFlows (Dupont et al., 2022a), GANs (Chen & Zhang, 2019; Skorokhodov et al., 2021; Anokhin\net al., 2021), and Diffusion Models (Dupont et al., 2022a; Zhuang et al., 2023). Especially, Dupont\net al. (2022a); Zhuang et al. (2023); Du et al. (2021); Dupont et al. (2022b) have focused on devel-\noping a generic framework that can be applied across different signal domains. This is primarily\n∗Corresponding author.\n1\narXiv:2401.12517v1  [cs.LG]  23 Jan 2024\nPublished as a conference paper at ICLR 2024\nImage generation\nArbitrary-resolution image generation\n3D shape generation\nText-to-shape generation\nNeural radiance field generation\nText prompt: “A curved chair”\nText prompt: “A metallic folding chair”\nVideo generation\n64x64\n128x128\n256x256\n384x384\n512x512\n…\n…\nFigure 1: Generation results of DDMI. Our DDMI generates high-quality samples across four\ndistinct domains including image, shape, video, and Neural Radiance Fields. DDMI also shows\nremarkable results for applications like arbitrary-scale image generation or text-to-shape generation.\naccomplished by modeling the distribution of INR ‘weights’ by GAN (Dupont et al., 2022b), latent\ndiffusion model (Dupont et al., 2022a), or latent interpolation (Du et al., 2021). However, these mod-\nels often exhibit limitations in achieving high-quality results when dealing with large and complex\ndatasets. Arguably, this is mainly due to their reliance on generating weights for an INR function\nwith fixed PEs. This places a substantial burden on function weights to capture diverse details in\nmultiple signals, whereas the careful designs of PE (M¨uller et al., 2022; Chan et al., 2022; Cao &\nJohnson, 2023) have demonstrated greater efficiency and effectiveness in representing signals.\nTherefore, in this paper, we propose Domain-agnostic Latent Diffusion Model for INRs (DDMI)\nthat generates adaptive positional embeddings instead of neural networks’ weights (see Fig. 3 for\nconceptual comparison). Specifically, we introduce a Discrete to Continuous space Variational Au-\ntoEncoder (D2C-VAE) framework with an encoder that maps discrete data into the latent space and a\ndecoder that map the latent space to continuous function space. D2C-VAE generates basis fields us-\ning the decoder network conditioned on a latent variable. This means that we define sample-specific\nbasis functions for generating adaptive PEs, shifting the primary expressive power from MLP to PE.\nAdditionally, we propose two modules that further enhance the expressive capacity of INR: 1) Hi-\nerarchically Decomposed Basis Fields (HDBFs): we decompose the basis fields into multiple scales\nto better account for the multi-scale nature of signals. 2) Coarse-to-Fine Conditioning (CFC): we\nintroduce a novel conditioning method, where the multi-scale PEs from HDBFs are progressively\nconditioned on MLP in a coarse-to-fine manner. Based on D2C-VAE, we train the latent diffusion\nmodel on the shared latent space (see Fig. 2 for the overall framework). Ultimately, our model can\ngenerate high-quality continuous functions across a wide range of signal domains (see Fig. 1). To\nsummarize, our contributions are as follows:\n• We introduce Domain-agnostic Latent Diffusion Model for INRs (DDMI), a generative\nmodel synthesizing high-quality INRs across various signal domains.\n• We define a Discrete to Continuous space Variational AutoEncoder (D2C-VAE) that gener-\nates adaptive PEs and learns the shared latent space to connect the discrete data space and\nthe continuous function space.\n• We propose Hierarchically-Decomposed Basis Fields (HDBFs) and Coarse-to-Fine Condi-\ntioning (CFC) to enhance the expressive power.\n• Extensive experiments across four modalities and seven benchmark datasets demonstrate\nthe versatility of DDMI. The proposed method significantly outperforms the existing INR\ngenerative models, demonstrating the efficacy of our proposed methods.\n2\nPublished as a conference paper at ICLR 2024\nFigure 2: Overall pipeline of DDMI. Discrete data x and continuous function ω are connected\nin the shared latent space z (D2C-VAE). The decoder generates Hierarchically-Decomposed Basis\nFields (HDBFs) given latent variable z. p1 represents the coarsest scale PE and p3 corresponds to\nthe finest scale PE. The MLP returns the signal value for queried coordinate c using the Coarse-to-\nFine Conditioning method. Latent diffusion model operates on the shared latent space. Note that we\nuse a tri-plane latent variable for 3D and video, and a single plane for 2D image.\n2\nRELATED WORKS\nINR-based generative models.\nSeveral works have explored the use of INR in generative mod-\neling to leverage its continuous nature and expressivity. Especially, the INR generative models are\nknown for their ability to generate data at arbitrary scales with a single model. Thus, there has been\na surge of recent works across multiple modalities. For 2D images, CIPS (Anokhin et al., 2021) and\nINR-GAN (Skorokhodov et al., 2021) employ GANs to synthesize continuous image functions. For\n3D shape, recent studies (Nam et al., 2022a; Zheng et al., 2022; Li et al., 2023; Erkoc¸ et al., 2023)\nhave proposed generating shapes as Signed Distance Functions (SDFs) using GANs or diffusion\nmodels. Also, for videos, DIGAN (Yu et al., 2022) and StyleGAN-V (Skorokhodov et al., 2022)\nhave introduced GAN-based architectures to generate videos as continuous spatio-temporal func-\ntions. However, since these models are designed for specific modalities, they cannot easily adapt to\ndifferent types of signals. Another line of research has explored the domain-agnostic architectures\nfor INR generations, such as GASP (Dupont et al., 2022b), Functa (Dupont et al., 2022a), GEM (Du\net al., 2021), and DPF (Zhuang et al., 2023). Zhuang et al. (2023) directly applies diffusion models\nto explicit signal fields to generate samples at the targeted modality, yet it faces a scalability issue\nwhen dealing with large-scale datasets. Others (Dupont et al., 2022b;a; Du et al., 2021; Koyuncu\net al., 2023) attempt to model the weight distribution of INRs with GANs, diffusion models, or latent\ninterpolation. In contrast, we introduce a domain-agnostic generative model that generates adaptive\npositional embeddings instead of weights of MLP in INRs.\nLatent diffusion model. Diffusion models (Ho et al., 2020; 2022; Song et al., 2021) have demon-\nstrated remarkable success in generation tasks. They consistently achieve high-quality results and\nhigh distribution coverage (Dhariwal & Nichol, 2021), often outperforming GANs. However, their\niterative reverse process, typically involving a large number of steps (e.g., 1000 steps), renders them\nsignificantly slower and inefficient compared to the implicit generative models like VAEs (Kingma\n& Welling, 2013) and GANs. To alleviate this limitation, recent works (Rombach et al., 2022;\nVahdat et al., 2021) have proposed learning the data distribution in a low-dimensional latent space,\nwhich offers computational efficiency. Latent diffusion models (LDMs) strike a favorable balance\nbetween quality and efficiency, making them an attractive option for various applications. Our work\nadopts a latent space approach for designing the computational-efficient generative model.\n3\nPublished as a conference paper at ICLR 2024\n3\nMETHODOLOGY\nWe present a Domain-agnostic latent Diffusion Model for synthesizing high-quality Implicit neural\nrepresentations (DDMI). In order to learn to generate continuous functions (INRs) from discrete\ndata, e.g., images, we propose a novel VAE architecture D2C-VAE that maps discrete data to con-\ntinuous functions via a shared latent space in Sec. 3.1. To enhance the quality of INR generation, we\nintroduce a coarse-to-fine conditioning (CFC) mechanism for evaluating INRs with the generated\nhierarchically-decomposed basis fields (HDBFs). Sec. 3.2 outlines the two-stage training proce-\ndures of the proposed method. As with existing latent diffusion models (Rombach et al., 2022), in\nthe first stage, D2C-VAE learns the shared latent space. In the second stage, the proposed frame-\nwork trains a diffusion model in the shared latent space while keeping the other networks fixed. The\noverall pipeline is illustrated in Fig. 2.\n3.1\nDDMI\nFigure 3: Comparison between weight\ngeneration and PE generation for INR\ngenerative models G. c is a coordinate,\np is a PE, γ is a function that maps coor-\ndinates to PEs, πθ is MLP, and ˆω(c) is a\npredicted signal value. For PE generation,\nwe sample basis fields Ξ from G instead of\nθ. The red line indicates the generation.\nLet ω, ˆω ∈ Ω denote a continuous function repre-\nsenting an arbitrary signal and its approximation by\nneural networks respectively, where Ω is a continuous\nfunction space. Given a spatial or spatio-temporal co-\nordinate c ∈ Rm, and its corresponding signal value\nx ∈ Rn, training data x can be seen as the evaluations\nof a continuous function at a set of coordinates, i.e.,\nx = [ω(c)]I\ni=1 = ω(c), where x ∈ RI×n, c ∈ RI×m,\nand I is the number of coordinates.\nD2C-VAE. We propose an asymmetric VAE architec-\nture, dubbed as Discrete-to-continuous space Varia-\ntional Auto-Encoder (D2C-VAE), to seamlessly con-\nnect a discrete data space and a continuous function\nspace via the shared latent space. Specifically, the en-\ncoder Eϕ maps discrete data x to the latent variable z\nas 2D grid features, e.g., a 2D plane for images or a 2D\ntri-plane for 3D shapes and videos. The decoder Dψ\ngenerates basis fields Ξ, i.e., Ξ = Dψ(z), where we\ndefine Ξ as a set of dense grids consisting of generated\nbasis vectors. Then, the positional embedding p for\nthe coordinate c is computed by p = γ(c; Ξ), where a\nfunction γ performs bilinear interpolation on Ξ, calcu-\nlating the distance-weighted average of its four near-\nest basis vectors at coordinate c. For tri-plane basis\nfields, an axis-aligned orthogonal projection is applied\nbeforehand (see Fig. 2). In this manner, the proposed method adaptively generates PEs according to\ndifferent basis fields. Finally, the MLP πθ returns the signal value given the positional embedding,\ni.e., ˆx = πθ(p) = ˆω(c). Fig. 3 shows the distinction between the proposed method and existing\nINR generative models. We observed that the PE generation improves the expressive power of INRs\ncompared to weight generation approaches, see Sec. 4.\nHierarchically-decomposed basis fields. Instead of relying on a single-scale basis field, we pro-\npose decomposing it into multiple scales to better account for the multi-scale nature of signals.\nIn order to efficiently generate multi-scale basis fields, we leverage the feature hierarchy of a sin-\ngle neural network. Specifically, the decoder Dψ outputs feature maps at different scale i, i.e.,\nDψ(z) = {Ξi| i = 1, ..., n}. The feature maps undergo 1 × 1 convolution to match the feature\ndimensions across scales. We refer to these decomposed basis fields as Hierarchically-Decomposed\nBasis Fields (HDBFs). Then, we can compute multi-scale PEs {pi} as pi = γ(c; Ξi), for all i. In\npractice, we use three different scales of basis fields from three different levels of layers. Sec. 4.4\nqualitatively validates the spatial frequencies in HDBFs of generated samples are decomposed into\nmultiple levels. This demonstrates that each field is dedicated to learning a specific level of detail,\nleading to a more expressive representation.\n4\nPublished as a conference paper at ICLR 2024\nCoarse-to-fine conditioning (CFC). A na¨ıve approach to using multi-scale positional embeddings\nfrom HDBFs for MLP πθ is concatenating them along channel dimensions. However, we found\nthat it is suboptimal. Thus, we introduce a conditioning mechanism that gradually conditions MLP\nπθ on coarse PEs to fine PEs. The intuition is to encourage the lower-scale basis field to focus on\nthe details missing from the higher-scale basis field. To achieve this, we first feed the PE from the\nlowest-scale basis field as input to the MLP block and then concatenate (or element-wise sum) its\nintermediate output with the next PE from the higher-scale basis field. We continue this process for\nsubsequent scales until reaching the n-th scale (see Fig. 2).\n3.2\nTRAINING PROCEDURE AND INFERENCE\nThe training of DDMI involves two stages: VAE training and diffusion model training. In the first\nstage, D2C-VAE learns the shared latent space with an encoder Eϕ to map discrete data to latent\nvector z and a decoder Dψ to generate basis fields Ξ. In the second stage, a diffusion model is\ntrained in the latent space to learn the empirical distribution of latent vectors z.\nD2C-VAE training.\nWe define a training objective for D2C-VAE that maximizes the evidence\nlower bound (ELBO) of log-likelihood of the continuous function ω with discrete data x as:\nlog p(ω) = log\nZ\npψ,πθ(ω|z) · p(z)dz\n(1)\n= log\nZ pψ,πθ(ω|z)\nqϕ(z|x)\n· qϕ(z|x) · p(z) dz\n(2)\n≥\nZ\nlog\n\u0012pψ,πθ(ω|z)\nqϕ(z|x)\n· p(z)\n\u0013\n· qϕ(z|x) dz\n(3)\n=\nZ\nqϕ(z|x) ·\n\u0012\nlog pψ,πθ(ω|z) − log\n\u0012qϕ(z|x)\np(z)\n\u0013\u0013\n(4)\n= Eqϕ(z|x) [log pψ,πθ(ω|z)] − DKL(qϕ(z|x)||p(z)),\n(5)\nwhere the inequality in Eq. 3 is by Jensen’s inequality. qϕ(z|x) is the approximate posterior, p(z)\nis a prior, and pψ,πθ(x|z) is the likelihood. The first term in Eq. 5 measures the reconstruction loss,\nand the KL divergence between the posterior and prior distributions p(z) encourages latent vectors\nto follow the prior. However, since we do not have observation ω but only discrete data x = ω(c),\nwe approximate pψ,πθ(ω|z) by assuming coodinate-wise independence as\npψ,πθ(ω|z) ≈ pψ,πθ(ω(c)|z) =\nY\nc∈c\npψ,πθ(ω(c)|z),\n(6)\nwhere ˆω(c) = πθ(γ(c, Dψ(z))). Thus, our training objective in Eq. 5 can be approximated as\nLϕ,ψ,θ(x) := Eqϕ(z|x) [log pψ,πθ(ω|z)] − DKL(qϕ(z|x)||p(z))\n(7)\n≈ Eqϕ(z|x)\n\"X\nc∈c\nlog pψ,πθ(ω(c)|z)\n#\n− DKL(qϕ(z|x)||p(z)).\n(8)\nSince the reconstruction loss varies depending on the number of coordinates, i.e., |c|, in practice, we\ntrain D2C-VAE with the re-weighted objective function given as:\nLϕ,ψ,πθ(x) = Eqϕ(z|x),c∈c [log pψ,πθ(ω(c)|z)] − λz · DKL(qϕ(z|x)||p(z)),\n(9)\nwhere λz balances the two losses, and p(z) is a standard normal distribution.\nDiffusion model training.\nFollowing existing LDMs (Vahdat et al., 2021; Rombach et al., 2022;\nHo et al., 2020), the forward diffusion process is defined in the learned latent space as a Markov chain\nwith pre-defined Gaussian kernels q(zt|zt−1) := N(zt; √1 − βtzt−1, βtI). The forward diffusion\nprocess is given as q(z1:T |z0) = QT\nt=1 q(zt|zt−1), where T indicates the total number of diffusion\nsteps and βt is a pre-defined noise schedule that satisfies q(zT ) ≈ N(zT ; 0, I). The reverse diffusion\nprocess is also defined in the latent space as pφ(z0:T ) = p(zT ) QT\nt=1 pφ(zt−1|zt), where pφ(z0)\nis a LDM prior. The Gaussian kernels pφ(zt−1|zt) := N(zt; µφ(zt, t), ρ2\ntI) is parameterized by a\n5\nPublished as a conference paper at ICLR 2024\nTable 1: FID results on CelebA-HQ.\n642\n1282\n2562\n3842\n<Discrete representation>\nLDM (Rombach et al., 2022)\n-\n-\n5.51\n-\nLSGM (Vahdat et al., 2021)\n-\n-\n7.22\n-\n<Continuous representation>\nDomain-specific\nINR-GAN (Skorokhodov et al., 2021)\n-\n-\n10.3\n-\nCIPS (Anokhin et al., 2021)\n15.41\n13.53\n11.4\n15.8\nDomain-agnostic\nFuncta (Dupont et al., 2022a)\n40.4\n-\n-\n-\nGEM (Du et al., 2021)\n30.4\n-\n-\n-\nGASP (Dupont et al., 2022b)\n13.5\n19.2\n-\n-\nDPF (Zhuang et al., 2023)\n13.2\n-\n-\n-\nDDMI (Ours)\n9.74\n8.73\n7.25\n10.44\nTable 2: Precision and Recall results.\nCelebA-HQ\nAFHQv2 Cat\n<Continuous representation>\nP ↑\nR ↑\nP ↑\nR ↑\nINR-GAN (Skorokhodov et al., 2021)\n0.671\n0.333\n0.719\n0.281\nCIPS (Anokhin et al., 2021)\n0.682\n0.287\n0.716\n0.117\nDDMI (Ours)\n0.734\n0.408\n0.808\n0.367\nTable 3: FID results on AFHQv2 Dog.\n1282\n2562\n3842\n<Discrete representation>\nStyleGAN (Karras et al., 2020a)\n-\n6.73\n-\n<Continuous representation>\nDomain-specific\nINR-GAN (Skorokhodov et al., 2021)\n-\n31.27\n-\nCIPS (Anokhin et al., 2021)\n26.95\n23.93\n28.97\nDomain-agnostic\nGASP (Dupont et al., 2022b)\n-\n35.78\n-\nDDMI (Ours)\n10.81\n8.54\n11.47\nTable 4: FID results on AFHQv2 Cat.\n1282\n2562\n3842\n<Discrete representation>\nStyleGAN (Karras et al., 2020a)\n-\n3.25\n-\n<Continuous representation>\nDomain-specific\nINR-GAN (Skorokhodov et al., 2021)\n-\n11.2\n-\nCIPS (Anokhin et al., 2021)\n7.85\n7.35\n11.8\nDomain-agnostic\nGASP (Dupont et al., 2022b)\n-\n17.48\n-\nDDMI (Ours)\n5.88\n4.27\n7.94\nneural network µφ(zt, t) and ρ2\nt is the fixed variances. The reverse diffusion process pφ(zt−1|zt) is\ntrained with the following noise prediction using reparameterization trick:\nLφ(z) = Ez0,ϵ,t\n\u0002\nw(t)||ϵ − ϵφ(zt, t)||2\n2\n\u0003\n.\n(10)\nInference.\nContinuous function generation involves the reverse diffusion process pφ, D2C-VAE\ndecoder Dψ, and read-out MLP πθ. First, a latent z0 ∼ pφ(z0) is generated by iteratively conducting\nancestral sampling, zt−1 =\n1\n√αt\n\u0010\nzt −\nβt\n√1− ¯\nαt ϵφ(zt, t)\n\u0011\n+ σtζ, where ζ ∼ N(0, I) and p(zT ) =\nN(zT ; 0, I). Then, the latent z0 is fed to D2C-VAE decoder Dψ to generate HDBFs Ξ. Finally,\ncombining HDBFs with MLP πθ learned in the first stage, the proposed method parameterizes a\ncontinuous function ω(·).\n4\nEXPERIMENTS\nWe evaluate the effectiveness and versatility of DDMI through comprehensive experiments across\ndiverse modalities, including 2D images, 3D shapes, and videos. We assume a multivariate normal\ndistribution for the likelihood function qϕ(ω(c)|z) for images and videos, and Bernoulli distribution\nfor shapes (occupancy function). For all domains, the multivariate normal distribution is used for\nthe posterior pψ,πθ(z|x). For the type of latent variables for each domain, the encoder maps input\ndata to the latent variable z as 2D grid features, e.g., a single 2D plane for images using a 2D CNN-\nbased encoder (Ho et al., 2020) or a 2D tri-plane for 3D shapes following Conv-ONET (Peng et al.,\n2020) and videos as outlined in Timesformer (Bertasius et al., 2021). Then, we use a 2D CNN-based\ndecoder Dψ to convert latent variable z into basis fields Ξ. Additional experiments (e.g., NeRF) and\nmore implementation details, evaluation, and baselines are provided in the Appendix.\n4.1\n2D IMAGES\nDatasets and baselines.\nFor images, we evaluate models on AFHQv2 Cat and Dog (Choi\net al., 2020) and CelebA-HQ dataset (Karras et al., 2018) with a resolution of 2562. We com-\npared our method with three groups of models: 1) Domain-agnostic INR generative models such\nas Functa (Dupont et al., 2022a), GEM (Du et al., 2021), GASP (Dupont et al., 2022b), and\nDPF (Zhuang et al., 2023), which are the primary baselines, 2) Domain-specific INR generative\nmodels that are specifically tailored for image generation like INR-GAN (Skorokhodov et al., 2021)\nand CIPS (Anokhin et al., 2021). Apart from DPF, which generates the explicit signal field, every\nbaseline operates weight generation, whereas ours opts for PE generation. 3) Discrete represen-\ntation based generative models for reference. We provide results from state-of-the-art generative\nmodels (Vahdat et al., 2021; Rombach et al., 2022; Karras et al., 2020a) that learn to generate dis-\ncrete images.\n6\nPublished as a conference paper at ICLR 2024\n64!\n128!\n256!\n(a) ScaleParty (Discrete representation)\n64!\n128!\n256!\n(b) Ours\nFigure 4:\nComparison between DDMI and\nScaleParty on arbitrary-scale generation.\n256!\n512!\n1024!\n(a) CIPS (domain-specific INR)\n256!\n512!\n1024!\n(b) Ours\nFigure 5: Comparison between DDMI and CIPS\non arbitrary-scale generation.\nQuantitative analysis. We primarily measure FID (Heusel et al., 2017), following the setup in (Sko-\nrokhodov et al., 2021). Tab. 1, 3, and 4 showcase the consistent improvement of DDMI over pri-\nmary baselines for multiple resolutions. For instance, Tab. 1 shows that compared to DPF, the recent\ndomain-agnostic INR generative model, our approach achieves an FID score of 9.74 as opposed to\n13.2 on the CelebA-HQ at a resolution of 642. Moreover, on AFHQv2 Cat in Tab. 4, DDMI demon-\nstrates superior performance over CIPS, specifically developed for arbitrary scale image generation,\nachieving an average FID improvement of 2.97 across three different resolutions. In Tab. 2, we com-\npare precision and recall with image-targeted INR generative models. Here, precision and recall are\nindicative of fidelity and diversity, respectively. DDMI exhibits a significant advantage over both\nbaselines (Skorokhodov et al., 2021; Anokhin et al., 2021), demonstrating superior performance for\nboth CelebA-HQ and AFHQv2 Cat datasets. We provide additional results on CIFAR10 (Krizhevsky\net al., 2009) and Lsun Churches (Yu et al., 2015) in Tab. 11 of the Appendix.\nQualitative analysis. For further validation, we generate images at arbitrary scales and conduct\ncomparisons with two notable models: ScaleParty (Ntavelis et al., 2022), a recent generative model\ndesigned for multi-resolution discrete images, and CIPS, an image-targeted INR generative model.\nDDMI demonstrates an impressive capability to consistently generate clearer images across various\nresolutions, from low to high. In Fig. 4, our model excels at generating images with preserved facial\nstructure, whereas Ntavelis et al. (2022) struggles to maintain the global structure of the image\nfor lower-resolution cases. Also, in Fig. 5, DDMI succeeds in capturing high-frequency details\nacross images of varying resolutions, while Anokhin et al. (2021) tends to lack finer details as the\nresolution increases.\n4.2\n3D SHAPES\nDatasets and baselines.\nFor shapes, we adopt the ShapeNet dataset (Chang et al., 2015) with two\nsettings: a single-class dataset with 4K chair shapes and a multi-class dataset comprising 13 classes\nwith 35K shapes, following the experimental setup in (Peng et al., 2020). We learn the shape as\nthe occupancy function (Mescheder et al., 2019) ω : R3 → {0, 1}, where it maps 3D coordinates\nto occupancy values, e.g., 0 or 1. Still, domain-agnostic INR generative models from Sec. 4.1 are\nour primary baselines, where we also compare with domain-specific INR generative models for 3D\nshapes like 3D-LDM (Nam et al., 2022a) and SDF-Diffusion (Shim et al., 2023). We also include\nresults from generative models for generating discrete shapes, such as point clouds, for reference.\nQuantitative analysis. We conduct extensive experiments with unconditional and conditional shape\ngeneration. Beginning with unconditional shape generation (Tab. 5), we measure fidelity and diver-\nsity using Minimum Matching Distance (MMD) and Coverage (Achlioptas et al., 2018). DDMI\nsurpasses both domain-agnostic and domain-specific baselines, achieving the best MMD for the sin-\n7\nPublished as a conference paper at ICLR 2024\nTable 5: Generation results on 3D shapes.\nChair\nMulti Class\nMMD ↓\nCOV ↑\nMMD ↓\nCOV ↑\n<Discrete representation>\nPVD (Zhou et al., 2021)\n6.8\n0.421\n-\n-\nDPM3D (Luo & Hu, 2021)\n1.3∗\n0.567∗\n-\n-\n<Continuous representation>\nDomain-specific\nLatentGAN (Chen & Zhang, 2019)\n-\n-\n1.7\n0.389\n3D-LDM (Nam et al., 2022a)\n1.68\n0.426\n-\n-\nSDF-StyleGAN (Zheng et al., 2022)\n1.9\n0.411\n1.55\n0.398\nSDF-Diffusion (Shim et al., 2023)\n8.0\n0.498\n-\n-\nHyperDiffusion (Erkoc¸ et al., 2023)\n7.1\n0.530\n-\n-\nDomain-agnostic\nGASP (Dupont et al., 2022b)\n2.5\n0.353\n2.1\n0.341\nGEM (Du et al., 2021)\n-\n-\n1.4\n0.409\nDPF (Zhuang et al., 2023)\n-\n-\n1.6\n0.419\nDDMI (Ours)\n1.5\n0.510\n1.3\n0.421\n∗ are trained on Acronym (Eppner et al., 2021) dataset.\nTable 6: Generation results on videos.\nSkyTimelapse\nFVD ↓\n<Discrete representation>\nVideoGPT (Yan et al., 2021)\n222.7\nMoCoGAN (Tulyakov et al., 2018)\n206.6\nMoCoGAN-HD (Tian et al., 2021)\n164.1\nLVDM (He et al., 2022)\n95.2\nPVDM (Yu et al., 2023)\n71.46\n<Continuous representation>\nDomain-specific\nDIGAN (Yu et al., 2022)\n83.11\nStyleGAN-V (Skorokhodov et al., 2022)\n79.52\nDDMI (Ours)\n66.25\nTable 7: Text-to-shape generation results.\nAcc ↑\nCLIP-S ↑\nTMD ↓\nDomain-specific\nIMLE (Liu et al., 2022)\n34.79\n-\n0.891\nAuto-SDF (Mittal et al., 2022)\n83.88\n-\n0.581\nDiffusion-SDF (Li et al., 2023)\n88.56\n28.63\n0.169\nDomain-agnostic\nDDMI (Ours, w = 3)\n91.30\n30.30\n0.204\nTable 8: Ablation study.\nGeneration\ntarget\nHDBFs\nCFC\nFirst Stage\n(PSNR)\nSecond Stage\n(FID)\nPE\n32.72\n8.54\nPE\n✓\n33.17\n8.23\nPE\n✓\n✓\n33.56\n7.82\ngle chair class (1.5) and multi-class (1.3) settings. Moreover, in multi-class shape generation, we\nattain the highest COV, showcasing our ability to generate diverse shapes with high fidelity.\nNext, we provide text-guided shape generation results on Text2Shape (T2S) dataset (Chen et al.,\n2019), comprising 75K paired examples of text and shapes on chairs and tables. For training, we\nutilize pre-trained CLIP text encoder (Radford et al., 2021) τ for encoding text prompt t into em-\nbedding τ(t) and condition it to LDM eθ(zt, t, τ(t)) by cross-attention (Rombach et al., 2022). For\ngeneration, the text-conditioned score ˆe is derived using classifier-free guidance (Ho & Salimans,\n2022): ˆeθ(zt, t, τ(t)) = eθ(zt, t, ∅) + w · (eθ(zt, t, τ(t)) − eθ(zt, t, ∅)), where ∅ and w indicates an\nempty prompt and guidance scale, respectively. We measure classification accuracy, CLIP-S (CILP\nsimilarity score), and Total Mutual Difference (TMD) to evaluate against shape generative mod-\nels (Mittal et al., 2022; Li et al., 2023; Liu et al., 2022). Tab. 7 illustrates the strong performance of\nDDMI, indicating our method generates high-fidelity shapes (accuracy and CLIP-S) with a diversity\nlevel comparable to baselines (TMD).\nQualitative analysis. Fig. 1 and Fig.6 present qualitative results. Fig. 6 displays visualizations of\ntext-guided shape generations, demonstrating our model’s consistent generation of faithful shapes\ngiven text conditions (e.g., “a two-layered table”) over Diffusion-SDF (Li et al., 2023). Fig. 1 incor-\nporates the comprehensive results, including unconditional shape generation, text-conditioned shape\ngeneration, and unconditional Neural Radiance Field (NeRF) generation. Especially for NeRF, we\ntrain DDMI with SRN Cars dataset (Sitzmann et al., 2019). Specifically, we encode point clouds\nto the 2D-triplane HDBFs, allowing MLP to read out triplane features at queried coordinate and\nray direction into color and density via neural rendering (Mildenhall et al., 2021). For more re-\nsults, including a qualitative comparison between Functa (Dupont et al., 2022a) and ours on NeRF\ngeneration, see Fig. 15 in the Appendix.\n4.3\nVIDEOS\nDatasets and baselines.\nFor videos, we use the SkyTimelapse dataset (Xiong et al., 2018) and\npreprocess each video to have 16 frames and a resolution of 2562, following the conventional setup\nused in recent video generative models (Skorokhodov et al., 2022; Yu et al., 2023). We learn 2D\nvideos as a continuous function ω maps spatial-temporal coordinates to corresponding RGB values,\ni.e., ω : R3 → R3. We compare our results with domain-specific INR generative models (Yu et al.,\n2022; Skorokhodov et al., 2022) as well as discrete video generative models like LVDM (He et al.,\n2022) and PVDM (Yu et al., 2023).\n8\nPublished as a conference paper at ICLR 2024\nA rocking chair\nA two layered table\nDiffusion-SDF\nOurs\nDiffusion-SDF\nOurs\nFigure 6: Qualitative comparison on text-guided shape generation. We present a comparison of\nthe generation results produced by Diffusion-SDF and DDMI for given text prompt. DDMI excels in\ngenerating intricate details while preserving smooth surfaces. In contrast, Diffusion-SDF struggles\nto capture fine details and often produces less polished surfaces.\nQuantitative analysis. Tab. 6 illustrates the quantitative results of 2D video generation. We em-\nploy Fr´echet Video Distance (FVD) (Unterthiner et al., 2018) as the evaluation metric, following\nStyleGAN-V (Skorokhodov et al., 2022). DDMI shows competitive performance to the most recent\ndiffusion model (PVDM (Yu et al., 2023)), which is specifically trained on discrete video (pixels).\nThis validates the effectiveness of our design choices in enhancing the expressive power of INR. We\nalso provide qualitative results in Fig. 1 and 14, where the generated videos exhibit realistic quality\nin each frame and effectively capture the motion across frames, demonstrating the versatility of our\nmodel in handling not only spatial but also temporal dimensions.\n4.4\nANALYSIS\nFigure 7: HDBFs analysis.\nThe upper two\nrows show generated images whereas the bot-\ntom one shows the spectral magnitude after ap-\nplying Fourier transform. The far left column\nindicates the generated image. Other columns\nindicate the generated image with (a): Ξ2, Ξ3,\n(b): Ξ1, Ξ3, and (c): Ξ1, Ξ2 zeroed out, respec-\ntively. We employ histogram equalization for\nbetter visualization.\nDecomposition of HDBFs.\nIn Fig. 7, we con-\nduct an analysis to investigate the role of different\nscales of HDBFs in representing signals. Specif-\nically, we zero out all HDBFs except one during\ngeneration and observe the results in both spatial\nand spectral domains. When (a) Ξ2 and Ξ3 are\nzeroed out, the generated image contains coarser\ndetails, such as colors, indicating that the first Ξ1\nfocuses on larger-scale components. In contrast,\n(c) with the third Ξ3, our model generates images\nof high-frequency details, such as whiskers and\nfurs. Results in the spectral domain after Fourier\ntransform also match the tendency in the spatial\ndomain, as the spectra of (c) exhibit high mag-\nnitudes in the high-frequency domains, whereas\nthose of (a) have high magnitudes in the low-\nfrequency domains. The analysis indicates that\nHDBFs effectively decompose basis fields to cap-\nture the signals of different scales.\nAblation. Here, we conduct an ablation study to\nevaluate the impact of each component in DDMI.\nWe train DDMI using various configurations on\nthe AFHQv2 Cat dataset with a resolution of\n3842.\nThe baseline refers to D2C-VAE with-\nout HDBFs and CFC. We incrementally introduce\ncomponents and present the Peak Signal-to-Noise\nRatio (PSNR) and FID scores at the end of the\nfirst and second training stages in Tab. 8. The results reveal a gradual increase in PSNR and a cor-\nresponding decrease in FID scores as we incorporate additional components. This underscores the\neffectiveness of these components in enhancing the accuracy and realism of INR.\n9\nPublished as a conference paper at ICLR 2024\n5\nCONCLUSION\nIn this paper, we have introduced DDMI, a domain-agnostic latent diffusion model designed to syn-\nthesize high-quality Implicit Neural Representations (INRs) across various signal domains. Our ap-\nproach defines the Discrete-to-continuous space Variational AutoEncoder (D2C-VAE), which gen-\nerates Positional Embeddings (PEs) and establishes a seamless connection between discrete data and\ncontinuous functions. Leveraging this foundation and our novel coarse-to-fine conditioning mech-\nanism with Hierarchically Decomposed Basis Fields (HDBFs), our extensive experiments across a\nwide range of domains have consistently demonstrated the versatility and superior performance of\nDDMI when compared to existing INR-based generative models.\nREFERENCES\nPanos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representa-\ntions and generative models for 3d point clouds. In International Conference on Machine Learn-\ning, 2018.\nIvan Anokhin, Kirill Demochkin, Taras Khakhulin, Gleb Sterkin, Victor Lempitsky, and Denis Ko-\nrzhenkov. Image generators with conditionally-independent pixel synthesis. In Conference on\nComputer Vision and Pattern Recognition, 2021.\nJonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and\nPratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields.\nIn International Conference on Computer Vision, 2021.\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? In International Conference on Machine Learning, 2021.\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler,\nand Karsten Kreis.\nAlign your latents: High-resolution video synthesis with latent diffusion\nmodels. In Conference on Computer Vision and Pattern Recognition, 2023.\nAng Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. arXiv preprint\narXiv:2301.09632, 2023.\nEric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio\nGallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware\n3d generative adversarial networks. In Conference on Computer Vision and Pattern Recognition,\n2022.\nAngel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\nSilvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\nmodel repository. In arXiv preprint arXiv:1512.03012, 2015.\nKevin Chen, Christopher B Choy, Manolis Savva, Angel X Chang, Thomas Funkhouser, and Silvio\nSavarese. Text2shape: Generating shapes from natural language by learning joint embeddings. In\n14th Asian Conference on Computer Vision, 2019.\nYinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local\nimplicit image function. In Conference on Computer Vision and Pattern Recognition, 2021.\nZeyuan Chen, Yinbo Chen, Jingwen Liu, Xingqian Xu, Vidit Goel, Zhangyang Wang, Humphrey\nShi, and Xiaolong Wang. Videoinr: Learning video implicit neural representation for continuous\nspace-time super-resolution. In Conference on Computer Vision and Pattern Recognition, 2022.\nZhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Conference\non Computer Vision and Pattern Recognition, 2019.\nYunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis\nfor multiple domains. In Conference on Computer Vision and Pattern Recognition, 2020.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In Ad-\nvances in Neural Information Processing Systems, 2021.\n10\nPublished as a conference paper at ICLR 2024\nYilun Du, M. Katherine Collins, B. Joshua Tenenbaum, and Vincent Sitzmann. Learning signal-\nagnostic manifolds of neural fields. In Advances in Neural Information Processing Systems, 2021.\nEmilien Dupont, Hyunjik Kim, S. M. Ali Eslami, Danilo Jimenez Rezende, and Dan Rosenbaum.\nFrom data to functa: Your data point is a function and you can treat it like one. In International\nConference on Machine Learning, 2022a.\nEmilien Dupont, Yee Whye Teh, and Arnaud Doucet. Generative models as distributions of func-\ntions. In International Conference on Artificial Intelligence and Statistics, 2022b.\nClemens Eppner, Arsalan Mousavian, and Dieter Fox. Acronym: A large-scale grasp dataset based\non simulation. In International Conference on Robotics and Automation, 2021.\nZiya Erkoc¸, Fangchang Ma, Qi Shan, Matthias Nießner, and Angela Dai. Hyperdiffusion: Generat-\ning implicit neural fields with weight-space diffusion. In International Conference on Computer\nVision, 2023.\nPatrick Esser, Robin Rombach, and Bj¨orn Ommer. A note on data biases in generative models. In\narXiv preprint arXiv:2012.02516, 2020.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial networks. In Communications of\nthe ACM, 2020.\nYingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion mod-\nels for high-fidelity video generation with arbitrary lengths. In arXiv preprint arXiv:2211.13221,\n2022.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances\nin Neural Information Processing Systems, 2017.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\nIn arXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\nmans. Cascaded diffusion models for high fidelity image generation. In The Journal of Machine\nLearning Research, 2022.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for im-\nproved quality, stability, and variation. In International Conference on Learning Representations,\n2018.\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. In Advances in Neural Information Processing\nSystems, 2020a.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\ning and improving the image quality of stylegan. In Conference on Computer Vision and Pattern\nRecognition, 2020b.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. In Advances in Neural Information Processing Systems, 2022.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In arXiv preprint\narXiv:1412.6980, 2014.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\n11\nPublished as a conference paper at ICLR 2024\nBatuhan Koyuncu, Pablo Sanchez-Martin, Ignacio Peis, Pablo M Olmos, and Isabel Valera. Vari-\national mixture of hypergenerators for learning distributions over functions. In arXiv preprint\narXiv:2302.06223, 2023.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nToronto, ON, Canada, 2009.\nTuomas Kynk¨a¨anniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved\nprecision and recall metric for assessing generative models. In Advances in Neural Information\nProcessing Systems, 2019.\nMuheng Li, Yueqi Duan, Jie Zhou, and Jiwen Lu. Diffusion-sdf: Text-to-shape via voxelized diffu-\nsion. In Conference on Computer Vision and Pattern Recognition, 2023.\nZhengzhe Liu, Yi Wang, Xiaojuan Qi, and Chi-Wing Fu. Towards implicit text-guided 3d shape\ngeneration. In Conference on Computer Vision and Pattern Recognition, 2022.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\nIn arXiv preprint\narXiv:1711.05101, 2017.\nShitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Confer-\nence on Computer Vision and Pattern Recognition, 2021.\nRicardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovit-\nskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo\ncollections. In Conference on Computer Vision and Pattern Recognition, 2021.\nLars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\nOccupancy networks: Learning 3d reconstruction in function space. In Conference on Computer\nVision and Pattern Recognition, 2019.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Communica-\ntions of the ACM, 2021.\nParitosh Mittal, Yen-Chi Cheng, Maneesh Singh, and Shubham Tulsiani. Autosdf: Shape priors for\n3d completion, reconstruction and generation. In Conference on Computer Vision and Pattern\nRecognition, 2022.\nThomas M¨uller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics prim-\nitives with a multiresolution hash encoding. In ACM Transactions on Graphics, 2022.\nGimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto Tono, Linqi Zhou, and Paul Guerrero.\n3d-ldm: Neural implicit 3d shape generation with latent diffusion models.\narXiv preprint\narXiv:2212.00842, 2022a.\nSeonghyeon Nam, Marcus A Brubaker, and Michael S Brown. Neural image representations for\nmulti-image fusion and layer separation. In European Conference on Computer Vision, 2022b.\nEvangelos Ntavelis, Mohamad Shahbazi, Iason Kastanis, Radu Timofte, Martin Danelljan, and Luc\nVan Gool.\nArbitrary-scale image synthesis.\nIn Conference on Computer Vision and Pattern\nRecognition, 2022.\nJeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\nDeepsdf: Learning continuous signed distance functions for shape representation. In Conference\non Computer Vision and Pattern Recognition, 2019.\nKeunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M\nSeitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Conference on\nComputer Vision and Pattern Recognition, 2021.\nSongyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Convolu-\ntional occupancy networks. In European Conference on Computer Vision, 2020.\n12\nPublished as a conference paper at ICLR 2024\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for\n3d classification and segmentation. In Conference on Computer Vision and Pattern Recognition,\n2017.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\nGirish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In International Conference on Machine Learning,\n2021.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj¨orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Conference on Computer Vision and\nPattern Recognition, 2022.\nAditya Sanghi, Hang Chu, Joseph G Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and\nKamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In Conference\non Computer Vision and Pattern Recognition, 2022.\nJaehyeok Shim, Changwoo Kang, and Kyungdon Joo. Diffusion-based signed distance fields for 3d\nshape generation. In Conference on Computer Vision and Pattern Recognition, 2023.\nVincent Sitzmann, Michael Zollh¨ofer, and Gordon Wetzstein. Scene representation networks: Con-\ntinuous 3d- structure-aware neural scene representations. In Advances in Neural Information\nProcessing Systems, 2019.\nVincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Im-\nplicit neural representations with periodic activation functions. In Advances in Neural Information\nProcessing Systems, 2020.\nIvan Skorokhodov, Savva Ignatyev, and Mohamed Elhoseiny. Adversarial generation of continuous\nimages. In Conference on Computer Vision and Pattern Recognition, 2021.\nIvan Skorokhodov, Sergey Tulyakov, and Mohamed Elhoseiny. Stylegan-v: A continuous video\ngenerator with the price, image quality and perks of stylegan2.\nIn Conference on Computer\nVision and Pattern Recognition, 2022.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In Interna-\ntional Conference on Learning Representations, 2021.\nMatthew Tancik, Pratul Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\nSinghal, Ravi Ramamoorthi, Jonathan Barron, and Ren Ng. Fourier features let networks learn\nhigh frequency functions in low dimensional domains. In Advances in Neural Information Pro-\ncessing Systems, 2020.\nYu Tian, Jian Ren, Menglei Chai, Kyle Olszewski, Xi Peng, Dimitris N Metaxas, and Sergey\nTulyakov. A good image generator is what you need for high-resolution video synthesis. In\nInternational Conference on Learning Representations, 2021.\nPatrick Tinsley, Adam Czajka, and Patrick Flynn. This face does not exist... but it might be yours!\nidentity leakage in generative models. In Winter Conference on Applications of Computer Vision,\n2021.\nSergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion\nand content for video generation. In Conference on Computer Vision and Pattern Recognition,\n2018.\nThomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski,\nand Sylvain Gelly. Towards accurate generative models of video: A new metric & challenges.\narXiv preprint arXiv:1812.01717, 2018.\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. In\nAdvances in Neural Information Processing Systems, 2021.\n13\nPublished as a conference paper at ICLR 2024\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, 2017.\nWei Xiong, Wenhan Luo, Lin Ma, Wei Liu, and Jiebo Luo. Learning to generate time-lapse videos\nusing multi-stage dynamic generative adversarial networks. In Conference on Computer Vision\nand Pattern Recognition, 2018.\nXingqian Xu, Zhangyang Wang, and Humphrey Shi. Ultrasr: Spatial encoding is a missing key for\nimplicit image function-based arbitrary-scale super-resolution. arXiv preprint arXiv:2103.12716,\n2021.\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\nvq-vae and transformers. In arXiv preprint arXiv:2104.10157, 2021.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop. In arXiv\npreprint arXiv:1506.03365, 2015.\nSihyun Yu, Jihoon Tack, Sangwoo Mo, Hyunsu Kim, Junho Kim, Jung-Woo Ha, and Jinwoo Shin.\nGenerating videos with dynamics-aware implicit generative adversarial networks. In International\nConference on Learning Representations, 2022.\nSihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in\nprojected latent space. In Conference on Computer Vision and Pattern Recognition, 2023.\nXinyang Zheng, Yang Liu, Pengshuai Wang, and Xin Tong. Sdf-stylegan: Implicit sdf-based style-\ngan for 3d shape generation. In Computer Graphics Forum, 2022.\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel\ndiffusion. In Conference on Computer Vision and Pattern Recognition, 2021.\nPeiye Zhuang, Samira Abnar, Jiatao Gu, Alex Schwing, Joshua M Susskind, and Miguel ´Angel\nBautista. Diffusion probabilistic fields. In International Conference on Learning Representations,\n2023.\nA\nIMPLEMENTATION DETAILS\nWe here provide more details of our implementation, including the architecture of our models and\nhyperparameters. The hyperparameters used in experiments are provided in Tab. 9.\nEncoders and latent spaces.\nWe use slightly different encoder backbones and structures for the\nlatent variable depending on the signal domains. For a 2D image x, we encode it into a latent\nvariable z as a 2D plane, using a 2D CNN-based U-Net encoder (Ho et al., 2020).\nFor a 3D point cloud input x ∈ R3×N, consisting of N points in xyz-coordinates, we encode it into a\ntri-plane latent variable z = {zxy, zyz, zxz}, following Conv-ONet (Peng et al., 2020). Specifically,\ngiven an input point cloud, we employ PointNet (Qi et al., 2017) for feature extraction and apply\nan orthographic projection to map its feature onto three canonical planes. Features projecting onto\nthe same pixel are aggregated using a local pooling operation, resulting in three distinct 2D grid\nfeatures. We then apply a shared 2D CNN encoder to the projected features on grid features to\nobtain the tri-plane latent variables.\nLastly, for 2D video x, we encode it into a latent variable as a 2D tri-plane. We first convert it\ninto 3D features using TimesFormer (Bertasius et al., 2021) encoder. Then, we perform 2D projec-\ntion onto three canonical planes for latent variables z = [zxy, zys, zxs] using three separate small\nTransformers (Vaswani et al., 2017), respectively, following Yu et al. (2023). Note that s denotes a\ntemporal dimension.\n14\nPublished as a conference paper at ICLR 2024\nTable 9: Hyperparameters of our models.\nAFHQ Cat/Dog\nCelebA-HQ\nCIFAR10\nLsun Church\nShapeNet\nSKY\n2562\n2562\n322\n1282\nChair\nMulti Class\n2562\nEncoder\nlatent z type\n2D plane\n2D plane\n2D plane\n2D plane\n2D tri-plane\n2D tri-plane\n2D tri-plane\nspatial dims. of zij\n642\n162\n322\n642\n162\n162\n642\nchannel dims. of zij\n256\n256\n256\n256\n128\n128\n128\nDecoder\nchannel multiplier\n(4,3,2,1)\n(4,4,3,2,1)\n(4,3,2,1)\n(4,3,2,1)\n(4,4,2,1)\n(4,4,2,1)\n(3,2,2,1,1)\nhead channel\n64\n64\n64\n64\n32\n32\n64\nbasis field Ξ type\n2D plane\n2D plane\n2D plane\n2D plane\n2D tri-plane\n2D tri-plane\n2D tri-plane\nspatial dims. of Ξ1\nij\n642\n642\n-\n322\n162\n162\n642\nspatial dims. of Ξ2\nij\n1282\n1282\n-\n642\n322\n322\n1282\nspatial dims. of Ξ3\nij\n2562\n2562\n322\n1282\n642\n642\n2562\nchannel dims. of Ξ\n64\n64\n64\n64\n128\n128\n64\nMLP\nnumber of blocks\n5\n5\n5\n5\n5\n5\n5\nnπ\n3\n3\n3\n3\n2\n2\n2\nwπ\n256\n256\n128\n256\n256\n256\n128\nLatent diffusion model\nnumber of scales\n4\n4\n4\n4\n3\n3\n3\nresidual blocks per scale\n4\n8\n4\n8\n5\n5\n3\nchannel multiplier\n(1,2,2,2)\n(1,2,2,2)\n(1,2,2,2)\n(1,2,2,2)\n(1,1,2)\n(1,1,2)\n(1,1,2)\nhead channel\n256\n256\n128\n256\n384\n384\n384\ndropout\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\n0.2\ni, j ∈ {x, y, z}.\nDecoders.\nFor all domains, we use 2D CNN-based U-Net decoder Dψ to convert latent variable z\ninto basis fields Ξ. We maintain the structure of the basis fields to be consistent with the structure\nof the latent variables. For instance, in the case of 3D shapes, we compute tri-plane basis fields as\nΞ = {Ξxy, Ξyz, Ξxz}, where Ξxy = Dψ(zxy), Ξyz = Dψ(zyz), and Ξxz = Dψ(zxz). The same\nprocedure is applied to other data domains, e.g., Ξ = Ξxy for images and Ξ = [Ξxy, Ξys, Ξxs] for\nvideos. As mentioned in the main paper, we use three different scales of basis fields (Ξ1, Ξ2, and\nΞ3) from our HDBFs.\nMLP function.\nGiven the basis fields Ξ computed above, we compute positional embedding p\nfor given coordinate c by bilinear interpolation that uses the distance-weighted average of the four\nnearest features of Ξ from the coordinate c. We apply the same procedure to each scale of basis\nfields for a given coordinate in order to achieve p1, p2, and p3. For tri-plane Ξ, an axis-aligned\northogonal projection for each plane is applied beforehand. Then the positional embedding p at c is\nfed into MLP-based residual blocks nπ with wπ channels.\nLatent diffusion model.\nFor the LDM backbone, we adopt the 2D Unet model from LSGM (Song\net al., 2021) and LDM (Rombach et al., 2022) and modify the hyperparameters.\nB\nTRAINING AND INFERENCE\nOur framework performs two-stage training. In the first stage, the proposed method learns the latent\nspace by optimizing D2C-VAE. In the second stage, we optimize a diffusion model in the learned\nlatent space. In this work, all experiments are conducted on 8 NVIDIA RTX3090 and 8 V100 GPUs.\nWe provide the hyperparameters of training in Tab. 10.\nB.1\nFIRST-STAGE TRAINING.\nIn the first stage, we optimize the encoder, decoder, and MLP of D2C-VAE using the Adam (Kingma\n& Ba, 2014) optimizer. We minimize the re-weighted ELBO objective of D2C-VAE in Eq. 9. We\nassume the standard normal prior distribution p(z) and a multivariate normal distribution for the\nposterior qϕ(z|x). The KL divergence loss between the posterior and the prior is calculated using\nthe reparameterization trick Kingma & Welling (2013). For image, video, and NeRF, we assume\n15\nPublished as a conference paper at ICLR 2024\nTable 10: Training details of our models\nAFHQ Cat/Dog\nCelebA-HQ\nCIFAR10\nLsun Church\nShapeNet\nSKY\n2562\n2562\n322\n1282\nChair\nMulti Class\n2562\nFirst stage training\nlearning rate\n2e-4\n2e-4\n2e-4\n2e-4\n2e-4\n2e-4\n2e-4\noptimizer\nAdam\nAdam\nAdam\nAdam\nAdam\nAdam\nAdam\noptimizer weight decay\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\nAdam β1\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\nAdam β2\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nSN weight start\n10\n10\n10\n10\n10\n10\n10\nSN weight decay anneal\nlinear\nlinear\nlinear\nlinear\nlinear\nlinear\nlinear\nλz start\n1e-4\n1e-4\n1e-4\n1e-4\n1e-4\n1e-4\n1e-4\nλz end\n0.2\n0.2\n0.2\n0.2\n5e-3\n5e-3\n1.0\nλz anneal\nlinear\nlinear\nlinear\nlinear\nlinear\nlinear\nlinear\nλz anneal portion\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\nbatch size per GPU\n4\n4\n24\n8\n32\n32\n2\nnumber of GPU\n4\n8\n8\n8\n4\n4\n4\nepochs\n400\n200\n400\n400\n1000\n300\n400\nSecond stage training\nlearning rate\n1e-4\n1e-4\n1e-4\n1e-4\n2e-4\n2e-4\n1e-4\noptimizer\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\nAdamW\noptimizer weight decay\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\n3e-4\nAdam β1\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\n0.9\nAdam β2\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\n0.99\nbatch size per GPU\n8\n8\n24\n8\n64\n64\n8\nnumber of GPU\n4\n8\n8\n8\n4\n4\n4\nepochs\n1000/2000\n1000\n1500\n1000\n3000\n1000\n6000\nthat pψ,πθ(ω(c)|z) follows the multivariate normal distribution with isotropic covariance σ:\npψ,πθ(ω(c)|z) =\nY\nc∈c\n1\nZ exp\n\u0012\n−||ω(c) − ˆω(c)||2\n2\n2σ2\n\u0013\n, ω(c) ∈ R.\n(11)\nFor the occupancy function, we assume the Bernoulli distribution:\npψ,πθ(ω(c)|z) =\nY\nc∈c\nˆω(c)ω(c) · (1 − ˆω(c))ω(c), ω(c) ∈ {0, 1}.\n(12)\nIn practice, we employ ℓ1-loss for image, video, and NeRF, and binary cross-entropy for 3D shapes.\nWe linearly increase the weight λz during training and apply a spectral normalization (SN) regular-\nization for the encoder, following Vahdat et al. (2021).\nMulti-scale training and scale injection.\nFor training our model on images, we incorporate the\nmulti-scale training scheme (Ntavelis et al., 2022) to further encourage the estimation of accurate\nˆω. In our approach, we incorporate a scale variable s ∈ R that represents the sampling period of\nthe coordinate set c for the discrete data x. Through the training of D2C-VAE with data of diverse\nscales, e.g., utilizing multi-resolution data or applying random crop augmentation, we enable the\nINR to explore a wider range of coordinate sets c in Eq. 9, enabling high-quality generation at\narbitrary scales. In addition to multi-scale training, we incorporate a Scale Injection (SI) to enhance\nthe performance of our framework further. We modulate the weights of each fully connected layer\nof MLP with the scale variable s, similar to style modulation in StyleGAN Karras et al. (2020b).\nThe injection of scale information makes INR aware of the target scale. Specifically, the (i, j) entry\nof lth layer weight ϕl ∈ Rm×n is modulated to ˆϕl:\nˆϕl\ni,j(s) =\nϕl\ni,j · Al(s)j\nr\nΣk\n\u0010\nϕl\ni,k · Al(s)k\n\u00112\n+ ε\n,\n(13)\nwhere ε is a small constant for numerical stability and Al indicates layer-wise mapping function.\ns undergoes Fourier embedding with a fully connected layer before the layer-wise mapping. In\npractice, we provide the encoder with full-frame fixed-resolution images of size r×r (e.g., 256×256)\nto make fixed-size latent variables. Then, for each batch of latent variables, we randomly select a\nresolution from a predefined set of resolutions {r, 1.5 × r, 2 × r} that the INR aims to present in\na discrete image. For images with a higher resolution than r, we randomly crop them to match the\nresolution r.\n16\nPublished as a conference paper at ICLR 2024\nB.2\nSECOND-STAGE TRAINING.\nIn the second stage, we optimize LDM on the empirical distribution of the latent variables using the\nAdamW (Loshchilov & Hutter, 2017) optimizer. We minimize the noise prediction loss, which is\ndiscussed in the main paper, while the other networks trained in the first stage are frozen. In the case\nof tri-plane latent variables, e.g., 3D and video, we use a single 2D Unet model to denoise each latent\nplane and add attention layers that operate on the intermediate features of three latent planes (Yu\net al., 2023). This not only allows us to model the dependency between planes effectively but also to\nuse the shared 2D Unet structure across different domains. The training objective of LDM in Eq. 10\nis by reparameterization trick µφ(zt, t) =\n1\n√αt (zt −\nβt\n√1−¯αt ϵφ(zt, t)), where αt := 1−βt and ¯αt :=\nQt\ns=1 αs. We mostly follow techniques proposed in the previous literature. Specifically, we employ\ncontinuous VPSDE (Song et al., 2021) and utilize mixed parameterization of score function (Vahdat\net al., 2021). We found the mixed parameterization beneficial for training the latent space of INRs\nsince we also regularize the posterior distribution towards standard normal distribution in the first\nstage. We also adopt an importance sampling with re-weighted objective function (Vahdat et al.,\n2021), in order to reduce the variance of loss and stabilize the training procedure. Following existing\nworks, we use an exponential moving average (EMA) of the parameters of the LDM network with\na 0.9999 EMA decay rate.\nB.3\nINFERENCE.\nFor sampling, we use a reverse diffusion process with a fixed number of steps T = 1000 for all\nexperiments presented in the main paper unless stated otherwise. However, our model can also\nleverage recent advanced samplers as discussed in Tab. 12.\nC\nBASELINES AND EVALUATION METRICS\nIn this section, we provide a brief overview of baselines and the evaluation metrics employed to\nassess the performance of the model.\nC.1\n2D IMAGE\nBaselines.\nCIPS (Anokhin et al., 2021),\nINR-GAN (Skorokhodov et al., 2021),\nand\nScaleParty (Ntavelis et al., 2022) incorporate adversarial training (Goodfellow et al., 2020) within\nthe StyleGANv2 framework (Karras et al., 2020b). These models utilize fixed single-scale Fourier\nfeatures for PEs instead of constant input used in StyleGANv2 and modulate the activation function\nof the network with global latent variables. In particular, ScaleParty (Ntavelis et al., 2022) deploys a\nCNN-based architecture instead of an MLP and leverages multi-scale training to make the network\naware of the generation scale. VaMoH (Koyuncu et al., 2023) employs multiple hyper-generators\nthat generate the weights of INRs with fixed PEs to capture different aspects of signals.\nFr´echet Inception Distance (FID)\n(Heusel et al., 2017) is utilized to quantify the dissimilarity\nbetween two distributions, typically used in the context of generative models. It operates by com-\nparing the mean and standard deviation of features extracted from the deepest layer of the Inception\nv3 neural network. In our evaluation, we compute the FID between all available real samples, up\nto a maximum of 50K, and 50K generated samples, following Karras et al. (2020b). FID helps\ngauge the quality and diversity of the generated samples by assessing their proximity to the real data\ndistribution. Lower FID scores signify better agreement between the two distributions.\nImproved precision and recall (P&R)\n(Kynk¨a¨anniemi et al., 2019) measures the expected likeli-\nhood of real (fake) samples belonging to the support of fake (real) distribution. They approximate the\nsupport of distribution by constructing K-nearest neighbor hyperpheres around each sample. P&R\ntypically represents fidelity and diversity of generative model, respectively. In our evaluation, we\ncompute the P&R between all available real samples, up to a maximum of 50K, and 50K generated\nsamples.\n17\nPublished as a conference paper at ICLR 2024\nC.2\n3D SHAPE\nBaselines.\nDPM3D (Luo & Hu, 2021) and PVD construct diffusion processes for point clouds by\nutilizing point cloud generators. LatentGAN (Chen & Zhang, 2019) and SDF-StyleGAN (Zheng\net al., 2022) both make use of a global latent vector to represent global 3D shape features through\nadversarial training. Specifically, SDF-StyleGAN employs global and local discriminators to gen-\nerate fine details in the 3D shapes. 3D-LDM (Nam et al., 2022a) learns the global latent vector of\nSDF through an auto-decoder and trains diffusion models in the latent space. SDF-Diffusion (Shim\net al., 2023) introduces a diffusion model applied to voxel-based Signed Distance Fields (SDF)\nand another diffusion model for patch-wise SDF voxel super-resolution. AutoSDF (Mittal et al.,\n2022) combines a non-sequential autoregressive prior for 3D shapes conditioned on a text prompt.\nDiffusion-SDF (Li et al., 2023) encodes point clouds into a voxelized latent space and introduces a\nvoxelized diffusion model for text-guided generation.\nClassification Accuracy\ninvolves the use of a voxel-based classifier that has been pre-trained\nto classify objects into 13 different categories within the ShapeNet dataset (Chang et al., 2015),\nfollowing the approach (Sanghi et al., 2022). It is employed to gauge the semantic authenticity of\nthe generated samples, assessing how well the generated shapes align with the expected categories\nin ShapeNet.\nCLIP similarity score (CLIP-S)\nemploys the pre-trained CLIP model for measuring the corre-\nspondence between images and text descriptions by computing the cosine similarity. It evaluates\nhow well the generated shape aligns with the intended text. We follow the evaluation protocol in\nLi et al. (2023): we render five different views for the generated shape measure CLIP-S for these\nrendered images and use the highest score obtained for each text description.\nTotal mutual difference (TMD).\nIn order to measure TMD, we follow the protocols in Li et al.\n(2023). Specifically, we generate ten different samples for each given text description. Subsequently,\nwe calculate the average Intersection over Union (IoU) score for each generated shape concerning\nthe other 9 shapes. The metric then computes the average IoU score across all text queries. TMD\nserves as a measure of generation diversity for each specific text query.\nC.3\nVIDEO\nBaselines.\nMoCoGAN (Tulyakov et al., 2018) decomposes motion and content in video generation\nby employing separate generators. DIGAN (Yu et al., 2022) and StyleGAN-V (Skorokhodov et al.,\n2022) introduce Implicit Neural Representation (INR)-based video generators with computation-\nally efficient discriminators. VideoGPT (Yan et al., 2021) encodes videos into sequences of latent\nvectors using VQ-VAE and learns autoregressive Transformers. PVDM (Yu et al., 2023) also en-\ncodes videos into a projected tri-plane latent space and subsequently learns a latent diffusion model.\nWhile our model shares a similar structure with PVDM, it is primarily designed for generating fixed\ndiscrete pixels, which limits its adaptability to other domains. There are other recent works, such\nas VLDM (Blattmann et al., 2023), that utilize diffusion models on spatio-temporal latent spaces.\nHowever, VLDM employs a 3D voxel-based latent space and a 3D-CNN-based network, which can\nbe computationally more intensive. Furthermore, their work relies on a large-scale in-house dataset\nwithout open-source code availability, which hinders a fair comparison with our model.\nFr´echet Video Distance (FID)\nis a metric that calculates the Fr´echet distance between the feature\nrepresentations of real and generated videos. To obtain appropriate feature representations, FVD\nemploys a pre-trained Inflated 3D Convnet. In our evaluation, we compute FVD by comparing 2,048\nreal video samples with 2,048 fake samples, following the preprocessing protocol recommended in\nSkorokhodov et al. (2022).\n18\nPublished as a conference paper at ICLR 2024\nTable 11: Generation results on diverse datasets.\nCIFAR10 322\nLsun Churches 1282\nFID ↓\nFID ↓\n<Discrete representation>\nDDPM++(VP) (Song et al., 2021)\n2.47\n-\nStyleGANv2 (Karras et al., 2020b)\n11.07\n3.78\nLSGM (Vahdat et al., 2021)\n2.10\n-\n<Continuous representation>\nDomain-specific\nCIPS (Skorokhodov et al., 2021)\n8.62\n7.38\nDomain-agnostic\nGEM (Du et al., 2021)\n23.83\n-\nDPF (Zhuang et al., 2023)\n15.1\n-\nDDMI (Ours)\n4.53\n5.12\nD\nADDITIONAL RESULTS\nD.1\nQUANTITATIVE RESULTS\nCIFAR10 and Lsun Church.\nTo demonstrate that DDMI can generalize effectively to datasets\nwith more diverse global structures and classes, we have trained our model on the CIFAR10 and\nLSUN Church datasets. As shown in Tab. 11, DDMI achieves a significant performance improve-\nment over both domain-specific INR generative model (CIPS) and domain-agnostic INR generative\nmodels (GEM and DPF) on both datasets. This result further affirms the efficacy of our design\nchoices, e.g., basie fields generation with D2C-VAE, HDBFs, and CFC, holds for datasets with\ndiverse characteristics.\nEfficient sampling with ODE solver.\nRecent advances in diffusion models, such as sam-\npling via ODE (Ordinary Differential Equation) solvers Song et al. (2021); Vahdat et al.\n(2021); Karras et al. (2022), have shown promising results in reducing sampling time.\nWe\nuse an RK45 ODE solver, instead of ancestral sampling, similar to\nVahdat et al. (2021).\nTable 12: NFE vs. FID.\nAFHQv2 Cat\nAFHQv2 Dog\nAvg. NFE\nFID\nFID\n1000\n4.27\n8.54\n50\n5.32\n11.04\n25\n5.93\n14.23\nBy increasing the ODE solver error tolerances, we\ngenerate samples with lower NFEs (number of func-\ntion evaluations). In Tab. 12, we report FID scores\non AFHQv2 Cat and Dog datasets for three cases:\n1) T=1000, 2) ODE solver error tolerances of 10−3,\nand 3) ODE solver error tolerances of 10−5. The\nresults demonstrate that our DDMI framework per-\nforms efficient sampling while still achieving satis-\nfactory performance (see Fig. 16 and 17 for qualita-\ntive results).\nD.2\nQUALITATIVE RESULTS\nVisualization of the nearest neighbor samples.\nIn order to confirm that our model is not over-\nfitting to the training dataset, we employ a visualization technique that displays the nearest training\nsamples to the generated samples in the VGG feature space. This visualization is presented in\nFig. 13.\nArbitrary-scale 2D image synthesis.\nFig. 8, 9, and 10 show the generated 2D images at arbitrary\nresolutions, including 2562, 3842, and 5122. These results demonstrate the ability of our model to\ngenerate continuous representations with high quality.\n3D shape generation.\nIn Fig 12, we compare the qualitative results on 3D shape with two recent\nbaselines: GASP (Dupont et al., 2022b) (domain-agnostic) and SDF-StyleGAN (Zheng et al., 2022)\n(domain-specific). The result demonstrates that our method generates more sophisticated details\nwhile maintaining smooth surfaces compared to the baselines. Moreover, Fig. 11 shows that our\nDDMI generates diverse and high-fidelity 3D shapes across various object categories: airplane, car,\nchair, desk, gun, lamp, ship, etc.\n19\nPublished as a conference paper at ICLR 2024\nVideo generation.\nFig. 14 presents video generation results on the SkyTimelapse dataset. The re-\nsults demonstrate the capability of our model to generate visually appealing and coherent sequences\nof images.\nNeural radiance fields generation.\nFig. 15 displays the NeRF generation results of our model on\nthe SRN car dataset in comparison to Functa.\nE\nBROADER IMPACTS\nThe development of generative models with controllability across diverse data domains has been a\nlong-standing objective in the field. Our proposed generative model, DDMI, represents a significant\nadvancement in the realm of INR generative models, offering high-fidelity generation and effective\napplicability to a wide range of data domains. Moreover, one key strength of DDMI is its capability\nto generate samples at arbitrary scales, providing enhanced controllability that can be leveraged in\nconjunction with existing conditional generation techniques Rombach et al. (2022). This opens up\nexciting possibilities for tasks like text-guided super-resolution video generation, where the fine-\ngrained control offered by DDMI can yield compelling results. By enabling controllable generation\nacross different data modalities, DDMI has the potential to drive further advancements in creative\napplications, and other domains that rely on generative modeling.\nHowever, it is important to acknowledge potential concerns that arise with the deployment of gen-\nerative models Tinsley et al. (2021), including DDMI. Like other generative models, there is a risk\nof revealing private or sensitive information present in the data. Additionally, generative models\nmay exhibit biases Esser et al. (2020) that are present in the dataset used for training. Addressing\nthese concerns and ensuring the ethical and responsible deployment of generative models is crucial\nto mitigate potential negative impacts and promote fair and inclusive use of the technology.\nF\nLIMITATIONS\nDomain-agnostic INR generation has shown promising results in a wide range of applications with\ngreat flexibility. The proposed method enhances the expressive power of INR generation via asym-\nmetric variational autoencoder (VAE) connecting the discrete data space and the continuous function\nspace. One caveat of the proposed method is the relatively long generation time. This is one well-\nknown common disadvantage of diffusion models. In Sec. D.1, we studied advanced solvers to\nspeed up the generation process but it is not satisfactory. With a small number of function evalu-\nations (NFEs), the generation suffers from quality degradation. More efficient generation schemes\nleveraging compact latent spaces will be an interesting future direction.\n20\nPublished as a conference paper at ICLR 2024\nFigure 8: Uncurated samples on AFHQv2 Cat for various resolutions generated from single DDMI\nmodel.\nFigure 9: Uncurated samples on AFHQv2 Dog for various resolutions generated from single DDMI\nmodel.\n21\nPublished as a conference paper at ICLR 2024\nFigure 10: Uncurated samples on CelebA-HQ for various resolutions generated from single DDMI\nmodel.\nFigure 11: Uncurated samples on ShapeNet generated from DDMI model.\n22\nPublished as a conference paper at ICLR 2024\n(a) GASP Dupont et al. (2022b)\n(b) SDF-StyleGAN Zheng et al. (2022)\n(c) Ours\nFigure 12: Qualitative comparison on 3D INR generation. We compare GASP (domain-agnostic),\nSDF-StyleGAN (domain-specific), and DDMI on 3D shapes.\nFigure 13: Nearest neighbors of our CelebA-HQ model, computed in the features of VGG16. The\nleftmost sample is from our model, and the remaining samples in each row are its 7 nearest neigh-\nbors.\n23\nPublished as a conference paper at ICLR 2024\nFigure 14: Uncurated samples on SkyTimelapse generated from DDMI.\n24\nPublished as a conference paper at ICLR 2024\n(a) Functa Dupont et al. (2022a)\n(b) Ours\nFigure 15: Qualitative comparison on NeRF generation. Our model generates sophisticated de-\ntails and vivid color texture while maintaining smooth surfaces. In contrast, Functa exhibits limita-\ntion in capturing intricate details and tends to produce blurry texture.\n25\nPublished as a conference paper at ICLR 2024\nFigure 16: Uncurated samples on AFHQv2 Cat generated from DDMI model using ODE solver\nwith 10−5 tolerance (Avg. NFE=25).\nFigure 17: Uncurated samples on AFHQv2 Dog generated from DDMI model using ODE solver\nwith 10−3 tolerance (Avg. NFE=50).\n26\n"
}