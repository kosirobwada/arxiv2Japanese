{
    "optim": "Exploratory Evaluation of Speech Content Masking Jennifer Williams 1, Karla Pizzi 2,3, Paul-Gauthier Noé 4, Sneha Das 5 1 Electronics and Computer Science, University of Southampton, UK 2 Fraunhofer AISEC, Garching, Germany; 3 Technical University Munich, Germany 4 Laboratoire Informatique d’Avignon (LIA), University of Avignon, France 5 Dept. of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark Email: j.williams@soton.ac.uk, karla.pizzi@aisec.fraunhofer.de Abstract Most recent speech privacy efforts have focused on anonymizing acoustic speaker attributes but there has not been as much research into protecting information from speech content. We introduce a toy problem that explores an emerging type of privacy called “content masking” which conceals selected words and phrases in speech. In our efforts to define this problem space, we evaluate an intro- ductory baseline masking technique based on modifying sequences of discrete phone representations (phone codes) produced from a pre-trained vector-quantized variational au- toencoder (VQ-VAE) and re-synthesized using WaveRNN. We investigate three different masking locations and three types of masking strategies: noise substitution, word dele- tion, and phone sequence reversal. Our work attempts to characterize how masking affects two downstream tasks: automatic speech recognition (ASR) and automatic speaker verification (ASV). We observe how the different masks types and locations impact these downstream tasks and discuss how these issues may influence privacy goals. 1 Introduction Audio recordings comprise private information in many ways. In this paper, we focus on content-related informa- tion, ranging from cues about location or acoustic scene [1] to sensitive words and phrases. Speech data can therefore expose privacy risks even if a speaker’s voice has been anonymized, since typical goals for voice anonymization in- clude preserving intelligibility [2]. Content involving words or phrases could be used (or misused) to reveal personal data such as birth date or credit card details, or enable an attacker to identify a person based on spoken names or other similar references. Therefore, the capability to achieve fine- grained control over masking particular aspects of speech data beyond speaker voice characteristics must become center-stage for privacy protection research. In this paper, we introduce a toy problem to explore the concept of content privacy for speech, as a complement to the extensive ongoing efforts from the biennial Voice Privacy Challenges1 (VPC). Systems developed through the VPC aim to conceal, remove, or neutralize speaker-related voice attributes. While the VPC systems have many prac- tical uses, that singular form of privacy is not enough to achieve full anonymization, due to the ability to link spoken content. Consider the example of an audio recording in a dataset dealing with medical health issues with the follow- ing utterance: “My name is James Smith and I have severe depression.” Since the speaker reveals information in the spoken content, anonymizing the acoustic-related speaker attributes alone would not conceal the sensitive informa- tion (the person’s identity and health condition) since this 1https://www.voiceprivacychallenge.org/. can be obtained simply by analyzing the words. Sharing speech data across research areas (e.g., from the medical re- search domain to the speech recognition domain) is difficult due to compliance with GDPR [3] and other related ethical concerns. Useful applications of content masking include sharing sensitive speech databases for research purposes, concealing sensitive words or phrases from speech synthesis screen readers, and disentanglement of speech features and content for more deeper privacy controls. If there was a way to achieve full anonymisation and redact all sensitive infor- mation reliably, this could make it more feasible to share datasets that are important for speech research, ultimately benefiting speech technology development. While targeted aspects of the speech signal become incomprehensible from masking, the speech signal should only be ‘destroyed’ on private information. Our approach serves as foundational work, presenting a first-ever baseline approach using speech re-synthesis. Since we are introducing content privacy and its eval- uation in this paper, there are no established techniques or evaluation protocols [4, 5]. We adopt a re-synthesis tech- nique based on vector-quantized variational autoencoders (VQ-VAE) [6] and present baselines for masking content. We do not propose that VQ-VAE is the best content mask- ing approach, but instead we assess whether re-synthesis techniques could be useful. The VQ phone codes [7] are se- quential and preserve temporal structure of speech, meaning that a string of phones is temporally related to the sequence of words [8]. To mask target words in the speech, we modify sub-strings of the VQ sequence and re-synthesize from the VQ codes using WaveRNN [9]. In this paper, we explore masking using different types of masks (noise, deletion, and reversal), and we target words at different locations in the utterances (start, middle, and end). We evaluate masking using two downstream tasks: automatic speech recognition (ASR), and automatic speaker verification (ASV). We in- vestigated ASR and ASV performance to explore how this type of problem could be evaluated. We also perform masking on original natural speech by removing or replacing segments corresponding to the three types of mask. This baseline simulates perfect masking with perfect quality of speech, to compare against the VQ- VAE speech re-synthesis technique. The contributions of this paper are: • Explore how to modify and replace sequences of VQ- VAE phone codes to conceal content of selected words and phrases • Understand how masking words in an utterance will af- fect downstream tasks such as ASR and ASV by mask- ing original speech (assumes perfect synthesis) as well as re-synthesis from VQ-VAE arXiv:2401.03936v1  [eess.AS]  8 Jan 2024 2 Related Work Previous work in masking speech content involved conceal- ing conversations that take place indoors using a technique called scrambling [10]. This technique was reported to be very effective at masking speech in order to make it un- available to eavesdroppers [11–13]. One of the issues is that the method depends on knowing the room characteris- tics beforehand. The method does not generalise to other use-cases wherein we may want a finer-grained approach to conceal content without disturbing neighboring words or while preserving speaker voice characteristics. Recent research has explored content privacy for ASR, specifically for training language models on texts that have had named entities redacted due to privacy [14]. Their redaction technique (called sanitization) involved swap- ping one named entity for another. They found that the technique results in significant increase in word error rate (WER) when training new language models. The drop in performance is likely due to the sequential nature of lan- guage modeling. The authors did not explore other types of sanitization. With this paper, we aim at filling this gap. We use a method presented in [15]. Here, content masking us- ing VQ-VAE was mentioned as being a possible technique for masking. However, their results are difficult to inter- pret because the evaluation involved human judgments of speaker similarity and WER between a very small sample of original and masked utterances. A privacy-preserving ASR system, called Prεεch [16], was recently proposed to preserve voice biometrics and content privacy for offline and cloud-based ASR services. Specifically, the system employs a combination of segmen- tation and shuffling of signals, identification and removal of sensitive information, and injection of dummy phrases to preserve content privacy. Unfortunately, the data is trans- formed into a ‘bag of words’ which may make it less useful for further downstream applications outside of ASR. Beyond this, adversarial techniques for speech infor- mation hiding have been recently proposed in [17], which demonstrates a method to encode utterances into a database and recover them later using a neural network. The ap- proach relies heavily on forced alignments. Further, they do not assess impacts on downstream tasks nor do they conceal words/phrases at specific positions in an utterance. Whether private information should be recoverable depends upon the use-case. In our work, we consider both cases where recovery is and is not desirable. Recovery is especially un- desirable for sensitive private information due to adversarial attacks. 3 Masking Methodology 3.1 Data and Pre-Processing The data that we use in this paper was the Voice Cloning Toolkit (VCTK) v0.92 [18]. The dataset contains 110 unique speakers, both male and female, of varying ages and English accents. We chose the VCTK dataset because it is studio-quality speech, which helps us establish our content masking baselines. We obtained forced alignments for each audio file from the Montreal forced aligner [19]. As this paper explores the concept of content masking, we made use of forced alignments to ensure that we were able to target specific words at locations in each utterance. In real-world applications of content masking, it may not be possible or desireable to use forced alignments, and instead a technique such as keyword spotting would be more appro- priate depending on the specific application scenario. 3.2 Masking Technique We compare masking for two types of speech: original (natural) utterances and re-synthesized utterances. Words are masked in original speech by relying on forced align- ments. This provides us with high-quality masked speech, and sets an upper-bound as if the speech re-synthesis and learned latent representations were of perfect quality. We are interested in VQ-VAE speech re-synthesis because this technique can learn rich latent representations using autoen- coders. For our baselines involving natural speech from VCTK, we replace segments of natural speech with the masks. For baselines involving speech re-synthesis with VQ-VAE, we modify sequences of VQ phone codes that representing the target words to be masked. One of the benefits of using VQ-VAE is that it allows us to consider content masking while speech is in a coded or compressed state [20], wherein the masking is done strictly by manipulating sequences of VQ phone codes. This mim- ics realistic scenarios where speech may be compressed or in transmission and we may have some temporal infor- mation or keys to designate which sequences of codes to conceal for privacy purposes. The VQ-VAE system that we used in this paper comes from [21]. It is a dual-encoder model which models phone content information and speaker identity. Using that system and pre-trained model, we ex- plored three different locations of the mask within an utter- ance, and three types of masks. 3.3 Mask Types Many types of masks are relevant to speech content masking. For example, it is common in live television to replace certain offensive words with a long ‘beep’ sound [4]. In this work, we examine three types of masks and describe them here. Note that all mask types can change the usual structure of the sentence, which might potentially lead to wrong transcriptions if an automatic speech recognition (ASR) system with a language model is in place. Noise. The noise mask involves replacing content with a temporally-modulated speech-shaped noise masker (ICRA noise 9 from [22]). This was proposed by [21], however they did not evaluate how masking affects downstream tasks. When masking original speech, we use forced alignments as a guide to replace segments of the natural speech with an identical length segment of the speech-shaped noise masker. When masking re-synthesized speech using VQ codes, we first obtain VQ codes corresponding to noise-only and then swap the sequence of VQ codes (of the same length). Deletion. A deletion mask involves deleting content en- tirely from an utterance and the content will not be recover- able. For natural speech, we slice the waveform and remove the segment that contains the target words. For re-synthesis, we remove the VQ phone code sequence corresponding to the target words, before synthesis. We include it in this paper in order to assess impacts on speech re-synthesis as well as impacts on the downstream tasks. Reversal. We explore a third mask type that reverses target words in the time-domain. For original speech, this involves slicing the waveform and reversing only the seg- ment that corresponds to the target words. For re-synthesis, we simply reverse the sequence of VQ codes while keeping the surrounding VQ code sequences in-tact. 3.4 Materials for Evaluation We compiled a subset of speech samples with and without masking. The speakers and utterances were selected from the “condition #3” that was reported by the authors who provided the system and pre-trained model [21]. This par- ticular condition contained samples wherein the speakers were unseen during training and the content was seen dur- ing training. We selected this subset since we are especially interested in content masking, but wanted to avoid using data that had been fully seen during training of the VQ-VAE system. From this subset, we selected 9 speakers (p260, p285, p294, p300, p305, p307, p310, p347, and p351). These speakers were selected because they had an average speaking rate of < 5 words/second, the total duration of the utterances contained > 300 VQ phone codes (1.2 seconds), and number of words spoken was ≥ 7. When counting words, we ignored all “SIL” tokens that were generated from forced alignments by the Montreal aligner. With these speakers and utterances, we applied the masking to create samples for downstream evaluation2. 4 Masking Evaluation 4.1 Automatic Speech Recognition (ASR) To measure the effect of masking on speech recognition, we employed four state-of-the-art ASR models: two Speech- Brain systems [23] and two Whisper systems [24]. The two SpeechBrain systems are sequence to sequence-based models with a combination of convolutional, recurrent, and fully-connected networks (CRDNN) trained on LibriSpeech [25]: (1) a CRDNN with an RNN-based language model (SB-RNN)3; and (2) a CRDNN with a transformer-based language model (SB-Transformer)4. For Whisper, we used the “base” (WH-Base) as well as the “medium” (WH- Medium) models. Both Whisper models are end-to-end encoder-decoder transformer models. Due to the architec- ture and the variety of data Whisper has been trained on, it exhibits improved robustness to accents, background noise and technical language [24]. Original Original Speech + Masking System Speech Noise Deletion Reversal SB-RNN 13.9 22.9 20.6 37.5 SB-Trans. 10.5 19.6 17.2 36.4 WH-Base 4.15 10.6 10.5 32.2 WH-Medium 1.24 9.30 7.72 24.7 VQ-VAE VQ-VAE + Masking System Speech Noise Deletion Reversal SB-RNN 47.5 75.5 65.2 78.3 SB-Trans. 38.8 76.0 59.0 75.4 WH-Base 47.7 78.7 66.2 79.5 WH-Medium 32.3 63.0 55.3 69.1 Table 1: ASR system ↓WER% for original and re- synthesised un-masked and masked utterances. We examine the impact of masking on ASR by mea- suring word error rate (WER%) while comparing the four different ASR models, as shown in Table 1. First, we eval- uated each ASR system using un-masked speech from the 2https://rhoposit.github.io/ITG2023 3https://huggingface.co/speechbrain/ asr-crdnn-rnnlm-librispeech 4https://huggingface.co/speechbrain/ asr-crdnn-transformerlm-librispeech 0 20 40 60 80 100 120 Word Error Rate (WER %) 0.00 0.01 0.02 0.03 0.04 0.05 Kernel Density Estimation WER Distributions for Masked Original Speech noise-end noise-mid noise-start deletion-end deletion-mid deletion-start reversal-end reversal-mid reversal-start Figure 1: Kernel density estimation of ↓WER% for WH- Medium on original speech that was masked. This plot shows the mask types and positions (averaged across all speakers). original and VQ-VAE re-synthesis to establish a benchmark of performance on the VCTK dataset, using the original VCTK transcripts as reference. Next, we evaluated ASR on masked speech using the masking techniques described in Section 3.2, using VCTK transcripts that had the target (masked) words removed, as the reference. In some cases, ASR systems demonstrated catastrophic failure, such as re- peating words 5 or more times, which occurred most often with the noise and reversal masks. We omitted utterances wherein the ASR transcription was more than 30 characters longer than the reference transcript. Even with this quality control in place, we did occasionally observe WER values above 100%, but less so with the WH-Medium model. The WER for un-masked audio and un-masked tran- scription represents our empirical WER. We conducted a paired t-test (p ≪ 0.05, 95% confidence) for each ASR sys- tem separately, and found that the results are significant except for original masked speech, where noise and dele- tion masks are not statistically significant. Likewise, the differences between noise and reversal are not significant for VQ-VAE masked speech for the SpeechBrain models. For SB-RNN and SB-Transformer, these perform worse overall compared to the Whisper models. This is likely due to SpeechBrain models being trained on LibriSpeech, whereas Whisper models are trained on a diverse dataset, including different English dialects. All models indicated worse performance when reversal masking was used. Is- sues with reversal masking may result from audio retaining speech features, as tonal and atonal sounds are roughly kept, while the ASR system is not able to detect meaningful words. This is especially problematic if a language model is in place. From these initial results, we further characterized ASR evaluation using only WH-Medium since it performed best on our original and VQ-VAE speech. We show the kernel density estimations (KDE)5 of the distributions of per-utterance WER scores in Figure 1 and Figure 2. These 5Gaussian KDE has an unbounded support ]−∞,+∞[. Because the WER is positive, we estimate the density of the log WER. The estimated density is then mapped back in the WER domain using the change of variable formula and nomalization. As it is a density estimation (a curve fit to a histogram) there is typical congestion at zero. None With Masking Mask position – Start Middle End Mask type Noise Delete Reversal Noise Delete Reversal Noise Deletion Reversal Original 0.04 0.02 0.08 0.13 0.27 0.07 0.17 0.07 0.00 0.00 VQ-VAE 16.75 19.65 22.53 23.35 21.81 19.85 23.13 23.89 22.29 21.80 Table 2: Comparison of ASV performance measuring ↓EER% using original un-masked enrollment utterances and either original or VQ-VAE masked test utterances. two figures are showing that certain mask positions and types result in different WER. In the original masked speech (Fig. 1), the noise mask at the end of an utterance had the least negative impact on ASR performance whereas the reversal at the middle of an utterance generally had a large impact. Performance on VQ-VAE speech for the WH- medium model (Fig. 2) also shows that WER differs by mask location. The highest WERs came from masking words in the middle of a sentence. This may be due to the ASR model architectures, as the middle influences both previous and subsequent word transcriptions, or due to speech re-synthesis which used a recurrent network in the WaveRNN vocoder. 0 20 40 60 80 100 120 Word Error Rate (WER %) 0.0000 0.0025 0.0050 0.0075 0.0100 0.0125 0.0150 0.0175 0.0200 Kernel Density Estimation WER Distributions for Masked VQVAE Re-Synthesis noise-end noise-mid noise-start deletion-end deletion-mid deletion-start reversal-end reversal-mid reversal-start Figure 2: Kernel density estimation of ↓WER% for WH- Medium on VQ-VAE re-synthesized speech that was masked. This plot shows the mask types and positions (averaged across all speakers). 4.2 Automatic Speaker Verification (ASV) In the ASV evaluation, we compare each utterance with all others, resulting in 460 target trials and 4096 non-target trials. The cosine similarity between the ECAPA-TDNN speaker embedding [26] of each utterance is used as the comparison score. Table 2 shows the results in terms of equal-error rate (EER%). EER designates the error rate where both the false acceptance and false rejection rate are equal. Lower EER scores indicate better discrimination performance. Because the number of speakers being tested is small, it is therefore delicate to draw general conclusions. However, we can already see from the first line (original speech), a slight drop of performance when the mask is applied in the middle of the sample with both noise and re- versal type on original speech. This could be due to the fact that the attention mechanism of ECAPA-TDNN, which has a similar role as a voice activity detector, does not properly ignore the mask when it is at the middle of an utterance. For the deletion mask, we do not observe this issue because the segment being protected is cut from the audio, so there is therefore nothing for the attention to try and ignore. From the second line (VQ-VAE speech), we can see that, without masking, applying the VQ-VAE significantly perturbs the speaker information with an increase of EER from 0.04 to 16.75. Applying the masking further degrades the ASV performance but without significant differences between mask type and position. In future works, analysing how the attention of the ECAPA-TDNN behaves against the mask could help in better understanding this drop of performance. 5 Summary and Future Work We have presented a framework and evaluation for masking speech content that uses both original and re-synthesised utterances. While masking natural speech represents an idealized baseline, the re-synthesized speech has been com- pressed into a rich latent space using VQ-VAE. We assessed how masking the utterance content can affect performance with ASR and ASV as two relevant downstream tasks. We showed that mask types and locations affect ASR and ASV performance differently. Our chosen toy problem masking technique allowed us to work directly with audio-only data, but the RNN-based vocoder suffers from modified VQ sequences as evidenced by poor WER performance mid-utterance. We therefore pro- pose that future experiments explore a variety of vocoders beyond WaveRNN. We were not able to compare impacts of masking across genders due to the small amount of data in our concept problem but we plan to investigate this in future work. Further, datasets other than VCTK may be affected by masking differently. Importantly, both the ASV and ASR tasks are impacted by the speech re-synthesis quality. We expect the field of speech synthesis to con- tinually improve. Our idea of content masking is based on speech compression and re-synthesis that can tolerate mid-utterance disruptions, similar to disfluencies or signal drop-outs. For real-world applications, we are interested to further explore masking methods that allow for recovery of the original content, including the role of an adversary. We are also interested to conduct future work that can address whether particular types of masks are more or less difficult for an attacker to discover the hidden content, including what kind of information an adversary would need in order to reconstruct hidden content. We would like to find out if we can trick a speech or speaker recognition system on purpose, based on content masking techniques. Acknowledgements This work was partially supported by the UK EPSRC Trustworthy Autonomous Systems Hub (EP/V00784X/1); a JST CREST Grant (JPMJCR18A6, VoicePersonae project), Japan; the Bavarian Ministry of Economic Affairs, Regional Development, and Energy as well as the German Federal Ministry for the Environment, Nature Conservation, Nu- clear Safety and Consumer Protection. References [1] Z. Tang, N. J. Bryan, D. Li, T. R. Langlois, and D. Manocha, “Scene-aware audio rendering via deep acoustic analysis,” IEEE Transactions on Visualization and Computer Graphics, vol. 26, no. 5, pp. 1991–2001, 2020. [2] N. Tomashenko, X. Wang, E. Vincent, J. Patino, B. M. L. Srivastava, P.-G. Noé, A. Nautsch, N. Evans, J. Yamagishi, et al., “The VoicePrivacy 2020 Challenge: Results and find- ings,” Computer Speech & Language, vol. 74, p. 101362, 2022. [3] A. Nautsch, C. Jasserand, E. Kindt, M. Todisco, I. Trancoso, and N. Evans, “The GDPR & Speech Data: Reflections of Legal and Technology Communities, First Steps Towards a Common Understanding,” in Proc. Interspeech, pp. 3695– 3699, 2019. [4] J. Williams, J. Yamagishi, P.-G. Noé, C. Valentini-Botinhao, and J.-F. Bonastre, “Revisiting Speech Content Privacy,” in 1st ISCA Symposium of the Security & Privacy in Speech Communication, 2021. [5] J. Williams, K. Pizzi, S. Das, and P.-G. Noé, “New Chal- lenges for Content Privacy in Speech and Audio,” in 2nd ISCA Symposium of the Security & Privacy in Speech Com- munication, 2022. [6] A. Van Den Oord, O. Vinyals, et al., “Neural discrete rep- resentation learning,” Advances in Neural Information Pro- cessing Systems, vol. 30, 2017. [7] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura, “VQ-VAE Unsupervised Unit Discovery and Multi-Scale Code2Spec Inverter for Zerospeech Challenge 2019,” Proc. Interspeech 2019, pp. 1118–1122, 2019. [8] J. Fong, J. Williams, and S. King, “Analysing Temporal Sensitivity of VQ-VAE Sub-Phone Codebooks,” in Proc. of the 11th ISCA Speech Synthesis Workshop (SSW11), pp. 27– 231, 2021. [9] D. Paul, Y. Pantazis, and Y. Stylianou, “Speaker Condi- tional WaveRNN: Towards Universal Neural Vocoder for Unseen Speaker and Recording Conditions,” Proc. Inter- speech, pp. 235–239, 2020. [10] J. F. de Andrade, M. L. de Campos, and J. A. Apolinario, “Speech privacy for modern mobile communication systems,” in 2008 IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 1777–1780, IEEE, 2008. [11] A. S. Bopardikar, Speech encryption using wavelet packets. Indian Institute of Science, 2005. [12] F. Ma, “Wavelet transform-based analogue speech scram- bling scheme,” Electron. Lett., vol. 32, no. 8, pp. 719–721, 1996. [13] N. Jayant, B. McDermott, S. Christensen, and A. Quinn, “A comparison of four methods for analog speech privacy,” IEEE Transactions on Communications, vol. 29, no. 1, pp. 18–23, 1981. [14] M. A. T. Turan, D. Klakow, E. Vincent, and D. Jouvet, “Adapting Language Models When Training on Privacy- Transformed Data,” in LREC 2022-13th Language Resources and Evaluation Conference, 2022. [15] J. Williams, J. Fong, E. Cooper, and J. Yamagishi, “Explor- ing Disentanglement with Multilingual and Monolingual VQ-VAE,” in Proc. of the 11th ISCA Speech Synthesis Work- shop (SSW11), pp. 124–129, 2021. [16] S. Ahmed, A. R. Chowdhury, K. Fawaz, and P. Ramanathan, “Preech: A System for {Privacy-Preserving} Speech Tran- scription,” in 29th USENIX Security Symposium (USENIX Security 20), pp. 2703–2720, 2020. [17] M. Dong, D. Yan, and R. Wang, “Adversarial Pri- vacy Protection on Speech Enhancement,” arXiv preprint arXiv:2206.08170, 2022. [18] J. Yamagishi, C. Veaux, K. MacDonald, et al., “CSTR VCTK Corpus: English Multi-Speaker Corpus for CSTR Voice Cloning Toolkit (version 0.92),” 2019. [19] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, “Montreal Forced Aligner: Trainable Text- Speech Alignment Using Kaldi,” Proc. Interspeech 2017, 2017. [20] J. Casebeer, V. Vale, U. Isik, J.-M. Valin, R. Giri, and A. Krishnaswamy, “Enhancing into the codec: Noise ro- bust speech coding with vector-quantized autoencoders,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 711–715, 2021. [21] J. Williams, Y. Zhao, E. Cooper, and J. Yamagishi, “Learning disentangled phone and speaker representations in a semi- supervised VQ-VAE paradigm,” in 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7053–7057, 2021. [22] M. Cooke, C. Mayo, C. Valentini-Botinhao, Y. Stylianou, B. Sauert, and Y. Tang, “Evaluating the intelligibility benefit of speech modifications in known noise conditions,” Speech Communication, vol. 55, no. 4, pp. 572–585, 2013. [23] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, et al., “SpeechBrain: A general-purpose speech toolkit,” 2021. arXiv:2106.04624. [24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust Speech Recognition via Large- Scale Weak Supervision,” tech. rep., Technical report, Ope- nAI, 2022. URL https://cdn. openai. com/papers/whisper. pdf, 2022. [25] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib- rispeech: an ASR corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206–5210, 2015. [26] B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA- TDNN: emphasized channel attention, propagation and ag- gregation in TDNN based speaker verification,” in Inter- speech 2020 (H. Meng, B. Xu, and T. F. Zheng, eds.), pp. 3830–3834, ISCA, 2020. "
}