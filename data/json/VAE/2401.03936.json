{
    "optim": "Exploratory Evaluation of Speech Content Masking\nJennifer Williams 1, Karla Pizzi 2,3, Paul-Gauthier Noé 4, Sneha Das 5\n1 Electronics and Computer Science, University of Southampton, UK\n2 Fraunhofer AISEC, Garching, Germany; 3 Technical University Munich, Germany\n4 Laboratoire Informatique d’Avignon (LIA), University of Avignon, France\n5 Dept. of Applied Mathematics and Computer Science, Technical University of Denmark, Denmark\nEmail: j.williams@soton.ac.uk, karla.pizzi@aisec.fraunhofer.de\nAbstract\nMost recent speech privacy efforts have focused on\nanonymizing acoustic speaker attributes but there has not\nbeen as much research into protecting information from\nspeech content. We introduce a toy problem that explores an\nemerging type of privacy called “content masking” which\nconceals selected words and phrases in speech. In our\nefforts to define this problem space, we evaluate an intro-\nductory baseline masking technique based on modifying\nsequences of discrete phone representations (phone codes)\nproduced from a pre-trained vector-quantized variational au-\ntoencoder (VQ-VAE) and re-synthesized using WaveRNN.\nWe investigate three different masking locations and three\ntypes of masking strategies: noise substitution, word dele-\ntion, and phone sequence reversal. Our work attempts to\ncharacterize how masking affects two downstream tasks:\nautomatic speech recognition (ASR) and automatic speaker\nverification (ASV). We observe how the different masks\ntypes and locations impact these downstream tasks and\ndiscuss how these issues may influence privacy goals.\n1 Introduction\nAudio recordings comprise private information in many\nways. In this paper, we focus on content-related informa-\ntion, ranging from cues about location or acoustic scene [1]\nto sensitive words and phrases. Speech data can therefore\nexpose privacy risks even if a speaker’s voice has been\nanonymized, since typical goals for voice anonymization in-\nclude preserving intelligibility [2]. Content involving words\nor phrases could be used (or misused) to reveal personal\ndata such as birth date or credit card details, or enable an\nattacker to identify a person based on spoken names or other\nsimilar references. Therefore, the capability to achieve fine-\ngrained control over masking particular aspects of speech\ndata beyond speaker voice characteristics must become\ncenter-stage for privacy protection research.\nIn this paper, we introduce a toy problem to explore\nthe concept of content privacy for speech, as a complement\nto the extensive ongoing efforts from the biennial Voice\nPrivacy Challenges1 (VPC). Systems developed through the\nVPC aim to conceal, remove, or neutralize speaker-related\nvoice attributes. While the VPC systems have many prac-\ntical uses, that singular form of privacy is not enough to\nachieve full anonymization, due to the ability to link spoken\ncontent. Consider the example of an audio recording in a\ndataset dealing with medical health issues with the follow-\ning utterance: “My name is James Smith and I have severe\ndepression.” Since the speaker reveals information in the\nspoken content, anonymizing the acoustic-related speaker\nattributes alone would not conceal the sensitive informa-\ntion (the person’s identity and health condition) since this\n1https://www.voiceprivacychallenge.org/.\ncan be obtained simply by analyzing the words. Sharing\nspeech data across research areas (e.g., from the medical re-\nsearch domain to the speech recognition domain) is difficult\ndue to compliance with GDPR [3] and other related ethical\nconcerns. Useful applications of content masking include\nsharing sensitive speech databases for research purposes,\nconcealing sensitive words or phrases from speech synthesis\nscreen readers, and disentanglement of speech features and\ncontent for more deeper privacy controls. If there was a way\nto achieve full anonymisation and redact all sensitive infor-\nmation reliably, this could make it more feasible to share\ndatasets that are important for speech research, ultimately\nbenefiting speech technology development. While targeted\naspects of the speech signal become incomprehensible from\nmasking, the speech signal should only be ‘destroyed’ on\nprivate information. Our approach serves as foundational\nwork, presenting a first-ever baseline approach using speech\nre-synthesis.\nSince we are introducing content privacy and its eval-\nuation in this paper, there are no established techniques or\nevaluation protocols [4, 5]. We adopt a re-synthesis tech-\nnique based on vector-quantized variational autoencoders\n(VQ-VAE) [6] and present baselines for masking content.\nWe do not propose that VQ-VAE is the best content mask-\ning approach, but instead we assess whether re-synthesis\ntechniques could be useful. The VQ phone codes [7] are se-\nquential and preserve temporal structure of speech, meaning\nthat a string of phones is temporally related to the sequence\nof words [8]. To mask target words in the speech, we modify\nsub-strings of the VQ sequence and re-synthesize from the\nVQ codes using WaveRNN [9]. In this paper, we explore\nmasking using different types of masks (noise, deletion, and\nreversal), and we target words at different locations in the\nutterances (start, middle, and end). We evaluate masking\nusing two downstream tasks: automatic speech recognition\n(ASR), and automatic speaker verification (ASV). We in-\nvestigated ASR and ASV performance to explore how this\ntype of problem could be evaluated.\nWe also perform masking on original natural speech by\nremoving or replacing segments corresponding to the three\ntypes of mask. This baseline simulates perfect masking\nwith perfect quality of speech, to compare against the VQ-\nVAE speech re-synthesis technique. The contributions of\nthis paper are:\n• Explore how to modify and replace sequences of VQ-\nVAE phone codes to conceal content of selected words\nand phrases\n• Understand how masking words in an utterance will af-\nfect downstream tasks such as ASR and ASV by mask-\ning original speech (assumes perfect synthesis) as well\nas re-synthesis from VQ-VAE\narXiv:2401.03936v1  [eess.AS]  8 Jan 2024\n2 Related Work\nPrevious work in masking speech content involved conceal-\ning conversations that take place indoors using a technique\ncalled scrambling [10]. This technique was reported to be\nvery effective at masking speech in order to make it un-\navailable to eavesdroppers [11–13]. One of the issues is\nthat the method depends on knowing the room characteris-\ntics beforehand. The method does not generalise to other\nuse-cases wherein we may want a finer-grained approach\nto conceal content without disturbing neighboring words or\nwhile preserving speaker voice characteristics.\nRecent research has explored content privacy for ASR,\nspecifically for training language models on texts that have\nhad named entities redacted due to privacy [14]. Their\nredaction technique (called sanitization) involved swap-\nping one named entity for another. They found that the\ntechnique results in significant increase in word error rate\n(WER) when training new language models. The drop in\nperformance is likely due to the sequential nature of lan-\nguage modeling. The authors did not explore other types of\nsanitization. With this paper, we aim at filling this gap. We\nuse a method presented in [15]. Here, content masking us-\ning VQ-VAE was mentioned as being a possible technique\nfor masking. However, their results are difficult to inter-\npret because the evaluation involved human judgments of\nspeaker similarity and WER between a very small sample\nof original and masked utterances.\nA privacy-preserving ASR system, called Prεεch [16],\nwas recently proposed to preserve voice biometrics and\ncontent privacy for offline and cloud-based ASR services.\nSpecifically, the system employs a combination of segmen-\ntation and shuffling of signals, identification and removal\nof sensitive information, and injection of dummy phrases\nto preserve content privacy. Unfortunately, the data is trans-\nformed into a ‘bag of words’ which may make it less useful\nfor further downstream applications outside of ASR.\nBeyond this, adversarial techniques for speech infor-\nmation hiding have been recently proposed in [17], which\ndemonstrates a method to encode utterances into a database\nand recover them later using a neural network. The ap-\nproach relies heavily on forced alignments. Further, they do\nnot assess impacts on downstream tasks nor do they conceal\nwords/phrases at specific positions in an utterance. Whether\nprivate information should be recoverable depends upon\nthe use-case. In our work, we consider both cases where\nrecovery is and is not desirable. Recovery is especially un-\ndesirable for sensitive private information due to adversarial\nattacks.\n3 Masking Methodology\n3.1 Data and Pre-Processing\nThe data that we use in this paper was the Voice Cloning\nToolkit (VCTK) v0.92 [18].\nThe dataset contains 110\nunique speakers, both male and female, of varying ages\nand English accents. We chose the VCTK dataset because\nit is studio-quality speech, which helps us establish our\ncontent masking baselines. We obtained forced alignments\nfor each audio file from the Montreal forced aligner [19].\nAs this paper explores the concept of content masking, we\nmade use of forced alignments to ensure that we were able\nto target specific words at locations in each utterance. In\nreal-world applications of content masking, it may not be\npossible or desireable to use forced alignments, and instead\na technique such as keyword spotting would be more appro-\npriate depending on the specific application scenario.\n3.2 Masking Technique\nWe compare masking for two types of speech: original\n(natural) utterances and re-synthesized utterances. Words\nare masked in original speech by relying on forced align-\nments. This provides us with high-quality masked speech,\nand sets an upper-bound as if the speech re-synthesis and\nlearned latent representations were of perfect quality. We\nare interested in VQ-VAE speech re-synthesis because this\ntechnique can learn rich latent representations using autoen-\ncoders. For our baselines involving natural speech from\nVCTK, we replace segments of natural speech with the\nmasks. For baselines involving speech re-synthesis with\nVQ-VAE, we modify sequences of VQ phone codes that\nrepresenting the target words to be masked.\nOne of the benefits of using VQ-VAE is that it allows us\nto consider content masking while speech is in a coded or\ncompressed state [20], wherein the masking is done strictly\nby manipulating sequences of VQ phone codes. This mim-\nics realistic scenarios where speech may be compressed\nor in transmission and we may have some temporal infor-\nmation or keys to designate which sequences of codes to\nconceal for privacy purposes. The VQ-VAE system that we\nused in this paper comes from [21]. It is a dual-encoder\nmodel which models phone content information and speaker\nidentity. Using that system and pre-trained model, we ex-\nplored three different locations of the mask within an utter-\nance, and three types of masks.\n3.3 Mask Types\nMany types of masks are relevant to speech content masking.\nFor example, it is common in live television to replace\ncertain offensive words with a long ‘beep’ sound [4]. In\nthis work, we examine three types of masks and describe\nthem here. Note that all mask types can change the usual\nstructure of the sentence, which might potentially lead to\nwrong transcriptions if an automatic speech recognition\n(ASR) system with a language model is in place.\nNoise. The noise mask involves replacing content with a\ntemporally-modulated speech-shaped noise masker (ICRA\nnoise 9 from [22]). This was proposed by [21], however\nthey did not evaluate how masking affects downstream tasks.\nWhen masking original speech, we use forced alignments\nas a guide to replace segments of the natural speech with an\nidentical length segment of the speech-shaped noise masker.\nWhen masking re-synthesized speech using VQ codes, we\nfirst obtain VQ codes corresponding to noise-only and then\nswap the sequence of VQ codes (of the same length).\nDeletion. A deletion mask involves deleting content en-\ntirely from an utterance and the content will not be recover-\nable. For natural speech, we slice the waveform and remove\nthe segment that contains the target words. For re-synthesis,\nwe remove the VQ phone code sequence corresponding to\nthe target words, before synthesis. We include it in this\npaper in order to assess impacts on speech re-synthesis as\nwell as impacts on the downstream tasks.\nReversal. We explore a third mask type that reverses\ntarget words in the time-domain. For original speech, this\ninvolves slicing the waveform and reversing only the seg-\nment that corresponds to the target words. For re-synthesis,\nwe simply reverse the sequence of VQ codes while keeping\nthe surrounding VQ code sequences in-tact.\n3.4 Materials for Evaluation\nWe compiled a subset of speech samples with and without\nmasking. The speakers and utterances were selected from\nthe “condition #3” that was reported by the authors who\nprovided the system and pre-trained model [21]. This par-\nticular condition contained samples wherein the speakers\nwere unseen during training and the content was seen dur-\ning training. We selected this subset since we are especially\ninterested in content masking, but wanted to avoid using\ndata that had been fully seen during training of the VQ-VAE\nsystem. From this subset, we selected 9 speakers (p260,\np285, p294, p300, p305, p307, p310, p347, and p351).\nThese speakers were selected because they had an average\nspeaking rate of < 5 words/second, the total duration of the\nutterances contained > 300 VQ phone codes (1.2 seconds),\nand number of words spoken was ≥ 7. When counting\nwords, we ignored all “SIL” tokens that were generated\nfrom forced alignments by the Montreal aligner. With these\nspeakers and utterances, we applied the masking to create\nsamples for downstream evaluation2.\n4 Masking Evaluation\n4.1 Automatic Speech Recognition (ASR)\nTo measure the effect of masking on speech recognition, we\nemployed four state-of-the-art ASR models: two Speech-\nBrain systems [23] and two Whisper systems [24]. The\ntwo SpeechBrain systems are sequence to sequence-based\nmodels with a combination of convolutional, recurrent, and\nfully-connected networks (CRDNN) trained on LibriSpeech\n[25]: (1) a CRDNN with an RNN-based language model\n(SB-RNN)3; and (2) a CRDNN with a transformer-based\nlanguage model (SB-Transformer)4. For Whisper, we\nused the “base” (WH-Base) as well as the “medium” (WH-\nMedium) models. Both Whisper models are end-to-end\nencoder-decoder transformer models. Due to the architec-\nture and the variety of data Whisper has been trained on, it\nexhibits improved robustness to accents, background noise\nand technical language [24].\nOriginal\nOriginal Speech + Masking\nSystem\nSpeech\nNoise\nDeletion\nReversal\nSB-RNN\n13.9\n22.9\n20.6\n37.5\nSB-Trans.\n10.5\n19.6\n17.2\n36.4\nWH-Base\n4.15\n10.6\n10.5\n32.2\nWH-Medium\n1.24\n9.30\n7.72\n24.7\nVQ-VAE\nVQ-VAE + Masking\nSystem\nSpeech\nNoise\nDeletion\nReversal\nSB-RNN\n47.5\n75.5\n65.2\n78.3\nSB-Trans.\n38.8\n76.0\n59.0\n75.4\nWH-Base\n47.7\n78.7\n66.2\n79.5\nWH-Medium\n32.3\n63.0\n55.3\n69.1\nTable 1:\nASR system ↓WER% for original and re-\nsynthesised un-masked and masked utterances.\nWe examine the impact of masking on ASR by mea-\nsuring word error rate (WER%) while comparing the four\ndifferent ASR models, as shown in Table 1. First, we eval-\nuated each ASR system using un-masked speech from the\n2https://rhoposit.github.io/ITG2023\n3https://huggingface.co/speechbrain/\nasr-crdnn-rnnlm-librispeech\n4https://huggingface.co/speechbrain/\nasr-crdnn-transformerlm-librispeech\n0\n20\n40\n60\n80\n100\n120\nWord Error Rate (WER %)\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nKernel Density Estimation\nWER Distributions for Masked Original Speech\nnoise-end\nnoise-mid\nnoise-start\ndeletion-end\ndeletion-mid\ndeletion-start\nreversal-end\nreversal-mid\nreversal-start\nFigure 1: Kernel density estimation of ↓WER% for WH-\nMedium on original speech that was masked. This plot\nshows the mask types and positions (averaged across all\nspeakers).\noriginal and VQ-VAE re-synthesis to establish a benchmark\nof performance on the VCTK dataset, using the original\nVCTK transcripts as reference. Next, we evaluated ASR\non masked speech using the masking techniques described\nin Section 3.2, using VCTK transcripts that had the target\n(masked) words removed, as the reference. In some cases,\nASR systems demonstrated catastrophic failure, such as re-\npeating words 5 or more times, which occurred most often\nwith the noise and reversal masks. We omitted utterances\nwherein the ASR transcription was more than 30 characters\nlonger than the reference transcript. Even with this quality\ncontrol in place, we did occasionally observe WER values\nabove 100%, but less so with the WH-Medium model.\nThe WER for un-masked audio and un-masked tran-\nscription represents our empirical WER. We conducted a\npaired t-test (p ≪ 0.05, 95% confidence) for each ASR sys-\ntem separately, and found that the results are significant\nexcept for original masked speech, where noise and dele-\ntion masks are not statistically significant. Likewise, the\ndifferences between noise and reversal are not significant\nfor VQ-VAE masked speech for the SpeechBrain models.\nFor SB-RNN and SB-Transformer, these perform worse\noverall compared to the Whisper models. This is likely\ndue to SpeechBrain models being trained on LibriSpeech,\nwhereas Whisper models are trained on a diverse dataset,\nincluding different English dialects. All models indicated\nworse performance when reversal masking was used. Is-\nsues with reversal masking may result from audio retaining\nspeech features, as tonal and atonal sounds are roughly\nkept, while the ASR system is not able to detect meaningful\nwords. This is especially problematic if a language model\nis in place.\nFrom these initial results, we further characterized ASR\nevaluation using only WH-Medium since it performed\nbest on our original and VQ-VAE speech. We show the\nkernel density estimations (KDE)5 of the distributions of\nper-utterance WER scores in Figure 1 and Figure 2. These\n5Gaussian KDE has an unbounded support ]−∞,+∞[. Because the\nWER is positive, we estimate the density of the log WER. The estimated\ndensity is then mapped back in the WER domain using the change of\nvariable formula and nomalization. As it is a density estimation (a curve\nfit to a histogram) there is typical congestion at zero.\nNone\nWith Masking\nMask position\n–\nStart\nMiddle\nEnd\nMask type\nNoise\nDelete\nReversal\nNoise\nDelete\nReversal\nNoise\nDeletion\nReversal\nOriginal\n0.04\n0.02\n0.08\n0.13\n0.27\n0.07\n0.17\n0.07\n0.00\n0.00\nVQ-VAE\n16.75\n19.65\n22.53\n23.35\n21.81\n19.85\n23.13\n23.89\n22.29\n21.80\nTable 2: Comparison of ASV performance measuring ↓EER% using original un-masked enrollment utterances and either\noriginal or VQ-VAE masked test utterances.\ntwo figures are showing that certain mask positions and\ntypes result in different WER. In the original masked speech\n(Fig. 1), the noise mask at the end of an utterance had the\nleast negative impact on ASR performance whereas the\nreversal at the middle of an utterance generally had a large\nimpact. Performance on VQ-VAE speech for the WH-\nmedium model (Fig. 2) also shows that WER differs by\nmask location. The highest WERs came from masking\nwords in the middle of a sentence. This may be due to\nthe ASR model architectures, as the middle influences both\nprevious and subsequent word transcriptions, or due to\nspeech re-synthesis which used a recurrent network in the\nWaveRNN vocoder.\n0\n20\n40\n60\n80\n100\n120\nWord Error Rate (WER %)\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\n0.0175\n0.0200\nKernel Density Estimation\nWER Distributions for Masked VQVAE Re-Synthesis\nnoise-end\nnoise-mid\nnoise-start\ndeletion-end\ndeletion-mid\ndeletion-start\nreversal-end\nreversal-mid\nreversal-start\nFigure 2: Kernel density estimation of ↓WER% for WH-\nMedium on VQ-VAE re-synthesized speech that was\nmasked. This plot shows the mask types and positions\n(averaged across all speakers).\n4.2 Automatic Speaker Verification (ASV)\nIn the ASV evaluation, we compare each utterance with all\nothers, resulting in 460 target trials and 4096 non-target\ntrials. The cosine similarity between the ECAPA-TDNN\nspeaker embedding [26] of each utterance is used as the\ncomparison score. Table 2 shows the results in terms of\nequal-error rate (EER%). EER designates the error rate\nwhere both the false acceptance and false rejection rate are\nequal. Lower EER scores indicate better discrimination\nperformance. Because the number of speakers being tested\nis small, it is therefore delicate to draw general conclusions.\nHowever, we can already see from the first line (original\nspeech), a slight drop of performance when the mask is\napplied in the middle of the sample with both noise and re-\nversal type on original speech. This could be due to the fact\nthat the attention mechanism of ECAPA-TDNN, which has\na similar role as a voice activity detector, does not properly\nignore the mask when it is at the middle of an utterance. For\nthe deletion mask, we do not observe this issue because the\nsegment being protected is cut from the audio, so there is\ntherefore nothing for the attention to try and ignore. From\nthe second line (VQ-VAE speech), we can see that, without\nmasking, applying the VQ-VAE significantly perturbs the\nspeaker information with an increase of EER from 0.04\nto 16.75. Applying the masking further degrades the ASV\nperformance but without significant differences between\nmask type and position. In future works, analysing how the\nattention of the ECAPA-TDNN behaves against the mask\ncould help in better understanding this drop of performance.\n5 Summary and Future Work\nWe have presented a framework and evaluation for masking\nspeech content that uses both original and re-synthesised\nutterances. While masking natural speech represents an\nidealized baseline, the re-synthesized speech has been com-\npressed into a rich latent space using VQ-VAE. We assessed\nhow masking the utterance content can affect performance\nwith ASR and ASV as two relevant downstream tasks. We\nshowed that mask types and locations affect ASR and ASV\nperformance differently.\nOur chosen toy problem masking technique allowed us\nto work directly with audio-only data, but the RNN-based\nvocoder suffers from modified VQ sequences as evidenced\nby poor WER performance mid-utterance. We therefore pro-\npose that future experiments explore a variety of vocoders\nbeyond WaveRNN. We were not able to compare impacts\nof masking across genders due to the small amount of data\nin our concept problem but we plan to investigate this in\nfuture work. Further, datasets other than VCTK may be\naffected by masking differently. Importantly, both the ASV\nand ASR tasks are impacted by the speech re-synthesis\nquality. We expect the field of speech synthesis to con-\ntinually improve. Our idea of content masking is based\non speech compression and re-synthesis that can tolerate\nmid-utterance disruptions, similar to disfluencies or signal\ndrop-outs. For real-world applications, we are interested to\nfurther explore masking methods that allow for recovery of\nthe original content, including the role of an adversary. We\nare also interested to conduct future work that can address\nwhether particular types of masks are more or less difficult\nfor an attacker to discover the hidden content, including\nwhat kind of information an adversary would need in order\nto reconstruct hidden content. We would like to find out\nif we can trick a speech or speaker recognition system on\npurpose, based on content masking techniques.\nAcknowledgements\nThis work was partially supported by the UK EPSRC\nTrustworthy Autonomous Systems Hub (EP/V00784X/1); a\nJST CREST Grant (JPMJCR18A6, VoicePersonae project),\nJapan; the Bavarian Ministry of Economic Affairs, Regional\nDevelopment, and Energy as well as the German Federal\nMinistry for the Environment, Nature Conservation, Nu-\nclear Safety and Consumer Protection.\nReferences\n[1] Z. Tang, N. J. Bryan, D. Li, T. R. Langlois, and D. Manocha,\n“Scene-aware audio rendering via deep acoustic analysis,”\nIEEE Transactions on Visualization and Computer Graphics,\nvol. 26, no. 5, pp. 1991–2001, 2020.\n[2] N. Tomashenko, X. Wang, E. Vincent, J. Patino, B. M. L.\nSrivastava, P.-G. Noé, A. Nautsch, N. Evans, J. Yamagishi,\net al., “The VoicePrivacy 2020 Challenge: Results and find-\nings,” Computer Speech & Language, vol. 74, p. 101362,\n2022.\n[3] A. Nautsch, C. Jasserand, E. Kindt, M. Todisco, I. Trancoso,\nand N. Evans, “The GDPR & Speech Data: Reflections of\nLegal and Technology Communities, First Steps Towards a\nCommon Understanding,” in Proc. Interspeech, pp. 3695–\n3699, 2019.\n[4] J. Williams, J. Yamagishi, P.-G. Noé, C. Valentini-Botinhao,\nand J.-F. Bonastre, “Revisiting Speech Content Privacy,” in\n1st ISCA Symposium of the Security & Privacy in Speech\nCommunication, 2021.\n[5] J. Williams, K. Pizzi, S. Das, and P.-G. Noé, “New Chal-\nlenges for Content Privacy in Speech and Audio,” in 2nd\nISCA Symposium of the Security & Privacy in Speech Com-\nmunication, 2022.\n[6] A. Van Den Oord, O. Vinyals, et al., “Neural discrete rep-\nresentation learning,” Advances in Neural Information Pro-\ncessing Systems, vol. 30, 2017.\n[7] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and\nS. Nakamura, “VQ-VAE Unsupervised Unit Discovery and\nMulti-Scale Code2Spec Inverter for Zerospeech Challenge\n2019,” Proc. Interspeech 2019, pp. 1118–1122, 2019.\n[8] J. Fong, J. Williams, and S. King, “Analysing Temporal\nSensitivity of VQ-VAE Sub-Phone Codebooks,” in Proc. of\nthe 11th ISCA Speech Synthesis Workshop (SSW11), pp. 27–\n231, 2021.\n[9] D. Paul, Y. Pantazis, and Y. Stylianou, “Speaker Condi-\ntional WaveRNN: Towards Universal Neural Vocoder for\nUnseen Speaker and Recording Conditions,” Proc. Inter-\nspeech, pp. 235–239, 2020.\n[10] J. F. de Andrade, M. L. de Campos, and J. A. Apolinario,\n“Speech privacy for modern mobile communication systems,”\nin 2008 IEEE International Conference on Acoustics, Speech\nand Signal Processing, pp. 1777–1780, IEEE, 2008.\n[11] A. S. Bopardikar, Speech encryption using wavelet packets.\nIndian Institute of Science, 2005.\n[12] F. Ma, “Wavelet transform-based analogue speech scram-\nbling scheme,” Electron. Lett., vol. 32, no. 8, pp. 719–721,\n1996.\n[13] N. Jayant, B. McDermott, S. Christensen, and A. Quinn,\n“A comparison of four methods for analog speech privacy,”\nIEEE Transactions on Communications, vol. 29, no. 1,\npp. 18–23, 1981.\n[14] M. A. T. Turan, D. Klakow, E. Vincent, and D. Jouvet,\n“Adapting Language Models When Training on Privacy-\nTransformed Data,” in LREC 2022-13th Language Resources\nand Evaluation Conference, 2022.\n[15] J. Williams, J. Fong, E. Cooper, and J. Yamagishi, “Explor-\ning Disentanglement with Multilingual and Monolingual\nVQ-VAE,” in Proc. of the 11th ISCA Speech Synthesis Work-\nshop (SSW11), pp. 124–129, 2021.\n[16] S. Ahmed, A. R. Chowdhury, K. Fawaz, and P. Ramanathan,\n“Preech: A System for {Privacy-Preserving} Speech Tran-\nscription,” in 29th USENIX Security Symposium (USENIX\nSecurity 20), pp. 2703–2720, 2020.\n[17] M. Dong, D. Yan, and R. Wang, “Adversarial Pri-\nvacy Protection on Speech Enhancement,” arXiv preprint\narXiv:2206.08170, 2022.\n[18] J. Yamagishi, C. Veaux, K. MacDonald, et al., “CSTR VCTK\nCorpus: English Multi-Speaker Corpus for CSTR Voice\nCloning Toolkit (version 0.92),” 2019.\n[19] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and\nM. Sonderegger, “Montreal Forced Aligner: Trainable Text-\nSpeech Alignment Using Kaldi,” Proc. Interspeech 2017,\n2017.\n[20] J. Casebeer, V. Vale, U. Isik, J.-M. Valin, R. Giri, and\nA. Krishnaswamy, “Enhancing into the codec: Noise ro-\nbust speech coding with vector-quantized autoencoders,” in\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 711–715, 2021.\n[21] J. Williams, Y. Zhao, E. Cooper, and J. Yamagishi, “Learning\ndisentangled phone and speaker representations in a semi-\nsupervised VQ-VAE paradigm,” in 2021 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP), pp. 7053–7057, 2021.\n[22] M. Cooke, C. Mayo, C. Valentini-Botinhao, Y. Stylianou,\nB. Sauert, and Y. Tang, “Evaluating the intelligibility benefit\nof speech modifications in known noise conditions,” Speech\nCommunication, vol. 55, no. 4, pp. 572–585, 2013.\n[23] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell,\nL. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong,\net al., “SpeechBrain: A general-purpose speech toolkit,”\n2021. arXiv:2106.04624.\n[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey,\nand I. Sutskever, “Robust Speech Recognition via Large-\nScale Weak Supervision,” tech. rep., Technical report, Ope-\nnAI, 2022. URL https://cdn. openai. com/papers/whisper.\npdf, 2022.\n[25] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib-\nrispeech: an ASR corpus based on public domain audio\nbooks,” in 2015 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), pp. 5206–5210,\n2015.\n[26] B. Desplanques, J. Thienpondt, and K. Demuynck, “ECAPA-\nTDNN: emphasized channel attention, propagation and ag-\ngregation in TDNN based speaker verification,” in Inter-\nspeech 2020 (H. Meng, B. Xu, and T. F. Zheng, eds.),\npp. 3830–3834, ISCA, 2020.\n"
}