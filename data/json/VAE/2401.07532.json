{
    "optim": "MULTI-VIEW MIDIVAE: FUSING TRACK- AND BAR-VIEW REPRESENTATIONS FOR\nLONG MULTI-TRACK SYMBOLIC MUSIC GENERATION\nZhiwei Lin1,2,‚àó, Jun Chen1,3,‚àó, Boshi Tang1, Binzhu Sha1, Jing Yang2, Yaolong Ju2,\nFan Fan2, Shiyin Kang3, Zhiyong Wu1,4,‚Ä†, Helen Meng4\n1 Shenzhen International Graduate School, Tsinghua University, Shenzhen, China\n2 Huawei Technologies Co., Ltd., Shenzhen, China\n3Skywork AI PTE. LTD.\n4 The Chinese University of Hong Kong, Hong Kong SAR, China\n{lzw22, y-chen21}@mails.tsinghua.edu.cn, zywu@sz.tsinghua.edu.cn\nABSTRACT\nVariational Autoencoders (VAEs) constitute a crucial component of\nneural symbolic music generation, among which some works have\nyielded outstanding results and attracted considerable attention.\nNevertheless, previous VAEs still encounter issues with overly long\nfeature sequences and generated results lack contextual coherence,\nthus the challenge of modeling long multi-track symbolic music\nstill remains unaddressed.\nTo this end, we propose Multi-view\nMidiVAE, as one of the pioneers in VAE methods that effectively\nmodel and generate long multi-track symbolic music. The Multi-\nview MidiVAE utilizes the two-dimensional (2-D) representation,\nOctupleMIDI, to capture relationships among notes while reducing\nthe feature sequences length. Moreover, we focus on instrumental\ncharacteristics and harmony as well as global and local information\nabout the musical composition by employing a hybrid variational\nencoding-decoding strategy to integrate both Track- and Bar-view\nMidiVAE features. Objective and subjective experimental results\non the CocoChorales dataset demonstrate that, compared to the\nbaseline, Multi-view MidiVAE exhibits significant improvements in\nterms of modeling long multi-track symbolic music.\nIndex Terms‚Äî symbolic music generation, long multi-track,\nMulti-view MidiVAE\n1. INTRODUCTION\nAutomatic music generation is a prominent and active research field,\nwhich has a wide range of applications from human-computer in-\nteractive composition to commercial composition software [1]. In\nrecent years, the advancement of deep learning technology further\npromotes the development of auto music generation. Generally, au-\ntomatic music generation can be categorized into symbolic music\ngeneration [2‚Äì6] and music audio generation [7‚Äì12]. While music\naudio generation directly models continuous audio signals, symbolic\nmusic generation models and synthesizes music as a sequence of\nsymbols, which by nature is more conducive to subsequent music\nediting and adjustments [13]. Therefore, symbolic music generation\nstays an important research topic.\nAmong symbolic music generation methods, the VAE ap-\nproaches [14‚Äì18] have been widely applied due to their promising\nperformance. Additionally, as conditional and multi-modal music\ngeneration becomes increasingly popular, VAEs can also serve as\nan essential component for the extraction and representation of la-\ntent features [19]. Consequently, the VAE-based symbolic music\n*These authors contributed equally to this work as first authors.\n‚Ä† Corresponding author.\ngeneration methods have garnered widespread attention.\nMusic-\nVAE [14] employs a Long Short-Term Memory (LSTM)-based VAE\narchitecture to effectively reconstruct and generate symbolic music.\nHowever, due to the instrument-specific Encoder and Decoder de-\nsign, MusicVAE is restricted to music generation within designated\ninstruments, which significantly limits its applicability. In response\nto this limitation, the Multi-track MusicVAE [20] incorporates the\nMIDI-like [21] event-based representation that can denote different\ninstruments, and utilizes a universal Encoder and Decoder for mod-\neling. This innovation successfully broadens the scope of symbolic\nmusic reconstruction and generation from specific instruments to\narbitrary instruments, establishing it as one of the most effective\nVAE-based symbolic music generation methods so far.\nDespite the impressive performance, VAE approaches still strug-\ngle with the emerging challenge of modeling long multi-track sym-\nbolic music. The MIDI-like [21] input to the Multi-track Music-\nVAE is a one-dimensional (1-D) representation neglecting the rela-\ntionships among notes, which potentially impairs the model‚Äôs learn-\ning for the musical composition. More critically, converting long\nmulti-track symbolic music into this representation leads to exces-\nsively long feature sequences, which substantially increases the bur-\nden on model training and inference. Furthermore, when tackling\nlong multi-track symbolic music, the Multi-track MusicVAE can\nonly process one bar at a time, which results in the final generated\nmusic lack of contextual consistency and coherence, severely deteri-\norating the quality of the symbolic music.\nIn this paper, to confront the challenges above, we propose\nMulti-view MidiVAE, a pioneer VAE that efficiently models and\ngenerates the long multi-track symbolic music. In light of the is-\nsues associated with the aforementioned MIDI-like features, we\nintroduce a 2-D representation called OctupleMIDI [22]. This rep-\nresentation takes the relationships among notes into account while\nreducing the sequence length, subsequently alleviating the complex-\nity of feature extraction and reconstruction of the model. Moreover,\nwe model from dual views: 1) To focus on instrumental charac-\nteristics and harmony as well as the global information of musical\ncomposition, we design a transformer-based [23] Track-view Midi-\nVAE, which adopts an intra- and inter-instrument modeling over\nthe musical composition; 2) Alternatively, the transformer-based\nBar-view MidiVAE implements an intra- and inter-bar architecture,\nconcentrating on the finer details of notes within bars.\nEventu-\nally, through a hybrid variational encoding-decoding strategy, we\nintegrate the outputs from these two VAE views to form our final\nMulti-view MidiVAE model. Objective and subjective experimental\nresults on the CocoChorales dataset [24] validate the efficacy of our\narXiv:2401.07532v1  [cs.SD]  15 Jan 2024\nloss\nTrack-view \nEncoder\nBar-view\nEncoder\n‚Ñét\nùëÜùë°\nùëÜùëè\nùëÜ\n‚Ñéùëè\nConv\n1d\nz\nTrack-view \nDecoder\nBar-view\nDecoder\nPt\nPb\n·àöùëÜùë°\n·àöùëÜb\n·àöùëÜ\nùõº\n1-ùõº\nloss\nloss\nlinear\nlinear\nùúá\nùúé\nOctupleMIDI\nP\nMIF\nAFF\nFig. 1. The overall diagram of the proposed Multi-view MidiVAE. The model mainly contains Track- and Bar-view encoders, a multi-view\ninformation fusion (MIF), Track- and Bar-view decoders as well as an adaptive feature fusion (AFF).\nimprovements, demonstrating that the Multi-view MidiVAE is in-\ndeed a potent VAE approach for modeling long multi-track symbolic\nmusic.\n2. METHODOLOGY\nWe focus on the task of long multi-track music generation, i.e., gen-\nerating multi-bar music with various instruments played together.\nTo this end, we propose Multi-view MidiVAE that effectively mod-\nels and generates long multi-track symbolic music. As shown in\nFig.1, our Multi-view MidiVAE adopts a hybrid variational encoder-\ndecoder architecture, which mainly contains Track- and Bar-view\nencoders, a multi-view information fusion, Track- and Bar-view de-\ncoders as well as an adaptive feature fusion. More specifically, the\ninput OctupleMIDI sequences S are transformed into track-view se-\nquences St and bar-view sequences Sb. These two sequences are fed\ninto the Track- and Bar-view encoders to extract track-view infor-\nmation contained in feature ht and bar-view information included in\nfeature hb. By means of concatenating ht and hb, and passing them\nthrough a Conv1d layer, we achieve multi-view information fusion\nand then obtain the hybrid embedding that is utilized to get the mean\n¬µ and variance œÉ of the latent distribution z. After that, the Track-\nand Bar-view decoders generate probability matrix Pt and Pb. Even-\ntually, the adaptive feature fusion fuses Pt and Pb into the final prob-\nability matrix P for generating the reconstruction sequences eS. We\nwill elaborate on our improvements in the following subsections.\nDur11\nPitch35\nInstoboe\nBar0\n+\n+\n+\n+\nDur4\nPitch38\nInstoboe\nBar0\n+\n+\n+\n+\nDur8\nPitch33\nInsttuba\nBar0\n+\n+\n+\n+\nDur10\nPitch32\nInstviola\nBar0\n+\n+\n+\n+\nDur16\nPitch26\nInstoboe\nBar1\n+\n+\n+\n+\n1\n2\n3\n4\n5\n...\n...\n...\n...\n...\nFig. 2. The schematic diagram of OctupleMIDI. The ‚ÄúDur‚Äù and\n‚ÄúInst‚Äù mean duration and instrument respectively.\n2.1. OctupleMIDI Representation\nPrevious 1-D event-based representation [21] of Multi-track Mu-\nsicVAE exhibits certain limitations. First of all, the representation\ntreats all musical attributes equally, thus limiting the model‚Äôs abil-\nity to understand the differences and connections between attributes.\nBesides, it makes it difficult to capture complex relationships be-\ntween notes, e.g., accurately representing the simultaneous play of\ntwo notes.\nFinally, for processing the long multi-track symbolic\nmusic, the 1-D representation increases computational resource and\ntime expenses.\nIn order to address the aforementioned problems, we adopt the\nOctupleMIDI representation [22], which consists of sequences of\noctuple tokens. As shown in Fig.2, each token consists of eight mu-\nsic attributes, namely pitch, velocity, duration, instrument, position,\nbar, time signature and tempo. The representation employs the note\nas the fundamental unit and consolidates all musical attributes into a\nsingle octuple token. This enables the model to differentiate music\nattributes and effectively capture relationships between notes, while\nefficiently reducing sequence length.\n2.2. Bar-view MidiVAE\nGenerating long music usually involves long feature sequences,\nwhich makes it difficult to model the details of the musical com-\nposition [13]. For the purpose of allowing the model to fully learn\nthe local details about the musical composition, we design Bar-view\nMidiVAE, which consists of a hierarchical structure of encoder and\ndecoder.\nFig.3(a) shows the details of the Bar-view MidiVAE. The\nBar-view encoder is made up of two parts:\na weight-sharing\nintra-bar encoder and an inter-bar encoder.\nBoth of them com-\nprise of a transformer encoder and a multi-head attention module.\nThe intra-bar encoder is designed to model the bar embeddings\n\b\nE1, E2, . . . , EB|Ei ‚àà RDb, 1 ‚â§ i ‚â§ B\n\t\n, where Db represents\nthe dimension of the embedding and B denotes the number of bars.\nDue to the obvious temporal relationship between bars, we add\npositional encoding to the bar embeddings and then pass them into\nthe inter-bar encoder to model the inter-bar relationships. Finally,\nwe utilize a learnable query parameter to capture bar-view embed-\nding hb. There are two components inside the Bar-view decoder: a\nweight-sharing transformer decoder-based intra-bar decoder and a\nmutli-head attention-based inter-bar decoder. The inter-bar decoder\nregards z as the key and value, and uses a learnable query param-\neter with positional encoding to model guidance bar embeddings\nn\neE1, eE2, . . . , eEB| eEi ‚àà RDb\no\n. To guide the reconstruction of the\ncorresponding subsequence, the guidance embeddings are fed to the\nintra-bar decoder one after another. From the view of bar, Bar-view\nMidiVAE effectively model the finer details of notes within bars and\nthe local information of the music composition.\n2.3. Track-view MidiVAE\nIn multi-track music, there are multiple instruments played simulta-\nneously. Notes within each instrument collaborate with each other,\nwhile the instruments themselves perform independently yet har-\nmoniously. Therefore, it is crucial to model the characteristics of\neach instrument and their relationships, which is the key to captur-\ning global information of the musical composition [13].\nTo this end, we design a Track-view MidiVAE that consists of a\nhierarchical structure of encoder and decoder, as shown in Fig.3(b).\nThe Track-view encoder consists of two components: a parameter-\nsharing intra-instrument encoder and an inter-instrument encoder,\nE2\nE1\nQ\n. . . .\nQ\nInter-bar\nEncoder\nK\nV\nlinear\nlinear\nùúá\nùúé\nùëß\nInter-bar\nDecoder\nK\nV\nK\nV\nOutput\n(shifted right)\nBar-view Encoder\nBar-view Decoder\nIntra-bar\nEncoder\n. . . .\nIntra-bar\nDecoder\n...\n...\nPE\nPE\nPE\nEB\n‡∑©E2\n‡∑©EB\n‡∑©E1\nQ\n...\n...\nQ\n(a) Bar-view MidiVAE\n. . . .\nInter-inst\nEncoder\nK\nV\nùëß\nInter-inst\nDecoder\nK\nV\nK\nV\nTrack-view Encoder\nTrack-view Decoder\nIntra-inst\nEncoder\n. . . .\nIntra-inst\nDecoder\n...\n...\nOutput\n(shifted right)\nPE\nQ\nQ\nQ\nI2\nIT\nI1\n·àöI2\n·àöI1\n·àöIT\n...\n...\nQ\n(b) Track-view MidiVAE\n‚Ñéùëè\nlinear\nùúá\nùúé\n‚Ñéùë°\nlinear\nFig. 3. (a) The details of Bar-view MidiVAE, where ‚ÄúPE‚Äù refers to the positional encoding. (b) The details of Track-view MidiVAE, where\n‚ÄúIntra-inst‚Äù and ‚ÄúInter-inst‚Äù respectively denote Intra-instrument and Inter-instrument.\nboth of which consist of a transformer encoder and a multi-head at-\ntention module. The intra-instrument encoder is utilized to model the\ninstrumental characteristics\n\b\nI1, I2, . . . , IT |Ij ‚àà RDt, 1 ‚â§ j ‚â§ T\n\t\n,\nwhere Dt represents the dimension of the characteristic and T de-\nnotes the number of instruments.\nThese characteristics serve as\nthe key and value for attention module, while there is a learnable\nquery parameter to exploit associations between characteristics, and\nultimately generates track-view embedding ht.\nCorrespondingly,\nthe decoder comprises two components: a parameter-sharing trans-\nformer decoder-based intra-instrument decoder and a multi-head\nattention-based inter-instrument decoder.\nThe inter-instrument\ndecoder regards z as the key and value, and uses a learnable\nquery parameter to model guidance instrumental characteristics\nn\neI1, eI2, . . . , eIT |eIj ‚àà RDt, 1 ‚â§ j ‚â§ T\no\n. Each guidance character-\nistic is then passed into the intra-instrument decoder to guide the\nreconstruction of the corresponding subsequence. From the view\nof track, Track-view MidiVAE effectively model the instrumental\ncharacteristics and harmony as well as the global information of the\nmusical composition.\n2.4. Multi-view MidiVAE\nThe aforementioned Track- and Bar-view MidiVAE capture the\nglobal information, instrumental characteristics and harmony as\nwell as local details of the music composition, respectively. For the\npurpose of leveraging the strengths of both models, we consider the\nmusical composition from these dual views.\nWe first convert the original sequence S into a Track-view se-\nquence St ‚àà RT √óLt√óM and a Bar-view sequence Sb ‚àà RB√óLb√óM,\nwhere Lt and Lb represent the number of notes in a track and the\nnumber of notes in a bar, respectively, and M means the number of\nmusical attributes. The conversion process can be represented by the\nfollowing equation:\nSt = Xt ¬∑ S, Sb = Xb ¬∑ S\n(1)\nwhere Xt represents the transformation matrix according to the\ntrack-based rule and Xb represents the transformation matrix ac-\ncording to the bar-based rule. Then St and Sb are passed into the\nTrack- and Bar-view encoders to model ht and hb, respectively.\nGiven the multi-view information, we can implement a hybrid\nvariational encoding-decoding strategy to map them to the same\nlatent space and then reconstruct with an adaptive feature fusion\nmethod, as shown in Fig.1. To be specific, ht and hb are concate-\nnated together and fed into a Conv1d layer for multi-view informa-\ntion fusion to obtain the hybrid embedding, from which we can get\n¬µ and œÉ of the latent distribution. We utilized reparameterization to\nobtain the latent representation z ‚àà RDz, which serves as the input\nof the Track- and Bar-view decoders to predict probability matrix\nPt and Pb of two sequences. To obtain the probability matrix P of\nthe reconstruction sequence, we utilize an adaptive weight vector Œ±\nto fuse Pt and Pb by the following equation:\nP = Œ± ¬∑ X‚àí1\nt\n¬∑ Pt + (1 ‚àí Œ±) ¬∑ X‚àí1\nb\n¬∑ Pb\n(2)\nDuring training, our model needs to reconstruct S, St and Sb simul-\ntaneously for loss computation. We formulate our loss function as\nfollows:\nLtotal = Lrs + Lrst + Lrsb + Œ≤ ¬∑ LKL\n(3)\nLrs, Lrst, and Lrsb represent the reconstruction loss of S, St and Sb,\nwhile LKL stands for the Kullback-Leibler divergence and Œ≤ is the\nhyperparameter of Œ≤-VAE [25].\n3. EXPERIMENTS\n3.1. Dataset\nTo train and evaluate our models, we utilize the CocoChorales\ndataset [24]. The dataset consists of 240,000 pieces, and each of\nthem is a standard four-part chorale (Soprano, Alto, Tenor, Bass) in\n4\n4 time of 8 bars. The training set contains 192,000 pieces, whereas\nthe validation and test sets each contains 24,000 pieces. In total,\nthere are 13 different instruments included in the dataset, namely\nViolin, Viola, Cello, Trumpet, French Horn, Trombone, Tuba, Flute,\nOboe, Clarinet, Saxophone, Bassoon and Double Bass. Each sample\nconsists of a combination of four of these instruments.\n3.2. Experiment Setup\nAll models were trained using the Adam optimizer with an initial\nlearning rate of 1e-4.\nWe used a cross-entropy loss against the\nground truth and teacher forcing for all models.\nTo testify the effectiveness of our improvements, the following\nmodels were compared. Aiming at a fair comparison, the identical\nTable 1. The reconstruction performance measured in terms of accuracy: overall, duration, pitch, position, instrument, bar and tempo.\nModel\nOverall ‚Üë\nAttributes\nDuration ‚Üë\nPitch ‚Üë\nPosition ‚Üë\nInstrument ‚Üë\nBar ‚Üë\nTempo ‚Üë\nMulti-track MusicVAE (REMI+)\n0.8704\n0.6660\n0.6679\n0.9832\n0.9433\n0.9993\n0.9628\nMulti-track MusicVAE (OctupleMIDI)\n0.9018\n0.6818\n0.7540\n0.9845\n0.9942\n0.9993\n0.9975\nTrack-view MidiVAE\n0.9482\n0.8115\n0.8968\n0.9867\n0.9956\n0.9997\n0.9991\nBar-view MidiVAE\n0.9450\n0.8224\n0.8740\n0.9880\n0.9863\n0.9998\n0.9994\nMulti-view MidiVAE\n0.9690\n0.8940\n0.9294\n0.9909\n0.9999\n0.9999\n0.9999\nexperimental setup was implemented for each model. (1) Multi-\ntrack MusicVAE: We chose Multi-track MusicVAE with REMI+\n[26] as the baseline. For a fair evaluation, we considered 8 bars as\nan extended bar and fed it into the baseline. Besides, a Multi-track\nMusicVAE using OctupleMIDI as the input was implemented. Fol-\nlowing [20], the hidden size of LSTM in encoder and decoder were\n{2048, 1024}. The dimension of z is set to 512. (2) Track-view\nMidiVAE: The layers of the transformer encoders and decoders were\n{4, 8}, while all multi-head attention modules employed 8 heads.\n(3) Bar-view MidiVAE: For a fair comparison, the Bar-view Midi-\nVAE used the same configurations as the Track-view MidiVAE in\ntransformer blocks and multi-head attention modules. (4) Multi-\nview MidiVAE: The configurations of the modules followed the\nTrack- and Bar-view MidiVAE. Additionally, the hyperparameters\nwere Db = Dt = Dz = 512.\nWe performed reconstruction and generation experiments to\ngauge the model performance. The reconstruction experiments are\nbased on overall accuracy and attributes accuracy of sequence re-\nconstruction, with the results presented in Table 1. For generating\nexperiment, we organized a listening test involving 20 individuals\ninstructed to listen to 20 audio samples generated by each model,\nand provided ratings for each sample based on three criteria: melody,\ninstrumental harmony and overall quality. The mean opinion score\n(MOS) results are shown in Table 2.\n3.3. Comparison with Baseline\nTo compare our model with baseline, we performed reconstruction\nand generation experiments. Table 1 shows a significant improve-\nment in the reconstruction accuracy of our purposed model, with the\noverall, duration and pitch accuracy improved by 9.9%, 22.8% and\n26.1%, respectively. In addition, it also outperforms baseline in other\nattributes accuracy. As can be seen from Table 2, the music samples\ngenerated by Multi-view MidiVAE exceed the baseline‚Äôs by 0.673\nin overall MOS, 0.683 in melody MOS, and 0.655 in instrumental\nharmony MOS, achieving the best results in all metrics. Both sets of\nexperimental results demonstrate the effectiveness of our improve-\nments in the usage of OctupleMIDI and Multi-view strategies.\n3.4. Ablation Study\n3.4.1. The Effect of OctupleMIDI\nWith the purpose of demonstrating the effectiveness of the Octu-\npleMIDI, we compare the Multi-track MusicVAE using inputs of\nREMI+ and OctupleMIDI. Relative to REMI+, the usage of of Oc-\ntupleMIDI results in a 75% decrease in sequence length and a 357%\nincrease in the model‚Äôs inference speed. From results of Multi-track\nMusicVAE (REMI+) and Multi-track MusicVAE (OctupleMIDI) in\nTable 1 and Table 2 we can see that, the Multi-track MusicVAE us-\ning OctupleMIDI exhibits improved reconstruction and generation\ncapabilities, illustrating the usage of OctupleMIDI can improve the\nmodel‚Äôs ability to reconstruct and generate musical composition.\nTable 2. The MOS on melody, instrumental harmony and overall\nquality of different models with 95% confidence intervals. The ‚ÄúR‚Äù\nand ‚ÄúO‚Äù denote REMI+ and OctupleMIDI respectively.\nModel\nMelody\nInstrumental\nHarmony\nOverall\nMulti-track MusicVAE (R)\n2.769¬±0.098\n2.821 ¬± 0.097\n2.785¬± 0.092\nMulti-track MusicVAE (O)\n2.948¬± 0.090\n2.993 ¬± 0.081\n2.998¬± 0.075\nTrack-view MidiVAE\n3.224¬± 0.089\n3.398¬± 0.091\n3.276¬± 0.081\nBar-view MidiVAE\n3.335¬± 0.0954\n3.229¬± 0.101\n3.309¬± 0.082\nMulti-view MidiVAE\n3.452¬± 0.089\n3.476¬± 0.086\n3.458¬± 0.079\nGround truth\n3.671¬± 0.926\n3.726¬± 0.089\n3.74¬± 0.082\n3.4.2. Investigation of Multi-view\nTo demonstrate the effectiveness of Track- and Bar-view MidiVAE,\nwe conduct further comparisons. As shown in Table 1 and Table 2,\nboth of Track- and Bar-view MidiVAE show significant improve-\nments in reconstruction accuracy and sample qualities relative to\nMulti-track MusicVAE. In particular, Track-view MidiVAE achieves\nhigher results in pitch accuracy, overall accuracy and the instrumen-\ntal harmony MOS of the generated samples, which shows that the\nTrack-view MidiVAE exhibits a superior capability in modeling mu-\nsical global information, instrumental characteristics and harmony.\nWhile the Bar-view reflects a better performance of duration accu-\nracy and position accuracy, illustrating that the Bar-view MidiVAE\nperforms better in capturing local details. In addition, we also com-\npare Multi-view MidiVAE with Track- and Bar-view MidiVAE. The\nresults show that our proposed model yields significant improve-\nments in both reconstruction accuracy and MOS of generated sam-\nples, demonstrating it effectively leverages the strengths of both dual\nviews without conflict.\n4. CONCLUSIONS\nThis paper introduces Multi-view MidiVAE to effectively model and\ngenerate long multi-track symbolic music. The Multi-view Midi-\nVAE utilizes a 2-D representation, OctupleMIDI, to capture rela-\ntionships among notes while reducing the feature sequence length.\nFurthermore, through a hybrid variational encoding-decoding strat-\negy, we integrate both Track- and Bar-view MidiVAE features to\nconcentrate on instrumental characteristics and harmony as well as\nglobal and local information about the musical composition. Ob-\njective and subjective experimental results1 on the CocoChorales\ndataset demonstrate that Multi-view MidiVAE is indeed a efficient\nVAE approach for modeling long multi-track symbolic music.\nAcknowledgements: This work is supported by National Natu-\nral Science Foundation of China (62076144), Shenzhen Science and\nTechnology Program (WDZC20220816140515001, JCYJ20220818\n101014030) and Shenzhen Key Laboratory of next generation inter-\nactive media innovative technology (ZDSYS20210623092001004).\n1Demo page: https://thuhcsi.github.io/icassp2024-Multi-view MIDIVAE\n5. REFERENCES\n[1] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui,\nQiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong,\nMark D Plumbley, et al.,\n‚ÄúWavjourney: Compositional au-\ndio creation with large language models,‚Äù\narXiv preprint\narXiv:2307.14335, 2023.\n[2] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie-\nYan Liu, ‚ÄúPopmag: Pop music accompaniment generation,‚Äù\nin Proceedings of the 28th ACM international conference on\nmultimedia, 2020, pp. 1198‚Äì1206.\n[3] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan\nYang, ‚ÄúMusegan: Multi-track sequential generative adversar-\nial networks for symbolic music generation and accompani-\nment,‚Äù in Proceedings of the AAAI Conference on Artificial\nIntelligence, 2018, vol. 32.\n[4] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley,\nand Taylor Berg-Kirkpatrick, ‚ÄúMultitrack music transformer,‚Äù\nin ICASSP 2023-2023 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP). IEEE,\n2023, pp. 1‚Äì5.\n[5] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit,\nIan Simon, Curtis Hawthorne, Noam Shazeer, Andrew M\nDai, Matthew D Hoffman, Monica Dinculescu, and Douglas\nEck, ‚ÄúMusic Transformer: Generating Music with Long-Term\nStructure,‚Äù in International Conference on Learning Represen-\ntations, 2018.\n[6] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garri-\nson W Cottrell, and Julian McAuley, ‚ÄúLakhnes: Improving\nmulti-instrumental music generation with cross-domain pre-\ntraining,‚Äù in Proc. ISMIR, 2019.\n[7] Andrea Agostinelli, Timo I Denk, Zal¬¥an Borsos, Jesse En-\ngel, Mauro Verzetti, Antoine Caillon, Qingqing Huang,\nAren Jansen, Adam Roberts, Marco Tagliasacchi, et al.,\n‚ÄúMusiclm:\nGenerating music from text,‚Äù\narXiv preprint\narXiv:2301.11325, 2023.\n[8] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu,\nDanilo Mandic, Wenwu Wang, and Mark D Plumbley, ‚ÄúAudi-\noLDM: Text-to-audio generation with latent diffusion models,‚Äù\narXiv preprint arXiv:2301.12503, 2023.\n[9] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant,\nGabriel Synnaeve, Yossi Adi, and Alexandre D¬¥efossez, ‚ÄúSim-\nple and Controllable Music Generation,‚Äù\narXiv preprint\narXiv:2306.05284, 2023.\n[10] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng,\nMing Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song,\net al., ‚ÄúEfficient Neural Music Generation,‚Äù arXiv preprint\narXiv:2305.15719, 2023.\n[11] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk,\nAndy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang,\nJiahui Yu, Christian Frank, et al.,\n‚ÄúNoise2music:\nText-\nconditioned music generation with diffusion models,‚Äù arXiv\npreprint arXiv:2302.03917, 2023.\n[12] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhen-\nhui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and\nZhou Zhao, ‚ÄúMake-An-Audio 2: Temporal-Enhanced Text-to-\nAudio Generation,‚Äù arXiv preprint arXiv:2305.18474, 2023.\n[13] Shulei Ji, Xinyu Yang, and Jing Luo,\n‚ÄúA Survey on Deep\nLearning for Symbolic Music Generation: Representations,\nAlgorithms, Evaluations, and Challenges,‚Äù ACM Computing\nSurveys, 2023.\n[14] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne,\nand Douglas Eck, ‚ÄúA hierarchical latent vector model for learn-\ning long-term structure in music,‚Äù in International conference\non machine learning. PMLR, 2018, pp. 4364‚Äì4373.\n[15] Ga¬®etan Hadjeres, Frank Nielsen, and Franc¬∏ois Pachet, ‚ÄúGLSR-\nVAE: Geodesic latent space regularization for variational au-\ntoencoder architectures,‚Äù in 2017 IEEE symposium series on\ncomputational intelligence (SSCI). IEEE, 2017, pp. 1‚Äì7.\n[16] Ashis Pati and Alexander Lerch, ‚ÄúAttribute-based regulariza-\ntion of latent spaces for variational auto-encoders,‚Äù\nNeural\nComputing and Applications, vol. 33, pp. 4429‚Äì4444, 2021.\n[17] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan\nYang, Junbo Zhao, and Gus Xia, ‚ÄúPianotree vae: Structured\nrepresentation learning for polyphonic music,‚Äù in Proc. ISMIR,\n2020.\n[18] Shiqi Wei and Gus Xia, ‚ÄúLearning long-term music representa-\ntions via hierarchical contextual constraints,‚Äù in Proc. ISMIR,\n2021.\n[19] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon,\n‚ÄúSymbolic music generation with diffusion models,‚Äù\narXiv\npreprint arXiv:2103.16091, 2021.\n[20] Ian Simon, Adam Roberts, Colin Raffel, Jesse Engel, Curtis\nHawthorne, and Douglas Eck, ‚ÄúLearning a latent space of mul-\ntitrack measures,‚Äù arXiv preprint arXiv:1806.00195, 2018.\n[21] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and\nKaren Simonyan, ‚ÄúThis time with feeling: Learning expressive\nmusical performance,‚Äù Neural Computing and Applications,\nvol. 32, pp. 955‚Äì967, 2020.\n[22] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and\nTie-Yan Liu, ‚ÄúMusicBERT: Symbolic Music Understanding\nwith Large-Scale Pre-Training,‚Äù in Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021, 2021, pp.\n791‚Äì800.\n[23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin, ‚ÄúAttention is all you need,‚Äù Advances in neural\ninformation processing systems, vol. 30, 2017.\n[24] Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis\nHawthorne, and Jesse Engel, ‚ÄúThe Chamber Ensemble Gener-\nator: Limitless High-Quality MIR Data via Generative Model-\ning,‚Äù arXiv preprint arXiv:2209.14458, 2022.\n[25] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess,\nXavier Glorot, Matthew Botvinick, Shakir Mohamed, and\nAlexander Lerchner,\n‚Äúbeta-vae: Learning basic visual con-\ncepts with a constrained variational framework,‚Äù in Interna-\ntional conference on learning representations, 2016.\n[26] Dimitri von R¬®utte, Luca Biggio, Yannic Kilcher, and Thomas\nHofmann, ‚ÄúFIGARO: Generating symbolic music with fine-\ngrained artistic control,‚Äù\narXiv preprint arXiv:2201.10936,\n2022.\n"
}