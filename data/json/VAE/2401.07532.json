{
    "optim": "MULTI-VIEW MIDIVAE: FUSING TRACK- AND BAR-VIEW REPRESENTATIONS FOR LONG MULTI-TRACK SYMBOLIC MUSIC GENERATION Zhiwei Lin1,2,‚àó, Jun Chen1,3,‚àó, Boshi Tang1, Binzhu Sha1, Jing Yang2, Yaolong Ju2, Fan Fan2, Shiyin Kang3, Zhiyong Wu1,4,‚Ä†, Helen Meng4 1 Shenzhen International Graduate School, Tsinghua University, Shenzhen, China 2 Huawei Technologies Co., Ltd., Shenzhen, China 3Skywork AI PTE. LTD. 4 The Chinese University of Hong Kong, Hong Kong SAR, China {lzw22, y-chen21}@mails.tsinghua.edu.cn, zywu@sz.tsinghua.edu.cn ABSTRACT Variational Autoencoders (VAEs) constitute a crucial component of neural symbolic music generation, among which some works have yielded outstanding results and attracted considerable attention. Nevertheless, previous VAEs still encounter issues with overly long feature sequences and generated results lack contextual coherence, thus the challenge of modeling long multi-track symbolic music still remains unaddressed. To this end, we propose Multi-view MidiVAE, as one of the pioneers in VAE methods that effectively model and generate long multi-track symbolic music. The Multi- view MidiVAE utilizes the two-dimensional (2-D) representation, OctupleMIDI, to capture relationships among notes while reducing the feature sequences length. Moreover, we focus on instrumental characteristics and harmony as well as global and local information about the musical composition by employing a hybrid variational encoding-decoding strategy to integrate both Track- and Bar-view MidiVAE features. Objective and subjective experimental results on the CocoChorales dataset demonstrate that, compared to the baseline, Multi-view MidiVAE exhibits significant improvements in terms of modeling long multi-track symbolic music. Index Terms‚Äî symbolic music generation, long multi-track, Multi-view MidiVAE 1. INTRODUCTION Automatic music generation is a prominent and active research field, which has a wide range of applications from human-computer in- teractive composition to commercial composition software [1]. In recent years, the advancement of deep learning technology further promotes the development of auto music generation. Generally, au- tomatic music generation can be categorized into symbolic music generation [2‚Äì6] and music audio generation [7‚Äì12]. While music audio generation directly models continuous audio signals, symbolic music generation models and synthesizes music as a sequence of symbols, which by nature is more conducive to subsequent music editing and adjustments [13]. Therefore, symbolic music generation stays an important research topic. Among symbolic music generation methods, the VAE ap- proaches [14‚Äì18] have been widely applied due to their promising performance. Additionally, as conditional and multi-modal music generation becomes increasingly popular, VAEs can also serve as an essential component for the extraction and representation of la- tent features [19]. Consequently, the VAE-based symbolic music *These authors contributed equally to this work as first authors. ‚Ä† Corresponding author. generation methods have garnered widespread attention. Music- VAE [14] employs a Long Short-Term Memory (LSTM)-based VAE architecture to effectively reconstruct and generate symbolic music. However, due to the instrument-specific Encoder and Decoder de- sign, MusicVAE is restricted to music generation within designated instruments, which significantly limits its applicability. In response to this limitation, the Multi-track MusicVAE [20] incorporates the MIDI-like [21] event-based representation that can denote different instruments, and utilizes a universal Encoder and Decoder for mod- eling. This innovation successfully broadens the scope of symbolic music reconstruction and generation from specific instruments to arbitrary instruments, establishing it as one of the most effective VAE-based symbolic music generation methods so far. Despite the impressive performance, VAE approaches still strug- gle with the emerging challenge of modeling long multi-track sym- bolic music. The MIDI-like [21] input to the Multi-track Music- VAE is a one-dimensional (1-D) representation neglecting the rela- tionships among notes, which potentially impairs the model‚Äôs learn- ing for the musical composition. More critically, converting long multi-track symbolic music into this representation leads to exces- sively long feature sequences, which substantially increases the bur- den on model training and inference. Furthermore, when tackling long multi-track symbolic music, the Multi-track MusicVAE can only process one bar at a time, which results in the final generated music lack of contextual consistency and coherence, severely deteri- orating the quality of the symbolic music. In this paper, to confront the challenges above, we propose Multi-view MidiVAE, a pioneer VAE that efficiently models and generates the long multi-track symbolic music. In light of the is- sues associated with the aforementioned MIDI-like features, we introduce a 2-D representation called OctupleMIDI [22]. This rep- resentation takes the relationships among notes into account while reducing the sequence length, subsequently alleviating the complex- ity of feature extraction and reconstruction of the model. Moreover, we model from dual views: 1) To focus on instrumental charac- teristics and harmony as well as the global information of musical composition, we design a transformer-based [23] Track-view Midi- VAE, which adopts an intra- and inter-instrument modeling over the musical composition; 2) Alternatively, the transformer-based Bar-view MidiVAE implements an intra- and inter-bar architecture, concentrating on the finer details of notes within bars. Eventu- ally, through a hybrid variational encoding-decoding strategy, we integrate the outputs from these two VAE views to form our final Multi-view MidiVAE model. Objective and subjective experimental results on the CocoChorales dataset [24] validate the efficacy of our arXiv:2401.07532v1  [cs.SD]  15 Jan 2024 loss Track-view  Encoder Bar-view Encoder ‚Ñét ùëÜùë° ùëÜùëè ùëÜ ‚Ñéùëè Conv 1d z Track-view  Decoder Bar-view Decoder Pt Pb ·àöùëÜùë° ·àöùëÜb ·àöùëÜ ùõº 1-ùõº loss loss linear linear ùúá ùúé OctupleMIDI P MIF AFF Fig. 1. The overall diagram of the proposed Multi-view MidiVAE. The model mainly contains Track- and Bar-view encoders, a multi-view information fusion (MIF), Track- and Bar-view decoders as well as an adaptive feature fusion (AFF). improvements, demonstrating that the Multi-view MidiVAE is in- deed a potent VAE approach for modeling long multi-track symbolic music. 2. METHODOLOGY We focus on the task of long multi-track music generation, i.e., gen- erating multi-bar music with various instruments played together. To this end, we propose Multi-view MidiVAE that effectively mod- els and generates long multi-track symbolic music. As shown in Fig.1, our Multi-view MidiVAE adopts a hybrid variational encoder- decoder architecture, which mainly contains Track- and Bar-view encoders, a multi-view information fusion, Track- and Bar-view de- coders as well as an adaptive feature fusion. More specifically, the input OctupleMIDI sequences S are transformed into track-view se- quences St and bar-view sequences Sb. These two sequences are fed into the Track- and Bar-view encoders to extract track-view infor- mation contained in feature ht and bar-view information included in feature hb. By means of concatenating ht and hb, and passing them through a Conv1d layer, we achieve multi-view information fusion and then obtain the hybrid embedding that is utilized to get the mean ¬µ and variance œÉ of the latent distribution z. After that, the Track- and Bar-view decoders generate probability matrix Pt and Pb. Even- tually, the adaptive feature fusion fuses Pt and Pb into the final prob- ability matrix P for generating the reconstruction sequences eS. We will elaborate on our improvements in the following subsections. Dur11 Pitch35 Instoboe Bar0 + + + + Dur4 Pitch38 Instoboe Bar0 + + + + Dur8 Pitch33 Insttuba Bar0 + + + + Dur10 Pitch32 Instviola Bar0 + + + + Dur16 Pitch26 Instoboe Bar1 + + + + 1 2 3 4 5 ... ... ... ... ... Fig. 2. The schematic diagram of OctupleMIDI. The ‚ÄúDur‚Äù and ‚ÄúInst‚Äù mean duration and instrument respectively. 2.1. OctupleMIDI Representation Previous 1-D event-based representation [21] of Multi-track Mu- sicVAE exhibits certain limitations. First of all, the representation treats all musical attributes equally, thus limiting the model‚Äôs abil- ity to understand the differences and connections between attributes. Besides, it makes it difficult to capture complex relationships be- tween notes, e.g., accurately representing the simultaneous play of two notes. Finally, for processing the long multi-track symbolic music, the 1-D representation increases computational resource and time expenses. In order to address the aforementioned problems, we adopt the OctupleMIDI representation [22], which consists of sequences of octuple tokens. As shown in Fig.2, each token consists of eight mu- sic attributes, namely pitch, velocity, duration, instrument, position, bar, time signature and tempo. The representation employs the note as the fundamental unit and consolidates all musical attributes into a single octuple token. This enables the model to differentiate music attributes and effectively capture relationships between notes, while efficiently reducing sequence length. 2.2. Bar-view MidiVAE Generating long music usually involves long feature sequences, which makes it difficult to model the details of the musical com- position [13]. For the purpose of allowing the model to fully learn the local details about the musical composition, we design Bar-view MidiVAE, which consists of a hierarchical structure of encoder and decoder. Fig.3(a) shows the details of the Bar-view MidiVAE. The Bar-view encoder is made up of two parts: a weight-sharing intra-bar encoder and an inter-bar encoder. Both of them com- prise of a transformer encoder and a multi-head attention module. The intra-bar encoder is designed to model the bar embeddings \b E1, E2, . . . , EB|Ei ‚àà RDb, 1 ‚â§ i ‚â§ B \t , where Db represents the dimension of the embedding and B denotes the number of bars. Due to the obvious temporal relationship between bars, we add positional encoding to the bar embeddings and then pass them into the inter-bar encoder to model the inter-bar relationships. Finally, we utilize a learnable query parameter to capture bar-view embed- ding hb. There are two components inside the Bar-view decoder: a weight-sharing transformer decoder-based intra-bar decoder and a mutli-head attention-based inter-bar decoder. The inter-bar decoder regards z as the key and value, and uses a learnable query param- eter with positional encoding to model guidance bar embeddings n eE1, eE2, . . . , eEB| eEi ‚àà RDb o . To guide the reconstruction of the corresponding subsequence, the guidance embeddings are fed to the intra-bar decoder one after another. From the view of bar, Bar-view MidiVAE effectively model the finer details of notes within bars and the local information of the music composition. 2.3. Track-view MidiVAE In multi-track music, there are multiple instruments played simulta- neously. Notes within each instrument collaborate with each other, while the instruments themselves perform independently yet har- moniously. Therefore, it is crucial to model the characteristics of each instrument and their relationships, which is the key to captur- ing global information of the musical composition [13]. To this end, we design a Track-view MidiVAE that consists of a hierarchical structure of encoder and decoder, as shown in Fig.3(b). The Track-view encoder consists of two components: a parameter- sharing intra-instrument encoder and an inter-instrument encoder, E2 E1 Q . . . . Q Inter-bar Encoder K V linear linear ùúá ùúé ùëß Inter-bar Decoder K V K V Output (shifted right) Bar-view Encoder Bar-view Decoder Intra-bar Encoder . . . . Intra-bar Decoder ... ... PE PE PE EB ‡∑©E2 ‡∑©EB ‡∑©E1 Q ... ... Q (a) Bar-view MidiVAE . . . . Inter-inst Encoder K V ùëß Inter-inst Decoder K V K V Track-view Encoder Track-view Decoder Intra-inst Encoder . . . . Intra-inst Decoder ... ... Output (shifted right) PE Q Q Q I2 IT I1 ·àöI2 ·àöI1 ·àöIT ... ... Q (b) Track-view MidiVAE ‚Ñéùëè linear ùúá ùúé ‚Ñéùë° linear Fig. 3. (a) The details of Bar-view MidiVAE, where ‚ÄúPE‚Äù refers to the positional encoding. (b) The details of Track-view MidiVAE, where ‚ÄúIntra-inst‚Äù and ‚ÄúInter-inst‚Äù respectively denote Intra-instrument and Inter-instrument. both of which consist of a transformer encoder and a multi-head at- tention module. The intra-instrument encoder is utilized to model the instrumental characteristics \b I1, I2, . . . , IT |Ij ‚àà RDt, 1 ‚â§ j ‚â§ T \t , where Dt represents the dimension of the characteristic and T de- notes the number of instruments. These characteristics serve as the key and value for attention module, while there is a learnable query parameter to exploit associations between characteristics, and ultimately generates track-view embedding ht. Correspondingly, the decoder comprises two components: a parameter-sharing trans- former decoder-based intra-instrument decoder and a multi-head attention-based inter-instrument decoder. The inter-instrument decoder regards z as the key and value, and uses a learnable query parameter to model guidance instrumental characteristics n eI1, eI2, . . . , eIT |eIj ‚àà RDt, 1 ‚â§ j ‚â§ T o . Each guidance character- istic is then passed into the intra-instrument decoder to guide the reconstruction of the corresponding subsequence. From the view of track, Track-view MidiVAE effectively model the instrumental characteristics and harmony as well as the global information of the musical composition. 2.4. Multi-view MidiVAE The aforementioned Track- and Bar-view MidiVAE capture the global information, instrumental characteristics and harmony as well as local details of the music composition, respectively. For the purpose of leveraging the strengths of both models, we consider the musical composition from these dual views. We first convert the original sequence S into a Track-view se- quence St ‚àà RT √óLt√óM and a Bar-view sequence Sb ‚àà RB√óLb√óM, where Lt and Lb represent the number of notes in a track and the number of notes in a bar, respectively, and M means the number of musical attributes. The conversion process can be represented by the following equation: St = Xt ¬∑ S, Sb = Xb ¬∑ S (1) where Xt represents the transformation matrix according to the track-based rule and Xb represents the transformation matrix ac- cording to the bar-based rule. Then St and Sb are passed into the Track- and Bar-view encoders to model ht and hb, respectively. Given the multi-view information, we can implement a hybrid variational encoding-decoding strategy to map them to the same latent space and then reconstruct with an adaptive feature fusion method, as shown in Fig.1. To be specific, ht and hb are concate- nated together and fed into a Conv1d layer for multi-view informa- tion fusion to obtain the hybrid embedding, from which we can get ¬µ and œÉ of the latent distribution. We utilized reparameterization to obtain the latent representation z ‚àà RDz, which serves as the input of the Track- and Bar-view decoders to predict probability matrix Pt and Pb of two sequences. To obtain the probability matrix P of the reconstruction sequence, we utilize an adaptive weight vector Œ± to fuse Pt and Pb by the following equation: P = Œ± ¬∑ X‚àí1 t ¬∑ Pt + (1 ‚àí Œ±) ¬∑ X‚àí1 b ¬∑ Pb (2) During training, our model needs to reconstruct S, St and Sb simul- taneously for loss computation. We formulate our loss function as follows: Ltotal = Lrs + Lrst + Lrsb + Œ≤ ¬∑ LKL (3) Lrs, Lrst, and Lrsb represent the reconstruction loss of S, St and Sb, while LKL stands for the Kullback-Leibler divergence and Œ≤ is the hyperparameter of Œ≤-VAE [25]. 3. EXPERIMENTS 3.1. Dataset To train and evaluate our models, we utilize the CocoChorales dataset [24]. The dataset consists of 240,000 pieces, and each of them is a standard four-part chorale (Soprano, Alto, Tenor, Bass) in 4 4 time of 8 bars. The training set contains 192,000 pieces, whereas the validation and test sets each contains 24,000 pieces. In total, there are 13 different instruments included in the dataset, namely Violin, Viola, Cello, Trumpet, French Horn, Trombone, Tuba, Flute, Oboe, Clarinet, Saxophone, Bassoon and Double Bass. Each sample consists of a combination of four of these instruments. 3.2. Experiment Setup All models were trained using the Adam optimizer with an initial learning rate of 1e-4. We used a cross-entropy loss against the ground truth and teacher forcing for all models. To testify the effectiveness of our improvements, the following models were compared. Aiming at a fair comparison, the identical Table 1. The reconstruction performance measured in terms of accuracy: overall, duration, pitch, position, instrument, bar and tempo. Model Overall ‚Üë Attributes Duration ‚Üë Pitch ‚Üë Position ‚Üë Instrument ‚Üë Bar ‚Üë Tempo ‚Üë Multi-track MusicVAE (REMI+) 0.8704 0.6660 0.6679 0.9832 0.9433 0.9993 0.9628 Multi-track MusicVAE (OctupleMIDI) 0.9018 0.6818 0.7540 0.9845 0.9942 0.9993 0.9975 Track-view MidiVAE 0.9482 0.8115 0.8968 0.9867 0.9956 0.9997 0.9991 Bar-view MidiVAE 0.9450 0.8224 0.8740 0.9880 0.9863 0.9998 0.9994 Multi-view MidiVAE 0.9690 0.8940 0.9294 0.9909 0.9999 0.9999 0.9999 experimental setup was implemented for each model. (1) Multi- track MusicVAE: We chose Multi-track MusicVAE with REMI+ [26] as the baseline. For a fair evaluation, we considered 8 bars as an extended bar and fed it into the baseline. Besides, a Multi-track MusicVAE using OctupleMIDI as the input was implemented. Fol- lowing [20], the hidden size of LSTM in encoder and decoder were {2048, 1024}. The dimension of z is set to 512. (2) Track-view MidiVAE: The layers of the transformer encoders and decoders were {4, 8}, while all multi-head attention modules employed 8 heads. (3) Bar-view MidiVAE: For a fair comparison, the Bar-view Midi- VAE used the same configurations as the Track-view MidiVAE in transformer blocks and multi-head attention modules. (4) Multi- view MidiVAE: The configurations of the modules followed the Track- and Bar-view MidiVAE. Additionally, the hyperparameters were Db = Dt = Dz = 512. We performed reconstruction and generation experiments to gauge the model performance. The reconstruction experiments are based on overall accuracy and attributes accuracy of sequence re- construction, with the results presented in Table 1. For generating experiment, we organized a listening test involving 20 individuals instructed to listen to 20 audio samples generated by each model, and provided ratings for each sample based on three criteria: melody, instrumental harmony and overall quality. The mean opinion score (MOS) results are shown in Table 2. 3.3. Comparison with Baseline To compare our model with baseline, we performed reconstruction and generation experiments. Table 1 shows a significant improve- ment in the reconstruction accuracy of our purposed model, with the overall, duration and pitch accuracy improved by 9.9%, 22.8% and 26.1%, respectively. In addition, it also outperforms baseline in other attributes accuracy. As can be seen from Table 2, the music samples generated by Multi-view MidiVAE exceed the baseline‚Äôs by 0.673 in overall MOS, 0.683 in melody MOS, and 0.655 in instrumental harmony MOS, achieving the best results in all metrics. Both sets of experimental results demonstrate the effectiveness of our improve- ments in the usage of OctupleMIDI and Multi-view strategies. 3.4. Ablation Study 3.4.1. The Effect of OctupleMIDI With the purpose of demonstrating the effectiveness of the Octu- pleMIDI, we compare the Multi-track MusicVAE using inputs of REMI+ and OctupleMIDI. Relative to REMI+, the usage of of Oc- tupleMIDI results in a 75% decrease in sequence length and a 357% increase in the model‚Äôs inference speed. From results of Multi-track MusicVAE (REMI+) and Multi-track MusicVAE (OctupleMIDI) in Table 1 and Table 2 we can see that, the Multi-track MusicVAE us- ing OctupleMIDI exhibits improved reconstruction and generation capabilities, illustrating the usage of OctupleMIDI can improve the model‚Äôs ability to reconstruct and generate musical composition. Table 2. The MOS on melody, instrumental harmony and overall quality of different models with 95% confidence intervals. The ‚ÄúR‚Äù and ‚ÄúO‚Äù denote REMI+ and OctupleMIDI respectively. Model Melody Instrumental Harmony Overall Multi-track MusicVAE (R) 2.769¬±0.098 2.821 ¬± 0.097 2.785¬± 0.092 Multi-track MusicVAE (O) 2.948¬± 0.090 2.993 ¬± 0.081 2.998¬± 0.075 Track-view MidiVAE 3.224¬± 0.089 3.398¬± 0.091 3.276¬± 0.081 Bar-view MidiVAE 3.335¬± 0.0954 3.229¬± 0.101 3.309¬± 0.082 Multi-view MidiVAE 3.452¬± 0.089 3.476¬± 0.086 3.458¬± 0.079 Ground truth 3.671¬± 0.926 3.726¬± 0.089 3.74¬± 0.082 3.4.2. Investigation of Multi-view To demonstrate the effectiveness of Track- and Bar-view MidiVAE, we conduct further comparisons. As shown in Table 1 and Table 2, both of Track- and Bar-view MidiVAE show significant improve- ments in reconstruction accuracy and sample qualities relative to Multi-track MusicVAE. In particular, Track-view MidiVAE achieves higher results in pitch accuracy, overall accuracy and the instrumen- tal harmony MOS of the generated samples, which shows that the Track-view MidiVAE exhibits a superior capability in modeling mu- sical global information, instrumental characteristics and harmony. While the Bar-view reflects a better performance of duration accu- racy and position accuracy, illustrating that the Bar-view MidiVAE performs better in capturing local details. In addition, we also com- pare Multi-view MidiVAE with Track- and Bar-view MidiVAE. The results show that our proposed model yields significant improve- ments in both reconstruction accuracy and MOS of generated sam- ples, demonstrating it effectively leverages the strengths of both dual views without conflict. 4. CONCLUSIONS This paper introduces Multi-view MidiVAE to effectively model and generate long multi-track symbolic music. The Multi-view Midi- VAE utilizes a 2-D representation, OctupleMIDI, to capture rela- tionships among notes while reducing the feature sequence length. Furthermore, through a hybrid variational encoding-decoding strat- egy, we integrate both Track- and Bar-view MidiVAE features to concentrate on instrumental characteristics and harmony as well as global and local information about the musical composition. Ob- jective and subjective experimental results1 on the CocoChorales dataset demonstrate that Multi-view MidiVAE is indeed a efficient VAE approach for modeling long multi-track symbolic music. Acknowledgements: This work is supported by National Natu- ral Science Foundation of China (62076144), Shenzhen Science and Technology Program (WDZC20220816140515001, JCYJ20220818 101014030) and Shenzhen Key Laboratory of next generation inter- active media innovative technology (ZDSYS20210623092001004). 1Demo page: https://thuhcsi.github.io/icassp2024-Multi-view MIDIVAE 5. REFERENCES [1] Xubo Liu, Zhongkai Zhu, Haohe Liu, Yi Yuan, Meng Cui, Qiushi Huang, Jinhua Liang, Yin Cao, Qiuqiang Kong, Mark D Plumbley, et al., ‚ÄúWavjourney: Compositional au- dio creation with large language models,‚Äù arXiv preprint arXiv:2307.14335, 2023. [2] Yi Ren, Jinzheng He, Xu Tan, Tao Qin, Zhou Zhao, and Tie- Yan Liu, ‚ÄúPopmag: Pop music accompaniment generation,‚Äù in Proceedings of the 28th ACM international conference on multimedia, 2020, pp. 1198‚Äì1206. [3] Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, and Yi-Hsuan Yang, ‚ÄúMusegan: Multi-track sequential generative adversar- ial networks for symbolic music generation and accompani- ment,‚Äù in Proceedings of the AAAI Conference on Artificial Intelligence, 2018, vol. 32. [4] Hao-Wen Dong, Ke Chen, Shlomo Dubnov, Julian McAuley, and Taylor Berg-Kirkpatrick, ‚ÄúMultitrack music transformer,‚Äù in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1‚Äì5. [5] Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam Shazeer, Andrew M Dai, Matthew D Hoffman, Monica Dinculescu, and Douglas Eck, ‚ÄúMusic Transformer: Generating Music with Long-Term Structure,‚Äù in International Conference on Learning Represen- tations, 2018. [6] Chris Donahue, Huanru Henry Mao, Yiting Ethan Li, Garri- son W Cottrell, and Julian McAuley, ‚ÄúLakhnes: Improving multi-instrumental music generation with cross-domain pre- training,‚Äù in Proc. ISMIR, 2019. [7] Andrea Agostinelli, Timo I Denk, Zal¬¥an Borsos, Jesse En- gel, Mauro Verzetti, Antoine Caillon, Qingqing Huang, Aren Jansen, Adam Roberts, Marco Tagliasacchi, et al., ‚ÄúMusiclm: Generating music from text,‚Äù arXiv preprint arXiv:2301.11325, 2023. [8] Haohe Liu, Zehua Chen, Yi Yuan, Xinhao Mei, Xubo Liu, Danilo Mandic, Wenwu Wang, and Mark D Plumbley, ‚ÄúAudi- oLDM: Text-to-audio generation with latent diffusion models,‚Äù arXiv preprint arXiv:2301.12503, 2023. [9] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre D¬¥efossez, ‚ÄúSim- ple and Controllable Music Generation,‚Äù arXiv preprint arXiv:2306.05284, 2023. [10] Max WY Lam, Qiao Tian, Tang Li, Zongyu Yin, Siyuan Feng, Ming Tu, Yuliang Ji, Rui Xia, Mingbo Ma, Xuchen Song, et al., ‚ÄúEfficient Neural Music Generation,‚Äù arXiv preprint arXiv:2305.15719, 2023. [11] Qingqing Huang, Daniel S Park, Tao Wang, Timo I Denk, Andy Ly, Nanxin Chen, Zhengdong Zhang, Zhishuai Zhang, Jiahui Yu, Christian Frank, et al., ‚ÄúNoise2music: Text- conditioned music generation with diffusion models,‚Äù arXiv preprint arXiv:2302.03917, 2023. [12] Jiawei Huang, Yi Ren, Rongjie Huang, Dongchao Yang, Zhen- hui Ye, Chen Zhang, Jinglin Liu, Xiang Yin, Zejun Ma, and Zhou Zhao, ‚ÄúMake-An-Audio 2: Temporal-Enhanced Text-to- Audio Generation,‚Äù arXiv preprint arXiv:2305.18474, 2023. [13] Shulei Ji, Xinyu Yang, and Jing Luo, ‚ÄúA Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges,‚Äù ACM Computing Surveys, 2023. [14] Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, and Douglas Eck, ‚ÄúA hierarchical latent vector model for learn- ing long-term structure in music,‚Äù in International conference on machine learning. PMLR, 2018, pp. 4364‚Äì4373. [15] Ga¬®etan Hadjeres, Frank Nielsen, and Franc¬∏ois Pachet, ‚ÄúGLSR- VAE: Geodesic latent space regularization for variational au- toencoder architectures,‚Äù in 2017 IEEE symposium series on computational intelligence (SSCI). IEEE, 2017, pp. 1‚Äì7. [16] Ashis Pati and Alexander Lerch, ‚ÄúAttribute-based regulariza- tion of latent spaces for variational auto-encoders,‚Äù Neural Computing and Applications, vol. 33, pp. 4429‚Äì4444, 2021. [17] Ziyu Wang, Yiyi Zhang, Yixiao Zhang, Junyan Jiang, Ruihan Yang, Junbo Zhao, and Gus Xia, ‚ÄúPianotree vae: Structured representation learning for polyphonic music,‚Äù in Proc. ISMIR, 2020. [18] Shiqi Wei and Gus Xia, ‚ÄúLearning long-term music representa- tions via hierarchical contextual constraints,‚Äù in Proc. ISMIR, 2021. [19] Gautam Mittal, Jesse Engel, Curtis Hawthorne, and Ian Simon, ‚ÄúSymbolic music generation with diffusion models,‚Äù arXiv preprint arXiv:2103.16091, 2021. [20] Ian Simon, Adam Roberts, Colin Raffel, Jesse Engel, Curtis Hawthorne, and Douglas Eck, ‚ÄúLearning a latent space of mul- titrack measures,‚Äù arXiv preprint arXiv:1806.00195, 2018. [21] Sageev Oore, Ian Simon, Sander Dieleman, Douglas Eck, and Karen Simonyan, ‚ÄúThis time with feeling: Learning expressive musical performance,‚Äù Neural Computing and Applications, vol. 32, pp. 955‚Äì967, 2020. [22] Mingliang Zeng, Xu Tan, Rui Wang, Zeqian Ju, Tao Qin, and Tie-Yan Liu, ‚ÄúMusicBERT: Symbolic Music Understanding with Large-Scale Pre-Training,‚Äù in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 791‚Äì800. [23] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin, ‚ÄúAttention is all you need,‚Äù Advances in neural information processing systems, vol. 30, 2017. [24] Yusong Wu, Josh Gardner, Ethan Manilow, Ian Simon, Curtis Hawthorne, and Jesse Engel, ‚ÄúThe Chamber Ensemble Gener- ator: Limitless High-Quality MIR Data via Generative Model- ing,‚Äù arXiv preprint arXiv:2209.14458, 2022. [25] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner, ‚Äúbeta-vae: Learning basic visual con- cepts with a constrained variational framework,‚Äù in Interna- tional conference on learning representations, 2016. [26] Dimitri von R¬®utte, Luca Biggio, Yannic Kilcher, and Thomas Hofmann, ‚ÄúFIGARO: Generating symbolic music with fine- grained artistic control,‚Äù arXiv preprint arXiv:2201.10936, 2022. "
}