{
    "optim": "STRUCTURED WORLD MODELING VIA\nSEMANTIC VECTOR QUANTIZATION\nYi-Fu Wu1∗, Minseung Lee2, Sungjin Ahn2\n1Rutgers University\n2KAIST\nABSTRACT\nNeural discrete representations are crucial components of modern neural networks.\nHowever, their main limitation is that the primary strategies such as VQ-VAE\ncan only provide representations at the patch level. Therefore, one of the main\ngoals of representation learning, acquiring structured, semantic, and compositional\nabstractions such as the color and shape of an object, remains elusive. In this\npaper, we present the first approach to semantic neural discrete representation\nlearning. The proposed model, called Semantic Vector-Quantized Variational\nAutoencoder (SVQ), leverages recent advances in unsupervised object-centric\nlearning to address this limitation. Specifically, we observe that a simple approach\nquantizing at the object level poses a significant challenge and propose constructing\nscene representations hierarchically, from low-level discrete concept schemas to\nobject representations. Additionally, we suggest a novel method for structured\nsemantic world modeling by training a prior over these representations, enabling\nthe ability to generate images by sampling the semantic properties of the objects\nin the scene. In experiments on various 2D and 3D object-centric datasets, we\nfind that our model achieves superior generation performance compared to non-\nsemantic vector quantization methods such as VQ-VAE and previous object-centric\ngenerative models. Furthermore, we find that the semantic discrete representations\ncan solve downstream scene understanding tasks that require reasoning about the\nproperties of different objects in the scene.\n1\nINTRODUCTION\nWhile there have been various findings regarding the purpose of the brain, it is fair to say that the\nhuman brain has at least two key functions. First, it constructs a good representation that captures the\nstructure of the world through perception. Second, it imagines or generates various possibilities of\nthe world. Similarly, AI systems that aim to be as generally capable as humans would also need to\nrealize similar capabilities computationally. Building such a learning system that can both structurally\nrecognize and generate has long been a desired vision in machine learning, from Helmholtz machines\n(Dayan et al., 1995; Wade, 2021) to Variational Autoencoders (Kingma & Welling, 2013; Rezende\net al., 2014). Although there could be various approaches to achieving this, in this work, we focus on\na specific type of modeling, which we call Generative Structured World Modeling, which satisfy the\nfollowing desiderata.\nFirst, when it comes to representating a visual scene, it appears that we do not perceive the scene sim-\nply as a monolithic vector of features. Instead, we view it structurally and semantically, recognizing\nit as a composition of meaningful components such as objects and their attributes like shape, color,\nand position (Palmer, 1977; Singer, 2007; Spelke & Kinzler, 2007). Various works in AI, particularly\nobject-centric approaches (Greff et al., 2020), have demonstrated that this structural decomposition\nfacilitates relational reasoning (Wu et al., 2021; Yoon et al., 2023; Webb et al., 2023a;b) and out-of-\ndistribution generalization (Dittadi et al., 2022; Yoon et al., 2023) due to improved compositional\ngeneralization. It has also been shown that a monolithic vector representation of a scene, such as\nVAE, fails in multi-object scenes (Wu et al., 2021; Dittadi et al., 2022; Yoon et al., 2023).\n∗Correspondence to yifu.wu@gmail.com\n1\narXiv:2402.01203v1  [cs.LG]  2 Feb 2024\nTable 1: Desiderata for Generative Structured World Modeling and Related Models\nVAE\nVQ-VAE\nSlot Attention\nSysBinder\nSVQ (Ours)\nSemantic\nDecomposition\nFactor\n✘\nObject\nObject & Factor\nObject & Factor\nDiscrete\n✘\n✓\n✘\n✘\n✓\nSampling\n✓\n✓\n✘\n✘\n✓\nMoreover, this structured and semantic understanding can be categorized and conceptualized dis-\ncretely in an unsupervised way. Such an ability is critical for organizing and comprehending the\ncomplexity of the environment, e.g., via language, as well as for implementing modularity (Andreas\net al., 2016) or symbolic reasoning (Lake et al., 2016). In AI, discrete representations are also\nuseful to leverage powerful learning models like transformers. One of the most popular models for\ndiscrete representation learning in AI is VQ-VAE (van den Oord et al., 2017). It has been shown to\nbe beneficial for image generation (Razavi et al., 2019; Esser et al., 2021) and probability density\nmodeling (Van den Oord et al., 2016). However, VQ-VAE and its variants, such as dVAE (Ramesh\net al., 2021; Singh et al., 2022a) and VQ-GAN (Esser et al., 2021), represent a scene as a grid of\nsmall patches, lacking the capability to capture the scene’s holistic structure and semantics.\nBesides, the ability to generate samples that adhere to the observed data distribution is foundational\nfor endowing AI the capabilities to imagine and simulate, e.g., for planning (Mattar & Lengyel, 2022;\nHafner et al., 2019). However, only a certain class of representation learning models supports this\nessential ability. While models like Slot Attention (Locatello et al., 2020) and SysBinder (Singh et al.,\n2023) offer structured, object-centric representations, in its original form it is unclear how to support\ndensity-based sampling. In contrast, VAE-based models, such as VAE and VQ-VAE, generally\nsupport this ability to sample from a prior distribution, but they either do not provide object-centric\nstructures (VAE) or are limited to patch-based representations (VQ-VAE).\nIn this work, we observe that, despite its significance, there is currently no method that simultaneously\nsatisfies all of the mentioned criteria of Generative Structured World Modeling, as summarized in\nTable 1. To address this issue, we propose the Semantic Vector-Quantized (SVQ) Variational Autoen-\ncoder. Our model achieves discrete semantic decomposition by learning hierarchical, composable\nfactors that closely align with the objects and the properties of objects in visual scenes. Similar to\npatch-based vector quantization methods, we can train an autoregressive prior to learn the distribution\nof the dataset. However, unlike VQ-VAE, we achieve this by learning the distribution of semantic\ndiscrete tokens, rather than patch tokens. As a result, the generation (or imagination) process is to\ncompose semantic concepts such as objects and their attributes, rather than stitching a grid of patches.\nIn our experiments, we demonstrate the following practical benefits of our method. First, we find that\nfor multi-object scenes, SVQ is able to model the prior distribution better than patch-based methods,\nas measured by the quality of the samples generated. Second, we find that SVQ representations\noutperform patch-based VQ representations on downstream tasks that require knowledge of the\ndifferent properties of the objects in the scene. We also find evidence that SVQ representations\ncan generalize better to out-of-distribution tasks compared to patch-based VQ representations and\nSysBinder continuous representations. Lastly, we show that despite introducing a discrete bottleneck,\nSVQ can work on the challenging CLEVRTex (Karazija et al., 2021) dataset, one of the most complex\ndatasets used in recent unsupervised object-centric representation learning models.\nOur contributions are as follows: First, we introduce SVQ, the first model to obtain semantic neural\ndiscrete representations without any supervision about the underlying factors in the scene. Second,\nby training a prior over these discrete representations, we are able to obtain an object-centric density\nmodel, capable of capturing the underlying data distribution and generating new samples. Third, we\nevaluate our model on several 2D and 3D datasets including the challenging CLEVRTex dataset,\nshowing superior downstream task performance and image generation quality.\n2\n2\nBACKGROUND\n2.1\nVECTOR-QUANTIZED VARIATIONAL AUTOENCODER (VQ-VAE)\nThe VQ-VAE (van den Oord et al., 2017) is a model that learns to compress high-dimensional data\ninto a discretized latent space. The latent space is maintained by a codebook of prototype vectors\ne ∈ RK×d where K is the size of the codebook and d is the dimensionality of each prototype\nvector. An input x is first passed through encoder E(x) to obtain latents ze ∈ Rd. A nearest-neighbor\nlookup between ze and each of the prototype vectors in the codebook yields a quantized representation\nzq = Quantize(ze) = ek where k = arg minj ||ze−ej||2. The decoder D then uses zq to reconstruct\nthe input: ˆx = D(zq). The model is trained with the following loss:\nL = ||x − ˆx||2\n2\n|\n{z\n}\nReconstruction\n+ ||sg[ze] − zq||2\n2\n|\n{z\n}\nCodebook\n+β ||ze − sg[zq]||2\n2\n|\n{z\n}\nCommitment\n.\nThe first term is a reconstruction loss and is used to train the encoder and decoder. A straight-through\nestimator (Bengio et al., 2013) is used to estimate the gradients through the quantization step by\ncopying the gradients from zq to ze. The second term is the codebook loss which encourages the\nprototype vectors in the codebook to be close to the output of the encoder. The third term, scaled by a\nconstant hyperparameter β, is the commitment loss and helps to stabilize the training by encouraging\nthe output of the encoder to not deviate too much from the chosen prototype vectors. Instead of the\ncodebook loss, we use exponential moving average (EMA) updates on the codebook, which we found\nto speed up training in our experiments (Razavi et al., 2019; Dhariwal et al., 2020; Yan et al., 2021).\nWhen VQ-VAEs are applied to images x ∈ RH×W ×C, the encoder E(x) is typically implemented as\na convolution encoder, outputting a feature map of latents ze ∈ RHz×Wz×d. This means that each\nlatent corresponds to a local area represented by a convolutional feature cell and thus can only capture\ninformation in a local receptive field (Figure 1a). However, images typically contain multiple objects,\nand the discrete factors underlying visual scenes typically correspond to different properties of the\nobjects in the scene, such as shape, color, type, and so on. The local patches from convolutional\nfeature maps are inadequate to capture this rich structure.\n2.2\nOBJECT-CENTRIC REPRESENTATIONS\nThe goal of unsupervised object-centric representation learning is to decompose a scene into a\nset of representations each capturing a different object in the scene. It is shown that this structural\ndecomposition, matching to the true factor structure of the world, facilitates some high-level cognition\nabilities such as relational reasoning (Wu et al., 2021; Yoon et al., 2023; Webb et al., 2023a;b) and\nout-of-distribution generalization (Dittadi et al., 2022; Yoon et al., 2023). We build on top of Slot\nAttention (Locatello et al., 2020), a spatial attention-based object-centric representation method.\nGiven an image x ∈ RH×W ×C, slot attention learns a set of slots, s = {s1, . . . , sN}, where sn ∈ Rds\nand N is the total number of slots. An encoder is applied to x and, after adding a positional encoding,\nthe result is flattened to an L-length input feature vector F ∈ RL×dF . Then, an iterative attention\nmechanism is used to spatially group the input features F to the slot representations s. First, the slots\nare randomly initialized from a Gaussian distribution with learned parameters. Then, in each iteration,\nthe slots are used as queries in an inverted version of dot-product attention (Tsai et al., 2020) with the\ninput features F as the keys and values. Instead of normalizing over the keys as is done in traditional\ndot-product attention, normalization is done over the queries (ie. slots). Additionally, a weighted\nmean is used to aggregate the values instead of the normal weighted sum, which is shown to stabilize\ntraining. The result is then used to update the slots with a per-slot GRU (Chung et al., 2014) followed\nby a per-slot residual MLP, both with shared parameters across the slots.\nThe slot representations are then used in a decoder to reconstruct the image and the entire model is\ntrained with an image reconstruction loss. The original formulation of slot attention used a spatial\nbroadcast decoder (Watters et al., 2019b) to create masked images per slot which are then combined\nto form a final reconstructed image. Recently, (Singh et al., 2022a) proposed using a transformer\ndecoder to reconstruct the image while attending to the slots with cross attention. This method was\nshown to scale to more complex scenes than the spatial broadcast decoder (Singh et al., 2022b) and is\nwhat we choose to use in our model.\n3\nFigure 1: Comparison between VQ-VAE, Quantized Slots, and SVQ. (a) VQ-VAE quantizes the scene at a local\npatch level and may not capture the semantic structure of the scene. (b) Quantized Slots (QS) would quantize the\nscene at the slot level but require a separate code for every possible configuration of an object. (c) SVQ quantizes\nat the block level, representing each factor (such as color or shape) as a code. In this example, to represent all\npossible object configurations, SVQ requires only 10 codebook entries at the block level while QS requires 25.\n3\nMETHOD\n3.1\nSEMANTIC VECTOR QUANTIZATION\nGiven a slot attention encoder that can obtain a set of representations of the objects in a scene, one\nmay think of a hypothetical method, applying vector quantization to the slot representation itself\nto obtain a set of semantic discrete representations (Figure 1b). While these representations would\nindeed correspond to the different objects in a scene, this scheme would essentially require one\ncodebook entry per possible object configuration and would be insufficient for anything beyond\ntrivially simple scenes.\nFor example, consider a simple scene containing a single object in a fixed position that only varies\nby color and shape. Assume there are c possible colors and s possible shapes for the object. With\nslot-level quantization, in order to represent all the potential objects, the codebook would require at\nleast c × s entries. This is because each slot representation is a single entangled representation so\neach combination of factors needs to be represented by a separate code. If instead, we were able to\ndisentangle the object-level representations into factor-level representations—representations that\nalign with the underlying latent factors of variation of each object—we would be able to describe the\npotentially large combinatorial space of each object with a much small number of discrete factors.\nIn the above example, if we had a fully disentangled representation of the color and the shape, we\nwould be able to represent all possible scenes with c + s codes (Figure 1c). See Appendix A.2 for\nfurther discussion.\nThis observation motivates us to design an architecture that further disentangles slot representations\nto factor representations that reflect the underlying discrete factors of the objects in the scene, and\nto perform vector quantization on these factor representations. Under this scheme, each object\nrepresentation would be composed of multiple discrete factors, and each factor would have its own\ncodebook that can be shared across objects. The resulting model, the Semantic Vector-Quantized\nVariational Autoencoder (SVQ), is depicted in Figure 2a and described below.\nTo obtain factored representations, we follow an approach motivated by Neural Systematic Binder\n(SysBinder) (Singh et al., 2023), where a binding mechanism is introduced to produce disentangled\nfactors within a slot. Specifically, the following modifications are applied to slot attention: First,\nwe maintain M codebooks C ∈ RM×K×dc, each with K discrete prototype vectors of dimension\ndc = ds\nM . Then, we split each of the N ds-dimensional slot representations into M equally-sized\nblocks, each of which will represent one factor. We denote the full set of block representations as\nze ∈ RN×M×dc. Crucially, we replace the slot-level GRUs and residual MLPs with block-level\nequivalents that have shared parameters across blocks corresponding to the same factor. At the end\nof each slot attention iteration, we apply vector quantization for each block using its corresponding\n4\nFigure 2: (a) Overall architecture of SVQ. We maintain M learned codebooks and split each slot into M blocks.\nAt the end of each Slot Attention iteration, we apply vector quantization to each block representation to obtain a\nset of discrete codes for each slot. Each block ends up specializing in different underlying factors of variation\nfor the objects in the scene. (b) The Autoregressive Semantic Prior. After training the model, we freeze SVQ\nand train and autoregressive prior over the discrete latent codes. Sampling from this prior allows us to generate\nan image one object at a time, based on their properties.\ncodebook to obtain a set of quantized blocks zq ∈ RN×M×dc. For n ∈ [1, N], m ∈ [1, M],\nzn,m\nq\n= Cm,k where k = arg min\nj\n||zn,m\ne\n− Cm,j||2 ,\nwhere zn,m\nq\ndenotes the m-th block in the n-th slot and Cm,k is the k-th prototype vector in the\nm-th codebook. By sharing the codebook for each block across all of the slots, each block ends\nup specializing in different underlying factors of the objects in the scene, such as color, shape, and\nposition. Thus, these quantized representations are semantic in the sense that they contain factor-level\nrepresentations mapping to the underlying structure of the scene.\nTo reconstruct the image, we use the autoregressive transformer decoder described in Section 2.2\nand condition on zq via cross attention. Similar to Singh et al. (2023), we first let the blocks within a\nslot interact with a single-layer transformer and then add a block-level positional encoding before\ninputting the representations as cross attention in the transformer decoder. We train the model with\nthe reconstruction loss, the VQ-VAE commitment loss, and we update the codebooks with EMA\nupdates. To prevent codebook collapse, we also incorporate random restarts for the embeddings,\nsimilar to previous work (Dhariwal et al., 2020). To achieve this, we keep a count of the usage of\neach code in the codebooks and randomly reset it to be near one of the encoder outputs of the current\nbatch if its usage falls below a threshold.\n3.2\nAUTOREGRESSIVE SEMANTIC PRIOR\nGiven these semantic discrete codes representing the different objects in the scene, we can now freeze\nthe SVQ and train an autoregressive prior p(zq) over these codes to obtain a generative model of the\nunderlying data that captures the structure and semantics of the data. We can then sample from this\nprior to obtain codes for new scenes and use these codes in the SVQ decoder to generate new images.\nCompared to patch-based VQ methods that generate tokens that correspond to a spatially local region\nof an image, this Autoregressive Semantic Prior (ASP) generates an image one object at a time, based\non their properties (Figure 2b).\nWe implement the prior using a simple autoregressive transformer decoder. First, we flatten zq along\nthe slot and block dimensions to a vector with dimensions NM × dc. We then apply a positional\nencoding across all slots and blocks and input the resulting vector to a transformer decoder with an\nobjective of predicting the discrete code of the next block. Although slot attention does not guarantee\nany specific ordering of the slots, the blocks within the slots are arranged in a predefined order.\nTherefore, the positional encoding is important in providing information about the ordering of the\nblocks as well as which block belongs to which slot.\n5\nNote that generating the latents of one image requires sampling NM blocks, but does not depend on\nthe size of the image. This is different than VQ-VAE, which scales with the size of the feature map\nand may become expensive for high-resolution images.\n4\nRELATED WORK\nNeural Discrete Representation Learning. Our work builds on top of neural discrete representation\nlearning, which has played a pivotal role in the advancement of generative models for images in\nrecent years (van den Oord et al., 2017; Razavi et al., 2019; Ramesh et al., 2021; Esser et al., 2021;\nYu et al., 2022). These methods typically follow a two-stage approach. First, an image is encoded\ninto a CNN feature map, which is then tokenized using vector quantization (Gray, 1984) into a set of\ndiscrete latent variables. In the second stage, a powerful autoregressive prior is then trained to model\nthe distribution of these discrete tokens, allowing for sampling new images from this distribution.\nOur model also follows this two-stage approach, except our latents correspond to the properties of\nobjects instead of cells in a CNN feature map.\nUnsupervised Object-Centric Learning. Recent unsupervised object-centric learning methods have\nbeen shown to decompose an image or video into a set of latents, each representing an object in the\nscene (Burgess et al., 2019; Greff et al., 2019; Anciukevicius et al., 2020; Locatello et al., 2020;\nGreff et al., 2017; Engelcke et al., 2020; 2022; von Kügelgen et al., 2020; Du et al., 2021; Kabra\net al., 2021; Zhang et al., 2022; Eslami et al., 2016; Lin et al., 2020b; Jiang & Ahn, 2020; Chen\net al., 2021; Deng et al., 2021; Lin et al., 2020b;a; Singh et al., 2023; Kipf et al., 2022; Singh et al.,\n2022b; Gopalakrishnan et al., 2022; Seitzer et al., 2022; Hénaff et al., 2022; Wang et al., 2023a;\nWu et al., 2021; Wen et al., 2022; Zoran et al., 2021). While most of these methods result in a\ndistributed representation per object, there have been several attempts at learning more structured\nor disentangled representations, such as those methods that decompose the latents into what and\nwhere components (Eslami et al., 2016; Crawford & Pineau, 2019b;a; Jiang et al., 2019; Jiang & Ahn,\n2020; Lin et al., 2020b;a; Chen et al., 2021) or those that learn disentangled latents via a VAE (Greff\net al., 2019; Zoran et al., 2021). Closely related to our work, recent methods have been designed\nto learn factor-level disentanglement (Singh et al., 2023; Kirilenko et al., 2023). However, these\nmethods still operate with continuous latents instead of discrete tokens and do not support sampling\nnew images. While there are several object-centric learning methods that do support sampling new\nimages (Engelcke et al., 2020; 2022; Jiang & Ahn, 2020; Wang et al., 2023b), these also do not use\nsemantic discrete latents as we do in our work.\n5\nEXPERIMENTS\nDatasets. We evaluate our model on two variants of a 2D Sprites dataset (Watters et al., 2019a;\nYoon et al., 2023) and three variants of the CLEVR dataset (Johnson et al., 2017), CLEVR-Easy,\nCLEVR-Hard, CLEVR-Tex. In the 2D Sprites datasets, objects of varying shapes and colors are\nplaced in a scene. In total, there are 7 possible colors and 12 possible shapes. In each image, one\nobject has a single property that is unique from the other objects. All other properties are shared by at\nleast two objects. This structure allows us to evaluate if the prior correctly models the dependencies\nbetween the properties of the scene. We test versions of this dataset with and without textured\nbackgrounds (Cimpoi et al., 2014). CLEVR-Easy, CLEVR-Hard, and CLEVR-Tex were previously\nused in (Singh et al., 2023) and are modified from the original CLEVR (Johnson et al., 2017) and\nCLEVR-Tex (Karazija et al., 2021) datasets to have larger objects so properties such as shape and\ntexture are more clearly visible. In CLEVR-Easy, objects may differ by only shape, color, and\nposition. In this dataset, there are 3 possible shapes and 8 possible colors. In CLEVR-Hard, objects\nmay differ by shape, color, position, size, and material. There are 3 possible shapes, 137 possible\ncolors, and 2 possible materials (shiny or matte). In CLEVR-Tex, there are 4 possible shapes and 58\npossible textures for the objects and background.\nBaselines. We compare our model with several patch-based quantization methods: VQ-VAE (van den\nOord et al., 2017) with a PixelCNN (Van den Oord et al., 2016) prior, and dVAE (Ramesh et al.,\n2021; Singh et al., 2022a) with a transformer decoder prior. For the dVAE baseline, we use the dVAE\nweights that are trained along with the SVQ. This provides a more direct ablation comparing the ASP\nof SVQ with the patch-based transformer decoder prior since the dVAE decoder is shared across these\n6\nFigure 3: Generated samples for the 4-object 2D Sprites and 4-object 2D Sprites with background datasets.\nmodels and will not contribute to differences in image quality. We also compare with GENESIS-v2\n(Engelcke et al., 2022), a continuous latent object-centric model with an autoregressive prior that can\nalso generate samples.\n5.1\nGENERATING SAMPLES WITH THE AUTOREGRESSIVE SEMANTIC PRIOR\n5.1.1\n2D SPRITES\nWe show the sample generations for the 2D Sprites datasets in Figure 3 and the FID results in Table 2.\nWe additionally calculate generation accuracy by manually inspecting 128 images per model to check\nif the generated images follow the constraints of the dataset. That is, each image must have exactly\none object that has a unique property. All other properties in the scene will have at least one duplicate\namong the other objects.\nWe see that for the simplest dataset with 3 objects and no background, SVQ achieves the lowest FID\nof the models and comparable generation accuracy to dVAE, generating about 75% of the scenes\ncorrectly. This setting may be simple enough that dVAE with a transformer prior can capture the\nstructure of the scene even with a patch-based discrete latent. As the scene complexity increases with\nmore objects and textured background, SVQ starts to outperform the baselines in terms of generation\naccuracy. Inspecting the qualitative results, we see that in the dataset with the background, VQ-VAE\nand dVAE start generating occasional blurry objects, whereas SVQ maintains clean-looking objects\nthat match the ground truth dataset. This may be because SVQ can segment the background into\nits own slot and factor the texture into a discrete latent, cleanly separating the representation of the\nobjects from the background. The patch-based methods, however, may have a harder time separating\nthe foreground from the background resulting in messier generations. Interestingly, despite the blurry\nshapes, VQ-VAE achieves the lowest FID score on the 2D Sprites dataset with background. We\nhypothesize this may be because the model spends more capacity modeling the background correctly\ninstead of the foreground, which may produce a better FID score, but not necessarily better generation\naccuracy. This is confirmed by the low generation accuracy of the VQ-VAE model this dataset, only\ngenerating 19.5% of the scenes correctly.\nTable 2: FID and Generation Accuracy on the 2D Sprites datasets. For Generation Accuracy, 128 samples were\ninspected manually to determine if they matched the constraints of the scene (ie. exactly one unique property\namong all the shapes). Underlined numbers indicate a minor difference from the best value.\nFID ↓\nGeneration Accuracy (in %) ↑\nDataset\nVQ-VAE\ndVAE\nSVQ (ours)\nVQ-VAE\ndVAE\nSVQ (ours)\n2D Sprites (3 obj)\n14.81\n7.26\n6.61\n28.91\n75.78\n75.00\n2D Sprites (4 obj)\n26.35\n19.15\n17.93\n21.88\n62.50\n66.41\n2D Sprites w/ bg (4 obj)\n58.14\n66.08\n58.50\n19.53\n30.47\n42.19\n5.1.2\nCLEVR\nIn Figure 4, we show sample generations after training the models on the CLEVR-Easy, CLEVR-\nHard, and CLEVR-Tex datasets. We report the Frechet Inception Distance (FID) in Table 3. We\nfind that compared to the other models, GENESIS-v2 generates very blurry images and completely\nfails on CLEVR-Tex, resulting in a high FID. While VQ-VAE produces sharper images, several of\nthe generated shapes are malformed or have mixed colors. The dVAE-generated images look closer\n7\nto the ground truth dataset, but still have some errors such as overlapping objects (first image) and\ngenerating scenes with more objects than seen in the training set (third image). SVQ has the lowest\nFID for all of these datasets and the generated images look very close to the ground truth dataset,\nindicating the usefulness of the ASP for generating these multi-object scenes.\nIn Appendix A.1, we show additional analysis of the learned codebook on the CLEVR-Easy dataset.\nFigure 4: Generated samples for the CLEVR-Easy, CLEVR-Hard, and CLEVR-Tex Datasets.\nTable 3: FID for the various models on the CLEVR datasets.\nFID ↓\nDataset\nGENESIS-v2\nVQ-VAE\ndVAE\nSVQ (ours)\nCLEVR-Easy\n115.56\n57.06\n40.30\n32.50\nCLEVR-Hard\n93.01\n73.33\n65.89\n43.12\nCLEVR-Tex\n225.08\n178.59\n112.80\n84.52\n5.2\nDOWNSTREAM TASKS\n5.2.1\nODD-ONE-OUT\nWe first evaluate on a downstream supervised learning task on the 2D Sprites dataset. We modify\nthe dataset by first dividing each image into four quadrants and ensuring exactly one object will be\nin each quadrant. As in our previous experiments, one object has a single property that is unique\nfrom the other objects. The goal of the task is to identify the quadrant of the odd-one-out object.\nWe first pretrain the baseline models on a dataset containing all 12 possible shapes and 7 possible\ncolors. Then, we freeze the underlying model and train a downstream model on top of the learned\nrepresentations with the supervised objective. The downstream model is trained on a dataset that\nonly contains 9 possible shapes and 4 possible colors. We then evaluate on both the in-distribution\n(ID) dataset and an out-of-distribution (OOD) dataset that consists of the remaining 3 shapes and 3\ncolors. In addition to dVAE and VQ-VAE, we use SysBinder as a baseline for this task, to compare\nits continuous representation with SVQ’s discrete representation. For the latent representation of\nSVQ, we include variants that use the codebook indices (SVQ Indices) and the codebook prototype\nvectors (SVQ Codebook).\nTable 4 shows the results of our experiments. Since all models can solve the task when evaluated on\nthe ID dataset, we report the number of steps to reach 98% accuracy on the validation dataset. We\nfind that SysBinder and SVQ Codebook learn the quickest in the ID setting. For the OOD setting, we\nfind that dVAE and VQ-VAE fail completely, not performing better than randomly guessing, showing\nthat the patch-based discrete latent is insufficient for OOD generalization in this task. SysBinder can\n8\nTable 4: Results for the downstream odd-one-\nout task. Since all the models can solve the in-\ndistribution (ID) task, we report the number of\nsteps to 98% ID Accuracy and out-of-distribution\n(OOD) accuracy.\nSteps to 98% (↓)\nOOD Acc. % (↑)\ndVAE Discrete\n37,000\n26.7\ndVAE Continuous\n32,000\n29.5\nVQ-VAE Indices\n77,000\n24.0\nVQ-VAE Codebook\n54,500\n55.6\nSysBinder\n27,000\n67.6\nSVQ Indices\n77,000\n46.8\nSVQ Codebook\n27,000\n99.1\nTable 5: Results for the downstream CLEVR-Hard\nProperty Comparison task.\nID Acc. % (↑)\nOOD Acc. % (↑)\ndVAE Discrete\n27.52\n19.87\ndVAE Continuous\n24.51\n20.51\nVQ-VAE Indices\n24.53\n17.74\nVQ-VAE Codebook\n23.73\n18.80\nSysBinder\n79.60\n70.09\nSVQ Indices\n68.21\n64.53\nSVQ Codebook\n75.86\n71.15\npartially solve the task in the OOD setting, while the SVQ Codebook seems to be able to solve the\ntask, achieving 99% accuracy. This indicates that the compact latent space offered by the discrete\ncode provides better out-of-distribution generalization abilities for this particular task. One possible\nexplanation for this is that since this is an odd-one-out task, the downstream network needs to do\ncomparisons between the properties of the objects and this may be easier to do with SVQ’s codebook\nvectors that are fixed. SysBinder’s continuous latents, on the other hand, offer greater variations\nfor the same concept. This increases the potential for the downstream network to learn spurious\ncorrelations in the data, which can negatively impact OOD performance. SVQ Indices is also only\nable to partially solve the task. This makes sense because in the out-of-distribution case, the model\ndoes not have any way of knowing two codebook indices are for the same property value (e.g. if two\ncodebook vectors both correspond to the color blue). Since SVQ Codebook uses the prototype vectors,\nit does not have this problem because the similarity can be determined by the vector representation.\n5.2.2\nCLEVR-HARD PROPERTY COMPARISON\nFor CLEVR-Hard, we construct a downstream task that assigns a number to each image as follows:\nFirst, we assign a number for each possible shape, color, and material present in the dataset. Then, for\na given image, we identify the maximum number for each of these three properties. Lastly, we sum\nthe max numbers for each of the properties to arrive at one integer label per image. We formulate the\nproblem as a classification problem to correctly identify the number for each image. For example,\nsuppose we have a scene containing a matte red cylinder and a shiny blue sphere. Assume we assign\nthe following numbers to the different property values: matte = 0, shiny = 1, red = 5, blue = 3,\ncylinder = 4, sphere = 6. Thus the two objects are represented by the numbers (0, 5, 4) and (1, 3, 6).\nThe max numbers for each of the properties is (1, 5, 6) and the final integer label is 1 + 5 + 6 = 12.\nSolving this task requires understanding the property values of each object in the scene.\nWe train the underlying models on the entire dataset consisting of all the possible property values.\nThen we randomly select 50 objects for an OOD dataset. Since our task relies on knowing the\nnumerical value of each property, the ID dataset we train on may still contain property values of\nobjects in the out-of-distribution dataset, but it will not contain objects where the combination of\nproperty values is present in the OOD dataset. Thus, when evaluating on the OOD dataset, we are\ntesting the model on novel combinations of property values, even if those property values were\nindividually observed during training. We show the ID and OOD results in Table 5. We see that SVQ\noutperforms the patch-based methods and performs comparably to SysBinder in both ID and OOD\nsettings. This shows that despite adding a discretization bottleneck, the latents in SVQ are still useful\nfor downstream tasks that rely on the properties of the objects in the scene.\n6\nCONCLUSION AND LIMITATIONS\nIn this work, we introduced the Semantic Vector-Quantized Variational Autoencoder. Unlike tradi-\ntional vector quantization methods, our model can obtain semantic neural discrete representations,\ncapturing the rich structure of the objects in a scene. We showed that by training a prior over these\nsemantic discrete tokens, we are able to generate multi-object scenes that follow the underlying\ndata distribution. These semantic discrete representations are also useful for downstream tasks,\noutperforming the representations from patch-based discretation methods. While our model is only\n9\nevaluated on static images, an interesting future direction would be to apply our method to videos\nto predict future frames. This may be fruitful for modeling longer video sequences since SVQ can\ncompress each frame into a set of latents that only depend on the number of objects in the scene.\nLimitations. While our method can learn semantic discrete representations and is capable of using\nthese representations to generate images of higher visual fidelity than previous object-centric methods\nsuch as GENESIS (Engelcke et al., 2020; 2022), it is still only shown to work well on synthetic\ndatasets with similar visual complexity as previous work (Singh et al., 2023). Although scaling\nunsupervised object-centric models to more realistic datasets is not a focus of this work, further\nimproving our model so that it can work well on more realistic scenes is an important avenue of\nfuture research. Another limitation of our model is that our latent representations are all discrete.\nAlthough our visual world does consist of many discrete concepts, factors such as position and pose\nare continuous. It would be interesting to explore ways to combine continuous and discrete factors to\nbetter model realistic scenes.\n7\nETHICS STATEMENT\nThe scope of our study was restricted to visually simple, procedurally generated scenes and in its\ncurrent form does not pose any immediate ethical concerns. Future work, however, that extends the\ncapabilities of our model to work on more complex scenes may have the potential to generate fake,\nrealistic-looking images. The semantic discrete latent may allow users to control scenes in ways that\nwere not previously explored. While this may serve to enhance productivity, such as for artists and\ngraphic designers, it could also be used maliciously in the hands of a bad actor. Future researchers\npursuing this direction should do so under strong ethical standards and be cognizant of the potential\nmisuse of this technology.\n8\nREPRODUCIBILITY STATEMENT\nIn addition to details about our model described in Section 3.1, we provide additional implementation\ndetails in Appendix B, including detailed hyperparameters used in our experiments. We will also\nrelease the source code upon acceptance of the paper.\n9\nACKNOWLEDGMENTS\nThis work is supported by Brain Pool PlusProgram (No. 2021H1D3A2A03103645) and Young\nResearcher Program (No. 2022R1C1C1009443) through the National Research Foundation of Korea\n(NRF) funded by the Ministry of Science and ICT. We thank Gautam Singh for insightful discussions\nand help with the CLEVR datasets. We also thank Sjoerd van Steenkiste for valuable feedback on an\nearlier draft of this paper.\nREFERENCES\nTitas Anciukevicius, Christoph H Lampert, and Paul Henderson. Object-centric image generation\nwith factored depths, locations, and appearances. arXiv preprint arXiv:2004.00642, 2020.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and Dan Klein. Neural module networks. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pp. 39–48, 2016.\nYoshua Bengio, Nicholas Léonard, and Aaron C. Courville. Estimating or propagating gradients\nthrough stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. URL\nhttp://arxiv.org/abs/1308.3432.\nChristopher P Burgess, Loic Matthey, Nicholas Watters, Rishabh Kabra, Irina Higgins, Matt Botvinick,\nand Alexander Lerchner. Monet: Unsupervised scene decomposition and representation. arXiv\npreprint arXiv:1901.11390, 2019.\nChang Chen, Fei Deng, and Sungjin Ahn. ROOTS: Object-centric representation and rendering of\n3D scenes. Journal of Machine Learning Research, 22(259):1–36, 2021. URL http://jmlr.\norg/papers/v22/20-1176.html.\n10\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of\ngated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555, 2014.\nM. Cimpoi, S. Maji, I. Kokkinos, S. Mohamed, , and A. Vedaldi. Describing textures in the wild. In\nProceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2014.\nEric Crawford and Joelle Pineau. Exploiting spatial invariance for scalable unsupervised object\ntracking. arXiv preprint arXiv:1911.09033, 2019a.\nEric Crawford and Joelle Pineau. Spatially invariant unsupervised object detection with convolutional\nneural networks. In Proceedings of AAAI, 2019b.\nPeter Dayan, Geoffrey E Hinton, Radford M Neal, and Richard S Zemel. The helmholtz machine.\nNeural computation, 7(5):889–904, 1995.\nFei Deng, Zhuo Zhi, Donghun Lee, and Sungjin Ahn.\nGenerative scene graph networks.\nIn\nInternational Conference on Learning Representations, 2021. URL https://openreview.\nnet/forum?id=RmcPm9m3tnk.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. CoRR, abs/2005.00341, 2020. URL https://arxiv.\norg/abs/2005.00341.\nAndrea Dittadi, Samuele S. Papa, Michele De Vita, Bernhard Schölkopf, Ole Winther, and Francesco\nLocatello. Generalization and robustness implications in object-centric learning. In International\nConference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,\nvolume 162 of Proceedings of Machine Learning Research, pp. 5221–5285. PMLR, 2022. URL\nhttps://proceedings.mlr.press/v162/dittadi22a.html.\nLaura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann,\nThomas B. McHugh, and Vincent Vanhoucke. Google scanned objects: A high-quality dataset of\n3d scanned household items, 2022. URL https://arxiv.org/abs/2204.11918.\nYilun Du, Kevin Smith, Tomer Ulman, Joshua Tenenbaum, and Jiajun Wu. Unsupervised discovery\nof 3d physical objects from video, 2021.\nMartin Engelcke, Adam R. Kosiorek, Oiwi Parker Jones, and Ingmar Posner. GENESIS: generative\nscene inference and sampling with object-centric latent representations. In 8th International\nConference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net, 2020. URL https://openreview.net/forum?id=BkxfaTVFwH.\nMartin Engelcke, Oiwi Parker Jones, and Ingmar Posner. Genesis-v2: Inferring unordered object\nrepresentations without iterative refinement, 2022.\nSM Ali Eslami, Nicolas Heess, Theophane Weber, Yuval Tassa, David Szepesvari, and Geoffrey E\nHinton. Attend, infer, repeat: Fast scene understanding with generative models. In Advances in\nNeural Information Processing Systems, pp. 3225–3233, 2016.\nPatrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image\nsynthesis.\nIn IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021,\nvirtual, June 19-25, 2021, pp. 12873–12883. Computer Vision Foundation / IEEE, 2021. doi:\n10.1109/CVPR46437.2021.01268. URL https://openaccess.thecvf.com/content/\nCVPR2021/html/Esser_Taming_Transformers_for_High-Resolution_\nImage_Synthesis_CVPR_2021_paper.html.\nAnand Gopalakrishnan, Kazuki Irie, Jürgen Schmidhuber, and Sjoerd van Steenkiste. Unsupervised\nlearning of temporal abstractions with slot-based transformers. arXiv preprint arXiv:2203.13573,\n2022.\nRobert M. Gray. Vector quantization. IEEE ASSP Magazine, 1:4–29, 1984. URL https://api.\nsemanticscholar.org/CorpusID:14754287.\nKlaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. Neural expectation maximization. In\nAdvances in Neural Information Processing Systems, pp. 6691–6701, 2017.\n11\nKlaus Greff, Raphaël Lopez Kaufmann, Rishab Kabra, Nick Watters, Chris Burgess, Daniel Zoran,\nLoic Matthey, Matthew Botvinick, and Alexander Lerchner. Multi-object representation learning\nwith iterative variational inference. arXiv preprint arXiv:1903.00450, 2019.\nKlaus Greff, Sjoerd van Steenkiste, and Jürgen Schmidhuber. On the binding problem in artificial\nneural networks. arXiv preprint arXiv:2012.05208, 2020.\nDanijar Hafner, Timothy Lillicrap, Jimmy Ba, and Mohammad Norouzi. Dream to control: Learning\nbehaviors by latent imagination. arXiv preprint arXiv:1912.01603, 2019.\nOlivier J Hénaff, Skanda Koppula, Evan Shelhamer, Daniel Zoran, Andrew Jaegle, Andrew Zisserman,\nJoão Carreira, and Relja Arandjelovi´c. Object discovery and representation networks. In ECCV,\npp. 123–143. Springer, 2022.\nJindong Jiang and Sungjin Ahn. Generative neurosymbolic machines. In Advances in Neural\nInformation Processing Systems, 2020.\nJindong Jiang, Sepehr Janghorbani, Gerard De Melo, and Sungjin Ahn. Scalor: Generative world mod-\nels with scalable object representations. In International Conference on Learning Representations,\n2019.\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C Lawrence Zitnick, and\nRoss Girshick. Clevr: A diagnostic dataset for compositional language and elementary visual\nreasoning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npp. 2901–2910, 2017.\nRishabh Kabra, Daniel Zoran, Goker Erdogan, Loic Matthey, Antonia Creswell, Matthew Botvinick,\nAlexander Lerchner, and Christopher P. Burgess. Simone: View-invariant, temporally-abstracted\nobject representations via unsupervised video decomposition. arXiv preprint arXiv:2106.03849,\n2021.\nLaurynas\nKarazija,\nIro\nLaina,\nand\nChristian\nRupprecht.\nClevrtex:\nA\ntexture-rich\nbenchmark for unsupervised multi-object segmentation.\nIn Proceedings of the Neu-\nral Information Processing Systems Track on Datasets and Benchmarks 1,\nNeurIPS\nDatasets\nand\nBenchmarks\n2021,\nDecember\n2021,\nvirtual,\n2021.\nURL\nhttps:\n//datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/\ne2c420d928d4bf8ce0ff2ec19b371514-Abstract-round2.html.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In 3rd International\nConference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings, 2015. URL http://arxiv.org/abs/1412.6980.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nThomas Kipf, Gamaleldin F. Elsayed, Aravindh Mahendran, Austin Stone, Sara Sabour, Georg\nHeigold, Rico Jonschkowski, Alexey Dosovitskiy, and Klaus Greff. Conditional Object-Centric\nLearning from Video. In International Conference on Learning Representations (ICLR), 2022.\nDaniil Kirilenko, Alexandr Korchemnyi, Alexey Kovalev, and Aleksandr Panov. Quantized disentan-\ngled representations for object-centric visual tasks, 2023. URL https://openreview.net/\nforum?id=JIptuwnqwn.\nBrenden M. Lake, Tomer D. Ullman, Joshua B. Tenenbaum, and Samuel J. Gershman. Building\nmachines that learn and think like people. CoRR, abs/1604.00289, 2016. URL http://arxiv.\norg/abs/1604.00289.\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Jindong Jiang, and Sungjin Ahn. Improving\ngenerative imagination in object-centric world models. In International Conference on Machine\nLearning, pp. 4114–4124, 2020a.\nZhixuan Lin, Yi-Fu Wu, Skand Vishwanath Peri, Weihao Sun, Gautam Singh, Fei Deng, Jindong\nJiang, and Sungjin Ahn. Space: Unsupervised object-oriented scene representation via spatial\nattention and decomposition. In International Conference on Learning Representations, 2020b.\n12\nFrancesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold,\nJakob Uszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention,\n2020.\nMarcelo G Mattar and Máté Lengyel. Planning in the brain. Neuron, 110(6):914–934, 2022.\nStephen E. Palmer.\nHierarchical structure in perceptual representation.\nCognitive Psychol-\nogy, 9(4):441–474, 1977.\nISSN 0010-0285.\ndoi: https://doi.org/10.1016/0010-0285(77)\n90016-0.\nURL\nhttps://www.sciencedirect.com/science/article/pii/\n0010028577900160.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139\nof Proceedings of Machine Learning Research, pp. 8821–8831. PMLR, 2021. URL http:\n//proceedings.mlr.press/v139/ramesh21a.html.\nAli Razavi, Aäron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with\nVQ-VAE-2. In Advances in Neural Information Processing Systems 32: Annual Conference on\nNeural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pp. 14837–14847, 2019. URL https://proceedings.neurips.cc/paper/\n2019/hash/5f8e2fa1718d1bbcadf1cd9c7a54fb8c-Abstract.html.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\nvariational inference in deep latent gaussian models. In International Conference on Machine\nLearning, volume 2, 2014.\nMaximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann\nSimon-Gabriel, Tong He, Zheng Zhang, Bernhard Scholkopf, Thomas Brox, and Francesco\nLocatello. Bridging the gap to real-world object-centric learning. arXiv preprint arXiv:2209.14860,\n2022.\nWolf Singer. Binding by synchrony. Scholarpedia, 2:1657, 2007.\nGautam Singh, Fei Deng, and Sungjin Ahn. Illiterate DALL-E learns to compose. In The Tenth Inter-\nnational Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net, 2022a. URL https://openreview.net/forum?id=h0OYV0We3oh.\nGautam\nSingh,\nYi-Fu\nWu,\nand\nSungjin\nAhn.\nSimple\nunsupervised\nobject-\ncentric\nlearning\nfor\ncomplex\nand\nnaturalistic\nvideos.\nIn\nNeurIPS,\n2022b.\nURL\nhttp://papers.nips.cc/paper_files/paper/2022/hash/\n735c847a07bf6dd4486ca1ace242a88c-Abstract-Conference.html.\nGautam Singh, Yeongbin Kim, and Sungjin Ahn. Neural systematic binder. In The Eleventh\nInternational Conference on Learning Representations, 2023. URL https://openreview.\nnet/forum?id=ZPHE4fht19t.\nElizabeth S Spelke and Katherine D Kinzler. Core knowledge. Developmental science, 10(1):89–96,\n2007.\nYao-Hung Hubert Tsai, Nitish Srivastava, Hanlin Goh, and Ruslan Salakhutdinov. Capsules with\ninverted dot-product attention routing. In 8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL\nhttps://openreview.net/forum?id=HJe6uANtwH.\nAaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional\nimage generation with pixelcnn decoders. In Advances in neural information processing systems,\npp. 4790–4798, 2016.\nAäron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu.\nNeural discrete representation\nlearning. In Advances in Neural Information Processing Systems 30: Annual Conference on\nNeural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pp.\n6306–6315, 2017. URL https://proceedings.neurips.cc/paper/2017/hash/\n7a98af17e63a0ac09ce2e96d03992fbc-Abstract.html.\n13\nJulius von Kügelgen, Ivan Ustyuzhaninov, Peter Gehler, Matthias Bethge, and Bernhard Schölkopf.\nTowards causal generative scene models via competition of experts, 2020.\nNicholas J Wade. The vision of helmholtz. Journal of the History of the Neurosciences, 30(4):\n405–424, 2021.\nXudong Wang, Rohit Girdhar, Stella X. Yu, and Ishan Misra. Cut and learn for unsupervised object\ndetection and instance segmentation, 2023a.\nYanbo Wang, Letao Liu, and Justin Dauwels. Slot-vae: Object-centric scene generation with slot\nattention. In International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Hon-\nolulu, Hawaii, USA, volume 202 of Proceedings of Machine Learning Research, pp. 36020–36035.\nPMLR, 2023b. URL https://proceedings.mlr.press/v202/wang23r.html.\nNicholas Watters,\nLoic Matthey,\nSebastian Borgeaud,\nRishabh Kabra,\nand Alexander\nLerchner.\nSpriteworld:\nA flexible, configurable reinforcement learning environment.\nhttps://github.com/deepmind/spriteworld/, 2019a. URL https://github.com/deepmind/\nspriteworld/.\nNicholas Watters, Loïc Matthey, Christopher P. Burgess, and Alexander Lerchner. Spatial broad-\ncast decoder: A simple architecture for learning disentangled representations in vaes. CoRR,\nabs/1901.07017, 2019b. URL http://arxiv.org/abs/1901.07017.\nTaylor W. Webb, Shanka Subhra Mondal, and Jonathan D. Cohen. Systematic visual reasoning\nthrough object-centric relational abstraction. CoRR, abs/2306.02500, 2023a. doi: 10.48550/arXiv.\n2306.02500. URL https://doi.org/10.48550/arXiv.2306.02500.\nTaylor W. Webb, Shanka Subhra Mondal, and Jonathan D. Cohen. Systematic visual reasoning\nthrough object-centric relational abstraction. CoRR, abs/2306.02500, 2023b. doi: 10.48550/arXiv.\n2306.02500. URL https://doi.org/10.48550/arXiv.2306.02500.\nXin Wen, Bingchen Zhao, Anlin Zheng, X. Zhang, and Xiaojuan Qi. Self-supervised visual represen-\ntation learning with semantic grouping. arXiv preprint arXiv:2205.15288, 2022.\nYi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Generative video transformer: Can objects be the words?\nIn International Conference on Machine Learning, pp. 11307–11318. PMLR, 2021.\nWilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas. Videogpt: Video generation using\nVQ-VAE and transformers. CoRR, abs/2104.10157, 2021. URL https://arxiv.org/abs/\n2104.10157.\nJaesik Yoon, Yi-Fu Wu, Heechul Bae, and Sungjin Ahn. An investigation into pre-training object-\ncentric representations for reinforcement learning. CoRR, abs/2302.04419, 2023. doi: 10.48550/\narXiv.2302.04419. URL https://doi.org/10.48550/arXiv.2302.04419.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQGAN.\nIn The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=\npfNyExj7z2.\nRuixiang Zhang, Tong Che, B. Ivanovic, Renhao Wang, Marco Pavone, Yoshua Bengio, and Liam\nPaull. Robust and controllable object-centric learning through energy-based models. arXiv preprint\narXiv:2210.05519, 2022.\nDaniel Zoran, Rishabh Kabra, Alexander Lerchner, and Danilo J Rezende. Parts: Unsupervised seg-\nmentation with slots, attention and independence maximization. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 10439–10447, 2021.\n14\nA\nADDITIONAL EXPERIMENTAL RESULTS\nFigure 5: Sample scene we use in our codebook analysis.\nA.1\nCODEBOOK ANALYSIS\nLatent Traversal. In this section, we qualitatively analyze the codebook for a sample scene. Figure\n5 shows the sample we will use in our analysis. First, we run the image through the pretrained SVQ\nencoder to obtain a set of semantic discrete latents. Each latent represents one block from one slot\nand is provided by a prototype vector in the corresponding codebook for that block. To investigate\nthe effect of traversing through the codebook, we replace each block with a different code in the\ncodebook while keeping all other latents fixed. We then reconstruct the scene with the SVQ decoder\nand dVAE, essentially generating a new image that only differs from the original image by one\ndiscrete latent.\nFigure 6: Latent traversal changing one latent in one block at a time while keeping all other latents fixed. The\nimage is then reconstructed with the single changed latent.\nFigure 6 shows the results for several sample blocks for the first slot (which corresponds to the\nteal ball) and the fourth slot (which corresponds to the gray cylinder). For each block, we choose\nthe same set of 16 prototype vectors to display. First, we see that the slots are disentangled at the\nobject level—changing one block in one slot does not affect the other objects. We also see that the\ndifferent blocks specialize in different factors. Block 1 corresponds to the left and right placement\nof the object. Block 3 also corresponds to the placement of the object, but seems to also control the\nforward and backward placement of the object, as well as the size of the object. We notice that in this\nparticular case, the factors of position and size are not completely disentangled. This may be because\nin this scene, the size depends on the placement of the object (e.g. closer objects are bigger). Block 7\ncontrols the color of the object. We see that the same prototype vector seems to produce the same\ncolor, although there are some inconsistencies such as the disappearing cylinder in the bottom left.\n15\nThe color also seems to be cleanly disentangled from the other factors—changing the color does not\naffect other factors like shape, size, or position.\nBlock Analysis. Next, to further explore the representation captured in the codebook, we visualize\nthe objects that are attended to for different prototype vectors. To achieve this, we run the pretrained\nSVQ on 1000 images obtaining the semantic discrete latents and slot attention segmentation maps for\nthe objects in the images. Then, for each prototype vector in the codebook, we find and visualize the\ncorresponding slots that are utilizing that code in one of its blocks. Note that unlike Singh et al. (2023),\nwe do not need to do any k-means clustering to obtain this visualization since our representations are\ndiscrete representations in the codebook. Figures 7 and 8 show sample objects corresponding to three\ndifferent prototype vectors for block 3 and block 7. We see that block 3 corresponds to object size\nand block 7 corresponds to object color. These results are consistent with the previous latent traversal\nexperiments. Furthermore, the three prototype vectors we chose for block 7 correspond with the first\nthree latents in Figure 6 (right), showing that these three prototype vectors represent gray, purple, and\nteal, respectively.\nFigure 7: Objects attended to when the latent for block 3 is set to three different prototype vectors.\nFigure 8: Objects attended to when the latent for block 7 is set to three different prototype vectors.\nA.2\nCOMPARISON WITH SLOT-LEVEL QUANTIZATION\nAs discussed in Section 3.1, we hypothesize that slot-level discretization would struggle with complex\nscenes due to the combinatorial nature of the underlying factors of the objects. We test this hypothesis\nby running experiments on 2D Sprites and CLEVR-Easy where we set the number of blocks M to 1\nand tune the size of the codebook, essentially doing slot-level quantization. In Figures 9, we show\nthe masked attention of each slot on the input image as well as the image reconstruction. We find\nthat with slot-level quantization, the model completely fails on the CLEVR-Easy dataset, unable\nto cleanly attend to the objects and reconstruct the image. On the 2D sprites dataset, we see that\nwith slot discretization, one slot ends up attending to all the foreground objects and the model still\ncannot reconstruct the input image correctly. These results point to the importance of our choice to\ndo block-level discretization.\n16\nFigure 9: Comparison of slot discretization and block discretization on CLEVR-Easy (top) and 2D sprites\n(bottom).\nA.3\nPRIOR MODEL CAPACITY FOR DVAE\nIn order to evaluate whether or not a larger capacity prior may improve the results for the patch-based\ndVAE baseline, we ran ablations using larger transformers for the dVAE prior on the CLEVR-Hard\ndataset. The results are presented in Table 6. We see that while increasing the size of the transformer\nfor the prior does slightly improve the FID for dVAE, it still underperforms when compared to SVQ,\nindicating that simply scaling the dVAE prior may not be sufficient to match SVQ performance.\nTable 6: Effect of increasing the dVAE prior model capacity on FID on the CLEVR-Hard dataset.\nPrior Model\nFID\ndVAE (8-layer)\n65.89\ndVAE (12-layer)\n61.74\ndVAE (16-layer)\n60.75\nSVQ (8-layer)\n43.12\nA.4\nNUMBER OF BLOCKS ABLATION\nTable 7 shows the results of changing the number of blocks in SVQ on the 2D Sprites (3 obj) dataset.\nWe see that when the number of blocks is too small, the model performs poorly and fails to generate\nscenes corresponding to the data distribution. For a sufficiently large number of blocks, the model\nis able to segment the scene, but Generation Accuracy decreases when the number of blocks is too\nlarge. We suspect this to be because with more blocks, the model may require a higher capacity prior,\nwhich we kept fixed in this ablation.\nA.5\nCODEBOOK SIZE ABLATION\nTable 8 shows the results of changing the codebook size in SVQ on the 2D Sprites (3 obj) dataset.\nWe had also tried smaller codebook sizes of 4 and 16, but found that for codebook sizes smaller than\n32, the SVQ model did not converge well, resulting in black reconstructions. Similar to the number\nof blocks ablations, we see that for larger codebook sizes, the FID scores are similar, but generation\naccuracy decreases for codebook sizes larger than 64. This again may be because the larger codebook\nsizes require a higher capacity prior, which was fixed in these ablations.\n17\nTable 7: Effect of changing the number of blocks in SVQ on the 2D Sprites (3 obj) dataset.\nNumber of Blocks\nFID\nGeneration Accuracy (in %)\n1\n465.60\n0.00\n2\n80.71\n1.56\n4\n7.76\n75.78\n8\n6.61\n75.00\n16\n7.17\n55.47\n32\n8.74\n54.69\nTable 8: Effect of changing the codebook size in SVQ on the 2D Sprites (3 obj) dataset.\nCodebook Size\nFID\nGeneration Accuracy (in %)\n32\n7.31\n76.56\n64\n6.61\n75.00\n96\n6.64\n51.56\n128\n7.58\n48.44\n256\n8.73\n32.81\nA.6\nFG-ARI SEGMENTATION RESULTS\nTable 9 shows the Foreground Adjusted-Rand-Index (FG-ARI) metric for the CLEVR datasets for\ndifferent slot-based models. We see that when compared to SysBinder, SVQ performs similarly in\nterms of FG-ARI on CLEVR-Easy and CLEVR-Hard and slightly underperforms on CLEVR-Tex.\nCompared to vanilla Slot Attention, SVQ achieves higher FG-ARI on all 3 datasets.\nTable 9: FG-ARI results on CLEVR datasets.\nSlot Attention\nSLATE\nSysBinder\nSVQ\nCLEVR\n85.85\n91.65\n92.58\n91.37\nCLEVR-Hard\n81.29\n76.79\n90.43\n90.48\nCLEVR-Tex\n24.67\n73.85\n78.12\n70.93\nA.7\nEXPERIMENTS ON GOOGLE SCANNED OBJECTS\nIn order to evaluate SVQ on a more realistic dataset, we use a dataset where the objects are taken from\nthe Google Scanned Objects (Downs et al., 2022). Specifically, we use the objects from the \"Shoe\"\nand \"Bottles and Cans and Cups\" categories. We evaluate on two versions of SVQ: SVQ-small uses\nthe same hyperparameters as we used for the CLEVR-Hard dataset (see Table 11) and SVQ-large\nincreases the codebook size to 256 and increases the size of the transformer decoder from 8 layers, 4\nheads, model size 192 to 16 layers, 8 heads, model size 512.\nWe present the FID results in Table 10 and qualitative samples in Figure 10. We see that while\nSVQ-small is able to generate objects from the dataset, the objects are smoothed out, resulting in\na high FID score. Increasing the model size to SVQ-large significantly improves the quality of the\ngenerated scenes and the FID score, providing some evidence that SVQ can be scaled to work on\nmore realistic datasets.\nB\nIMPLEMENTATION DETAILS\nB.1\nTRAINING AND IMPLEMENTATION DETAILS.\nWe use input images of 64x64 resolution for the 2D Sprites datasets and 128x128 for the CLEVR\ndatasets. Each model is trained on NVIDIA Quadro RTX 8000 GPUs with 48GB memory and we\n18\nTable 10: FID for the Google Scanned Objects dataset.\nModel\nFID\nSVQ-small\n114.16\nSVQ-large\n72.68\nFigure 10: Samples for SVQ on the Google Scanned Objects dataset. Scaling to a larger model size noticeably\nimproves the quality of the samples.\nuse half-precision floating-point format. We train SVQ for 400k iterations which takes around 80\nhours for the CLEVR datasets and 50 hours for the 2D datasets. We then train the prior for 1 million\niterations which takes around 40 hours. For the 2D Sprites dataset, similar to (Yoon et al., 2023), we\nfirst train the underlying models on a dataset of random shapes without any relationship between the\nobjects. We then train the prior models on the odd-one-out datasets.\nB.2\nHYPERPARAMETERS\nDataset\nModule\nHyperparameter\nCLEVR-Easy\nCLEVR-Hard\n2D Sprites\n2D Sprites w/ BG\nGeneral\nBatch Size\n40\n40\n40\n40\nTraining Steps\n400K\n400K\n400K\n400K\nImage Size\n128 × 128\n128 × 128\n64 × 64\n64 × 64\nSVQ\nCodebook Dimension\n256\n128\n256\n32\n# Blocks\n8\n16\n8\n8\nCodebook Size\n64\n64\n64\n128\n# Iterations\n3\n3\n3\n3\n# Slots\n4\n4\n6\n8\nβ\n50\n50\n50\n50\nLearning Rate\n0.0001\n0.0001\n0.0001\n0.0001\nTable 11: Hyperparameters of our model used in our experiments.\nTable 11 shows the hyperparameters we used for the different datasets in our experiments with SVQ.\nFor the dVAE and Transformer Decoder, we follow the hyperparameters, architecture, and training\nprocedure provided in Singh et al. (2023) for CLEVR-Easy and CLEVR-Hard. For the 2D Sprites\ndatasets, we use the same hyperparameters as we do for CLEVR-Easy for those components. All\nmodels are trained with the Adam optimizer (Kingma & Ba, 2015) with β1 = 0.9 and β2 = 0.999.\n19\nB.3\nPRIOR MODELS\nFor the SVQ and DVAE prior models, we use a transformer architecture with 8 layers, 4 heads, model\ndimension 192, feedforward dimension 768, and a dropout probability of 0.1. We use a learning rate\nof 0.0003 and 30,000 warmup steps. For VQ-VAE, we use a 20-layers PixelCNN prior, as proposed\nin the original paper (van den Oord et al., 2017).\nB.4\nDOWNSTREAM MODELS\nFor the 2D Sprites downstream experiments, we use a transformer architecture with 3 layers, 8 heads,\nmodel dimension 192, feedforward dimension 768, and a dropout probability of 0.1 for all models.\nWe use the Adam optimizer with a learning rate of 0.0003.\nFor the CLEVR-Hard downstream experiments, we use a transformer architecture with 8 layers, 4\nheads, model dimension 192, feedforward dimension 768, and a dropout probability of 0.1 for all\nmodels. We use the Adam optimizer with a learning rate of 0.0001.\n20\n"
}