{
    "optim": "Draft Fast gradient-free activation maximization for neurons in spiking neural networks Nikita Pospelov1∗ Andrei Chertkov2,3 Maxim Beketov4 Ivan Oseledets2,3 Konstantin Anokhin1 1Laboratory of Neuronal Intelligence, Institute for Advanced Brain Studies, Lomonosov Moscow State University 2Artificial Intelligence Research Institute (AIRI) 3Skolkovo Institute of Science and Technology 4HSE University Moscow, Russia Abstract Neural networks (NNs), both living and artificial, work due to being complex systems of neurons, each having its own specialization. Revealing these specializations is important for understanding NNs’ inner working mechanisms. The only way to do this for a living system, the neural response of which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop of exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction of maximal response. To test such a loop on a living network, one should first learn how to run it quickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons’ activa- tion) in least possible number of iterations. We present a framework with an effective design of such a loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the behaviour of NNs in living brains). Our optimization method used for activation maximization (AM) was based on low-rank tensor decomposition (Tensor Train, TT) of the activation function’s discretization over its domain – the latent parameter space of stimuli (CIFAR10-size color images, generated by either VQ-VAE or SN-GAN from their latent description vectors, fed to the SNN). To our knowledge, the present work is the first attempt to perform effective AM for SNNs. The source code of our framework, MANGO (for Maximization of neural Activation via Non-Gradient Optimization) is available on GitHub1. Keywords: neuron specialization, activation maximization, effective stimuli, neural representation, derivative-free optimization, spiking neural networks, Tensor-Train decomposition, generative models: (Vector-Quantized) Variational AutoEncoders (VQ-VAEs), (Spectrally Normalized) Generative Adversarial Networks (SN-GANs) Contents 1 Introduction and problem statement 2 2 Background 3 2.1 Activation Maximization in artificial and living neural networks . . . . . . . . . . . . . . . 3 2.2 Spiking Neural Networks (SNNs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3 Methods 6 3.1 Tensor Train (TT) decomposition and TT-based optimization techniques . . . . . . . . . 6 3.2 Generative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 3.2.1 Vector-Quantized Variational AutoEncoders (VQ-VAEs) . . . . . . . . . . . . . . . 8 3.2.2 Generative Adversarial Networks (GANs) . . . . . . . . . . . . . . . . . . . . . . . 9 ∗Corresponding author, nik-pos@yandex.ru 1https://github.com/iabs-neuro/mango 1 arXiv:2401.10748v1  [cs.NE]  28 Dec 2023 Draft 3.3 Spiking neural network architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.4 Optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 3.5 MANGO framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 4 Results 10 4.1 Tensor Train-based optimization methods outperform non-gradient benchmarks . . . . . . 10 4.2 Comparison of GAN- and VAE- based generators . . . . . . . . . . . . . . . . . . . . . . . 11 4.3 Highly selective neurons are present in early layers of the network . . . . . . . . . . . . . . 13 4.4 MEIs are more diverse in deep layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 5 Discussion 16 5.1 Limitations of the current work and directions of future research . . . . . . . . . . . . . . 16 5.2 Activation maximization in ANNs & explainable AI . . . . . . . . . . . . . . . . . . . . . 16 5.3 Possible applications for maximization of activation in biological neurons . . . . . . . . . . 16 6 Acknowledgements 17 7 References 17 1 Introduction and problem statement Brain neurons specialize on certain properties of stimuli – e.g. low-level neurons of the vision cortex specialise on simple properties of the visible image: the direction of movement [1] or color [2], while higher-level neurons specialize on more complex ones, e.g. presence of a face within the field of view. Identifying the specialization of living neurons can be done by varying the properties of the stimulus (e.g. image shown to the the object) in an iterative manner to find those which promote most intensive response [3, 4] – these perspective approaches were termed activation maximization (AM). However, to solve such a problem effectively in vivo one needs to 1. find an optimal stimulus as quickly (in as few iterations) and accurately as possible 2. investigate this stimulus space – for possibly multiple activation optima, local vs global, etc. This is especially important in light of ethical considerations [5, 6] of such studies in neuroscience – the faster & the better one gets such results – the less stressful it is for the living object of study, and the less objects are needed (needless to say, if the results are reproducible over a population). This challenge requires developing and incorporating new mathematical optimization methods into such experimental setups, allowing to obtain best approximations to the global activation optimum at a minimal number of iterations (of exposing the living object to stimuli – e.g. images or sounds). Modern artificial neural networks (ANNs), however variable they are in architecture types and ap- proaches to training them, exhibit very similar selectivity to complex stimuli [7] (though whether their learning mechanisms are similar to those in living NNs, and thus whether one can directly compare ones to the others – is a matter of ongoing debates [8, 9]). Interestingly, just like in living brains, ANN neurons can have multiple specializations each [10]. Since (most) ANNs are essentially computational graphs of functions parameterized by layers’ weights, with modern computational hardware (GPUs and TPUs) being quite capable of effectively computing (in parallel) both their value at a point (for inference) and, more importantly, their derivatives w.r.t. parameters (to backpropagate the error for training), one can relatively easily “dissect” ANNs and anatomize the “responsibility areas” of each of the artificial neurons [11]. This makes ANNs perfect objects for exercising such activation maximization techniques. Thus, in [12], 15 various methods based on genetic and evolutionary optimization algorithms were compared in maximizing the response of a certain neuron in a deep ANN by generating new artificial images with a pre-trained generative adversarial network (GAN) [13]. A wider comparative analysis of various op- timization methods was done in [14], including genetic and evolutionary algorithms, where an original approach based on Tensor Train decomposition (TT-decomposition, [15]) was shown to have significant advantages – in speed, accuracy and stability (at the same iteration count). A more biologically realistic model of living NNs is the family of (artificial) spiking neural networks (SNNs) [16], the activity of which unfolds in time – in contrast to ordinary “feedforward” NNs, which are, mathematically, just functions, without any explicit temporal dynamics. With that biological relevance in mind, in present work we focus on activation maximization of SNN’s neurons. Quite recently, inner 2 Draft neural representations of data in SNNs were studied [17] and compared to that in ordinary ANNs. In our work we attempt a related but different problem – of finding effective stimuli (MEIs for Most Exciting Images, termed in [18] – maximizing certain neurons’ activations) specifically for SNNs effectively (in least number of iterations and as quickly as possible, with a view towards future applications to living objects) – to our knowledge, for the first time. That said, futher development of neuronal activations approximation and global optimization meth- ods based on TT-decomposition, applied to ANNs in general, seems very perspective. Modified appro- priately, such an approach could then be transferred to studies of living objects (for a quick and effective maximization of biological neurons’ response). Or it could be used within the developing field of inter- pretable AI and ANN visualisation ([11], also see [19] for an overview) – helping to establish connections between the workings of natural and artificial intelligence. 2 Background 2.1 Activation Maximization in artificial and living neural networks Our objective in the present work is to optimize the activation of a certain neuron in a (spiking) ANN by finding its most exciting input (MEI) stimulus. This problem of activation maximization (AM) stands at the crossroads of contemporary neuroscience & deep learning (DL). In DL, well-known works of XDream [20], Inception loops [18] – demonstrated how one could use generative models’ ability to effectively map the latent space of stimuli (images) for AM on ANNs. In neuroscience, some outstanding recent results show that one can study the function of living neurons by utilizing the above approach – incorporating a generative model like XDream into a feedback loop of exposing a living object (a mammal with well-developed vision system) to the generated images: in [3], XDream was used to reveal the MEIs of macaque monkeys’ visual cortex; in [4] this approach was taken further to find out that these MEIs not necessarily encode exactly faces, but sometimes abstract objects (many of them looking face-like, though)! With the present work we aim to contribute to this rapidly developing field of incorporating DL techniques in neuroscience to answer longstanding questions of the latter – by introducing a pipeline in which this activation maximization problem can be solved effectively, requiring less iterations to converge to a MEI. With the activation intensity/frequency of a certain neuron viewed, mathematically, as a function of the input “fed” to the NN (– be it a living or artificial one, the neuron is part of), MEI is basically MEIi def = arg max x∈StimulusSpace Activationi neuron(x) (1) the arg max of some function, A(x), which 1. can only be measured (computed) at a point x – in several points, but in the less – the better 2. is measured with noise. This is informally referred to as black box function optimization problem – without explicit access to the gradient (derivatives) of the function, it is a whole research field of its own. Optimization methods in general can be roughly divided into 1. Gradient-based methods – dating back to Newton and his method, that have dramatically evolved, with variations of Stochastic Gradient Descent (SGD) powering the training of millions of instances of NN-based machine learning models around the world today 2. Gradiend-free methods – of which there are numerous families: both deterministic and proba- bilistic (stochastic), evolutionary (genetic) algorithms, etc. Gradient-free methods are naturally more appropriate to the problem of AM in real-world systems due to activation being a black-box-like function. Of course, even if not explicitly accessible, the gradient can be approximated by finite differences, but 1) if the function’s landscape is very “wobbly” – quicky changing – the error of gradient estimates will be effectively unbounded; and 2) noise that is present in measurements of function’s values will also contribute to the error of gradient estimates. 3 Draft In the context of AM, quite recently such gradient-free methods have been explored [21], and ex- tensively compared to each other [12] – both in silico and in vivo, with Covariance Matrix Adaptation (CMA) outperforming most in both settings. In present work, we introduce (for the first time) another family of gradient-free optimization al- gorithms in the problem of SNN’s neuron activation maximization – ones based on low-rank tensor decompositions (Tensor Train, [15]) of the optimized function. Some of these methods are essentially search for a maximum in an array, but a smart one, utilizing the low-rank tensor nature of the the ar- ray, thus achieving exponential speedup over straightforward search. Others belong to the Monte-Carly (MC) family of methods, which are less prone to being stuck in local optima by stochastically “jumping” around the array, not necessarily towards the optimum. See the corresponding section of Methods for a self-contained introduction of these methods. 2.2 Spiking Neural Networks (SNNs) Ordinary ANNs are, from a mathematical point of view, just functions – given input data, they trans- form it in a parameterized nonlinear way to turn it into output – such ANNs are called feedforward (of course, the field of contemporary machine learning, and its most productive sub-field of deep learning, DL, [22, 23], is rich in further complications of this idea, e.g. NN-based generative models can be said to be non-deterministic functions, but for our purposes we limit ourselves to the basics). The activity of biological NNs (in living brains) always unfolds in time: over time, living neurons accumulate signals from their neighbours they share synaptic connections with, and, if the overall (voltage) amplitude of incoming signals (membrane potential, MP) exceeds some threshold, the neuron “fires” – emits what’s called a spike, a signal the form of which is very special (and well-studied [24]) due to the bio-chemical dynamics that governs its emission – that travels along the synapse to neuron’s neighbours – to provoke them to spike in their turn. This inherent temporal nature of biological NNs was overlooked when the first artificial models of living NNs, namely the 1943 neuron model of McCulloch & Pitts [25], further developed and implemented in hardware and termed Perceptron by Rosenblatt [26, 27] in 1957-58, were introduced – perhaps due to hardware limitations of the time, implementing “feedforward” functions lack- ing any oscillatory internal dynamics was more straightforward. The invention of error back-propagation algorithm [28] in 1986 as a useful method for training such ANNs (e.g. to perform classification tasks after learning on exemplar data) further cemented the dominance of ANNs of feedforward nature as machine models of intelligence. It was Maass’s work [29] in 1997 that introduced networks of artificial spiking neurons as a model of biological NNs. Figure 1: Schematic of a spiking neuron of an SNN (image from [30]) An (artificial) spiking neural network (SNN), just like a biological one, can be thought of as a dynamical system, with each neuron being a subsystem with its own internal dynamics. Neurons are connected with each other, forming layers with connections being directional. Over time, a neuron (see Fig.1) receives signals from its sourcing neighbours – signals being aggregated (summed, basically) with weights designating the importance of connections – to give the total value of “membrane potential”. If this value exceeds a certain threshold (one of the hyperparameters of this model), the neuron “fires” – emits a spike into its output. For this to be effectively modelled digitally, this dynamics is discretized – the time-dependence of signal is now basically encoded by an array of numbers (signal amplitudes), the blue bars seen on Fig.1 are (significantly) non-zero elements of such arrays, all that being a discrete model of what’s called a spike train (term used for signals in biological NNs as well). As for the goodness of such approximation of (the envelope of) signals in biological NNs, this discretization is not too far-fetched – spikes in the brain are indeed nearly “binary” – MP changes in almost discrete steps. 4 Draft Nowadays there are several software implementations of artificial SNNs, with at least two outstand- ing frameworks – snnTorch [31] & SpikingJelly [32], allowing to do complete training and inference procedures – these two were used in present work. There are several important questions to answer when working with such a model. First, how is input data encoded for the SNN to process it. In our work we “expose” an SNN to static images. For that there are at least two approaches [31]: 1. simply repeating the image (unaltered) a certain number of frames (kind of a static video) 2. showing the image on a certain number of frames, but altering it with some sort of noise. We’ve chosen the first approach for simplicity, the image being repeated from 20 to 100 frames. Each frame is first fed to a pre-trained (feedforward) convolutional neural network (CNN) with residual blocks (spiking analogue of ResNet [33]) – for it to extract basic image features for our SNN to process. Second, what exact model of a spiking neuron does one use? The above description is very schematic, one needs to specify a certain mechanistic model (an electric circuit, to be precise) of the living neuron, so that the dynamics of that model accurately approximated reality. For practical appli- cations, Hodgkin-Huxley model [34], while being most biologically (physically) accurate, is complicated to implement and work with. So a practical choice is the one of so-called Leaky Integrate-and-Fire (LIF) neuron models, that date back to Louis Lapicque’s 1907 work [35]. Figure 2: Spiking neuron models (image from snnTorch online tutorial [31]) In LIF models, a spiking neuron is represented as an RC circuit – instead of directly summing the incoming (spike) voltages, such a neuron integrates input signal over time with leakage (due to R for Resistance present in the circuit). The LIF neuron abstracts away the shape and profile of the output spike; it is simply treated as a discrete event. As a result, information is not stored within the spike, but rather the timing (or frequency) of spikes. Discrete-time equations of the spiking dynamics for such a neuron thus are: S[t] = Θ(U[t] − Uthr) = ( 1, if U[t] > Uthr 0, otherwise (2) with S[t] being the signal (spike) intensity of a neuron at (a discrete) moment of time t, U[t] being the MP. With Θ(x) being Heaviside’s step function, a neuron discretely fires if its MP U[t] exceeds a threshold value. The RC-circuit nature of a LIF neuron is described by the following temporal dynamics equation on U[t]: U[t + 1] = βU[t] | {z } decay + WX[t + 1] | {z } input − S[t]Uthr | {z } reset (3) 5 Draft – at next moment of time, t + 1, the MP is given by a linear combination (with learnable weights W) of inputs X[t] from sourcing neigbhour neurons; but there’s also some decay (leakage) and a reset term. With Uthr set to, say, 1, the only hyperparameter left to set (governing the dynamics) is the decay rate, β. Third, given the above, how does one learn the weights of neurons’ connections W? For that the total loss function of SNN’s activation on certain input data, LW (input), should be differentiable w.r.t. parameters of the network, W. The problem is that Heaviside’s theta-function in Eq. 2 is not differ- entiable at zero (its derivative being Dirac’s delta-function), and having zero derivative elsewhere. A traditional [31] way to overcome this is to smooth Θ, replacing S[t] with its surrogate, ˜S[t] – it could be any sigmoidal function – typically sigmoid function or arctangent, so at the backward pass (error back-propagation) the derivative is (e.g. for arctan) ∂ ˜S ∂U ← 1 π(1 + (Uπ)2). (4) The choice of surrogate function is another hyperparameter to be set when defining an artificial SNN to work with. 3 Methods 3.1 Tensor Train (TT) decomposition and TT-based optimization techniques As described above, our goal is to optimize (maximize) a certain function – activation (spiking frequency) of certain neurons in an SNN (trained on CIFAR-10 dataset to classify images of 32x32 pixels) over its domain – the ∼ 102−3-dimensional latent space of image features. If one picks a certain region of this domain and discretizes it and the objective function (in our experiments, the latent space of image-generating models was discretized into a 128-dimensional cube with 64 points on each side), this high-dimensional tensor might be of quite low rank – since both ANNs (by design) and biological NNs (due to restrictions of the physical world) don’t have exponential resources to encode all the places of interest in this space. Under such assumptions, optimization methods that are based on low-rank tensor decompositions might prove very effective. One kind of such decompositions that turned into a whole fruitful research field of its own is the Tensor Train (TT) decomposition, introduced in 2011 by Oseledets [15]. It allows to encode a low-rank high-dimensional tensor in a compact and convenient format, only using a polynomial (in dimension of the tensor) number of variables, instead of exponential required in general (the format resembles, if depicted graphically as what’s called a tensor network, linked train cars, hence the name) – in this format, tensors can be effectively operated with: added, multiplied, convoluted, etc. All that provides a basis for applying TT-decomposition to solve various linear and nonlinear equations, PDEs, etc. Of many possible applications of TT-decomposition, other than to optimization problems (described further), one could especially note “tensorizing NNs” [36, 37] – effectively building a compact tensor approximation to a nonlinear NN – apart from speeding up inference tasks, this seems very related to the subject of present work – the whole landscape of activation values of a certain neuron in an ANN could be compressed and investigated in detail. If the conjecture of low rank holds for activation functions of living neurons, this would mean that only a polynomial (in latent space dimension) number of “requests” (expositions to stimuli) is needed to effectively approximate the response of the living NN – this is a promising direction of future research. Thus, there are several new optimization algorithms based on the TT-decomposition, where the (discretized) optimized function f(x) is better and better approximated ˜f(x) with a Tensor Train at each iteration of computing its values at different points, then the optimum candidate (optimum of the approximation) is quickly found exploiting the structure of TT well suited for this problem – this is the basic idea behind TTOpt optimization method [14]. Another related method is Optima-TT [38]. Also see [39, 40] for an overview of tensor approximation methods for optimization. 6 Draft Figure 3: Schematic of the PROTES method. For the purpose of present work we’ve tried TTOpt, but at some point switched to a more recent TT-based optimization method, PROTES [41], that turned to be more effective in our specific problem. PROTES stands for “Probabilistic Optimization with Tensor Sampling” – it is a probabilistic optimiza- tion method, the main idea of which is similar to that of Simulated Annealing methods (see [42]) of the Monte-Carlo family – while gradient-based methods may be stuck in local optima, such methods have some probability of “jumping” out of these, if the temperature (yet another hyperparameter) of the “optimum-seeking particles” is high enough. The intuition behind PROTES is as follows: given a target function f(x) to minimize (maximization can be done by minimizing −f(x)), apply the following monotonic (Fermi-Dirac) transformation to it: F[f](x) = 1 exp \u0000(f(x) − ymin − E)/T \u0001 + 1, (5) with ymin being an exact or approximate minumum of f, T > 0 - the “temperature” parameter, and E – some “energy” threshold. With F being a CDF-like function, one can (stably) find the maximum of its expectation: maxθ EξθF[f](ξθ) where a family of random variables ξθ has a parametric distribution with density pθ(x) (this distribution is effectively expressed in TT-format). Using the REINFORCE trick [43], one can estimate the expectation gradient as ∇θEξθF[f](ξθ) ≈ 1 M M X i=1 F[f](xi)∇θ log pθ(xi) (6) with Monte-Carlo – {xi}M 1 being i.i.d. realizations of the r.v. ξθ. If one manages to find optimal parameter values ˜θ of pθ, then p˜θ is expected to have a peak at the maximum of F[f]. At “low temper- ature” (small values of T), only a few terms contribute to the sum (6) – namely, with those xi for which f(xi) − ymin < E (“low energy particles”) – for those, F[f] ≈ 1, while for others its ≈ 0. Thus one keeps only a few best values of the sample. With pθ having a low-rank TT-representation, the above procedure (sampling and finding top-n values of the array) can be done quickly and effectively. 3.2 Generative models The task of a generative model in the pipeline of our experiment is to effectively “map” the space of stimuli, providing lower-dimensional latent coordinates for the subspace of “natural” images in the huge space of all possible images of given size. The models were trained on CIFAR-10 [44] – a dataset of 60.000 color images of size 32x32 pixels, split into 10 classes (6.000 images per class): 0) airplane, 1) automobile, 2) bird, 3) cat, 4) deer, 5) dog, 6) frog, 7) horse, 8) ship, 9) truck (see Fig. 4). 7 Draft Figure 4: 10 classes of images in CIFAR10 dataset [44] We’ve tried two types of generative models that are known to perform well on this task: 1. Variational AutoEncoders (VAEs) [45], specifically their “discretized” (vector-quantized) version – VQ-VAEs [46] 2. Generative Adversarial Networks (GANs) [13], specifically their spectral-normalized version (SN- GAN) [47] 3.2.1 Vector-Quantized Variational AutoEncoders (VQ-VAEs) A Vector-Quantized Variational AutoEncoder (VQ-VAEs) [46] is a generative model of the VAE family [45]. VAEs were chosen for our task mainly due being quick to generate data – only one forward pass is needed to obtain a sample; but also due to simplicity of architecture – encoder-decoder, where the endoder gives the latent coordinates of a sample. “Vanilla” VAEs take a sample of data, x (e.g. a batch of images) and transform it, layer by layer of the encoder (if data is images, some convolutional layers are applied first), to a point in the latent space z. On that space livings a family of parameterized probability distributions (posteriors) pθ(z|x) (one can say that the encoder just outputs a vector of parameters θ of this distribution, thus specifying a representative of this family – doing what’s called parametric inference in statistics). One then generates a sample z ∼ pθ(z|x) from that distribution – this is a non-deterministic action and so at first it seems non-differentiable for one to design a backward pass (error backpropagation), but a clever idea of the reparameterization trick [45] overcomes this problem (by separating parameterization of a distribution from sampling from it). This “latent code” z that somewhat describes the input sample x is then passed to the decoder (if the VAE has to generate images – last layers of the decoder will be deconvolutional, etc.) to produce an output – a new sample of data. There is also what’s called a prior distribution p(z) of latent codes, which, for the case of latent space being Rd, is uninformatively chosen to be standard normal (uniform distribution can’t be supported on Rd). There exists a problem of “posterior collapse” – when the posterior distribution gets too close to the uninformative prior on some latent codes. Various modifications of VAEs aim to fix this problem, with VQ-VAE doing so quite successfully under certain conditions. Vector-Quantised VAEs [46] take VAEs this idea to a “discrete” setting – the family of posterior distributions pθ(z|x) is now discrete rather than continuous (multidimensional normal in vanilla VAEs), supported on a fixed-size dictionary of latent codes (codewords). The motivation behind this is that, for any finite sample of natural images, there will only be a discrete set of classes in it – dogs, cats, cars, etc. So VQ-VAEs tend to produce far sharper images than their vanilla VAE counterparts (which tend to blur the image, with the uncertainty of the posterior distribution translating into gaussian-like, although nonlinearly transformed, blur on the resulting images), which at first seemed better for our needs. The encoder of VQ-VAE thus learns to output these discrete posteriors (the latent codes and their posterior probabilities) so that samples from them (output by the decoder) were alike the training data. Since 8 Draft the prior p(z) is also supported on this discrete codebase (which can support a uniform distribution), choosing it to be uniform helps prevent posterior collapse. In our experiments, we’ve tested VQ-VAEs with latent dim = 64 with dictionary size of 512 – while CIFAR-10 only has 10 classes, codes are assigned not to singular images, but to their batches (samples). Both encoder and decoder were built with ResidualStacks [33] of 3-4 convolutional layers. 3.2.2 Generative Adversarial Networks (GANs) Another classic family of generative models, that we successfully applied to our case is Generative Adversarial Networks (GANs) [13]. A GAN is comprised of two NNs, a generator G and a discriminator D, playing a minimax game: G has to learn a distribution close to that of natural examples (images), fooling D that they are real, not fake; D has to discriminate fake images produced by G from real ones – by learning a probability distribution of, say, real ones (supported on the same space of latent variables of images’ features). For the purposes of working with images, both NNs of course should have some pre-processing (convolution) layers, etc. Due to the nature of this minimax game G and D are playing, vanilla GANs suffered from instability issues on datasets as high-dimensional as CIFAR-10 (as noted, 32x32 color images of 10 classes), so many regularization techniques were proposed. Quite popular one being the Spectral Normalization (SN-GAN) [47] – with the weight matrices of NN layers being penalized so that their Lipschits constant (a measure of continuity of a function) was bounded from above by 1. This simple yet elegant solution allowed the authors of SN-GAN reach then-state-of-the-art results on CIFAR-10 in terms of so-called inception score [48] and Fr´echet inception distance (FID) [49]. We’ve been using an SN-GAN with the latent space of 128 latent dimensions discretized into 64 points each. 3.3 Spiking neural network architectures Before moving on to experiments on maximizing the neural responses in vivo, it is necessary to fully prepare and test the pipeline of such an experiment, where living neurons are replaced by some adequate in silico model. The most studied class of such models are artificial spiking neural networks (SNN). Among all the software implementations of SNN, two most well-known and developed one were chosen - the SNNTorch library [31] and SpikingJelly library [32]. The specificity of spiking neural networks, as well as of biological ones, lies in the discreteness of spike events, and therefore their non-differentiability. This creates a problem for the backpropagation algorithm, which cannot assign coefficients to update the weights on a backward pass. In this work, we use the solution introduced in [31], based on the so-called surrogate gradient method, see Fig. 2. First, a simple convolutional network based on SNNtorch was implemented, trained to solve the prob- lem of image classification from the CIFAR-10 dataset (images 32x32 pixels, 10 classes). The architecture of the model consisted of “regular” (non-spike) convolutional layers, as well as fully-connected layers. The spiking nature of the model was achieved by projecting the input of ordinary layers to the Leaky Integrate-and-Fire neurons with a membrane decay constant of 0.9. The trained spiking network achieved an accuracy of 72% on the CIFAR-10 dataset. This accuracy is lower than that of state-of-the-art non-spike architectures, but it was quite sufficient for the question being studied about the formation of cognitive specializations in neural networks. To speed up inference and make the network deeper, we integrated the SpikingJelly SNN library into our framework, which supports a fast CuPy backend. We used a spiking analogue of a celebrated ResNet18 model [33] for the main results, since this architecture provided an optimal tradeoff between depth and inference speed. Again, we used LIF neurons with membrane decay constant equal to 0.9. The trained network achieved 86% accuracy on CIFAR10 dataset. Single neuron activity was determined by the number of spikes that the neuron produced over a fixed period of time (100 counts for an SNN-based network and 20 for SpikingJelly-based one). Activity was averaged over all feature channels and normalized to the range [0,1]. 3.4 Optimization algorithms One of the most popular and effective software packages for gradient-free optimizaton is Nevergrad frame- work [50] — methods implemented there were used as benchmarks. Notably, Nevergrad also has several methods that are combining several optimization approaches into one – -these are called “Portfolios”. 9 Draft Based on our testing, the basic Portfolio method – combining methods of 1) Covariance matrix adap- tation evolution strategy (CMA-ES), 2) Differential evolution (DE) and 3) scr-Hammersley – performed best of all in our problem setting. We compared it to other popular Nevergrad methods of 1) OnePlu- sOne, 2) NoisyBandit and 3) Simultaneous perturbation stochastic approximation (SPSA) — all of them have shown poorer results to the abovementioned Portfolio. As an alternative, we’ve used PROTES optimization method [41] (described in prior sections), based on low-rank tensor decomposition, Tensor Train (TT) [15]. We’ve used additional quantization (bina- rization) of tensor modes (each tensor dimension is transformed into a new set of dimensions, having smaller mode), which allowed us to reach better results than baseline PROTES. So the optimization methods compared in the Results section are: 1. (baseline) Portfolio from the Nevergrad package 2. (baseline) PROTES with (k = 10; k-top = 1) 3. TT-s = PROTES with (k = 5; k-top = 1) + quantization 4. TT-b = PROTES (k = 25; k-top = 5) + quantization 3.5 MANGO framework We created a framework for fast and accurate computation of MEIs in artificial networks - MANGO (Maximization of neural Activation via Non-Gradient Optimization). It was of particular interest to us to consider models that are closer to the mechanisms of functioning of biological neurons, in particular spiking neural networks (SNN), since one of our main goals in the future is to apply the developed methods to living systems. Despite this, the gradient-free optimization methods we propose in this work are well applicable to classical neural networks. Within the framework, it is possible to select a dataset, generator model, target neural network model and optimization method; carrying out numerical calculations with various hardware backends; saving and analysing the results. Here is a list of options that have been added to the framework: 1. Datasets: MNIST, Fashion-MNIST, CIFAR10, Imagenet [44, 51–53] 2. Generative models: VAE-VQ [46], GAN-SN [47] 3. Classic neural networks: AlexNet [54], Densenet [55], VGG [56] 4. Spiking neural networks: SNNTorch-supported model CNN and SpikingJelly-supported spiking ResNet18 [31–33] 5. Optimization methods: Nevergrad-based benchmarks and Tensor Train decomposition-based meth- ods (TTOpt, PROTES, etc.) [14, 41, 50] 6. backends: CPU (with multithreading), GPU, CuPy for SNN-based models The selected dataset is used to train a convolutional (probably spiking) network for the image clas- sification task, as well as to train a generator network for the task of compressing input data into an effective latent representation. After training, the generator creates random latent representations of z, which are transformed into images and presented to the convolutional network. The activation of the neuron of interest is measured and fed as input to the optimizer, which produces an improved latent vector z that maximizes the likely response of the neuron studied. This process is repeated multiple times until convergence or until the optimizer query budget is exhausted. MANGO code is available on GitHub https://github.com/iabs-neuro/mango, including training and analysis scripts. Full sets of MEIs will be provided to interested readers upon a reasonable request. 4 Results 4.1 Tensor Train-based optimization methods outperform non-gradient bench- marks Among the methods used, approaches based on Tensor Train decomposition showed performance equal to or 10-20% better than the benchmarks (mainly the Portfolio method, as the most effective of the 10 Draft non-gradient family under consideration). Performance was measured as MEI-related target neuron activation. Figure 5: Activation of one selected neuron (unit 0, layer ”sn1”, spiking ResNet18) depending on the number of requests to the optimizer. Left: full optimization history, right: inset for optimization budget from 1000 to 12000 The PROTES method in various modifications turned out to be 2-4 times faster than the Nevergrad benchmark, depending on the parameters and properties of the image. At the same time, tensor methods required fewer steps to achieve a high level of activation of the target neuron (see Fig. 5). Three different (albeit related) tensor methods and a non-gradient benchmark from the Nevergrad library converged on the same images very often. This is quite surprising, given the huge potential number of image variants sampled from the latent space of the generator. Despite the fact that in deep layers the difference between MEIs became larger (see section 4.4), in the early layers of the network they are very close to each other(see Figs 6, 7). This suggests that all discrete optimization algorithms used indeed converge to a good optimum in the latent space of the generator. Taking into account the results from Section 4.2, we can extend these results to the general space of all images and conclude that discrete optimization methods, especially those based on Tensor Train decomposition, perform well in finding MEIs. 4.2 Comparison of GAN- and VAE- based generators We created MEIs for each neuron of all spiking layers from spiking ResNet18. The analysis showed that the maximal activations of neurons on images from VAE-VQ were smaller than on images from GAN-SN. The magnitude of the gap depended on the layer and ranged from 50% in early layers to 5% in deep layers (Fig. 11). In general, despite neuronal activations were significantly above chance, the MEIs generated by VAE often contain no visible structure and do not provide any insight about neuronal specializations. The exception is the early layers, in which the MEIs obtained using both generators are similar to each other. This allows us to expect that discrete optimization methods actually find generator-agnostic maxima in the global image space, and difficulties in obtaining interpretable images can be solved by further improving the structure of the latent space of generators (see Discussion). 11 Draft Figure 6: MEIs for unit 16 of the first spiking layer from spiking ResNet18. Top row: images generated with GAN-SN, bottom row: images generated with VQ- VAE. Columns correspond to TT-based optimization methods. Thus, VAE-generated images are closest to richer GAN-generated ones where there are uniform colors or simple geometric patterns in the image (see Figs 6, 7). We believe that this is a sign of too much compression of information in the latent space of VAE-VQ generator, which leads to the loss of significant image details. Figure 7: MEIs for unit 35 of the first spiking layer from spiking ResNet18. Top row: images generated with GAN-SN, bottom row: images generated with VQ- VAE. Columns correspond to TT optimization methods. The possible reason for VQ-VAE performing poorer in our setup is the following: as described in the above sections, VQ-VAEs learn discretized, rather than continuous posterior distributions of data samples – supported on a finite set (dictionary) of (learned) vectors in the latent space, the distribution is then akin to multinomial on that discrete pointset. For that reason, VQ-VAEs seem to be not so good at interpolating between probable datapoints (images) – designed to mitigate posterior collapse and gaussian-like blur of generated images in vanilla VAEs, it seems they traded in the ability to generate interesting out-of-distribution samples. Of course, some interpolation is possible, but it seems we didn’t 12 Draft find good balance between discretization parameters of VQ-VAE and TT – that is matter of future investigation. SN-GANs performed far better in our experiment, due to providing nice continuous (benefits of Spectral Normalization) “coordinates” on the latent space of images. However, the above results on neuron specialization distributions seem to be universal – no matter what generative model was used. 4.3 Highly selective neurons are present in early layers of the network An important issue when analyzing MEIs is the relationship between the identified neuronal specializa- tions and the patterns that the network studied has learned in the dataset. We analyzed the activations of Resnet18 spiking layers for correspondence to objects in the images from the training set. Surprisingly, it turned out that highly specialized neurons can appear in the network as early as in the third spiking layer(see Fig. 8) Figure 8: “Pink horse” neuron 52 from LIF layer 1.1 of spiking ResNet18. Images were generated using tensor-train based methods. Numbers show activations of the target neuron on MEI In general, as expected, neurons in early layers are maximally activated by simple geometric features or color patterns, while neurons in later layers show selective activation for one class or another. To quantify neuronal selectivity, we performed network inference on MEIs, obtaining the probabili- ties for each class. Our hypothesis was that more uniform probability distributions correspond to more abstract and general images, whereas high probability for one of the classes indicates presence of a corre- sponding pattern in a MEI. For each MEI, we calculated the entropy of the final probability distribution and normalized it to the maximal possible entropy obtained from a completely uniform distribution. The procedure was repeated 100 times to account for single-trial spike variability inevitably present in SNN. The results are shown in Fig. 9. Note a sharp peak of low-entropy MEIs for the last network spiking layer and the presence of the same smaller peak for the first spiking layer. Already in the middle of the network (layer 2.1), most neurons are specifically associated with some class. 13 Draft Figure 9: Layerwise normalized entropy distributions of probalilities from spiking ResNet18 generated on corresponding neuron MEIs from a given layer. Images were generated with GAN-SN generator and averaged over 100 iterations. 4.4 MEIs are more diverse in deep layers MEIs generated using different discrete optimization methods contain the same patterns of varying degrees of complexity, but are not identical to each other. In order to evaluate the variability of the resulting images, we calculated the average Euclidean distance between the latent vectors corresponding to the MEIs (GAN-SN generator was used). The variability of possible images in the last layers of spiking ResNet18 turned out to be greater than in the first ones (see Fig. 10). Average distances between the generated MEIs in the last spiking layer turned out to be about 20% higher than in the first one, revealing significant transformations of the optimal image landscape. This effect can be explained by the large number of possible ways in which a complex concept, such as a horse, can be implemented in an image. It is known that neural networks learn to nonlinearly project the data space during training, simultaneously identifying some of its areas with each other. Thus, the number of local maxima in the overall image space grows with increasing specialization of neurons. 14 Draft Figure 10: Distribution of distances between MEIs for a given neuron in latent space of GAN-SN. Blue: first spiking layer, red: last spiking layer The growing diversity of maximally activating images is accompanied by an increase in the activations they produce on target neurons( 11).This may be explained by the finer specialization of neurons in the deep layers, in accordance with the results from section 4.3. At the same time, the dips in maximum activations observed in layers 3.1 and 4.1 are reproduced by both generators and show that the overall picture may be more complex. Perhaps the optimization budget used was not enough to get to the true MEIs of the neurons of these layers. Figure 11: Layerwise activation distributions of LIF neuron spiking layers from Spiking ResNet18. For better visibility, every second layer is present. Activation of each neuron was recorded after network inference on the corresponding MEI. Left: activations for VAE-VQ generated images, right: for GAN-SN generated images. 15 Draft 5 Discussion 5.1 Limitations of the current work and directions of future research As our results show, the quality of the calculated MEIs depends significantly on the image generator used. In the future, we plan to add support to MANGO for new generators that are well suited for the MEI search task (i.e., combining the discrete structure of the latent space with the high plausibility of the generated images). It is of interest to consider the popular direction of discrete GANs [57]. An important research question remains the patterns and mechanisms of the neuronal specializations formation over time. Although in this work we examined MEIs for an already trained network, it is of great interest to be able to monitor their evolution as training progresses. We plan to add this feature to our MANGO framework in the future. One of the most interesting and potentially fruitful applications of our tools is maximizing neuronal activation in vivo. Since biological neurons have an irreducible minimum response time to a stimulus, research into algorithms aimed at reducing the optimizer budget required for MEI generation seems very relevant. In particular, we plan to introduce into our framework the latest discrete optimization methods based on Tensor Train decomposition, specifically optimized for this task. 5.2 Activation maximization in ANNs & explainable AI The problem of maximization of neuronal activation (response) in ANNs is being actively studied within the field of ANN visualisation, a subfield of a more general direction – development of inter- pretable/explainable AI models. To succesfully integrate ANN-based solutions in critical systems – such as medical or law practices – one needs a human-understandable interpretation of the algorithm’s decision process. In recent years, analysis methods based on various techniques of visualising the computation graph of an ANN, its loss function profile, parameter (space) of its certain layers or even certain neurons – have gained popularity for creating interpretable deep learning models. Considered methods allow one to delve deeper and better understand the inner workinds of neural algorithms. A recent review [19] of such visualisation techniques contains: the method of activation maximization [58], the Grad-CAM method [59], as well as the Integrated Gradients method [60]. Another outstanding recent work worth noting is [61], containing a wide demonstration of stimuli that maximize the activation of various neurons of an ANN, where the effect of multimodality was demonstrated for artificial neurons. However, all of the aforementioned activation maximization methods are based on automatic differ- entiation: the computation graph of the considered ANN is formed, which allows to compute gradients with minimal complexity (computational complexity of a NN gradient is practically the same as that of its forward-pass), which allows to use gradient descent (or ascent, in case of maximization) and its stochastic modifications. For living systems – gradient based methods are clearly inapplicable, so one should either use gradient-free methods (e.g. genetic algorithms), or other methods using a local ap- proximation of the gradient (e.g. evolutionary algorithms), or other approaches, including optimization with TT-decomposition-based approximation of the optimized function. Notably, it was genetic and evolutionary algorithms that were considered in [12] for activation maximization in an ANN. 5.3 Possible applications for maximization of activation in biological neurons Modern optical imaging techniques allow one to register the activity of hundreds of neurons unfold in their temporal dynamics in vivo. On one hand, activation (response) maximization of multiple neurons (both separately and group-wise) could become a powerful tool for analyzing the functions of the nervous system at a cell level, shed light on the formation and spread of cognitive specializations of living neurons. On the other hand, the framing of biological experiment poses serious constraints on applicability of various optimization methods – these should be sufficiently quick, accurate and capable of tuning (post-training) on new-coming data. We expect that TT-decomposition-based methods developed in course of present work would possess all such properties. A separate problem is the search for optimal stimuli in non-visual spaces (e.g. in the space of sounds or smells, which could be far more relevant for the living object, rather than for a human). We also hope to obtain some estimates of applicability of TT-decomposition-based methods in case of such latent spaces of stimuli. Studies of neural specializations in spiking neural networks are of great interest because, using a biologically plausible model, they reveal the mechanisms of operation of the functional units of the neural network. Thus, the framework we created can be used as a bridge between neurobiological and 16 Draft in silico experiments, allowing one to explore the general principles of information encoding in deep neural networks. We also hope that this work will significantly bring activation maximization algorithms closer to the requirements of a real biological experiment and thereby open a new tool for analyzing the cognitive functions of individual neurons. 6 Acknowledgements This work was supported by Non-Commercial Foundation for Support of Science and Education ”IN- TELLECT” and by Lomonosov Moscow State University. N. Pospelov acknowledges support from the Brain Program of the IDEAS Research Center. This research was supported in part through computa- tional resources of HPC facilities at HSE University [62] – for training the models & running the pipeline part of the times. The authors are grateful to all members of Laboratory of Neuronal Intelligence and especially to Ksenia A.Toropova for numerous fruitful discussions. 7 References References [1] D. H. Hubel and T. N. Wiesel, Brain and visual perception: the story of a 25-year collaboration. Oxford University Press, 2004. [2] J. Vierock, S. Rodriguez-Rozada, A. Dieter, F. Pieper, R. Sims, F. Tenedini, A. C. Bergs, I. Ben- difallah, F. Zhou, N. Zeitzschel, et al., “Bipoles is an optogenetic tool developed for bidirectional dual-color control of neurons,” Nature communications, vol. 12, no. 1, p. 4527, 2021. [3] C. R. Ponce, W. Xiao, P. F. Schade, T. S. Hartmann, G. Kreiman, and M. S. Livingstone, “Evolving images for visual neurons using a deep generative network reveals coding principles and neuronal preferences,” Cell, vol. 177, no. 4, pp. 999–1009, 2019. [4] A. Bardon, W. Xiao, C. R. Ponce, M. S. Livingstone, and G. Kreiman, “Face neurons encode nonse- mantic features,” Proceedings of the national academy of sciences, vol. 119, no. 16, p. e2118705119, 2022. [5] A. J. Shriver and T. M. John, “Neuroethics and animals: report and recommendations from the university of pennsylvania animal research neuroethics workshop,” ILAR journal, vol. 60, no. 3, pp. 424–433, 2019. [6] P. Singer and Y. F. Tse, “Ai ethics: the case for including animals,” AI and Ethics, vol. 3, no. 2, pp. 539–551, 2023. [7] K. Dobs, J. Martinez, A. J. Kell, and N. Kanwisher, “Brain-like functional specialization emerges spontaneously in deep neural networks,” Science advances, vol. 8, no. 11, p. eabl8913, 2022. [8] A. M. Zador, “A critique of pure learning and what artificial neural networks can learn from animal brains,” Nature communications, vol. 10, no. 1, p. 3770, 2019. [9] R. Schaeffer, M. Khona, and I. Fiete, “No free lunch from deep learning in neuroscience: A case study through models of the entorhinal-hippocampal circuit,” Advances in Neural Information Processing Systems, vol. 35, pp. 16052–16067, 2022. [10] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah, “Multimodal neurons in artificial neural networks,” Distill, vol. 6, no. 3, p. e30, 2021. [11] C. Olah, A. Mordvintsev, and L. Schubert, “Feature visualization,” Distill, 2017. https://distill.pub/2017/feature-visualization. [12] B. Wang and C. R. Ponce, “High-performance evolutionary algorithms for online neuron control,” in Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1308–1316, 2022. [13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27, 2014. 17 Draft [14] K. Sozykin, A. Chertkov, R. Schutski, A.-H. Phan, A. S. CICHOCKI, and I. Oseledets, “Ttopt: A maximum volume quantized tensor train-based optimization and its application to reinforcement learning,” Advances in Neural Information Processing Systems, vol. 35, pp. 26052–26065, 2022. [15] I. V. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientific Computing, vol. 33, no. 5, pp. 2295–2317, 2011. [16] K. Yamazaki, V.-K. Vo-Ho, D. Bulsara, and N. Le, “Spiking neural networks and their applications: A review,” Brain Sciences, vol. 12, no. 7, p. 863, 2022. [17] Y. Li, Y. Kim, H. Park, and P. Panda, “Uncovering the representation of spiking neural networks trained with surrogate gradient,” arXiv preprint arXiv:2304.13098, 2023. [18] E. Y. Walker, F. H. Sinz, E. Cobos, T. Muhammad, E. Froudarakis, P. G. Fahey, A. S. Ecker, J. Reimer, X. Pitkow, and A. S. Tolias, “Inception loops discover what excites neurons most using deep predictive models,” Nature neuroscience, vol. 22, no. 12, pp. 2060–2065, 2019. [19] S. A. Matveev, I. V. Oseledets, E. S. Ponomarev, and A. V. Chertkov, “Overview of visualization methods for artificial neural networks,” Computational Mathematics and Mathematical Physics, vol. 61, no. 5, pp. 887–899, 2021. [20] W. Xiao and G. Kreiman, “Xdream: Finding preferred stimuli for visual neurons using generative networks and gradient-free optimization,” PLoS computational biology, vol. 16, no. 6, p. e1007973, 2020. [21] W. Xiao and G. Kreiman, “Gradient-free activation maximization for identifying effective stimuli,” arXiv preprint arXiv:1905.00378, 2019. [22] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553, pp. 436–444, 2015. [23] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016. [24] E. M. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions on neural networks, vol. 14, no. 6, pp. 1569–1572, 2003. [25] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous activity,” The bulletin of mathematical biophysics, vol. 5, pp. 115–133, 1943. [26] F. Rosenblatt, The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aero- nautical Laboratory, 1957. [27] F. Rosenblatt, “The perceptron: a probabilistic model for information storage and organization in the brain.,” Psychological review, vol. 65, no. 6, p. 386, 1958. [28] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating errors,” nature, vol. 323, no. 6088, pp. 533–536, 1986. [29] W. Maass, “Networks of spiking neurons: the third generation of neural network models,” Neural networks, vol. 10, no. 9, pp. 1659–1671, 1997. [30] N. Anwani and B. Rajendran, “Training multi-layer spiking neural networks using normad based spatio-temporal error backpropagation,” Neurocomputing, vol. 380, pp. 67–77, 2020. [31] J. K. Eshraghian, M. Ward, E. Neftci, X. Wang, G. Lenz, G. Dwivedi, M. Bennamoun, D. S. Jeong, and W. D. Lu, “Training spiking neural networks using lessons from deep learning,” Proceedings of the IEEE, vol. 111, no. 9, pp. 1016–1054, 2023. [32] W. Fang, Y. Chen, J. Ding, Z. Yu, T. Masquelier, D. Chen, L. Huang, H. Zhou, G. Li, and Y. Tian, “Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence,” Science Advances, vol. 9, no. 40, p. eadi1480, 2023. [33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016. 18 Draft [34] A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane current and its application to conduction and excitation in nerve,” The Journal of physiology, vol. 117, no. 4, p. 500, 1952. [35] N. Brunel and M. C. Van Rossum, “Lapicque’s 1907 paper: from frogs to integrate-and-fire,” Bio- logical cybernetics, vol. 97, no. 5-6, pp. 337–339, 2007. [36] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensorizing neural networks,” Advances in neural information processing systems, vol. 28, 2015. [37] D. Liu, L. T. Yang, P. Wang, R. Zhao, and Q. Zhang, “Tt-tsvd: A multi-modal tensor train decom- position with its application in convolutional neural networks for smart healthcare,” ACM Trans- actions on Multimedia Computing, Communications, and Applications (TOMM), vol. 18, no. 1s, pp. 1–17, 2022. [38] A. Chertkov, G. Ryzhakov, G. Novikov, and I. Oseledets, “Optimization of functions given in the tensor train format,” arXiv preprint arXiv:2209.14808, 2022. [39] A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, D. P. Mandic, et al., “Tensor networks for dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions,” Foundations and Trends® in Machine Learning, vol. 9, no. 4-5, pp. 249–429, 2016. [40] A. Cichocki, A.-H. Phan, Q. Zhao, N. Lee, I. Oseledets, M. Sugiyama, D. P. Mandic, et al., “Tensor networks for dimensionality reduction and large-scale optimization: Part 2 applications and future perspectives,” Foundations and Trends® in Machine Learning, vol. 9, no. 6, pp. 431–673, 2017. [41] A. Batsheva, A. Chertkov, G. Ryzhakov, and I. Oseledets, “PROTES: Probabilistic optimization with tensor sampling,” Advances in Neural Information Processing Systems, 2023. [42] M. W. Trosset, “What is simulated annealing?,” Optimization and Engineering, vol. 2, pp. 201–213, 2001. [43] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement learning,” Machine learning, vol. 8, pp. 229–256, 1992. [44] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” 2009. [45] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114, 2013. [46] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” Advances in neural information processing systems, vol. 30, 2017. [47] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral normalization for generative adver- sarial networks,” arXiv preprint arXiv:1802.05957, 2018. [48] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved tech- niques for training gans,” Advances in neural information processing systems, vol. 29, 2016. [49] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time- scale update rule converge to a local nash equilibrium,” Advances in neural information processing systems, vol. 30, 2017. [50] J. Rapin and O. Teytaud, “Nevergrad - A gradient-free optimization platform.” https://GitHub. com/FacebookResearch/Nevergrad, 2018. [51] Y. LeCun, “The mnist database of handwritten digits,” http://yann. lecun. com/exdb/mnist/, 1998. [52] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms,” arXiv preprint arXiv:1708.07747, 2017. [53] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical image database,” in 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255, Ieee, 2009. [54] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional neural networks,” Advances in neural information processing systems, vol. 25, 2012. 19 Draft [55] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional net- works,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700– 4708, 2017. [56] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recogni- tion,” arXiv preprint arXiv:1409.1556, 2014. [57] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution image synthesis,” in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873– 12883, 2021. [58] D. Erhan, Y. Bengio, A. Courville, and P. Vincent, “Visualizing higher-layer features of a deep network,” University of Montreal, vol. 1341, no. 3, p. 1, 2009. [59] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Vi- sual explanations from deep networks via gradient-based localization,” in Proceedings of the IEEE international conference on computer vision, pp. 618–626, 2017. [60] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep networks,” in International conference on machine learning, pp. 3319–3328, PMLR, 2017. [61] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah, “Multimodal neurons in artificial neural networks,” Distill, vol. 6, no. 3, p. e30, 2021. [62] P. Kostenetskiy, R. Chulkevich, and V. Kozyrev, “Hpc resources of the higher school of economics,” in Journal of Physics: Conference Series, vol. 1740, p. 012050, IOP Publishing, 2021. 20 "
}