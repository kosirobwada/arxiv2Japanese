{
    "optim": "Draft\nFast gradient-free activation maximization\nfor neurons in spiking neural networks\nNikita Pospelov1∗\nAndrei Chertkov2,3\nMaxim Beketov4\nIvan Oseledets2,3\nKonstantin Anokhin1\n1Laboratory of Neuronal Intelligence, Institute for Advanced Brain Studies,\nLomonosov Moscow State University\n2Artificial Intelligence Research Institute (AIRI)\n3Skolkovo Institute of Science and Technology\n4HSE University\nMoscow, Russia\nAbstract\nNeural networks (NNs), both living and artificial, work due to being complex systems of neurons,\neach having its own specialization. Revealing these specializations is important for understanding\nNNs’ inner working mechanisms. The only way to do this for a living system, the neural response\nof which to a stimulus is not a known (let alone differentiable) function is to build a feedback loop\nof exposing it to stimuli, the properties of which can be iteratively varied aiming in the direction\nof maximal response. To test such a loop on a living network, one should first learn how to run it\nquickly and efficiently, reaching most effective stimuli (ones that maximize certain neurons’ activa-\ntion) in least possible number of iterations. We present a framework with an effective design of such\na loop, successfully testing it on an artificial spiking neural network (SNN, a model that mimics the\nbehaviour of NNs in living brains). Our optimization method used for activation maximization\n(AM) was based on low-rank tensor decomposition (Tensor Train, TT) of the activation function’s\ndiscretization over its domain – the latent parameter space of stimuli (CIFAR10-size color images,\ngenerated by either VQ-VAE or SN-GAN from their latent description vectors, fed to the SNN).\nTo our knowledge, the present work is the first attempt to perform effective AM for SNNs. The\nsource code of our framework, MANGO (for Maximization of neural Activation via Non-Gradient\nOptimization) is available on GitHub1.\nKeywords: neuron specialization, activation maximization, effective stimuli, neural representation,\nderivative-free optimization, spiking neural networks, Tensor-Train decomposition,\ngenerative models: (Vector-Quantized) Variational AutoEncoders (VQ-VAEs),\n(Spectrally Normalized) Generative Adversarial Networks (SN-GANs)\nContents\n1\nIntroduction and problem statement\n2\n2\nBackground\n3\n2.1\nActivation Maximization in artificial and living neural networks . . . . . . . . . . . . . . .\n3\n2.2\nSpiking Neural Networks (SNNs) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4\n3\nMethods\n6\n3.1\nTensor Train (TT) decomposition and TT-based optimization techniques\n. . . . . . . . .\n6\n3.2\nGenerative models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n3.2.1\nVector-Quantized Variational AutoEncoders (VQ-VAEs) . . . . . . . . . . . . . . .\n8\n3.2.2\nGenerative Adversarial Networks (GANs) . . . . . . . . . . . . . . . . . . . . . . .\n9\n∗Corresponding author, nik-pos@yandex.ru\n1https://github.com/iabs-neuro/mango\n1\narXiv:2401.10748v1  [cs.NE]  28 Dec 2023\nDraft\n3.3\nSpiking neural network architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.4\nOptimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n3.5\nMANGO framework . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n10\n4\nResults\n10\n4.1\nTensor Train-based optimization methods outperform non-gradient benchmarks . . . . . .\n10\n4.2\nComparison of GAN- and VAE- based generators . . . . . . . . . . . . . . . . . . . . . . .\n11\n4.3\nHighly selective neurons are present in early layers of the network . . . . . . . . . . . . . .\n13\n4.4\nMEIs are more diverse in deep layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n14\n5\nDiscussion\n16\n5.1\nLimitations of the current work and directions of future research\n. . . . . . . . . . . . . .\n16\n5.2\nActivation maximization in ANNs & explainable AI\n. . . . . . . . . . . . . . . . . . . . .\n16\n5.3\nPossible applications for maximization of activation in biological neurons . . . . . . . . . .\n16\n6\nAcknowledgements\n17\n7\nReferences\n17\n1\nIntroduction and problem statement\nBrain neurons specialize on certain properties of stimuli – e.g. low-level neurons of the vision cortex\nspecialise on simple properties of the visible image: the direction of movement [1] or color [2], while\nhigher-level neurons specialize on more complex ones, e.g. presence of a face within the field of view.\nIdentifying the specialization of living neurons can be done by varying the properties of the stimulus\n(e.g. image shown to the the object) in an iterative manner to find those which promote most intensive\nresponse [3, 4] – these perspective approaches were termed activation maximization (AM). However,\nto solve such a problem effectively in vivo one needs to\n1. find an optimal stimulus as quickly (in as few iterations) and accurately as possible\n2. investigate this stimulus space – for possibly multiple activation optima, local vs global, etc.\nThis is especially important in light of ethical considerations [5, 6] of such studies in neuroscience –\nthe faster & the better one gets such results – the less stressful it is for the living object of study, and the\nless objects are needed (needless to say, if the results are reproducible over a population). This challenge\nrequires developing and incorporating new mathematical optimization methods into such experimental\nsetups, allowing to obtain best approximations to the global activation optimum at a minimal number\nof iterations (of exposing the living object to stimuli – e.g. images or sounds).\nModern artificial neural networks (ANNs), however variable they are in architecture types and ap-\nproaches to training them, exhibit very similar selectivity to complex stimuli [7] (though whether their\nlearning mechanisms are similar to those in living NNs, and thus whether one can directly compare ones\nto the others – is a matter of ongoing debates [8, 9]). Interestingly, just like in living brains, ANN neurons\ncan have multiple specializations each [10]. Since (most) ANNs are essentially computational graphs\nof functions parameterized by layers’ weights, with modern computational hardware (GPUs and TPUs)\nbeing quite capable of effectively computing (in parallel) both their value at a point (for inference) and,\nmore importantly, their derivatives w.r.t. parameters (to backpropagate the error for training), one can\nrelatively easily “dissect” ANNs and anatomize the “responsibility areas” of each of the artificial neurons\n[11]. This makes ANNs perfect objects for exercising such activation maximization techniques. Thus,\nin [12], 15 various methods based on genetic and evolutionary optimization algorithms were compared\nin maximizing the response of a certain neuron in a deep ANN by generating new artificial images with\na pre-trained generative adversarial network (GAN) [13]. A wider comparative analysis of various op-\ntimization methods was done in [14], including genetic and evolutionary algorithms, where an original\napproach based on Tensor Train decomposition (TT-decomposition, [15]) was shown to have significant\nadvantages – in speed, accuracy and stability (at the same iteration count).\nA more biologically realistic model of living NNs is the family of (artificial) spiking neural networks\n(SNNs) [16], the activity of which unfolds in time – in contrast to ordinary “feedforward” NNs, which are,\nmathematically, just functions, without any explicit temporal dynamics. With that biological relevance\nin mind, in present work we focus on activation maximization of SNN’s neurons. Quite recently, inner\n2\nDraft\nneural representations of data in SNNs were studied [17] and compared to that in ordinary ANNs. In our\nwork we attempt a related but different problem – of finding effective stimuli (MEIs for Most Exciting\nImages, termed in [18] – maximizing certain neurons’ activations) specifically for SNNs effectively (in\nleast number of iterations and as quickly as possible, with a view towards future applications to living\nobjects) – to our knowledge, for the first time.\nThat said, futher development of neuronal activations approximation and global optimization meth-\nods based on TT-decomposition, applied to ANNs in general, seems very perspective. Modified appro-\npriately, such an approach could then be transferred to studies of living objects (for a quick and effective\nmaximization of biological neurons’ response). Or it could be used within the developing field of inter-\npretable AI and ANN visualisation ([11], also see [19] for an overview) – helping to establish connections\nbetween the workings of natural and artificial intelligence.\n2\nBackground\n2.1\nActivation Maximization in artificial and living neural networks\nOur objective in the present work is to optimize the activation of a certain neuron in a (spiking) ANN\nby finding its most exciting input (MEI) stimulus.\nThis problem of activation maximization (AM)\nstands at the crossroads of contemporary neuroscience & deep learning (DL). In DL, well-known works\nof XDream [20], Inception loops [18] – demonstrated how one could use generative models’ ability to\neffectively map the latent space of stimuli (images) for AM on ANNs. In neuroscience, some outstanding\nrecent results show that one can study the function of living neurons by utilizing the above approach\n– incorporating a generative model like XDream into a feedback loop of exposing a living object (a\nmammal with well-developed vision system) to the generated images: in [3], XDream was used to reveal\nthe MEIs of macaque monkeys’ visual cortex; in [4] this approach was taken further to find out that\nthese MEIs not necessarily encode exactly faces, but sometimes abstract objects (many of them looking\nface-like, though)!\nWith the present work we aim to contribute to this rapidly developing field of incorporating DL\ntechniques in neuroscience to answer longstanding questions of the latter – by introducing a pipeline in\nwhich this activation maximization problem can be solved effectively, requiring less iterations to converge\nto a MEI.\nWith the activation intensity/frequency of a certain neuron viewed, mathematically, as a function of\nthe input “fed” to the NN (– be it a living or artificial one, the neuron is part of), MEI is basically\nMEIi\ndef\n=\narg max\nx∈StimulusSpace\nActivationi neuron(x)\n(1)\nthe arg max of some function, A(x), which\n1. can only be measured (computed) at a point x – in several points, but in the less – the better\n2. is measured with noise.\nThis is informally referred to as black box function optimization problem – without explicit access\nto the gradient (derivatives) of the function, it is a whole research field of its own.\nOptimization methods in general can be roughly divided into\n1. Gradient-based methods – dating back to Newton and his method, that have dramatically\nevolved, with variations of Stochastic Gradient Descent (SGD) powering the training of millions of\ninstances of NN-based machine learning models around the world today\n2. Gradiend-free methods – of which there are numerous families: both deterministic and proba-\nbilistic (stochastic), evolutionary (genetic) algorithms, etc.\nGradient-free methods are naturally more appropriate to the problem of AM in real-world systems\ndue to activation being a black-box-like function. Of course, even if not explicitly accessible, the gradient\ncan be approximated by finite differences, but 1) if the function’s landscape is very “wobbly” – quicky\nchanging – the error of gradient estimates will be effectively unbounded; and 2) noise that is present in\nmeasurements of function’s values will also contribute to the error of gradient estimates.\n3\nDraft\nIn the context of AM, quite recently such gradient-free methods have been explored [21], and ex-\ntensively compared to each other [12] – both in silico and in vivo, with Covariance Matrix Adaptation\n(CMA) outperforming most in both settings.\nIn present work, we introduce (for the first time) another family of gradient-free optimization al-\ngorithms in the problem of SNN’s neuron activation maximization – ones based on low-rank tensor\ndecompositions (Tensor Train, [15]) of the optimized function. Some of these methods are essentially\nsearch for a maximum in an array, but a smart one, utilizing the low-rank tensor nature of the the ar-\nray, thus achieving exponential speedup over straightforward search. Others belong to the Monte-Carly\n(MC) family of methods, which are less prone to being stuck in local optima by stochastically “jumping”\naround the array, not necessarily towards the optimum. See the corresponding section of Methods for a\nself-contained introduction of these methods.\n2.2\nSpiking Neural Networks (SNNs)\nOrdinary ANNs are, from a mathematical point of view, just functions – given input data, they trans-\nform it in a parameterized nonlinear way to turn it into output – such ANNs are called feedforward (of\ncourse, the field of contemporary machine learning, and its most productive sub-field of deep learning,\nDL, [22, 23], is rich in further complications of this idea, e.g. NN-based generative models can be said\nto be non-deterministic functions, but for our purposes we limit ourselves to the basics). The activity of\nbiological NNs (in living brains) always unfolds in time: over time, living neurons accumulate signals\nfrom their neighbours they share synaptic connections with, and, if the overall (voltage) amplitude of\nincoming signals (membrane potential, MP) exceeds some threshold, the neuron “fires” – emits what’s\ncalled a spike, a signal the form of which is very special (and well-studied [24]) due to the bio-chemical\ndynamics that governs its emission – that travels along the synapse to neuron’s neighbours – to provoke\nthem to spike in their turn. This inherent temporal nature of biological NNs was overlooked when the\nfirst artificial models of living NNs, namely the 1943 neuron model of McCulloch & Pitts [25], further\ndeveloped and implemented in hardware and termed Perceptron by Rosenblatt [26, 27] in 1957-58, were\nintroduced – perhaps due to hardware limitations of the time, implementing “feedforward” functions lack-\ning any oscillatory internal dynamics was more straightforward. The invention of error back-propagation\nalgorithm [28] in 1986 as a useful method for training such ANNs (e.g. to perform classification tasks\nafter learning on exemplar data) further cemented the dominance of ANNs of feedforward nature as\nmachine models of intelligence. It was Maass’s work [29] in 1997 that introduced networks of artificial\nspiking neurons as a model of biological NNs.\nFigure 1: Schematic of a spiking neuron of an SNN (image from [30])\nAn (artificial) spiking neural network (SNN), just like a biological one, can be thought of as a\ndynamical system, with each neuron being a subsystem with its own internal dynamics. Neurons are\nconnected with each other, forming layers with connections being directional. Over time, a neuron (see\nFig.1) receives signals from its sourcing neighbours – signals being aggregated (summed, basically) with\nweights designating the importance of connections – to give the total value of “membrane potential”. If\nthis value exceeds a certain threshold (one of the hyperparameters of this model), the neuron “fires” –\nemits a spike into its output. For this to be effectively modelled digitally, this dynamics is discretized\n– the time-dependence of signal is now basically encoded by an array of numbers (signal amplitudes),\nthe blue bars seen on Fig.1 are (significantly) non-zero elements of such arrays, all that being a discrete\nmodel of what’s called a spike train (term used for signals in biological NNs as well).\nAs for the\ngoodness of such approximation of (the envelope of) signals in biological NNs, this discretization is not\ntoo far-fetched – spikes in the brain are indeed nearly “binary” – MP changes in almost discrete steps.\n4\nDraft\nNowadays there are several software implementations of artificial SNNs, with at least two outstand-\ning frameworks – snnTorch [31] & SpikingJelly [32], allowing to do complete training and inference\nprocedures – these two were used in present work. There are several important questions to answer when\nworking with such a model.\nFirst, how is input data encoded for the SNN to process it. In our work we “expose” an SNN to\nstatic images. For that there are at least two approaches [31]:\n1. simply repeating the image (unaltered) a certain number of frames (kind of a static video)\n2. showing the image on a certain number of frames, but altering it with some sort of noise.\nWe’ve chosen the first approach for simplicity, the image being repeated from 20 to 100 frames.\nEach frame is first fed to a pre-trained (feedforward) convolutional neural network (CNN)\nwith residual blocks (spiking analogue of ResNet [33]) – for it to extract basic image features for our\nSNN to process.\nSecond, what exact model of a spiking neuron does one use?\nThe above description is very\nschematic, one needs to specify a certain mechanistic model (an electric circuit, to be precise) of the\nliving neuron, so that the dynamics of that model accurately approximated reality. For practical appli-\ncations, Hodgkin-Huxley model [34], while being most biologically (physically) accurate, is complicated\nto implement and work with. So a practical choice is the one of so-called Leaky Integrate-and-Fire\n(LIF) neuron models, that date back to Louis Lapicque’s 1907 work [35].\nFigure 2: Spiking neuron models (image from snnTorch online tutorial [31])\nIn LIF models, a spiking neuron is represented as an RC circuit – instead of directly summing the\nincoming (spike) voltages, such a neuron integrates input signal over time with leakage (due to R for\nResistance present in the circuit). The LIF neuron abstracts away the shape and profile of the output\nspike; it is simply treated as a discrete event. As a result, information is not stored within the spike,\nbut rather the timing (or frequency) of spikes. Discrete-time equations of the spiking dynamics for such\na neuron thus are:\nS[t] = Θ(U[t] − Uthr) =\n(\n1,\nif U[t] > Uthr\n0,\notherwise\n(2)\nwith S[t] being the signal (spike) intensity of a neuron at (a discrete) moment of time t, U[t] being\nthe MP. With Θ(x) being Heaviside’s step function, a neuron discretely fires if its MP U[t] exceeds a\nthreshold value. The RC-circuit nature of a LIF neuron is described by the following temporal dynamics\nequation on U[t]:\nU[t + 1] = βU[t]\n| {z }\ndecay\n+ WX[t + 1]\n|\n{z\n}\ninput\n− S[t]Uthr\n| {z }\nreset\n(3)\n5\nDraft\n– at next moment of time, t + 1, the MP is given by a linear combination (with learnable weights W)\nof inputs X[t] from sourcing neigbhour neurons; but there’s also some decay (leakage) and a reset term.\nWith Uthr set to, say, 1, the only hyperparameter left to set (governing the dynamics) is the decay\nrate, β.\nThird, given the above, how does one learn the weights of neurons’ connections W? For that the\ntotal loss function of SNN’s activation on certain input data, LW (input), should be differentiable w.r.t.\nparameters of the network, W. The problem is that Heaviside’s theta-function in Eq. 2 is not differ-\nentiable at zero (its derivative being Dirac’s delta-function), and having zero derivative elsewhere. A\ntraditional [31] way to overcome this is to smooth Θ, replacing S[t] with its surrogate, ˜S[t] – it could\nbe any sigmoidal function – typically sigmoid function or arctangent, so at the backward pass (error\nback-propagation) the derivative is (e.g. for arctan)\n∂ ˜S\n∂U ←\n1\nπ(1 + (Uπ)2).\n(4)\nThe choice of surrogate function is another hyperparameter to be set when defining an artificial SNN\nto work with.\n3\nMethods\n3.1\nTensor Train (TT) decomposition and TT-based optimization techniques\nAs described above, our goal is to optimize (maximize) a certain function – activation (spiking frequency)\nof certain neurons in an SNN (trained on CIFAR-10 dataset to classify images of 32x32 pixels) over\nits domain – the ∼ 102−3-dimensional latent space of image features.\nIf one picks a certain region\nof this domain and discretizes it and the objective function (in our experiments, the latent space of\nimage-generating models was discretized into a 128-dimensional cube with 64 points on each side), this\nhigh-dimensional tensor might be of quite low rank – since both ANNs (by design) and biological NNs\n(due to restrictions of the physical world) don’t have exponential resources to encode all the places of\ninterest in this space. Under such assumptions, optimization methods that are based on low-rank tensor\ndecompositions might prove very effective.\nOne kind of such decompositions that turned into a whole fruitful research field of its own is the\nTensor Train (TT) decomposition, introduced in 2011 by Oseledets [15]. It allows to encode a low-rank\nhigh-dimensional tensor in a compact and convenient format, only using a polynomial (in dimension\nof the tensor) number of variables, instead of exponential required in general (the format resembles, if\ndepicted graphically as what’s called a tensor network, linked train cars, hence the name) – in this format,\ntensors can be effectively operated with: added, multiplied, convoluted, etc. All that provides a basis for\napplying TT-decomposition to solve various linear and nonlinear equations, PDEs, etc. Of many possible\napplications of TT-decomposition, other than to optimization problems (described further), one could\nespecially note “tensorizing NNs” [36, 37] – effectively building a compact tensor approximation to a\nnonlinear NN – apart from speeding up inference tasks, this seems very related to the subject of present\nwork – the whole landscape of activation values of a certain neuron in an ANN could be compressed and\ninvestigated in detail. If the conjecture of low rank holds for activation functions of living neurons, this\nwould mean that only a polynomial (in latent space dimension) number of “requests” (expositions to\nstimuli) is needed to effectively approximate the response of the living NN – this is a promising direction\nof future research.\nThus, there are several new optimization algorithms based on the TT-decomposition, where the\n(discretized) optimized function f(x) is better and better approximated ˜f(x) with a Tensor Train at\neach iteration of computing its values at different points, then the optimum candidate (optimum of the\napproximation) is quickly found exploiting the structure of TT well suited for this problem – this is the\nbasic idea behind TTOpt optimization method [14]. Another related method is Optima-TT [38]. Also\nsee [39, 40] for an overview of tensor approximation methods for optimization.\n6\nDraft\nFigure 3: Schematic of the PROTES method.\nFor the purpose of present work we’ve tried TTOpt, but at some point switched to a more recent\nTT-based optimization method, PROTES [41], that turned to be more effective in our specific problem.\nPROTES stands for “Probabilistic Optimization with Tensor Sampling” – it is a probabilistic optimiza-\ntion method, the main idea of which is similar to that of Simulated Annealing methods (see [42]) of\nthe Monte-Carlo family – while gradient-based methods may be stuck in local optima, such methods\nhave some probability of “jumping” out of these, if the temperature (yet another hyperparameter) of\nthe “optimum-seeking particles” is high enough. The intuition behind PROTES is as follows: given a\ntarget function f(x) to minimize (maximization can be done by minimizing −f(x)), apply the following\nmonotonic (Fermi-Dirac) transformation to it:\nF[f](x) =\n1\nexp\n\u0000(f(x) − ymin − E)/T\n\u0001\n+ 1,\n(5)\nwith ymin being an exact or approximate minumum of f, T > 0 - the “temperature” parameter, and\nE – some “energy” threshold. With F being a CDF-like function, one can (stably) find the maximum of\nits expectation: maxθ EξθF[f](ξθ) where a family of random variables ξθ has a parametric distribution\nwith density pθ(x) (this distribution is effectively expressed in TT-format). Using the REINFORCE\ntrick [43], one can estimate the expectation gradient as\n∇θEξθF[f](ξθ) ≈ 1\nM\nM\nX\ni=1\nF[f](xi)∇θ log pθ(xi)\n(6)\nwith Monte-Carlo – {xi}M\n1\nbeing i.i.d. realizations of the r.v. ξθ. If one manages to find optimal\nparameter values ˜θ of pθ, then p˜θ is expected to have a peak at the maximum of F[f]. At “low temper-\nature” (small values of T), only a few terms contribute to the sum (6) – namely, with those xi for which\nf(xi) − ymin < E (“low energy particles”) – for those, F[f] ≈ 1, while for others its ≈ 0. Thus one keeps\nonly a few best values of the sample. With pθ having a low-rank TT-representation, the above procedure\n(sampling and finding top-n values of the array) can be done quickly and effectively.\n3.2\nGenerative models\nThe task of a generative model in the pipeline of our experiment is to effectively “map” the space of\nstimuli, providing lower-dimensional latent coordinates for the subspace of “natural” images in the huge\nspace of all possible images of given size. The models were trained on CIFAR-10 [44] – a dataset of\n60.000 color images of size 32x32 pixels, split into 10 classes (6.000 images per class): 0) airplane, 1)\nautomobile, 2) bird, 3) cat, 4) deer, 5) dog, 6) frog, 7) horse, 8) ship, 9) truck (see Fig. 4).\n7\nDraft\nFigure 4: 10 classes of images in CIFAR10 dataset [44]\nWe’ve tried two types of generative models that are known to perform well on this task:\n1. Variational AutoEncoders (VAEs) [45], specifically their “discretized” (vector-quantized) version –\nVQ-VAEs [46]\n2. Generative Adversarial Networks (GANs) [13], specifically their spectral-normalized version (SN-\nGAN) [47]\n3.2.1\nVector-Quantized Variational AutoEncoders (VQ-VAEs)\nA Vector-Quantized Variational AutoEncoder (VQ-VAEs) [46] is a generative model of the VAE family\n[45]. VAEs were chosen for our task mainly due being quick to generate data – only one forward pass\nis needed to obtain a sample; but also due to simplicity of architecture – encoder-decoder, where the\nendoder gives the latent coordinates of a sample.\n“Vanilla” VAEs take a sample of data, x (e.g. a batch of images) and transform it, layer by layer\nof the encoder (if data is images, some convolutional layers are applied first), to a point in the latent\nspace z. On that space livings a family of parameterized probability distributions (posteriors) pθ(z|x)\n(one can say that the encoder just outputs a vector of parameters θ of this distribution, thus specifying a\nrepresentative of this family – doing what’s called parametric inference in statistics). One then generates\na sample z ∼ pθ(z|x) from that distribution – this is a non-deterministic action and so at first it seems\nnon-differentiable for one to design a backward pass (error backpropagation), but a clever idea of the\nreparameterization trick [45] overcomes this problem (by separating parameterization of a distribution\nfrom sampling from it). This “latent code” z that somewhat describes the input sample x is then passed\nto the decoder (if the VAE has to generate images – last layers of the decoder will be deconvolutional,\netc.) to produce an output – a new sample of data. There is also what’s called a prior distribution p(z)\nof latent codes, which, for the case of latent space being Rd, is uninformatively chosen to be standard\nnormal (uniform distribution can’t be supported on Rd). There exists a problem of “posterior collapse”\n– when the posterior distribution gets too close to the uninformative prior on some latent codes. Various\nmodifications of VAEs aim to fix this problem, with VQ-VAE doing so quite successfully under certain\nconditions.\nVector-Quantised VAEs [46] take VAEs this idea to a “discrete” setting – the family of posterior\ndistributions pθ(z|x) is now discrete rather than continuous (multidimensional normal in vanilla VAEs),\nsupported on a fixed-size dictionary of latent codes (codewords). The motivation behind this is that,\nfor any finite sample of natural images, there will only be a discrete set of classes in it – dogs, cats, cars,\netc. So VQ-VAEs tend to produce far sharper images than their vanilla VAE counterparts (which tend to\nblur the image, with the uncertainty of the posterior distribution translating into gaussian-like, although\nnonlinearly transformed, blur on the resulting images), which at first seemed better for our needs. The\nencoder of VQ-VAE thus learns to output these discrete posteriors (the latent codes and their posterior\nprobabilities) so that samples from them (output by the decoder) were alike the training data. Since\n8\nDraft\nthe prior p(z) is also supported on this discrete codebase (which can support a uniform distribution),\nchoosing it to be uniform helps prevent posterior collapse.\nIn our experiments, we’ve tested VQ-VAEs with latent dim = 64 with dictionary size of 512 – while\nCIFAR-10 only has 10 classes, codes are assigned not to singular images, but to their batches (samples).\nBoth encoder and decoder were built with ResidualStacks [33] of 3-4 convolutional layers.\n3.2.2\nGenerative Adversarial Networks (GANs)\nAnother classic family of generative models, that we successfully applied to our case is Generative\nAdversarial Networks (GANs) [13]. A GAN is comprised of two NNs, a generator G and a discriminator\nD, playing a minimax game: G has to learn a distribution close to that of natural examples (images),\nfooling D that they are real, not fake; D has to discriminate fake images produced by G from real ones –\nby learning a probability distribution of, say, real ones (supported on the same space of latent variables\nof images’ features). For the purposes of working with images, both NNs of course should have some\npre-processing (convolution) layers, etc.\nDue to the nature of this minimax game G and D are playing, vanilla GANs suffered from instability\nissues on datasets as high-dimensional as CIFAR-10 (as noted, 32x32 color images of 10 classes), so many\nregularization techniques were proposed. Quite popular one being the Spectral Normalization (SN-GAN)\n[47] – with the weight matrices of NN layers being penalized so that their Lipschits constant (a measure\nof continuity of a function) was bounded from above by 1. This simple yet elegant solution allowed the\nauthors of SN-GAN reach then-state-of-the-art results on CIFAR-10 in terms of so-called inception score\n[48] and Fr´echet inception distance (FID) [49].\nWe’ve been using an SN-GAN with the latent space of 128 latent dimensions discretized into 64 points\neach.\n3.3\nSpiking neural network architectures\nBefore moving on to experiments on maximizing the neural responses in vivo, it is necessary to fully\nprepare and test the pipeline of such an experiment, where living neurons are replaced by some adequate\nin silico model. The most studied class of such models are artificial spiking neural networks (SNN).\nAmong all the software implementations of SNN, two most well-known and developed one were chosen\n- the SNNTorch library [31] and SpikingJelly library [32].\nThe specificity of spiking neural networks, as well as of biological ones, lies in the discreteness of\nspike events, and therefore their non-differentiability. This creates a problem for the backpropagation\nalgorithm, which cannot assign coefficients to update the weights on a backward pass. In this work, we\nuse the solution introduced in [31], based on the so-called surrogate gradient method, see Fig. 2.\nFirst, a simple convolutional network based on SNNtorch was implemented, trained to solve the prob-\nlem of image classification from the CIFAR-10 dataset (images 32x32 pixels, 10 classes). The architecture\nof the model consisted of “regular” (non-spike) convolutional layers, as well as fully-connected layers.\nThe spiking nature of the model was achieved by projecting the input of ordinary layers to the Leaky\nIntegrate-and-Fire neurons with a membrane decay constant of 0.9.\nThe trained spiking network achieved an accuracy of 72% on the CIFAR-10 dataset. This accuracy\nis lower than that of state-of-the-art non-spike architectures, but it was quite sufficient for the question\nbeing studied about the formation of cognitive specializations in neural networks.\nTo speed up inference and make the network deeper, we integrated the SpikingJelly SNN library\ninto our framework, which supports a fast CuPy backend. We used a spiking analogue of a celebrated\nResNet18 model [33] for the main results, since this architecture provided an optimal tradeoff between\ndepth and inference speed. Again, we used LIF neurons with membrane decay constant equal to 0.9.\nThe trained network achieved 86% accuracy on CIFAR10 dataset.\nSingle neuron activity was determined by the number of spikes that the neuron produced over a fixed\nperiod of time (100 counts for an SNN-based network and 20 for SpikingJelly-based one). Activity was\naveraged over all feature channels and normalized to the range [0,1].\n3.4\nOptimization algorithms\nOne of the most popular and effective software packages for gradient-free optimizaton is Nevergrad frame-\nwork [50] — methods implemented there were used as benchmarks. Notably, Nevergrad also has several\nmethods that are combining several optimization approaches into one – -these are called “Portfolios”.\n9\nDraft\nBased on our testing, the basic Portfolio method – combining methods of 1) Covariance matrix adap-\ntation evolution strategy (CMA-ES), 2) Differential evolution (DE) and 3) scr-Hammersley – performed\nbest of all in our problem setting. We compared it to other popular Nevergrad methods of 1) OnePlu-\nsOne, 2) NoisyBandit and 3) Simultaneous perturbation stochastic approximation (SPSA) — all of them\nhave shown poorer results to the abovementioned Portfolio.\nAs an alternative, we’ve used PROTES optimization method [41] (described in prior sections), based\non low-rank tensor decomposition, Tensor Train (TT) [15]. We’ve used additional quantization (bina-\nrization) of tensor modes (each tensor dimension is transformed into a new set of dimensions, having\nsmaller mode), which allowed us to reach better results than baseline PROTES.\nSo the optimization methods compared in the Results section are:\n1. (baseline) Portfolio from the Nevergrad package\n2. (baseline) PROTES with (k = 10; k-top = 1)\n3. TT-s = PROTES with (k = 5; k-top = 1) + quantization\n4. TT-b = PROTES (k = 25; k-top = 5) + quantization\n3.5\nMANGO framework\nWe created a framework for fast and accurate computation of MEIs in artificial networks - MANGO\n(Maximization of neural Activation via Non-Gradient Optimization). It was of particular interest to us\nto consider models that are closer to the mechanisms of functioning of biological neurons, in particular\nspiking neural networks (SNN), since one of our main goals in the future is to apply the developed\nmethods to living systems. Despite this, the gradient-free optimization methods we propose in this work\nare well applicable to classical neural networks.\nWithin the framework, it is possible to select a dataset, generator model, target neural network model\nand optimization method; carrying out numerical calculations with various hardware backends; saving\nand analysing the results.\nHere is a list of options that have been added to the framework:\n1. Datasets: MNIST, Fashion-MNIST, CIFAR10, Imagenet [44, 51–53]\n2. Generative models: VAE-VQ [46], GAN-SN [47]\n3. Classic neural networks: AlexNet [54], Densenet [55], VGG [56]\n4. Spiking neural networks: SNNTorch-supported model CNN and SpikingJelly-supported spiking\nResNet18 [31–33]\n5. Optimization methods: Nevergrad-based benchmarks and Tensor Train decomposition-based meth-\nods (TTOpt, PROTES, etc.) [14, 41, 50]\n6. backends: CPU (with multithreading), GPU, CuPy for SNN-based models\nThe selected dataset is used to train a convolutional (probably spiking) network for the image clas-\nsification task, as well as to train a generator network for the task of compressing input data into an\neffective latent representation. After training, the generator creates random latent representations of z,\nwhich are transformed into images and presented to the convolutional network. The activation of the\nneuron of interest is measured and fed as input to the optimizer, which produces an improved latent\nvector z that maximizes the likely response of the neuron studied. This process is repeated multiple\ntimes until convergence or until the optimizer query budget is exhausted.\nMANGO code is available on GitHub https://github.com/iabs-neuro/mango, including training\nand analysis scripts. Full sets of MEIs will be provided to interested readers upon a reasonable request.\n4\nResults\n4.1\nTensor Train-based optimization methods outperform non-gradient bench-\nmarks\nAmong the methods used, approaches based on Tensor Train decomposition showed performance equal\nto or 10-20% better than the benchmarks (mainly the Portfolio method, as the most effective of the\n10\nDraft\nnon-gradient family under consideration).\nPerformance was measured as MEI-related target neuron\nactivation.\nFigure 5: Activation of one selected neuron (unit 0, layer ”sn1”, spiking ResNet18)\ndepending on the number of requests to the optimizer.\nLeft: full optimization\nhistory, right: inset for optimization budget from 1000 to 12000\nThe PROTES method in various modifications turned out to be 2-4 times faster than the Nevergrad\nbenchmark, depending on the parameters and properties of the image. At the same time, tensor methods\nrequired fewer steps to achieve a high level of activation of the target neuron (see Fig. 5).\nThree different (albeit related) tensor methods and a non-gradient benchmark from the Nevergrad\nlibrary converged on the same images very often.\nThis is quite surprising, given the huge potential\nnumber of image variants sampled from the latent space of the generator. Despite the fact that in deep\nlayers the difference between MEIs became larger (see section 4.4), in the early layers of the\nnetwork they are very close to each other(see Figs 6, 7).\nThis suggests that all discrete optimization algorithms used indeed converge to a good optimum in\nthe latent space of the generator. Taking into account the results from Section 4.2, we can extend these\nresults to the general space of all images and conclude that discrete optimization methods, especially\nthose based on Tensor Train decomposition, perform well in finding MEIs.\n4.2\nComparison of GAN- and VAE- based generators\nWe created MEIs for each neuron of all spiking layers from spiking ResNet18. The analysis showed that\nthe maximal activations of neurons on images from VAE-VQ were smaller than on images from GAN-SN.\nThe magnitude of the gap depended on the layer and ranged from 50% in early layers to 5% in deep\nlayers (Fig. 11).\nIn general, despite neuronal activations were significantly above chance, the MEIs generated by VAE\noften contain no visible structure and do not provide any insight about neuronal specializations.\nThe exception is the early layers, in which the MEIs obtained using both generators are similar to\neach other. This allows us to expect that discrete optimization methods actually find generator-agnostic\nmaxima in the global image space, and difficulties in obtaining interpretable images can be solved by\nfurther improving the structure of the latent space of generators (see Discussion).\n11\nDraft\nFigure 6: MEIs for unit 16 of the first spiking layer from spiking ResNet18. Top\nrow: images generated with GAN-SN, bottom row: images generated with VQ-\nVAE. Columns correspond to TT-based optimization methods.\nThus, VAE-generated images are closest to richer GAN-generated ones where there are uniform colors\nor simple geometric patterns in the image (see Figs 6, 7). We believe that this is a sign of too much\ncompression of information in the latent space of VAE-VQ generator, which leads to the loss of significant\nimage details.\nFigure 7: MEIs for unit 35 of the first spiking layer from spiking ResNet18. Top\nrow: images generated with GAN-SN, bottom row: images generated with VQ-\nVAE. Columns correspond to TT optimization methods.\nThe possible reason for VQ-VAE performing poorer in our setup is the following: as described in\nthe above sections, VQ-VAEs learn discretized, rather than continuous posterior distributions of data\nsamples – supported on a finite set (dictionary) of (learned) vectors in the latent space, the distribution\nis then akin to multinomial on that discrete pointset. For that reason, VQ-VAEs seem to be not so good\nat interpolating between probable datapoints (images) – designed to mitigate posterior collapse and\ngaussian-like blur of generated images in vanilla VAEs, it seems they traded in the ability to generate\ninteresting out-of-distribution samples. Of course, some interpolation is possible, but it seems we didn’t\n12\nDraft\nfind good balance between discretization parameters of VQ-VAE and TT – that is matter of future\ninvestigation.\nSN-GANs performed far better in our experiment, due to providing nice continuous (benefits of\nSpectral Normalization) “coordinates” on the latent space of images. However, the above results on\nneuron specialization distributions seem to be universal – no matter what generative model was used.\n4.3\nHighly selective neurons are present in early layers of the network\nAn important issue when analyzing MEIs is the relationship between the identified neuronal specializa-\ntions and the patterns that the network studied has learned in the dataset.\nWe analyzed the activations of Resnet18 spiking layers for correspondence to objects in the images\nfrom the training set.\nSurprisingly, it turned out that highly specialized neurons can appear in the\nnetwork as early as in the third spiking layer(see Fig. 8)\nFigure 8: “Pink horse” neuron 52 from LIF layer 1.1 of spiking ResNet18. Images\nwere generated using tensor-train based methods. Numbers show activations of the\ntarget neuron on MEI\nIn general, as expected, neurons in early layers are maximally activated by simple geometric features\nor color patterns, while neurons in later layers show selective activation for one class or another.\nTo quantify neuronal selectivity, we performed network inference on MEIs, obtaining the probabili-\nties for each class. Our hypothesis was that more uniform probability distributions correspond to more\nabstract and general images, whereas high probability for one of the classes indicates presence of a corre-\nsponding pattern in a MEI. For each MEI, we calculated the entropy of the final probability distribution\nand normalized it to the maximal possible entropy obtained from a completely uniform distribution. The\nprocedure was repeated 100 times to account for single-trial spike variability inevitably present in SNN.\nThe results are shown in Fig. 9. Note a sharp peak of low-entropy MEIs for the last network spiking\nlayer and the presence of the same smaller peak for the first spiking layer. Already in the middle of the\nnetwork (layer 2.1), most neurons are specifically associated with some class.\n13\nDraft\nFigure 9: Layerwise normalized entropy distributions of probalilities from spiking\nResNet18 generated on corresponding neuron MEIs from a given layer. Images were\ngenerated with GAN-SN generator and averaged over 100 iterations.\n4.4\nMEIs are more diverse in deep layers\nMEIs generated using different discrete optimization methods contain the same patterns of varying\ndegrees of complexity, but are not identical to each other. In order to evaluate the variability of the\nresulting images, we calculated the average Euclidean distance between the latent vectors corresponding\nto the MEIs (GAN-SN generator was used).\nThe variability of possible images in the last layers of spiking ResNet18 turned out to be greater\nthan in the first ones (see Fig. 10). Average distances between the generated MEIs in the last spiking\nlayer turned out to be about 20% higher than in the first one, revealing significant transformations of\nthe optimal image landscape.\nThis effect can be explained by the large number of possible ways in which a complex concept, such as\na horse, can be implemented in an image. It is known that neural networks learn to nonlinearly project\nthe data space during training, simultaneously identifying some of its areas with each other. Thus, the\nnumber of local maxima in the overall image space grows with increasing specialization of neurons.\n14\nDraft\nFigure 10: Distribution of distances between MEIs for a given neuron in latent space\nof GAN-SN. Blue: first spiking layer, red: last spiking layer\nThe growing diversity of maximally activating images is accompanied by an increase in the activations\nthey produce on target neurons( 11).This may be explained by the finer specialization of neurons in the\ndeep layers, in accordance with the results from section 4.3. At the same time, the dips in maximum\nactivations observed in layers 3.1 and 4.1 are reproduced by both generators and show that the overall\npicture may be more complex. Perhaps the optimization budget used was not enough to get to the true\nMEIs of the neurons of these layers.\nFigure 11: Layerwise activation distributions of LIF neuron spiking layers from\nSpiking ResNet18. For better visibility, every second layer is present. Activation of\neach neuron was recorded after network inference on the corresponding MEI. Left:\nactivations for VAE-VQ generated images, right: for GAN-SN generated images.\n15\nDraft\n5\nDiscussion\n5.1\nLimitations of the current work and directions of future research\nAs our results show, the quality of the calculated MEIs depends significantly on the image generator\nused. In the future, we plan to add support to MANGO for new generators that are well suited for the\nMEI search task (i.e., combining the discrete structure of the latent space with the high plausibility of\nthe generated images). It is of interest to consider the popular direction of discrete GANs [57].\nAn important research question remains the patterns and mechanisms of the neuronal specializations\nformation over time. Although in this work we examined MEIs for an already trained network, it is of\ngreat interest to be able to monitor their evolution as training progresses. We plan to add this feature\nto our MANGO framework in the future.\nOne of the most interesting and potentially fruitful applications of our tools is maximizing neuronal\nactivation in vivo. Since biological neurons have an irreducible minimum response time to a stimulus,\nresearch into algorithms aimed at reducing the optimizer budget required for MEI generation seems very\nrelevant. In particular, we plan to introduce into our framework the latest discrete optimization methods\nbased on Tensor Train decomposition, specifically optimized for this task.\n5.2\nActivation maximization in ANNs & explainable AI\nThe problem of maximization of neuronal activation (response) in ANNs is being actively studied\nwithin the field of ANN visualisation, a subfield of a more general direction – development of inter-\npretable/explainable AI models. To succesfully integrate ANN-based solutions in critical systems – such\nas medical or law practices – one needs a human-understandable interpretation of the algorithm’s decision\nprocess. In recent years, analysis methods based on various techniques of visualising the computation\ngraph of an ANN, its loss function profile, parameter (space) of its certain layers or even certain neurons\n– have gained popularity for creating interpretable deep learning models. Considered methods allow one\nto delve deeper and better understand the inner workinds of neural algorithms.\nA recent review [19] of such visualisation techniques contains: the method of activation maximization\n[58], the Grad-CAM method [59], as well as the Integrated Gradients method [60]. Another outstanding\nrecent work worth noting is [61], containing a wide demonstration of stimuli that maximize the activation\nof various neurons of an ANN, where the effect of multimodality was demonstrated for artificial neurons.\nHowever, all of the aforementioned activation maximization methods are based on automatic differ-\nentiation: the computation graph of the considered ANN is formed, which allows to compute gradients\nwith minimal complexity (computational complexity of a NN gradient is practically the same as that\nof its forward-pass), which allows to use gradient descent (or ascent, in case of maximization) and its\nstochastic modifications. For living systems – gradient based methods are clearly inapplicable, so one\nshould either use gradient-free methods (e.g. genetic algorithms), or other methods using a local ap-\nproximation of the gradient (e.g. evolutionary algorithms), or other approaches, including optimization\nwith TT-decomposition-based approximation of the optimized function. Notably, it was genetic and\nevolutionary algorithms that were considered in [12] for activation maximization in an ANN.\n5.3\nPossible applications for maximization of activation in biological neurons\nModern optical imaging techniques allow one to register the activity of hundreds of neurons unfold in their\ntemporal dynamics in vivo. On one hand, activation (response) maximization of multiple neurons (both\nseparately and group-wise) could become a powerful tool for analyzing the functions of the nervous system\nat a cell level, shed light on the formation and spread of cognitive specializations of living neurons. On\nthe other hand, the framing of biological experiment poses serious constraints on applicability of various\noptimization methods – these should be sufficiently quick, accurate and capable of tuning (post-training)\non new-coming data. We expect that TT-decomposition-based methods developed in course of present\nwork would possess all such properties.\nA separate problem is the search for optimal stimuli in non-visual spaces (e.g. in the space of sounds\nor smells, which could be far more relevant for the living object, rather than for a human). We also\nhope to obtain some estimates of applicability of TT-decomposition-based methods in case of such latent\nspaces of stimuli.\nStudies of neural specializations in spiking neural networks are of great interest because, using a\nbiologically plausible model, they reveal the mechanisms of operation of the functional units of the\nneural network. Thus, the framework we created can be used as a bridge between neurobiological and\n16\nDraft\nin silico experiments, allowing one to explore the general principles of information encoding in deep\nneural networks. We also hope that this work will significantly bring activation maximization algorithms\ncloser to the requirements of a real biological experiment and thereby open a new tool for analyzing the\ncognitive functions of individual neurons.\n6\nAcknowledgements\nThis work was supported by Non-Commercial Foundation for Support of Science and Education ”IN-\nTELLECT” and by Lomonosov Moscow State University. N. Pospelov acknowledges support from the\nBrain Program of the IDEAS Research Center. This research was supported in part through computa-\ntional resources of HPC facilities at HSE University [62] – for training the models & running the pipeline\npart of the times. The authors are grateful to all members of Laboratory of Neuronal Intelligence and\nespecially to Ksenia A.Toropova for numerous fruitful discussions.\n7\nReferences\nReferences\n[1] D. H. Hubel and T. N. Wiesel, Brain and visual perception: the story of a 25-year collaboration.\nOxford University Press, 2004.\n[2] J. Vierock, S. Rodriguez-Rozada, A. Dieter, F. Pieper, R. Sims, F. Tenedini, A. C. Bergs, I. Ben-\ndifallah, F. Zhou, N. Zeitzschel, et al., “Bipoles is an optogenetic tool developed for bidirectional\ndual-color control of neurons,” Nature communications, vol. 12, no. 1, p. 4527, 2021.\n[3] C. R. Ponce, W. Xiao, P. F. Schade, T. S. Hartmann, G. Kreiman, and M. S. Livingstone, “Evolving\nimages for visual neurons using a deep generative network reveals coding principles and neuronal\npreferences,” Cell, vol. 177, no. 4, pp. 999–1009, 2019.\n[4] A. Bardon, W. Xiao, C. R. Ponce, M. S. Livingstone, and G. Kreiman, “Face neurons encode nonse-\nmantic features,” Proceedings of the national academy of sciences, vol. 119, no. 16, p. e2118705119,\n2022.\n[5] A. J. Shriver and T. M. John, “Neuroethics and animals: report and recommendations from the\nuniversity of pennsylvania animal research neuroethics workshop,” ILAR journal, vol. 60, no. 3,\npp. 424–433, 2019.\n[6] P. Singer and Y. F. Tse, “Ai ethics: the case for including animals,” AI and Ethics, vol. 3, no. 2,\npp. 539–551, 2023.\n[7] K. Dobs, J. Martinez, A. J. Kell, and N. Kanwisher, “Brain-like functional specialization emerges\nspontaneously in deep neural networks,” Science advances, vol. 8, no. 11, p. eabl8913, 2022.\n[8] A. M. Zador, “A critique of pure learning and what artificial neural networks can learn from animal\nbrains,” Nature communications, vol. 10, no. 1, p. 3770, 2019.\n[9] R. Schaeffer, M. Khona, and I. Fiete, “No free lunch from deep learning in neuroscience: A case study\nthrough models of the entorhinal-hippocampal circuit,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 16052–16067, 2022.\n[10] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah,\n“Multimodal neurons in artificial neural networks,” Distill, vol. 6, no. 3, p. e30, 2021.\n[11] C.\nOlah,\nA.\nMordvintsev,\nand\nL.\nSchubert,\n“Feature\nvisualization,”\nDistill,\n2017.\nhttps://distill.pub/2017/feature-visualization.\n[12] B. Wang and C. R. Ponce, “High-performance evolutionary algorithms for online neuron control,”\nin Proceedings of the Genetic and Evolutionary Computation Conference, pp. 1308–1316, 2022.\n[13] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY. Bengio, “Generative adversarial nets,” Advances in neural information processing systems, vol. 27,\n2014.\n17\nDraft\n[14] K. Sozykin, A. Chertkov, R. Schutski, A.-H. Phan, A. S. CICHOCKI, and I. Oseledets, “Ttopt: A\nmaximum volume quantized tensor train-based optimization and its application to reinforcement\nlearning,” Advances in Neural Information Processing Systems, vol. 35, pp. 26052–26065, 2022.\n[15] I. V. Oseledets, “Tensor-train decomposition,” SIAM Journal on Scientific Computing, vol. 33, no. 5,\npp. 2295–2317, 2011.\n[16] K. Yamazaki, V.-K. Vo-Ho, D. Bulsara, and N. Le, “Spiking neural networks and their applications:\nA review,” Brain Sciences, vol. 12, no. 7, p. 863, 2022.\n[17] Y. Li, Y. Kim, H. Park, and P. Panda, “Uncovering the representation of spiking neural networks\ntrained with surrogate gradient,” arXiv preprint arXiv:2304.13098, 2023.\n[18] E. Y. Walker, F. H. Sinz, E. Cobos, T. Muhammad, E. Froudarakis, P. G. Fahey, A. S. Ecker,\nJ. Reimer, X. Pitkow, and A. S. Tolias, “Inception loops discover what excites neurons most using\ndeep predictive models,” Nature neuroscience, vol. 22, no. 12, pp. 2060–2065, 2019.\n[19] S. A. Matveev, I. V. Oseledets, E. S. Ponomarev, and A. V. Chertkov, “Overview of visualization\nmethods for artificial neural networks,” Computational Mathematics and Mathematical Physics,\nvol. 61, no. 5, pp. 887–899, 2021.\n[20] W. Xiao and G. Kreiman, “Xdream: Finding preferred stimuli for visual neurons using generative\nnetworks and gradient-free optimization,” PLoS computational biology, vol. 16, no. 6, p. e1007973,\n2020.\n[21] W. Xiao and G. Kreiman, “Gradient-free activation maximization for identifying effective stimuli,”\narXiv preprint arXiv:1905.00378, 2019.\n[22] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” nature, vol. 521, no. 7553, pp. 436–444,\n2015.\n[23] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.\n[24] E. M. Izhikevich, “Simple model of spiking neurons,” IEEE Transactions on neural networks, vol. 14,\nno. 6, pp. 1569–1572, 2003.\n[25] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in nervous activity,” The\nbulletin of mathematical biophysics, vol. 5, pp. 115–133, 1943.\n[26] F. Rosenblatt, The perceptron, a perceiving and recognizing automaton Project Para. Cornell Aero-\nnautical Laboratory, 1957.\n[27] F. Rosenblatt, “The perceptron: a probabilistic model for information storage and organization in\nthe brain.,” Psychological review, vol. 65, no. 6, p. 386, 1958.\n[28] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by back-propagating\nerrors,” nature, vol. 323, no. 6088, pp. 533–536, 1986.\n[29] W. Maass, “Networks of spiking neurons: the third generation of neural network models,” Neural\nnetworks, vol. 10, no. 9, pp. 1659–1671, 1997.\n[30] N. Anwani and B. Rajendran, “Training multi-layer spiking neural networks using normad based\nspatio-temporal error backpropagation,” Neurocomputing, vol. 380, pp. 67–77, 2020.\n[31] J. K. Eshraghian, M. Ward, E. Neftci, X. Wang, G. Lenz, G. Dwivedi, M. Bennamoun, D. S. Jeong,\nand W. D. Lu, “Training spiking neural networks using lessons from deep learning,” Proceedings of\nthe IEEE, vol. 111, no. 9, pp. 1016–1054, 2023.\n[32] W. Fang, Y. Chen, J. Ding, Z. Yu, T. Masquelier, D. Chen, L. Huang, H. Zhou, G. Li, and Y. Tian,\n“Spikingjelly: An open-source machine learning infrastructure platform for spike-based intelligence,”\nScience Advances, vol. 9, no. 40, p. eadi1480, 2023.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings\nof the IEEE conference on computer vision and pattern recognition, pp. 770–778, 2016.\n18\nDraft\n[34] A. L. Hodgkin and A. F. Huxley, “A quantitative description of membrane current and its application\nto conduction and excitation in nerve,” The Journal of physiology, vol. 117, no. 4, p. 500, 1952.\n[35] N. Brunel and M. C. Van Rossum, “Lapicque’s 1907 paper: from frogs to integrate-and-fire,” Bio-\nlogical cybernetics, vol. 97, no. 5-6, pp. 337–339, 2007.\n[36] A. Novikov, D. Podoprikhin, A. Osokin, and D. P. Vetrov, “Tensorizing neural networks,” Advances\nin neural information processing systems, vol. 28, 2015.\n[37] D. Liu, L. T. Yang, P. Wang, R. Zhao, and Q. Zhang, “Tt-tsvd: A multi-modal tensor train decom-\nposition with its application in convolutional neural networks for smart healthcare,” ACM Trans-\nactions on Multimedia Computing, Communications, and Applications (TOMM), vol. 18, no. 1s,\npp. 1–17, 2022.\n[38] A. Chertkov, G. Ryzhakov, G. Novikov, and I. Oseledets, “Optimization of functions given in the\ntensor train format,” arXiv preprint arXiv:2209.14808, 2022.\n[39] A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao, D. P. Mandic, et al., “Tensor networks\nfor dimensionality reduction and large-scale optimization: Part 1 low-rank tensor decompositions,”\nFoundations and Trends® in Machine Learning, vol. 9, no. 4-5, pp. 249–429, 2016.\n[40] A. Cichocki, A.-H. Phan, Q. Zhao, N. Lee, I. Oseledets, M. Sugiyama, D. P. Mandic, et al., “Tensor\nnetworks for dimensionality reduction and large-scale optimization: Part 2 applications and future\nperspectives,” Foundations and Trends® in Machine Learning, vol. 9, no. 6, pp. 431–673, 2017.\n[41] A. Batsheva, A. Chertkov, G. Ryzhakov, and I. Oseledets, “PROTES: Probabilistic optimization\nwith tensor sampling,” Advances in Neural Information Processing Systems, 2023.\n[42] M. W. Trosset, “What is simulated annealing?,” Optimization and Engineering, vol. 2, pp. 201–213,\n2001.\n[43] R. J. Williams, “Simple statistical gradient-following algorithms for connectionist reinforcement\nlearning,” Machine learning, vol. 8, pp. 229–256, 1992.\n[44] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features from tiny images,” 2009.\n[45] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint arXiv:1312.6114,\n2013.\n[46] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” Advances in neural\ninformation processing systems, vol. 30, 2017.\n[47] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida, “Spectral normalization for generative adver-\nsarial networks,” arXiv preprint arXiv:1802.05957, 2018.\n[48] T. Salimans, I. Goodfellow, W. Zaremba, V. Cheung, A. Radford, and X. Chen, “Improved tech-\nniques for training gans,” Advances in neural information processing systems, vol. 29, 2016.\n[49] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter, “Gans trained by a two time-\nscale update rule converge to a local nash equilibrium,” Advances in neural information processing\nsystems, vol. 30, 2017.\n[50] J. Rapin and O. Teytaud, “Nevergrad - A gradient-free optimization platform.” https://GitHub.\ncom/FacebookResearch/Nevergrad, 2018.\n[51] Y. LeCun, “The mnist database of handwritten digits,” http://yann. lecun. com/exdb/mnist/, 1998.\n[52] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset for benchmarking machine\nlearning algorithms,” arXiv preprint arXiv:1708.07747, 2017.\n[53] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-scale hierarchical\nimage database,” in 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255,\nIeee, 2009.\n[54] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification with deep convolutional\nneural networks,” Advances in neural information processing systems, vol. 25, 2012.\n19\nDraft\n[55] G. Huang, Z. Liu, L. Van Der Maaten, and K. Q. Weinberger, “Densely connected convolutional net-\nworks,” in Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700–\n4708, 2017.\n[56] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recogni-\ntion,” arXiv preprint arXiv:1409.1556, 2014.\n[57] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for high-resolution image synthesis,”\nin Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 12873–\n12883, 2021.\n[58] D. Erhan, Y. Bengio, A. Courville, and P. Vincent, “Visualizing higher-layer features of a deep\nnetwork,” University of Montreal, vol. 1341, no. 3, p. 1, 2009.\n[59] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Vi-\nsual explanations from deep networks via gradient-based localization,” in Proceedings of the IEEE\ninternational conference on computer vision, pp. 618–626, 2017.\n[60] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for deep networks,” in International\nconference on machine learning, pp. 3319–3328, PMLR, 2017.\n[61] G. Goh, N. Cammarata, C. Voss, S. Carter, M. Petrov, L. Schubert, A. Radford, and C. Olah,\n“Multimodal neurons in artificial neural networks,” Distill, vol. 6, no. 3, p. e30, 2021.\n[62] P. Kostenetskiy, R. Chulkevich, and V. Kozyrev, “Hpc resources of the higher school of economics,”\nin Journal of Physics: Conference Series, vol. 1740, p. 012050, IOP Publishing, 2021.\n20\n"
}