{
    "optim": "Statistical Test for Anomaly Detections by Variational Auto-Encoders Daiki Miwa Nagoya Institute of Technology miwa.daiki.mllab.nit@gmail.com Tomohiro Shiraishi Nagoya University shiraishi.tomohiro.nagoyaml@gmail.com Vo Nguyen Le Duy University of Information Technology, Ho Chi Minh City, Vietnam Vietnam National University, Ho Chi Minh City, Vietnam RIKEN duyvnl@uit.edu.vn Teruyuki Katsuoka Nagoya University katsuoka.teruyuki.nagoyaml@gmail.com Ichiro Takeuchi∗ Nagoya University RIKEN ichiro.takeuchi@mae.nagoya-u.ac.jp February 7, 2024 Abstract In this study, we consider the reliability assessment of anomaly detection (AD) using Vari- ational Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in various perspective, from method development to applied research. However, when the results of ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to ensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as a method for quantifying the statistical reliability of VAE-based AD within the framework of statistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a VAE can be quantified in the form of p-values. This means that if an anomaly is declared when the p-value is below a certain threshold, it is possible to control the probability of false detection ∗Corresponding author 1 arXiv:2402.03724v1  [stat.ML]  6 Feb 2024 to a desired level. Since the VAE-AD Test is constructed based on a new statistical inference framework called selective inference, its validity is theoretically guaranteed in finite samples. To demonstrate the validity and effectiveness of the proposed VAE-AD Test, numerical experiments on artificial data and applications to brain image analysis are conducted. 2 1 Introduction Anomaly detection (AD) is the process of identifying unusual deviations in data that do not conform to expected behavior. AD is crucial across various domains because it provides early warnings of potential issues, thereby enabling timely interventions to prevent critical events. Traditional AD techniques, while effective in straightforward scenarios, frequently fall short when dealing with complex data, thus motivating the use of deep learning-based AD approaches to better handle such complexities. In this study, among various deep learning-based AD approaches, we focus on AD using the Variational Auto-Encoder (VAE), and its application to medical images. A VAE serves as an effective tool for AD task. In the training phase, the VAE learns the dis- tribution of normal images by training exclusively on images that do not contain abnormal regions. It encodes the input image into a latent space and then reconstructs it back to its original form. The parameters of a VAE are optimized to minimize the reconstruction error, thereby learning a compressed representation of the normal data. In the test phase, when a test image is fed into the trained VAE, the model attempts to reconstruct the image based on its learned representation. The reconstruction error is then calculated, typically as the difference between the input image and its reconstructed version. Since the VAE is trained on normal data, it would successfully reconstruct the normal regions of the image, while it would fail to properly reconstruct the abnormal regions that were not included in the normal data. Therefore, regions with large reconstruction errors are detected as abnormal regions. Figure 1 shows an example of VAE-based AD for brain tumor images. When VAE-based AD is employed for high-stakes decision-making tasks, such as medical diagnosis, there is a significant risk that model inaccuracies might lead to critical errors, potentially resulting in false detections. To address this issue, in this study, we develop a statistical test for VAE-based AD, which we call VAE-AD Test. The proposed VAE-AD test enables us to obtain a quantifiable and interpretable measure for the detected anomaly region in the form of p-value. The obtained p- value represents the probability that the detected anomaly regions are obtained by chance due to the randomness contained in the data. For example, if we adopt a rule that classifies detected anomaly regions with a p-value less than 0.05 as anomalies, we can keep the false detection probability below 5% rigorously. It is important to note that the statistical test for detected abnormal regions is performed on data- driven hypothesis, as the abnormal region is selected based on the test image itself. In other words, since both of the selection of the hypothesis (selection of abnormal regions) and the evaluation of the hypothesis (evaluation of abnormal regions) are performed on the same data, applying traditional statistical test to the selected hypothesis (abnormal regions) leads to selection bias. Therefore, in this study, we introduce the framework of Conditional Selective Inference (CSI), which has been recognized 3 as a new statistical testing framework for data-driven hypotheses. CSI has primarily been studied in the context of feature selection for linear models, but in recent years, it has been extended to various problems in various models (see the related works section for more details). The main contributions in this study are the following three points. Our first contribution is the formulation of an approach that provides a quantifiable and interpretable measure for the reliability of anomalies detected by VAE in the form of a p-value based on a statistical test. With this approach, we can properly control the probability of false detection probability to the desired significance level in finite samples. The second contribution is the development of an SI method for VAEs. SI is formulated as a conditional test based on the conditions under which hypotheses are selected (see §3 for details), requiring computations concerning the hypothesis selection event by a VAE. Our main technical contribution is to establish a method to compute the hypothesis selection event for a wide class of VAEs. Finally, the third contribution is to demonstrate the effectiveness of the proposed VAE-AD Test by conducting numerical experiments with synthetic data and brain tumor images. The code is available at https://github.com/DaikiMiwa/VAE-AD-Test. 1.1 Related Work Over the last decade, there has been a significant pursuit in applying deep learning techniques to AD problems [Chalapathy and Chawla, 2019, Pang et al., 2021, Tao et al., 2022]. A large number of studies have been conducted for unsupervised AD using VAEs [Baur et al., 2021, Chen and Konukoglu, 2018, Chow et al., 2020, Jana et al., 2022]. VAE-based AD for images is broadly divided into two tasks. The first task is to determine whether the entire input image is normal or abnormal. The second task is to detect abnormal regions within the input image, which is sometimes referred to as anomaly localization [Zimmerer et al., 2019, Lu and Xu, 2018, Baur et al., 2019]. In this study, we focus on the second task. There are mainly two research directions for improving VAEs for AD. The first direction is on improving the indicators of the degree of anomaly [Zimmerer et al., 2019, Dehaene et al., 2020]. The second direction is on modifying the VAE itself to make it suitable for AD [Baur et al., 2019, Chen and Konukoglu, 2018, Wang et al., 2020]. However, to our knowledge, there has been no existing studies for quantifying the statistical reliability of detected abnormal regions with theoretical validity. In traditional statistical tests, the hypothesis needs to be predetermined and must remain inde- pendent of the data. However, in data-driven approaches, it is necessary to select hypotheses based on the data and then assess the reliability of the hypotheses using the same data. This issue, known as double dipping, arises because the same data is used for both the selection and evaluation of hypothe- ses, leading to selection bias [Breiman, 1992]. Because anomalies are detected based on data (a test image) in AD, when evaluating the reliability of the detected anomalies using the same data, the issue 4 ー Input Image Reconstruction Image Reconstruction Error Anomaly Region VAE (a) Image without tumor region. pnaive = 0.000 (false detection) and pselective = 0.668 (true negative). ー Input Image Reconstruction Image Reconstruction Error Anomaly Region VAE (b) Image with tumor region. pnaive = 0.000 (true detection) and pselective = 0.000 (true detection). Figure 1: An illustration of the proposed VAE-AD Test in brain image analysis. When an anomaly region is detected based on the difference between the original image and the reconstructed image by a VAE, the VAE-AD Test provides a p-value to quantify its statistical reliability. The upper plot shows the results of applying the VAE-AD Test and the conventional method, which does not consider the fact that the anomaly region is detected by VAE, to a case without anomaly regions. The lower plot shows the results for a case with anomaly regions. With the proposed method (pselective), correct judgments are made in both cases; the former has a large p-value and the latter has a small p-value. In contrast, with the conventional method (pnaive), both p-values are small, indicating false detection in the former case. of selection bias arises. CSI has recently gained attention as a framework for statistical hypothesis testing of data-driven hypotheses Lee et al. [2016], Taylor and Tibshirani [2015]. The key idea of CSI is to address the selection bias issue by considering a conditional test. CSI was initially developed for the statistical inference of feature selection in linear models [Fithian et al., 2015, Tibshirani et al., 2016, Loftus and Taylor, 2014, Suzumura et al., 2017, Le Duy and Takeuchi, 2021, Sugiyama et al., 2021], and then extended to various problems [Lee et al., 2015, Choi et al., 2017, Chen and Bien, 2020, Tanizaki et al., 2020, Duy et al., 2020, Gao et al., 2022]. CSI was first introduced to deep learn- ing context in Duy et al. [2022] and Miwa et al. [2023], with the former focusing on evaluating the reliability of segmentation and the latter aiming to assess the reliability of the saliency map. In this paper, we develop CSI for VAEs based on these studies and establish a framework for quantitatively evaluating the reliability of VAE-based AD1. 1A part of the authors of this paper are conducting research on CSI for Vision Transformers, aimed at quantifying the 5 2 Anomaly Detection (AD) by VAE In this section, we present how VAEs are used for AD. 2.1 Variational Autoencoder (VAE) VAEs are generative models originally proposed by Kingma and Welling [2013]. A VAE consists of an encoder network and a decoder network. Given an input image (denoted by x ∈ Rn), it is encoded as a latent vector (denoted by z ∈ Rm), and the latent vector is decoded back to the input image, where n is the number of pixels of an image and m is the dimension of a latent vector. In the generative process, it is assumed that a latent vector z is sampled from a prior distribution pθ∗(z) and then, image x is sampled from a conditional distribution pθ∗(x|z). The prior distribution pθ∗(z) and the conditional distribution pθ∗(x|z) belongs to family of distributions parametrized by θ and θ∗ denotes the true value of the parameter. Since the posterior distribution pθ(z|x) is assumed to be intractable, the encoder network ap- proximates it by the parametric distribution qϕ(z|x), where ϕ represents the set of parameters. The decoder network estimate the conditional distribution by pθ(x|z). The encoder and the decoder networks of a VAE are trained by maximizing so-called evidence lower bound (ELBO): Lθ,ϕ = Eqθ(z|x) [log pϕ(x|z)] − KL [qθ(z|x)||p(z)] , where KL [·||·] is the Kullback-Leibler divergence between two distributions. In this study, we model the approximated posterior distribution qϕ(z|x) as a normal distribution, represented by N(µϕ(x), Inσ2 ϕ(x)), where µϕ(x) and σ2 ϕ(x) are the outputs of the encoder network, and the condiional distribution pθ(x|z) as a normal distribution, represented by N(µθ(z), In), where µθ(z) is the outputs of the decoder network. The prior distribution pθ∗(z) is modeld as a standard normal distribution N(0, Im). The structure of the VAE used in this study is shown in Appendix A. 2.2 Anomaly Detection Using VAEs VAEs can be effectively used for AD task. As outlined in §1, AD tasks can be categorized into two types: anomaly detection (in a narrow sense) and anomaly localization. This paper concentrates on anomaly localization whose goal is to identify the abnormal region within a given test image. In the training phase, we assume that only normal images (e.g., brain images without tumors) are available. A VAE is trained on normal images to learn a compact representation of the normal image distribution in the latent space. reliability of attention maps [Anonymous, 2024b]. Additionally, there is another ongoing research on CSI for diffusion models, with the objective of extending CSI to generative models [Anonymous, 2024a]. 6 In the test phase, a test image x is fed into the trained VAE, and a reconstructed image is obtained by using the encoder and the decoder as ˆx = µθ (µϕ(x)). Since the VAE is trained only on normal images, normal region in the test image would be reconstructed well, whereas the reconstruction error of abnormal regions would be high. Therefore, it is reasonable to define the degree of anomaly of each pixel as Ei(x) = |xi − ˆxi|, i ∈ [n], (1) where xi and ˆxi is the ith pixel value of x and ˆx, respectively. Using a user-specified threshold λ > 0, the anomaly region of a test image x is defined as Ax = {i ∈ |n| | Ei(x) ≥ λ}. (2) As for the definition of the anomaly region, there are possibilities other than those given by Eqs. (1) and (2). In this paper, we proceed with these choices, but the proposed VAE-AD Test is generally applicable to other choices. 3 Statistical Test for Abnormal Regions In this section, we formulate a statistical test for abnormal region A in Eq. (2). Statistical model of an image. To formulate the reliability assessment of the abnormal region as a statistical testing problem, it is necessary to introduce a statistical model of an image. In this study, an image is considered as a sum of true signal component s ∈ Rn and noise component ϵ ∈ Rn. Regarding the true signal component, each pixel can have an arbitrary true signal value without any particular assumption or constraint. On the other hand, regarding the noise component, it is assumed to follow a normal distribution, and their covariance matrix is estimated using normal data different from that used for the training of the VAE. Namely, an image with n pixels can be represented as an n-dimensional random vector X = (X1, . . . , Xn) = s + ϵ, ϵ ∼ N(0, Σ), (3) where s ∈ Rn is the true signal vectors, and ϵ ∈ Rn is the noise vector with covariance matrix Σ. In the following, the capital X denotes an image as a random vector, while the lowercase x represents an observed image. To formulate the statistical test, we consider the AD using VAEs defined in Eq. (2) as a function A that maps a random input image X to the abnormal region AX, i.e., A : Rn ∋ X 7→ AX ∈ 2[n], (4) where 2[n] is the power set of [n] := {1, 2, . . . , n}. 7 Formulation of statistical test. Our goal is to make a judgment whether the abnormal region AX merely appears abnormal due to the influence of random noise, or if there is a true anomaly in the true signal in the abnormal region. In order to quantify the reliability of the detected abnormal region, the statistical test is performed for the difference between the true signal in the abnormal region {si}i∈AX and the true signal in the normal region {si}i∈Ac X where Ac X is the complement of the abnormal region. In this study, as an example, we consider the hypothesis for the difference in true mean signals between AX and Ac X by considering the following null and alternative hypotheses: H0 : 1 |AX| X i∈AX si = 1 |Ac X| X i∈Ac X si, v.s. H1 : 1 |AX| X i∈AX si ̸= 1 |Ac X| X i∈Ac X si. (5) For clarity, we mainly consider a test for the mean difference as a specific example — however, the proposed VAE-AD Test is applicable to a more general class of statistical tests. Specifically, let η ∈ Rn be an arbitrary n-dimensional vector depending on the abnormal region AX. Then, the proposed method can cover a statistical test represented as H0 : η⊤s = c v.s. H1 : η⊤s ̸= c, (6) where c is an arbitrary constant. The formulation in Eq. (6) covers a wide range of practically useful statistical tests. In fact, Eq. (5) is a special case of Eq. (6). It can cover differences not only in means but also in other measures such as maximum difference, and differences after applying some image filters (e.g., Gaussian filter). Test statistic. To evaluate the hypothesis defined in Eq. (5), we define the test statistic T(X) as follows: T(X) = 1 |AX| X i∈AX Xi − 1 |Ac X| X i∈Ac X Xi = η⊤X, (7) where η = 1 |AX|1AX − 1 |Ac X|1Ac X and 1A ∈ Rn is a vector whose i-th element is 1 if i ∈ AX and 0 otherwise. Naive p-values. When the test statistic in Eq. (7) is used for the statistical test in Eq. (5), the p-value can be easily calculated if η does not depend on the image X, i.e., if the abnormal region AX is detected without looking at the X. In this unrealistic situation, the p-value, which we call naive p-value can be computed as pnaive = PH0(|T(X)| ≥ |T(x)|), 8 where X is a random vector and x is the observed image. Under the unrealistic assumption, the pnaive can be easily computed because the null distribution of T(X) = η⊤X is normally distributed with N(0, η⊤Ση). Unfortunately, however, in the actual situation where η depends on X, a statistical test using pnaive is invalid in the sense that PH0(pnaive ≤ α) > α, ∃α ∈ [0, 1]. Namely, the probability of Type I error (an error that a normal region is mistakenly detected as anomaly) cannot be controlled at the desired significance level α. 4 CSI for VAE-based AD To obtain a valid p-value, we employ CSI framework. 4.1 Condtional Selective Inference (CSI) In CSI, p-values are computed based on the null distribution conditional on a event that a certain hypothesis is selected. The goal of CSI is to compute a p-value that satisfies PH0(p ≤ α | AX = A) ≤ α, (8) where the condition part AX = A in Eq. (8) indicates that we only consider images X for which a certain hypothesis (abnormal region) A is detected. If the conditional type I error can be controlled as in Eq. (8) for all possible hypotheses A ∈ 2[n], then, by the law of total probability, the marginal type I error can also be controlled for all α ∈ (0, 1) because PH0(p ≤ α) = X A∈2[n] PH0(A)(p ≤ α | AX = A) ≤ α. Therefore, in order to perform valid statistical test, we can employ p-values conditional on the hy- pothesis selection event. To compute a p-value that satisfies Eq. (8), we need to derive the sampling distribution of the test-statistic T(X)|{AX = Ax}. (9) As stated in §1, CSI has gained attention as a method of statistical inference for feature selection in linear models [Lee et al., 2016]. Most of the existing SI studies exploit the fact that a hypothesis selection event (e.g., the event that a certain set of features is selected by a feature selection algorithm for linear models) can be characterized as the intersection of linear inequalities in the data space, thus allowing the calculation of the sampling distribution of a conditional test statistic such as Eq. (9). On the other hand, the selection event of the VAE-based AD is difficult to characterize in a simple 9 form because it involves a complex computation of VAEs. Our main technical contribution in this study is to generalize the methods in Duy et al. [2022] and Miwa et al. [2023] to establish a method for calculating the sampling distribution in Eq. (9). 4.2 CSI for Piecewise-Assignment Functions We derive the CSI for algorithms expressed in the form of a piecewise-assignment function. Later on, we show that the mapping A : X 7→ AX in Eq. (4) is a piecewise-assignment function, and this will result in the proposed VAE-AD Test. Definition 1 (Piecewise-Assignment Function). Let us consider a function M : Rn ∋ X 7→ MX ∈ M which assigns an image X to a hypothesis among a finite set of hypotheses M. We call the function M a piecewise-assignment function if it is written as MX =                            M1, if X ∈ PM 1 , ... Mk, if X ∈ PM k , ... MKM , if X ∈ PM KM , (10) where PM k , k ∈ [KM], represents a polytope in Rn which can be written as PM k = {X ∈ Rn | ∆M k X′ ≤ δM k } using a certain matrix ∆M k and a vector δM k with appropriate sizes, and KM is the number of polytopes. Here, we note that the same hypothesis may be assigned to different polytopes. When a hypothesis is selected by a piecewise-assignment function in the form of Eq. (10), the following theorem tells that the conditional p-value that satisfies Eq. (8) can be derived by using truncated normal distribution. Theorem 4.1. Consider a random image X and an observed image x. Let MX and Mx be the hypotheses obtained by applying a piecewise-assignment function in the form of Eq. (10) to X and x, respectively. Let η ∈ Rn be a vector depending on Mx, and consider a test statistic in the form of T(X) = η⊤X. Furthermore, define QX = \u0012 In − Σηη⊤ η⊤Ση \u0013 X and Qx = \u0012 In − Σηη⊤ η⊤Ση \u0013 x. Then, the conditional distribution T(X) | {MX = Mx, QX = Qx} 10 is a truncated normal distribution TN(η⊤µ, η⊤Ση; Z) with the mean η⊤µ, the variance η⊤Ση, and the truncation intervals Z. The truncation intervals Z is represented as Z = [ k:Mk=Mx [LM k , U M k ], where, for k ∈ [KM], LM k and U M k are defined as follows: LM k = max j:(βM k )j>0 (αM k )j (βM k )j , U M k = min j:(βM k )j<0 (αM k )j (βM k )j , αM k = δM k − ∆M k Qx, βM k = ∆M k Ση(Ση⊤Ση)−1. The proof of Theorem 4.1 is deferred to Appendix B. Using the sampling distribution of the test statistic T(X) conditional on {MX = Mx, QX = Qx} in Theorem 4.1, we can define the p-value for CSI as pselective (11) = PH0(|T(X)| ≥ |T(x)| | MX = Mx, QX = Qx). The selective p-value pselective defined in Eq. (11) satisfies PH0(pselective ≤ α | MX = Mx) = α, ∀α ∈ [0, 1] because QX is independent of the test statistic T(X) = η⊤X. From the discussion in §4.1, a valid statistical test can be conducted by using pselective in Eq. (11). 4.3 Piecewise-Linear Functions We showed that, if the hypothesis selection algorithm is represented in the form of piecewise-assignment function, we can formulate valid selective p-values. The purpose of this subsection is to set the stage for demonstrating in the next subsection how the entire process of a trained VAE can be depicted as a piecewise-linear function, and how VAE-based AD algorithm in Eq. (4) is represented as a piecewise- assignment function. Definition 2 (Piecewise-Linear Function). A piecewise-linear function f : Rn → Rm is written as: f(X) =                            Ψf 1X + ψf 1 , if X ∈ Pf 1 , ... Ψf kX + ψf k, if X ∈ Pf k , ... Ψf Kf X + ψf Kf , if X ∈ Pf Kf , (12) 11 where Pf k represents a polytope in Rn written as Pf k = {X ∈ Rn | ∆f kX′ ≤ δf k} for k ∈ Kf with a certain matrix ∆f k and a vector δf k with appropriate sizes. Furthermore, Ψf k and ψf k for k ∈ Kf are the k-th linear transformation matrix and the bias vector, respectively, and Kf denotes the number of polytopes of a piecewise-linear function f. Considering piecewise-assignment and piecewise-linear functions, the following properties straight- forwardly hold: • The concatenation of two or more piecewise-linear functions results in a piecewise-linear function. • The composition of two or more piecewise-linear functions results in a piecewise-linear function. • The composition of a piecewise-linear function and a piecewise-assignment function results in a piecewise-assignment function. 4.4 VAE-based AD as Piecewise-Assignment Function In this subsection, we show that the VAE-based AD algorithm in Eq. (4) is a piecewise-assignment function by verifying that i) the reconstruction error in Eq. (1) is a piecewise-linear function, and ii) the thresholding in Eq. (2) is a piecewise-assignment function. Most of basic operations and common activation functions used in the encoder and decoder net- works can be represented as piecewise-linear functions in the form of Eq. (12). For example, the ReLU function is a piecewise-linear function. Operations like matrix-vector multiplication, convolution, and upsampling are linear, which categorizes them as special cases of piecewise-linear functions Further- more, operations like max-pooling and mean-pooling can be represented in the form of Eq. (12). For instance, max-pooling of two variables can be expressed as max{u, v} = u · I(u ≥ v) + v · I(v > u), which is a piecewise-linear function with Kf = 2. Consequently, the encoder and decoder networks of the VAE, composed or concatenated from piecewise-linear functions, form a piecewise-linear function. We note that this characteristic is not exclusive to our VAE; instead, it applies to the majority of CNN-type deep learning models2. Furthermore, the reconstruction error in Eq. (1) is also a piecewise-linear function. Specifically, let fabs be the absolute value function, which is clearly piecewise-linear function, fmm1 be a function for multiplying the matrix \u0010 In, −In \u0011 from the left, and fmm2 be a function for multiplying the matrix \u0010 In, In \u0011⊤ from the left. Then, the reconstruction error Ei(X) = |µθ(µϕ(X)) − X|i is given as the ith element of the following compositions of multiple piecewise-linear functions: fabs ◦ fmm1 ◦ h µθ ◦ µϕ In i ◦ fmm2(X). 2An example of components that do not exhibit piecewise linearity is nonlinear activation function such as the sigmoid function. However, since a one-dimensional nonlinear function can be approximated with high accuracy by a piecewise-linear function with sufficiently many segments, there are no practical problems. 12 The thresholding operation in Eq. (2) is clearly piecewise-assignment function. It means that the operation of detecting abnormal region AX in Eq. (4) is composition of piecewise-linear function and piecewise-assignment function, which results in a piecewise-assignment function. We summarize the aforementioned discussion into the following lemma. Lemma 1. The anomaly detection using VAE defined in Eq. (4), which uses piecewise-linear functions in the encoder and decoder network, is a piecewise-assignment function. Consequently, we can conduct the statistical test in (5) based on the selective p-value in (11) along with Theorem 4.1. 5 Computational Tricks In this section, we demonstrate the procedure for efficiently computing the truncated intervals Z derived from Eq. (4). The identification of Z is challenging because the VAE-based AD is comprised of a substantial number of known piecewise-linear functions and a piecewise-assignment function. There are two difficulties: i) which indices of k whose anomaly region is the same as the observed one, and ii) how to compute each truncated interval [LA k , U A k ]. Our idea is to leverage parametric programming in conjunction with auto-conditioning to efficiently compute Z. Specifically, we can identify only the necessary indices of k and determining their respective intervals [LA k , U A k ]. This enables us to bypass the unneeded computation of unnecessary components, thus saving computational time. 5.1 Parametric Programming In the Theorem 4.1, the truncated intervals Z can be regarded as the intersections of the polytopes {P A k }k:Ak=Ax with the line X = Qx + Ση(η⊤Ση)−1Z. This implies that determining the truncated intervals Z is accomplished by examining this specific line rather than the entire space. Alogorithm 1 outlines the procedure to identify Z. The algorithm starts at zmin and search for the truncated intervals along the line until zmax3. For each step, given z, the algorithm computes the lower bound LA k and upper bound U A k of the interval to which z belongs to, as well as corresponding anomaly region Ak = AX(z). The LA k and U A k are computed by the technique described in the next subsection. This procedure is commonly referred to as parametric programming (e.g., lasso regularization path). 3We set the zmin = −|T(x)| − 10σ and zmax = |T(x)| + 10σ, where σ is the standard deviation of test statistic. This is justified by the fact that the probability in the tails of the normal distribution can be considered negligible. 13 Algorithm 1 Parametric Programming-Based SI Require: x, zmin, zmax 1: Obtaine Ax and compute η. 2: z ← zmin and Z ← ∅ 3: while z ≤ zmax do 4: Compute LA k , UA k , and Ak respect to z by auto-conditioning (see Appendix C). 5: if Ak = Ax then 6: Z ← Z ∪ [LA k , UA k ] 7: end if 8: z ← UA k + δ, where δ is a small positive number. 9: end while 10: pselective ← (11) with Threorem 4.1 output pselective and Ax 5.2 Auto-Conditioning In line 4 of Algorithm 1, we utilize a technique referred to as auto-conditioning. Similar to auto- differentiation, this method leverages the fact that the entire computations of LA k and U A k executes a sequence of piece-wise linear operations. By applying the recursive rule repeatedly to these operations, LA k and U A k can be automatically computed. The details are deferred to Appendix C. This implies that by implementing the computational techniques for known piecewise-linear/assignment functions, we can automatically compute the truncation intervals and the anomaly region. This adaptability proves particularly advantageous when dealing with complex systems like Deep Neural Networks (DNNs), where frequent and detailed structural adjustments are often required. We note that the auto-conditioning technique is originally proposed in Miwa et al. [2023]. However, the authors concentrate on a specific application of the saliency region, and no existing studies recog- nize its crucial application in VAE literature. In this paper, we prove that a VAE can be represented as a piecewise-assignment function, thus highlighting the crucial application of auto-conditioning in efficiently conducting the proposed VAE-AD Test. 6 Experiment We demonstrate the performance of the proposed method. Experimental Setup. We compared the proposed method (VAE-AD Test) with OC (simple extension of SI literature to our setting), Bonferroni correction (Bonf) and naive method. More details can be found in Appendix D. We considered two covariance matrix structures: • Σ = In (Independence) 14 64 256 1024 4096 n 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Type I Error Rate VAE-AD Test OC Bonf Naive (a) Independence 64 256 1024 4096 n 0.0 0.2 0.4 0.6 0.8 1.0 Type I Error Rate VAE-AD Test OC Bonf Naive (b) Correlation Figure 2: Type I error rate comparison 1 2 3 4 0.0 0.2 0.4 0.6 0.8 1.0 Power VAE-AD Test OC Bonf (a) Independence 1 2 3 4 0.0 0.2 0.4 0.6 0.8 1.0 Power VAE-AD Test OC Bonf (b) Correlation Figure 3: Power comparison • Σ = AR(1)⊗AR(1) (Correlation) where AR(1) is the first-order autoregressive matrix {AR(1)}ij ∈ R √n×√n = 0.25|i−j| and ⊗ is kronecker dot. To examine the type I error rate, we generated 1000 null images X = (X1, . . . , Xn), where s = 0 and ϵ ∼ N(0, Σ), for each n ∈ {64, 256, 1024, 4096}. To examine the power, we set n = 256 and generated 1000 images in which ϵ ∼ N(0, Σ), the signals si = ∆ for any i /∈ S where S is the ”true” anomaly region whose location is randomly determined, and si = 0 for any i /∈ S. We set ∆ ∈ {1, 2, 3, 4}. In all experiments, we set the threshold λ = 1.2 for the anomaly detection, and the significance level α = 0.05. We also apply mean filtering to the reconstruction error to enhance the anomaly detection performance. Numerical results. The results of type I error rate and power are shown in Figs. 2 and 3, 15 0.01 0.02 0.03 0.04 WS Distance 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Type I Error t gennormflat gennormsteep exponnorm skewnorm (a) α = 0.05. 0.01 0.02 0.03 0.04 WS Distance 0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 Type I Error t gennormflat gennormsteep exponnorm skewnorm (b) α = 0.10. Figure 4: Robustness of the proposed method. respectively. The VAE-AD Test, OC, and Bonf successfully controlled the type I error rate in the both cases of independence and correlation, whereas the naive method could not. Since the naive method failed to control the type I error, we no longer considered its power. The power of the VAE- AD Test was the highest among the methods that controlled the type I error. The Bonferroni method has the lowest power because it is conservative due to considering the huge number of all possible hypotheses. OC also has low power because it considers extra conditioning, which causes the loss of power. Additionally, we confirm the robustness of the VAE-AD Test in terms of type I error control when the noise follows skew normal distribution, exponential normal distribution, generalized normal distribution with flat tails, and t distribution. More details can be found in Appendix D. The results are shown in Fig. 4. Our method still maintains good performance in type I error rate control. Real data experiments. We examined the brain image dataset extracted from Buda et al. [2019], which includes 939 and 941 images with and without tumors, respectively. The results of statistical testing for images without tumor and with tumor are presented in Figs. 5 and 6. The naive p-value is small even in cases where no tumor region exists in the image. This indicates that the naive p-value cannot be used to quantify the reliability of the result of anomaly detection using VAE. With the proposed selective p-values, we successfully identified false and true positive detections. 7 Conclusion We proposed a novel setup of testing the results of VAE-AD and introduced a method to compute a valid p-value for conducting the statistical test. The experiments were conducted on both synthetic 16 Input Image Reconstruction Reconstruction Error Anomaly Region (a) pnaive = 0.000, pselective = 0.431 Input Image Reconstruction Reconstruction Error Anomaly Region (b) pnaive = 0.000, pselective = 0.849 Input Image Reconstruction Reconstruction Error Anomaly Region (c) pnaive = 0.000, pselective = 0.196 Input Image Reconstruction Reconstruction Error Anomaly Region (d) pnaive = 0.000, pselective = 0.797 Input Image Reconstruction Reconstruction Error Anomaly Region (e) pnaive = 0.000, pselective = 0.299 Input Image Reconstruction Reconstruction Error Anomaly Region (f) pnaive = 0.000, pselective = 0.598 Figure 5: Anomaly detection for images without tumor. Input Image Reconstruction Reconstruction Error Anomaly Region (a) pnaive = 0.000, pselective = 0.048 Input Image Reconstruction Reconstruction Error Anomaly Region (b) pnaive = 0.000, pselective = 0.000 Input Image Reconstruction Reconstruction Error Anomaly Region (c) pnaive = 0.000, pselective = 0.017 Input Image Reconstruction Reconstruction Error Anomaly Region (d) pnaive = 0.000, pselective = 0.000 Input Image Reconstruction Reconstruction Error Anomaly Region (e) pnaive = 0.000, pselective = 0.000 Input Image Reconstruction Reconstruction Error Anomaly Region (f) pnaive = 0.000, pselective = 0.000 Figure 6: Anormaly detection for images with tumor. 17 and real-world datasets to showcase the superior performance of the proposed method. We believe that this study stands as a significant step toward reliability of VAE-driven hypotheses. Acknowledgements This work was partially supported by MEXT KAKENHI (20H00601), JST CREST (JPMJCR21D3), JST Moonshot R&D (JPMJMS2033-05), JST AIP Acceleration Research (JPMJCR21U2), NEDO (JPNP18002, JPNP20006), and RIKEN Center for Advanced Intelligence Project. References Anonymous. Statistical test for generated hypotheses by diffusion models. Unpublished, 2024a. Anonymous. Statistical test for attention maps in vision transformers. Unpublished, 2024b. C. Baur, B. Wiestler, S. Albarqouni, and N. Navab. Deep Autoencoding Models for Unsupervised Anomaly Segmentation in Brain MR Images, page 161–169. Springer International Publishing, 2019. ISBN 9783030117238. doi: 10.1007/978-3-030-11723-8 16. URL http://dx.doi.org/10. 1007/978-3-030-11723-8_16. C. Baur, S. Denner, B. Wiestler, N. Navab, and S. Albarqouni. Autoencoders for unsupervised anomaly segmentation in brain mr images: A comparative study. Medical Image Analysis, 69: 101952, 2021. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2020.101952. URL https: //www.sciencedirect.com/science/article/pii/S1361841520303169. L. Breiman. The little bootstrap and other methods for dimensionality selection in regression: X-fixed prediction error. Journal of the American Statistical Association, 87(419):738–754, 1992. M. Buda, A. Saha, and M. A. Mazurowski. Association of genomic subtypes of lower-grade gliomas with shape features automatically extracted by a deep learning algorithm. Computers in Biology and Medicine, 109:218–225, 2019. ISSN 0010-4825. doi: https://doi.org/10.1016/j.compbiomed.2019. 05.002. URL https://www.sciencedirect.com/science/article/pii/S0010482519301520. R. Chalapathy and S. Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019. S. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and Graphical Statistics, pages 1–12, 2019. 18 S. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and Graphical Statistics, 29(2):323–334, 2020. X. Chen and E. Konukoglu. Unsupervised detection of lesions in brain MRI using constrained adver- sarial auto-encoders. In Medical Imaging with Deep Learning, 2018. URL https://openreview. net/forum?id=H1nGLZ2oG. Y. Choi, J. Taylor, and R. Tibshirani. Selecting the number of principal components: Estimation of the true rank of a noisy matrix. The Annals of Statistics, 45(6):2590–2617, 2017. J. K. Chow, Z. Su, J. Wu, P. S. Tan, X. Mao, and Y.-H. Wang. Anomaly detection of defects on concrete structures with the convolutional autoencoder. Advanced Engineering Informatics, 45: 101105, 2020. D. Dehaene, O. Frigo, S. Combrexelle, and P. Eline. Iterative energy-based projection on a normal data manifold for anomaly localization, 2020. V. N. L. Duy, H. Toda, R. Sugiyama, and I. Takeuchi. Computing valid p-value for optimal changepoint by selective inference using dynamic programming. In Advances in Neural Information Processing Systems, 2020. V. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quantifying statistical significance of neural network-based image segmentation by selective inference. Advances in Neural Information Processing Systems, 35: 31627–31639, 2022. W. Fithian, J. Taylor, R. Tibshirani, and R. Tibshirani. Selective sequential model selection. arXiv preprint arXiv:1512.02565, 2015. L. L. Gao, J. Bien, and D. Witten. Selective inference for hierarchical clustering. Journal of the American Statistical Association, pages 1–11, 2022. D. Jana, J. Patil, S. Herkal, S. Nagarajaiah, and L. Duenas-Osorio. Cnn and convolutional autoencoder (cae) based real-time sensor fault detection, localization, and correction. Mechanical Systems and Signal Processing, 169:108723, 2022. D. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017. D. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. 19 V. N. Le Duy and I. Takeuchi. Parametric programming approach for more powerful and general lasso selective inference. In International conference on artificial intelligence and statistics, pages 901–909. PMLR, 2021. J. D. Lee, Y. Sun, and J. E. Taylor. Evaluating the statistical significance of biclusters. Advances in neural information processing systems, 28, 2015. J. D. Lee, D. L. Sun, Y. Sun, and J. E. Taylor. Exact post-selection inference, with application to the lasso. The Annals of Statistics, 44(3):907–927, 2016. J. R. Loftus and J. E. Taylor. A significance test for forward stepwise model selection. arXiv preprint arXiv:1405.3920, 2014. Y. Lu and P. Xu. Anomaly detection for skin disease images using variational autoencoder. arXiv preprint arXiv:1807.01349, 2018. D. Miwa, D. V. N. Le, and I. Takeuchi. Valid p-value for deep learning-driven salient region. In Proceedings of the 11th International Conference on Learning Representation, 2023. G. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: A review. ACM computing surveys (CSUR), 54(2):1–38, 2021. K. Sugiyama, V. N. Le Duy, and I. Takeuchi. More powerful and general selective inference for stepwise feature selection using homotopy method. In International Conference on Machine Learning, pages 9891–9901. PMLR, 2021. S. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and I. Takeuchi. Selective inference for sparse high-order interaction models. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 3338–3347. JMLR. org, 2017. K. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani, and I. Takeuchi. Computing valid p-values for image segmentation by selective inference. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9553–9562, 2020. X. Tao, X. Gong, X. Zhang, S. Yan, and C. Adak. Deep learning for unsupervised anomaly localization in industrial images: A survey. IEEE Transactions on Instrumentation and Measurement, 71:1–21, 2022. ISSN 1557-9662. doi: 10.1109/tim.2022.3196436. URL http://dx.doi.org/10.1109/TIM. 2022.3196436. J. Taylor and R. J. Tibshirani. Statistical learning and selective inference. Proceedings of the National Academy of Sciences, 112(25):7629–7634, 2015. 20 R. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tibshirani. Exact post-selection inference for se- quential regression procedures. Journal of the American Statistical Association, 111(514):600–620, 2016. X. Wang, Y. Du, S. Lin, P. Cui, Y. Shen, and Y. Yang. advae: A self-adversarial variational au- toencoder with gaussian anomaly prior knowledge for anomaly detection. Knowledge-Based Sys- tems, 190:105187, 2020. ISSN 0950-7051. doi: https://doi.org/10.1016/j.knosys.2019.105187. URL https://www.sciencedirect.com/science/article/pii/S0950705119305283. D. Zimmerer, F. Isensee, J. Petersen, S. Kohl, and K. Maier-Hein. Unsupervised anomaly localization using variational auto-encoders, 2019. A The details of VAE We used the architecture of the VAE as shown in Figure 7 and set m = 10 as a dimensionality of the latent space. We used ReLU as an activation function for the encoder and decoder. We generated 1000 images from N(0, In) as normal images and trained the VAE with these images, and used Adam [Kingma and Ba, 2017] as an optimizer. 1 √n x 4 4 √n Conv MaxPool 8 8 √n/2 Conv+ReLU 1 2n Reshape 1 m Dense (µϕ) 1 m Dense (σϕ) 1 m z ∼ N(µϕ, σ2 ϕ) 1 2n Dense Reshape 8 8 √n/2 Conv+ReLU UpSample 4 √n Conv 1 √n Conv 1 √n µθ Figure 7: Architecture of the VAE. B Proof of Theorem 4.1 Proof. The theorem is based on the Lemma 3.1 in Chen and Bien [2019]. By the definition of the piecewise-assignment function, the conditional part, {MX = Mx} can be characterized as the union of polytopes, {MX = Mx} = [ k:Mk=Mx P M k . 21 By substituting X(Z) = Qx + Ση(η⊤Ση)−1Z into the polytopes P M k , we obtain the truncated intervals Z in the lemma. For the set k such that Mk = Mx, we have QX ⊥ Z by orhtogonality of QX and η and by the properties of the normal distribution. Hence, we obtain Z | {MX = Mx, QX = Qx} d= Z | {Z ∈ Z, QX = Qx} d= Z | {Z ∈ Z} (∵ QX ⊥ Z) There is no randomness in Z, Z | {MX = Mx, QX = Qx} ∼ TN(η⊤µ, η⊤Ση; Z). ■ C The Details of auto-conditioning This section demonstrates the auto-conditioning algorithm, utilized to compute the truncated intervals [LA k , U A k ] and the corresponding anomaly region Ak respect to the z in Algorithm 1. The algorithm is introduced for the piecewise-assignment function, which is composed of piecewise-linear functions and a piecewise-assignment function. It is conceptualized as a directed acyclic graph (DAG) that delineates the processing of input data, similar to a computational graph in auto-differentiation. In this graph, the nodes symbolize the piecewise-linear and piecewise-assignment functions, each with an input and output edge to represent the function compositions. It should be noted that the node such as µϕ and µθ, may replace the other DAG express the piecewise-linear/assignment function of the node since it can be represented as the composition and concatenation of array of simpler piecewise-linear/assignment functions. The level of simplicity for a function of a node can be determined based on what is most convenient for the implementation. A special node, representing the concatenation of two piecewise-linear functions, features two input edges and one output edge. Figure 8 shows the directed acyclic graph of the anomaly detection using VAE in Eq. (4). C.1 Update rules for the nodes of the piecewise-assignment functions The computation of the interval [LA k , LA k ] is defined in a recursive way. The output of the node f : Rl → Rm in the DAG are denoted as af, bf ∈ Rm and Lf, Uf ∈ R. Update rule for the initial node. At first, the output of the initial node X(z) of the directional graph denoted as f0 for notational convention, are defined as af0 = Qx, bf0 = Ση(η⊤Ση)−1, Lf0 = 22 X(z) fmm2 µθ In µϕ concat fmm1 fabs Θ AX(z) Figure 8: The directed acyclic graph of the anomaly detection using VAE A : Rn → 2|n| defined in Eq. (4). Circles represent the piecewise-linear functions and the piecewise-assignment function. The rectangle represents the concatenation of piecewise-linear functions. The edges represent the composition of piecewise-linear functions. −∞, and Uf0 = ∞. It should be noted here that X(z) = af0 + bf0z is the line appeared in the proof of Theorem 4.1 in Section B. Update rule for the node of the piecewise-linear functions. Let us consider the output for the node g whose input is the output of the node f in the DAG. The inputs of the g’s node (i.e. output of node f) are denoted as af, bf, Lf and Uf. af is the summed point vector added in the piecewise-linear functions until reaching f, bf is the direction vector corresponding to z, multiplied in the piecewise-linear functions until reaching to f. Then, the output of the piecewise-linear function f is represented as af + bfz. Lf and Uf are the lower and upper bounds of the interval obtained at the piecewise-linear function f. The output of the node g is defined as follows: 1) Check the index j such that the output of f within the polytope of: P g j ∋ af + bfz. 2) Compute the point vector ag and the direction vector bg of the piecewise-linear function g with the index j, ag = Ψg jaf + ψg j , bg = Ψg jbf. (13) 3) Compute the lower and upper bounds of the interval Lg and Ug with the index j, L = max k:(βg j )k>0 (αg j)k (βg j )k , U = min k:(βg j )k<0 (αg j)k (βg j )k , where αg j = δg j − ∆g jaf and βf j = ∆g jbf. 4) Take the intersection of the interval [Lf, Uf] ∩ [L, U] as the interval [Lg, Ug] of the piecewise-assignment function g as Lg = max(Lf, L), Ug = min(Uf, U). This update rule is obtained from the Lemma 2 in Miwa et al. [2023]. 23 Update rule for the nodes of concatenation of two piecewise-linear functions. Let us consider the concatenation node of two piecewise-linear functions f and g denoted as concat. Let the inputs of the node be af, bf, Lf and Uf from the node f and ag, bg, Lg and Ug from the node g. The output of the concatenation node, aconcat, bconcat, Lconcat and Uconcat are defined as follows: 1) Concatenate the vector outputs of nodes f and g aconcat =  af ag   , bconcat =  bf bg   . 2) Take intersection of the interval [Lf, Uf] ∩ [Lg, Ug] as Lconcat = max(Lf, Lg), Uconcat = min(Uf, Ug). Update rule for the final node. At the final node Θ which is the piecewise-assignment function, it takes the same input as the node of piecewise-linear functions and outputs are the same except for the aΘ and bΘ. 1) It computes the index j such that the input falls into the polypotopes of P Θ j . 2) Then, the anomaly region Aj is obtained instead of Eq. (13) in the update rule for the node of piecewise-linear functions. 3) The computation of lower bounds LΘ and the upper bounds UΘ are the same as the update rule for the node of piecewise-linear functions. The output of the final node are the anomaly region Aj, the lower bounds LΘ and the upper bounds UΘ. Then, apply the above update rule to the directional graph of the piecewise-assignment function from the initial node f0 to the final node Θ. Consequently, the auto-conditioning algorithm computes the lower and upper bounds of the interval as the outputs of final node LA k = LΘ, LA k = LΘ and Ak = Aj. C.2 Implementation We implemented the auto-conditioning algorithm described above in Python using the tensorflow library. The codes construct the DAG of the piecewise-assignment function automatically from the trained Keras/tensorflow model. Then, we do not need further implementation to conduct CSI for each specific DNN model. This indicates that even if we change the architecture or adjust the hyper- parameters and retrain the DNN models, we can conduct the CSI without additional implementation. D Experimental Details Methods for comparison. We compared our proposed method with the following methods: • VAE-AD Test: our proposed method. 24 • OC: our proposed method conditioning on the only one polytope to which the observed image belongs x ∈ P A k . This method is computationally efficient; however, its power is low due to over- conditioning. • Bonf: the number of all possible hypotheses are considered to account for the selection bias. The p-value is computed by pbonf = min(1, pnaive × 2n) • Naive: the conventional method is used to compute the p-value. Experiment for robustness. We evaluate the robustness of our proposed methodology in terms of Type I error control, specifically under conditions where the noise distribution deviates from the Gaussian assumption. We investigate this robustness by applying our method across a range of non- Gaussian noise distributions, including: • Skew normal distribution (skewnorm) • Exponential normal distribution (exponorm) • Generalized normal distribution with steep tails (gennormsteep) • Generalized normal distribution with flat tails (gennomflat) • Student’s t distribution (t) We commence our analysis by identifying noise distributions from the aforementioned list that have a 1-Wasserstein distance of {0.01, 0.02, 0.03, 0.04} relative to the standard normal distribution N(0, 1). Subsequently, we standardize these noise distributions to ensure a mean of 0 and a variance of 1. Setting the sample size to n = 256, we generate 1000 samples from the selected distributions and apply hypothesis testing to each sample to obtain the Type I error rate. This process is conducted at significance levels α = {0.05, 0.10}. The results are shown in Figure 4. Our method still maintains good performance even when the assumption of Gaussian noise is violated. More results on brain image dataset. Additional results are shown in Figs. 9 and 10. 25 Input Image Reconstruction Reconstruction Error Anomaly Region (a) pnaive = 0.000, pselective = 0.668 Input Image Reconstruction Reconstruction Error Anomaly Region (b) pnaive = 0.000, pselective = 0.849 Input Image Reconstruction Reconstruction Error Anomaly Region (c) pnaive = 0.011, pselective = 0.500 Input Image Reconstruction Reconstruction Error Anomaly Region (d) pnaive = 0.012, pselective = 0.137 Figure 9: Anomaly detection for image without tumor. 26 Input Image Reconstruction Reconstruction Error Anomaly Region (a) pnaive = 0.000, pselective = 0.001 Input Image Reconstruction Reconstruction Error Anomaly Region (b) pnaive = 0.000, pselective = 0.000 Input Image Reconstruction Reconstruction Error Anomaly Region (c) pnaive = 0.000, pselective = 0.000 Input Image Reconstruction Reconstruction Error Anomaly Region (d) pnaive = 0.000, pselective = 0.017 Figure 10: Anormaly detection for image with tumor. 27 "
}