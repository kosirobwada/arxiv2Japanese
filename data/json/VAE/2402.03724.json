{
    "optim": "Statistical Test for Anomaly Detections by Variational\nAuto-Encoders\nDaiki Miwa\nNagoya Institute of Technology\nmiwa.daiki.mllab.nit@gmail.com\nTomohiro Shiraishi\nNagoya University\nshiraishi.tomohiro.nagoyaml@gmail.com\nVo Nguyen Le Duy\nUniversity of Information Technology, Ho Chi Minh City, Vietnam\nVietnam National University, Ho Chi Minh City, Vietnam\nRIKEN\nduyvnl@uit.edu.vn\nTeruyuki Katsuoka\nNagoya University\nkatsuoka.teruyuki.nagoyaml@gmail.com\nIchiro Takeuchi∗\nNagoya University\nRIKEN\nichiro.takeuchi@mae.nagoya-u.ac.jp\nFebruary 7, 2024\nAbstract\nIn this study, we consider the reliability assessment of anomaly detection (AD) using Vari-\national Autoencoder (VAE). Over the last decade, VAE-based AD has been actively studied in\nvarious perspective, from method development to applied research. However, when the results\nof ADs are used in high-stakes decision-making, such as in medical diagnosis, it is necessary to\nensure the reliability of the detected anomalies. In this study, we propose the VAE-AD Test as\na method for quantifying the statistical reliability of VAE-based AD within the framework of\nstatistical testing. Using the VAE-AD Test, the reliability of the anomaly regions detected by a\nVAE can be quantified in the form of p-values. This means that if an anomaly is declared when\nthe p-value is below a certain threshold, it is possible to control the probability of false detection\n∗Corresponding author\n1\narXiv:2402.03724v1  [stat.ML]  6 Feb 2024\nto a desired level. Since the VAE-AD Test is constructed based on a new statistical inference\nframework called selective inference, its validity is theoretically guaranteed in finite samples. To\ndemonstrate the validity and effectiveness of the proposed VAE-AD Test, numerical experiments\non artificial data and applications to brain image analysis are conducted.\n2\n1\nIntroduction\nAnomaly detection (AD) is the process of identifying unusual deviations in data that do not conform to\nexpected behavior. AD is crucial across various domains because it provides early warnings of potential\nissues, thereby enabling timely interventions to prevent critical events. Traditional AD techniques,\nwhile effective in straightforward scenarios, frequently fall short when dealing with complex data,\nthus motivating the use of deep learning-based AD approaches to better handle such complexities. In\nthis study, among various deep learning-based AD approaches, we focus on AD using the Variational\nAuto-Encoder (VAE), and its application to medical images.\nA VAE serves as an effective tool for AD task. In the training phase, the VAE learns the dis-\ntribution of normal images by training exclusively on images that do not contain abnormal regions.\nIt encodes the input image into a latent space and then reconstructs it back to its original form.\nThe parameters of a VAE are optimized to minimize the reconstruction error, thereby learning a\ncompressed representation of the normal data. In the test phase, when a test image is fed into the\ntrained VAE, the model attempts to reconstruct the image based on its learned representation. The\nreconstruction error is then calculated, typically as the difference between the input image and its\nreconstructed version. Since the VAE is trained on normal data, it would successfully reconstruct the\nnormal regions of the image, while it would fail to properly reconstruct the abnormal regions that\nwere not included in the normal data. Therefore, regions with large reconstruction errors are detected\nas abnormal regions. Figure 1 shows an example of VAE-based AD for brain tumor images.\nWhen VAE-based AD is employed for high-stakes decision-making tasks, such as medical diagnosis,\nthere is a significant risk that model inaccuracies might lead to critical errors, potentially resulting\nin false detections. To address this issue, in this study, we develop a statistical test for VAE-based\nAD, which we call VAE-AD Test. The proposed VAE-AD test enables us to obtain a quantifiable\nand interpretable measure for the detected anomaly region in the form of p-value. The obtained p-\nvalue represents the probability that the detected anomaly regions are obtained by chance due to the\nrandomness contained in the data. For example, if we adopt a rule that classifies detected anomaly\nregions with a p-value less than 0.05 as anomalies, we can keep the false detection probability below\n5% rigorously.\nIt is important to note that the statistical test for detected abnormal regions is performed on data-\ndriven hypothesis, as the abnormal region is selected based on the test image itself. In other words,\nsince both of the selection of the hypothesis (selection of abnormal regions) and the evaluation of\nthe hypothesis (evaluation of abnormal regions) are performed on the same data, applying traditional\nstatistical test to the selected hypothesis (abnormal regions) leads to selection bias. Therefore, in this\nstudy, we introduce the framework of Conditional Selective Inference (CSI), which has been recognized\n3\nas a new statistical testing framework for data-driven hypotheses. CSI has primarily been studied in\nthe context of feature selection for linear models, but in recent years, it has been extended to various\nproblems in various models (see the related works section for more details).\nThe main contributions in this study are the following three points. Our first contribution is the\nformulation of an approach that provides a quantifiable and interpretable measure for the reliability\nof anomalies detected by VAE in the form of a p-value based on a statistical test. With this approach,\nwe can properly control the probability of false detection probability to the desired significance level\nin finite samples.\nThe second contribution is the development of an SI method for VAEs.\nSI is\nformulated as a conditional test based on the conditions under which hypotheses are selected (see §3\nfor details), requiring computations concerning the hypothesis selection event by a VAE. Our main\ntechnical contribution is to establish a method to compute the hypothesis selection event for a wide\nclass of VAEs. Finally, the third contribution is to demonstrate the effectiveness of the proposed\nVAE-AD Test by conducting numerical experiments with synthetic data and brain tumor images.\nThe code is available at https://github.com/DaikiMiwa/VAE-AD-Test.\n1.1\nRelated Work\nOver the last decade, there has been a significant pursuit in applying deep learning techniques to AD\nproblems [Chalapathy and Chawla, 2019, Pang et al., 2021, Tao et al., 2022]. A large number of studies\nhave been conducted for unsupervised AD using VAEs [Baur et al., 2021, Chen and Konukoglu, 2018,\nChow et al., 2020, Jana et al., 2022]. VAE-based AD for images is broadly divided into two tasks.\nThe first task is to determine whether the entire input image is normal or abnormal. The second\ntask is to detect abnormal regions within the input image, which is sometimes referred to as anomaly\nlocalization [Zimmerer et al., 2019, Lu and Xu, 2018, Baur et al., 2019]. In this study, we focus on the\nsecond task. There are mainly two research directions for improving VAEs for AD. The first direction\nis on improving the indicators of the degree of anomaly [Zimmerer et al., 2019, Dehaene et al., 2020].\nThe second direction is on modifying the VAE itself to make it suitable for AD [Baur et al., 2019,\nChen and Konukoglu, 2018, Wang et al., 2020]. However, to our knowledge, there has been no existing\nstudies for quantifying the statistical reliability of detected abnormal regions with theoretical validity.\nIn traditional statistical tests, the hypothesis needs to be predetermined and must remain inde-\npendent of the data. However, in data-driven approaches, it is necessary to select hypotheses based on\nthe data and then assess the reliability of the hypotheses using the same data. This issue, known as\ndouble dipping, arises because the same data is used for both the selection and evaluation of hypothe-\nses, leading to selection bias [Breiman, 1992]. Because anomalies are detected based on data (a test\nimage) in AD, when evaluating the reliability of the detected anomalies using the same data, the issue\n4\nー\nInput Image\nReconstruction Image\nReconstruction Error\nAnomaly Region\nVAE\n(a) Image without tumor region. pnaive = 0.000 (false detection) and pselective = 0.668 (true negative).\nー\nInput Image\nReconstruction Image\nReconstruction Error\nAnomaly Region\nVAE\n(b) Image with tumor region. pnaive = 0.000 (true detection) and pselective = 0.000 (true detection).\nFigure 1: An illustration of the proposed VAE-AD Test in brain image analysis. When an anomaly\nregion is detected based on the difference between the original image and the reconstructed image\nby a VAE, the VAE-AD Test provides a p-value to quantify its statistical reliability. The upper plot\nshows the results of applying the VAE-AD Test and the conventional method, which does not consider\nthe fact that the anomaly region is detected by VAE, to a case without anomaly regions. The lower\nplot shows the results for a case with anomaly regions. With the proposed method (pselective), correct\njudgments are made in both cases; the former has a large p-value and the latter has a small p-value.\nIn contrast, with the conventional method (pnaive), both p-values are small, indicating false detection\nin the former case.\nof selection bias arises. CSI has recently gained attention as a framework for statistical hypothesis\ntesting of data-driven hypotheses Lee et al. [2016], Taylor and Tibshirani [2015]. The key idea of CSI\nis to address the selection bias issue by considering a conditional test. CSI was initially developed\nfor the statistical inference of feature selection in linear models [Fithian et al., 2015, Tibshirani et al.,\n2016, Loftus and Taylor, 2014, Suzumura et al., 2017, Le Duy and Takeuchi, 2021, Sugiyama et al.,\n2021], and then extended to various problems [Lee et al., 2015, Choi et al., 2017, Chen and Bien,\n2020, Tanizaki et al., 2020, Duy et al., 2020, Gao et al., 2022]. CSI was first introduced to deep learn-\ning context in Duy et al. [2022] and Miwa et al. [2023], with the former focusing on evaluating the\nreliability of segmentation and the latter aiming to assess the reliability of the saliency map. In this\npaper, we develop CSI for VAEs based on these studies and establish a framework for quantitatively\nevaluating the reliability of VAE-based AD1.\n1A part of the authors of this paper are conducting research on CSI for Vision Transformers, aimed at quantifying the\n5\n2\nAnomaly Detection (AD) by VAE\nIn this section, we present how VAEs are used for AD.\n2.1\nVariational Autoencoder (VAE)\nVAEs are generative models originally proposed by Kingma and Welling [2013]. A VAE consists of an\nencoder network and a decoder network. Given an input image (denoted by x ∈ Rn), it is encoded as\na latent vector (denoted by z ∈ Rm), and the latent vector is decoded back to the input image, where\nn is the number of pixels of an image and m is the dimension of a latent vector. In the generative\nprocess, it is assumed that a latent vector z is sampled from a prior distribution pθ∗(z) and then,\nimage x is sampled from a conditional distribution pθ∗(x|z). The prior distribution pθ∗(z) and the\nconditional distribution pθ∗(x|z) belongs to family of distributions parametrized by θ and θ∗ denotes\nthe true value of the parameter.\nSince the posterior distribution pθ(z|x) is assumed to be intractable, the encoder network ap-\nproximates it by the parametric distribution qϕ(z|x), where ϕ represents the set of parameters. The\ndecoder network estimate the conditional distribution by pθ(x|z).\nThe encoder and the decoder\nnetworks of a VAE are trained by maximizing so-called evidence lower bound (ELBO):\nLθ,ϕ = Eqθ(z|x) [log pϕ(x|z)] − KL [qθ(z|x)||p(z)] ,\nwhere KL [·||·] is the Kullback-Leibler divergence between two distributions.\nIn this study, we model the approximated posterior distribution qϕ(z|x) as a normal distribution,\nrepresented by N(µϕ(x), Inσ2\nϕ(x)), where µϕ(x) and σ2\nϕ(x) are the outputs of the encoder network,\nand the condiional distribution pθ(x|z) as a normal distribution, represented by N(µθ(z), In), where\nµθ(z) is the outputs of the decoder network. The prior distribution pθ∗(z) is modeld as a standard\nnormal distribution N(0, Im). The structure of the VAE used in this study is shown in Appendix A.\n2.2\nAnomaly Detection Using VAEs\nVAEs can be effectively used for AD task. As outlined in §1, AD tasks can be categorized into two\ntypes: anomaly detection (in a narrow sense) and anomaly localization. This paper concentrates on\nanomaly localization whose goal is to identify the abnormal region within a given test image.\nIn the training phase, we assume that only normal images (e.g., brain images without tumors) are\navailable. A VAE is trained on normal images to learn a compact representation of the normal image\ndistribution in the latent space.\nreliability of attention maps [Anonymous, 2024b]. Additionally, there is another ongoing research on CSI for diffusion\nmodels, with the objective of extending CSI to generative models [Anonymous, 2024a].\n6\nIn the test phase, a test image x is fed into the trained VAE, and a reconstructed image is obtained\nby using the encoder and the decoder as ˆx = µθ (µϕ(x)). Since the VAE is trained only on normal\nimages, normal region in the test image would be reconstructed well, whereas the reconstruction error\nof abnormal regions would be high. Therefore, it is reasonable to define the degree of anomaly of each\npixel as\nEi(x) = |xi − ˆxi|, i ∈ [n],\n(1)\nwhere xi and ˆxi is the ith pixel value of x and ˆx, respectively. Using a user-specified threshold λ > 0,\nthe anomaly region of a test image x is defined as\nAx = {i ∈ |n| | Ei(x) ≥ λ}.\n(2)\nAs for the definition of the anomaly region, there are possibilities other than those given by Eqs. (1)\nand (2). In this paper, we proceed with these choices, but the proposed VAE-AD Test is generally\napplicable to other choices.\n3\nStatistical Test for Abnormal Regions\nIn this section, we formulate a statistical test for abnormal region A in Eq. (2).\nStatistical model of an image.\nTo formulate the reliability assessment of the abnormal region\nas a statistical testing problem, it is necessary to introduce a statistical model of an image. In this\nstudy, an image is considered as a sum of true signal component s ∈ Rn and noise component ϵ ∈ Rn.\nRegarding the true signal component, each pixel can have an arbitrary true signal value without any\nparticular assumption or constraint. On the other hand, regarding the noise component, it is assumed\nto follow a normal distribution, and their covariance matrix is estimated using normal data different\nfrom that used for the training of the VAE. Namely, an image with n pixels can be represented as an\nn-dimensional random vector\nX = (X1, . . . , Xn) = s + ϵ, ϵ ∼ N(0, Σ),\n(3)\nwhere s ∈ Rn is the true signal vectors, and ϵ ∈ Rn is the noise vector with covariance matrix Σ. In\nthe following, the capital X denotes an image as a random vector, while the lowercase x represents an\nobserved image. To formulate the statistical test, we consider the AD using VAEs defined in Eq. (2)\nas a function A that maps a random input image X to the abnormal region AX, i.e.,\nA : Rn ∋ X 7→ AX ∈ 2[n],\n(4)\nwhere 2[n] is the power set of [n] := {1, 2, . . . , n}.\n7\nFormulation of statistical test.\nOur goal is to make a judgment whether the abnormal region\nAX merely appears abnormal due to the influence of random noise, or if there is a true anomaly in\nthe true signal in the abnormal region. In order to quantify the reliability of the detected abnormal\nregion, the statistical test is performed for the difference between the true signal in the abnormal\nregion {si}i∈AX and the true signal in the normal region {si}i∈Ac\nX where Ac\nX is the complement of\nthe abnormal region. In this study, as an example, we consider the hypothesis for the difference in\ntrue mean signals between AX and Ac\nX by considering the following null and alternative hypotheses:\nH0 :\n1\n|AX|\nX\ni∈AX\nsi =\n1\n|Ac\nX|\nX\ni∈Ac\nX\nsi,\nv.s.\nH1 :\n1\n|AX|\nX\ni∈AX\nsi ̸=\n1\n|Ac\nX|\nX\ni∈Ac\nX\nsi.\n(5)\nFor clarity, we mainly consider a test for the mean difference as a specific example — however, the\nproposed VAE-AD Test is applicable to a more general class of statistical tests.\nSpecifically, let\nη ∈ Rn be an arbitrary n-dimensional vector depending on the abnormal region AX.\nThen, the\nproposed method can cover a statistical test represented as\nH0 : η⊤s = c\nv.s.\nH1 : η⊤s ̸= c,\n(6)\nwhere c is an arbitrary constant. The formulation in Eq. (6) covers a wide range of practically useful\nstatistical tests. In fact, Eq. (5) is a special case of Eq. (6). It can cover differences not only in means\nbut also in other measures such as maximum difference, and differences after applying some image\nfilters (e.g., Gaussian filter).\nTest statistic.\nTo evaluate the hypothesis defined in Eq. (5), we define the test statistic T(X) as\nfollows:\nT(X) =\n1\n|AX|\nX\ni∈AX\nXi −\n1\n|Ac\nX|\nX\ni∈Ac\nX\nXi = η⊤X,\n(7)\nwhere η =\n1\n|AX|1AX −\n1\n|Ac\nX|1Ac\nX and 1A ∈ Rn is a vector whose i-th element is 1 if i ∈ AX and 0\notherwise.\nNaive p-values.\nWhen the test statistic in Eq. (7) is used for the statistical test in Eq. (5), the\np-value can be easily calculated if η does not depend on the image X, i.e., if the abnormal region AX\nis detected without looking at the X. In this unrealistic situation, the p-value, which we call naive\np-value can be computed as\npnaive = PH0(|T(X)| ≥ |T(x)|),\n8\nwhere X is a random vector and x is the observed image. Under the unrealistic assumption, the pnaive\ncan be easily computed because the null distribution of T(X) = η⊤X is normally distributed with\nN(0, η⊤Ση). Unfortunately, however, in the actual situation where η depends on X, a statistical test\nusing pnaive is invalid in the sense that\nPH0(pnaive ≤ α) > α, ∃α ∈ [0, 1].\nNamely, the probability of Type I error (an error that a normal region is mistakenly detected as\nanomaly) cannot be controlled at the desired significance level α.\n4\nCSI for VAE-based AD\nTo obtain a valid p-value, we employ CSI framework.\n4.1\nCondtional Selective Inference (CSI)\nIn CSI, p-values are computed based on the null distribution conditional on a event that a certain\nhypothesis is selected. The goal of CSI is to compute a p-value that satisfies\nPH0(p ≤ α | AX = A) ≤ α,\n(8)\nwhere the condition part AX = A in Eq. (8) indicates that we only consider images X for which a\ncertain hypothesis (abnormal region) A is detected. If the conditional type I error can be controlled\nas in Eq. (8) for all possible hypotheses A ∈ 2[n], then, by the law of total probability, the marginal\ntype I error can also be controlled for all α ∈ (0, 1) because\nPH0(p ≤ α) =\nX\nA∈2[n]\nPH0(A)(p ≤ α | AX = A) ≤ α.\nTherefore, in order to perform valid statistical test, we can employ p-values conditional on the hy-\npothesis selection event. To compute a p-value that satisfies Eq. (8), we need to derive the sampling\ndistribution of the test-statistic\nT(X)|{AX = Ax}.\n(9)\nAs stated in §1, CSI has gained attention as a method of statistical inference for feature selection\nin linear models [Lee et al., 2016]. Most of the existing SI studies exploit the fact that a hypothesis\nselection event (e.g., the event that a certain set of features is selected by a feature selection algorithm\nfor linear models) can be characterized as the intersection of linear inequalities in the data space, thus\nallowing the calculation of the sampling distribution of a conditional test statistic such as Eq. (9).\nOn the other hand, the selection event of the VAE-based AD is difficult to characterize in a simple\n9\nform because it involves a complex computation of VAEs. Our main technical contribution in this\nstudy is to generalize the methods in Duy et al. [2022] and Miwa et al. [2023] to establish a method\nfor calculating the sampling distribution in Eq. (9).\n4.2\nCSI for Piecewise-Assignment Functions\nWe derive the CSI for algorithms expressed in the form of a piecewise-assignment function. Later on,\nwe show that the mapping A : X 7→ AX in Eq. (4) is a piecewise-assignment function, and this will\nresult in the proposed VAE-AD Test.\nDefinition 1 (Piecewise-Assignment Function). Let us consider a function M : Rn ∋ X 7→ MX ∈ M\nwhich assigns an image X to a hypothesis among a finite set of hypotheses M. We call the function\nM a piecewise-assignment function if it is written as\nMX =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nM1,\nif\nX ∈ PM\n1 ,\n...\nMk,\nif\nX ∈ PM\nk ,\n...\nMKM ,\nif\nX ∈ PM\nKM ,\n(10)\nwhere PM\nk , k ∈ [KM], represents a polytope in Rn which can be written as PM\nk\n= {X ∈ Rn | ∆M\nk X′ ≤\nδM\nk } using a certain matrix ∆M\nk\nand a vector δM\nk\nwith appropriate sizes, and KM is the number of\npolytopes. Here, we note that the same hypothesis may be assigned to different polytopes.\nWhen a hypothesis is selected by a piecewise-assignment function in the form of Eq. (10), the\nfollowing theorem tells that the conditional p-value that satisfies Eq. (8) can be derived by using\ntruncated normal distribution.\nTheorem 4.1. Consider a random image X and an observed image x. Let MX and Mx be the\nhypotheses obtained by applying a piecewise-assignment function in the form of Eq. (10) to X and x,\nrespectively. Let η ∈ Rn be a vector depending on Mx, and consider a test statistic in the form of\nT(X) = η⊤X. Furthermore, define\nQX =\n\u0012\nIn − Σηη⊤\nη⊤Ση\n\u0013\nX and Qx =\n\u0012\nIn − Σηη⊤\nη⊤Ση\n\u0013\nx.\nThen, the conditional distribution\nT(X) | {MX = Mx, QX = Qx}\n10\nis a truncated normal distribution TN(η⊤µ, η⊤Ση; Z) with the mean η⊤µ, the variance η⊤Ση, and\nthe truncation intervals Z. The truncation intervals Z is represented as\nZ =\n[\nk:Mk=Mx\n[LM\nk , U M\nk ],\nwhere, for k ∈ [KM], LM\nk\nand U M\nk\nare defined as follows:\nLM\nk =\nmax\nj:(βM\nk )j>0\n(αM\nk )j\n(βM\nk )j\n, U M\nk\n=\nmin\nj:(βM\nk )j<0\n(αM\nk )j\n(βM\nk )j\n,\nαM\nk = δM\nk − ∆M\nk Qx, βM\nk = ∆M\nk Ση(Ση⊤Ση)−1.\nThe proof of Theorem 4.1 is deferred to Appendix B. Using the sampling distribution of the test\nstatistic T(X) conditional on {MX = Mx, QX = Qx} in Theorem 4.1, we can define the p-value for\nCSI as\npselective\n(11)\n= PH0(|T(X)| ≥ |T(x)| | MX = Mx, QX = Qx).\nThe selective p-value pselective defined in Eq. (11) satisfies\nPH0(pselective ≤ α | MX = Mx) = α, ∀α ∈ [0, 1]\nbecause QX is independent of the test statistic T(X) = η⊤X. From the discussion in §4.1, a valid\nstatistical test can be conducted by using pselective in Eq. (11).\n4.3\nPiecewise-Linear Functions\nWe showed that, if the hypothesis selection algorithm is represented in the form of piecewise-assignment\nfunction, we can formulate valid selective p-values. The purpose of this subsection is to set the stage\nfor demonstrating in the next subsection how the entire process of a trained VAE can be depicted as a\npiecewise-linear function, and how VAE-based AD algorithm in Eq. (4) is represented as a piecewise-\nassignment function.\nDefinition 2 (Piecewise-Linear Function). A piecewise-linear function f : Rn → Rm is written as:\nf(X) =\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nΨf\n1X + ψf\n1 ,\nif X ∈ Pf\n1 ,\n...\nΨf\nkX + ψf\nk,\nif X ∈ Pf\nk ,\n...\nΨf\nKf X + ψf\nKf ,\nif X ∈ Pf\nKf ,\n(12)\n11\nwhere Pf\nk represents a polytope in Rn written as Pf\nk = {X ∈ Rn | ∆f\nkX′ ≤ δf\nk} for k ∈ Kf with a\ncertain matrix ∆f\nk and a vector δf\nk with appropriate sizes. Furthermore, Ψf\nk and ψf\nk for k ∈ Kf are\nthe k-th linear transformation matrix and the bias vector, respectively, and Kf denotes the number of\npolytopes of a piecewise-linear function f.\nConsidering piecewise-assignment and piecewise-linear functions, the following properties straight-\nforwardly hold:\n• The concatenation of two or more piecewise-linear functions results in a piecewise-linear function.\n• The composition of two or more piecewise-linear functions results in a piecewise-linear function.\n• The composition of a piecewise-linear function and a piecewise-assignment function results in a\npiecewise-assignment function.\n4.4\nVAE-based AD as Piecewise-Assignment Function\nIn this subsection, we show that the VAE-based AD algorithm in Eq. (4) is a piecewise-assignment\nfunction by verifying that i) the reconstruction error in Eq. (1) is a piecewise-linear function, and ii)\nthe thresholding in Eq. (2) is a piecewise-assignment function.\nMost of basic operations and common activation functions used in the encoder and decoder net-\nworks can be represented as piecewise-linear functions in the form of Eq. (12). For example, the ReLU\nfunction is a piecewise-linear function. Operations like matrix-vector multiplication, convolution, and\nupsampling are linear, which categorizes them as special cases of piecewise-linear functions Further-\nmore, operations like max-pooling and mean-pooling can be represented in the form of Eq. (12). For\ninstance, max-pooling of two variables can be expressed as max{u, v} = u · I(u ≥ v) + v · I(v > u),\nwhich is a piecewise-linear function with Kf = 2. Consequently, the encoder and decoder networks of\nthe VAE, composed or concatenated from piecewise-linear functions, form a piecewise-linear function.\nWe note that this characteristic is not exclusive to our VAE; instead, it applies to the majority of\nCNN-type deep learning models2.\nFurthermore, the reconstruction error in Eq. (1) is also a piecewise-linear function. Specifically,\nlet fabs be the absolute value function, which is clearly piecewise-linear function, fmm1 be a function\nfor multiplying the matrix\n\u0010\nIn, −In\n\u0011\nfrom the left, and fmm2 be a function for multiplying the matrix\n\u0010\nIn, In\n\u0011⊤\nfrom the left. Then, the reconstruction error Ei(X) = |µθ(µϕ(X)) − X|i is given as the\nith element of the following compositions of multiple piecewise-linear functions:\nfabs ◦ fmm1 ◦\nh\nµθ ◦ µϕ\nIn\ni\n◦ fmm2(X).\n2An example of components that do not exhibit piecewise linearity is nonlinear activation function such as the\nsigmoid function. However, since a one-dimensional nonlinear function can be approximated with high accuracy by a\npiecewise-linear function with sufficiently many segments, there are no practical problems.\n12\nThe thresholding operation in Eq. (2) is clearly piecewise-assignment function. It means that the\noperation of detecting abnormal region AX in Eq. (4) is composition of piecewise-linear function and\npiecewise-assignment function, which results in a piecewise-assignment function. We summarize the\naforementioned discussion into the following lemma.\nLemma 1. The anomaly detection using VAE defined in Eq. (4), which uses piecewise-linear functions\nin the encoder and decoder network, is a piecewise-assignment function.\nConsequently, we can conduct the statistical test in (5) based on the selective p-value in (11) along\nwith Theorem 4.1.\n5\nComputational Tricks\nIn this section, we demonstrate the procedure for efficiently computing the truncated intervals Z\nderived from Eq. (4). The identification of Z is challenging because the VAE-based AD is comprised of\na substantial number of known piecewise-linear functions and a piecewise-assignment function. There\nare two difficulties: i) which indices of k whose anomaly region is the same as the observed one, and\nii) how to compute each truncated interval [LA\nk , U A\nk ]. Our idea is to leverage parametric programming\nin conjunction with auto-conditioning to efficiently compute Z. Specifically, we can identify only the\nnecessary indices of k and determining their respective intervals [LA\nk , U A\nk ]. This enables us to bypass\nthe unneeded computation of unnecessary components, thus saving computational time.\n5.1\nParametric Programming\nIn the Theorem 4.1, the truncated intervals Z can be regarded as the intersections of the polytopes\n{P A\nk }k:Ak=Ax with the line X = Qx + Ση(η⊤Ση)−1Z. This implies that determining the truncated\nintervals Z is accomplished by examining this specific line rather than the entire space.\nAlogorithm 1 outlines the procedure to identify Z. The algorithm starts at zmin and search for the\ntruncated intervals along the line until zmax3. For each step, given z, the algorithm computes the lower\nbound LA\nk and upper bound U A\nk of the interval to which z belongs to, as well as corresponding anomaly\nregion Ak = AX(z). The LA\nk and U A\nk are computed by the technique described in the next subsection.\nThis procedure is commonly referred to as parametric programming (e.g., lasso regularization path).\n3We set the zmin = −|T(x)| − 10σ and zmax = |T(x)| + 10σ, where σ is the standard deviation of test statistic. This\nis justified by the fact that the probability in the tails of the normal distribution can be considered negligible.\n13\nAlgorithm 1 Parametric Programming-Based SI\nRequire: x, zmin, zmax\n1: Obtaine Ax and compute η.\n2: z ← zmin and Z ← ∅\n3: while z ≤ zmax do\n4:\nCompute LA\nk , UA\nk , and Ak respect to z by auto-conditioning (see Appendix C).\n5:\nif Ak = Ax then\n6:\nZ ← Z ∪ [LA\nk , UA\nk ]\n7:\nend if\n8:\nz ← UA\nk + δ, where δ is a small positive number.\n9: end while\n10: pselective ← (11) with Threorem 4.1\noutput pselective and Ax\n5.2\nAuto-Conditioning\nIn line 4 of Algorithm 1, we utilize a technique referred to as auto-conditioning. Similar to auto-\ndifferentiation, this method leverages the fact that the entire computations of LA\nk and U A\nk executes a\nsequence of piece-wise linear operations. By applying the recursive rule repeatedly to these operations,\nLA\nk and U A\nk can be automatically computed. The details are deferred to Appendix C.\nThis implies that by implementing the computational techniques for known piecewise-linear/assignment\nfunctions, we can automatically compute the truncation intervals and the anomaly region.\nThis\nadaptability proves particularly advantageous when dealing with complex systems like Deep Neural\nNetworks (DNNs), where frequent and detailed structural adjustments are often required.\nWe note that the auto-conditioning technique is originally proposed in Miwa et al. [2023]. However,\nthe authors concentrate on a specific application of the saliency region, and no existing studies recog-\nnize its crucial application in VAE literature. In this paper, we prove that a VAE can be represented\nas a piecewise-assignment function, thus highlighting the crucial application of auto-conditioning in\nefficiently conducting the proposed VAE-AD Test.\n6\nExperiment\nWe demonstrate the performance of the proposed method.\nExperimental Setup. We compared the proposed method (VAE-AD Test) with OC (simple\nextension of SI literature to our setting), Bonferroni correction (Bonf) and naive method.\nMore\ndetails can be found in Appendix D. We considered two covariance matrix structures:\n• Σ = In (Independence)\n14\n64\n256\n1024\n4096\nn\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nType I Error Rate\nVAE-AD Test\nOC\nBonf\nNaive\n(a) Independence\n64\n256\n1024\n4096\nn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nType I Error Rate\nVAE-AD Test\nOC\nBonf\nNaive\n(b) Correlation\nFigure 2: Type I error rate comparison\n1\n2\n3\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPower\nVAE-AD Test\nOC\nBonf\n(a) Independence\n1\n2\n3\n4\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nPower\nVAE-AD Test\nOC\nBonf\n(b) Correlation\nFigure 3: Power comparison\n• Σ = AR(1)⊗AR(1) (Correlation) where AR(1) is the first-order autoregressive matrix {AR(1)}ij ∈\nR\n√n×√n = 0.25|i−j| and ⊗ is kronecker dot.\nTo examine the type I error rate, we generated 1000 null images X = (X1, . . . , Xn), where s = 0\nand ϵ ∼ N(0, Σ), for each n ∈ {64, 256, 1024, 4096}. To examine the power, we set n = 256 and\ngenerated 1000 images in which ϵ ∼ N(0, Σ), the signals si = ∆ for any i /∈ S where S is the\n”true” anomaly region whose location is randomly determined, and si = 0 for any i /∈ S. We set\n∆ ∈ {1, 2, 3, 4}. In all experiments, we set the threshold λ = 1.2 for the anomaly detection, and the\nsignificance level α = 0.05. We also apply mean filtering to the reconstruction error to enhance the\nanomaly detection performance.\nNumerical results. The results of type I error rate and power are shown in Figs. 2 and 3,\n15\n0.01\n0.02\n0.03\n0.04\nWS Distance\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nType I Error\nt\ngennormflat\ngennormsteep\nexponnorm\nskewnorm\n(a) α = 0.05.\n0.01\n0.02\n0.03\n0.04\nWS Distance\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\nType I Error\nt\ngennormflat\ngennormsteep\nexponnorm\nskewnorm\n(b) α = 0.10.\nFigure 4: Robustness of the proposed method.\nrespectively. The VAE-AD Test, OC, and Bonf successfully controlled the type I error rate in the\nboth cases of independence and correlation, whereas the naive method could not. Since the naive\nmethod failed to control the type I error, we no longer considered its power. The power of the VAE-\nAD Test was the highest among the methods that controlled the type I error. The Bonferroni method\nhas the lowest power because it is conservative due to considering the huge number of all possible\nhypotheses. OC also has low power because it considers extra conditioning, which causes the loss of\npower.\nAdditionally, we confirm the robustness of the VAE-AD Test in terms of type I error control\nwhen the noise follows skew normal distribution, exponential normal distribution, generalized normal\ndistribution with flat tails, and t distribution. More details can be found in Appendix D. The results\nare shown in Fig. 4. Our method still maintains good performance in type I error rate control.\nReal data experiments.\nWe examined the brain image dataset extracted from Buda et al.\n[2019], which includes 939 and 941 images with and without tumors, respectively. The results of\nstatistical testing for images without tumor and with tumor are presented in Figs. 5 and 6. The naive\np-value is small even in cases where no tumor region exists in the image. This indicates that the naive\np-value cannot be used to quantify the reliability of the result of anomaly detection using VAE. With\nthe proposed selective p-values, we successfully identified false and true positive detections.\n7\nConclusion\nWe proposed a novel setup of testing the results of VAE-AD and introduced a method to compute a\nvalid p-value for conducting the statistical test. The experiments were conducted on both synthetic\n16\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(a) pnaive = 0.000, pselective = 0.431\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(b) pnaive = 0.000, pselective = 0.849\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(c) pnaive = 0.000, pselective = 0.196\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(d) pnaive = 0.000, pselective = 0.797\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(e) pnaive = 0.000, pselective = 0.299\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(f) pnaive = 0.000, pselective = 0.598\nFigure 5: Anomaly detection for images without tumor.\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(a) pnaive = 0.000, pselective = 0.048\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(b) pnaive = 0.000, pselective = 0.000\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(c) pnaive = 0.000, pselective = 0.017\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(d) pnaive = 0.000, pselective = 0.000\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(e) pnaive = 0.000, pselective = 0.000\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(f) pnaive = 0.000, pselective = 0.000\nFigure 6: Anormaly detection for images with tumor.\n17\nand real-world datasets to showcase the superior performance of the proposed method. We believe\nthat this study stands as a significant step toward reliability of VAE-driven hypotheses.\nAcknowledgements\nThis work was partially supported by MEXT KAKENHI (20H00601), JST CREST (JPMJCR21D3),\nJST Moonshot R&D (JPMJMS2033-05), JST AIP Acceleration Research (JPMJCR21U2), NEDO\n(JPNP18002, JPNP20006), and RIKEN Center for Advanced Intelligence Project.\nReferences\nAnonymous. Statistical test for generated hypotheses by diffusion models. Unpublished, 2024a.\nAnonymous. Statistical test for attention maps in vision transformers. Unpublished, 2024b.\nC. Baur, B. Wiestler, S. Albarqouni, and N. Navab. Deep Autoencoding Models for Unsupervised\nAnomaly Segmentation in Brain MR Images, page 161–169.\nSpringer International Publishing,\n2019. ISBN 9783030117238. doi: 10.1007/978-3-030-11723-8 16. URL http://dx.doi.org/10.\n1007/978-3-030-11723-8_16.\nC. Baur, S. Denner, B. Wiestler, N. Navab, and S. Albarqouni.\nAutoencoders for unsupervised\nanomaly segmentation in brain mr images: A comparative study.\nMedical Image Analysis, 69:\n101952, 2021. ISSN 1361-8415. doi: https://doi.org/10.1016/j.media.2020.101952. URL https:\n//www.sciencedirect.com/science/article/pii/S1361841520303169.\nL. Breiman. The little bootstrap and other methods for dimensionality selection in regression: X-fixed\nprediction error. Journal of the American Statistical Association, 87(419):738–754, 1992.\nM. Buda, A. Saha, and M. A. Mazurowski. Association of genomic subtypes of lower-grade gliomas\nwith shape features automatically extracted by a deep learning algorithm. Computers in Biology and\nMedicine, 109:218–225, 2019. ISSN 0010-4825. doi: https://doi.org/10.1016/j.compbiomed.2019.\n05.002. URL https://www.sciencedirect.com/science/article/pii/S0010482519301520.\nR. Chalapathy and S. Chawla.\nDeep learning for anomaly detection: A survey.\narXiv preprint\narXiv:1901.03407, 2019.\nS. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and\nGraphical Statistics, pages 1–12, 2019.\n18\nS. Chen and J. Bien. Valid inference corrected for outlier removal. Journal of Computational and\nGraphical Statistics, 29(2):323–334, 2020.\nX. Chen and E. Konukoglu. Unsupervised detection of lesions in brain MRI using constrained adver-\nsarial auto-encoders. In Medical Imaging with Deep Learning, 2018. URL https://openreview.\nnet/forum?id=H1nGLZ2oG.\nY. Choi, J. Taylor, and R. Tibshirani. Selecting the number of principal components: Estimation of\nthe true rank of a noisy matrix. The Annals of Statistics, 45(6):2590–2617, 2017.\nJ. K. Chow, Z. Su, J. Wu, P. S. Tan, X. Mao, and Y.-H. Wang. Anomaly detection of defects on\nconcrete structures with the convolutional autoencoder.\nAdvanced Engineering Informatics, 45:\n101105, 2020.\nD. Dehaene, O. Frigo, S. Combrexelle, and P. Eline. Iterative energy-based projection on a normal\ndata manifold for anomaly localization, 2020.\nV. N. L. Duy, H. Toda, R. Sugiyama, and I. Takeuchi. Computing valid p-value for optimal changepoint\nby selective inference using dynamic programming. In Advances in Neural Information Processing\nSystems, 2020.\nV. N. L. Duy, S. Iwazaki, and I. Takeuchi. Quantifying statistical significance of neural network-based\nimage segmentation by selective inference. Advances in Neural Information Processing Systems, 35:\n31627–31639, 2022.\nW. Fithian, J. Taylor, R. Tibshirani, and R. Tibshirani. Selective sequential model selection. arXiv\npreprint arXiv:1512.02565, 2015.\nL. L. Gao, J. Bien, and D. Witten. Selective inference for hierarchical clustering. Journal of the\nAmerican Statistical Association, pages 1–11, 2022.\nD. Jana, J. Patil, S. Herkal, S. Nagarajaiah, and L. Duenas-Osorio. Cnn and convolutional autoencoder\n(cae) based real-time sensor fault detection, localization, and correction. Mechanical Systems and\nSignal Processing, 169:108723, 2022.\nD. P. Kingma and J. Ba. Adam: A method for stochastic optimization, 2017.\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114,\n2013.\n19\nV. N. Le Duy and I. Takeuchi. Parametric programming approach for more powerful and general\nlasso selective inference. In International conference on artificial intelligence and statistics, pages\n901–909. PMLR, 2021.\nJ. D. Lee, Y. Sun, and J. E. Taylor. Evaluating the statistical significance of biclusters. Advances in\nneural information processing systems, 28, 2015.\nJ. D. Lee, D. L. Sun, Y. Sun, and J. E. Taylor. Exact post-selection inference, with application to the\nlasso. The Annals of Statistics, 44(3):907–927, 2016.\nJ. R. Loftus and J. E. Taylor. A significance test for forward stepwise model selection. arXiv preprint\narXiv:1405.3920, 2014.\nY. Lu and P. Xu. Anomaly detection for skin disease images using variational autoencoder. arXiv\npreprint arXiv:1807.01349, 2018.\nD. Miwa, D. V. N. Le, and I. Takeuchi. Valid p-value for deep learning-driven salient region. In\nProceedings of the 11th International Conference on Learning Representation, 2023.\nG. Pang, C. Shen, L. Cao, and A. V. D. Hengel. Deep learning for anomaly detection: A review.\nACM computing surveys (CSUR), 54(2):1–38, 2021.\nK. Sugiyama, V. N. Le Duy, and I. Takeuchi. More powerful and general selective inference for stepwise\nfeature selection using homotopy method. In International Conference on Machine Learning, pages\n9891–9901. PMLR, 2021.\nS. Suzumura, K. Nakagawa, Y. Umezu, K. Tsuda, and I. Takeuchi. Selective inference for sparse\nhigh-order interaction models. In Proceedings of the 34th International Conference on Machine\nLearning-Volume 70, pages 3338–3347. JMLR. org, 2017.\nK. Tanizaki, N. Hashimoto, Y. Inatsu, H. Hontani, and I. Takeuchi. Computing valid p-values for image\nsegmentation by selective inference. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 9553–9562, 2020.\nX. Tao, X. Gong, X. Zhang, S. Yan, and C. Adak. Deep learning for unsupervised anomaly localization\nin industrial images: A survey. IEEE Transactions on Instrumentation and Measurement, 71:1–21,\n2022. ISSN 1557-9662. doi: 10.1109/tim.2022.3196436. URL http://dx.doi.org/10.1109/TIM.\n2022.3196436.\nJ. Taylor and R. J. Tibshirani. Statistical learning and selective inference. Proceedings of the National\nAcademy of Sciences, 112(25):7629–7634, 2015.\n20\nR. J. Tibshirani, J. Taylor, R. Lockhart, and R. Tibshirani. Exact post-selection inference for se-\nquential regression procedures. Journal of the American Statistical Association, 111(514):600–620,\n2016.\nX. Wang, Y. Du, S. Lin, P. Cui, Y. Shen, and Y. Yang. advae: A self-adversarial variational au-\ntoencoder with gaussian anomaly prior knowledge for anomaly detection. Knowledge-Based Sys-\ntems, 190:105187, 2020. ISSN 0950-7051. doi: https://doi.org/10.1016/j.knosys.2019.105187. URL\nhttps://www.sciencedirect.com/science/article/pii/S0950705119305283.\nD. Zimmerer, F. Isensee, J. Petersen, S. Kohl, and K. Maier-Hein. Unsupervised anomaly localization\nusing variational auto-encoders, 2019.\nA\nThe details of VAE\nWe used the architecture of the VAE as shown in Figure 7 and set m = 10 as a dimensionality of the\nlatent space. We used ReLU as an activation function for the encoder and decoder. We generated\n1000 images from N(0, In) as normal images and trained the VAE with these images, and used Adam\n[Kingma and Ba, 2017] as an optimizer.\n1\n√n\nx\n4\n4\n√n\nConv\nMaxPool\n8\n8\n√n/2\nConv+ReLU\n1\n2n\nReshape\n1\nm\nDense (µϕ)\n1\nm\nDense (σϕ)\n1\nm\nz ∼ N(µϕ, σ2\nϕ)\n1\n2n\nDense\nReshape\n8\n8\n√n/2\nConv+ReLU\nUpSample\n4\n√n\nConv\n1\n√n\nConv\n1\n√n\nµθ\nFigure 7: Architecture of the VAE.\nB\nProof of Theorem 4.1\nProof. The theorem is based on the Lemma 3.1 in Chen and Bien [2019]. By the definition of the\npiecewise-assignment function, the conditional part, {MX = Mx} can be characterized as the union\nof polytopes,\n{MX = Mx} =\n[\nk:Mk=Mx\nP M\nk .\n21\nBy substituting X(Z) = Qx + Ση(η⊤Ση)−1Z into the polytopes P M\nk , we obtain the truncated\nintervals Z in the lemma. For the set k such that Mk = Mx, we have QX ⊥ Z by orhtogonality of\nQX and η and by the properties of the normal distribution. Hence, we obtain\nZ | {MX = Mx, QX = Qx}\nd= Z | {Z ∈ Z, QX = Qx}\nd= Z | {Z ∈ Z} (∵ QX ⊥ Z)\nThere is no randomness in Z,\nZ | {MX = Mx, QX = Qx} ∼ TN(η⊤µ, η⊤Ση; Z).\n■\nC\nThe Details of auto-conditioning\nThis section demonstrates the auto-conditioning algorithm, utilized to compute the truncated intervals\n[LA\nk , U A\nk ] and the corresponding anomaly region Ak respect to the z in Algorithm 1. The algorithm\nis introduced for the piecewise-assignment function, which is composed of piecewise-linear functions\nand a piecewise-assignment function.\nIt is conceptualized as a directed acyclic graph (DAG) that delineates the processing of input\ndata, similar to a computational graph in auto-differentiation. In this graph, the nodes symbolize the\npiecewise-linear and piecewise-assignment functions, each with an input and output edge to represent\nthe function compositions. It should be noted that the node such as µϕ and µθ, may replace the\nother DAG express the piecewise-linear/assignment function of the node since it can be represented\nas the composition and concatenation of array of simpler piecewise-linear/assignment functions. The\nlevel of simplicity for a function of a node can be determined based on what is most convenient for\nthe implementation. A special node, representing the concatenation of two piecewise-linear functions,\nfeatures two input edges and one output edge.\nFigure 8 shows the directed acyclic graph of the\nanomaly detection using VAE in Eq. (4).\nC.1\nUpdate rules for the nodes of the piecewise-assignment functions\nThe computation of the interval [LA\nk , LA\nk ] is defined in a recursive way.\nThe output of the node\nf : Rl → Rm in the DAG are denoted as af, bf ∈ Rm and Lf, Uf ∈ R.\nUpdate rule for the initial node.\nAt first, the output of the initial node X(z) of the directional\ngraph denoted as f0 for notational convention, are defined as af0 = Qx, bf0 = Ση(η⊤Ση)−1, Lf0 =\n22\nX(z)\nfmm2\nµθ\nIn\nµϕ\nconcat\nfmm1\nfabs\nΘ\nAX(z)\nFigure 8: The directed acyclic graph of the anomaly detection using VAE A : Rn → 2|n| defined\nin Eq. (4). Circles represent the piecewise-linear functions and the piecewise-assignment function.\nThe rectangle represents the concatenation of piecewise-linear functions. The edges represent the\ncomposition of piecewise-linear functions.\n−∞, and Uf0 = ∞. It should be noted here that X(z) = af0 + bf0z is the line appeared in the proof\nof Theorem 4.1 in Section B.\nUpdate rule for the node of the piecewise-linear functions.\nLet us consider the output for\nthe node g whose input is the output of the node f in the DAG. The inputs of the g’s node\n(i.e.\noutput of node f) are denoted as af, bf, Lf and Uf. af is the summed point vector added in the\npiecewise-linear functions until reaching f, bf is the direction vector corresponding to z, multiplied in\nthe piecewise-linear functions until reaching to f. Then, the output of the piecewise-linear function\nf is represented as af + bfz. Lf and Uf are the lower and upper bounds of the interval obtained at\nthe piecewise-linear function f. The output of the node g is defined as follows: 1) Check the index j\nsuch that the output of f within the polytope of: P g\nj ∋ af + bfz. 2) Compute the point vector ag\nand the direction vector bg of the piecewise-linear function g with the index j,\nag = Ψg\njaf + ψg\nj , bg = Ψg\njbf.\n(13)\n3) Compute the lower and upper bounds of the interval Lg and Ug with the index j,\nL =\nmax\nk:(βg\nj )k>0\n(αg\nj)k\n(βg\nj )k\n, U =\nmin\nk:(βg\nj )k<0\n(αg\nj)k\n(βg\nj )k\n,\nwhere αg\nj = δg\nj − ∆g\njaf and βf\nj = ∆g\njbf. 4) Take the intersection of the interval [Lf, Uf] ∩ [L, U] as\nthe interval [Lg, Ug] of the piecewise-assignment function g as\nLg = max(Lf, L), Ug = min(Uf, U).\nThis update rule is obtained from the Lemma 2 in Miwa et al. [2023].\n23\nUpdate rule for the nodes of concatenation of two piecewise-linear functions.\nLet us\nconsider the concatenation node of two piecewise-linear functions f and g denoted as concat. Let\nthe inputs of the node be af, bf, Lf and Uf from the node f and ag, bg, Lg and Ug from the node g.\nThe output of the concatenation node, aconcat, bconcat, Lconcat and Uconcat are defined as follows: 1)\nConcatenate the vector outputs of nodes f and g\naconcat =\n\naf\nag\n\n , bconcat =\n\nbf\nbg\n\n .\n2) Take intersection of the interval [Lf, Uf] ∩ [Lg, Ug] as\nLconcat = max(Lf, Lg), Uconcat = min(Uf, Ug).\nUpdate rule for the final node.\nAt the final node Θ which is the piecewise-assignment function,\nit takes the same input as the node of piecewise-linear functions and outputs are the same except\nfor the aΘ and bΘ. 1) It computes the index j such that the input falls into the polypotopes of P Θ\nj .\n2) Then, the anomaly region Aj is obtained instead of Eq. (13) in the update rule for the node of\npiecewise-linear functions. 3) The computation of lower bounds LΘ and the upper bounds UΘ are the\nsame as the update rule for the node of piecewise-linear functions. The output of the final node are\nthe anomaly region Aj, the lower bounds LΘ and the upper bounds UΘ.\nThen, apply the above update rule to the directional graph of the piecewise-assignment function\nfrom the initial node f0 to the final node Θ. Consequently, the auto-conditioning algorithm computes\nthe lower and upper bounds of the interval as the outputs of final node LA\nk = LΘ, LA\nk = LΘ and\nAk = Aj.\nC.2\nImplementation\nWe implemented the auto-conditioning algorithm described above in Python using the tensorflow\nlibrary. The codes construct the DAG of the piecewise-assignment function automatically from the\ntrained Keras/tensorflow model. Then, we do not need further implementation to conduct CSI for\neach specific DNN model. This indicates that even if we change the architecture or adjust the hyper-\nparameters and retrain the DNN models, we can conduct the CSI without additional implementation.\nD\nExperimental Details\nMethods for comparison.\nWe compared our proposed method with the following methods:\n• VAE-AD Test: our proposed method.\n24\n• OC: our proposed method conditioning on the only one polytope to which the observed image\nbelongs x ∈ P A\nk . This method is computationally efficient; however, its power is low due to over-\nconditioning.\n• Bonf: the number of all possible hypotheses are considered to account for the selection bias. The\np-value is computed by pbonf = min(1, pnaive × 2n)\n• Naive: the conventional method is used to compute the p-value.\nExperiment for robustness.\nWe evaluate the robustness of our proposed methodology in terms\nof Type I error control, specifically under conditions where the noise distribution deviates from the\nGaussian assumption. We investigate this robustness by applying our method across a range of non-\nGaussian noise distributions, including:\n• Skew normal distribution (skewnorm)\n• Exponential normal distribution (exponorm)\n• Generalized normal distribution with steep tails (gennormsteep)\n• Generalized normal distribution with flat tails (gennomflat)\n• Student’s t distribution (t)\nWe commence our analysis by identifying noise distributions from the aforementioned list that\nhave a 1-Wasserstein distance of {0.01, 0.02, 0.03, 0.04} relative to the standard normal distribution\nN(0, 1). Subsequently, we standardize these noise distributions to ensure a mean of 0 and a variance\nof 1. Setting the sample size to n = 256, we generate 1000 samples from the selected distributions and\napply hypothesis testing to each sample to obtain the Type I error rate. This process is conducted\nat significance levels α = {0.05, 0.10}. The results are shown in Figure 4. Our method still maintains\ngood performance even when the assumption of Gaussian noise is violated.\nMore results on brain image dataset.\nAdditional results are shown in Figs. 9 and 10.\n25\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(a) pnaive = 0.000, pselective = 0.668\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(b) pnaive = 0.000, pselective = 0.849\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(c) pnaive = 0.011, pselective = 0.500\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(d) pnaive = 0.012, pselective = 0.137\nFigure 9: Anomaly detection for image without tumor.\n26\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(a) pnaive = 0.000, pselective = 0.001\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(b) pnaive = 0.000, pselective = 0.000\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(c) pnaive = 0.000, pselective = 0.000\nInput Image\nReconstruction\nReconstruction Error\nAnomaly Region\n(d) pnaive = 0.000, pselective = 0.017\nFigure 10: Anormaly detection for image with tumor.\n27\n"
}