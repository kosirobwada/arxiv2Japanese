{
    "optim": "Published as a conference paper at ICLR 2024\nSELF-SUPERVISED SPEECH QUALITY ESTIMATION AND\nENHANCEMENT USING ONLY CLEAN SPEECH\nSzu-Wei Fu 1, Kuo-Hsuan Hung 1∗, Yu Tsao 2, Yu-Chiang Frank Wang 1\n1 NVIDIA, 2 Research Center for Information Technology Innovation, Academia Sinica\nszuweif@nvidia.com,d07528023@ntu.edu.tw,\nyu.tsao@citi.sinica.edu.tw,frankwang@nvidia.com\nABSTRACT\nSpeech quality estimation has recently undergone a paradigm shift from human-\nhearing expert designs to machine-learning models. However, current models\nrely mainly on supervised learning, which is time-consuming and expensive for\nlabel collection. To solve this problem, we propose VQScore, a self-supervised\nmetric for evaluating speech based on the quantization error of a vector-quantized-\nvariational autoencoder (VQ-VAE). The training of VQ-VAE relies on clean speech;\nhence, large quantization errors can be expected when the speech is distorted.\nTo further improve correlation with real quality scores, domain knowledge of\nspeech processing is incorporated into the model design. We found that the vector\nquantization mechanism could also be used for self-supervised speech enhancement\n(SE) model training. To improve the robustness of the encoder for SE, a novel\nself-distillation mechanism combined with adversarial training is introduced. In\nsummary, the proposed speech quality estimation method and enhancement models\nrequire only clean speech for training without any label requirements. Experimental\nresults show that the proposed VQScore and enhancement model are competitive\nwith supervised baselines. The code will be released after publication.\n1\nINTRODUCTION\nSpeech quality estimators are important tools in speech-related applications such as text-to-speech,\nspeech enhancement (SE), and speech codecs, etc. A straightforward approach to measure speech\nquality is through subjective listening tests. During the test, participants are asked to listen to audio\nsamples and provide their judgment (for example, on a 1 to 5 Likert scale). Hence, the mean opinion\nscore (MOS) of an utterance can be obtained by averaging the scores given by different listeners.\nAlthough subjective listening tests are generally treated as the \"gold standard,\" such tests are time-\nconsuming and expensive, which restricts their scalability. Therefore, objective metrics have been\nproposed and applied as surrogates for subjective listening tests.\nObjective metrics can be categorized into handcrafted and machine learning-based methods. The\nhandcrafted metrics are typically designed by speech experts. Examples of this approach include\nthe perceptual evaluation of speech quality (PESQ) (Rix et al., 2001), perceptual objective listening\nquality analysis (POLQA) (Beerends et al., 2013), virtual speech quality objective listener (ViSQOL)\n(Chinen et al., 2020), short-time objective intelligibility (STOI) (Taal et al., 2011), hearing-aid speech\nquality index (HASQI) (Kates & Arehart, 2014a), and hearing-aid speech perception index (HASPI)\n(Kates & Arehart, 2014b), etc. The computation of these methods is mainly based on comparing\ndegraded speech with its clean reference and hence belongs to intrusive metrics. The requirement for\nclean speech references significantly hinders their application in real-world conditions.\nMachine-learning-based methods have been proposed to eliminate the dependence on clean speech\nreferences during inference and can be further divided into two categories. The first attempts to\nnon-intrusively estimate the objective scores mentioned above (Fu et al., 2018; Dong & Williamson,\n2020; Zezario et al., 2020; Catellier & Voran, 2020; Yu et al., 2021b; Xu et al., 2022; Kumar et al.,\n2023). However, during training, noisy/processed and clean speech pairs are still required to obtain\n∗Internship at NVIDIA\n1\narXiv:2402.16321v1  [cs.SD]  26 Feb 2024\nPublished as a conference paper at ICLR 2024\nthe objective scores as model targets. For example, Quality-Net (Fu et al., 2018) trains a bidirectional\nlong short-term memory (BLSTM) with frame-wise auxiliary loss to predict the PESQ score. Instead\nof treating it as a regression task, MetricNet (Yu et al., 2021b) estimates the PESQ score using a\nmulti-class classifier trained by the earth mover’s distance (Rubner et al., 2000). MOSA-Net (Zezario\net al., 2022) is an unified model that simultaneously predict multiple objective scores such as PESQ,\nSTOI, HASQI, and source-to-distortion ratio (SDR). NORESQA (Manocha et al., 2021) utilizes\nnon-matching references to predict relative speech assessment scores (i.e., the signal-to-noise ratio\n(SNR) and scale-invariant SDR (SI-SDR) (Le Roux et al., 2019)). Although these models release\nthe requirement of corresponding clean reference during inference, their training targets (objective\nmetrics) are generally not perfectly correlated with human judgments (Reddy et al., 2021b).\nThe second category of machine-learning-based methods (Lo et al., 2019; Leng et al., 2021; Mittag\net al., 2021; Tseng et al., 2021; Reddy et al., 2021b; 2022; Tseng et al., 2022; Manocha & Kumar,\n2022) has been proposed to solve this problem by using speech and its subjective scores (e.g., MOS)\nfor model training. VoiceMOS challenge (Huang et al., 2022) is targeted for automatic prediction\nof MOS for synthesized speech. DNSMOS (Reddy et al., 2021b; 2022) is trained on large-scale\ncrowdsourced MOS rating data using a multi-stage self-teaching approach. MBNet (Leng et al., 2021)\nconsists of MeanNet, which predicts the mean score of an utterance, and BiasNet, which considers\nthe bias caused by listeners. NORESQA-MOS (Manocha & Kumar, 2022) leverages pre-trained\nself-supervised models with non-matching references to estimate the MOS and was recently released\nas a package in TorchAudio (Kumar et al., 2023).\nHowever, to train a robust quality estimator, large-scale listening tests are required to collect paired\nspeech and MOS data for model supervision. For example, the training data for DNSMOS P.835\n(Reddy et al., 2022) was 75 h. NORESQA-MOS (Manocha & Kumar, 2022) was trained on 7,000\naudio recordings and their corresponding MOS ratings. In addition to the high cost of collecting\ntraining data, these supervised models may exhibit poor generalizability to new domains (Maiti et al.,\n2022). To address this issue, the SpeechLMScore (Maiti et al., 2022), an unsupervised metric for\nevaluating speech quality using a speech-based language model, was proposed. This metric first\nmaps the input speech to discrete tokens, and then applies a language model to compute its average\nlog-likelihood. Because the language model is trained only on clean speech, a higher likelihood\nimplies better speech quality.\nInspired by SpeechLMScore, we investigated unsupervised speech quality estimation in this study,\nbut used a different approach. Our method was motivated by autoencoder-based anomaly detection\n(An & Cho, 2015). Because the autoencoder is trained only on normal data, during inference, we\nexpect to obtain a low reconstruction error for normal data and a large error for abnormal data. Kim\n(2017) applied a speech autoencoder, whose input and output were trained to be as similar as possible\nif inputs clean speech, to select the most suitable speech enhancement model from a set of candidates.\nAlthough (Soni & Patil, 2016; Wang et al., 2019; Martinez et al., 2019) also used autoencoders for\nspeech-quality estimation, the autoencoders in their works were only used for feature extraction.\nTherefore, a supervised model and quality labels are still required. Other works, such as (Giri et al.,\n2020; Pereira et al., 2021; Ribeiro et al., 2020; Hayashi et al., 2020; Abbasi et al., 2021) mainly\napplied autoencoders for audio anomaly detection.\nOur proposed quality estimator is based on the quantization error of VQ-VAE (Van Den Oord\net al., 2017), and we found that VQ-VAE can also be used for self-supervised SE. To align the\nembedding space, Wang et al. (2020) applied cycle loss to share the latent representation between\nclean autoencoder and mixture autoencoder. Although paired training data is not required for their\nmodel training, noisy speech and noise samples are still needed. On the other hand, we achieve\nrepresentation sharing through the codebook of VQ-VAE. In addition, by the proposed self-distillation\nand adversarial training, the enhancement performance can be further improved.\n2\nMETHOD\n2.1\nMOTIVATION\nAs mentioned in the previous section, the proposed speech quality estimator was motivated by\nautoencoder-based anomaly detection. By measuring the reconstruction error with a suitable threshold,\nanomalies can be detected even though only normal data are used for model training. In this study,\n2\nPublished as a conference paper at ICLR 2024\nIN(Conv1d (k=7, c=c1))\nF=257\nT\nk\nIN (c=257)\nIN(Conv1d (k=7, c=c1))\nIN(Conv1d (k=7, c=c2))\nIN(Conv1d (k=7, c=c2))\nIN(Conv1d (k=7, c=d))\nIN(Conv1d (k=7, c=d))\nConv1d (k=7, c=257)\nConv1d (k=7, c=c1)\nConv1d (k=7, c=c1)\nConv1d (k=7, c=c2)\nConv1d (k=7, c=c2)\nConv1d (k=7, c=d)\nF\nT\nVector \nQuantizer \n(Q)\nEncoder\nDecoder\nCodebook (C)\nd\nV\nLarge => Bad speech quality\nSmall => Good speech quality\n  or\nTransformer\nTransformer\nFigure 1: Proposed VQ-VAE for self-supervised speech quality estimation and enhancement. The\nTransformer blocks are only used for speech enhancement.\nwe go one step further based on the assumption that the reconstruction error and speech quality\nmay appear in an inverse proportion relationship (i.e., a larger reconstruction error may imply lower\nspeech quality). People usually rate the speech quality based on an implicit comparison to what clean\nspeech should sound. The purpose of training the VQ-VAE with a large amount of clean speech\nis to guide the model in building its own image of clean speech (stored in the codebook). In this\nstudy, we conducted a comprehensive investigation to address the following questions: 1) Which\nmetric should be used to estimate the reconstruction error? 2) Where should reconstruction error be\nmeasured?\nWhile developing a self-supervised speech quality estimator, we also found a potential method for\ntraining a speech enhancement model using only clean speech.\n2.2\nPROPOSED MODEL FRAMEWORK\nThe proposed model comprises three building blocks, as shown in Figure 1.\n1) Encoder (E) maps the input spectrogram X ∈ RF ×T onto a sequence of embeddings Z ∈ Rd×T ,\nwhere F and T are the frequency and time dimensions of the spectrogram, respectively, and d is the\nembedding feature dimension.\nWe first treat the input spectrogram X as a T-length 1-D signal with F channels, and then build\nthe encoder using a series of 1-D convolution layers, as shown in Figure 1. In this figure, k and c\nrepresent the kernel size and number of output channels (number of filters), respectively. Note that\nwe apply instance normalization (IN) (Ulyanov et al., 2016) to input X and after every convolutional\nlayer. We found that normalization is a critical step for boosting the quality estimation performance.\nBetween the IN and convolution layers, LeakyReLU (Xu et al., 2015) was applied as an activation\nfunction.\n3\nPublished as a conference paper at ICLR 2024\nTo increase the model’s capacity for speech enhancement, two Transformer encoder layers were\ninserted before and after the vector quantization module (as indicated by the dashed rectangles in\nFigure 1), respectively. The standard deviation normalization used in IN is inappropriate for SE\nbecause the volume information is also important for signal reconstruction. Therefore, we only\nmaintain the mean removal operation in IN for SE.\n2) Vector quantizer (Q) replaces each embedding Zt ∈ Rd×1 with its nearest neighbor in the\ncodebook C ∈ Rd×V , where t is the index along the time dimension and V is the size of the\ncodebook. The codebook is initialized using the k-means algorithm on the first training batch as\nSoundStream (Zeghidour et al., 2021) and is updated using the exponential moving average (EMA)\n(Van Den Oord et al., 2017). During inference, the quantized embedding Zqt ∈ Rd×1 is chosen from\nV candidates of the codebook, such that it has the smallest L2 distance:\nZqt = arg min\nCv∈C\n||Zt − Cv||2\n(1)\nWe can also normalize the embedding and codebook to have unit L2 norm before calculating the L2\ndistance:\nZqt = arg min\nCv∈C\n||normL2(Zt) − normL2(Cv)||2\n(2)\nThis is equivalent to choosing the quantized embedding based on cosine similarity (Yu et al., 2021a;\nChiu et al., 2022). These two criteria have their own applications. For example, Eq. (1) is suitable for\nspeech enhancement while Eq. (2) is good at modeling speech quality. We will discuss details in the\nfollowing sections.\n3) Decoder (D), which generates a reconstruction of input ˆX ∈ RF ×T from quantized embeddings\nZq ∈ Rd×T . The decoder architecture is similar to the encoder as shown in Figure 1.\n2.3\nTRAINING OBJECTIVE\nThe training loss of the VQ-VAE includes three loss terms (Van Den Oord et al., 2017):\nL = dist(X, ˆX) + ||sg(Zt) − Zqt||2 + β||Zt − sg(Zqt)||2\n(3)\nwhere sg(.) represents the stop-gradient operator. The second term is used to update the codebook,\nwhere, in practice, the EMA is applied. The third term is the commitment loss, which causes the\nencoder to commit to the codebook. In this study, the commitment weight β was set to 1.0 and 3.0 for\nquality estimation and SE, respectively, based on the performance on validation set. The first term is\nthe reconstruction loss of the input X and output ˆX. Conventionally, this is simply an L1 or L2 loss.\nHowever, for speech quality estimation, we applied negative cosine similarity as the distance metric.\nThe reason for using cosine similarity in Eqs. (2) and (3) for quality estimation is that we want\nsimilar phonemes can be grouped in the same token of the codebook. Using cosine similarity can\nignore the volume difference and focus more on the content. For example, if we apply L2 loss to\nminimize the reconstruction loss, louder and quieter ’a’ sounds may not be grouped into the same\ncode, which hinders the evaluation of speech quality.\n2.4\nVQSCORE FOR SPEECH QUALITY ESTIMATION\nIn conventional autoencoder-based anomaly detection, the criterion for determining an anomaly is\nbased on the reconstruction errors of the model input and output. In this study, we found that the\nquantization error between Z and Zq can provide a much higher correlation with human hearing\nperception. Note that being able to calculate the quantization error is a unique property of the\nVQ-VAE that other autoencoders do not have. Because the VQ-VAE was trained only with clean\nspeech, its codebook can be treated as a high-level representation (e.g., phonemes) for speech signals.\nTherefore, the similarity calculated in this space aligns better with subjective quality scores. The\nproposed VQScore is hence defined as:\nV QScore(cos,z)(X) = 1\nT\nT\nX\nt=1\ncos(Zt, Zqt)\n(4)\n4\nPublished as a conference paper at ICLR 2024\nwhere cos(.) is cosine similarity. (cos, z) in V QScore(cos,z) represents using the cosine similarity as\nthe distance metric and it is calculated in code space (z). We compare the performances of different\ncombinations (e.g., V QScore(cos,x) and V QScore(L2,x), etc.) in the Section B of Appendix.\n2.5\nSELF-DISTILLATION WITH ADVERSARIAL TRAINING TO IMPROVE MODEL ROBUSTNESS\nFOR SPEECH ENHANCEMENT\nThe robustness of the encoder to out-of-domain data (i.e., noisy speech) is the key to self-supervised\nSE. Once the encoder can map the noisy speech to the corresponding tokens of clean speech, or\nthe decoder has the error correction ability, speech enhancement can be achieved. Based on this\nobservation, our proposed self-supervised SE model training contains 2 steps.\nStep 1 (VQ-VAE training): Train a normal VQ-VAE using Eq. (3) with clean speech. After the\nVQ-VAE training converges, it will be served as a teacher model T. In addition, initialize a student\nmodel S from the weights of the teacher model. The self-distillation (Zhang et al., 2019) mechanism\nwill be used for the next training step.\nStep 2-1 (Adversarial attack): To further improve the robustness of the student model, its encoder\nand decoder are fine-tuned by adversarial training (AT) (Goodfellow et al., 2014; Bai et al., 2021)\nwith its codebook being fixed.\nInstead of adding some predefined noise that follows a certain probability distribution (e.g., Gaussian\nnoise) to the input clean speech, the adversarial noise is applied, which is the most confusing noise to\nthe model for making incorrect token predictions. Given a clean speech X and the encoder of the\nteacher model Tenc, its quantized token ZTq can be obtained using Eq. (1). The adversarial noise δ of\nthe encoder of the student model Senc can be found by solving the following optimization problem:\nmax\nδ\nLce(Senc(X + δ), ZTq|C)\n(5)\nBecause the token selection is based on the distance between the encoder output and the candidates\nin the dictionary C, (i.e., Eq. (1)), we can formulate this process as a probability distribution based\non the distance and softmax operation (e.g., if the distance is smaller, it is more likely to be chosen).\nTherefore, the cross-entropy loss Lce in Eq. (5) can be calculated as :\nLce = − 1\nT\nT\nX\nt=1\nlog(\nexp(−||(Senc(X + δ)t − ZTqt||2)\nPV\nv=1 exp(−||(Senc(X + δ)t − Cv||2)\n)\n(6)\nThe obtained noise δ when adding to the clean speech X, will maximize the cross-entropy loss\nbetween tokens from the student and teacher model.\nStep 2-2 (Adversarial training): To improve the robustness of the encoder part of the student model,\nthe adversarial attacked input X + δ will be fed into the student model and the weights are updated to\nminimize the cross-entropy loss between its token predictions and the ground truth tokens provided\nby the teacher model (with clean speech as input) using the following loss function:\nmin\nSenc Lce(Senc(X + δ), ZTq|C)\n(7)\nIn addition, to obtain a robust decoder of the student model, an L1 loss between clean speech and the\ndecoder output (with adversarial attacked tokens as inputs) is also applied. Experimental results show\nthat this will slightly improve the overall performance.\nSteps 2-1 and 2-2 will be alternatively applied, and the student model serves as the final SE model.\n3\nEXPERIMENTS\n3.1\nTEST SETS AND BASELINES FOR SPEECH QUALITY ESTIMATION\nThe test set used for the speech quality estimation was obtained from the Conferencing Speech 2022\nChallenge (Yi et al., 2022). First, we randomly sampled 200 clips from IU Bloomington COSINE\n5\nPublished as a conference paper at ICLR 2024\n(IUB_cosine) (Stupakov et al., 2009) and VOiCES (IUB_voices) (Richey et al., 2018), individually.\nFor the VOiCES dataset, acoustic conditions such as foreground speech (reference), low-pass filtered\nreference (anchor), and reverberants were included. For the COSINE dataset, close-talking mic\n(reference) and chest or shoulder mic (noisy) data were provided. The second source of the test set was\nthe Tencent corpus, which included Chinese speech with (Tencent_wR) and without (Tencent_woR)\nreverberation. In the without-reverberation condition, speech clips were artificially added with some\ndamage to simulate a scenario that may be encountered in an online meeting (e.g., background noise,\nhigh-pass/low-pass filtering, amplitude clipping, codec processing, noise suppression, and packet\nloss concealment). In the reverberation condition, simulated reverberation and speech recorded in a\nrealistic reverberant room were provided. We randomly sampled 250 clips from each subset. A list of\nsampled clips will be released to facilitate model comparison. The VoiceBank-DEMAND noisy test\nset (Valentini-Botinhao et al., 2016) was selected as the validation set. Because it does not come with\nquality labels, we set the training stop point when the VQScore reached the highest correlation with\nits DNSMOS P.835 (OVRL) (Reddy et al., 2022).\nTwo supervised quality estimators, DNSMOS P.835 and TorchaudioSquim (MOS) (Kumar et al.,\n2023), were used for baseline comparison. DNSMOS P.835 provided three audio scores: speech\nquality (SIG), background noise quality (BAK), and overall quality (OVRL). OVRL was selected\nas the baseline because it had a higher correlation with the real quality scores in our preliminary\nexperiments. The SpeechLMScore was selected as the baseline for the self-supervised method.\n3.2\nEXPERIMENTAL RESULTS OF SPEECH QUALITY ESTIMATION\nThe training data used to train our VQ-VAE for quality estimation was the LibriSpeech clean 460\nhours (Panayotov et al., 2015). The model structure is shown in Figure 1, where the codebook\nsize V and code dimension d are set to (2048, 32) and (c1, c2)=(128, 64). We first calculated\nthe conventional objective quality metrics (i.e., SNR, PESQ, and STOI) and DNSMOS P.835 on\nthe validation set (VoiceBank-DEMAND noisy test set). We then calculated the linear correlation\ncoefficient (Pearson, 1920) between those scores with SpeechLMScore and the proposed VQScore.\nThe experimental results are presented in Table 1. From this table, we can observe that, for metrics\nrelated to human perception, the VQScore calculated in the code space (z) generally performs much\nbetter than that calculated in the signal space (x). Our VQScore(cos,z) had a very high correlation with\nDNSMOS P.835 (BAK), implying that it has a superior ability to detect noise. It also outperformed\nSpeechLMScore across all metrics.\nNext, as shown in Table 2, we compare the correlation between different quality estimators and real\nquality scores on the test set (the scatter plots are shown in Section C of Appendix). TorchaudioSquim\n(MOS) did not perform as well as DNSMOS, possibly due to limited training data and domain mis-\nmatch (its training data, BVCC (Cooper & Yamagishi, 2021) came from text-to-speech and the voice\nconversion challenge). In contrast, the proposed VQScore was competitive with DNSMOS, although\nNO quality labels were required during training. The VQScore also outperformed SpeechLMScore,\npossibly because the SpeechLMScore is based on the perplexity of the language model, so minor\ndegradation or noise may not change the output of the tokenizer and the following language model.\nNote that the training data of our VQScore is only based on LibriSpeech clean 460 hours which is a\nsubset (roughly 460/(960+5,600) ≈ 7%) used to train SpeechLMScore. The proposed framework\ncan also be used for frame-level SNR estimation as discussed in the section D of Appendix.\n3.3\nTEST SETS AND BASELINES FOR SPEECH ENHANCEMENT\nThe test sets used for evaluating different speech enhancement models came from three sets: the\nVoiceBank-DEMAND noisy test set, DNS1 test set (Reddy et al., 2020), and DNS3 test set (Reddy\net al., 2021a).\nThe VoiceBank-DEMAND noisy test set is a relatively simple dataset for SE because only two\nspeakers and additive noise are included. In contrast, the blind test set in DNS1 covers different\nacoustic conditions, such as noisy speech without reverberation (noreverb), noisy reverberant speech\n(reverb), and noisy real recordings (real). The DNS3 test set can be divided into subcategories based\non realness (real or synthetic) and language (English or non-English).\n6\nPublished as a conference paper at ICLR 2024\nTable 1: Linear correlation coefficient between different objective metrics and the proposed VQScore\non the VoiceBank-DEMAND noisy test set (Valentini-Botinhao et al., 2016). For metrics related to\nhuman perception, VQScore(cos,z) performs much better than VQScore(cos,x).\nSpeechLMScore\n(Maiti et al., 2022)\nVQScore(cos,x)\nVQScore(cos,z)\nSNR\n0.4806\n0.5177\n0.5327\nPESQ\n0.5940\n0.6514\n0.7941\nSTOI\n0.6023\n0.5451\n0.7490\nDNSMOS P.835 (SIG)\n0.5310\n0.4051\n0.5620\nDNSMOS P.835 (BAK)\n0.7106\n0.6836\n0.8773\nDNSMOS P.835 (OVR)\n0.7045\n0.6370\n0.8386\nTable 2: Linear correlation coefficient between real quality scores and different quality estimators on\ndifferent test sets.\nSupervised training\nSelf-Supervised training\nDNSMOS P.835\n(Reddy et al., 2022)\nTorchaudioSquim\n(Kumar et al., 2023)\nSpeechLMScore\n(Maiti et al., 2022)\nVQScore(cos,z)\nTencent_wR\n0.6566\n0.4040\n0.5910\n0.5865\nTencent_woR\n0.7769\n0.5025\n0.7079\n0.7159\nIUB_cosine\n0.3938\n0.3991\n0.3913\n0.4880\nIUB_voices\n0.8181\n0.6984\n0.6891\n0.8604\nFor comparison with our self-supervised SE model, two signal-processing-based methods, MMSE\n(Ephraim & Malah, 1984) and Wiener filter (Loizou, 2013) were included as baselines. Noisy-target\ntraining (NyTT) (Fujimura et al., 2021) and MetricGAN-U (Fu et al., 2022) are two approaches that\nare different from conventional supervised SE model training. NyTT does not need noisy and clean\ntraining pairs, it creates training pairs by adding noise to noisy speech. The noise-added signal and\noriginal noisy speech are used as the model input and target, respectively. MetricGAN-U is trained on\nnoisy speech with the loss from the DNSMOS model (which actually requires extra (speech, MOS)\npairs data for training). For the supervised baselines, the first one is CNN-Transformer, which has the\nsame model structure as our self-supervised-based model except for the removal of the VQ module.\nAnother model that achieved good results on the VoiceBank-DEMAND noisy test set was also\nselected: Demucs (Defossez et al., 2020) is an SE model that operates in the waveform domain. Our\nself-supervised-based SE model is VQ-VAE trained only on clean speech with (V , d, c1, c2)=(4096,\n128, 200, 150).\n3.4\nEXPERIMENTAL RESULTS OF SPEECH ENHANCEMENT\n3.4.1\nSPEECH ENHANCEMENT RESULTS OF MATCHED AND MISMATCHED CONDITIONS\nTo make a fair comparison with the supervised baselines, we provide the results of our self-supervised\nSE model trained only on the clean speech of the VoiceBank-DEMAND training set (i.e., the\ncorresponding noisy speech is NOT used during our model training). In Table 3, we present the\nresults for the matched condition, in which the training and evaluation sets were from the same\nsource. Compared with noisy speech and traditional signal-processing-based methods, Proposed +\nAT showed a significant improvement in SIG, BAK, and OVRL. As expected, the effect of AT is\n1Note that several recent papers have shown that PESQ can not reflect the true speech quality, especially for\nspeech generated by a generative model (such as GAN Kumar et al. (2020); Liu et al. (2022), vocoder Maiti\n& Mandel (2020); Li & Yamagishi (2020); Du et al. (2020), diffusion model Serrà et al. (2022), etc.) We also\nobserve that the PESQ scores of enhanced speech generated by models with VQ usually CANNOT reflect the\ntrue speech quality. This is mainly because the discrete tokens in VQ are shared by similar sounds, which makes\nthe generated speech have less fidelity. However, as pointed out by DNSMOS and the following subjective\nlistening test, this does not imply its generated speech has lower quality.\n2Comes from several different data sources.\n7\nPublished as a conference paper at ICLR 2024\nTable 3: Comparison of different SE models on the VoiceBank-DEMAND noisy test set. Training\ndata comes from the training set of VoiceBank-DEMAND. The underlined numbers represent the\nbest results for the supervised models. The bold numbers represent the best results for the models\nthat do not need (noisy, clean) training data pairs.\nModel\nTraining data\nPESQ1\nSIG\nBAK\nOVRL\nClean\n-\n4.64\n3.463\n3.961\n3.152\nNoisy\n-\n1.97\n3.273\n2.862\n2.524\nCNN-Transformer\n(noisy, clean) pairs\n2.79\n3.389\n3.927\n3.070\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n2.95\n3.436\n3.951\n3.123\nNyTT (Fujimura et al., 2021)\n(noisy speech, noise) 2\n2.30\n3.444\n3.106\n2.736\nMetricGAN-U (Fu et al., 2022)\nnoisy speech\n+ DNSMOS model\n2.13\n3.200\n3.400\n2.660\nMMSE (Ephraim & Malah, 1984)\n-\n2.19\n3.215\n3.089\n2.566\nWiener (Loizou, 2013)\n-\n2.23\n3.208\n2.983\n2.501\nProposed\nclean speech\n2.20\n3.329\n3.646\n2.876\nProposed + AT\nclean speech\n2.38\n3.300\n3.838\n2.941\nmainly to make the encoder more robust to noise, and hence boost the model’s denoise ability (PESQ\nand BAK improve by 0.18 and 0.192, respectively). Compared to NyTT and MetricGAN-U, our\nProposed + AT has significant improvement in PESQ, BAK, and OVRL. Both CNN-Transformer and\nDemucs can generate speech with good quality (in terms of the DNSMOS scores) under this matched\ncondition.\nTo evaluate the model’s generalization ability, we compared their performance under mismatched\nconditions, where the training and testing sets originated from different sources. Table 4 lists the\nresults for the DNS1 test set. Although the performance of CNN-Transformer is worse than Demucs\nin the matched condition (Table 3), it generally performs better in this mismatched condition. In\naddition, it can be observed that even though adding adversarial noise or Gaussian noise on the clean\ninput for self-supervised model training can further improve the scores of all the evaluation metrics,\nthe improvement from adversarial noise was more prominent. For the OVRL scores, Proposed + AT\nwas competitive with the supervised CNN-Transformer, and outperformed Demucs, especially in the\nmore mismatched cases (Real and Reverb cases). The experimental results for DNS3 are presented in\nSection E of Appendix. The same trend appeared as in the case of DNS1: the proposed model with\nAT can significantly outperform Demucs in the more mismatched cases (i.e., Real and non-English).\n3.4.2\nRESULTS OF LISTENING TEST\nSince objective evaluation metrics may not consistently capture the genuine perceptual experience,\nwe conducted a subjective listening test. In order to assess the subjective perception, we compare our\nProposed + AT with noisy, Wiener, CNN-Transformer, and Demucs. For each acoustic condition\n(real, noreverb, and reverb), 8 samples were randomly selected from the test set, amounting to a\ntotal of 8 × 5 (different enhancement methods and noisy) × 3 (acoustic conditions) = 120 utterances\nthat each listener was required to evaluate. For each signal, the listener rated the speech quality\n(SIGsub), background noise removal (BAKsub), and the overall quality (OV RLsub) follows ITU-T\nP.835. 17 listeners participated in the study. Table 5 shows the results of the listening test. It can\nbe observed that in every scenario, our Proposed + AT exhibits the best noise reduction capability\n(highest BAKsub score). On the other hand, our method has larger speech distortion (lower SIGsub\nscore) compared to the CNN-Transformer, which has the same model structure but is trained in a\nsupervised way. In terms of OV RLsub, our Proposed + AT is competitive with the CNN-Transformer\nand outperforms other baselines. These results verify that our self-supervised model has better\ngeneralization capability than Demucs and is comparable to CNN-Transformer.\n4\nCONCLUSION\nIn this study, we propose a novel self-supervised speech quality estimator trained only on clean\nspeech. Motivated by anomaly detection, if the input speech has a different pattern from that of the\n8\nPublished as a conference paper at ICLR 2024\nTable 4: Comparison of different SE models on the DNS1 test set. Training data comes from the\ntraining set of VoiceBank-DEMAND.\nSubset\nModel\nTraining data\nSIG\nBAK\nOVRL\nNoisy\n-\n3.173\n2.367\n2.238\nCNN-Transformer\n(noisy, clean) pairs\n3.074\n3.339\n2.620\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n3.073\n3.335\n2.570\nReal\nWiener (Loizou, 2013)\n-\n3.207\n2.579\n2.313\nProposed\nclean speech\n3.095\n3.365\n2.589\nProposed + Gaussian\nclean speech\n3.152\n3.458\n2.673\nProposed + AT\nclean speech\n3.156\n3.640\n2.750\nNoisy\n-\n3.492\n2.577\n2.513\nCNN-Transformer\n(noisy, clean) pairs\n3.515\n3.786\n3.124\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n3.535\n3.651\n3.073\nNoreverb\nWiener (Loizou, 2013)\n-\n3.311\n2.747\n2.447\nProposed\nclean speech\n3.463\n3.764\n3.066\nProposed + Gaussian\nclean speech\n3.484\n3.830\n3.115\nProposed + AT\nclean speech\n3.481\n3.960\n3.162\nNoisy\n-\n2.057\n1.576\n1.504\nCNN-Transformer\n(noisy, clean) pairs\n2.849\n3.352\n2.409\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n2.586\n3.260\n2.175\nReverb\nWiener (Loizou, 2013)\n-\n2.649\n2.251\n1.838\nProposed\nclean speech\n2.911\n3.097\n2.325\nProposed + Gaussian\nclean speech\n2.930\n3.222\n2.394\nProposed + AT\nclean speech\n2.949\n3.361\n2.456\nTable 5: Listening test results of different SE models on the DNS1 test set.\nSubset\nModel\nSIGsub\nBAKsub\nOV RLsub\nNoisy\n3.890\n2.294\n2.809\nCNN-Transformer\n3.537\n2.801\n3.044\nReal\nDemucs (Defossez et al., 2020)\n2.890\n2.515\n2.515\nWiener (Loizou, 2013)\n3.787\n2.250\n2.868\nProposed + AT\n3.272\n2.978\n3.000\nNoisy\n3.765\n2.059\n2.647\nCNN-Transformer\n3.706\n2.809\n3.088\nNoreverb\nDemucs (Defossez et al., 2020)\n3.676\n2.779\n3.051\nWiener (Loizou, 2013)\n3.404\n2.147\n2.654\nProposed + AT\n3.404\n3.162\n3.132\nNoisy\n3.169\n1.691\n2.176\nCNN-Transformer\n2.610\n2.632\n2.382\nReverb\nDemucs (Defossez et al., 2020)\n1.588\n1.934\n1.515\nWiener (Loizou, 2013)\n2.963\n2.015\n2.250\nProposed + AT\n2.522\n2.721\n2.382\nclean speech, the reconstruction error may be larger. Instead of directly computing the error in the\nsignal domain, we find that it can provide a higher correlation with other objective and subjective\nscores when the distance is calculated in the code space (i.e., the quantization error) of the VQ-\nVAE. Although no quality labels are required during model training, the correlation coefficient\nbetween the real quality scores and the proposed VQScore is competitive with that of the supervised\nestimators. Next, under the VQ-VAE framework, the key to self-supervised speech enhancement is\nthe robustness of the encoder and decoder. Therefore, a novel self-distillation mechanism combined\nwith adversarial training is proposed which can achieve good SE results without the need for any\n(noisy, clean) speech training pairs. Both the objective and subjective experimental results show\nthat the proposed self-supervised framework is competitive with that of supervised SE models under\nmismatch conditions.\n9\nPublished as a conference paper at ICLR 2024\nREFERENCES\nSaad Abbasi, Mahmoud Famouri, Mohammad Javad Shafiee, and Alexander Wong. Outliernets:\nHighly compact deep autoencoder network architectures for on-device acoustic anomaly detection.\nSensors, 21(14):4805, 2021.\nJinwon An and Sungzoon Cho. Variational autoencoder based anomaly detection using reconstruction\nprobability. Special lecture on IE, 2(1):1–18, 2015.\nTao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training\nfor adversarial robustness. arXiv preprint arXiv:2102.01356, 2021.\nJohn G Beerends, Christian Schmidmer, Jens Berger, Matthias Obermann, Raphael Ullmann, Joachim\nPomy, and Michael Keyhl. Perceptual objective listening quality assessment (polqa), the third\ngeneration itu-t standard for end-to-end speech quality measurement part i—temporal alignment.\nJournal of the Audio Engineering Society, 61(6):366–384, 2013.\nAndrew A Catellier and Stephen D Voran. Wawenets: A no-reference convolutional waveform-based\napproach to estimating narrowband and wideband speech quality. In ICASSP 2020-2020 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 331–335.\nIEEE, 2020.\nMichael Chinen, Felicia SC Lim, Jan Skoglund, Nikita Gureev, Feargus O’Gorman, and Andrew\nHines. Visqol v3: An open source production ready objective speech and audio metric. In 2020\ntwelfth international conference on quality of multimedia experience (QoMEX), pp. 1–6. IEEE,\n2020.\nChung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. Self-supervised learning\nwith random-projection quantizer for speech recognition. In International Conference on Machine\nLearning, pp. 3915–3924. PMLR, 2022.\nErica Cooper and Junichi Yamagishi. How do voices from past speech synthesis challenges compare\ntoday? arXiv preprint arXiv:2105.02373, 2021.\nAlexandre Defossez, Gabriel Synnaeve, and Yossi Adi. Real time speech enhancement in the\nwaveform domain. arXiv preprint arXiv:2006.12847, 2020.\nXuan Dong and Donald S Williamson. An attention enhanced multi-task model for objective speech\nassessment in real-world environments. In ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pp. 911–915. IEEE, 2020.\nZhihao Du, Xueliang Zhang, and Jiqing Han. A joint framework of denoising autoencoder and\ngenerative vocoder for monaural speech enhancement. IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, 28:1493–1505, 2020.\nYariv Ephraim and David Malah. Speech enhancement using a minimum-mean square error short-\ntime spectral amplitude estimator. IEEE Transactions on acoustics, speech, and signal processing,\n32(6):1109–1121, 1984.\nSzu-Wei Fu, Yu Tsao, Hsin-Te Hwang, and Hsin-Min Wang. Quality-net: An end-to-end non-intrusive\nspeech quality assessment model based on blstm. arXiv preprint arXiv:1808.05344, 2018.\nSzu-Wei Fu, Cheng Yu, Kuo-Hsuan Hung, Mirco Ravanelli, and Yu Tsao. Metricgan-u: Unsupervised\nspeech enhancement/dereverberation based only on noisy/reverberated speech. In ICASSP 2022-\n2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp.\n7412–7416. IEEE, 2022.\nTakuya Fujimura, Yuma Koizumi, Kohei Yatabe, and Ryoichi Miyazaki. Noisy-target training: A\ntraining strategy for dnn-based speech enhancement without clean speech. In 2021 29th European\nSignal Processing Conference (EUSIPCO), pp. 436–440. IEEE, 2021.\nRitwik Giri, Fangzhou Cheng, Karim Helwani, Srikanth V Tenneti, Umut Isik, and Arvindh Kr-\nishnaswamy. Group masked autoencoder based density estimator for audio anomaly detection.\n2020.\n10\nPublished as a conference paper at ICLR 2024\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial\nexamples. arXiv preprint arXiv:1412.6572, 2014.\nTomoki Hayashi, Takenori Yoshimura, and Yusuke Adachi. Conformer-based id-aware autoencoder\nfor unsupervised anomalous sound detection. DCASE2020 Challenge, Tech. Rep., 2020.\nYuchen Hu, Chen Chen, Qiushi Zhu, and Eng Siong Chng. Wav2code: Restore clean speech\nrepresentations via codebook lookup for noise-robust asr. arXiv preprint arXiv:2304.04974, 2023.\nWen-Chin Huang, Erica Cooper, Yu Tsao, Hsin-Min Wang, Tomoki Toda, and Junichi Yamagishi.\nThe voicemos challenge 2022. arXiv preprint arXiv:2203.11389, 2022.\nJames M Kates and Kathryn H Arehart. The hearing-aid speech quality index (hasqi) version 2.\nJournal of the Audio Engineering Society, 62(3):99–117, 2014a.\nJames M Kates and Kathryn H Arehart. The hearing-aid speech perception index (haspi). Speech\nCommunication, 65:75–93, 2014b.\nMinje Kim. Collaborative deep learning for speech enhancement: A run-time model selection method\nusing autoencoders. In 2017 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 76–80. IEEE, 2017.\nAnurag Kumar, Ke Tan, Zhaoheng Ni, Pranay Manocha, Xiaohui Zhang, Ethan Henderson, and Buye\nXu. Torchaudio-squim: Reference-less speech quality and intelligibility measures in torchaudio.\narXiv preprint arXiv:2304.01448, 2023.\nRithesh Kumar, Kundan Kumar, Vicki Anand, Yoshua Bengio, and Aaron Courville. Nu-gan: High\nresolution neural upsampling with gan. arXiv preprint arXiv:2010.11362, 2020.\nJonathan Le Roux, Scott Wisdom, Hakan Erdogan, and John R Hershey. Sdr–half-baked or well\ndone? In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pp. 626–630. IEEE, 2019.\nYichong Leng, Xu Tan, Sheng Zhao, Frank Soong, Xiang-Yang Li, and Tao Qin. Mbnet: Mos predic-\ntion for synthesized speech with mean-bias network. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 391–395. IEEE, 2021.\nHao Li, DeLiang Wang, Xueliang Zhang, and Guanglai Gao. Frame-level signal-to-noise ratio\nestimation using deep learning. In INTERSPEECH, pp. 4626–4630, 2020.\nHaoyu Li and Junichi Yamagishi. Noise tokens: Learning neural noise templates for environment-\naware speech enhancement. arXiv preprint arXiv:2004.04001, 2020.\nHaohe Liu, Xubo Liu, Qiuqiang Kong, Qiao Tian, Yan Zhao, DeLiang Wang, Chuanzeng Huang,\nand Yuxuan Wang. Voicefixer: A unified framework for high-fidelity speech restoration. arXiv\npreprint arXiv:2204.05841, 2022.\nChen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, Xin Wang, Junichi Yamagishi, Yu Tsao, and Hsin-Min\nWang. Mosnet: Deep learning based objective assessment for voice conversion. arXiv preprint\narXiv:1904.08352, 2019.\nPhilipos C Loizou. Speech enhancement: theory and practice. CRC press, 2013.\nSoumi Maiti and Michael I Mandel. Speaker independence of neural vocoders and their effect on\nparametric resynthesis speech enhancement. In ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP), pp. 206–210. IEEE, 2020.\nSoumi Maiti, Yifan Peng, Takaaki Saeki, and Shinji Watanabe. Speechlmscore: Evaluating speech\ngeneration using speech language model. arXiv preprint arXiv:2212.04559, 2022.\nPranay Manocha and Anurag Kumar. Speech quality assessment through mos using non-matching\nreferences. arXiv preprint arXiv:2206.12285, 2022.\nPranay Manocha, Buye Xu, and Anurag Kumar. Noresqa: A framework for speech quality assessment\nusing non-matching references. Advances in Neural Information Processing Systems, 34, 2021.\n11\nPublished as a conference paper at ICLR 2024\nHelard Martinez, Mylène CQ Farias, and Andrew Hines. Navidad: A no-reference audio-visual\nquality metric based on a deep autoencoder. In 2019 27th European Signal Processing Conference\n(EUSIPCO), pp. 1–5. IEEE, 2019.\nGabriel Mittag, Babak Naderi, Assmaa Chehadi, and Sebastian Möller. Nisqa: A deep cnn-self-\nattention model for multidimensional speech quality prediction with crowdsourced datasets. arXiv\npreprint arXiv:2104.09494, 2021.\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: an asr corpus\nbased on public domain audio books. In 2015 IEEE international conference on acoustics, speech\nand signal processing (ICASSP), pp. 5206–5210. IEEE, 2015.\nKarl Pearson. Notes on the history of correlation. Biometrika, 13(1):25–45, 1920.\nPedro José Pereira, Gabriel Coelho, Alexandrine Ribeiro, Luís Miguel Matos, Eduardo C Nunes,\nAndré Ferreira, André Pilastri, and Paulo Cortez. Using deep autoencoders for in-vehicle audio\nanomaly detection. Procedia Computer Science, 192:298–307, 2021.\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever.\nRobust speech recognition via large-scale weak supervision. In International Conference on\nMachine Learning, pp. 28492–28518. PMLR, 2023.\nChandan KA Reddy, Vishak Gopal, Ross Cutler, Ebrahim Beyrami, Roger Cheng, Harishchandra\nDubey, Sergiy Matusevych, Robert Aichner, Ashkan Aazami, Sebastian Braun, et al. The in-\nterspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and\nchallenge results. arXiv preprint arXiv:2005.13981, 2020.\nChandan KA Reddy, Harishchandra Dubey, Kazuhito Koishida, Arun Nair, Vishak Gopal, Ross\nCutler, Sebastian Braun, Hannes Gamper, Robert Aichner, and Sriram Srinivasan. Interspeech\n2021 deep noise suppression challenge. arXiv preprint arXiv:2101.01902, 2021a.\nChandan KA Reddy, Vishak Gopal, and Ross Cutler. Dnsmos: A non-intrusive perceptual objective\nspeech quality metric to evaluate noise suppressors. In ICASSP 2021-2021 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6493–6497. IEEE, 2021b.\nChandan KA Reddy, Vishak Gopal, and Ross Cutler. Dnsmos p. 835: A non-intrusive perceptual\nobjective speech quality metric to evaluate noise suppressors.\nIn ICASSP 2022-2022 IEEE\nInternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 886–890.\nIEEE, 2022.\nAlexandrine Ribeiro, Luis Miguel Matos, Pedro José Pereira, Eduardo C Nunes, André L Ferreira,\nPaulo Cortez, and André Pilastri. Deep dense and convolutional autoencoders for unsupervised\nanomaly detection in machine condition sounds. arXiv preprint arXiv:2006.10417, 2020.\nColleen Richey, Maria A Barrios, Zeb Armstrong, Chris Bartels, Horacio Franco, Martin Graciarena,\nAaron Lawson, Mahesh Kumar Nandwana, Allen Stauffer, Julien van Hout, et al. Voices obscured\nin complex environmental settings (voices) corpus. arXiv preprint arXiv:1804.05053, 2018.\nAntony W Rix, John G Beerends, Michael P Hollier, and Andries P Hekstra. Perceptual evaluation\nof speech quality (pesq)-a new method for speech quality assessment of telephone networks and\ncodecs. In 2001 IEEE international conference on acoustics, speech, and signal processing.\nProceedings (Cat. No. 01CH37221), volume 2, pp. 749–752. IEEE, 2001.\nYossi Rubner, Carlo Tomasi, and Leonidas J Guibas. The earth mover’s distance as a metric for\nimage retrieval. International journal of computer vision, 40(2):99, 2000.\nJoan Serrà, Santiago Pascual, Jordi Pons, R Oguz Araz, and Davide Scaini. Universal speech\nenhancement with score-based diffusion. arXiv preprint arXiv:2206.03065, 2022.\nMeet H Soni and Hemant A Patil. Novel deep autoencoder features for non-intrusive speech quality\nassessment. In 2016 24th European Signal Processing Conference (EUSIPCO), pp. 2315–2319.\nIEEE, 2016.\n12\nPublished as a conference paper at ICLR 2024\nAlex Stupakov, Evan Hanusa, Jeff Bilmes, and Dieter Fox. Cosine-a corpus of multi-party conversa-\ntional speech in noisy environments. In 2009 IEEE International Conference on Acoustics, Speech\nand Signal Processing, pp. 4153–4156. IEEE, 2009.\nCees H Taal, Richard C Hendriks, Richard Heusdens, and Jesper Jensen. An algorithm for intelligi-\nbility prediction of time–frequency weighted noisy speech. IEEE Transactions on Audio, Speech,\nand Language Processing, 19(7):2125–2136, 2011.\nWei-Cheng Tseng, Chien-yu Huang, Wei-Tsung Kao, Yist Y Lin, and Hung-yi Lee. Utilizing\nself-supervised representations for mos prediction. arXiv preprint arXiv:2104.03017, 2021.\nWei-Cheng Tseng, Wei-Tsung Kao, and Hung-yi Lee. Ddos: A mos prediction framework utilizing\ndomain adaptive pre-training and distribution of opinion scores. arXiv preprint arXiv:2204.03219,\n2022.\nDmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing\ningredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016.\nCassia Valentini-Botinhao, Xin Wang, Shinji Takaki, and Junichi Yamagishi. Investigating rnn-based\nspeech enhancement methods for noise-robust text-to-speech. In SSW, pp. 146–152, 2016.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nJing Wang, Yahui Shan, Xiang Xie, and Jingming Kuang. Output-based speech quality assessment\nusing autoencoder and support vector regression. Speech Communication, 110:13–20, 2019.\nYu-Che Wang, Shrikant Venkataramani, and Paris Smaragdis. Self-supervised learning for speech\nenhancement. arXiv preprint arXiv:2006.10388, 2020.\nBing Xu, Naiyan Wang, Tianqi Chen, and Mu Li. Empirical evaluation of rectified activations in\nconvolutional network. arXiv preprint arXiv:1505.00853, 2015.\nZiyi Xu, Maximilian Strake, and Tim Fingscheidt.\nDeep noise suppression maximizing non-\ndifferentiable pesq mediated by a non-intrusive pesqnet. IEEE/ACM Transactions on Audio,\nSpeech, and Language Processing, 2022.\nGaoxiong Yi, Wei Xiao, Yiming Xiao, Babak Naderi, Sebastian Möller, Gabriel Mittag, Ross Cutler,\nZhuohuang Zhang, Donald S Williamson, Fei Chen, et al. Conferencingspeech 2022 challenge\nevaluation plan. 2022.\nJiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong\nXu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan.\narXiv preprint arXiv:2110.04627, 2021a.\nMeng Yu, Chunlei Zhang, Yong Xu, Shixiong Zhang, and Dong Yu. Metricnet: Towards improved\nmodeling for non-intrusive speech quality assessment. arXiv preprint arXiv:2104.01227, 2021b.\nNeil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Sound-\nstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 30:495–507, 2021.\nRyandhimas E Zezario, Szu-Wei Fu, Chiou-Shann Fuh, Yu Tsao, and Hsin-Min Wang. Stoi-net: A\ndeep learning based non-intrusive speech intelligibility assessment model. In 2020 Asia-Pacific\nSignal and Information Processing Association Annual Summit and Conference (APSIPA ASC), pp.\n482–486. IEEE, 2020.\nRyandhimas E Zezario, Szu-Wei Fu, Fei Chen, Chiou-Shann Fuh, Hsin-Min Wang, and Yu Tsao.\nDeep learning-based non-intrusive multi-objective speech assessment model with cross-domain\nfeatures. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 31:54–70, 2022.\nLinfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen, Chenglong Bao, and Kaisheng Ma. Be your\nown teacher: Improve the performance of convolutional neural networks via self distillation. In\nProceedings of the IEEE/CVF international conference on computer vision, pp. 3713–3722, 2019.\n13\nPublished as a conference paper at ICLR 2024\nAppendix\nA\nLEARNING CURVES ON VOICEBANK-DEMAND NOISY TEST SET FOR\nSPEECH QUALITY ESTIMATION\n0\n10\n20\n30\n40\n50\n60\n70\n0.45\n0.5\n0.55\n0.6\n0.65\n0.7\n0.75\n0.8\n0.85\n0.9\nNumber of iterations (× 10k)\nCorrelation coefficient\n \n \nBAK\nOVR\nPESQ\nSIG\nSNR\nFigure 2: Learning curves of the correlation coefficient between various objective metrics and the\nproposed VQScore(cos,z) on the VoiceBank-DEMAND noisy test set (Valentini-Botinhao et al.,\n2016).\nFigure 2 presents the learning curves of the correlation coefficient between various objective metrics\nand the proposed VQScore(cos,z) on the noisy test set of VoiceBank-DEMAND. The figure illustrates\na general trend of increasing correlation coefficient with the number of iterations for most objective\nmetrics. Notably, our VQScore(cos,z) exhibited exceptionally high correlations with BAK, OVR, and\nPESQ.\nB\nCOMPARISON OF DIFFERENT VQSCORES\nTable 6: Linear correlation coefficient between real quality scores and various VQScores on different\ntest sets.\nVQScore(L2,x)\nVQScore(L2,z)\nVQScore(cos,x)\nVQScore(cos,z)\nTencent_wR\n-0.0081\n-0.3709\n0.0988\n0.5865\nTencent_woR\n0.4925\n-0.5983\n0.5636\n0.7159\nIUB_cosine\n0.0320\n-0.4266\n0.1819\n0.4880\nIUB_voices\n0.1764\n-0.8436\n0.6943\n0.8604\nIn Section 2.4, we discussed four combinations of distance metrics and targets for calculating\nVQScore. Table 6 presents the correlation coefficients between real quality scores and various\nVQScores on different test sets. It is worth noting that VQScores using L2 as the distance metric\nare expected to exhibit a negative correlation with true quality scores (i.e., a larger distance implies\npoorer speech quality). The results demonstrate that employing cosine similarity in the code space\n(z) can significantly outperform the other alternatives.\n14\nPublished as a conference paper at ICLR 2024\n(a) SIG\n(b) BAK\n(c) OVR\n(d) PESQ\nFigure 3: Scatter plots between various objective metrics and the proposed VQScore(cos,z) on the\nVoiceBank-DEMAND noisy test set. (a) SIG, (b) BAK, (c) OVR, and (d) PESQ.\nC\nSCATTER PLOTS FOR SPEECH QUALITY ESTIMATION\nFigure 3 illustrates the scatter plots between the proposed VQScore(cos,z) and various objective\nmetrics on the noisy test set of VoiceBank-DEMAND. From Figure 3 (a), it can be observed that the\ncorrelation between VQScore(cos,z) and SIG is low, particularly when the value of SIG is high. On\nthe other hand, Figure 3 (d) reveals a low correlation between VQScore(cos,z) and PESQ when the\nvalue of PESQ is low. These findings suggest that modeling quality scores in extreme cases may\npresent greater challenges. Furthermore, Figure 4 displays the scatter plots between the proposed\nVQScore(cos,z) and real subjective quality scores. Similar trends can be found in Figure 4 (c) and (d),\nindicating a low correlation between VQScore(cos,z) and real scores when the speech quality is poor.\nD\nFRAME-LEVEL SNR ESTIMATOR\nMost machine-learning-based quality estimators are black-box, so people find it hard to understand\nthe reason for their evaluation. On the other hand, from the definition of VQScore (Eq. 4), we can\nobserve that the utterance score is based on summing up all the similarity scores of every frame.\nWe, therefore, want to further verify that the proposed method can localize the frames where quality\ndegrades (i.e., due to noise or speech distortion, etc.) in an utterance. Because most off-the-shelf\nmetrics cannot provide such scores on a frame basis, here we use frame-level SNR as the ground truth.\nGiven a synthetic noisy utterance, the frame-level SNR is calculated on the magnitude spectrogram\nfor each frame. Since our preliminary experiments suggest that calculating the L2 distance in the\nsignal space has a higher correlation with SNR, here we define the predicted frame-based quality as:\n15\nPublished as a conference paper at ICLR 2024\n(a) IUB_cosine\n(b) IUB_voices\n(c) Tencent_woR\n(d) Tencent_wR\nFigure 4: Scatter plots between real subjective quality scores and the proposed VQScore(cos,z) on (a)\nIUB_cosine, (b) IUB_voices, (c) Tencent_woR, and (d) Tencent_wR.\nTable 7: Average linear correlation coefficient between frame-level SNR and the proposed method on\nthe VoiceBank-DEMAND noisy test set.\nSupervised\nSelf-Supervised\nLi et al. (2020)\nProposed\nFrame-level SNR\n0.721\n0.789\n||Xt||2\n||Xt − ˆXt||2\n(8)\nThe denominator is used to measure the reconstruction error and the numerator is for normalization.\nIn this experiment, we train the VQ-VAE with L2 loss and evaluate the average correlation coefficient\nwith ground-truth frame-level SNR on the VoiceBank-DEMAND noisy test set. Table 7 shows that\nour proposed metric can achieve a higher correlation than the supervised baseline (Li et al., 2020),\nwhich uses frame-level SNR as the model’s training target. Figure 5 shows examples from the\nVoiceBank-DEMAND noisy test set, featuring spectrograms alongside their corresponding frame-\nlevel SNR and predicted frame-level quality using Eq. (8). Comparing Figure 5 (c) with (e), and (d)\nwith (f), it can be observed that the overall shapes are similar, indicating a high correlation between\nthem.\n16\nPublished as a conference paper at ICLR 2024\n20\n40\n60\n80\n100\n120\n140\n50\n100\n150\n200\n250\n(a)\n20\n40\n60\n80\n100\n120\n140\n160\n50\n100\n150\n200\n250\n(b)\n20\n40\n60\n80\n100\n120\n140\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nFrame index\nFrame−level SNR\n(c)\n20\n40\n60\n80\n100\n120\n140\n160\n−20\n−15\n−10\n−5\n0\n5\n10\n15\n20\nFrame index\nFrame−level SNR\n(d)\n20\n40\n60\n80\n100\n120\n140\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\nFrame index\nPredicted frame−level quality\n(e)\n20\n40\n60\n80\n100\n120\n140\n160\n1\n1.1\n1.2\n1.3\n1.4\n1.5\n1.6\n1.7\n1.8\n1.9\n2\nFrame index\nPredicted frame−level quality\n(f)\nFigure 5: Examples of spectrogram, its corresponding frame-level SNR and the predicted frame-level\nquality. (c) and (d) are the frame-level SNR. (e) and (f) are our predicted frame-level quality.\nE\nSPEECH ENHANCEMENT RESULTS ON THE DNS3 TEST SET\nTable 8 displays the speech enhancement results on the DNS3 test set. Similar to the findings in\nDNS1 test set, it can be observed that applying AT in our model training can also further improve the\nscores of all the evaluation metrics. In addition, Proposed + AT can outperform Demucs, especially\nin the more mismatched conditions (i.e., Real or non-English cases).\n17\nPublished as a conference paper at ICLR 2024\nTable 8: Comparison of different SE models on the DNS3 test set. Training data comes from the\ntraining set of VoiceBank-DEMAND.\nSubset\nModel\nTraining data\nSIG\nBAK\nOVRL\nNoisy\n-\n3.094\n2.178\n2.078\nCNN-Transformer\n(noisy, clean) pairs\n2.887\n3.421\n2.468\nReal\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n2.749\n3.316\n2.325\nEnglish\nWiener (Loizou, 2013)\n-\n3.057\n2.361\n2.125\nProposed\nclean speech\n2.844\n3.157\n2.305\nProposed + AT\nclean speech\n2.888\n3.468\n2.456\nNoisy\n-\n3.154\n3.000\n2.487\nCNN-Transformer\n(noisy, clean) pairs\n3.183\n3.644\n2.798\nReal\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n2.842\n3.442\n2.451\nnon-English\nWiener (Loizou, 2013)\n-\n3.142\n3.099\n2.489\nProposed\nclean speech\n3.120\n3.602\n2.733\nProposed + AT\nclean speech\n3.179\n3.726\n2.820\nNoisy\n-\n3.165\n2.597\n2.300\nCNN-Transformer\n(noisy, clean) pairs\n3.053\n3.590\n2.645\nSynthetic\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n2.716\n3.526\n2.374\nnon-English\nWiener (Loizou, 2013)\n-\n3.174\n2.749\n2.361\nProposed\nclean speech\n2.962\n3.334\n2.470\nProposed + AT\nclean speech\n3.019\n3.644\n2.627\nNoisy\n-\n3.501\n2.900\n2.646\nCNN-Transformer\n(noisy, clean) pairs\n3.470\n4.069\n3.214\nSynthetic\nDemucs (Defossez et al., 2020)\n(noisy, clean) pairs\n3.357\n3.929\n3.053\nEnglish\nWiener (Loizou, 2013)\n-\n3.411\n3.216\n2.736\nProposed\nclean speech\n3.365\n3.937\n3.062\nProposed + AT\nclean speech\n3.381\n4.039\n3.117\nF\nSPECTROGRAM COMPARISON OF ENHANCED SPEECH\nFigure 6 and 7 present examples of enhanced spectrograms obtained from various SE models in the\nReal and Reverb conditions from the DNS1 test set, respectively. These figures visually reveal that\nthe proposed self-supervised SE model exhibits good noise removal capabilities compared to other\nbaselines.\nG\nADVERSARIAL TRAINING’S LEARNING CURVES\nFigure 8 shows the adversarial training’s learning curves on the VoiceBank-DEMAND noisy test\nset. From the curves, we can observe that the process of AT is quite stable. The scores of most\nevaluation metrics (except for SIG) first gradually increase and then converge to a better optimum\ncompared to the initial (the result after Step 1 in Section 2.5). Compared to normal training, our AT\nonly needs another forward and backward pass of the computational graph (i.e., adversarial attack,\nEq. 5), therefore, the computation cost is roughly twice of normal training. However, as illustrated in\nthe learning curves, AT is efficient and can converges quickly.\nH\nDISTRIBUTION OF VQSCORE\nTo study the distribution of the VQScore, we first divide the test set into 3 subsets with equal size\nbased on the sorted MOS. The first and the third subset corresponds to the group of the lowest and\nhighest MOS, respectively. Figure 9 shows the histogram of the VQScore on the IUB test set, where\nblue and orange represent the first and the third set, respectively. Although there is some overlap in\nbetween for IUB_cosine, speech with higher MOS usually also have higher VQScore.\n18\nPublished as a conference paper at ICLR 2024\n(a) Noisy\n(b) Wiener\n(c) Demucs\n(d) Proposed + AT\nFigure 6: Spectrograms generated by different SE models. This utterance (realrec_fileid_10) is\nselected from the DNS1 Real test set. (a) Noisy, (b) Wiener, (c) Demucs, and (d) Proposed + AT.\n(a) Noisy\n(b) Wiener\n(c) Demucs\n(d) Proposed + AT\nFigure 7: Spectrograms generated by different SE models. This utterance (reverb_fileid_5) is selected\nfrom the DNS1 Reverb test set. (a) Noisy, (b) Wiener, (c) Demucs, and (d) Proposed + AT.\n19\nPublished as a conference paper at ICLR 2024\n(a) SIG\n(b) BAK\n(c) OVRL\n(d) PESQ\nFigure 8: Adversarial training’s learning curves on the VoiceBank-DEMAND noisy test set. The\nstarting point is the result after Step 1 in Section 2.5 (i.e., VQ-VAE trained on clean speech has\nconverged). (a) SIG, (b) BAK, (c) OVRL, and (d) PESQ.\nI\nSENSITIVITY TO HYPER-PARAMETERS\nIn this section, we study the effect of hyper-parameters on model performance. All the hyper-\nparameters were decided based on the performance of DNSMOS (OVRL) on the validation set.\nFor quality estimation, it is the LCC between DNSMOS (OVRL) and VQScore. For the speech\nenhancement, it is the score itself. We first investigate the influence of codebook size on quality\nestimation. From Table 9, we can observe that except for very small codebook dimensions (i.e., 16),\nthe performance is quite robust to codebook number and dimension. The case for speech enhancement\nis similar, except for very small codebook dimensions and numbers, the performance is robust to\nthe codebook setting. We next investigate the effect of setting different β in Eq. 3. Table 10 shows\nthat the SE performance is also robust to different β which aligns with the observation made in (Van\nDen Oord et al., 2017).\nTable 9: LCC between DNSMOS (OVRL) and VQScores under different codebook sizes.\nCodebook size (number, dim)\nLCC\n(1024, 32)\n0.8332\n(2048, 16)\n0.7668\n(2048, 32)\n0.8386\n(2048, 64)\n0.8317\n(4096, 32)\n0.8297\n20\nPublished as a conference paper at ICLR 2024\n(a) IUB_voices\n(b) IUB_cosine\nFigure 9: Histogram of VQScore. Blue and orange represent the sets with lower and higher MOS\nscores, respectively. (a) IUB_voices, (b) IUB_cosine.\nTable 10: DNSMOS (OVRL) of enhanced speech from our models trained with different β.\nβ\nDNSMOS (OVRL)\n1\n2.865\n2\n2.872\n3\n2.876\nJ\nMODEL COMPLEXITY\nIn this section, we compare model complexity based on the number of parameters and the number of\ncomputational operations as shown in Table 11. MACs stand for multiply–accumulate operation and\nare calculated based on 1 sec of audio input. Because NyTT doesn’t release the model, it is difficult\nto accurately estimate its model complexity. However, its model structure is based on CNN-BLSTM,\nso we can expect it to have higher model complexity compared to MetricGAN-U, which is based on\nsimple BLSTM. CNN-Transformer is the supervised version (and removing VQ) of our proposed\nmodel and hence has a similar model complexity. Demucs is a CNN encoder and decoder framework\nwith BLSTM in between to model temporal relationships. Because directly models the waveform, its\nmodel complexity is significantly higher than others.\nTable 11: Model complexity for the proposed approach and baselines.\nParams (M)\nMACs (G)\nCNN-Transformer\n2.51\n0.32\nDemucs\n60.81\n75.56\nMetricGAN-U\n1.90\n0.24\nNyTT\n-\n-\nProposed\n2.51\n0.32\nK\nSTATISTICAL SIGNIFICANCE\nIn Table 12, we report the T-test of DNSMOS (OVR) between Proposed + AT and different baselines\non the DNS1 test set to show the statistical significance. In the table, the results shown in bold\nrepresent Proposed + AT is statistically significant (p-value<0.05) better than the baseline. It can be\nobserved that Proposed + AT is significantly better than most of the baselines (noisy, Wiener, and\nDemucs) and is comparable to CNN-Transformer.\n21\nPublished as a conference paper at ICLR 2024\nTable 12: P-value of DNSMOS (OVR) between Proposed + AT and baselines on the DNS1 test set.\nNoisy\nWiener\nDemucs\nCNN-Transformer\nReal\n1.35e-36\n4.84e-31\n8.97e-07\n0.0001\nNoreverb\n7.84e-43\n1.18e-47\n0.0042\n0.141\nReverb\n9.20e-54\n8.70e-34\n3.21e-08\n0.205\nL\nASR RESULTS OF ENHANCED SPEECH\nIn this section, we apply Whisper-medium (Radford et al., 2023) as the ASR model and compute the\nword error rate (WER) of speech generated by different SE models (with dry/wet knob technique\nas proposed in (Defossez et al., 2020)) on the VoiceBank-DEMAND noisy test set. Table 13 shows\nthat all the SE can improve the WER performance, and our proposed method can achieve the lowest\nWER.\nTable 13: WER of speech generated by different SE models on the VoiceBank-DEMAND noisy test\nset.\nNoisy\nWiener\nDemucs\nCNN-Transformer\nProposed\nWhisper ASR\n14.25\n12.60\n13.75\n11.84\n11.65\nM\nLIMITATIONS AND FUTURE WORKS\n1) Speech quality estimation:\nOur preliminary experiment results show that the VQScore can obtain LCC around 0.46 with the\nVoiceMOS 2022 challenge test set (Huang et al., 2022). This result is comparable to SpeechLMScore\n(Pre) (0.452) but worse than SpeechLMScore(LSTM)+rep (0.582). One possible reason is that\nVQScore trained on LibriSpeech clean-460 hours only uses <10% training data of SpeechLMScore.\nAnother possible reason is that if we observe each frame generated by a TTS system, it resembles a\nclean frame. In the TTS evaluation, people may focus more on global conditions such as naturalness,\netc. In other words, it cares more about the relation of each frame with each other. On the other hand,\nVQScore pays more attention to the degradation of each frame.\nAs can be observed in Eq. 4, the VQScore is based on the average of the cosine similarity of input\nframes. However, people may put larger weights on the frames with louder volume when evaluating\nspeech quality. It is difficult to design/learn the weights of each frame in the unsupervised setting.\nIf we extend to a semi-supervised framework, we believe this consideration will bring further\nimprovements.\n2) Speech enhancement:\nAs discussed in the previous section, we observed a more pronounced speech distortion in the speech\ngenerated by our method. In fact, the results of our listening test indicate that while our model receives\nhigher scores for noise removal (BAK), its speech distortion score (SIG) is comparatively worse than\nthat of conventional methods. Further analysis revealed that the primary source of speech distortion\nmay come from the finite combination of the discrete tokens in the VQ module. In summary, while\nthe VQ module contributes to the model’s great noise removal capability, it simultaneously introduces\nspeech distortion. One possible solution is to fuse the distorted enhanced speech with the original\nnoisy speech to recover some over-suppressed information (Hu et al., 2023). Our future efforts in the\ndevelopment of this SE approach will be dedicated to mitigating the speech distortion caused by the\nVQ module.\n22\n"
}